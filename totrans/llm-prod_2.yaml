- en: '2 Large Language Models: A Deep Dive Into Language Modeling'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 大型语言模型：深入探究语言建模
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括：
- en: Linguistic background for understanding meaning and interpretation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解含义和解释的语言学背景
- en: A comparative study on language modeling techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模技术的比较研究
- en: Attention and the transformer architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力和Transformer架构
- en: How Large Language Models both fit into and build upon these histories
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型如何融入和建立在这些历史之上
- en: All good stories start with “Once upon a time,” but unfortunately this isn’t
    a story, it’s a book on creating and productionizing LLMs. So instead this chapter
    delves into linguistics as it relates to the development of Large Language Models
    (LLMs), exploring the foundations of semiotics, linguistic features, and the progression
    of language modeling techniques that have shaped the field of natural language
    processing (NLP). We will begin by studying the basics of linguistics and its
    relevance to LLMs in section 2.1, highlighting key concepts such as syntax, semantics,
    and pragmatics, that form the basis of natural language and play a crucial role
    in the functioning of LLMs. We will delve into semiotics, the study of signs and
    symbols, and explore how its principles have informed the design and interpretation
    of LLMs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所有好故事都以“从前有一位”为开头，但不幸的是，本书不是故事，而是关于创建和生产化LLMs的书籍。因此，本章将深入探讨与大型语言模型（LLMs）的开发相关的语言学知识，探索形式语言学的基础、语言特征和塑造自然语言处理（NLP）领域的语言建模技术的演变。我们将从研究语言学的基础和其与LLMs的关联性开始，重点介绍句法、语义和语用等关键概念，这些概念构成了自然语言的基础，并在LLMs的运行中起着至关重要的作用。我们将深入探讨符号学，即符号和符号的研究，并探讨其原则如何影响LLMs的设计和解释。
- en: We will then trace the evolution of language modeling techniques in section
    2.2, providing an overview of early approaches, including N-grams, Naive Bayes
    classifiers, and neural network-based methods such as Multi-Layer Perceptrons
    (MLPs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTM) networks.
    We will also discuss the groundbreaking shift to transformer-based models in section
    2.3, which have laid the foundation for the emergence of LLMs—which are just really
    big transformer based models. Finally, we will introduce LLMs in 2.4 and their
    distinguishing features, discussing how they have built upon and surpassed earlier
    language modeling techniques to revolutionize the field of Natural Language Processing
    (NLP).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将在2.2节中追溯语言建模技术的演变，概述了早期方法，包括N-Gram、朴素贝叶斯分类器以及基于神经网络的方法，如多层感知机（MLP）、循环神经网络（RNN）和长短期记忆（LSTM）网络。然后在2.3节中讨论了基于Transformer模型的革命性转变，这为大型语言模型（LLMs）的出现奠定了基础，而LLMs只是非常大型的基于Transformer模型。最后，在2.4节中介绍了LLMs及其独特的特点，讨论了它们如何在建立在并超过早期的语言建模技术基础上，从而彻底改变了自然语言处理（NLP）领域。
- en: This is a book about LLMs in production. We firmly believe that if you want
    to turn an LLM into an actual product, understanding the technology better will
    improve your results and save you from making costly and time-consuming mistakes.
    Any engineer can figure out how to lug a big model into production and throw a
    ton of resources at it to make it run, but that brain-dead strategy completely
    misses the lessons people have already learned trying to do the same thing before,
    which is what we are trying to solve with LLMs in the first place. Having a grasp
    of these fundamentals will better prepare you for the tricky parts, the gotchas,
    and the edge cases you are going to run into when working with LLMs. By understanding
    the context in which LLMs emerged, we can appreciate their transformative impact
    on NLP and how to enable them to create a myriad of applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一本关于LLMs在生产中的书籍。我们坚决认为，如果你想将LLMs变成实际产品，更好地理解这项技术将提高你的成果，避免犯下代价高昂且耗时的错误。任何工程师都可以弄清楚如何将大型模型引入生产环境并投入大量资源使其运行，但这种愚蠢的策略完全忽略了人们在之前尝试做同样事情时已经学到的教训，而这正是我们首先要解决的问题。掌握这些基础知识将更好地为您准备处理LLMs时可能遇到的棘手部分、陷阱和边界情况。通过了解LLMs出现的背景，我们可以欣赏它们对NLP的变革性影响，以及如何使其能够创建各种应用程序。
- en: 2.1 Language Modeling
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 语言建模
- en: It would be a great disservice to address LLMs in any depth without first addressing
    language, to begin with. To that end, we will start with a brief but comprehensive
    overview of language modelling, focusing on the lessons that can help us with
    modern LLMs. Let’s first discuss levels of abstraction as it will help us garner
    an appreciation for language modelling.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论LLM之前，如果不首先讨论语言，那么就对LLM做了一个极大的伤害，首先我们将从简短但全面的语言模型概述开始，重点是可以帮助我们理解现代LLM的教训。让我们首先讨论抽象层级，因为这将帮助我们欣赏语言建模。
- en: Language, as a concept, is an abstraction of the feelings and thoughts that
    occur to us in our heads. Likewise, math is an abstraction of language, focusing
    on logic and provability, but as any mathematician will tell you, it is a subset
    of language used to describe and define in an organized and “logical” way. From
    math comes the language of binary, a base-2 system of numerical notation consisting
    of either on or off.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 语言作为一个概念，是我们头脑中产生的感觉和思维的抽象。同样，数学是语言的一种抽象，着重于逻辑和可证明性，但正如任何数学家所说的那样，它是一种用于有组织和“逻辑”方式描述和定义的语言的子集。二进制语言就源于数学，它是一种由`开`和`关`组成的二进制数字表示系统。
- en: Binary is very useful abstraction of math, which is an abstraction of language
    which is an abstraction of our feelings because it is what is used underneath
    the hood for everything to do with software, models, and computers in general.
    When computers were first made, we communicated with them through punched cards,
    or binary directly. Unfortunately, this ends up taking too long for humans to
    communicate important things in, so binary was also abstracted to assembly, a
    more human-comprehensible language for communicating with computers. This was
    further abstracted to the high-level assembly language, C, which has been even
    further abstracted to object-oriented languages like Python. The flow we just
    discussed is outlined in Figure 2.1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制是数学的非常有用的抽象，它又是语言的抽象，而语言又是我们感觉的抽象，因为它是软件、模型和计算机中使用的底层工具。当计算机首次制造出来时，我们通过打孔卡片或直接使用二进制与它们交流。不幸的是，人类用这种方式沟通重要事物需要太长时间，因此二进制也被抽象为汇编语言，这是一种更符合人类理解的与计算机交流的语言。这又进一步抽象为高级汇编语言C，甚至进一步抽象为面向对象的编程语言如Python。我们刚才讨论的流程如图2.1所示。
- en: Figure 2.1 We compare cognitive layers of abstraction to programming layers
    of abstraction down to the logical binary abstraction. Python doesn’t come from
    C, nor does it compile into C. Python is, however, another layer of abstraction
    distant from binary. Similarly, language follows a similar path. Each layer of
    abstraction creates a potential point of failure. There are also several layers
    of abstraction to creating a model, each of which are important.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1比较了认知抽象层级与编程抽象层级，直到逻辑二进制抽象。Python并不来自C，也不编译为C。然而，Python是另一种与二进制相距一定抽象层级的语言。类似地，语言也经过类似的路径。每一层的抽象都会造成潜在的错误点。创建模型也有多层次的抽象，每一层都很重要。
- en: '![](images/02__image002.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image002.png)'
- en: This is obviously a reduction, however, it’s useful to understand that the feelings
    you have in your head are the same number of abstractions away from binary, the
    language the computer actually reads, as the languages most people use to program
    in. Some people might argue that there are more steps between Python and binary,
    such as compilers or using assembly to support the C language, and that’s true,
    but there are more steps on the language side too, such as morphology, syntax,
    logic, and agreement.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这是一种简化，但了解一下，你在头脑中所感受到的情感与计算机实际读取的二进制相隔着相同数量的抽象层级，就像大多数人用来编程的语言一样。有些人可能会认为Python和二进制之间存在更多的步骤，例如编译器或使用汇编语言支持C语言，这是正确的，但在语言方面也有更多的步骤，例如形态学、语法、逻辑和一致性。
- en: This can help us understand how difficult the process of getting what we want
    to be understood by an LLM actually is, and even help us understand language modeling
    techniques better. The reason we focus on binary here is simply to illustrate
    that there are a similar number of abstract layers to get from an idea you have
    or from one of our code samples to a working model. Like the children’s telephone
    game where participants whisper into each other's ears, each abstraction layer
    creates a disconnect point or barrier where mistakes can be made.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以帮助我们理解让人工智能语言模型（LLM）理解我们想要表达的内容的过程有多困难，甚至可以帮助我们更好地理解语言建模技术。我们在这里专注于二进制的原因仅仅是为了说明从你有的想法或我们的代码示例之一到一个工作模型的抽象层次是相似的数量。就像儿童的电话游戏，参与者互相耳语一样，每一个抽象层次都会产生一个断开点或障碍，错误可能在这些地方发生。
- en: Figure 2.1 is also meant to illustrate the difficulty in not only creating reliable
    code and language input, but to draw attention to how important the intermediary
    abstraction steps like tokenization and embeddings are for the model itself. Even
    if you have perfectly reliable code and perfectly expressed ideas, the meaning
    may be fumbled by one of those processes before it ever reaches the LLM.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1也旨在说明不仅创建可靠的代码和语言输入的困难，而且要引起注意的是中间的抽象步骤（如标记化和嵌入）对于模型本身是多么重要。即使你有完全可靠的代码和完美表达的想法，含义在到达LLM之前可能会在这些过程中被错误地处理。
- en: In this chapter we will try and help you understand what you can do to reduce
    the risks of these failure points, whether that be on the language, coding, or
    modeling side. Unfortunately, it’s a bit tricky to strike a balance between giving
    you too much linguistics that doesn’t immediately matter for the task at hand
    versus giving you too much technical knowledge that, while useful, doesn’t help
    you develop an intuition for language modeling as a practice. With this in mind,
    you should know that linguistics can be traced thousands of years back in our
    history and there’s lots to learn from it. We’ve included a ***brief*** overview
    for interested readers of how language modeling has progressed over time in Appendix
    A answer encourage you to take a look.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试帮助您理解如何减少这些失败点的风险，无论是在语言、编码还是建模方面。不幸的是，在为当前任务不立即重要的语言学知识和为您提供太多的技术知识之间取得平衡有点棘手，尽管它们很有用，但并不能帮助您培养对语言建模作为一种实践的直觉。在此基础上，您应该知道语言学可以追溯到我们历史上数千年前，并且有很多可以从中学到的东西。我们在附录A中提供了一个***简要***的概述，供有兴趣的读者了解语言建模是如何随着时间的推移发展的，并鼓励您查阅。
- en: Let’s start with our focus on the building blocks that constitute language itself.
    We expect our readers to have at least attempted language modeling before and
    to maybe have heard of libraries like PyTorch and Tensorflow, but we do not expect
    most of our readers to have considered the language side of things before. By
    understanding the essential features that make up language, we can better appreciate
    the complexities involved in creating effective language models and how these
    features interact with one another to form the intricate web of communication
    that connects us all. In the following section, we will examine the various components
    of language, such as phonetics, pragmatics, morphology, syntax, and semantics,
    as well as the role they play in shaping our understanding and usage of languages
    around the world. Let’s take a moment to explore how we currently understand language
    along with the challenges we face that LLMs are meant to solve.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们专注于构成语言本身的基本组成部分开始。我们期望我们的读者至少在尝试过语言建模之前，并可能听说过像PyTorch和Tensorflow这样的库，但我们不指望大多数读者之前曾考虑过语言方面的问题。通过理解构成语言的基本特征，我们可以更好地欣赏创建有效语言模型所涉及的复杂性，以及这些特征如何相互作用形成我们所有人之间相互连接的复杂的交流网络。在接下来的章节中，我们将研究语言的各个组成部分，如语音学、语用学、形态学、语法和语义，以及它们在塑造我们对世界各地语言的理解和使用方面所起的作用。让我们花一点时间来探索我们目前如何理解语言以及我们面临的LLM意图解决的挑战。
- en: 2.1.1 Linguistic Features
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 语言特征
- en: 'Our current understanding of language is that language is made up of at least
    5 parts: Phonetics, Syntax, Semantics, Pragmatics, and Morphology. Each of these
    portions contributes significantly to the overall experience and meaning being
    ingested by the listener in any conversation. Not all of our communication uses
    all of these forms, for example, the book you’re currently reading is devoid of
    phonetics, which is one of the reasons why so many people think text messages
    are unsuited for a more serious or complex conversation. Let’s work through what
    each of these is to figure out how to present them to a language model for a full
    range of communicative power.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前对语言的理解是，语言由至少5个部分组成：语音学、句法学、语义学、语用学和形态学。这些部分中的每一个都对任何对话中听者所接受的整体体验和意义做出了重大贡献。并非所有的交流都使用所有这些形式，例如，你正在阅读的书籍中没有语音学，这就是为什么很多人认为短信不适合进行更严肃或更复杂的对话的原因之一。让我们仔细分析一下这些内容，以确定如何向语言模型呈现它们，以实现全面的沟通能力。
- en: Phonetics
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语音学
- en: Probably the easiest for a language model to ingest, phonetics involves the
    actual sound of the language. This is where accent manifests and deals with the
    production and perception of speech sounds, with phonology focusing on the way
    sounds are organized within a particular language system. Similarly to computer
    vision, while a sound isn’t something necessarily easy to deal with as a whole,
    there’s no ambiguity for how to parse, vectorize, or tokenize the actual sound
    waves. They have a numerical value attached to each part, the crest, the trough,
    and the slope during each frequency cycle. It is vastly easier than text to be
    tokenized and processed by a computer while being no less complex. Sound inherently
    contains more encoded meaning than the text as well, for example, imagine someone
    saying the words “yeah, right,” to you. Could be sarcastic, could be congratulatory,
    depending on the tone and English isn’t even tonal! Phonetics, unfortunately,
    doesn’t have Terabyte-sized datasets commonly associated with it, and performing
    data acquisition and cleaning on phonetic data, especially on the scale needed
    to train an LLM is difficult at best. In an alternate world where audio data was
    more prevalent than text data, and took up a smaller memory footprint, phonetic-based
    or phonetic-aware LLMs would be much more sophisticated, and creating that world
    is a solid goal to work towards.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言模型来说，语音学可能是最容易吸收的，它涉及到语言的实际发音。这就是口音的表现形式，涉及到语音的产生和感知，而音韵学则侧重于声音在特定语言系统内的组织方式。类似于计算机视觉，虽然作为一个整体来处理声音并不一定容易，但如何解析、向量化或标记实际的声波并不模糊。它们在每个部分都附有一个数字值，包括波峰、波谷以及每个频率周期内的斜率。相比文本，它更容易被计算机标记和处理，但同样复杂。声音本质上包含的编码意义比文本更多，例如，想象一下有人对你说“是的，对啊”。可能是讽刺的，也可能是祝贺的，这取决于语调，而英语甚至不是语调语言！不幸的是，语音学通常没有与之相关的千兆字节数据集，并且在语音数据的数据采集和清理方面，尤其是在需要训练LLM的规模上，这是非常困难的。在一个声音数据比文本数据更普遍且占用更小内存空间的替代世界中，基于语音或具有语音感知能力的LLM将会更加复杂，创造这样一个世界是一个可靠的目标。
- en: 'Anticipating this problem, a system created in 1888 called the International
    Phonetic Alphabet (IPA) has been revised in both the 20th and 21st centuries to
    be more concise, more consistent, and more clear, which could be a way to insert
    phonetic awareness into text data. IPA functions as an internationally standardized
    version of every language’s sound profile. A sound profile is the set of sounds
    that a language uses, for example in English, we never have the /ʃ/ (she, shirt,
    sh) next to the /v/ sound. IPA is used to write sounds, rather than writing an
    alphabet or logograms, as most languages do. For example, you could simply describe
    how to pronounce the word “cat” using these symbols: /k/, /æ/, and /t/. That’s
    of course a *very* simplified version of it, but for models it doesn’t have to
    be. You can describe tone and aspiration as well. This could be a happy medium
    between text and speech, capturing some phonetic information. Think of the phrase
    “what’s up?” Your pronunciation and tone can drastically change how you understand
    that phrase, sometimes sounding like a friendly “wazuuuuup,” and other an almost
    threatening, “‘sup,” which IPA would fully capture. IPA isn’t a perfect solution
    though, for example, it doesn’t solve the problem of replicating tone very well.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个问题，创建于1888年的国际音标（IPA）在20世纪和21世纪都进行了修订，使其更加简洁、一致和清晰，这可能是将语音认知融入文本数据的一种方式。
    IPA作为每种语言的音韵特征的国际标准化版本。音韵特征是语言使用的一套音素，例如在英语中，我们绝不会把音素/ʃ/（she, shirt, sh）与/v/音放在一起。
    IPA用来书写音素，而不是像大多数语言那样书写字母或表意文字。例如，你可以用这些符号简单地描述如何发音单词“cat”：/k/、/æ/和/t/。当然这是一个*非常*简化的版本，但对于模型来说，不必太复杂。你还可以描述语调和送气。这可能是文本和语音之间的一种折中，捕捉到一些音韵信息。想想短语“what’s
    up?” 你的发音和语调可以极大地改变你理解这个短语的方式，有时听起来像友好的“wazuuuuup”，有时像一种几乎威胁的“‘sup”，而这些都会被IPA完全捕捉到。然而，IPA并不是一个完美的解决方案，例如，它并不能很好地解决模拟语调的问题。
- en: Phonetics is listed first here because it’s the place that LLMs have been applied
    to the least out of all the features and therefore have the largest space for
    improvement. Even modern TTS and Voice cloning models for the most part end up
    converting the sound to a spectrogram and analyzing that image rather than incorporating
    any type of phonetic language modeling. This is something to look for as far as
    research goes in the coming months and years.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 音韵学被列为首位，因为在所有特点中，LLMs对其应用最少，因此在改进空间方面有最大的可能性。即使是现代的TTS和语音合成模型大部分时间最终会将声音转换为频谱图，并分析该图像，而不是将任何类型的音韵语言建模纳入其中。这是未来几个月甚至几年研究的一个方向。
- en: Syntax
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语法
- en: This is the place where current LLMs are highest-performing, both in parsing
    syntax from the user and generating its own. Syntax is generally what we think
    of as grammar and word order, and is the study of how words can combine to form
    phrases, clauses, and sentences. Syntax is also the first place that language
    learning programs start to help people acquire new languages, especially based
    on where you’re coming from natively. For example, it is important for a native
    English speaker learning Turkish to know that the syntax is completely different,
    and you can often build entire sentences in Turkish that are just one long compound
    word, whereas in English, we never put our subject and verb together into one
    word.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是当前LLM性能最好的领域，既能从用户那里解析语法，又能生成自己的语法。语法通常是我们所说的语法和词序，研究单词如何组合形成短语、从句和句子。语法也是语言学习程序开始帮助人们习得新语言的第一个地方，特别是根据你的母语出发。例如，对于一个以英语为母语的人学习土耳其语来说，知道语法完全不同很重要，你可以经常构建完全由一个长复合词组成的土耳其语句子，而在英语中，我们从来不把主语和动词合为一个词。
- en: 'Syntax is largely separate from meaning in language, as the famous sentence
    from Noam Chomsky the so-called father of syntax demonstrates: “Colorless green
    ideas sleep furiously.” Everything about that sentence is both grammatically correct
    and semantically understandable. The problem isn’t that it doesn’t make sense,
    it’s that it does, and the encoded meanings of those words conflict. This is a
    reduction, however, you can think of all the times LLMs give nonsense answers
    as this phenomenon manifesting. Unfortunately for us, the syntax is also where
    ambiguity is the most commonly found. Consider the sentence, “I saw an old man
    and woman.” Now answer the question: is the woman also old? This is syntactic
    ambiguity, where we aren’t sure whether the modifier “old” applies to all people
    in the following phrase or just the one it immediately precedes. This is less
    consequential than the fact that semantic and pragmatic ambiguity also show up
    in syntax. Consider this sentence now, “I saw a man on a hill with a telescope,”
    and answer the question: Where is the speaker, and what are they doing? Is the
    speaker on the hill cutting a man in half using a telescope? Likely, you didn’t
    even consider this option when you read the sentence, because when we interpret
    syntax, all of our interpretations are at least semantically and pragmatically
    informed. We know from lived experience that that interpretation isn’t at all
    likely, so we throw it out immediately, usually without even taking time to process
    that we’re eliminating it from the pool of probable meanings. Think about this
    later as we’re doing projects with LLMs.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 语法与语言意义大体上是分开的，正如语法之父诺姆·乔姆斯基（Noam Chomsky）所说的那句名言：“无色的绿色思想在狂暴地睡眠。” 这句话的一切都在语法上是正确的，并且在语义上是可以理解的。问题不在于它毫无意义，而在于它确实有意义，而且这些词的编码意义相互冲突。然而，这只是一种简化，你可以把所有时候大型语言模型给出荒谬答案看作是这种现象的表现。不幸的是，语法也是歧义最常见的地方。考虑一下这个句子，“我看见一个老人和一个女人。”现在回答这个问题：这个女人也老吗？这是语法歧义，我们不确定修饰语“老”的范围是适用于后续短语中的所有人还是仅仅适用于紧接着的那个人。这比语义和语用歧义出现在语法中更不重要。现在考虑这个句子，“我在一个山上看见一个人，他手里拿着望远镜”，然后回答这个问题：说话者在哪里，正在做什么？说话者在山上用望远镜将一个人割开吗？可能你在读这个句子时甚至没有考虑到这个选项，因为当我们解释语法时，我们所有的解释至少在语义和语用上都是得到了启发的。我们从生活经验中知道，那种解释根本不可能发生，所以我们立即将其排除，通常甚至没有时间去处理我们将其从可能含义的范围中消除的事实。当我们与大型语言模型进行项目时，请稍后考虑这一点。
- en: It shouldn’t take any logical leap for why LLMs need to be syntax-aware in order
    to be high-performing. LLMs that don’t get word order correct or generate nonsense
    aren’t usually described as “good.” LLMs being syntax-dependent is something that
    has prompted even, Chomsky to call LLMs “stochastic parrots.” In the authors’
    opinions, GPT2 in 2018 was when language modeling solved syntax as a completely
    meaning-independent demonstration, and we’ve been happy to see the more recent
    attempts to combine the syntax that GPT2 output so well with encoded and entailed
    meaning, which we’ll get into now.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于为什么大型语言模型需要具备语法意识以实现高性能，不应该需要任何逻辑推理。那些不能正确排序单词或生成无意义内容的大型语言模型通常不被描述为“优秀”。大型语言模型对语法的依赖性是导致乔姆斯基甚至称大型语言模型为“随机鹦鹉”的原因之一。在作者看来，2018年的GPT2是语言建模将语法解决为完全独立于意义的演示，我们很高兴看到最近的尝试将GPT2如此出色的语法与编码和暗示的意义结合起来，我们将在接下来详细讨论。
- en: Semantics
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义
- en: 'Semantics are the literal encoded meaning of words in utterances. People automatically
    optimize semantic meaning, only using words that they consider meaningful in the
    current language epoch. If you’ve ever created or used an embedding with language
    models (word2vec, ELMO, BERT [the E is for Embedding], MUSE, etc.]) you’ve used
    a semantic approximation. Words often go through semantic shifts, and while we
    won’t cover all of this topic nor go in-depth, here are some common ones you may
    already be familiar with: narrowing, a broader meaning to a more specific one,
    broadening, the inverse of narrowing going from a specific meaning to a broad
    one, and reinterpretations, going through whole or partial transformations. These
    shifts do not have some grand logical underpinning. They don’t even have to correlate
    with reality, nor do speakers of a language hardly ever consciously think about
    the changes as they’re happening. That doesn’t stop the change from occurring,
    and in the context of language modeling it doesn’t stop us from having to keep
    up with that change.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 语义是话语中词语的字面编码含义。人们会自动优化语义含义，只使用他们认为在当前语言时期有意义的词语。如果你曾经创建或使用过语言模型的嵌入（如word2vec、ELMO、BERT【E代表嵌入】、MUSE等），你就使用了语义近似。词语经常经历语义转变，虽然我们不会涵盖这个话题的全部内容，也不会深入讨论，但下面是一些你可能已经熟悉的常见情况：狭义化，从更广泛的含义变为更具体的含义；扩义，与狭义化相反，从具体含义变为广泛含义；重新解释，经历整体或部分转变。这些转变并没有某种伟大的逻辑基础。它们甚至不必与现实相关，说某种语言的人几乎从来不会在这些变化发生时有意识地考虑。但这并不妨碍变化的发生，在语言建模的背景下，这也不会阻止我们跟上这种变化的步伐。
- en: 'Some examples: narrowing includes “deer” which in Old and Middle English just
    meant any wild animal, even a bear or a cougar, and now means only one kind of
    forest animal. For broadening we have “dog” which used to refer to only one canine
    breed from England, and now can be used to refer to any domesticated canine. One
    fun tangent about dog-broadening is in the *FromSoft* game *Elden Ring,* where
    because of a limited message system between players, they will use “dog” to refer
    to anything from a turtle to a giant spider and literally everything in between.
    For reinterpretation, we can consider “pretty” which used to mean clever or well-crafted,
    not visually attractive. Another good example is “bikini” which went from referring
    to a particular atoll, to referring to clothing you might have worn when visiting
    that atoll to people acting as if the “bi-” was referring to the two-piece structure
    of the clothing, thus inventing the tankini and monokini. Based on expert research
    and decades of study, we can think of language as being constantly compared and
    reevaluated by native language speakers out of which common patterns emerge. The
    spread of those patterns is closely studied in sociolinguistics and is largely
    out-of-scope for the current purpose, but we encourage the reader to look into
    it if interested, as sociolinguistic phenomena such as prestige can help in designing
    systems that work well for everyone.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子：狭义化包括“deer”，在古英语和中古英语中它只是指任何野生动物，甚至包括熊或美洲狮，而现在只指一种森林动物。扩义的例子是“dog”，它曾经只指英国的一种犬类，现在可以用来指任何驯化的犬类动物。关于“dog”扩义的有趣细节是在FromSoft的游戏《Elden
    Ring》中，由于玩家之间的信息传递系统有限，他们会用“dog”来指代从海龟到巨型蜘蛛甚至所有中间形态的任何东西。关于重新解释，我们可以考虑“pretty”，它曾经意味着聪明或制作精良，而不是视觉上吸引人。另一个很好的例子是“bikini”，它从指代一个特定的环礁，变成指代你在访问那个环礁时可能穿的衣服，最后人们把“bi-”解释为衣服的两部分结构，于是发明了泳装和连体泳衣。根据专家的研究和几十年的研究，我们可以把语言看作是由母语人士不断比较和重新评估的，其中会出现共同的模式。这些模式的传播在社会语言学中得到了密切研究，这在当前目的上大多不在讨论范围之内，但我们鼓励读者如果感兴趣可以研究一下，因为社会语言学现象如声望可以帮助设计对每个人都有效的系统。
- en: In the context of LLMs, so-called semantic embeddings are vectorized versions
    of text that attempt to mimic semantic meaning. The most popular way of doing
    this currently is by tokenizing or assigning an arbitrary number in a dictionary
    to each subword in an utterance (think prefixes, suffixes, and morphemes generally),
    applying a continuous language model to increase the dimensionality of each token
    within the vector so that there’s a larger vector representing each index of the
    tokenized vector, then applying a positional encoding to each of those vectors
    to capture word order. Each subword ends up being compared to other words in the
    larger dictionary based on how it’s used. We’ll show an example of this later.
    Something to consider when thinking about word embeddings is that they struggle
    to capture deep encoded meaning of those tokens, and simply adding more dimensions
    to the embeddings hasn’t shown marked improvement. One evidence that embeddings
    are working in a similar way to humans is that you can apply a distance function
    to related words and see that they are closer together than unrelated words. This
    is another area to expect groundbreaking research in the coming years and months
    for how to capture and represent meaning more completely.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的语境中，所谓的语义嵌入是文本的向量化版本，试图模拟语义含义。目前最流行的做法是通过对话中的每个子词（例如前缀、后缀和词素）进行标记或分配字典中的任意数字，应用连续语言模型来增加向量中每个令牌的维度，以便有一个更大的向量表示每个令牌的每个索引，然后对每个向量应用位置编码来捕获词序。每个子词最终都会根据其用法与较大词典中的其他词进行比较。考虑词嵌入时需要考虑的一点是，它们很难捕捉到这些令牌的深层编码含义，而仅仅增加嵌入的维度并没有显示出明显的改善。证明嵌入正在以类似于人类的方式工作的一个证据是，你可以将一个距离函数应用于相关的单词，看到它们比不相关的单词更接近。这是另一个可以期待在未来几年和几个月内进行开创性研究的领域，以更完整地捕捉和表示含义。
- en: Pragmatics
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实用语用学
- en: Sometimes omitted from linguistics, due to its referent being all the non-linguistic
    context affecting a listener’s interpretation and the speaker’s decision to express
    things in a certain way. Pragmatics refers in a large part to dogmas followed
    in cultures, regions, socio-economic classes, and shared lived experiences played
    off of to take shortcuts in conversations using entailment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会被语言学省略，因为其指称对象是影响听者解释和说话者决定以某种方式表达事物的所有非语言语境。语用学在很大程度上指的是文化、地区、社会经济阶层和共同的生活经验中遵循的教条，通过使用蕴涵在对话中采取捷径。
- en: If I were to say, “A popular celebrity was just taken into the ICU,” your pragmatic
    interpretation based on lived experience might be to assume that a well-beloved
    person has been badly injured and is now undergoing medical treatment in a well-equipped
    hospital. You may wonder about which celebrity it is, whether they will have to
    pay for the medical bills, or if the injury was self-inflicted, also based on
    your lived experience. None of these things can be inferred directly from the
    text and its encoded meaning by itself. You would need to know that ICU stands
    for a larger set of words, and what those words are. You would need to know what
    a hospital is and why someone would need to be taken there instead of going there
    themselves. If any of these feel obvious, good. You live in a society and your
    pragmatic knowledge of that society overlaps well with the example provided. If
    I share an example from a less-populated society, “Janka got her grand-night lashings
    yesterday, she’s gonna get Peter tomorrow” you might be left scratching your head.
    If you are, realize this probably looks like how a lot of text data ends up looking
    to an LLM (anthropomorphization acknowledged). For those wondering, this sentence
    comes from Slovak Easter traditions. There’s a lot of meaning here that will just
    be missed and go unexplained if you are unaccustomed to these particular traditions
    as they stand in that culture. I personally have had the pleasure of trying to
    explain the Easter Bunny and its obsession with eggs to foreign colleagues and
    enjoyed the satisfaction of looking crazy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我说，“一个知名的名人刚刚被送进了ICU”，根据生活经验，你可能会推断一位备受喜爱的人受了严重的伤，现在正在一家设备齐全的医院接受治疗。你可能会想知道是哪位名人，他们是否需要支付医疗费用，或者伤害是否是自己造成的，这些也是基于你的生活经验。这些都不能直接从文本及其编码的意义中推断出来。你需要知道ICU代表一系列更广泛的词语，以及这些词语是什么。你需要知道医院是什么，以及为什么有人需要被送到那里而不是自己去。如果这些都感觉很明显，那很好。你生活在一个社会中，你对社会的语用知识与提供的例子之间有很好的重叠。如果我分享一个来自人口较少的社会的例子，“Janka昨天晚上挨了打，明天轮到Peter了”，你可能会感到困惑。如果你确实感到困惑，意识到这可能就是LLM看待很多文本数据的方式（人格化得到认可）。对于那些想知道的人，这个句子来自斯洛伐克的复活节传统。这里有很多意义，如果你不习惯这个文化中这些特定传统，那么这些意义将被忽略和不解释。我个人有幸尝试向外国同事解释复活节兔子及其对蛋的痴迷，并享受着看起来像个疯子的满足感。
- en: In the context of LLMs, we can effectively group all out-of-text context into
    pragmatics. This means LLMs start without any knowledge of the outside world,
    and do not gain it during training. They only gain a knowledge of how humans respond
    to particular pragmatic stimuli. LLMs do not understand social class or race or
    gender or presidential candidates or anything else that might spark some type
    of emotion in you based on your life experience. Pragmatics isn’t something that
    we expect will be able to be directly incorporated into a model at any point,
    but we have already seen the benefits of incorporating it indirectly through data
    engineering and curation, prompting, and supervised fine-tuning on instruction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的语境中，我们可以有效地将所有与文本无关的内容归为语用学范畴。这意味着LLM在开始时没有任何关于外部世界的知识，并且在训练过程中也不会获取。它们只会获取人类对特定语用刺激的反应知识。LLM并不理解社会阶级、种族、性别、总统候选人或者其他可能基于个人生活经验引发某种情绪的事物。语用学不是我们期望直接整合到模型中的内容，但我们已经通过数据工程和策划、提示以及指导下的监督微调看到了间接整合的好处。
- en: Pragmatic structure gets added whether you mean to add it or not as soon as
    you acquire the data you are going to train on. You can think of this type of
    pragmatic structure as bias, not inherently good or bad, but impossible to get
    rid of. Later down the line you get to pick what types of bias you’d like your
    data to keep by normalizing and curating, augmenting particular underrepresented
    points and cutting overrepresented or noisy examples.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你获得了要训练的数据，无论你是否打算，语用结构都会被添加进去。你可以将这种类型的语用结构视为偏见，它并非本质上的好或者坏，但无法摆脱。后面你可以通过规范化和策划来选择你希望你的数据保留的偏见类型，增加特定的代表不足的点，减少代表过多或噪音干扰的例子。
- en: There’s a fine line between data engineering and pragmatic context, and it’s
    just a matter of understanding what entailments exist in your data. An entailment,
    is a pragmatic marker within your data, as opposed to the literal text that your
    dataset contains. For example, let’s say you have a model attempting to take an
    input like “write me a speech about frogs eating soggy socks that doesn’t rhyme
    and where the first letters of each line spell amphibian,” and actually follow
    that instruction. You can immediately tell that this input is asking for a lot.
    The balance for you as a data engineer would be to make sure that everything the
    input is asking for is explicitly accounted for in your data. You need to have
    examples of speeches, examples of what frogs and socks are and how they behave,
    and examples of acrostic poems. If you don’t, the model might be able to understand
    just from whatever entailments exist in your dataset, but it’s pretty up in the
    air. If you go the extra mile and keep track of entailed vs explicit information
    and tasks in your dataset, along with data distributions, you’ll have examples
    to answer, “What is the garbage-in resulting in our garbage-out?”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程和语用语境之间存在着微妙的界限，这只是理解你的数据中存在的蕴涵。蕴涵是你的数据中的语用标记，而不是你的数据集中包含的字面文本。例如，假设你有一个模型试图接受像“写一篇关于青蛙吃湿袜子的演讲，不要押韵，每行的首字母拼成两栖动物”这样的输入，并实际按照这个指令来操作。你可以立即知道这个输入要求很多。作为数据工程师，你的平衡点就是确保输入要求在你的数据中被明确考虑到。你需要有演讲的例子，青蛙和袜子以及它们的行为的例子，还有藏头诗的例子。如果没有这些，模型可能仅仅能够从数据集中存在的蕴涵中理解，但是结果可能不确定。如果你更上一层楼并且记录数据集中的蕴涵信息和显性信息以及任务，还有数据分布，你就会有例子来回答“碎垃圾导致了什么样的结果？”
- en: LLMs struggle to pick up on pragmatics, even more so than people, but they do
    pick up on the things that your average standard deviation of people would. They
    can even replicate responses from people outside that standard deviation, but
    pretty inconsistently without the exact right stimulus. This is where everything
    during and after training comes in. Instruction-based datasets attempt to manufacture
    those stimuli during training by asking questions that entail representative responses.
    It is impossible to account for every possible situation in training, and you
    may inadvertently create new types of responses from your end users by trying
    to account for everything. After training, you can coax particular information
    from your model through prompting, which has a skill ceiling based on what your
    data originally entailed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs很难理解语用语言学，甚至比人类更难，但它们能够捕捉到大多数人的平均标准差。它们甚至可以复制来自标准差之外的人的回答，但是在没有准确的刺激下很难保持一致。这就是培训期间和培训后的关键。基于指令的数据集试图通过在培训过程中问问题来制造这些刺激，从而得到代表性的回答。在培训中不可能考虑到每种可能的情况，而且你试图考虑到所有情况可能会无意中造成用户新类型的回答。在培训之后，你可以通过提示从模型中获取特定的信息，这取决于你的数据最初包含的内容而有一定的限度。
- en: Morphology
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词形变化。
- en: Morphology is the study of word structures and how they are formed from smaller
    units called morphemes. Morphemes are the smallest units of meaning, like the
    "re-" in "redo" or "relearn." However, not all parts of words are morphemes, such
    as "ra-" in "ration" or "na-" in "nation."
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 词形变化是研究词结构以及词是如何由更小的单元（称为词素）组成的。词素是最小的具有意义的单位，例如“重新”在“重新做”或“重新学习”中。但是，并非所有单词的部分都是词素，比如“饲料”中的“ra-”或“国家”中的“na-”。
- en: Understanding how words are constructed helps create better language models
    and parsing algorithms, which are essential for tasks like tokenization. Tokens
    are somewhere between words and morphemes, they are statistically the most-likely
    candidates for units of meaning, but don’t necessarily correspond to existing
    morphemes. The effectiveness of a language model can depend on how well it can
    understand and process these tokens. For instance, in tokenization, a model needs
    to store a set of dictionaries to convert between words and their corresponding
    indices. One of these tokens is usually an "<UNK>" token, which represents any
    word that the model does not recognize. If this token is used too frequently,
    it can hinder the model's performance, either because the model's vocabulary is
    too small or because the tokenizer is not using the right algorithm for the task.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 理解单词构造的方式有助于创建更好的语言模型和解析算法，这对于诸如标记化之类的任务至关重要。标记介于单词和形态素之间，在统计上它们是最可能表示意义单位的候选词，但不一定对应现有的形态素。语言模型的有效性可能取决于它有多么好地理解和处理这些标记。例如，在标记化中，模型需要存储一组字典，以便在单词和它们对应的索引之间进行转换。其中一个标记通常是"<UNK>"标记，它代表着模型不认识的任何单词。如果这个标记使用得太频繁，它可能会妨碍模型的性能，要么是因为模型的词汇量太小，要么是因为标记器没有为任务使用正确的算法。
- en: Consider a scenario where you want to build a code completion model, but you're
    using a tokenizer that only recognizes words separated by whitespace, like the
    nltk "punkt" tokenizer. When it encounters the string "def add_two_numbers_together(x,
    y):," it will pass "[def, <UNK>, y]" to the model. This causes the model to lose
    valuable information, not only because it doesn't recognize the punctuation, but
    also because the important part of the function's purpose is replaced with an
    unknown token due to the tokenizer's morphological algorithm. To improve the model's
    performance, a better understanding of word structure and the appropriate parsing
    algorithms is needed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，你想构建一个代码补全模型，但是你使用的标记器只识别由空格分隔的单词，就像nltk的"punkt"标记器一样。当它遇到字符串"def add_two_numbers_together(x,
    y):,"时，它会将"[def, <UNK>, y]"传递给模型。这导致模型丢失了宝贵的信息，不仅因为它无法识别标点符号，还因为函数的重要部分由于标记器的形态学算法而被替换为未知标记。要提高模型的性能，需要更好地理解单词结构和适当的解析算法。
- en: 2.1.2 Semiotics
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 符号学
- en: After exploring the fundamental features of language and examining their significance
    in the context of large language models, it is important to consider the broader
    perspective of meaning-making and interpretation in human communication. Semiotics,
    the study of signs and symbols, offers a valuable lens through which we can better
    understand how people interpret and process language. In the following section,
    we will delve into the realm of semiotics, examining the relationship between
    signs, signifiers, and abstractions, as well as how these elements are utilized
    by LLMs to generate meaningful output. This discussion will provide a deeper understanding
    of the intricate processes through which LLMs manage to mimic human-like understanding
    of language, while also shedding light on the challenges and limitations they
    face in this endeavor. It should be noted that the authors do not believe that
    mimicking human behavior is necessarily the right answer for LLM improvement,
    only that mimicry is how the field has evaluated itself so far.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 探讨了语言的基本特征并在大型语言模型的语境中考察它们的重要性之后，重要的是考虑人类交流中意义构建和解释的更广阔视角。符号学，即标志和符号的研究，为我们提供了一个宝贵的视角，通过这个视角我们可以更好地理解人们如何解释和处理语言。在接下来的部分中，我们将深入探讨符号学的领域，考察符号、指示符和抽象之间的关系，以及LLMs如何利用这些元素生成有意义的输出。这次讨论将更深入地理解LLMs模拟人类语言理解的复杂过程，同时也揭示了它们在这一努力中面临的挑战和局限性。值得注意的是，作者并不认为模仿人类行为对LLM的改进必然是正确答案，只是模仿是该领域迄今为止评估自己的方式。
- en: In our introduction to semiotics let’s consider Figure 2.2 an adapted Peircean
    semiotic triangle. These are used to organize base ideas into sequences of firstness,
    secondness, and thirdness, with firstness being at the top left, secondness at
    the bottom, and thirdness being at the top right. If you’ve ever seen a semiotic
    triangle before, you may be surprised at the number of corners and orientation.
    To explain, we’ve turned them upside down to make it slightly easier to read,
    and because the system is recursive, we’re showing how the system can model the
    entire process and each piece individually simultaneously. While the whole concept
    of these ideas is very cool, it’s outside of the scope of this book to really
    delve into the philosophy. Instead, we can focus on the cardinal parts of those
    words (first, second, third) as showing the sequence things are processed in.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 This is a recursive Peircean semiotic triangle. It’s a system of
    organizing the process of extracting meaning from anything, in our case from language.
    Each point on the triangle illustrates one of the minimal parts needed to synthesize
    meaning within whatever the system is being used to describe, so here, each of
    these points are minimal units in meaning for language. Firstness, Secondness,
    and Thirdness are not points on the triangle, more markers for the people versed
    in Semiotics to be able to orient themselves in this diagram.
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image003.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: We can also look at each intersection of each of the triangles to gain an idea
    of why things are presented in the order they are. Feelings can be attached to
    images and encodings way before they can be attached to words and tables. Ritual
    and common scripts give a space for interpreted action that’s just second nature
    and doesn’t have to be thought about, similarly to how most phrases just come
    together from words without the native speaker needing to perform metacognition
    about each word individually. All of these eventually lead towards an interpretation
    or a document (a collection of utterances), and in our case, that interpretation
    should be reached by the LLM. This is why, for example, prompt engineering can
    boost model efficacy. Foundation LLMs that have trained on millions of examples
    of ritual scripts are able to replicate the type of script significantly better
    when you explicitly tell the model in the prompt which script needs to be followed.
    Try asking the model to give a step-by-step explanation, maybe prepend your generation
    with “Let’s think about this step-by-step”, you will see the model will generate
    step by step scripts based on previous scripts it’s seen play out.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: For those interested, there are specific ways of reading these figures and a
    whole field of semiotics to consider, however, it’s not guaranteed that you’ll
    be able to create the best LLMs by understanding the whole thing. Instead of diving
    really deeply into this, we’ll consider the bare minimum that can help you build
    the best models, UX, and UI for everyone to interact with. For example, one aspect
    of the process of creating meaning is recursiveness. When someone is talking to
    you, and they say something that doesn’t make sense (is “meaningless” to you),
    what do you do? Generally, people will ask one or more clarifying questions to
    figure out what the meaning is, and the process is started over and over until
    the meaning is put across. The most state-of-the-art models that are currently
    on the market do not do this, but they can be made to do it through very purposeful
    prompting, but many people wouldn’t even know to do that without having it pointed
    out to them. In other words, this is a brief introduction to semiotics. You don’t
    need to be able to give in-depth and accurate coordinate-specific explanations
    to experts in the semiotic field by the end of this section. The point I’m really
    trying to push is that this is an organizational system showcasing the minimum
    number of things you actually need to create a full picture of meaning for another
    person to interpret. We are not giving the same amount of the same kinds of information
    to our models during training, but if we did, it would result in a marked improvement
    in model behavior.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些感兴趣的人来说，有特定的方法来阅读这些图表，以及整个符号学领域需要考虑的内容，然而，并不能保证通过理解整个过程就能创建最好的LLM。与其深入研究这个问题，我们将考虑能帮助您为所有人构建最佳模型、用户体验和用户界面的最低要求。例如，创建意义的过程的一个方面是递归性。当有人对你说一些对你来说毫无意义的话时，你会怎么做？通常，人们会问一个或多个澄清问题，以弄清意思，这个过程一遍又一遍地开始，直到意思被传达。目前市场上最先进的模型并没有做到这一点，但通过非常有目的的提示可以做到这一点，但许多人甚至不知道要这样做，除非有人指出。换句话说，这是对符号学的简要介绍。你不需要能够在本节结束时向符号学领域的专家提供深入和准确的坐标特定解释。我真正想强调的是，这是一个组织系统，展示了你实际上需要创建另一个人能够解释的意义的完整图景的最少数量的事物。在训练过程中，我们没有向我们的模型提供相同数量和相同类型的信息，但如果我们这样做了，模型的行为将会显著改善。
- en: These figures are meant to represent a minimal organizational model, where each
    of these pieces is essential. Let’s consider Figure 2.3 which walks through an
    example of using a semiotic triangle. Consider Images, Pictures, and Memories
    and think about what it would be like to try and absorb the knowledge in this
    book without your eyes to process images, and without orthography (a writing system)
    to abstract the knowledge. Looking at Bullet Points, etc., how could you read
    this book without sections, whitespace between letters, and bullet points to show
    you the order and structure to process information with? Look at Semantics and
    literal encoded meaning and imagine the book without diagrams and words that didn’t
    have dictionary definitions. Looking at spreadsheets in the middle, that could
    be a book without any tables or comparative informational organizers, including
    these figures. What would it be like trying to read this book without a culture
    or society that has habits and dogma to use as a lens for our interpretations?
    All of these points form our ability to interpret information, along with the
    lens that we end up passing our information through to recognize patterns.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表旨在代表一个最小的组织模型，其中每个部分都是必不可少的。让我们考虑一下图2.3，它展示了使用符号三角形的示例。考虑一下图像、图片和记忆，并想象一下如果没有眼睛来处理图像，没有文字来抽象知识，那么试图吸收这本书中的知识会是什么样子。看看项目符号等，如果没有章节，字母之间的空白和项目符号来显示你处理信息的顺序和结构，你怎么能读这本书？看看语义学和文字编码的意义，想象一下如果没有图表和没有字典定义的词，这本书会是什么样子。看看中间的电子表格，那可能是一本没有任何表格或比较信息组织者的书，包括这些图表。如果没有一个习惯和教条的文化或社会来作为我们解释的镜头，那么试图读这本书会是什么样子？所有这些观点构成了我们解释信息的能力，以及我们最终通过的镜头来识别模式。
- en: Figure 2.3 Starting at the top left corner, follow the arrows to see the general
    order that we use to build our interpretations and extract meaning from things
    we interact with. Here, we’ve replaced the descriptive words with some examples
    of each point. Try to imagine interpreting this diagram without any words, without
    examples, without the arrows, or even without the pragmatic context of knowing
    what a figure in a book like this is supposed to be for.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image004.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'So the important question then is: how many of these things do you see LLMs
    having access to to return meaningful interpretations? Does an LLM have access
    to feelings or societal rituals? Currently, they do not, but think about this
    as we go through traditional and newer techniques for NLP inference and think
    about what different models have access to.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Multilingual NLP
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last challenge that we need to touch on before we evaluate previous NLP
    techniques and current-generation LLMs is the foundation of linguistics and the
    reason LLMs even exist. People have wanted to understand or exploit each other
    since the first civilizations made contact. These cases have resulted in the need
    for translators, and this need has only exponentially increased as the global
    economy has grown and flourished.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: It’s pretty simple math for business as well. Did you know that there are almost
    as many native speakers of Bengali as there are native speakers of English? If
    this is the first time you’ve heard of the Bengali language, this should hopefully
    color your perception that there is a valuable market for multilingual models.
    There are billions of people in the world, but only about a third of one billion
    of them speak English natively. If your model is anglocentric, like most are,
    you are missing out on 95% of the people in the world as customers and users.
    Spanish and Mandarin Chinese are easy wins in this area, but more people don’t
    even go that far.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: There are many more politically-charged examples of calling things the same
    or different languages that are out of the scope of this book. These are most
    often because of external factors like government involvement. Keeping these two
    points in mind–that a monolingual system focusing on English doesn’t have the
    coverage or profit potential as many businesses act like and that the boundaries
    between languages and dialects are unreliable at best and systematically harmful
    at worst–it should highlight the dangerous swamp of opinions. Many businesses
    and research scientists don’t even pretend to want to touch this swamp with a
    50-foot pole when designing a product or system.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, no easy solutions exist at this time. However, the consideration
    of these factors can help you as a scientist or engineer (and hopefully an ethical
    person) to design LLMs that, at the very least, don’t exacerbate and negatively
    contribute to the problems that already exist. The first step in this process
    is deciding on a directional goal from the beginning of the project, either towards
    localization (L10n) or internationalization (I18n). Localization is an approach
    exemplified by Mozilla, which has a different version of their browser available
    through crowdsourced L10n in over 90 languages with no indications of stopping
    that effort. Internationalization is similar, but in the opposite direction, for
    example, Ikea tries to put as few words as possible in their instructional booklets,
    opting instead for internationally recognized symbols and pictures to help customers
    navigate the DIY projects. Deciding at the beginning of the project cuts down
    the effort required to expand to either solution exponentially. Large enough to
    switch the perception of translation and formatting from a cost to an investment.
    In the context of LLMs and their rapid expansion across the public consciousness,
    it becomes even more important to make that consideration early. Hitting the market
    with a world-changing technology that automatically disallows most of the world
    from interacting with it devalues those voices. Having to wait, jeopardizes their
    economic prospects.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing, let’s take a moment to reflect on what we’ve discussed so
    far. We’ve hit important points in linguistics illustrating concepts for us to
    consider like understanding that the structure of language is separate from its
    meaning. We have demonstrated quite a journey that each of us takes, both personally
    and as a society, towards having the metacognition to understand and represent
    language in a coherent way for computers to work with. This understanding will
    only improve as we deepen our knowledge of cognitive fields, and as we solve for
    the linguistic features that we encounter. Going along with Figure 2.1, we will
    now demonstrate the computational path for language modeling that we have followed,
    and explore how it has and hasn’t solved for any of those linguistic features
    or strived to create meaning. Let’s move into evaluating the various techniques
    for representing a language algorithmically.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Language Modeling Techniques
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having delved into the fundamental features of language, the principles of semiotics,
    and the ways in which large language models interpret and process linguistic information,
    we now transition into a more practical realm. In the following section, we will
    explore the various natural language processing techniques that have been developed
    and employed to create these powerful language models. By examining the strengths
    and weaknesses of each approach, we will gain valuable insights into the effectiveness
    of these techniques in capturing the essence of human language and communication.
    This knowledge will not only help us appreciate the advancements made in the field
    of NLP but also enable us to better understand the current limitations of these
    models and the challenges that lie ahead for future research and development.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入研究语言的基本特征、符号学的原则以及大型语言模型解释和处理语言信息的方式，我们现在将转向更加实际的领域。在接下来的部分中，我们将探讨各种自然语言处理技术的发展和应用，这些技术已被用于创建这些强大的语言模型。通过检验每种方法的优势和弱点，我们将对这些技术在捕捉人类语言和交流精髓方面的有效性获得宝贵的见解。这样的知识不仅将帮助我们欣赏自然语言处理领域取得的进步，而且还将使我们更好地了解这些模型当前的局限性以及未来研究和发展所面临的挑战。
- en: Let’s take a second to just go over some data processing that will be universal
    to all language modeling. First, we’ll need to decide how we want to break up
    the words and symbols that we’ll be passing into our model, effectively deciding
    what a token will be in our model. Then, we’ll need a way to convert those tokens
    to numerical values and back again. Then, we’ll need to pick how our model will
    actually process the tokenized inputs. Each of the following techniques will build
    upon the previous techniques in at least one of these ways.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间来回顾一些数据处理，这些处理将适用于所有语言建模。首先，我们需要决定如何分割要传入模型的单词和符号，有效地决定我们模型中的一个标记将是什么。然后，我们需要找到一种方法将这些标记转换为数字值，然后再转换回来。接下来，我们需要选择我们的模型将如何处理标记化输入。以下每种技术都将至少在其中一种方面基于先前的技术进行构建。
- en: The first of these techniques is called a Bag of Words (BoW) model, and it consists
    of simply counting words as they appear in text. It can be accomplished very easily
    with a dictionary that scans through text, creating a new vocabulary entry for
    each new word as a key and an incrementing value starting at 1\. Considering its
    simplicity, even this model based entirely on frequency can be quite powerful
    when trying to gain insight into a speaker’s intentions or at least their idiosyncrasies.
    For example, you could run a simple BoW model on inaugural speeches of US presidents,
    searching for the words freedom, economy, and enemy to gain a pretty good insight
    about which presidents assumed office under peacetime, during wartime, and during
    times of monetary strife, just based on how many times each word was mentioned.
    The BoW model’s weaknesses are many, however, as the model provides no images,
    semantics, pragmatics, phrases, or feelings. It doesn’t have any mechanisms to
    evaluate context or phonetics, and because it divides words by default on white
    space (you can obviously tokenize however you want, but try tokenizing on subwords
    and see what happens with this model—spoiler it is bad), it doesn’t account for
    morphology either. Altogether, it should be considered a weak model for representing
    language, but a strong baseline for evaluating other models against. In order
    to solve the problem of Bag of Words models not capturing any sequence data, N-Gram
    models were conceived.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术中的第一种被称为词袋（BoW）模型，它只是简单地计算文本中单词的出现次数。它可以很容易地通过一个扫描文本的字典来完成，为每个新单词创建一个新的词汇表条目作为关键字，并以1开始递增的值。考虑到它的简单性，即使完全基于频率的这种模型在试图洞悉演讲者意图或至少是他们的怪癖时也可能非常有用。例如，你可以在美国总统就职演讲中运行一个简单的BoW模型，搜索自由、经济和敌人这几个词，以便从提及每个单词的次数多少来相当不错地洞悉哪位总统是在和平时期上任的、在战时上任的以及在货币困难时期上任的。然而，BoW模型也有很多弱点，因为这种模型不提供任何图像、语义、语用、短语或感情。它没有任何机制来评估上下文或语音学，并且因为它默认以空格划分单词（显然你可以按照自己的方式进行标记化，但尝试在子词上进行标记化并观察这个模型会发生什么事情——
    剧透，结果糟糕），它也不考虑形态。总的来说，它应该被认为是一种代表语言的弱模型，但也是评估其他模型的强基准。为了解决词袋模型不捕捉任何序列数据的问题，N-Gram模型应运而生。
- en: 2.2.1 N-Gram and Corpus-based techniques
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 N-Gram和基于语料库的技术
- en: N-Gram models represent a marked but efficient improvement to BoW, where you
    are able to give the model a sort of context, represented by N. They are relatively
    simple statistical models that allow you to generate words based on the N-1 context
    space. Looking at Listing 2.1, I’m using trigrams which means N=3\. I clean the
    text and give it minimal padding/formatting to help the model, then we train using
    everygrams, which is meant to prioritize flexibility over efficiency so that you
    could train a pentagram or a septagram (N=5, N=7) model if you wanted instead.
    At the end of the listing where I’m generating, I can give the model up to 2 tokens
    to help it figure out how to generate further. N-Gram models were not created,
    and have never even claimed to attempt, complete modeling systems of linguistic
    knowledge, but they are widely useful in practical applications. They ignore all
    linguistic features, including syntax, and only attempt to draw probabilistic
    connections between words appearing in an N-length phrase.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Looking at Figure 2.2, N-Grams really only use static signals (whitespace, orthography)
    and words to try to extract any meaning. It tries to measure phrases manually,
    assuming that all of the phrases will be the same length. That said, N-Grams can
    be used to create powerful baselines for text analysis, and if the pragmatic context
    of the utterance is already known by the analyst, they can be used to give quick
    and accurate insight into real-world scenarios. It is possible to make an N-Gram
    LLM by just making N=1000000000 or higher, but this doesn’t have any practical
    application, as 99.9% of all text and 100% of all meaningful text contains fewer
    than one billion tokens appearing more than once and that computational power
    can be much better spent elsewhere.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Generative N-Grams Language Model Implementation
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The above code is all that you need to create a generative N-gram model. For
    those of you interested in being able to evaluate that model further, we've included
    the below code to grab probabilities and log scores, or analyze the entropy and
    perplexity of a particular phrase. Because this is all frequency-based, even though
    it’s mathematically significant, it still does a pretty bad job of describing
    how perplexing or frequent real-world language actually is.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: While this code example illustrates creating a trigram language model, unfortunately,
    not all phrases needing to be captured are only 3 tokens long. For example, from
    Hamlet, “To be or not to be,” consists of one phrase with 2 words and one phrase
    with 4 words. This type of phrasal modeling also fails to capture any semantic
    encodings that individual words could have. In order to solve these problems,
    Bayesian statistics were applied to language modeling.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Bayesian Techniques
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bayes’ theorem is one of the most mathematically sound and simple theories
    present in describing the occurrence of your output within your input space. Essentially,
    it calculates the probability of an event occurring based on prior knowledge.
    The theorem posits that the probability of a hypothesis being true given evidence,
    for example that a sentence has a positive sentiment, is equal to the probability
    of the evidence occurring given the hypothesis is true multiplied by the probability
    of the hypothesis occurring, all divided by the probability of the evidence being
    true. Or, expressed mathematically:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: P(hypothesis | evidence) = (P(evidence | hypothesis) * P(hypothesis)) / P(evidence)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Or
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: P(A|B) * P(B) = P(B|A) * P(A)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Because this is neither a math book, nor do we care to dive too much into theory,
    we’ll trust you can get further informed about this theorem.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, even though the theorem represents the data in a mathematically
    sound way, it doesn’t account for any stochasticity or multiple meanings of words.
    One word you can always throw at a Bayesian model to confuse it is the word, “it.”
    Any demonstrative pronoun ends up getting assigned values in the same LogPrior
    and LogLikelihood way as all of the other words and gets a static value, which
    is antithetical to the usage of those words. For example, if you’re trying to
    perform sentiment analysis on an utterance, it would be better for you to assign
    all pronouns a null value than to even let them go through the Bayesian training.
    It should be noted also that Bayesian techniques don’t end up creating generative
    language models the way the rest of these will. Because of the nature of Bayes’
    theorem validating a hypothesis, these models work for classification, and can
    bring powerful augmentation to a generative language model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In Listing 2.2 we show how to create a Naive Bayes classification language model.
    Instead of using a package like sklearn or something that would make writing the
    code a little easier, we opted to write out what we were doing, so it’s a bit
    longer, but should be more informative about how it works. We are using the least-complex
    version of a Naive Bayes model. We haven’t made it multinomial or added anything
    fancy and this could obviously work better if you opted to upgrade it for any
    problem you want. And we highly recommend you do.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Categorical Naive Bayes Language Model Implementation
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This theorem doesn’t create the same type of language model, but one with a
    list of probabilities associated with one hypothesis. As such, Bayesian language
    models can’t be used effectively to generate language, but it can be very powerfully
    implemented for classification tasks. In my opinion though, Bayesian models are
    often overhyped for even this task. One of the crowning achievements of my career
    was replacing and removing a Bayesian model from production.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian models one of the big issues is essentially that all sequences are
    completely unconnected, like BoW models, moving us to the opposite end of sequence
    modeling from N-Grams. Similarly to a pendulum, language modeling swings back
    towards sequence modeling and language generation with Markov Chains.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Markov Chains
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often called Hidden Markov Models (HMMs), Markov Chains essentially add state
    to the N-Gram models mentioned before, storing probabilities using hidden states.
    They are often used to help parse text data for even larger models, doing things
    like Part-of-Speech tagging (PoS Tagging, marking words with their part of speech)
    and Named Entity Recognition (NER, marking identifying words with their referent
    and usually type, e.g. LA - Los Angeles - City) on textual data. Markov models,
    as opposed to previous Bayesian models, rely completely on stochasticity (predictable
    randomness) whereas the Bayesian models pretended it didn’t exist. The idea is
    similarly mathematically sound, however, that the probability of anything happening
    *next* depends completely upon the state of *now*. So instead of modeling words
    based solely on their historical occurrence and drawing a probability from that,
    we model their future and past collocation based on what is currently occurring.
    So the probability of “happy” occurring goes down to almost zero if “happy” was
    just output, but goes up significantly if “am” has just occurred. Markov chains
    are so intuitive that they were incorporated into later iterations of Bayesian
    statistics, and are still used in production systems today.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: In Listing 2.3 we train a Markov chain generative language model. This is the
    first model where we’ve used a specific tokenizer, which in this case will just
    tokenize based on the white space between words. This is also only the second
    time we’ve referred to a collection of utterances meaning to be viewed together
    as a document. As you play around with this one, pay close attention and make
    some comparisons yourself for how well the HMM generates compared to even a large
    N-gram model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Generative Hidden Markov Language Model Implementation
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code shows a basic implementation of a Markov model for generation, and
    we’d encourage the reader to experiment with it, give it text from songs from
    your favorite musicians or books from your favorite authors and see whether what
    comes out actually sounds like them. HMMs are incredible fast and often used in
    predictive text or predictive search applications. Markov models represent the
    first comprehensive attempt to actually model language from a descriptive linguistic
    perspective, as opposed to a prescriptive one, which is interesting, because Markov
    did not originally intend for linguistic modeling, only to win an argument about
    continuous independent states. Later, Markov used Markov Chains to model vowel
    distribution in a Pushkin novel, so he was at least aware of the possible applications.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The difference between descriptive and prescriptive linguistics is that one
    focuses on how things *ought* to be, while the other focuses on how things *are*.
    From a language modeling perspective, it has proven vastly more effective to describe
    what language is doing from a corpus or Markov perspective, rather than to attempt
    to prescribe how language ought to behave. Unfortunately, a current state by itself
    cannot be used to give context beyond the now, so historical or societal context
    is unable to be represented effectively in a Markov model. Semantic encoding of
    words also becomes a problem, as is represented in the code example, Markov chains
    will output syntactically correct chains of words that semantically are nonsense,
    similar to “colorless green ideas sleep furiously.” To attempt to solve this problem,
    “continuous” models were developed to allow for a “semantic embedding” representation
    of tokens.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Continuous Language Modeling
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Continuous Bag of Words (CBoW), much like its namesake, the Bag of Words,
    is a frequency-based approach to analyzing language, meaning that it models words
    based on how often they occur. The next word in an utterance has never been determined
    based on probability or frequency. Due to this, the example given will be for
    how to create word embeddings to be ingested or compared by other models using
    a CBoW. We’ll use a neural network for this to give you a good methodology.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: This is the first language modeling technique we’ll see that essentially slides
    a context window over a given utterance (the context window is an N-gram model)
    and attempts to guess what word is in the middle based upon surrounding words
    in the window. For example, let’s say your window has a length of 5, and your
    sentence is, “Learning about linguistics makes me happy,” you would give the CBoW
    [‘learning’, ‘about’, ‘makes’, ‘me’] and try to get the model to guess “linguistics,”
    based upon how many times the model has seen that word occur in similar places
    previously. This should show you why generation is difficult for models trained
    like this, because if you give the model [‘makes’, ’me’, ’</s>] as input, first
    of all it only has 3 pieces of information to try to figure out instead of 4,
    and it also will be biased towards only guessing words it has seen at the end
    of sentences before, as opposed to getting ready to start new clauses. It’s not
    all bad though, one feature that makes continuous models stand out for embeddings
    is that it doesn’t just have to look at words before the target word, they can
    also use words that come after the target to gain some semblance of context.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: In Listing 2.4 we create our first continuous model. In our case, to keep things
    as simple as possible, we use a bag of words for the language processing and a
    one-layer neural network with two parameters for the embedding estimation, although
    both of those could be substituted out for any other models. For example, you
    could substitute N-grams for the BoW and a Naive Bayes for the neural network,
    and get a Continuous Naive N-gram model. The point is that the actual models used
    in this technique are a bit arbitrary, it’s more the Continuous technique that’s
    important. To illustrate this further, we don’t use any packages other than numpy
    to do the math for the neural network, even though it’s our first one appearing
    in this section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Pay special attention to the steps below, initializing the model weights, the
    ReLU activation function, the final softmax layer, forward and backpropagation,
    and then how it all fits together in the `gradient_descent` function. These are
    pieces in the puzzle that you will see crop up again and again, regardless of
    programming language or framework. You will need to initialize models, pick activation
    functions, pick final layers, and define forward and backward propagation in Tensorflow,
    Pytorch and HuggingFace, and if you ever start creating your own models as opposed
    to using someone else’s.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Generative Continuous Bag of Words Language Model Implementation
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The CBoW example is our first code example to showcase a full and effective
    training loop in machine learning. Within all of that, we asked the reader to
    pay special attention to the steps in a training loop, especially the activation
    function, ReLU. As we expect the reader to be at least familiar with various ML
    paradigms, including different activations, we won’t explain the ReLU here, rather
    why you should use it and why you shouldn’t. ReLUs, while solving the vanishing
    gradient problem, don't solve the exploding gradient problem, and they sharply
    destroy all negative comparisons within the model. Better situational variants
    include the ELU, which allows negative numbers normalizing to alpha, or the GEGLU/SWIGLU,
    which works well in increasingly perplex scenarios, like language. However, people
    often use ReLUs, not because they are the best in a situation, but because they
    are easy-to-understand, easy-to-code, and intuitive, even more so than the activations
    they were created to replace like the sigmoid or tanh.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: A lot of this ends up being abstracted with packages and the like, but knowing
    what’s going on under the hood will be very helpful for you as someone putting
    LLMs in production. You should be able to predict with some certainty how different
    models will behave in various situations. The next section will dive into one
    of those abstractions, in this case being the abstraction that is created by the
    continuous modeling technique.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 Embeddings
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hearkening back to our features of language, it should be easy to connect why
    continuous-style language modeling was such a breakthrough. Embeddings take the
    tokenized vectors we’ve created that don’t contain any meaning, and attempt to
    insert that meaning based on observations that can be made about the text, such
    as word order and subwords appearing in similar contexts. Despite the primary
    mode of meaning being collocation (co-located, words that appear next to each
    other), they prove useful and even show some similarities to human-encoded word
    meaning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The quintessential example from Word2Vec, one of the first pre-trained vector
    embeddings, was taking the vector for “king” subtracting the vector for “man”
    adding the vector for “woman” and finding the nearest neighbor to the sum was
    the vector for the word “queen”. This makes sense to us as it mimics human semantics.
    One of the major differences is one that’s already been mentioned a couple of
    times, pragmatics. Humans use pragmatic context to inform semantic meaning, understanding
    that just because you said, “I need food,” doesn’t mean you are actually in physical
    danger without it. Embeddings are devoid of any influence outside of pure usage,
    which feels like it could be how humans learn as well, and there are good arguments
    on all sides here. The one thing holding is that if we can somehow give models
    more sense data, that may open the door to more effective embeddings.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: In Listing 2.5, we’ll dive into how to visualize embeddings using pyplot. We
    will be going more in-depth into embeddings in later chapters. This is helpful
    for model explainability and also for validation during your pre-training step.
    If you see that your semantically similar embeddings are relatively close to each
    other on the graph, then you’re likely going in the right direction.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Embedding Visualization
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Figure 2.4 A visualization technique for word embeddings. Visualizing embeddings
    can be important for model explainability.
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image005.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: As we can see in figure 2.4, this is a successful, but a very sparse embedding
    representation that we trained from our CBoW model. Getting those semantic representations
    (embeddings) to be denser is the main place that we can see improvement in this
    field, although many successful experiments have been run where denser semantic
    meaning has been supplanted with greater pragmatic context through instruct and
    different thought chaining techniques. We will address Chain of Thought (CoT)
    and other techniques later. For now, let’s pivot to discussing why our continuous
    embedding technique can even be successful, given frequency-based models are characteristically
    difficult to correlate with reality. All of this starts with the Multilayer Perceptron,
    more than half a century ago.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.6 Multilayer Perceptrons
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLPs are the embodiment of the sentiment, “Machines are really good at doing
    one thing, so I wish we could just use a bunch of machines that are really good
    at the one thing to make one that’s good at a lot of things.” Every weight and
    bias in the neural network of the MLP is good at detecting one feature, so we
    bind a whole bunch of them together to detect larger, more complex features. MLPs
    serve as the primary building block in most neural network architectures. The
    key distinctions between architectures, such as convolutional neural networks
    and recurrent neural networks, mainly arise from data loading methods and the
    handling of tokenized and embedded data as it flows through the layers of the
    model, rather than the functionality of individual layers, particularly the fully-connected
    layers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: In Listing 2.6 we provide a more dynamic class of neural network that can have
    as many layers and parameters as deemed necessary for your task. We give a more-defined
    and explicit class using pytorch to give you the tools to implement the MLP for
    use in whatever you’d like, both from scratch and in a popular framework.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Multilayer Perceptron Pytorch Class Implementation
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From the code we can see, as opposed to the CBoW implementation which had a
    static two layers, this MLP is not static in size until it has been instantiated.
    If you wanted to give this model one million layers, you would just have to put
    num_hidden_layers=1000000 when you instantiate the class, although just because
    you *can* give a model that many parameters it won’t make it immediately better.
    LLMs are more than just a lot of layers. Like RNNs and CNNs, the magic of LLMs
    is in how data goes in and moves through the model. To illustrate, let’s look
    at the RNN and one of its variations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.7 RNNs and LSTMs
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) are a class of neural networks designed to
    analyze sequences, based on the weaknesses in previous language modeling techniques.
    The logic goes that if language is presented in a sequence, then maybe it should
    be processed in a sequence, as opposed to one token at a time. RNNs accomplish
    this by using logic we’ve seen before, both in MLPs and in Markov Chains, where
    an internal state or memory is referred to when new inputs are processed, and
    creating cycles when connections between nodes are detected as being useful.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In fully recurrent networks, like the one in Listing 2.7, all nodes start out
    initially connected to all subsequent nodes, but those connections can be set
    to zero to simulate them being broken if they are not useful. This solves one
    of the biggest problems that earlier models suffered from, static input size,
    and enables an RNN and its variants to process variable length inputs. Unfortunately,
    longer sequences create a new problem. Because each neuron in the network has
    connections to subsequent neurons, longer sequences create smaller changes to
    the overall sum, making the gradients smaller and eventually vanishing, even with
    important words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s consider these sentences with the task sentiment analysis,
    “I loved the movie last night,” and, “The movie I went to see last night was the
    very best I had ever expected to see.” These sentences can be considered semantically
    similar, even if they aren’t exactly the same. When moving through an RNN, each
    word in the first sentence is worth more, and the consequence is that the first
    sentence has a higher positive rating than the second sentence, just because of
    the first sentence being shorter. The inverse is true also, exploding gradients
    are also a consequence of this sequence processing, which makes training deep
    RNNs difficult.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, long short-term memories (LSTMs), which are a type of
    RNN, use memory cells and gating mechanisms to keep being able to process sequences
    of variable length, but without the problems of longer and shorter sequences being
    comprehended differently. Anticipating multilingual scenarios and understanding
    that people don’t think about language in only one direction, LSTMs can also process
    sequences bidirectionally by concatenating the outputs of two RNNs, one reading
    the sequence from left to right, and the other from right to left. This bidirectionality
    improves results, allowing for information to be seen and remembered even after
    thousands of tokens have passed.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: In Listing 2.7 we give classes for both an RNN and an LSTM. In the code in the
    repo associated with this book, you can see the results of training both the RNN
    and LSTM, where the takeaway is that the LSTM gets better accuracy on both training
    and validation sets in half as many epochs (25 vs 50 with RNN). One of the innovations
    to note is the packed embeddings that utilize padding, extending all variable-length
    sequences to the maximum length in order to allow processing any length input,
    as long as it is shorter than the maximum.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 Recurrent Neural Network and Long Short-Term Memory Pytorch Class
    Implementations
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Looking above at our classes and instantiations, you should see that the LSTM
    is not vastly different from the RNN. The only differences in the `init` input
    variables are `n_layers` (for convenience, you can also specify it with RNNs),
    `bidirectional`, and `dropout`. Bidirectional allows LSTMs to look ahead in sequences
    to help with meaning and context, but also drastically helps with multilingual
    scenarios, as left-to-right languages like English are not the only format for
    orthography. Dropout, another huge innovation, changes the paradigm of overfitting
    from being only data-dependent, and helps the model not overfit by turning off
    random nodes layer-by-layer during training to force all nodes not to correlate
    to each other. The only differences in the out-of-model parameters is that the
    best optimizer for an RNN is SGD, like our CBoW, and the LSTM uses Adam (could
    use any, including AdamW). Below, we define our training loop and train the LSTM.
    Compare this training loop to the one defined in Listing 2.4 in the `gradient_descent`
    function.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: One of the amazing things demonstrated in the code here is how much quicker
    the LSTM can learn, compared to previous model iterations, thanks to both bidirectionality
    and dropout. The previous models, though training faster, take hundreds of epochs
    to get the same performance as an LSTM in just 25 epochs. The performance on the
    validation set, as its name implies, adds validity to the architecture, performing
    inference during training on examples it has not trained on and keeping accuracy
    fairly close to the training set.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: The problems with these models are not as pronounced, manifesting primarily
    as being incredibly resource-heavy, especially when being applied to longer, more
    detail-oriented problems, like healthcare and law. Despite the incredible advantages
    of Dropout and Bidirectional processing, they both at least double the amount
    of processing power required to train, so while inference ends up being only 2-3x
    as expensive as an MLP of the same size, training becomes 10-12x as expensive.
    They solved exploding gradients nicely, but exploded the compute required to train
    instead. To combat this a shortcut was devised and implemented which allowed any
    model, including an LSTM, to figure out which parts of a sequence were the most
    influential and which parts could be safely ignored, known as attention.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.8 Attention
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention is a mathematical shortcut for solving larger context windows faster
    by telling the model through an emergent mathematical formula which parts of an
    input to consider and how much. This is all based upon an upgraded version of
    a dictionary, where instead of just Key and Value pairs, a contextual Query is
    added. We will go more into Attention in later chapters. For now, know that the
    below code is the 10 steps taken from the original paper, and that it’s the big
    differentiator between older NLP techniques and modern ones.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Attention solves the slowness of training LSTMs, but keeps the high performance
    on a low number of epochs. There are multiple types of attention as well. The
    dot product attention method captures the relationships between each word (or
    embedding) in your query and every word in your key. When queries and keys are
    part of the same sentences, this is known as bi-directional self-attention. However,
    in certain cases, it is more suitable to only focus on words that precede the
    current one. This type of attention, especially when queries and keys come from
    the same sentences, is referred to as causal attention. Language modeling further
    improves by masking parts of a sequence and forcing the model to guess what should
    be behind the mask. Both Dot Product Attention and masked attention are demonstrated
    with functions below.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 Multi-Head Attention Implementation
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Above, in the full implementation of Attention you may have noticed some terminology
    you’re familiar with, namely Key and Value, but you may not have been introduced
    to Query before. Key and Value pairs are familiar because of dictionaries and
    lookup tables, where we map a set of keys to an array of values. Query should
    feel intuitive as a sort of search for retrieval. The Query is compared to the
    Keys, from which a Value is retrieved in a normal operation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: In Attention, the Query and Keys undergo dot product similarity comparison to
    obtain an attention score, which is later multiplied by the Value in order to
    get an ultimate score for how much Attention the model should pay to that portion
    of the sequence. This can get more complex, depending upon your model’s architecture
    because both encoder and decoder sequence lengths have to be accounted for, but
    suffice it to say for now that the most efficient way to model in this space is
    to project all input sources into a common space and compare using dot product
    for efficiency.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'This code explanation was a bit more math-heavy than the previous examples,
    but it is needed to illustrate the concept. The math behind Attention is truly
    innovative and has rocketed the field forward. Unfortunately, even with the advantages
    Attention brings to the process of sequence modeling, with LSTMs and RNNs there
    were still issues with speed and memory size. You may notice from the code and
    the math that there is a square root taken, meaning that attention as we use it
    is quadratic. Since then, there have been various techniques, including subquadratics
    like Hyena and the Recurrent Memory Transformer (RMT, basically an RNN combined
    with a transformer) to combat these problems, which we will cover in more detail
    later. For now, let’s move on to the ultimate application of Attention: the Transformer.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Attention is All You Need
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the seminal paper, Attention is All You Need[[1]](#_ftn1) Vaswani et al
    take the mathematical shortcut several steps further, positing that for performance
    absolutely no recurrence (the “R” in RNN) or any convolutions[[2]](#_ftn2) were
    needed at all. Instead, they opted to use only Attention and simply specify where
    Q, K, and V were taken from much more carefully. We’ll dive into this presently.
    In our review of this diverse range of NLP techniques, we have observed their
    evolution over time and the ways in which each approach has sought to improve
    upon its predecessors. From rule-based methods to statistical models and neural
    networks, the field has continually strived for more efficient and accurate ways
    to process and understand natural language. Now, we turn our attention to a groundbreaking
    innovation that has revolutionized the field of NLP: the Transformer architecture.
    In the following section, we will explore the key concepts and mechanisms that
    underpin Transformers, and how they have enabled the development of state-of-the-art
    language models that surpass the performance of previous techniques. We will also
    discuss the impact of Transformers on the broader NLP landscape and consider the
    potential for further advancements in this exciting area of research.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Encoders
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Encoders are the first half of a full transformer model, excelling in areas
    like classification and feature engineering. One thing Vaswani et al. (2017) figured
    out is that after the embedding layer inside the encoder, any additional transformations
    done to the tensors could end up harming their ability to be compared “semantically,”
    which was the point of the embedding layer. These models rely heavily upon self-attention
    and a clever positional encoding to manipulate those vectors without significantly
    decreasing the similarity expressed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, a key thing about embeddings: they are vector representations of data,
    in our case tokens. Tokens are whatever you pick to represent language. We recommend
    subwords as a general rule, but you will get a feel for which types of tokens
    work well where. Consider the sentence, “The cat in the hat rapidly leapt above
    the red fox and the brown unmotivated dog.” “Red,” and “brown,” should be semantically
    similar, and they are similarly represented after the embedding layer, but they
    fall on positions 10 and 14 respectively in the utterance, assuming that we’re
    tokenizing by word, therefore the positional encoding puts distance between them.
    However, once the sine and cosine functions[[3]](#_ftn3) are applied, it brings
    their meaning back to only a little further apart than they were after the encoding,
    and this encoding mechanism scales brilliantly with recurrence and more data.
    To illustrate, let’s say there was a 99% cosine similarity between [red], and
    [brown] after embedding. Encoding would drastically reduce that, to around 85-86%
    similarity. Applying sine and cosine methodologies as described brings their similarity
    back up to around 96%.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: BERT was one of the first architectures to come after the original paper and
    are examples of encoder-only transformers. BERT is an incredibly powerful model
    architecture for how small it is that it is still used in production systems today.
    BERT was the first encoder-only transformer to surge in popularity, showcasing
    that performing continuous modeling using a transformer results in much better
    embeddings than Word2Vec. We can see that these embeddings were better because
    they could be very quickly applied to new tasks and data with minimal training,
    with human-preferred results over Word2Vec embeddings. This resulted in most people
    using BERT-based models for few-shot learning tasks on smaller datasets for a
    while. BERT puts state-of-the-art performance within arms reach for most researchers
    and businesses with minimal effort required.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 An Encoder, visualized. Encoders are the first half of the full transformer
    architecture, and excel in NLU tasks like classification or NER. Encoder models
    improve upon previous designs by not requiring any priors or recurrence, and use
    clever positional encoding and multihead attention.
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image006.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Strengths:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Classification and hierarchical tasks showcasing understanding
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blazing fast, considering the long-range dependency modeling
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Builds off of known models, CBoW in Embedding, MLP in Feed Forward, etc.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaknesses:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: As suggested, requires lots of data to be effective (although less than RNNs)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even more complex architecture
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3.2 Decoders
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decoder models, as shown below, are larger versions of encoders that have 2
    multi-head attention blocks and 3 sum and normalize layers in their base form.
    They are the 2nd half of a transformer behind an encoder. This results in a model
    that is very good at masked language modeling and learning and applying syntax
    super quickly leading to the almost immediate idea that decoder-only models are
    needed to achieve Artificial General Intelligence. A useful reduction of encoder
    vs decoder tasks is that encoders excel in natural language understanding (NLU)
    tasks, while decoders excel in natural language generation (NLG) tasks. Some examples
    of decoder-only transformer architectures are the Generative Pretrained Transformer
    (GPT) family of models. These models follow the logic of transformational generative
    grammar being completely syntax based, allowing for infinite generation of all
    possible sentences in a language.[[4]](#_ftn4)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 a decoder visualized. Decoders are the second half of a full transformer,
    and they excel in NLG tasks like chatbots and storytelling. Decoders improve upon
    previous architectures in the same way as encoders, but they add shifting their
    output one space to the right for next-word generation to help utilize the advantages
    of multihead self-attention.
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image007.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Strengths:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Generating the next token in a sequence (shifted right means taking already-generated
    tokens into account)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building off of both known models and also encoders
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be streamed during generation for great UX
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaknesses:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Syntax only models can often struggle to insert the expected or intended meaning
    (see all “I force an AI to watch 1000 hours of x and generated” memes from 2018-present)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucinations
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3.3 Transformers
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full transformer architecture takes advantage of both encoders and decoders,
    passing the understanding of the encoder into the second Multi-Head Attention
    block of the decoder before giving output. As each piece of the transformer has
    a specialty in either understanding or generation, it should feel intuitive for
    the full product to be best at conditional generation tasks like translation or
    summarization, where some level of understanding is required before generation
    occurs. Encoders are geared towards processing input at a high level, and decoders
    focus more on generating coherent output, the full transformer architecture can
    successfully understand, then generate based on that understanding. Transformer
    models have an advantage in that they are built around parallelization, which
    adds speed that can’t currently be replicated in LSTMs. If LSTMs ever get to a
    point where they can run as quickly as transformers, they may become competitive
    in the state-of-the-art field. The Text-To-Text Transfer Transformer (T5) family
    of models are examples of transformers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 A full transformer visualized. A full transformer combines both the
    encoder and the decoder and does well on all of the tasks of each, as well as
    conditional generation tasks such as summarization and translation. Because transformers
    are bulkier and slower than each of their halves, researchers and businesses have
    generally opted to use those halves over the whole thing, despite the speed and
    memory boosts being minimal.
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image008.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'Strengths:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Both an encoder and decoder, so is good at everything each of those are good
    at
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly parallelized for speed and efficiency
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaknesses:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Memory intensive, but still less than LSTMs of the same size
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires large amounts of data and VRAM for training
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you’ve probably noticed, most of the models we’ve discussed aren’t at all
    linguistically focused, being heavily syntax-focused, if attempting to model real
    language at all. Models, even state-of-the-art transformers only have semantic
    approximations, no pragmatics, no phonetics, and only really utilize morphology
    during tokenization. This doesn’t mean the models can’t learn these, nor does
    it mean that, for example, transformers can’t take audio as an input, just that
    the average usage doesn’t. With this in mind, it is nothing short of a miracle
    that they work as well as they do, and they really should be appreciated as such.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Through this chapter so far we’ve attempted to highlight where the current
    limitations are in models, and we will dive into where to go to improve upon them
    in the rest of this book. One such route is one that’s already been and being
    explored to great success, transfer learning and finetuning large foundational
    models. This technique came about soon after BERT’s initial release, when researchers
    discovered that although BERT performed generally well on a large number of tasks,
    if they wanted it to perform better on a particular task or data domain, all they
    needed to do was retrain the model on data representative of the task or domain,
    but not from scratch. Take all of the pretrained weights that BERT learned while
    creating the semantic approximation embeddings on a much larger dataset, then
    significantly less data is required to get state-of-the-art (SotA) performance
    on the portion that you need. We’ve seen this with BERT, and with the GPT family
    of models as they’ve come out respectively, and now we’re seeing it again to solve
    exactly the challenges brought up: semantic approximation coverage, domain expertise,
    availability of data.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Really Big Transformers
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enter the Large Language Model. Since their introduction transformer based models
    have continued to only get larger and larger, and not just by their size and number
    of parameters, but also the size of their training datasets and training cycles
    has gotten larger and longer as well. If you ever studied machine learning or
    deep learning during the 2010s, you likely heard the moniker, “more layers doesn’t
    make the model better.” LLMs prove this both wrong and right. Wrong because their
    performance is unparalleled, oftentimes even matching smaller models that have
    been meticulously finetuned on a particular domain and dataset, even the ones
    trained on proprietary data. Right because of the challenges that come with both
    training and deploying them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: One of the major differences between LLMs and LMs involves transfer learning
    and finetuning. Exactly the same as the previously-large LMs, LLMs are pretrained
    on massive text corpora, enabling them to learn general language features and
    representations that can be finetuned for specific tasks. Because LLMs are so
    massive though and their training datasets so large LLMs are able to achieve better
    performance with less labeled data, which was a significant limitation of earlier
    language models. Often times you can finetune an LLM to do highly specialized
    tasks with only a dozen or so examples.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: However, what really makes LLMs powerful and has opened the door to widespread
    business use cases is their ability to do specialized tasks without any finetuning,
    but just simple prompting. Just give a few examples of what you want in your query
    and the LLM is able to produce results. This is called few-shot prompting when
    it’s trained on smaller labeled data sizes, one-shot, when given only one example,
    and zero-shot, when the task is totally novel. LLMs, especially those trained
    using RLHF and prompt engineering methodologies, can perform few-shot learning
    on a whole new level, where they can generalize and solve tasks with only a few
    examples. This ability is a significant advancement over earlier models that required
    extensive fine-tuning or large amounts of labeled data for each specific task.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: LMs previously have shown promise in the few and zero-shot learning domains,
    and LLMs have proven that promise to be true. As models have gotten larger we
    find they are capable of accomplishing new tasks where smaller models can’t. We
    call this emergent behaviors[[5]](#_ftn5) and figure 2.8 demonstrates eight different
    tasks that LMs couldn’t perform better than random, then suddenly once the models
    got large enough they could.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 Examples of LLMs demonstrating emergent behaviors when tasked with
    few-shot prompting tasks after the model scale reaches a certain size.
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image009.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: LLMs have demonstrably great Zero-Shot capabilities as well, which is both due
    to their vast parameter sizes, and also the main reason for their popularity and
    viability in the business world. LLMs also exhibit improved handling of ambiguity
    due to their large size and capacity. They are better at disambiguating words
    with multiple meanings and understanding the nuances of language, resulting in
    more accurate predictions and responses. This isn’t because of an improved ability
    or architecture as they share their architecture with smaller transformers, but
    because they have vastly more examples of how people generally disambiguate. LLMs
    therefore respond with the same disambiguation as is generally represented in
    the dataset. Thanks to the diverseness of the text data LLMs are trained on, they
    exhibit increased robustness in handling various input styles, noisy text, and
    grammatical errors.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Another key difference between LLMs and LMs is input space. A larger input space
    is important since it makes few-shot prompting tasks that much more viable. Many
    LLMs have max input sizes of 8000+ tokens (GPT-4 currently sports 32k), and while
    all the models previously discussed in the chapter could also have input spaces
    that high, they generally aren’t considered to. We have recently seen a boom in
    this field as well, with techniques like Recurrent Memory Transformer (RMT) allowing
    1,000,000+ token context spaces, which rocket LLMs even more towards proving that
    bigger models really are always better. LLMs are designed to capture long-range
    dependencies within text, allowing them to understand context more effectively
    than their predecessors. This improved understanding enables LLMs to generate
    more coherent and contextually relevant responses in tasks like machine translation,
    summarization, and conversational AI.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have revolutionized NLP by offering powerful solutions to problems that
    were challenging for earlier language models. They bring substantial improvements
    in contextual understanding, transfer learning, and few-shot learning. As the
    field of NLP continues to evolve, researchers are actively working to maximize
    the benefits of LLMs while mitigating all potential risks. Because a better way
    to approximate semantics hasn’t been found, they make bigger and more dimensional
    approximations. Because a good way of storing pragmatic context hasn’t been found,
    LLMs often allow inserting context either into the prompt directly, into a part
    of the input set aside for context, or even through sharing of databases with
    the LLM at inference. This doesn’t create pragmatics or a pragmatic system within
    the models, same as embeddings don’t create semantics, but it allows the model
    to correctly generate syntax that mimics how humans respond to those pragmatic
    and semantic stimuli. Phonetics is a place where LLMs could likely make gigantic
    strides, either as completely text-free models, or as a text-phonetic hybrid model,
    maybe utilizing IPA in addition to or instead of text. It is exciting to consider
    the possible developments that we are watching sweep this field right now.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: At this point you should have a pretty good understanding of what LLMs are and
    some key principles of linguistics that will come in handy when putting LLMs in
    Production. Mainly, you should be able to now start reasoning what type of products
    will be easier or harder to build. Consider figure 2.9, tasks in the lower left
    hand corner like Writing Assistants and ChatBots are LLMs bread and butter. Text
    generation based on a little context from a prompt are problems that are strictly
    syntax based, with a large enough model trained on enough data we can do this
    pretty easily. A Shopping Assistant is pretty similar and rather easy to build
    as well, however, we are just missing pragmatics. The assistant needs to know
    a bit more about the world like products, stores, and prices. With a little engineering
    we can add this information into a database and give this context to the model
    through prompting.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: On the other end consider a ChessBot. LLMs *can* play chess. They aren’t any
    good. They have been trained on chess games and understand that “E4” is a common
    first move, but their understanding is completely syntactical. LLMs really only
    understand that the text they should generate should contain a letter between
    A and H and a number between 1 and 8\. Like the Shopping Assistant, they are missing
    pragmatics and don’t have a clear model of the game of chess. In addition, they
    are also missing semantics. Encoders might help us understand the words “King”
    and “Queen” are similar to each other, but they don’t really help us understand
    that “E4” is a great move one moment for one player and that same “E4” move is
    a terrible move the very next moment for a different player. LLMs are also completely
    lacking knowledge based on phonetics and morphology for chess as well, but these
    are not as important for this case. Either way, we hope this exercise will better
    inform you and your team on your next project.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 How difficult or easy certain tasks are for LLMs and what approaches
    to solve them.
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image010.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: LLMs have amazing benefits, but with all of these capabilities come some limitations.
    Foundational LLMs require vast computational resources for training, making them
    less accessible for individual researchers and smaller organizations. This is
    being remedied with techniques we’ll talk about throughout the book like Quantization,
    Textual Embeddings, Low-Rank Adaptation, Parameter-Efficient Fine Tuning, and
    Graph Optimization, but foundation models are still currently solidly out of the
    average individual’s ability to train effectively. Beyond that, there are concerns
    that the energy consumption associated with training LLMs could have significant
    environmental impact and problems associated with sustainability. This is a complex
    issue largely out of the scope of this book, but we would be remiss not to bring
    it up. Last, but not least, since LLMs are trained on large-scale datasets containing
    real-world text, they may learn and perpetuate biases present in the data, leading
    to ethical concerns, not by the fault of the researchers or the algorithms, more
    because real-world people aren’t censoring themselves to provide optimal unbiased
    data. For example, if you ask a text-to-image diffusion LLM to generate 1000 images
    of “leader,” 99% of the images feature men, and 95% of the images feature people
    with white skin. The concern here isn’t that men or white people aren’t or shouldn’t
    be depicted as leaders, rather that it shows that the model isn’t truly representing
    the world accurately.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 Midjourney 5, which is currently the most popular text2img model
    on the market, when prompted with only one token, “Leader,”(Shown Left) changed
    a well-known popular feminist icon, Rosie the Riveter into a male depiction. ChatGPT
    (Shown Right) writes a function to place you in your job based on your race, gender,
    and age. These are examples of unintended outputs.
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/02__image011.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Sometimes, more nuanced bias is brought out, for example, in the Midjourney
    example demonstrated in Figure 2.10 a popular feminist icon “Rosie the Riveter,”
    without being prompted at all (the only prompt given to the model was the word
    “leader”) was changed to a man. The model didn’t think about this change at all,
    it just determined during its sampling steps that the prompt “leader,” looks more
    like a man. Many people will argue about what “good,” and “bad,” mean in this
    context, and instead of entering that discussion we’ll simply talk about what
    accurate means. LLMs are trained on a plethora of data with the purpose of returning
    the most accurate representations possible. When they are still unable to return
    accurate representations, especially with their heightened abilities to disambiguate,
    we can view that as bias that is harmful to the model’s ability to fulfill its
    purpose. Later we will discuss techniques to combat this harmful bias, not for
    any political purpose, but to allow you as an LLM creator to get the exact outputs
    that you intend and minimize the number of outputs that you do not intend.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Alright, we’ve been building up to this the entire chapter let’s go ahead and
    run our first LLM! In listing 2.9 we download the Bloom model, one of the first
    open source LLMs to be created, and generate text! Very exciting stuff.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 Running our first LLM
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Did you try to run it?!? If you did, you probably just crashed your laptop.
    Oopsie! Forgive me for a little harmless MLOps hazing, but getting some first-hand
    experience on how large these models can get and how difficult they can be to
    run is helpful experience to have. We will be talking more about the difficulties
    of running LLMs and give you some of the tools you need to actually run this in
    the next chapter. If you don’t want to wait and would like to get a similar but
    much smaller LLM running change the model name to `“bigscience/bloom-3b”` and
    run it again. It should work just fine this time on most hardware.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: All in all, LLMs are an amazing technology that allow our imaginations to run
    wild with possibility, and deservedly so. The number one use case for considering
    an LLM over a smaller LM is for when few-shot capabilities will come into play
    for whoever the model will be helping, such as helping a CEO when raising funds
    or a software engineer when writing code. They have this ability precisely because
    of their size. The larger number of parameters in LLMs directly enable the ability
    to generalize over smaller spaces in larger dimensions. In this chapter, we’ve
    hit the lesser-known side to LLMs, the linguistic and language modeling side.
    In the next chapter, we’ll cover the other half, the MLOps side, where we dive
    into exactly how that large parameter size affects the model and the systems designed
    to support that model and make it accessible to the customers or employees the
    model is intended for.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Summary
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The five components of linguistics are phonetics, syntax, semantics, pragmatics,
    and morphology.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phonetics can be added through a multimodal model that processes audio files
    and is likely to improve LLMs in the future, but current datasets are too small.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syntax is what current models are good at.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantics is added through the embedding layer.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pragmatics can be added through engineering efforts.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morphology is added in the tokenization layer.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language does not necessarily correlate with reality. Understanding the process
    that people use to create meaning outside of reality is useful to training meaningful
    (to people) models.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper tokenization can be a major hurdle due to too many <UNK> tokens, especially
    when it comes to specialized problems like code or math.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilingual processing has always outperformed monolingual processing, even
    on monolingual tasks without models.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each language model type has been built specifically to combat the weaknesses
    of the previous models, as opposed to trying to solve for particular features
    of language.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling has seen an exponential increase in efficacy, correlating
    to how linguistics-focused the modeling has been.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention is a mathematical shortcut for solving larger context windows faster
    and is the backbone of modern architectures - Encoders, Decoders, and Transformers.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoders improve the semantic approximations in embeddings.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoders are best at text generation.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers combine the two.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger models demonstrate emergent behavior suddenly being able to accomplish
    tasks they couldn’t before.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_ftnref1) Vaswani et al 2017 Attention Is All You Need [https://arxiv.org/abs/1706.03762](abs.html)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_ftnref2) We didn’t go over these because they aren’t good for NLP,
    but they are popular especially in computer vision'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_ftnref3) Not a math or history book'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_ftnref4) See Appendix A'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_ftnref5) J. Wei et al., “Emergent Abilities of Large Language Models,”
    Transactions on Machine Learning Research, Aug. 2022, Available: [https://openreview.net/forum?id=yzkSU5zdwD](openreview.net.html)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
