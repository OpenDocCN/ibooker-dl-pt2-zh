- en: Appendix A. Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When discussing machine learning with newcomers to the field, I find that many
    have picked up on the basics but found the quantity of information about the topic
    and the depth of the mathematics intimidating. I remember that when I was just
    starting out with machine learning, I had a similar experience: it felt like there
    was just too much to learn. This appendix is designed for those who may have attempted
    to understand machine learning from a patchwork of tutorials or a few online courses.
    In the appendix, I organize basic machine learning concepts into a big picture
    and explain how the concepts fit together so that you have enough of a review
    of the basics to attempt the project in this book. Whenever possible I am going
    to present machine learning concepts intuitively, keeping mathematical notation
    to a minimum. My goal is not to replace comprehensive coursework on machine learning
    or deep-dive blog posts; instead, I’d like to show you the most important, most
    salient parts of machine learning needed for practical applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Novice students of machine learning often start their educational journey with
    in-depth study of machine learning algorithms. This is a mistake. Machine learning
    algorithms enable solutions to problems, and the full understanding of the problems
    that are suitable for machine learning should come first. As a machine learning
    practitioner (e.g., as a machine learning engineer or a data scientist), you are
    going to be expected to understand your customer’s business problem and decide
    whether it can be recast as a machine learning problem. So, section A.1 through
    section A.3 introduce you to the fundamentals of machine learning and cover the
    most common machine learning use cases for structured data sets. Starting with
    section A.4 and through the conclusion of this appendix, I introduce you to the
    machine learning algorithms that can be used to solve machine learning problems
    as well as to the details about how to apply the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Why machine learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are reading this book, chances are that you are at least willing to consider
    machine learning as a topic of study or perhaps even as a solution to your problem.
    However, is machine learning the right technology for you to study or to use?
    When does applying machine learning make sense? Even if you are interested in
    machine learning you may find the barrier to entry (which is substantial) intimidating
    and decide against putting in the effort needed to understand machine learning
    in enough depth to apply the technology. In the past, numerous technologies came
    to market claiming to “change everything” but failed to deliver on the promise.
    Is machine learning destined to capture the headlines for a few years and then
    fade away into obscurity? Or is there something different about machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: On the surface, machine learning can appear quite ordinary to the users of contemporary
    computer software and hardware. Machine learning depends on human beings who write
    code, and the code in turn depends on information technology resources such as
    compute, storage, networking, as well as the input and output interfaces. However,
    to gain a perspective on the magnitude of the change brought by machine learning
    to the field of computing, it is useful to revisit the time computing went through
    a transformation at a similar scale.
  prefs: []
  type: TYPE_NORMAL
- en: You may find it surprising to read in a machine learning book about a 1940s
    “computer” for mathematical computations. That is, if you don’t know that prior
    to the invention and a broad adoption of electronic and digital computers in the
    1950s, the term *computer* was used to describe a human being who performed mathematical
    computations, often in concert with a group of other “computers.” A photograph
    of a computer team from 1949 is shown in figure A.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-01](Images/A-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.1 An office of human computers at work in the Dryden Flight Research
    Center Facilities during the summer of 1949\. (This photograph is in the public
    domain; more information is available from [http://mng.bz/XrBv](http://mng.bz/XrBv).)
  prefs: []
  type: TYPE_NORMAL
- en: At its core, computing is about programs (algorithms) that use data to answer
    questions. Prior to the broad deployment of digital computers in the 1950—60s,
    computers (the human beings) played key roles in answering computational questions.
    For assistance in this job, the human computers often relied on external computing
    tools that ranged from pen and paper to early calculators or punch card-based
    tabulating machines. In this paradigm of computing, the computing instructions,
    the program describing how to compute, remained in the human computers’ minds
    (left side of figure A.2).
  prefs: []
  type: TYPE_NORMAL
- en: '![A-02](Images/A-02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.2 Human computers relied on devices to store the data used in computing.
    The devices ranged from pen and paper to electro-mechanical calculators and even
    punch card—based tabulating machines. However, none of these had an internal memory,
    which was used for the storage and execution of computing instructions, in other
    words, the program code (left side of the figure). In contrast, the von Neumann
    architecture for computing, which used computer device memory for both the data
    and the computer instructions, created a transformative practice of computer programming:
    the transfer of instructions for how to compute (the program) to the memory of
    the computing device for storage and execution (right side of the figure).'
  prefs: []
  type: TYPE_NORMAL
- en: The modern computers transformed this computing paradigm by changing the role
    of the human beings in relation to the computing devices. Instead of storing the
    program with the computing instructions in the human mind, human programmers took
    on the job of entering the programs into the memory of the computing devices in
    the form of code, or machine-readable instructions for how to compute (right side
    of figure A.2).
  prefs: []
  type: TYPE_NORMAL
- en: The von Neumann architecture changed the global economy at a scale comparable
    to the transformations brought about by the Industrial Revolution. Nearly every
    contemporary computing device on the planet, from pocket-sized mobile phones to
    massive servers powering cloud-computing data centers, uses the von Neumann architecture.
    The field of artificial intelligence became possible once computation transitioned
    away from the paradigm, where instructions for computing were stored in the biological
    minds of human computers.
  prefs: []
  type: TYPE_NORMAL
- en: The von Neumann computing paradigm also produced notable breakthroughs for the
    field of artificial intelligence; for example, DeepBlue, the first chess program
    to defeat the human chess champion Garry Kasparov, was built by IBM based on the
    paradigm. Despite this and many other successes, the hard-crafted programs produced
    by human programmers proved too simplistic for many subfields of artificial intelligence,
    including computer vision, speech recognition, natural language understanding,
    and many others. The code programmed by humans to perform tasks such as classifying
    objects in digital images or recognizing speech ended up too inaccurate and too
    brittle for broad adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Contemporary machine learning is changing the relationship of the programmer
    to the modern computing devices at a level that is as fundamental as the transformation
    that happened with computing in the 1950s. Instead of programming a computer,
    a machine learning practitioner trains a machine learning system (using a machine
    learning algorithm) with bespoke data sets to produce a machine learning model
    (figure A.3). Since a machine learning model is just computer code, a machine
    learning algorithm can empower a machine learning practitioner with the ability
    to produce code capable of computing answers to questions that are beyond the
    capacity of human-generated programs. For instance, in the 2010s machine learning
    models were used to classify images with superhuman performance, recognize human
    speech so effectively that many households installed speech-recognizing digital
    assistants (such as Amazon Alexa and Google Home), and defeat Lee Sedol, the human
    champion at the ancient board game of Go.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-03](Images/A-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.3 Machine learning relies on a machine learning practitioner who uses
    a machine learning algorithm to “train” a machine learning model based on a bespoke
    data set. Although a trained machine learning model is just code created by the
    machine learning algorithm, the model can answer questions that are too complex
    for a human to manually program as code. For example, machine learning models
    can classify objects in digital images or recognize human speech in digital audio
    better than handcrafted code developed by human programmers.
  prefs: []
  type: TYPE_NORMAL
- en: This appendix introduces machine learning as a subfield of computer science
    focused on using computers to learn from data. Although this definition is accurate,
    it does not fully communicate the importance and the lasting effect of machine
    learning on transforming the field of computing. At the other end of the spectrum,
    marketing slogans about machine learning as “the new electricity” or the enabler
    of artificial general intelligence sensationalize and obfuscate the field. It
    is clear that machine learning is changing the parts of the computing architecture
    that remained fundamentally static from the 1950s to 2010s. The extent to which
    machine learning will transform computing is unclear. I hope you find the uncertainty
    and the potential role that you can play in this transformation as exciting as
    I do!
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Machine learning at first glance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces you to the changes brought to traditional computer science
    by the machine learning algorithms, illustrates machine learning with an easy-to-understand
    example, and describes how to implement the machine learning example using the
    Python programming language and the pandas, Numpy, and scikit-learn libraries.
    By the conclusion of this section you should be prepared to explain basic machine
    learning concepts and use the concepts with simple machine learning examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the advent of machine learning, the traditional computer science algorithms[¹](#pgfId-1011940)
    had focused on computing answers on the basis of what is known: the data. Machine
    learning expanded the field of computer science with algorithms that use data
    to compute answers to questions on the basis of what is possible but is not known:
    the uncertain.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the essence of the change machine learning brought to computer
    science, suppose you are working with the following easy-to-understand data set
    describing Ford Mustangs manufactured from 1971 through 1982 and their fuel efficiency[²](#pgfId-1018783)
    in terms of miles per gallon (mpg) fuel consumption. With the Python programming
    language and the pandas library for structured data,[³](#pgfId-1012332) you can
    prepare this data set for analysis by instantiating it in the computer memory
    as a pandas data structure called a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.1 Listing A.1 Creating a data set in memory as a pandas DataFrame
    to be ready for analysis
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Import the pandas library and give it the alias pd.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Import the NumPy library and give it the alias np.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The pandas DataFrame for storing and managing structured data can be constructed
    from a list of Python dictionaries such that each row is specified using a dictionary
    instance with the data frame column names as the keys and the rows’ contents as
    the dictionary values.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ To avoid printing the default, zero-based index for each row, df.to_string(index=False)
    is used instead of print(df).
  prefs: []
  type: TYPE_NORMAL
- en: This produces the output, shown as a table on the left side of figure A.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-04](Images/A-04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.4 Ford Mustang fuel efficiency data set (left) and the corresponding
    scatter plot (right) based on the same data set. (This publicly available data
    set is sourced from University of California Irvine Machine Learning Repository:
    [https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG).)'
  prefs: []
  type: TYPE_NORMAL
- en: It should come as no surprise that well-known computer science algorithms and
    data structures (e.g., a hash table) can be used to answer questions about what
    is known from this data set, such as the mpg efficiency of a Ford Mustang with
    a weight of 2,905 pounds. With pandas this question can be answered using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'which outputs a NumPy[⁴](#pgfId-1012833) array with a single element corresponding
    to the value of the mpg column and the fourth row of the data set from listing
    A.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: pandas is used throughout this appendix to illustrate common operations on machine
    learning data sets. While a pandas DataFrame is an easy-to-learn and easy-to-use
    data structure, it does not scale to data sets that do not fit into the memory
    of a single node (a computer on a network). Further, pandas was not designed for
    data analysis with distributed computing clusters like those that are available
    in cloud computing environments from major cloud providers. This appendix will
    continue using pandas DataFrames to introduce machine learning; however, the rest
    of the book focuses on SQL tables and PySpark DataFrames that scale to much larger
    data sets than pandas. Many concepts about pandas DataFrames apply directly to
    PySpark and SQL. For example, the descriptions of structured data sets and supervised
    machine learning in section A.3 apply to data sets regardless of whether they
    are managed using pandas, PySpark, or SQL.
  prefs: []
  type: TYPE_NORMAL
- en: What about the fuel efficiency of a 3,000-pound Ford Mustang? Answering this
    question was outside the scope of the traditional computer science algorithms
    prior to the advent of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, as a human being, you can observe the data set (the right side
    of figure A.4) and notice a *pattern* (a recurring rule) across the related Ford
    Mustangs described by the data set: as the weight of the vehicles increases, their
    fuel efficiency decreases. If asked to estimate fuel effiency of a 3,000-pound
    Ford Mustang (which is not specified by the data set), you can apply your mental
    model of the pattern to estimate an answer of roughly 20 miles per gallon.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the right machine learning algorithm, a computer can learn a software-based
    model (known as a machine learning model, to be defined more precisely in section
    A.3) of the data set, such that the learned (also known as trained) model can
    output estimates much like the mental model you intuited to estimate the fuel
    efficiency of the 3,000-pound Ford Mustang.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn, a popular machine learning library,[⁵](#pgfId-1012919) includes
    a variety of ready-to-use machine learning algorithms, including several that
    can construct a machine learning model of the pattern you have observed in the
    data set. The steps to create the model using a machine learning algorithm known
    as *linear regression* based on the values just from the weight column are shown.[⁶](#pgfId-1012958)
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.2 A simple model of the Ford Mustang data set
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Import the linear regression implementation from the scikit-learn library.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create an instance of a linear regression machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Train (fit) the linear regression model using the weight column from the Ford
    Mustang data set as the model input and the mpg values as the model output. The
    reshape function reshapes the NumPy array returned by df['weight'].values to a
    matrix consisting of a single column. The reshape is needed here due to the scikit-learn
    requirement for the model input to be structured as a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression algorithm, widely considered one of the fundamental algorithms
    in machine learning,[⁷](#pgfId-1013152) *trains* (i.e., “fits”) the machine learning
    model instance based on the data set passed to the fit method of the LinearRegression
    class. Once trained by the fit method, the model instance can answer questions
    such as “What is the estimated fuel efficiency of a 3,000-pound Ford Mustang?”
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.3 Using the trained linear regression model to estimate mpg
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Reshape an array containing a single value of 3,000 (representing a 3,000-pound
    Ford Mustang) into a matrix with one row and one column and ask the model to estimate
    the output value using the predict method. Since the output of the predict method
    is returned as a NumPy array, retrieve the first element from the array of the
    estimated values using a [0] index.
  prefs: []
  type: TYPE_NORMAL
- en: This outputs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: which represents the estimate of roughly 20.03 miles per gallon (MPG). The machine
    learning model can also produce estimates for other values of the vehicle weight.
    Note that the code that follows is more straightforward and thus easier to understand
    for existing Python developers who are new to machine learning.[⁸](#pgfId-1013369)
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.4 Estimating MPG for vehicles weighing 2,500, 3,000, and 3,500 pounds
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '❶ The Python for expression iterates over weight values from the [2_500, 3_000,
    3_500] list. For each weight in the list, the expression returns a row of a matrix
    consisting of two columns: the left column with the value of mpg predicted by
    the model for the weight, and the right column with the value of the weight itself.
    The resulting matrix is stored in the ds variable.'
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The pandas DataFrame is instantiated using the ds matrix and annotated with
    column names mpg_est and weight for the left and right columns, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ To avoid printing the default, zero-based index for each row, df.to_string(index=False)
    is used instead of print(df).
  prefs: []
  type: TYPE_NORMAL
- en: This outputs the results shown as a pandas DataFrame on the left side of figure
    A.5\. The right side of figure A.5 shows that the model learned by LinearRegression
    from the original data set has a dashed line and can be used to estimate mpg values
    for arbitrary values of weight.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-05](Images/A-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.5 A table of the estimated fuel efficiency mpg for hypothetical Ford
    Mustangs with the weight values given by the weight column (left) and the corresponding
    linear model (right) plotted by connecting the data points from the table
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned from an example where the machine learning problem,
    the data set, and the machine learning algorithm were prepared for you in advance.
    This meant that you did not have to fully understand the nuances of the problem,
    how to prepare the data set for the problem, or how to choose the right machine
    learning algorithm to solve the problem. In the rest of the appendix, you will
    explore these dimensions of working with machine learning in greater depth and
    prepare to apply your understanding to the machine learning project in this book.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Machine learning with structured data sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you were introduced to an application of machine learning
    using an example data set describing the fuel efficiency of Ford Mustangs. This
    section teaches the concepts needed to apply machine learning across arbitrary
    structured data sets.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this appendix, a *structured data set* stores observations[⁹](#pgfId-1013710)
    about
  prefs: []
  type: TYPE_NORMAL
- en: Related objects, for example cars of different makes and models, publicly traded
    companies in the S&P 500 index, iris flowers of different sub-species, or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurring events, such as won or lost sales opportunities, on-time or late meal
    deliveries, or button clicks on a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as records (usually rows) in a table, where each *record* consists of numeric
    values organized as at least two but typically three or more columns of the table.
    Note that figure A.6 illustrates a structured data set with N observations (records
    in rows) and M columns, as well as the related terminology explained later in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-06](Images/A-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.6 A conceptual representation of a structured data set for machine
    learning. The data set is organized into N rows of observations such that each
    row has an observation label column along with M - 1 (the remaining) columns of
    features. By convention, the first column is often the observation label. In cases
    when observations are about events (e.g., labeling whether a sales opportunity
    was won or lost), the label is sometimes called the *outcome* of the event. When
    using the data set for machine learning, the label is often described as the *truth,
    target*, or *actual* value. Variables y and X are commonly used in machine learning
    to denote the label and the features, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: A *supervised machine learning algorithm* (e.g., the linear regression algorithm
    used in section A.2) for a structured data set trains (outputs) a machine learning
    model, which can estimate the value for one of the columns in a record (the label)
    using the values from the other columns in the record (the features).
  prefs: []
  type: TYPE_NORMAL
- en: In supervised machine learning, a *label* is the numeric value used as the target[^(10)](#pgfId-1013877)
    for the estimates produced by a machine learning model during training. By convention,
    machine learning practitioners often use the first column of a structured machine
    learning data set to store the values for the label (often designating it using
    the variable y) and the remaining columns (using variable X) to store the features.
    The *features* are the numeric values used by a supervised machine learning model
    to estimate the label value. When a collection of records consisting of labels
    and features is used to train a machine learning model, the records are described
    as a *training data set*.
  prefs: []
  type: TYPE_NORMAL
- en: The term *label* is ambiguous in machine learning
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the use of the word *label* isn’t consistent in the machine learning
    community, leading to much confusion for newcomers to machine learning. While
    the use of the word is frequent when the observations in a structured data set
    describe related objects (e.g., Ford Mustangs), the word *outcome* is used synonymously
    with label when the observations in the data set describe events (e.g., sales
    opportunities). Machine learning practitioners with a statistics background often
    resort to describing the label as the *dependent variable* and the features as
    the *independent variables*. Others use *target value* or *actual value* synonymously
    with *label*. This book aims to simplify the terminology and uses the word *label*
    whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: The field of machine learning is much broader than supervised machine learning
    and includes topics like unsupervised machine learning (where the label is not
    used or not available), reinforcement learning (where algorithms seek to maximize
    a reward), generative adversarial networks (where neural nets compete to generate
    and classify data), and more. However, even at Google, arguably a leader in adopting
    machine learning, more than 80% of the machine learning models that are put into
    production are based on supervised machine learning algorithms using structured
    data. Hence, this book focuses entirely on this important area of machine learning.[^(11)](#pgfId-1014116)
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical definition of supervised machine learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it is not required for this book, here’s a more formal definition
    of supervised machine learning: if *y*[i] is the value to be estimated from a
    record with an index i, then a supervised machine learning model can be described
    as a function F that outputs the estimate *F*(*X[i]*) based on values *X*[i] of
    the remaining (i.e., other than *y*[i]) columns in the record. The training process
    for a supervised machine learning model describes construction of the function
    F based on a training data set *y*, *X*. The training algorithm is often iterative
    using *y*, *X*, *F*(*X*), where the base *F*(*X*) is produced from some random
    initialization of F.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an illustration of supervised machine learning, recall that in section
    A.2 you learned about a structured data set describing the fuel efficiency of
    Ford Mustangs. The training data set consisted of just two columns: a label with
    the mpg values, and a single feature with the value of the vehicle weight. This
    data set is also shown as an example in figure A.7\. The corresponding supervised
    machine learning model based on the data set can estimate the average fuel efficiency
    in miles per gallon (mpg column) for 1971 to 1982 models of Ford Mustangs, based
    on the values from the weight column.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A-07](Images/A-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.7 A sample data set of Ford Mustang fuel efficiency in terms of miles
    per gallon.
  prefs: []
  type: TYPE_NORMAL
- en: 'With most machine learning algorithms, training is an iterative process (illustrated
    in figure A.8) that starts with a machine learning algorithm producing the first
    iteration of a machine learning model. Some algorithms use the training data set
    to create the first iteration of the model, but this is not required: most neural
    network-based deep learning models are initialized according to a simple random
    scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the first iteration of the model is ready, it is used to output the first
    iteration of the estimates (predictions) based on the features from the training
    data set. Next, the quality of the estimates is evaluated by the machine learning
    algorithm by comparing how close the estimates are to the labels from the training
    data set. This quantifiable measure used to evaluate the quality (i.e., the performance)
    of the machine learning model is known as *loss* (also known as the *cost* or
    *objective function*) and is covered in more detail in section A.4\. For the next
    iteration of the process, the loss along with the training data set are used by
    the algorithm to produce the next iteration of the model. The non-iterative machine
    learning algorithms can output a machine learning model after a single iteration
    of the process shown in figure A.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-08](Images/A-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.8 The initial machine learning model produced by the machine learning
    algorithm is used to output estimated label values (y_est), estimates per record
    based on the records’ feature values. The machine learning model is then improved
    iteratively by changing the model (specifically the model parameters) to improve
    the model performance score (loss), which is based on the comparison of the estimated
    and label values.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative machine learning algorithms vary in how they approach the decision
    to stop the training process; some have built-in criteria used to stop iterating,
    while others require the machine learning practitioner to provide explicit stopping
    criteria, or to provide a schedule or a function for deciding when to stop. Additional
    details on how to approach the stopping criteria when covering different machine
    learning algorithms starts in section A.4.
  prefs: []
  type: TYPE_NORMAL
- en: So far, this appendix has used the phrase *numeric values* intuitively, without
    providing a clear definition of the numeric values suitable for machine learning.
    As mentioned in section A.1, machine learning algorithms require a custom prepared
    data set and may fail when used with arbitrary data values. Hence, it is imperative
    for a machine learning practitioner to have a clear understanding of the numeric
    values present in a structured data set. Illustrated in figure A.9 is a detailed
    taxonomy (originating from a similar taxonomy in statistics) for classifying numeric
    variables based on their values.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-09](Images/A-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.9 The categories of numeric values for supervised machine learning
    are adapted from a well-known framework for classifying statistical variables
    by Stanley Smith Stevens ([http://mng.bz/0w4z](http://mng.bz/0w4z)). Numeric values
    can be classified into mutually exclusive subsets of continuous and categorical
    values. Continuous values can be further classified as interval or ratio, while
    categorical can be either nominal or ordinal.
  prefs: []
  type: TYPE_NORMAL
- en: This appendix and the rest of the book focus on machine learning with continuous
    and categorical variables, specifically using interval, ratio, and nominal data.
    Whenever possible, the book provides hints and tips for working with ordinal values;
    however, the project in the book does not cover any specific use cases for these
    two types of values. As a machine learning practitioner, you are expected to prepare
    and convert the messy, real-world data sets to the numeric values that can be
    correctly used by machine learning. Much of part 1 is dedicated to sharpening
    your skills in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned about training, the sequence of steps performed
    by a machine learning algorithm to produce a machine learning model. During training,
    a subset of the structured data set (known as the *training data set*) is used
    by the algorithm to produce the model, which can then be used to output the estimates
    (also known as *predictions*), given the feature values from a record.
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Regression with structured data sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will learn about the two commonly used categories of supervised
    machine learning problems: regression and classification. The section introduces
    a definition of loss (also known as the cost or the objective function), a quantitative
    and technical measure of the performance of a machine learning model on a data
    set of labels and features. By the conclusion of the section, you will be familiar
    with the terminology related to the problems and review applications of regression.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Regression* for structured data sets is a supervised machine learning problem
    where the label is a continuous (as defined in figure A.9) variable. For example,
    when estimating Ford Mustang fuel efficiency in section A.2, you worked on an
    instance of a regression problem since mpg is a continuous (more precisely an
    interval) value. In section A.5, you will learn more about classification for
    structured data sets, a supervised machine learning problem where the label is
    a categorical variable. Comprehensive understanding of these machine learning
    problems is critical to a machine learning practitioner, since, as explained in
    section A.3, regression and classification make up over 80% of production machine
    learning models at top information technology companies like Google.'
  prefs: []
  type: TYPE_NORMAL
- en: An example of a regression problem and the related loss calculations based on
    the Ford Mustang data set is shown in figure A.10\. Recall that in section A.2
    you played the role of the machine learning algorithm and intuited a mental model
    for estimating the mpg value. Suppose that you reprise the same role in this section,
    but here you estimate the mpg value by taking the value of the weight and multiplying
    it by 0.005\. Since the training process is iterative, the value of 0.005 is just
    an initial (perhaps a lucky) but a reasonable guess. Better approaches for making
    the guesses will be introduced shortly. In the meantime, the values of the estimates
    based on this calculation are shown in the Estimate column of figure A.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-10](Images/A-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.10 In regression problems, many machine learning practitioners start
    with an application of the mean squared error loss function to establish a baseline
    before moving on to more complex machine learning approaches and experimenting
    with more complex loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from the process explained in figure A.8 that the next step is to evaluate
    the loss, a quantifiable measure of the quality of the estimates produced by the
    machine learning model. The choice of the loss function depends on the machine
    learning problem and more precisely on the numeric type of both the label and
    estimated values in the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the regression problem, one of the most frequently used loss functions is
    the *mean squared error* (MSE), defined as the arithmetic mean of the individual
    squared error (also known as *residual* or *difference*) values, as illustrated
    in the Squared Error column of figure A.10.
  prefs: []
  type: TYPE_NORMAL
- en: The Python code to derive the values shown in the columns of figure A.10 is
    provided.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.5 The squared errors for the calculation of the model loss
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Instantiate the Ford Mustang data set as a pandas DataFrame. For brevity and
    following an accepted practice, this example uses y for the label and X for the
    feature(s). A more detailed explanation is available in listing A.1.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The variable name W is often used to represent the values for the machine
    learning model parameters. Notice that the NumPy slicing notation [:, None] is
    equivalent to using reshape(1,1) to reshape W to a matrix needed in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The double-squared bracket notation used by the expression df[['X']] returns
    a matrix of feature values and the matrix product (using the @ operation), with
    the matrix containing the model parameter value producing the resulting matrix
    with a single column containing the weights multiplied by 0.005\. Matrix multiplication
    is used here since it easily scales to many features and model parameter values
    without having to change the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The error is just the difference between the label and the estimated values.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The squared_error is calculated using Python ** exponentiation notation.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ The list of the column names is specified to ensure that the order of the
    columns in the output corresponds to the order shown in figure A.10.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming the values for the squared error are stored in a pandas DataFrame column
    named squared_error, the corresponding value for MSE can be computed using simply
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: which in the case of the values from figure A.10 outputs a number that is approximately
    equal to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you should expect, since the value of W was picked randomly, the value of
    the mean squared error is nowhere near zero. In figure A.11, you can explore the
    results of various random choices for W (subplot (a) corresponds to using 0.005
    as W) as well as the relationship between the random values of W and the corresponding
    mean squared error.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-11](Images/A-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.11 Subplots a—d illustrate the impact of choosing alternative, randomly
    selected values of W on the mean squared error. Notice that in all cases, the
    value of W corresponds to the slope of the line that passes through the points
    specified by the pair of the feature and label values. Unlike the linear regression
    model used in figure A.5, the lines on this figure pass through the origin and
    hence do not capture the pattern of lower weight corresponding to higher miles
    per gallon in fuel efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Since the line-based (linear) model based on W is so simple, instead of randomly
    guessing the value of W, you can rely on an analytical solution for the problem
    of estimating W that minimizes the mean squared error for the data set. The analytical
    solution known as the *ordinary least squares* (OLS) *formula* (X^TX)^(-1)X^Ty
    can be implemented using Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.6 Ordinary least squares solution for linear regression
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Assign X as a NumPy array of the feature values.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Assign y as a NumPy array of the label values.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Compute the expression X.T @ X from the OLS formula, convert it to a NumPy
    matrix (using ndmin=2), and invert the resulting matrix using np.linalg.inv.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Multiply the inverted matrix by X.T @ y from the OLS formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'This returns a 1 × 1 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can confirm, using the sample code from listing A.5, that when using the
    optimal value of W based on the OLS formula produces MSE of 40.88. What does this
    mean? Note that unlike the LinearRegression model shown in figure A.5, the model
    based only on W is not complex enough to capture the underlying pattern in the
    data: the heavier weight leads to lower fuel efficiency. Of course, just by visual
    examination of the subplots in figure A.11 the reason is obvious: a line based
    on a single W parameter must pass though the origin (i.e., the y intercept is
    zero); hence, it is impossible to use it to model the inverse relationship between
    the greater-than-zero values in the mpg and weight data columns.'
  prefs: []
  type: TYPE_NORMAL
- en: However, when working with more complex data sets, where the data has too many
    dimensions to be easily visualized, a visualization does not help with the decision
    about whether the model is sufficiently flexible to capture the desired patterns
    in the data set. Instead of relying on a visualization, you can perform additional
    tests to evaluate the flexibility of the model. In a regression problem, you can
    compare your model to the mean squared error of estimating the label values using
    the mean label value from the training data set. For example, using the DataFrame
    with the training data set, this can be done by evaluating
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: which should output approximately
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The difference between the optimal model MSE of 40.88 and the naive MSE (using
    the average) of 27.03 shows that the model is not sufficiently complex (has too
    few parameters) to capture the desired pattern in the data.
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Classification with structured data sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces you to and illustrates the cross-entropy loss function
    used by many machine learning algorithms to train classification models. With
    the understanding of cross-entropy in place, the section walks you through the
    steps needed to implement the one-hot encoding of labels and how to use the encoded
    labels to compute values of cross-entropy loss. Before concluding, the section
    teaches you the best practices for using NumPy, pandas, and scikit-learn so that
    you can train and evaluate a baseline LogisticRegression classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from section A.4 that classification for structured data sets is a machine
    learning problem of estimating the value of a categorical label from the feature(s).
    For example, the Ford Mustang data set shown in figure A.12 can be used to train
    a classification model (also known as a *classifier*) to estimate the decade of
    the model year, 1970s versus 1980s, using the mpg (fuel efficiency) and weight
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-12](Images/A-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.12 The one-hot encoding of the model year label using columns named
    1970s and 1980s representing the Ford Mustang decade of the car’s make (left).
    The one-hot nature of the encoding refers to a single 1 value across every row
    in the columns used for the encoding; the remainder of the values are 0. A scatter
    plot (right) for the data set illustrates the vehicles from 1970s and 1980s using
    “x” and “•” markers, respectively. Given any location on the grid (not limited
    to the locations shown using the markers), a trained classification model must
    be able to estimate whether the vehicle was manufactured in the 1970s or 1980s.
  prefs: []
  type: TYPE_NORMAL
- en: Although the mean squared error loss function can be used for some classification
    problems,[^(12)](#pgfId-1015715) many baseline classifiers use the *cross-entropy
    loss*. Machine learning algorithms designed to optimize the cross-entropy loss
    include logistic regression (which is a machine learning algorithm for classification
    and should not be confused with the regression machine learning problem) and neural
    networks, among others. A closely related loss function known as *Gini impurity*
    is used by the popular decision tree and random forest algorithms. This section
    explains classification using the cross-entropy loss first, to prepare you to
    understand the variations on cross-entropy used by the more advanced classification
    machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the mean squared loss, which expects the output of a regression model
    to be a single numeric value, cross-entropy loss expects the classification model
    to output a probability for each possible value of the categorical label. Continuing
    with the working data set for estimation of the decade of Ford Mustang models,
    figure A.13 illustrates four informative examples of the outputs for a hypothetical
    classification model based on the data set.
  prefs: []
  type: TYPE_NORMAL
- en: In example 1 shown on the upper left of figure A.13, the classification model
    assigns a 0.6 probability to the estimate of 1970s as the decade of the model
    year. Since probabilities must add up to 1, the estimate of 1980s is 0.4. In this
    example, the corresponding loss value (shown in the title of the example) of approximately
    0.51 is significantly greater than zero since the classification model estimates
    the correct value (1970s) but lacks confidence in the estimate. Observe from example
    2 on the upper right of figure A.13 that when the model is completely uncertain
    of the correct value, lacking confidence or preference to estimate either 1970s
    or 1980s (due to 0.5 probability for both),[^(13)](#pgfId-1015871) the loss increases
    further, reaching approximately 0.6931.[^(14)](#pgfId-1015894) In a nutshell,
    the cross-entropy loss function decreases toward zero when the classification
    model outputs a high probability (effectively a high confidence) value for the
    correct label estimate and increases otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function increases even further, beyond the loss number reported in
    case of the complete uncertainty, if the classification model is incorrect in
    the estimation of the label value, as shown in example 3 on the lower left of
    figure A.13\. In this example, the correct label value is 1980s while the model
    is slightly more confident in the estimate of 1970s than 1980s with probabilities
    of 0.6 and 0.4, respectively. Note that the loss value increases further, from
    0.9163 in example (3) to 4.6052 in example 4, on lower right of figure A.13, where
    the classification model is highly confident about the wrong estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-13](Images/A-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.13 (1) The model is slightly more confident in the correct value. (2)
    The model is entirely uncertain about which of the two values to choose. (3) The
    model is slightly more confident in the incorrect value. (4) The model is highly
    confident in the incorrect value.
  prefs: []
  type: TYPE_NORMAL
- en: Since the output of a classification model consists of probabilities (a probability
    distribution) for label values, the original labels in the working data set must
    be encoded (converted) into the same format before training or testing the machine
    learning model. The result of the encoding is shown in columns 1970s and 1980s
    on the left side of figure A.12\. The process of this conversion is known as *one-hot
    encoding*, referring to the fact that just a single value is set to be one (1)
    across the entire row in the columns encoding the label.
  prefs: []
  type: TYPE_NORMAL
- en: The cross-entropy loss function can be defined in terms of NumPy operations.
    The xe_loss function definition implements the calculation for cross-entropy loss
    given an array of the classification model output y_est and the corresponding
    one-hot encoded array of the label values y. Note that with the implementation
    you need to take care not to confuse the label and the model output array parameters
    because the np.log function outputs -Inf and 0.0 for the values of 0 and 1, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.7 xe_loss computing and returning the cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Compute the loss value of 0.9163 based on example 3 of figure A.13.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Compute the loss value of 0.6931 based on example 2 of figure A.13.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Compute the loss value of 0.9163 based on example 3 of figure A.13.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Compute the loss value of 4.6052 based on example 4 of figure A.13.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code from listing A.7 outputs the following cross-entropy loss
    values corresponding to examples 1—4 from figure A.13:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Mathematical definition of cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a mathematical description of cross-entropy loss: given a single training
    example *y*, *X* of a label and features, respectively, the function is defined
    as ![A-13_EQ01](Images/A-13_EQ01.png), where *K* is the number of the values of
    the categorical label variable, *y[k]* is the probability of a specific label
    value in a one-hot encoded label *y*, and ![A-13_EQ02](Images/A-13_EQ02.png) is
    the probability estimate of a specific label value for the estimate ![A-13_EQ03](Images/A-13_EQ03.png)produced
    by a classification model.'
  prefs: []
  type: TYPE_NORMAL
- en: The examples used so far in this section relied on label values that have already
    been one-hot encoded for you. In practice, you must implement the label encoding
    before training a classification model. Although it is possible to use a comprehensive
    set of scikit-learn classes to one-hot encode the model year column label,[^(15)](#pgfId-1016630)
    since in this appendix the data set is instantiated as a pandas DataFrame it is
    easier to use a generic pandas get_dummies method for label encoding. The whimsical
    get_dummies naming of the method stems from *dummy variables*, a term used in
    statistics to describe binary indicator variables that are either 1 to indicate
    a presence or 0 to indicate absence of a value.
  prefs: []
  type: TYPE_NORMAL
- en: Given the label and the features for the data set as a pandas DataFrame, a direct
    application of the get_dummies method to the model year label is shown.
  prefs: []
  type: TYPE_NORMAL
- en: Listing A.8 Using get_dummies on a categorical label
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Instantiate the data set as a pandas DataFrame. Since this instantiation uses
    model year as the label, it is placed in the leading column.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Using get_dummies with a pandas Series identifies the set of unique values
    in a series and creates a new column for each value in the set. The prefix parameter
    ensures that each new column is named using the specified prefix. Setting sparse
    to True may result in, but does not guarantee, reduced memory utilization by the
    resulting DataFrame. Labels with a larger number of distinct values and correspondingly
    more columns in a one-hot encoded format benefit from sparse array representations
    enabled by sparse set to True.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Print the resulting enc_df DataFrame without the zero-based index.
  prefs: []
  type: TYPE_NORMAL
- en: This produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'which is not the desired encoding. Although you can easily implement the code
    to convert the exact model year of the vehicle from the column name to the desired
    encoding of the model decade, *binning* is an alternative and a more flexible
    approach to perform label encoding for this use case. Using the pandas cut method,
    you can “bin” the label values into a range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'which outputs a pandas.Series of range intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the first three vehicles were correctly placed in the 1970s (1969
    is excluded, as indicated by the open parenthesis) while the remaining vehicles
    are placed in the 1980s.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the label binning with get_dummies for one-hot encoding,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'outputs the desired encoding from figure A.12:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Prior to evaluating the cross-entropy loss function with the encoded values,
    it is convenient to join the columns of the label encoding with the original data
    set, replacing the original label values with the encoded ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: which results in
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: At this point the data set is prepared to be partitioned into the label and
    the features used for training and then converted to NumPy arrays. Starting with
    the label values,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: outputs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: To place the feature values into the NumPy X_train array, use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: which prints out
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: At this point you are ready to train a LogisticRegression classifier model,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: and compute the cross-entropy loss,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: which outputs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: A.6 Training a supervised machine learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training a machine learning model, you will virtually never use your entire
    structured data set as the training data set. Instead, the pattern followed by
    most machine learning practitioners is to partition the initial data set into
    two mutually exclusive subsets:[^(16)](#pgfId-1017607) a development (dev) data
    set and a test (also known as a held-out) data set.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-14](Images/A-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.14 Once the machine learning project data set is partitioned to extract
    the held-out, test data set, it is common to start exploratory data analysis and
    machine learning model training directly with the dev data set, in other words
    to use the dev data set as the training data set (left). A more mature machine
    learning training workflow that can help detect overfitting early and optimize
    the hyperparameters includes further partitioning of the dev data set into training
    and validation data sets, as well using cross-validation with the dev data set.
  prefs: []
  type: TYPE_NORMAL
- en: Many newcomers to the machine learning field are not familiar with the concept
    of the dev data set. While it is possible to use it directly as the training data
    set (and many online courses and tutorials use this simplified approach), training
    machine learning for production requires a more robust approach. The distinction
    between the two approaches is illustrated in figure A.14\. As shown on the right-hand
    side of figure A.14, the dev data set is further split into the training and validation
    data sets. As before, the purpose of the training data set is to train the machine
    learning model; however, the purpose of the validation (or evaluation) data set
    is to estimate the expected performance of the trained machine learning model
    on the held-out (test) data set. For example, in the Ford Mustang fuel efficiency
    data set, one record can be chosen randomly to be in the test data set, and four
    records in the dev data set. Next, one record chosen randomly from the dev data
    set can again be placed in the validation data set, and the remaining three records
    placed in the training data set to train a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Since the purpose of the validation data set is to estimate the performance
    of the training machine learning model on the test data set, having just one record
    in the validation data set is an issue. However, it is also valuable to use as
    many observations from the dev data set for training as possible. One solution
    to this dilemma is to use a technique known as *K-fold* *cross-validation*, illustrated
    in figure A.15\. The key idea behind using K-fold cross-validation is to train
    K different machine learning models by reusing the dev data set K times, each
    time partitioning the dev data set into K folds where K-1 folds are used as the
    training data set and the remaining K-th fold is used as the validation data set.
    The example in figure A.15 uses three partitions, or three-fold cross-validation.
    When the number of observations in the data set does not divide without a remainder
    into K-folds, the partition with the smallest number of observations is designated
    as the validation data set. Otherwise, all the partitions have the same size in
    terms of the number of the observations.
  prefs: []
  type: TYPE_NORMAL
- en: Next, K separate machine learning models are trained using the K-1 training
    data set partitions and are validated using the remaining K-th validation partition.
    Hence, in the example in figure A.15, three separate machine learning models are
    trained using the two training folds in each of the three different partitions.
  prefs: []
  type: TYPE_NORMAL
- en: '![A-15](Images/A-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A.15 The K-fold cross-validation technique includes training K different
    machine learning models and reporting the training as well as the validation loss
    (and metric) based on the average of the training and validation values obtained
    from each of the K independent models. Note that each of the K models is validated
    using a different validation partition of the dev data set, using the remainder
    of the dev data set for training.
  prefs: []
  type: TYPE_NORMAL
- en: The dev data set may be used as-is to train a machine learning model, in other
    words, as a training data set. However, this is rarely the case for production
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the dev data set is split further into training and validation data
    sets. Chapter 4 explains and illustrates this process in more detail, but for
    the purposes of this appendix, you can expect that the validation data set is
    used to estimate the performance of the machine learning model on the held-out
    (test) data set, which is not used for training.
  prefs: []
  type: TYPE_NORMAL
- en: ^(1.)A comprehensive review of the traditional computer science algorithms is
    available from Donald E. Knuth in his seminal work *The Art of Computer Programming*
    (Addison-Wesley Professional, 2011).
  prefs: []
  type: TYPE_NORMAL
- en: ^(2.)The values are based on a data set from the publicly available University
    of California Irvine Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)
  prefs: []
  type: TYPE_NORMAL
- en: ^(3.)Also known as panel or tabular data, structured data sets are based on
    values organized into rows and columns
  prefs: []
  type: TYPE_NORMAL
- en: ^(4.)NumPy is a Python library for high-performance numerical computing. pandas
    wraps the NumPy library and uses it for high-performance data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: ^(5.)scikit-learn ([scikit-learn.org](http://scikit-learn.org)) was designed
    for machine learning with in-memory data sets as is used here to illustrate machine
    learning with a simple example. Machine learning with out-of-memory data sets
    using cloud computing requires other frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: ^(6.)Although it is commonly associated with modern statistics, linear regression
    originates from the early 19th-century work by Gauss and Lagendere on predictions
    of planetary movement.
  prefs: []
  type: TYPE_NORMAL
- en: ^(7.)As well as in many other scientific fields, including statistics, econometrics,
    astronomy, and more.
  prefs: []
  type: TYPE_NORMAL
- en: ^(8.)It is possible to implement this code more concisely at the expense of
    having to explain additional NumPy and pandas concepts, including multidimensional
    arrays. If you are interested in learning more about this topic and a more general
    topic of tensors, check out chapter 5.
  prefs: []
  type: TYPE_NORMAL
- en: ^(9.)To be mathematically precise, the observations are expected to be statistically
    independent and identically distributed, though most real-world data sets exist
    in the gray area of this definition.
  prefs: []
  type: TYPE_NORMAL
- en: ^(10.)Hence, you should not be surprised when you hear the label described as
    a “target value.”
  prefs: []
  type: TYPE_NORMAL
- en: '^(11.)If you are interested in broadening your knowledge of machine learning
    beyond supervised machine learning and are willing to read more math-heavy books,
    check out *Artificial Intelligence: A Modern Approach* by Stuart Russell and Peter
    Norvig (Pearson, 2020); *Pattern Recognition and Machine Learning* by Christopher
    Bishop (Springer, 2006); *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and
    Aaron Courville (The MIT Press, 2016); and *The Elements of Statistical Learning*
    by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (Springer, 2016).'
  prefs: []
  type: TYPE_NORMAL
- en: ^(12.)It is possible to use mean squared error for classification problems if
    the label is binary and is encoded as either —1 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: ^(13.)This state of a uniform probability distribution is also known as *maximum
    entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: ^(14.)You may have noticed that this is approximately the value of the e constant
    since the cross-entropy calculations here use natural logarithms.
  prefs: []
  type: TYPE_NORMAL
- en: ^(15.)scikit-learn provides a comprehensive set of classes, including LabelEncoder
    and LabelBinarizer, designed to help with label encoding, as well as OneHotEncoder
    and OrdinalEncoder for feature encoding; these classes are best suited to the
    development scenarios that do not use pandas to store and manage data sets. For
    example, if your entire data set is stored as NumPy arrays, these scikit-learn
    classes are a good option. However, if your data set is a pandas DataFrame or
    a pandas Series, it is more straightforward to apply pandas’s own get_dummies
    method for both label and feature encoding.
  prefs: []
  type: TYPE_NORMAL
- en: ^(16.)Here “mutually exclusive” means that duplicate records are removed prior
    to partitioning, and following the de-duplication any given record exists either
    in one or the other subsets.
  prefs: []
  type: TYPE_NORMAL
