- en: '8 Deep Learning: the foundational concepts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building blocks of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers in a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning using deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning using deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python code using tensorflow and keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Life is really simple, but we insist on making it complicated – Confucius”
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the third part of the book. So far, you have covered a lot of concepts
    and case studies and Python code. From this chapter onwards, the level of complexity
    will be even higher.
  prefs: []
  type: TYPE_NORMAL
- en: In the first two parts of the book, we covered various unsupervised learning
    algorithms like clustering, dimensionality reduction etc. We discussed both simpler
    and advanced algorithms. We also covered working on text data in the last part
    of the book. Starting from this third part of the book, we will start our journey
    on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and neural networks have changed the world and the business domains.
    You must have heard about deep learning and neural networks. Their implementations
    and sophistication results in better cancer detection, autonomous driving cars,
    improved disaster management systems, better pollution control systems, reduced
    fraud in transactions and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the third part of the book, we will be exploring unsupervised learning using
    deep learning. We will be studying what is deep learning and the basics of neural
    networks to start with. We will study what are the layers in a neural network,
    activation functions, process of deep learning and various libraries. Then we
    will move to Autoencoders and Generative Adversarial Networks and Deep Belief
    Networks. The topics are indeed complex and sometime quite mathematically heavy.
    We will using different kinds of datasets for working on the problems, but primarily
    the datasets will be unstructured in nature. As always, Python will be used to
    generate the solutions. We are also sharing a lot of external resources to complement
    the concepts. Please note that these are pretty advanced topics and a lot of research
    is still undergoing for these topics.
  prefs: []
  type: TYPE_NORMAL
- en: We have divided the third part of the book in three chapters. The eighth chapter
    covers the foundation concepts of deep learning and neural networks required.
    The next two chapters will focus on autoencoders, GAN and Deep belief networks.
    The final chapter of the book talks about the deployment of these models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter 8 discusses the concepts of neural networks and deep learning.
    We will be discussing what is a neural network, what are activation functions,
    what are different optimization functions, neural network training process etc.
    It is vital for you to know these concepts of deep learning before you can understand
    the deeper concepts of auto encoders and GAN. The concepts covered in this chapter
    form the base of neural networks and deep learning and subsequent learning in
    the next two chapters. Hence, it is vital that you are clear about these concepts.
    The best external resource to get these concepts in more details are given at
    the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the eighth chapter and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at this location.
  prefs: []
  type: TYPE_NORMAL
- en: You would need to install a few Python libraries in this chapter which are –
    tensorflow and keras.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with Chapter 8 of the book!
  prefs: []
  type: TYPE_NORMAL
- en: '8.2 Deep Learning: What is it? What does it do?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has gathered a lot of momentum in the last few years. Neural networks
    are pushing the boundaries of machine learning solutions. Deep learning is machine
    learning only. Deep learning is based on neural networks. It utilizes the similar
    concept i.e. using the historical data, understanding the attributes and the intelligence
    gathered can be then used for finding the patterns or predicting the future, albeit
    deep learning is more complex than the algorithms we have covered so far.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from Chapter 1 where we covered concepts of structured and unstructured
    datasets. Unstructured datasets are the text, images, audio, video etc. In Figure
    8-1 we describe the major sources of text, images, audio, video datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-1 Unstructured datasets like text, audio, images, videos can be analyzed
    using deep learning. There are multiple sources of such datasets.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone Description automatically generated](images/08image004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Whilst deep learning can be implemented for structured datasets too, it is
    mostly working wonders on unstructured datasets. One of the prime reasons is that
    the classical machine learning algorithms are sometimes not that effective on
    unstructured datasets like that of images, text, audios and videos. We are listing
    a few of the path breaking solutions delivered by deep learning across various
    domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Medical and pharmaceuticals**: Deep learning sees application in areas such
    as the identification of bones and joint problems, or in determining if there
    are any clots in arteries or veins. In the pharmaceuticals field, it expedites
    clinical trials and helps to reach the target drug faster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Banking and financial sector**: Deep learning-based algorithms are used to
    detect potential fraud in transactions. Using image recognition-based algorithms,
    we can also distinguish fake signatures on cheques.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Automobile sector:** You must have heard about autonomous driving aka self-driving
    cars. Using deep learning, the algorithms are able to detect traffic signals,
    pedestrians, other vehicles on the road, their respective distances and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Automatic speech recognition** is possible with deep learning. Using the
    sophisticated neural networks, humans are able to create speech recognition algorithms.
    These solutions are being used across Siri, Alexa, Translator, Baidu etc.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retail:** In the retail sector, using deep learning-based algorithms, humans
    are able to improve the customer targeting and develop advanced and customized
    marketing tactics. The recommendation models to provide next-best products to
    the customers have been improved using deep learning. We are able to get better
    ROI (return-on-investments) and able to improve cross-sell and upsell strategies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Image recognition**: Neural networks are improving our image recognition
    techniques: It can be done using convolutional neural networks which is improving
    computer vision. The use cases are many like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep learning is quite effective for differentiation between cancerous cells
    and benign cells. It can be achieved by using the images of cancerous cells and
    benign cells.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Automated number plate reading system are already developed using neural networks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Object detection methods can be developed using deep learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Motion sensing and tracking systems can be developed using deep learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In disaster management systems, deep learning can detect the presence of humans
    in the impacted areas. Just imagine how precious moments and eventually human
    lives can be saved using better detection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The use cases listed are certainly not exhaustive. Using deep learning, we are
    able to improve natural language processing (NLP) solutions used to measure customer’s
    sentiments, language translation, text classification, named-entity-recognition
    etc. Across use cases in bioinformatics, military, mobile advertising, telecom,
    technology, supply chain and so on – deep learning is paving the path for the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered how powerful deep learning is. We will now start with the
    building blocks of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Building blocks of a neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial Neural Networks (ANNs) are said to be inspired by the way a human
    brain works. The human brain is the best machine we currently have access to.
    When we see a picture or a face or hear a tune, we associate a label or a name
    against it. That allows us to train our brain and senses to recognize a picture
    or a face or a tune when we see/hear it again.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs learn to perform similar tasks by learning or getting trained.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  What is the meaning of deep learning?
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Neural networks cannot be used for unsupervised learning. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Explore more use cases for deep learning in non-conventional business domains.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to explore how neural network helps with some business solutions
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1 Neural Networks for solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In deep learning too, the concepts of supervised and unsupervised learning
    are applicable. We are going to cover both types of trainings of the network:
    supervised and unsupervised. It gives you the complete picture. At the same time,
    to fully appreciate the unsupervised deep learning, it is suggested to be clear
    on supervised deep learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand the deep learning process using an example. Consider this,
    we wish to create a solution which can identify faces, that means a solution which
    can distinguish faces and also identify the person by allocating a name to the
    face. For training the model, we will have a dataset which will have images of
    people’s face and corresponding names. ANN will start with no prior understanding
    of the image’s dataset or the attributes. The ANN during the process of training,
    will learn the attributes and the identification characteristics from the training
    data and learn them. These learned attributes are then used to distinguish between
    faces. At this moment, we are only covering the process at a high level; we will
    cover this process in much more detail in subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: You can see a representation of a neural network in Figure 8-2.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-2 A typical neural network with neurons and various layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/08image006.png)'
  prefs: []
  type: TYPE_IMG
- en: The process in a neural network is quite complex. We will first cover all the
    building blocks of a neural network – like neuron, activation functions, weights,
    bias terms etc. and then move to the process followed in a neural network. We
    will start with the protagonist – a neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2 Artificial neuron and perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A human brain contains billions of neurons. The neurons are interconnected cells
    in our brains. These neurons receive signals, process them and generate results.
    Artificial neurons are based on biological neurons only and can be said as simplified
    computational models of biological neurons.
  prefs: []
  type: TYPE_NORMAL
- en: In the year 1943, researchers Warren McCullock and Walter Pitts proposed the
    concept of a simplified brain cells calls McCullock-Pitts (MCP) neuron. It can
    be thought as a simple logic gate with binary outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The working methodology for artificial neurons is similar to biological neurons,
    albeit they are far simpler than biological neurons. The perceptron is a mathematical
    model of a biological neuron. In the actual biological neurons, dendrites receive
    electrical signals from the axons of other neurons. In the perceptron, these electrical
    signals are represented as numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: The artificial neuron receives inputs from the previous neurons or can receive
    the input data. It then processes that input information and shares an output.
    The input can be the raw data or processed information from a preceding neuron.
    The neuron then combines the input with their own internal state, weighs them
    separately and passes the output received through a non-linear function to generate
    output. These non-linear functions are also called as activation functions (we
    will cover them later). You can consider an activation function as a mathematical
    funcation. A neuron can be represented as in Figure 8-3.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-3 A neuron gets the inputs, processes it using mathematical functions
    and then generates the output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/08image007.png)'
  prefs: []
  type: TYPE_IMG
- en: In simpler terms, a neuron can be termed as a mathematical function which computes
    the weighted average of its input datasets, then this sum is passed through activation
    functions. The output of the neuron can then be the input to the next neuron which
    will again process the input received. Let’s go a bit deeper.
  prefs: []
  type: TYPE_NORMAL
- en: In a perceptron, each input value is multiplied by a factor called the *weight*.
    Biological neuron fires once the total strength of the input signals exceed a
    certain threshold. The similar format is followed in a perceptron too. In a perceptron,
    a weighted sum of the inputs is calculated to get the total strength of the input
    data and then an activation function is applied to each of the outputs. Each output
    can then be fed to the next layerof perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s us assume that there are two input values “a” and “b” for a perceptron
    X which for the sake of simplicity has only one output. Let the respective weights
    for a and b are P and Q. So, the weighted sum can be said as : P*x + Q*b. The
    perceptron will only fire or will have a non-zero output, only if the weighted
    sum exceeds a certain threshold. Let’s call the threshold as “C”. So, we can say
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: Output of X will be 0 if P*x + Q*y <= C
  prefs: []
  type: TYPE_NORMAL
- en: Output of X will be 1 if P*x + Q*y > C
  prefs: []
  type: TYPE_NORMAL
- en: If we generalize this understanding we can represent as shown below. If we represent
    a perceptron as a function, which maps input “x” as the function below
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = 1 if w*x + b > 0
  prefs: []
  type: TYPE_NORMAL
- en: 0 otherwise
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: x is vector of input values
  prefs: []
  type: TYPE_NORMAL
- en: w represents the vector of weights
  prefs: []
  type: TYPE_NORMAL
- en: b is the bias term
  prefs: []
  type: TYPE_NORMAL
- en: We will explain the bias and the weight terms now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the linear equation: y = mx+ c where m is the slope of the straight
    line and c is the constant term. Both bias and weight can be defined using the
    same linear equation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weight: the role of weight is similar to the slope of the line in linear equation.
    It defines what is the change in the value of f(x) by a unit change in the value
    of x.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias: the role of the bias is similar to the role of a constant in a linear
    function. In case there is no bias, the input to the activation function is x
    multiplied by the weight.'
  prefs: []
  type: TYPE_NORMAL
- en: Weights and bias terms are the parameters which get trained in a network.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the function will depend on the activation function which is used.
    We will cover various types of activation functions in the next section after
    we have covered different layers in a network.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3 Different layers in a network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simple and effective way of organizing neurons is the following. Rather than
    allowing arbitrary neurons connected with arbitrary others, neurons are organized
    in layers. A neuron in a layer has all its inputs coming *only* from the previous
    layer, and all its output going *only* to the next. There are no other connections,
    for example between neurons of the same layer or between neurons in neurons belonging
    in distant layers (with a small exception for a pretty specialized case which
    is beyond the scope of this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that information flows through a neural network. That information is
    processed and passed on from one layer to another layer in a network. There are
    three layers in a neural network as shown in Figure 8-4\. A typical neural network
    looks like the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-4 A typical neural network with neurons and input, hidden and output
    layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/08image008.png)'
  prefs: []
  type: TYPE_IMG
- en: The neural network shown in Figure 8-4 has 3 input units, 2 hidden layers with
    4 neurons each and one final output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Layer**: as the name signifies it receives the input data and shares
    it with the hidden layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hidden Layer**: it is the heart and soul of the network. The number of hidden
    layers depends on the problem at hand, the number of layers can range from a few
    to hundredsAll the processing, feature extraction, learning of the attributes
    is done in these layers. In the hidden layers, all the input raw data is broken
    into attributes and features. This learning is useful for decision making at a
    later stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output Layer**: the decision layer and final piece in a network. It accepts
    the outputs from the preceding hidden layers and then makes a prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, the input training data will have raw images or processed images.
    These images will be fed to the input layer. The data now travels to the hidden
    layers where all the calculations are done. These calculations are done by neurons
    in each layer. The output is the task that needs to be accomplished for example
    identification of an object or if we want to classify an image etc.
  prefs: []
  type: TYPE_NORMAL
- en: The ANN consists of various connections. Each of the connection aims to receive
    the input and provide the output to the next neuron. This output to the next neuron
    will serve as an input to it. Also, as discussed earlier, each connection is assigned
    a weight which is a representative of its respective importance. It is important
    to note that a neuron can have multiple input and output connections which means
    it can receive inputs and deliver multiple outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  The input data is fed to the hidden layers in a neural network. True or
    False.
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Bias term is similar to the slop of a linear equation. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Find and explore the deepest neural network ever trained.
  prefs: []
  type: TYPE_NORMAL
- en: So, what is the role of a layer? A layer receives inputs, processes them and
    passes the output to the next layer. Technically, it is imperative that the transformation
    implemented by a layer is parameterized by its weights which are also referred
    to as parameters of a layer. To simplify, to make it possible that a neural network
    is "trained" to a specific task, something must be changed in the network. It
    turns out that changing the architecture of the network (i.e. how neurons are
    connected with each other) have only a small effect. On the other hand, as we
    will see later in this chapter, changing the weights is key to the "learning"
    process.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to the very important topic of activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.4 Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall in the last sections, we introduced activation functions. The primary
    role of an activation function is to decide whether a neuron/perceptron should
    fire or not. They play a central role in training of the network at a later stage.
    They are sometimes referred as *Transfer Functions*. It is also important to know
    that why we need non-linear activation functions. If we use only linear activation
    functions, the output will also be linear. At the same time, the derivative of
    a linear function will be constant. Hence, there will not be much learning possible.
    Because of such reasons, we prefer to have non-linear activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: We will study the most common activation functions now.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Sigmoid is a bounded monotonic mathematical function. The Sigmoid is a mathetical
    function which always increase its output value when the input values increases.
    It output value is always between -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: It is a differentiable function with an S-shaped curve and its first derivative
    function is bell-shaped. It has a non-negative derivative function and is defined
    for all real input values. The Sigmoid function is used if the output value of
    a neuron is between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, a sigmoid function is:'
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 8.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: S(x) = 1 = e^x
  prefs: []
  type: TYPE_NORMAL
- en: 1 + e^(-x) e^x +1
  prefs: []
  type: TYPE_NORMAL
- en: Graph of a sigmoid function can be shown in Figure 8-5.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-5 A sigmoid function is shown here. Note the shape of the function
    and the min/max values.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![page9image55589296](images/08image009.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid function finds its applications in complex learning systems. It is usually
    used for binary classification and in the final output layer of the network.
  prefs: []
  type: TYPE_NORMAL
- en: tanh Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In mathematics, Tangent Hyperbolic Function or tanh is a differentiable Hyperbolic
    Function. It is a smooth function and its input values are in the range of -1
    to +1.
  prefs: []
  type: TYPE_NORMAL
- en: A tanh function is written as Equation 8.2.
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 8.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Tanh(x) = ex – e-x
  prefs: []
  type: TYPE_NORMAL
- en: ex + e-x
  prefs: []
  type: TYPE_NORMAL
- en: Graphical representation of tanh is shown in Figure 8-6\. It is a scaled version
    of Sigmoid function and hence a tanh function can be derived from Sigmoid function
    and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-6 A tanh function is shown here which is scaled version of sigmoid
    function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![page10image55837712](images/08image010.png)'
  prefs: []
  type: TYPE_IMG
- en: A tanh function is generally used in the hidden layers. It makes the mean closer
    to zero which makes the training easier for the next layer in the network. This
    is also referred as *Centering the data*.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified Linear Unit or ReLU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rectified Linear Unit or ReLU is an activation function that defines the positives
    of an argument. We are showing the ReLU function below. Note that the value is
    0 even for the negative values and from 0 the value starts to incline.
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 8.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: F(x) = max (0, x)
  prefs: []
  type: TYPE_NORMAL
- en: i.e., will give output as x if positive else 0
  prefs: []
  type: TYPE_NORMAL
- en: A ReLU function is shown as Figure 8-7
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-7 A ReLU function is shown here, it is one of the favored activation
    function in the hidden layers of a neural network. ReLU is simple to use and less
    expensive to train.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![page11image55974400](images/08image011.png)'
  prefs: []
  type: TYPE_IMG
- en: It is a simple function and hence less expensive to compute and much faster.
    It is unbounded and not centred at zero. It is differentiable at all the places
    except zero. Since ReLU function in less complex, is computationally less expensive
    and hence is widely used in the hidden layers to train the networks faster.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The softmax function is used in the final layer of the neural network to generate
    the output from the network. It is the activation functionThe output can be a
    final classification of an image for distinct categories. It is an activation
    function that is useful for multi-class classification problems and forces the
    neural network to output the sum of 1.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, if the number of distinct class for an image are cars, bikes
    or trucks, the softmax function will generate three probabilities for each category.
    The category which has received the highest probability will be the predicted
    category.
  prefs: []
  type: TYPE_NORMAL
- en: There are other activation functions too like ELU, PeLU etc. which are beyond
    the scope of this book. We are providing the summary of various activation functions
    at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will now cover hyperparameters in the next section. They are the control
    levers we have while the network is trained.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.5 Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During training a network, the algorithm is constantly learning the attributes
    of the raw input data. At the same time, the network cannot learn everything itself,
    there are a few parameters which require initial settings to be provided. These
    are the variables that determine the structure of the neural network and the respective
    variables which are useful to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: We provide the hyperparameters before we start the network training. A few examples
    of hyperparameters are number of hidden layers in the network, number of neurons
    in each layer, activation functions used in layer, weight initialization etc.
    We have to pick the best values of the hyperparameters. To do so, weselect some
    reasonable values for the hyperparameters, train the network, then measure the
    performance of the network and then tweak the Hyperparameters and then re-train
    the network, re-evaluate and re- tweak and this process continues.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are controlled by us as we input hyperparameters to improve
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent and Stochastic Gradient Descent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In any prediction-based solution, we would want to predict as best as we can
    or in other words, we wish to reduce the error as much as we can. Error is the
    difference between the actual values and the predicted values. The purpose of
    a Machine Learning solution is to find the most optimum value for our functions.
    We want to decrease the error or maximize the accuracy. Gradient Descent can help
    to achieve this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent technique is an optimization technique used to find the global
    minimaof a function. We proceed in the direction of the steepest descent iteratively
    which is defined by the negative of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: But gradient descent can be slow to run on very large datasets or the datasets
    with a very high number of dimensions. It is due to the fact that one iteration
    of the gradient descent algorithm predicts for every instance in the training
    dataset. Hence, it is obvious that it will take a lot of time if we have thousands
    of records. For such a situation, we have Stochastic Gradient Descent.
  prefs: []
  type: TYPE_NORMAL
- en: In Stochastic Gradient Descent, rather than at the end of the batch of the data,
    the coefficients are updated for each training instance and hence it takes less
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The image below shows the way a Gradient Descent works. Notice how we can progress
    downwards towards the Global Minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-8 The concept of gradient descent. It is the mechanism to minimize
    the loss function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![page14image55583056](images/08image012.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning and Learning Rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a network, we take a various steps to improve the performance of the solution,
    learning rate is one of them. The learning rate will define the size of the corrective
    steps which a model takes to reduce the errors. Learning rate defines the amount
    by which we should adjust the values of weights of the network with respect the
    loss gradients (more on this process will come in the next section). If we have
    a higher learning rate, the accuracy will be lower. If we have a very low learning
    rate, the training time will increase a lot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Compare and contrast between sigmoid and tanh function.
  prefs: []
  type: TYPE_NORMAL
- en: 2.  ReLU is generally used in output layer of the network. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.  Gradient descent is an optimization technique. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: We have now examined the main concepts of Deep Learning. Now let us study how
    a Neural Network works. We will understand how the various layers interact with
    each other and how information is passed from one layer to another.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 How Does Deep Learning Work in a supervised manner?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now covered the major components of a neural network. It is the time
    for all the pieces to come together and orchestrate the entire learning. The training
    of a neural network is quite a complex process. The entire process can be examined
    as below in a step-by-step fashion.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering about what is meant by learning of a neural network.
    Learning is a process to find the best and most optimized values for weights and
    bias for all the layers of the network so that we can achieve the best accuracy.
    As deep neural networks can have practically infinite possibilities for weights
    and bias terms, we have to find the most optimum value for all the parameters.
    This seems like a herculean task considering that changing one value impacts the
    other values and indeed it is process where the various parameters of the networks
    are changing.
  prefs: []
  type: TYPE_NORMAL
- en: Recall in the first chapter we covered the basics of supervised learning. We
    will refresh that understanding here. The reason to refresh supervised learning
    is to ensure that you are fully able to appreciate the process of training the
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Supervised learning algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A quick definition is - supervised learning algorithms have a “guidance” or
    “supervision” to direct toward the business goal of making predictions for the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, supervised models are statistical models which use both the input
    data and the desired output to predict for the future. The output is the value
    which we wish to predict and is referred as the *target variable* and the data
    used to make that prediction is called as *training data*. Target variable is
    sometimes referred as the *label*. The various attributes or variables present
    in the data are called as *independent variables*. Each of the historical data
    point or a *training example* contains these independent variables and corresponding
    target variable. Supervised learning algorithms make a prediction for the unseen
    future data. The accuracy of the solution depends on the training done and patterns
    learned from the labelled historical data.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the deep learning solutions are based on supervised learning. Unsupervised
    deep learning is rapidly gaining traction, however, as unlabelled datasets are
    far more abundant than labelled ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we wish to create a solution which can identify faces of people.
    In this case, we will have:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training data: different images of faces of the people from various angles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Target variable: name of person.'
  prefs: []
  type: TYPE_NORMAL
- en: This training dataset can be fed to the algorithm. The algorithm will then understand
    the attributes of various faces, or in other words, **learn** the various attributes.
    Based on the training done, the algorithm can then make a prediction on the faces.
    The prediction will be a probability score if the face belongs to Mr. X. If the
    probability is high enough, we can safely say that the face belongs to Mr.X.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning problems are used in demand prediction, credit card fraud
    detection, customer churn prediction, premium estimation etc. They are heavily
    used across retail, telecom, banking and finance, aviation, insurance etc.
  prefs: []
  type: TYPE_NORMAL
- en: We have now refreshed the concepts of supervised learning for you. We will now
    move to the first step in the training of the neural network which is feed-forward
    propagation.
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.2 Step 1: feed-forward propagation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us start the the process done in a Neural Network. We have tried to create
    an illustrative diagram in Figure 8-9\. This is the basic skeleton of a network
    we have created to explain the process. Let’s consider we have some input data
    points and we will have the input data layer, which will consume the input data.
    The information is flowing from the input layer to the data transformation layers
    (hidden layers). In the hidden layers, the data is processed using the activation
    functions and based on the weights and bias terms. And then a prediction is made
    on the data set. It is called *feed-forward propagation* as during this process
    the input variables are calculated in a sequence from the input layer to the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the same problem we used to explain the process in a supervised algorithm
    in section 8.3\. For example, if we wish to create a solution which can identify
    faces of people. In this case, we will have the
  prefs: []
  type: TYPE_NORMAL
- en: 'Training data: different images of faces of the people from various angles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Target variable: name of person.'
  prefs: []
  type: TYPE_NORMAL
- en: This training dataset can be fed to the algorithm. The network will then understand
    the attributes of various faces, or in other words, **learn** the various attributes
    of a facial data. Based on the training done, the algorithm can then make a prediction
    on the faces. The prediction will be a probability score if the face belongs to
    Mr. X. If the probability is high enough, we can safely say that the face belongs
    to Mr.X.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-9 The basic skeleton of a neural network training process, we have
    the input layers and data transformation layers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/08image014.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the data processing is done in the hidden layers, a prediction is generated,
    which is the probability if the face belongs to Mr. X.
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.3 Step 2: adding the loss function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output is generated in step 1\. Now we have to gauge the accuracy of this
    network. We want out network to have the best possible accuracy in identifying
    the faces. And using the prediction made by the algorithm, we will control and
    improve the accuracy of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy measurement in the network can be achieved by the loss function, also
    called the *objective function*. The loss function compares the actual values
    and the predicted values. The loss function computes the difference score, and
    hence is able to measure how well the network has done and what are the error
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s update the diagram we created in Step 1 by adding a Loss Function and
    corresponding Loss Score, used to measure the accuracy for the network as shown
    in Figure 8-10.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-10 A loss function has been added to measure the accuracy.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/08image015.png)'
  prefs: []
  type: TYPE_IMG
- en: '8.4.4 Step 3: calculating the error'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We generated the predictions in Step 1 of the network. In step 2, we compared
    the output with the actual values to get the error in prediction. The objective
    of our solution is to minimize this error which means the same as maximizing the
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In order to constantly lower the error, the loss score (Predictions – Actual)
    is then used as a feedback to adjust the value of the weights. This task is done
    by the Backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the step 3 of the last section, we said we use an optimizer to constantly
    update the weights to reduce the error. While the learning rate defines the size
    of the corrective steps to reduce the error, backpropagation is used to adjust
    the connection weights. These weights are updated backward basis the error. Following
    it, the errors are recalculated, gradient descent is calculated and the respective
    weights are adjusted. And hence, back-propagation is sometimes called the central
    algorithm in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The back-propagation was originally suggested in 1970s. Then in 1986, David
    Rumelhartm Geoffrey Hinton and Ronald Williams’s paper got a lot of appreciation.
    In the modern days, back-propagation is the backbone of deep learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The image below shows the process for Backpropagation where the information
    flows from the output layer back to the hidden layers. Note that the flow of information
    is backwards as compared to forward propagation where the information flows from
    left to right. The process for a backpropagation is shown in (Figure 8-11).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-11 Backpropagation as a process- the information flows from the final
    layers to the initial layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![page12image55891232](images/08image016.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we will describe the process at very high level. Remember that in step
    1, at the start of the training process, some random values are assigned to the
    weights. Using these random values, an initial output is generated. Since it is
    the very first attempt, the output received can be quite different from the real
    values and the loss score is accordingly very high. But this is going to improve.
    While training the Neural Network, the weights (and biases) are adjusted a little
    in the correct direction, and subsequently, the loss score decreases. We iterate
    this training loop many times and it results in most optimum weight values that
    minimize the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Back-propagation allows us to iteratively reduce the error during the network
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: The following section is going to be mathematically heavy. If you are not keen
    to understand the mathematics behind the process, you can skip it.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.1 Mathematics behind back-propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To we train a neural network, we calculate a loss function. The loss function
    tells us how different are the predictions from the actual values. Backpropagation
    calculates the gradient of the loss function with respect to the each of the weights.
    And with this information, each weight can be updated individually over iterations
    which reduces the loss gradually.
  prefs: []
  type: TYPE_NORMAL
- en: In back-prop (back-propagation is also called as back-prop), the gradient is
    calculated backwards i.e., from the last layer of the network through the hidden
    layers to the very first layer. The gradients of all the layers are combined using
    calculus chain rule to get the gradient of any particular layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now go into more details of the process. Let’s denote a few mathematical
    symbols first:'
  prefs: []
  type: TYPE_NORMAL
- en: h^(i)) – output of the hidden layer i
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: g^(i))- activation function of hidden layer i
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: w^(i))- hidden weights matrix in the layer i
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b^(i))- the bias in layer i
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: x- the input vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: N – the total number of layers in the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: W^(i))[jk]- weight of the network from node j in layer (i-1) too node k in layer
    i
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '∂A/∂B: it is partial derivative of A with respect to B'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During the training of the network, the input x is fed to the network and it
    passes through the layers to generate an output ŷ. The expected output is y. Hence,
    the cost function or the loss function to compare y and ŷ is going to be C(y,
    ŷ). Also, the output for any hidden layer of the network can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 8.4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: h^((i)) = g^((i)) (W^((i)T) x + b^((i)))
  prefs: []
  type: TYPE_NORMAL
- en: where i (index) can be any layer in the network
  prefs: []
  type: TYPE_NORMAL
- en: 'The final layer’s output is:'
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 8.5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: y(x) = W^((N)T) h^((N-1)) + b^((N))
  prefs: []
  type: TYPE_NORMAL
- en: During the training of the network, we adjust the network’s weights so that
    C is reduced. And hence, we calculate the derivative of C with respect to every
    weight in the network. The following is the derivative of C with respect to every
    weight in the network
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/08image017.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we know that a neural network has many layers. The back-propagation algorithm
    starts at calculating the derivatives at the last layer of the network, which
    is the N^(th) layer. And then these derivatives are few backwards. So, the derivatives
    at the Nth layers will be fed to (N-1) layer of the network and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Each component of the derivatives of C are calculated individually using the
    calculus chain rule.
  prefs: []
  type: TYPE_NORMAL
- en: As per the chain rule, for a function c depending of b, where b depends on a,
    the derivative of c with respect to a can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/08image018.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, in the back-propagation the derivatives of the layer N are used in the
    layer (N-1) so that they are saved and again used in (N-2) layer. We start with
    last layer of the network, through all the layers to the first layer, and each
    of the times, we use the derivatives of the last calculations made to get the
    derivatives of the current layers. And hence, back-prop turns out to be an extremely
    efficient as compared to a normal approach where we would have calculated each
    weight in the network individually.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have calculated the gradients, we would update all the weights in the
    network. The objective being to minimize the cost function. We have already studied
    methods like Gradient Descent in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: We will now continue to the next step in the neural network training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: OPTIMIZATION'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We studied the back propagation in the last section. It allows us to optimize
    our network and achieve the best accuracy. And hence, we have updated the figure
    too in Figure 8-12\. Notice the optimizer which provides regular and continuous
    feedback to reach the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8-12 Optimization is the process to minimize the loss function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/08image019.png)'
  prefs: []
  type: TYPE_IMG
- en: Once we have achieved the best values of the weights and biases for our network,
    we call that our network istrained. We can now use to make predictions on unseen
    dataset which has not been used for training the network.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have understood what the various components of Deep Learning are and
    how they work together in supervised manner. We will now briefly describe the
    unsupervised deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 How deep learning works in unsupervised manner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know that unsupervised learning solutions work on unlabeled datasets. For
    deep learning in unsupervised settings, the training dataset is unlabeled
  prefs: []
  type: TYPE_NORMAL
- en: As compared to supervised datasets where we have tags, unsupervised methods
    have to self-organize themselves to get densities, probabilities distributions,
    preferences and groupings. We can solve a similar problem using supervised and
    unsupervised methods. For example, a supervised deep learning method can be used
    to identify dogs vs cats while an unsupervised deep learning method might be used
    to cluster the pictures of dogs and cats into different groups. It is also observed
    in machine learning that a lot of solutions which were initially conceived as
    supervised learning ones, over the period of time employed unsupervised learning
    methods to enrich the data and hence improve the supervised learning solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the learning phase in unsupervised deep learning, it is expected that
    the network will mimic the data and will then improve itself based on the errors.
    In supervised learning algorithm, there are other methods which play the same
    part as back-prop algorithm. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann learning rule
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Contrastive divergence
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maximum likelihood
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hopfield learning rule
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gibbs sampling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep belief networks and so on
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this book, we are going to cover autoencoders and deep belief networks. We
    are also going to explore GAN (Generative Adversarial Networks). It is time for
    you to now examine all the tools and libraries for Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Write in a simple form, the major steps in a back-prop technique.
  prefs: []
  type: TYPE_NORMAL
- en: 2.  Back-prop is preferred in unsupervised learning. True or False.
  prefs: []
  type: TYPE_NORMAL
- en: 3.  The objective of deep learning is to maximise the loss function. True or
    False.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7 Popular Deep learning libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last few chapters, we have used a lot of libraries and packages for
    implementing solutions. There are quite a few libraries which are available in
    the industry for deep learning. These packages expedite the solution building
    and reduce the efforts as most of the heavy-lifting is done by these libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are discussing the most popular deep learning libraries here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow**: TensorFlow (TF) developed by Google is arguably one of the
    most popular and widely used Deep Learning frameworks. It was launched in 2015
    and since is being used by a number of businesses and brands across the globe.'
  prefs: []
  type: TYPE_NORMAL
- en: Python is mostly used for TF but C++, Java, C#, Javascript, Julia can also be
    used. You have to install TF library on your system and import the library.
  prefs: []
  type: TYPE_NORMAL
- en: Go to [www.tensorflow.org/install](www.tensorflow.org.html) and follow the instructions
    to install TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is one of the most popular libraries and can work on mobile devices
    like iOS and Android too.
  prefs: []
  type: TYPE_NORMAL
- en: '**Keras**: Keras is a mature API driven solution and quite easy to use. It
    is one of the best choices for starters and amongst the best for prototyping simple
    concepts in an easy and fast manner. Keras was initially released in 2015 and
    is one of the most recommended libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://keras.io](.html) and follow the instructions to install Keras.
    Tf.keras can be used as an API.
  prefs: []
  type: TYPE_NORMAL
- en: Serialization/Deserialization APIs, call backs, and data streaming using Python
    generators are very mature. Massive models in Keras are reduced to single-line
    functions which makes it a less configurable environment and hence very convenient
    and easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: '**PyTorch**: Facebook’s brain-child PyTorch was released in 2016 and is another
    popular framework. PyTorch operates with dynamically updated graphs and allows
    data parallelism and distributed learning model. There are debuggers like pdb
    or PyCharm available in PyTorch. For small projects and prototyping, PyTorch can
    be a good choice.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sonnet**: DeepMind’s Sonnet is developed using and on top of TF. Sonnet is
    designed for complex Neural Network applications and architectures. It works by
    creating primary Python objects corresponding to a particular part of the neural
    network. Then these Python objects are independently connected to the computational
    TF graph. Because of this separation (creating Python objects and associating
    them to a graph), the design is simplified.'
  prefs: []
  type: TYPE_NORMAL
- en: Having high-level object-oriented libraries is very helpful as the abstraction
    is allowed when we develop machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**MXNet**: Apache''s MXNet is a highly scalable deep learning tool that is
    easy to use and has detailed documentation. A large number of languages like C
    ++, Python, R, Julia, JavaScript, Scala, Go, Perl are supported by MXNet.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other frameworks too like Swift, Gluon, Chainer, DL4J, etc, however,
    we've only discussed the popular ones here.
  prefs: []
  type: TYPE_NORMAL
- en: We will now examine a short code in tensorflow and keras. It is just to test
    that you have installed these libraries correctly. You can learn more about tensorflow
    at https://www.tensorflow.org and keras at https://keras.io.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7.1 Python code for keras and tensorflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are implementing a very simple code in tensorflow. We simply import the tensor
    flow library and print “hello”. We also check the version of tensorflow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If this code runs for you and prints the version of tensorflow for you it means
    that you have installed `tensorflow` correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If this code runs for you and prints the version of keras, it means that you
    have installed `keras` correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 Closing words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is changing the world we live in. It is enabling us to train and
    create really complex solutions which were a mere thought earlier. The impact
    of deep learning can be witnessed across multiple domains and industries. Perhaps
    there are no industries which have been left unimpacted by the marvels of deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is one of the most-sought after field for research and development.
    Every year there are many journals and papers which are published on deep learning.
    Researchers across the prominent institutions and universities (like Oxford, Stanford
    etc.) of the world are engrossed in finding improved neural network architectures.
    At the same time, professionals and engineers in the reputed organizations (like
    Google, Facebook etc.) are working hard to create sophisticated architectures
    to improve our performance.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is making our systems and machines able to solve problems typically
    assumed to be realm of humans . Humans have improved clinical trials process for
    the pharma sector, we have improved fraud detection softwares, automatic speech
    detection systems, various image recognition solution, more robust natural language
    processing solutions, targeted marketing solutions improving customer relationship
    managements and recommendation systems, better safety processes and so on. The
    list is quite long and growing day-by-day.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, there are still a few challenges. The expectations from deep
    learning continue to increase. Deep learning is not a silver bullet or a magic
    wand to resolve all the issues. It is surely one of the more sophisticated solutions
    but it is certainly not the 100% solution to all the business problems. At the
    same time, the dataset we need to feed the algorithms is not always available.
    There is a dearth of good quality datasets which are representatives of the business
    problem. Often it is observed that, big organizations like Google or Meta or Amazon
    can afford to collect such massive datasets. And many times, we do find a lot
    of quality issues in the data. Having processing power to train these complex
    algorithms is also a challenge. With the advent of cloud computing, though this
    problem has been resolved to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explored the basics of neural networks and deep learning.
    We covered the details around neurons, activation function, different layers of
    a network and loss function. We also covered in detail the back-propagation algorithm
    – the central algorithm used to train a supervised deep learning solution. Then,
    we briefly went through unsupervised deep learning algorithms. We will be covering
    these unsupervised deep learning solutions in a great details in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We are also showing the major activation functions in Figure 8-13.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8-13 Major activation functions at a glance (image: towardsdatascience)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Activation Functions in Neural Networks | by SAGAR SHARMA | Towards Data
    Science](images/08image020.png)'
  prefs: []
  type: TYPE_IMG
- en: You can now move to the practice questions.
  prefs: []
  type: TYPE_NORMAL
- en: Practical next steps and suggested readings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This book Deep Learning with Python by François Chollet is one of the best resources
    to clarify the concepts of deep learning. It covers all the concepts of deep learning
    and neural networks and is written by the creator of Keras.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read the following research papers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distilling the Knowledge in a Neural Network by G.Hinton et al ([https://arxiv.org/pdf/1503.02531.pdf](pdf.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training very deep networks by R. Srivastava et al ([https://arxiv.org/pdf/1503.02531.pdf](pdf.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed Representations of Words and Phrases and their compositionality
    (Word2Vec) by Tomas Mikolov et al ([https://arxiv.org/abs/1310.4546](abs.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generative Adversarial Networks (GANs) by Ian J. Goodfellow et al ([https://arxiv.org/abs/1406.2661](abs.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Residual Learning for Image Recognition (ResNet) by Kaining He et al ([https://arxiv.org/abs/1512.03385](abs.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 8.9 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning and neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Definition of neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of optimization functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various libraries for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
