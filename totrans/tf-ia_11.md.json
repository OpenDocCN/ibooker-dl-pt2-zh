["```py\nimport os\nimport requests\nimport gzip\nimport shutil\n\n# Retrieve the data\nif not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):      ❶\n    url = \n➥ \"http:/ /deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_\n➥ 5.json.gz\"\n    # Get the file from web\n    r = requests.get(url)\n\n    if not os.path.exists('data'):\n        os.mkdir('data')\n\n    # Write to a file\n    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:\n        f.write(r.content)\nelse:                                                                     ❷\n    print(\"The tar file already exists.\")\n\nif not os.path.exists(os.path.join('data', 'Video_Games_5.json')):        ❸\n    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:\n        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\nelse:\n    print(\"The extracted data already exists\")\n```", "```py\n{\"overall\": 5.0, \"verified\": true, \"reviewTime\": \"10 17, 2015\", \n➥ \"reviewerID\": \"xxx\", \"asin\": \"0700026657\", \"reviewerName\": \"xxx\", \n➥ \"reviewText\": \"This game is a bit hard to get the hang of, but when you \n➥ do it's great.\", \"summary\": \"but when you do it's great.\", \n➥ \"unixReviewTime\": 1445040000}\n{\"overall\": 4.0, \"verified\": false, \"reviewTime\": \"07 27, 2015\", \n➥ \"reviewerID\": \"xxx\", \"asin\": \"0700026657\", \"reviewerName\": \"xxx\", \n➥ \"reviewText\": \"I played it a while but it was alright. The steam was a \n➥ bit of trouble. The more they move ... looking forward to anno 2205 I \n➥ really want to play my way to the moon.\", \"summary\": \"But in spite of \n➥ that it was fun, I liked it\", \"unixReviewTime\": 1437955200}\n{\"overall\": 3.0, \"verified\": true, \"reviewTime\": \"02 23, 2015\", \n➥ \"reviewerID\": \"xxx\", \"asin\": \"0700026657\", \"reviewerName\": \"xxx\", \n➥ \"reviewText\": \"ok game.\", \"summary\": \"Three Stars\", \"unixReviewTime\": \n➥ 1424649600}\n```", "```py\nimport pandas as pd\n\n# Read the JSON file\nreview_df = pd.read_json(\n    os.path.join('data', 'Video_Games_5.json'), lines=True, orient='records'\n)\n# Select on the columns we're interested in \nreview_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\nreview_df.head()\n```", "```py\nreview_df = review_df[~review_df[\"reviewText\"].isna()]\nreview_df = review_df[review_df[\"reviewText\"].str.strip().str.len()>0]\n```", "```py\nreview_df[\"verified\"].value_counts()\n```", "```py\nTrue     332504\nFalse    164915\nName: verified, dtype: int64\n```", "```py\nverified_df = review_df.loc[review_df[\"verified\"], :]\n```", "```py\nverified_df[\"overall\"].value_counts()\n```", "```py\n5    222335\n4     54878\n3     27973\n1     15200\n2     12118\nName: overall, dtype: int64\n```", "```py\nverified_df[\"label\"]=verified_df[\"overall\"].map({5:1, 4:1, 3:0, 2:0, 1:0})\n```", "```py\nverified_df[\"label\"].value_counts()\n```", "```py\n1    277213\n0     55291\nName: label, dtype: int64\n```", "```py\nverified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n```", "```py\ninputs, labels = verified_df[\"reviewText\"], verified_df[\"label\"]\n```", "```py\nimport nltk\n\nnltk.download('averaged_perceptron_tagger', download_dir='nltk')\nnltk.download('wordnet', download_dir='nltk')\nnltk.download('omw-1.4', download_dir='nltk')\nnltk.download('stopwords', download_dir='nltk')\nnltk.download('punkt', download_dir='nltk')\nnltk.data.path.append(os.path.abspath('nltk'))\n```", "```py\n    doc = doc.lower()\n```", "```py\nimport re\n\ndoc = re.sub(pattern=r\"\\w+n\\'t \", repl=\"not \", string=doc)\n```", "```py\ndoc = re.sub(pattern=r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", repl=\" \", string=doc)\n```", "```py\n    doc = re.sub(pattern=r\"/d+\", repl=\"\", string=doc)\n```", "```py\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport string\n\nEN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n\n(doc) if w not in EN_STOPWORDS and w not in string.punctuation]  \n```", "```py\nlemmatizer = WordNetLemmatizer()\n```", "```py\npos_tags = nltk.pos_tag(tokens)\n    clean_text = [\n        lemmatizer.lemmatize(w, pos=p[0].lower()) \\\n        if p[0]=='N' or p[0]=='V' else w \\\n        for (w, p) in pos_tags\n    ]\n```", "```py\ndef clean_text(doc):\n    \"\"\" A function that cleans a given document (i.e. a text string)\"\"\"\n\n    doc = doc.lower()                                        ❶\n    doc = doc.replace(\"n\\'t \", ' not ')                      ❷\n    doc = re.sub(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \", doc)    ❸\n    doc = re.sub(r\"/d+\",\"\", doc)                             ❹\n\n    tokens = [\n        w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in \n➥ string.punctuation\n    ]                                                        ❺\n\n    pos_tags = nltk.pos_tag(tokens)                          ❻\n    clean_text = [\n        lemmatizer.lemmatize(w, pos=p[0].lower()) \\          ❼\n        if p[0]=='N' or p[0]=='V' else w \\                   ❼\n        for (w, p) in pos_tags                               ❼\n    ]\n\n    return clean_text\n```", "```py\nsample_doc = 'She sells seashells by the seashore.'\nprint(\"Before clean: {}\".format(sample_doc))\nprint(\"After clean: {}\".format(clean_text(sample_doc)))\n```", "```py\nBefore clean: She sells seashells by the seashore.\nAfter clean: [“sell”, “seashell”, “seashore”]\n```", "```py\ninputs = inputs.apply(lambda x: clean_text(x))\n```", "```py\ninputs.to_pickle(os.path.join('data','sentiment_inputs.pkl'))\nlabels.to_pickle(os.path.join('data','sentiment_labels.pkl'))\n```", "```py\nneg_indices = pd.Series(labels.loc[(labels==0)].index)\npos_indices = pd.Series(labels.loc[(labels==1)].index)\n```", "```py\nn_valid = int(\n    min([len(neg_indices), len(pos_indices)]) * ((1-train_fraction)/2.0)\n)\n```", "```py\nn_test = n_valid\n```", "```py\nneg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\nneg_valid_inds = neg_indices.loc[\n    ~neg_indices.isin(neg_test_inds)\n].sample(n=n_test, random_state=random_seed)\nneg_train_inds = neg_indices.loc[\n    ~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())\n]\n\npos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed\n)\npos_valid_inds = pos_indices.loc[\n    ~pos_indices.isin(pos_test_inds)\n].sample(n=n_test, random_state=random_seed)\npos_train_inds = pos_indices.loc[        \n    ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())\n]\n```", "```py\ntr_x = inputs.loc[\n    neg_train_inds.tolist() + pos_train_inds.tolist()\n].sample(frac=1.0, random_state=random_seed)\ntr_y = labels.loc[\n    neg_train_inds.tolist() + pos_train_inds.tolist()\n].sample(frac=1.0, random_state=random_seed)\n\nv_x = inputs.loc[\n    neg_valid_inds.tolist() + pos_valid_inds.tolist()\n].sample(frac=1.0, random_state=random_seed)\nv_y = labels.loc[\n    neg_valid_inds.tolist() + pos_valid_inds.tolist()\n].sample(frac=1.0, random_state=random_seed)\n\nts_x = inputs.loc[\n    neg_test_inds.tolist() + pos_test_inds.tolist()\n].sample(frac=1.0, random_state=random_seed)\nts_y = labels.loc[\n    neg_test_inds.tolist() + pos_test_inds.tolist()\n].sample(frac=1.0, random_state=random_seed)\n```", "```py\ndef train_valid_test_split(inputs, labels, train_fraction=0.8):\n    \"\"\" Splits a given dataset into three sets; training, validation and test \"\"\"\n\n    neg_indices = pd.Series(labels.loc[(labels==0)].index)                   ❶\n    pos_indices = pd.Series(labels.loc[(labels==1)].index)                   ❶\n\n    n_valid = int(min([len(neg_indices), len(pos_indices)]) \n       * ((1-train_fraction)/2.0))                                           ❷\n    n_test = n_valid                                                         ❷\n\n    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)   ❸\n    neg_valid_inds = neg_indices.loc[~neg_indices.isin(\n       neg_test_inds)].sample(n=n_test, random_state=random_seed)            ❹\n    neg_train_inds = neg_indices.loc[~neg_indices.isin(\n        neg_test_inds.tolist()+neg_valid_inds.tolist())]                     ❺\n\n    pos_test_inds = pos_indices.sample(n=n_test)                             ❻\n    pos_valid_inds = pos_indices.loc[\n        ~pos_indices.isin(pos_test_inds)].sample(n=n_test)                   ❻\n    pos_train_inds = pos_indices.loc[\n        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())    ❻\n    ]\n\n    tr_x = inputs.loc[neg_train_inds.tolist() + \n➥ pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)       ❼\n    tr_y = labels.loc[neg_train_inds.tolist() + \n➥ pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)       ❼\n    v_x = inputs.loc[neg_valid_inds.tolist() + \n➥ pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)       ❼\n    v_y = labels.loc[neg_valid_inds.tolist() + \n➥ pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)       ❼\n    ts_x = inputs.loc[neg_test_inds.tolist() + \n➥ pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)        ❼\n    ts_y = labels.loc[neg_test_inds.tolist() + \n➥ pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)        ❼\n\n    print('Training data: {}'.format(len(tr_x)))\n    print('Validation data: {}'.format(len(v_x)))\n    print('Test data: {}'.format(len(ts_x)))\n\n    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n```", "```py\n(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(data, labels)\n```", "```py\ndata_list = [w for doc in tr_x for w in doc]\n```", "```py\nfrom collections import Counter\ncnt = Counter(data_list)\n```", "```py\nfreq_df = pd.Series(\n    list(cnt.values()), \n    index=list(cnt.keys())\n).sort_values(ascending=False)\n\nprint(freq_df.head(n=10))\n```", "```py\ngame     407818\nnot      248244\nplay     128235\n's       127844\nget      108819\nlike     100279\ngreat     97041\none       89948\ngood      77212\ntime      63450\ndtype: int64\n```", "```py\nprint(freq_df.describe())\n```", "```py\ncount    133714.000000\nmean         75.768207\nstd        1754.508881\nmin           1.000000\n25%           1.000000\n50%           1.000000\n75%           4.000000\nmax      408819.000000\ndtype: float64\n```", "```py\nn_vocab = (freq_df >= 25).sum()\n```", "```py\nseq_length_ser = tr_x.str.len()\n```", "```py\np_10 = seq_length_ser.quantile(0.1)\np_90 = seq_length_ser.quantile(0.9)\n```", "```py\nseq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66])\n```", "```py\ncount    278675.000000\nmean         15.422596\nstd          16.258732\nmin           1.000000\n33%           5.000000\n50%          10.000000\n66%          16.000000\nmax          74.000000\nName: reviewText, dtype: float64\n```", "```py\nthe cat sat on the mat\n```", "```py\n[the, cat, sat, on, the, mat]\n```", "```py\n{the: 1, cat: 2, sat: 3, on: 4, mat: 5}\n```", "```py\n[1,2,3,4,1,5]\n```", "```py\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(\n    num_words=n_vocab, \n    oov_token='unk', \n    lower=False, \n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    split=' ', \n    char_level=False\n)\n```", "```py\n122143    [work, perfectly, wii, gamecube, issue, compat...\n444818    [loved, game, collectible, come, well, make, m...\n79331     ['s, okay, game, honest, bad, type, game, --, ...\n97250                        [excellent, product, describe]\n324411        [level, detail, great, feel, love, car, game]\n...\n34481     [not, actually, believe, write, review, produc...\n258474    [good, game, us, like, movie, franchise, hard,...\n466203    [fun, first, person, shooter, nice, combinatio...\n414288                       [love, amiibo, classic, color]\n162670    [fan, halo, series, start, enjoy, game, overal...\nName: reviewText, dtype: object\n```", "```py\n122143    1\n444818    1\n79331     0\n97250     1\n324411    1\n...\n34481     1\n258474    1\n466203    1\n414288    1\n162670    0\nName: label, dtype: int64\n```", "```py\ntokenizer.fit_on_texts(tr_x.tolist())\n```", "```py\ntokenizer.word_index[“game”]\n```", "```py\n2\n```", "```py\ntokenizer.index_word[4]\n```", "```py\n“play”\n```", "```py\ntr_x = tokenizer.texts_to_sequences(tr_x.tolist())\nv_x = tokenizer.texts_to_sequences(v_x.tolist())\nts_x = tokenizer.texts_to_sequences(ts_x.tolist())\n```", "```py\nText: ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', \n➥ 'loss', 'memory']\nSequence: [14, 295, 83, 572, 121, 1974, 2223, 345]\nText: ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', \n➥ 'big', 'almost', 'fit', 'face', 'impressive']\nSequence: [1592, 2, 2031, 32, 23, 16, 2345, 153, 200, 155, 599, 1133]\n\nText: [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", \n➥ 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', \n➥ 'enjoy', 'game']\nSequence: [5, 574, 2, 1264, 105, 197, 2, 112, 5, 274, 150, 354, 1, 290, \n➥ 400, 19, 67, 2]\n\nText: ['excellent', 'product', 'describe']\nSequence: [109, 55, 501]\n\nText: ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\nSequence: [60, 419, 8, 42, 13, 265, 2]\n```", "```py\n    data_seq = [[b]+a for a,b in zip(text_seq, labels) ]\n```", "```py\n[\n  [1,2],\n  [3,2,5,9,10],\n  [3,2,3]\n]\n```", "```py\nmax_length = 50    \ntf_data = tf.ragged.constant(data_seq)[:,:max_length]\n```", "```py\na = tf.ragged.constant([[1, 2, 3], [1,2], [1]])\n```", "```py\nb = tf.RaggedTensor.from_row_splits([1,2,3,4,5,6,7], row_splits=[0, 3, 3, 6, 7])\n```", "```py\n<tf.RaggedTensor [[1, 2, 3], [], [4, 5, 6], [7]]>\n```", "```py\n[4, None]\n```", "```py\nc = tf.RaggedTensor.from_nested_row_splits(\n    flat_values=[1,2,3,4,5,6,7,8,9], \n    nested_row_splits=([0,2,3],[0,4,6,9]))\n```", "```py\n<tf.RaggedTensor [[[1, 2, 3, 4], [5, 6]], [[7, 8, 9]]]>\n```", "```py\nprint(c[:1, :, :])\n```", "```py\n<tf.RaggedTensor [[[1, 2, 3, 4], [5, 6]]]>\n```", "```py\nprint(c[:,:1,:])\n```", "```py\n<tf.RaggedTensor [[[1, 2, 3, 4]], [[7, 8, 9]]]>\n```", "```py\nprint(c[:, :, :2])\n```", "```py\n<tf.RaggedTensor [[[1, 2], [5, 6]], [[7, 8]]]>\n```", "```py\n    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)\n```", "```py\ntext_ds = text_ds.filter(lambda x: tf.size(x)>1)\n```", "```py\n[r1, r3, r4]\n[r2, r5, r6]\n[r7, r8, r9]\n```", "```py\n[[0,11), [11, 21), [21, inf)) \n```", "```py\n[10, 12, 48, 21,  5]\n[ 1, 93, 28,  8, 20, 10]\n[32, 20,  1,  2]\n```", "```py\n[10, 12, 48, 21,  5,  0]\n[ 1, 93, 28,  8, 20, 10]\n[32, 20,  1,  2,  0,  0]\n```", "```py\nbucket_boundaries=[5,15]\nbatch_size = 64\nbucket_fn = tf.data.experimental.bucket_by_sequence_length(\n        element_length_func = lambda x: tf.cast(tf.shape(x)[0],'int32'), \n        bucket_boundaries=bucket_boundaries, \n        bucket_batch_sizes=[batch_size,batch_size,batch_size], \n        padding_values=0, \n        pad_to_bucket_boundary=False\n    )\n```", "```py\ntext_ds = text_ds.map(lambda x: x).apply(bucket_fn)\n```", "```py\nif shuffle:\n    text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n```", "```py\ntext_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))    \n```", "```py\ndef get_tf_pipeline(\n    text_seq, labels, batch_size=64, bucket_boundaries=[5,15], \n➥ max_length=50, shuffle=False\n):\n    \"\"\" Define a data pipeline that converts sequences to batches of data \"\"\"\n\n    data_seq = [[b]+a for a,b in zip(text_seq, labels) ]               ❶\n\n    tf_data = tf.ragged.constant(data_seq)[:,:max_length]              ❷\n\n    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)              ❸\n\n    bucket_fn = tf.data.experimental.bucket_by_sequence_length(        ❹\n        lambda x: tf.cast(tf.shape(x)[0],'int32'), \n        bucket_boundaries=bucket_boundaries,                           ❺\n        bucket_batch_sizes=[batch_size,batch_size,batch_size], \n        padded_shapes=None,\n        padding_values=0, \n        pad_to_bucket_boundary=False\n    )\n\n    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)                ❻\n\n    if shuffle:\n        text_ds = text_ds.shuffle(buffer_size=10*batch_size)           ❼\n\n    text_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))                 ❽\n\n    return text_ds\n```", "```py\nimport tensorflow as tf\ntf.keras.layers.LSTM(units=128, return_state=False, return_sequences=False)\n```", "```py\ntf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=(None,)),\n```", "```py\ntf.keras.layers.Masking(mask_value=0)\n```", "```py\nclass OnehotEncoder(tf.keras.layers.Layer):\n    def __init__(self, depth, **kwargs):\n        super(OnehotEncoder, self).__init__(**kwargs)\n        self.depth = depth\n\n    def build(self, input_shape):\n        pass\n\n    def call(self, inputs):        \n\n        inputs = tf.cast(inputs, 'int32')\n\n        if len(inputs.shape) == 3:\n            inputs = inputs[:,:,0]\n\n        return tf.one_hot(inputs, depth=self.depth)\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({'depth': self.depth})\n        return config\n```", "```py\nOnehotEncoder(depth=n_vocab),\n```", "```py\ntf.keras.layers.LSTM(units=128, return_state=False, return_sequences=False)\n```", "```py\ntf.keras.layers.Dense(512, activation='relu'),\n```", "```py\ntf.keras.layers.Dropout(0.5)\n```", "```py\ntf.keras.layers.Dense(1, activation='sigmoid')\n```", "```py\nmodel = tf.keras.models.Sequential([\n\n    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,1)),         ❶\n    OnehotEncoder(depth=n_vocab),                                          ❷\n    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False), ❸\n    tf.keras.layers.Dense(512, activation='relu'),                         ❹\n    tf.keras.layers.Dropout(0.5),                                          ❺\n    tf.keras.layers.Dense(1, activation='sigmoid')                         ❻\n])\n```", "```py\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n```", "```py\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmasking (Masking)            (None, None)              0         \n_________________________________________________________________\nlambda (Lambda)              (None, None, 11865)       0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 128)               6140928   \n_________________________________________________________________\ndense (Dense)                (None, 512)               66048     \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 513       \n=================================================================\nTotal params: 6,207,489\nTrainable params: 6,207,489\nNon-trainable params: 0\n_________________________________________________________________\n```", "```py\n# Using a batch size of 128\nbatch_size =128\n\ntrain_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\nvalid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size)\n```", "```py\nneg_weight = (tr_y==1).sum()/(tr_y==0).sum()\n```", "```py\nmodel.fit(\n    x=train_ds, \n    validation_data=valid_ds, \n    epochs=10, \n    class_weight={0:neg_weight, 1:1.0}\n)\n```", "```py\nos.makedirs('eval', exist_ok=True)\n\ncsv_logger = tf.keras.callbacks.CSVLogger(\n       os.path.join('eval','1_sentiment_analysis.log'))                       ❶\n\nmonitor_metric = 'val_loss'\nmode = 'min'\nprint(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8    ❷\n) \n\nes_callback = tf.keras.callbacks.EarlyStopping(\n    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False ❸\n)\n\nmodel.fit(\n    train_ds,                                                                 ❹\n    validation_data=valid_ds, \n    epochs=10, \n    class_weight={0:neg_weight, 1:1.0}, \n    callbacks=[es_callback, lr_callback, csv_logger])\n```", "```py\nUsing metric=val_loss and mode=min for EarlyStopping\nEpoch 1/10\n2427/2427 [==============================] - 72s 30ms/step - loss: 0.7640 - accuracy: 0.7976 - val_loss: 0.4061 - val_accuracy: 0.8193 - lr: 0.0010\n\n...\n\nEpoch 7/10\n2427/2427 [==============================] - 73s 30ms/step - loss: 0.2752 - accuracy: 0.9393 - val_loss: 0.7474 - val_accuracy: 0.8026 - lr: 1.0000e-04\nEpoch 8/10\n2427/2427 [==============================] - 74s 30ms/step - loss: 0.2576 - accuracy: 0.9439 - val_loss: 0.8398 - val_accuracy: 0.8041 - lr: 1.0000e-04\n```", "```py\nos.makedirs('models', exist_ok=True)\ntf.keras.models.save_model(model, os.path.join('models', '1_sentiment_analysis.h5'))\n```", "```py\ntest_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n```", "```py\nmodel.evaluate(test_ds)\n```", "```py\n87/87 [==============================] - 2s 27ms/step - loss: 0.8678 - accuracy: 0.8038\n```", "```py\nmodel = tf.keras.models.Sequential([\n    # Create a mask to mask out zero inputs\n    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,1)),\n    # After creating the mask, convert inputs to onehot encoded inputs\n    OnehotEncoder(depth=n_vocab),\n    # Defining an LSTM layer\n    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n    # Defining a Dense layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n```", "```py\nmodel = tf.keras.models.Sequential([                                               ❶\n\n    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, mask_zero=True),❷\n\n    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),         ❸\n    tf.keras.layers.Dense(512, activation='relu'),                                 ❹\n    tf.keras.layers.Dropout(0.5),                                                  ❺\n    tf.keras.layers.Dense(1, activation='sigmoid')                                 ❻\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  ❼\nmodel.summary()\n```", "```py\nUnknownError:  [_Derived_]  CUDNN_STATUS_BAD_PARAM\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1496): \n➥ 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, \n➥ max_seq_length, batch_size, data_size, seq_lengths_array, \n➥ (void*)&padding_fill)'\n         [[{{node cond_38/then/_0/CudnnRNNV3}}]]\n         [[sequential/lstm/StatefulPartitionedCall]]\n         [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_42]] \n➥ [Op:__inference_train_function_8225]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n```", "```py\nEpoch 1/25\n2427/2427 [==============================] - 30s 12ms/step - loss: 0.7552 - \n➥ accuracy: 0.7949 - val_loss: 0.3942 - val_accuracy: 0.8277 - lr: 0.0010\nEpoch 2/25\n\n...\n\nEpoch 8/25\n2427/2427 [==============================] - 29s 12ms/step - loss: 0.3059 - \n➥ accuracy: 0.9312 - val_loss: 0.6839 - val_accuracy: 0.8130 - lr: 1.0000e-04\n```", "```py\ntest_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\nmodel.evaluate(test_ds)\n```", "```py\n87/87 [==============================] - 0s 5ms/step - loss: 0.7214 - accuracy: 0.8111\n```", "```py\ntest_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n```", "```py\ntest_x = []\ntest_pred = []\ntest_y = []\nfor x, y in test_ds:\n    test_x.append(x)    \n    test_pred.append(model.predict(x))\n    test_y.append(y)\ntest_x = [doc for t in test_x for doc in t.numpy().tolist()]\ntest_pred = tf.concat(test_pred, axis=0).numpy()\ntest_y = tf.concat(test_y, axis=0).numpy()\n```", "```py\nsorted_pred = np.argsort(test_pred.flatten())\nmin_pred = sorted_pred[:5]\nmax_pred = sorted_pred[-5:]\n\nprint(\"Most negative reviews\\n\")\nprint(\"=\"*50)\nfor i in min_pred:    \n    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n\nprint(\"\\nMost positive reviews\\n\")\nprint(\"=\"*50)\nfor i in max_pred:\n    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n```", "```py\nMost negative reviews\n==================================================\nbuy game high rating promise gameplay saw youtube story so-so graphic \n➥ mediocre control terrible could not adjust control option preference ...\n\nattempt install game quad core windows 7 pc zero luck go back forth try \n➥ every suggestion rockstar support absolutely useless game ... \n\nway product 5 star 28 review write tone lot review similar play 2 song \n➥ expert drum say unless play tennis shoe fact screw not flush mean feel \n➥ every kick specifically two screw leave plus pedal completely torn \n➥ mount screw something actually go wrong pedal instal unscrew send back \n➥ ea \n\nunk interactive stranger unk unk genre develop operation flashpoint various \n➥ real-life computer sims military folk unk know come deliver good \n➥ recreation ultra deadly unk modern combat engagement arma attempt \n➥ simulate `` unk firepower '' soldier combine arm warfare set fictional \n➥ sprawl island nation conveniently mirror terrain middle eastern country \n\nnot cup tea \n\nMost positive reviews\n==================================================\nfind something love every final fantasy game play thus far ff13 different \n➥ really appreciate square enix 's latest trend shake gameplay new \n➥ release still hammer best look ... \nknow little squad base game genre know game fun not unk fun obliterate \n➥ enemy mobile suit satisfy blow zombie head resident evil graphic \n➥ presentation solid best franchise yes ... \n\nokay hdtv monitor cause ps3 switch game movie widescreen fullscreen every 5 \n➥ minute someone tell need get hd component cable look price saw brand \n➥ name sony much money basically name brand pay fancy retail packaging \n➥ generic cable get quality without fancy packaging name brand embed \n➥ cable favor save money \n\nabsolutely phenomenal gaming mouse love programmable size button mouse \n➥ surprising ergonomic design ... \n\nfirst motorstorm come unk racing type one pioneer physic base race every \n➥ track lot branch path every branch suitable different class vehicle \n➥ take next level race much bigger apart mud also obstacles individual \n➥ vehicle class small vehicle get stuck plant unk hazard time lot physic \n➥ people complain vehicle slide \n```", "```py\ns = s.replace(\"-\", ' ')\n   tokens = word_tokenize(s)                                                             \npos_tags = nltk.pos_tag(tokens)\nclean_text = [\n        lemmatizer.lemmatize(w, pos=p[0].lower()) if p[0]=='V' else w\n        for (w, p) in pos_tags\n]\n```", "```py\ns = “a_b_B_c_d_a_D_b_d_d”\ntok = Tokenizer(num_words = 3, split=”_”, lower=True)\ntok.fit_on_texts([s])\n\nMost common words get the lowest word ID (starting from 1). \n➥ tok.texts_to_sequences([s]) will produce [[3,2,2,1,3,1,2,1,1]]\n```", "```py\nbucket_fn = tf.data.experimental.bucket_by_sequence_length(\n        lambda x: tf.cast(tf.shape(x)[0],'int32'), \n        bucket_boundaries=[10, 25, 30], \n        bucket_batch_sizes=[batch_size,batch_size,batch_size, batch_size], \n        padded_shapes=None,\n        padding_values=0, \n        pad_to_bucket_boundary=True\n    )\n```", "```py\ninp = tf.keras.layers.Input(shape=(None, 30))\nlstm_out = tf.keras.layers.LSTM(32, return_sequences=True)(inp)\nsum_out = tf.keras.layers.Add(axis=1)(lstm_out)\ndense_out = tf.keras.layers.Dense(10, activation=’softmax’)(sum_out)\n```", "```py\nA - (25+50)/(10+25+50)\nB - (10+50)/(10+25+50)\nC - (10+25)/(10+25+50)\n```", "```py\ntf.keras.models.Sequential(\n[\n    tf.keras.layers.Embedding(input_dim=500, output_dim=32, input_shape=(500,)),\n    tf.keras.layers.Dense(500, activation=’softmax’)\n])\n```"]