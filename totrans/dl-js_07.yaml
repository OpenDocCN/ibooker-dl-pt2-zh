- en: Chapter 1\. Deep learning and JavaScript
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章。深度学习和 JavaScript
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容*'
- en: What deep learning is and how it is related to artificial intelligence (AI)
    and machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是什么以及它与人工智能（AI）和机器学习的关系
- en: What makes deep learning stand out among various machine-learning techniques,
    and the factors that led to the current “deep-learning revolution”
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使深度学习在各种机器学习技术中脱颖而出的因素，以及导致当前“深度学习革命”的因素
- en: The reasons for doing deep learning in JavaScript using TensorFlow.js
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow.js 进行 JavaScript 中深度学习的原因
- en: The overall organization of this book
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书的总体组织
- en: 'All the buzz around artificial intelligence (AI) is happening for a good reason:
    the deep-learning revolution, as it is sometimes called, has indeed happened.
    *Deep-learning revolution* refers to the rapid progress made in the speed and
    techniques of deep neural networks that started around 2012 and is still ongoing.
    Since then, deep neural networks have been applied to an increasingly wide range
    of problems, enabling machines to solve previously unsolvable problems in some
    cases and dramatically improving solution accuracy in others (see [table 1.1](#ch01table01)
    for examples). To experts in AI, many of these breakthroughs in neural networks
    were stunning. To engineers who use neural networks, the opportunities this progress
    has created are galvanizing.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）周围的热议完全有其原因：即所谓的深度学习革命确实已经发生。 *深度学习革命* 指的是从2012年开始并持续至今的深度神经网络速度和技术上的迅速进步。自那时以来，深度神经网络已被应用到越来越广泛的问题中，在某些情况下使得机器能够解决以前无法解决的问题，并在其他情况下显著提高了解决方案的准确性（有关示例，请参见
    [表1.1](#ch01table01)）。 对于AI专家来说，神经网络在许多方面的突破是令人震惊的。对于使用神经网络的工程师来说，这一进步所带来的机遇是令人振奋的。
- en: Table 1.1\. Examples of tasks in which accuracy improved significantly thanks
    to deep-learning techniques since the beginning of the deep-learning revolution
    around 2012\. This list is by no means comprehensive. The pace of progress will
    undoubtedly continue in the coming months and years.
  id: totrans-7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.1。自2012年深度学习革命开始以来，由于深度学习技术的显著改进而导致准确性显著提高的任务示例。这个列表并不全面。在未来的几个月和年份中，进展的速度无疑将继续。
- en: '| Machine-learning task | Representative deep-learning technology | Where we
    use TensorFlow.js to perform a similar task in this book |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习任务 | 代表性深度学习技术 | 我们在本书中使用 TensorFlow.js 执行类似任务的地方 |'
- en: '| --- | --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Categorizing the content of images | Deep convolutional neural networks (convnets)
    such as ResNet^([[a](#ch01table01fn01)]) and Inception^([[b](#ch01table01fn02)])
    reduced the error rate in the ImageNet classification task from ~25% in 2011 to
    below 5% in 2017.^([[c](#ch01table01fn03)]) | Training convnets for MNIST ([chapter
    4](kindle_split_015.html#ch04)); MobileNet inference and transfer learning ([chapter
    5](kindle_split_016.html#ch05)) |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: 图像内容分类 | 深度卷积神经网络（卷积网络）如 ResNet^([[a](#ch01table01fn01)]) 和 Inception^([[b](#ch01table01fn02)])
    将 ImageNet 分类任务的错误率从2011年的约25%降至2017年的不到5%。^([[c](#ch01table01fn03)]) | 为 MNIST
    训练卷积网络（[第4章](kindle_split_015.html#ch04)）；MobileNet 推断和迁移学习（[第5章](kindle_split_016.html#ch05))
    |
- en: '| Localizing objects and images | Variants of deep convnets^([[d](#ch01table01fn04)])
    reduced localization error from 0.33 in 2012 to 0.06 in 2017. | YOLO in TensorFlow.js
    ([section 5.2](kindle_split_016.html#ch05lev1sec2)) |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: 本地化对象和图像 | 深度卷积网络的变体^([[d](#ch01table01fn04)])将2012年的定位误差从0.33减少到2017年的0.06。
    | 在 TensorFlow.js 中使用 YOLO ([section 5.2](kindle_split_016.html#ch05lev1sec2))
    |
- en: '| Translating one natural language to another | Google’s neural machine translation
    (GNMT) reduced translation error by ~60% compared to the best traditional machine-translation
    techniques.^([[e](#ch01table01fn05)]) | Long Short-Term Memory (LSTM)-based sequence-to-sequence
    models with attention mechanisms ([chapter 9](kindle_split_021.html#ch09)) |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 将一种自然语言翻译成另一种自然语言 | Google 的神经机器翻译（GNMT）相比于最佳传统机器翻译技术减少了约60%的翻译错误。^([[e](#ch01table01fn05)])
    | 基于长短期记忆（LSTM）的序列到序列模型与注意力机制（[第9章](kindle_split_021.html#ch09)) |'
- en: '| Recognizing large-vocabulary, continuous speech | An LSTM-based encoder-attention-decoder
    architecture achieves a lower word-error rate than the best non-deep-learning
    speech recognition system.^([[f](#ch01table01fn06)]) | Attention-based LSTM small-vocabulary
    continuous speech recognition ([chapter 9](kindle_split_021.html#ch09)) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: 大词汇量连续语音识别 | 基于 LSTM 的编码器-注意力-解码器架构比最佳非深度学习语音识别系统具有更低的词错误率。^([[f](#ch01table01fn06)])
    | 基于注意力的 LSTM 小词汇量连续语音识别（[第9章](kindle_split_021.html#ch09)) |
- en: '| Generating realistic-looking images | Generative adversarial networks (GANs)
    are now capable of generating realistic-looking images based on training data
    (see [https://github.com/junyanz/CycleGAN](https://github.com/junyanz/CycleGAN)).
    | Generating images using variational autoencoders (VAEs) and GANs ([chapter 9](kindle_split_021.html#ch09))
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 生成逼真图像 | 生成对抗网络（GANs）现在能够根据训练数据生成逼真的图像（参见[https://github.com/junyanz/CycleGAN](https://github.com/junyanz/CycleGAN)）。
    | 使用变分自编码器（VAEs）和GANs生成图像（[第9章](kindle_split_021.html#ch09)） |'
- en: '| Generating music | Recurrent neural networks (RNNs) and VAEs are helping
    create music scores and novel instrument sounds (see [https://magenta.tensorflow.org/demos](https://magenta.tensorflow.org/demos)).
    | Training LSTMs to generate text ([chapter 9](kindle_split_021.html#ch09)) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 生成音乐 | 循环神经网络（RNNs）和变分自编码器（VAEs）正在帮助创作音乐乐谱和新颖的乐器声音（参见[https://magenta.tensorflow.org/demos](https://magenta.tensorflow.org/demos)）。
    | 训练LSTMs生成文本（[第9章](kindle_split_021.html#ch09)） |'
- en: '| Learning to play games | Deep learning combined with reinforcement learning
    (RL) lets machines learn to play simple Atari games using raw pixels as the only
    input.^([[g](#ch01table01fn07)]) Combining deep learning and Monte Carlo tree
    search, Alpha-Zero reached a super-human level of Go purely through self-play.^([[h](#ch01table01fn08)])
    | Using RL to solve the cart-pole control problem and a snake video game ([chapter
    11](kindle_split_023.html#ch11)) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 学习玩游戏 | 深度学习结合强化学习（RL）使机器能够学习使用原始像素作为唯一输入来玩简单的雅达利游戏。^([[g](#ch01table01fn07)])
    结合深度学习和蒙特卡洛树搜索，Alpha-Zero纯粹通过自我对弈达到了超人类水平的围棋水平。^([[h](#ch01table01fn08)]) | 使用RL解决杆-极控制问题和一个贪吃蛇视频游戏（[第11章](kindle_split_023.html#ch11)）
    |'
- en: '| Diagnosing diseases using medical images | Deep convnets were able to achieve
    specificity and sensitivity comparable to trained human ophthalmologists in diagnosing
    diabetic retinopathy based on images of patients’ retinas.^([[i](#ch01table01fn09)])
    | Transfer learning using a pretrained MobileNet image model ([chapter 5](kindle_split_016.html#ch05)).
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 使用医学图像诊断疾病 | 深度卷积网络能够根据患者视网膜图像诊断糖尿病视网膜病变，其特异性和敏感性与训练有素的人类眼科医生相当。^([[i](#ch01table01fn09)])
    | 使用预训练的MobileNet图像模型进行迁移学习（[第5章](kindle_split_016.html#ch05)）。'
- en: ^a
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kaiming He et al., “Deep Residual Learning for Image Recognition,” *Proc. IEEE
    Conference Computer Vision and Pattern Recognition* (CVPR), 2016, pp. 770–778,
    [http://mng.bz/PO5P](http://mng.bz/PO5P).
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kaiming He等人，“深度残差学习用于图像识别”，*IEEE计算机视觉与模式识别会议* (CVPR)论文集，2016年，第770–778页，[http://mng.bz/PO5P](http://mng.bz/PO5P)。
- en: ^b
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^b
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Christian Szegedy et al., “Going Deeper with Convolutions,” *Proc. IEEE Conference
    Computer Vision and Pattern Recognition* (CVPR), 2015, pp. 1–9, [http://mng.bz/JzGv](http://mng.bz/JzGv).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Christian Szegedy等人，“使用卷积进一步深入”，*IEEE计算机视觉与模式识别会议* (CVPR)论文集，2015年，第1–9页，[http://mng.bz/JzGv](http://mng.bz/JzGv)。
- en: ^c
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^c
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large Scale Visual Recognition Challenge 2017 (ILSVRC2017) results, [http://image-net.org/challenges/LSVRC/2017/results](http://image-net.org/challenges/LSVRC/2017/results).
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2017年大规模视觉识别挑战（ILSVRC2017）结果，[http://image-net.org/challenges/LSVRC/2017/results](http://image-net.org/challenges/LSVRC/2017/results)。
- en: ^d
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^d
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yunpeng Chen et al., “Dual Path Networks,” [https://arxiv.org/pdf/1707.01629.pdf](https://arxiv.org/pdf/1707.01629.pdf).
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Yunpeng Chen等人，“双路径网络”，[https://arxiv.org/pdf/1707.01629.pdf](https://arxiv.org/pdf/1707.01629.pdf)。
- en: ^e
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^e
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Yonghui Wu et al., “Google’s Neural Machine Translation System: Bridging the
    Gap between Human and Machine Translation,” submitted 26 Sept. 2016, [https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144).'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Yonghui Wu等人，“谷歌的神经机器翻译系统：弥合人机翻译差距”，提交于2016年9月26日，[https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144)。
- en: ^f
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^f
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chung-Cheng Chiu et al., “State-of-the-Art Speech Recognition with Sequence-to-Sequence
    Models,” submitted 5 Dec. 2017, [https://arxiv.org/abs/1712.01769](https://arxiv.org/abs/1712.01769).
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Chung-Cheng Chiu等人，“基于序列到序列模型的最新语音识别技术”，提交于2017年12月5日，[https://arxiv.org/abs/1712.01769](https://arxiv.org/abs/1712.01769)。
- en: ^g
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^g
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Volodymyr Mnih et al., “Playing Atari with Deep Reinforcement Learning,” NIPS
    Deep Learning Workshop 2013, [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602).
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Volodymyr Mnih等人，“使用深度强化学习玩雅达利游戏”，2013年NIPS深度学习研讨会，[https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)。
- en: ^h
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^h
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: David Silver et al., “Mastering Chess and Shogi by Self-Play with a General
    Reinforcement Learning Algorithm,” submitted 5 Dec. 2017, [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815).
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: David Silver等人，“通过自我对弈用通用强化学习算法掌握国际象棋和将棋”，提交于2017年12月5日，[https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)。
- en: ^i
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^i
- en: ''
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Varun Gulshan et al., “Development and Validation of a Deep Learning Algorithm
    for Detection of Diabetic Retinopathy in Retinal Fundus Photographs,” JAMA, vol.
    316, no. 22, 2016, pp. 2402–2410, [http://mng.bz/wlDQ](http://mng.bz/wlDQ).
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Varun Gulshan等人，“开发和验证用于检测视网膜底片中糖尿病视网膜病变的深度学习算法”，《JAMA》，第316卷，第22期，2016年，第2402–2410页，[http://mng.bz/wlDQ](http://mng.bz/wlDQ)。
- en: JavaScript is a language traditionally devoted to creating web browser UI and
    backend business logic (with Node.js). As someone who expresses ideas and creativity
    in JavaScript, you may feel a little left out by the deep-learning revolution,
    which seems to be the exclusive territory of languages such as Python, R, and
    C++. This book aims at bringing deep learning and JavaScript together through
    the JavaScript deep-learning library called TensorFlow.js. We do this so that
    JavaScript developers like you can learn how to write deep neural networks without
    learning a new language; more importantly, we believe deep learning and JavaScript
    belong together.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript是一种传统上用于创建Web浏览器UI和后端业务逻辑（使用Node.js）的语言。作为一个在JavaScript中表达想法和创造力的人，您可能会对深度学习革命感到有些被排斥，因为它似乎是Python、R和C++等语言的专属领域。本书旨在通过名为TensorFlow.js的JavaScript深度学习库将深度学习和JavaScript结合起来。我们这样做是为了让像您这样的JavaScript开发人员学会如何编写深度神经网络而不需要学习一门新的语言；更重要的是，我们相信深度学习和JavaScript是一对天生的组合。
- en: The cross-pollination will create unique opportunities, ones unavailable in
    any other programming language. It goes both ways for JavaScript and deep learning.
    With JavaScript, deep-learning applications can run on more platforms, reach a
    wider audience, and become more visual and interactive. With deep learning, JavaScript
    developers can make their web apps more intelligent. We will describe how later
    in this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉汇合将创造独特的机会，这是任何其他编程语言都无法提供的。对JavaScript和深度学习都是如此。通过JavaScript，深度学习应用可以在更多平台上运行，触及更广泛的受众，并变得更具视觉和交互性。通过深度学习，JavaScript开发人员可以使他们的Web应用程序更加智能。我们将在本章后面描述如何做到这一点。
- en: '[Table 1.1](#ch01table01) lists some of the most exciting achievements of deep
    learning that we’ve seen in this deep-learning revolution so far. In this book,
    we have selected a number of these applications and created examples of how to
    implement them in TensorFlow.js, either in their full glory or in reduced form.
    These examples will be covered in depth in the coming chapters. Therefore, you
    will not stop at marveling at the breakthroughs: you can learn about them, understand
    them, and implement them all in JavaScript.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1.1](#ch01table01) 列出了迄今为止在这场深度学习革命中我们所见过的一些最令人兴奋的成就。在本书中，我们选择了其中一些应用，并创建了如何在TensorFlow.js中实现它们的示例，无论是完整形式还是简化形式。这些示例将在接下来的章节中深入介绍。因此，你不仅仅会对这些突破感到惊叹：你还可以学习它们、理解它们，并在JavaScript中实现它们。'
- en: But before you dive into these exciting, hands-on deep-learning examples, we
    need to introduce the essential context around AI, deep learning, and neural networks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但在您深入研究这些令人兴奋的、实用的深度学习示例之前，我们需要介绍有关人工智能、深度学习和神经网络的基本背景。
- en: 1.1\. Artificial intelligence, machine learning, neural networks, and deep learning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 人工智能、机器学习、神经网络和深度学习
- en: Phrases like *AI*, *machine learning*, *neural networks*, and *deep learning*
    mean related but different things. To orient yourself in the dazzling world of
    AI, you need to understand what they refer to. Let’s define these terms and the
    relations among them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI*、*机器学习*、*神经网络*和*深度学习*等短语意味着相关但不同的事物。为了在令人眼花缭乱的人工智能世界中找到方向，您需要了解它们指代的内容。让我们定义这些术语及其之间的关系。'
- en: 1.1.1\. Artificial intelligence
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.1\. 人工智能
- en: 'As the Venn diagram in [figure 1.1](#ch01fig01) shows, AI is a broad field.
    A concise definition of the field would be as follows: *the effort to automate
    intellectual tasks normally performed by humans*. As such, AI encompasses machine
    learning, neural networks, and deep learning, but it also includes many approaches
    distinct from machine learning. Early chess programs, for instance, involved hard-coded
    rules crafted by programmers. Those didn’t qualify as machine learning because
    the machines were programmed explicitly to solve the problems instead of being
    allowed to discover strategies for solving the problems by learning from the data.
    For a long time, many experts believed that human-level AI could be achieved through
    handcrafting a sufficiently large set of explicit rules for manipulating knowledge
    and making decisions. This approach is known as *symbolic AI*, and it was the
    dominant paradigm in AI from the 1950s to the late 1980s.^([[1](#ch01fn1)])'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 1.1](#ch01fig01)中的维恩图所示，人工智能是一个广泛的领域。该领域的简明定义如下：*自动执行通常由人类执行的智力任务的努力*。因此，人工智能涵盖了机器学习、神经网络和深度学习，但它还包括许多与机器学习不同的方法。例如，早期的国际象棋程序涉及由程序员精心制定的硬编码规则。这些不被视为机器学习，因为机器是明确地编程来解决问题，而不是允许它们通过从数据中学习来发现解决问题的策略。很长一段时间以来，许多专家相信通过手工制作一套足够庞大的明确规则来操纵知识并做出决策，可以实现人类级别的人工智能。这种方法被称为*符号人工智能*，并且它是从
    1950 年代到 1980 年代末人工智能的主导范式。^([[1](#ch01fn1)])
- en: ¹
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'An important type of symbolic AI is *expert systems*. See this Britannica article
    to learn about them: [http://mng.bz/7zmy](http://mng.bz/7zmy).'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个重要的符号人工智能类型是*专家系统*。请参阅[这篇 Britannica 文章](http://mng.bz/7zmy)了解它们。
- en: Figure 1.1\. Relations between AI, machine learning, neural networks, and deep
    learning. As this Venn diagram shows, machine learning is a subfield of AI. Some
    areas of AI use approaches different from machine learning, such as symbolic AI.
    Neural networks are a subfield of machine learning. There exist non-neural-network
    machine-learning techniques, such as decision trees. Deep learning is the science
    and art of creating and applying “deep” neural networks—neural networks with multiple
    “layers”— versus “shallow” neural networks—neural networks with fewer layers.
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.1\. 人工智能、机器学习、神经网络和深度学习之间的关系。正如这个维恩图所示，机器学习是人工智能的一个子领域。人工智能的一些领域使用与机器学习不同的方法，如符号人工智能。神经网络是机器学习的一个子领域。存在非神经网络的机器学习技术，如决策树。深度学习是创建和应用“深度”神经网络的科学与艺术——多“层”的神经网络——与“浅层”神经网络——层次较少的神经网络相对。
- en: '![](01fig01_alt.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig01_alt.jpg)'
- en: '1.1.2\. Machine learning: How it differs from traditional programming'
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.2\. 机器学习：它与传统编程的不同之处
- en: 'Machine learning, as a subfield of AI distinct from symbolic AI, arises from
    a question: Could a computer go beyond what a programmer knows how to program
    it to perform, and learn on its own how to perform a specific task? As you can
    see, the approach of machine learning is fundamentally different from that of
    symbolic AI. Whereas symbolic AI relies on hard-coding knowledge and rules, machine
    learning seeks to avoid this hard-coding. So, if a machine isn’t explicitly instructed
    on how to perform a task, how would it learn how to do so? The answer is by learning
    from examples in the data.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为与符号人工智能不同的人工智能子领域，机器学习是从一个问题中产生的：计算机是否能超越程序员所知道的如何编程来执行，并且自行学习如何执行特定任务？正如你所看到的，机器学习的方法与符号人工智能的方法根本不同。而符号人工智能依赖于硬编码的知识和规则，机器学习则试图避免这种硬编码。那么，如果一台计算机没有明确指示如何执行任务，它将如何学习如何执行任务呢？答案是通过从数据中学习示例。
- en: This opened the door to a new programming paradigm ([figure 1.2](#ch01fig02)).
    To give an example of the machine-learning paradigm, let’s suppose you are working
    on a web app that handles photos uploaded by users. A feature you want in the
    app is automatic classification of photos into ones that contain human faces and
    ones that don’t. The app will take different actions on face images and no-face
    images. To this end, you want to create a program to output a binary face/no-face
    answer given any input image (made of an array of pixels).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这打开了一个新的编程范式（[图 1.2](#ch01fig02)）。举个机器学习范例，假设你正在开发一个处理用户上传照片的 Web 应用程序。你希望应用程序的一个功能是自动将照片分类为包含人脸和不包含人脸的照片。应用程序将针对人脸图像和非人脸图像采取不同的操作。为此，你想创建一个程序，在给定任何输入图像（由像素数组组成）时输出二进制的人脸/非人脸答案。
- en: Figure 1.2\. Comparing the classical programming paradigm and the machine-learning
    paradigm
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.2\. 比较传统编程范式和机器学习范式
- en: '![](01fig02_alt.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig02_alt.jpg)'
- en: 'We humans can perform this task in a split second: our brains’ genetic hardwiring
    and life experience give us the ability to do so. However, it is hard for any
    programmer, no matter how smart and experienced, to write an explicit set of rules
    in a programming language (the only practical way for humans to communicate with
    a computer) on how to accurately decide whether an image contains a human face.
    You can spend days poring over code that does arithmetic on the RGB (red-green-blue)
    values of pixels to detect elliptic contours that look like faces, eyes, and mouths,
    as well as devising heuristic rules on the geometric relations between the contours.
    But you will soon realize that such effort is laden with arbitrary choices of
    logic and parameters that are hard to justify. More importantly, it is hard to
    make it work well!^([[2](#ch01fn2)]) Any heuristic you come up with is likely
    to fall short when facing the myriad variations that faces can present in real-life
    images, such as differences in the size, shape, and details of the face; facial
    expression; hairstyle; skin color; orientation; the presence or absence of partial
    obscuring; glasses; lighting conditions; objects in the background; and so on.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类可以在一瞬间完成这个任务：我们大脑的基因硬编码和生活经验赋予了我们这样做的能力。然而，对于任何程序员来说，无论多么聪明和经验丰富，都很难用编程语言（人类与计算机交流的唯一实用方式）编写出如何准确判断图像是否包含人脸的一套明确规则。你可以花费几天的时间查看对
    RGB（红绿蓝）像素值进行算术运算的代码，以便检测看起来像脸、眼睛和嘴巴的椭圆轮廓，以及设计关于轮廓之间几何关系的启发式规则。但你很快会意识到，这样的努力充满了难以证明的逻辑和参数的任意选择。更重要的是，很难让它工作得好！^([[2](#ch01fn2)])
    你想出的任何启发式方法在面对现实生活图像中人脸可能呈现的各种变化时都很可能不够用，比如脸部的大小、形状和细节的差异；面部表情；发型；肤色；方向；部分遮挡的存在或不存在；眼镜；光照条件；背景中的物体；等等。
- en: ²
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In fact, such approaches have indeed been attempted before and did not work
    very well. This survey paper provides good examples of handcrafting rules for
    face detection before the advent of deep learning: Erik Hjelmås and Boon Kee Low,
    “Face Detection: A Survey,” *Computer Vision and Image Understanding*, Sept. 2001,
    pp. 236–274, [http://mng.bz/m4d2](http://mng.bz/m4d2).'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '实际上，以前确实尝试过这样的方法，但效果并不好。这份调查报告提供了深度学习出现之前人脸检测的手工制定规则的很好的例子：Erik Hjelmås 和 Boon
    Kee Low，“Face Detection: A Survey”，*计算机视觉与图像理解*，2001 年 9 月，第 236–274 页，[http://mng.bz/m4d2](http://mng.bz/m4d2)。'
- en: In the machine-learning paradigm, you recognize that handcrafting a set of rules
    for such a task is futile. Instead, you find a set of images, some with faces
    in them and some without. Then you enter the desired (that is, correct) face or
    no-face answer for each one. These answers are referred to as *labels*. This is
    a much more tractable (in fact, trivial) task. It may take some time to label
    all the images if there are a lot of them, but the labeling task can be divided
    among several humans and can proceed in parallel. Once you have the images labeled,
    you apply machine learning and let machines discover the set of rules on their
    own. If you use the correct machine-learning techniques, you will arrive at a
    trained set of rules capable of performing the face/no-face task with an accuracy
    > 99%—far better than anything you can hope to achieve with handcrafted rules.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习范式中，你意识到为这样的任务手工制定一套规则是徒劳的。相反，你找到一组图像，其中一些有脸，一些没有。然后，你为每个图像输入期望的（即正确的）脸部或非脸部答案。这些答案被称为*标签*。这是一个更容易处理的（事实上，微不足道的）任务。如果图像很多，可能需要一些时间来为它们标记标签，但是标记任务可以分配给几个人，并且可以并行进行。一旦你标记了图像，你就应用机器学习，让机器自己发现一套规则。如果你使用正确的机器学习技术，你将得到一套训练有素的规则，能够以超过
    99% 的准确率执行脸部/非脸部任务——远远优于任何你希望通过手工制定规则实现的东西。
- en: 'From the previous example, we can see that machine learning is the process
    of automating the discovery of rules for solving complex problems. This automation
    is beneficial for problems like face detection, in which humans know the rules
    intuitively and can easily label the data. For other problems, the rules are not
    known intuitively. For example, consider the problem of predicting whether a user
    will click an ad displayed on a web page, given the page’s and the ad’s contents
    and other information, such as time and location. No human has a good sense about
    how to make accurate predictions for such problems in general. Even if one does,
    the pattern will probably change with time and with the appearance of new content
    and new ads. But the labeled training data is available from the ad service’s
    history: it is available from the ad servers’ logs. The availability of the data
    and labels alone makes machine learning a good fit for problems like this.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子中，我们可以看到机器学习是自动发现解决复杂问题规则的过程。这种自动化对于像面部检测这样的问题非常有益，人类直觉地知道规则并且可以轻松标记数据。对于其他问题，规则并不是直观的。例如，考虑预测用户是否会点击网页上显示的广告的问题，给定页面和广告的内容以及时间和位置等其他信息。一般来说，没有人能准确预测这种问题。即使有人能够，模式也可能随着时间和新内容、新广告的出现而变化。但是标记的训练数据来自广告服务的历史：它来自广告服务器的日志。仅凭数据和标签的可用性就使机器学习成为解决这类问题的良好选择。
- en: In [figure 1.3](#ch01fig03), we take a closer look at the steps involved in
    machine learning. There are two important phases. The first is the *training phase*.
    This phase takes the data and answers, together referred to as the *training data*.
    Each pair of input data and the desired answer is called an *example*. With the
    help of the examples, the training process produces the automatically discovered
    *rules*. Although the rules are discovered automatically, they are not discovered
    entirely from scratch. In other words, machine-learning algorithms are not creative
    in coming up with rules. In particular, a human engineer provides a blueprint
    for the rules at the outset of training. The blueprint is encapsulated in a *model*,
    which forms a *hypothesis space* for the rules the machine may possibly learn.
    Without this hypothesis space, there is a completely unconstrained and infinite
    space of possible rules to search in, which is not conducive to finding good rules
    in a limited amount of time. We will describe in great detail the kinds of models
    available and how to choose the best ones based on the problem at hand. For now,
    it suffices to say that in the context of deep learning, models vary in terms
    of how many layers the neural network consists of, what types of layers they are,
    and how they are wired together.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图1.3](#ch01fig03)中，我们更详细地探讨了机器学习涉及的步骤。有两个重要阶段。第一个是*训练阶段*。这个阶段使用数据和答案，称为*训练数据*。每对输入数据和期望的答案被称为*例子*。借助这些例子，训练过程产生了自动发现的*规则*。尽管规则是自动发现的，但它们并不是完全从零开始发现的。换句话说，机器学习算法并不创造性地提出规则。特别是，人类工程师在训练开始时提供规则的蓝图。这个蓝图被封装在一个*模型*中，形成了机器可能学习的规则的*假设空间*。如果没有这个假设空间，就会有一个完全不受限制的、无限的可能规则搜索空间，这不利于在有限的时间内找到好的规则。我们将详细描述可用的模型种类以及根据手头的问题选择最佳模型的方法。目前，可以说，在深度学习的背景下，模型在神经网络由多少层、它们是什么类型的层以及它们如何连接方面有所不同。
- en: 'Figure 1.3\. A more detailed view of the machine-learning paradigm than that
    in [figure 1.2](#ch01fig02). The workflow of machine learning consists of two
    phases: training and inference. Training is the process of the machine automatically
    discovering the rules that convert the data into answers. The learned rules, encapsulated
    in a trained “model,” are the fruit of the training phase and form the basis of
    the inference phase. Inference means using the model to obtain answers for new
    data.'
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3\. 比[图1.2](#ch01fig02)中更详细的机器学习范式视角。机器学习的工作流程包括两个阶段：训练和推断。训练是机器自动发现将数据转换为答案的规则的过程。学习到的规则被封装在一个经过训练的“模型”中，是训练阶段的成果，并构成推断阶段的基础。推断意味着使用模型为新数据获取答案。
- en: '![](01fig03_alt.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig03_alt.jpg)'
- en: With the training data and the model architecture, the training process produces
    the learned rules, encapsulated in a trained model. This process takes the blueprint
    and alters (or tunes) it in ways that nudge the model’s output closer and closer
    to the desired output. The training phase can take anywhere from milliseconds
    to days, depending on the amount of training data, the complexity of the model
    architecture, and how fast the hardware is. This style of machine learning—namely,
    using labeled examples to progressively reduce the error in a model’s outputs—is
    known as *supervised learning*.^([[3](#ch01fn3)]) Most of the deep-learning algorithms
    we cover in this book are supervised learning. Once we have the trained model,
    we are ready to apply the learned rules on new data—data that the training process
    has never seen. This is the second phase, or *inference phase*. The inference
    phase is less computationally intensive than the training phase because 1) inference
    usually happens on one input (for instance, one image) at a time, whereas training
    involves going through all the training data; and 2) during inference, the model
    does not need to be altered.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据和模型架构，训练过程会产生学习到的规则，封装在一个训练模型中。 这个过程采用蓝图，并以各种方式改变（或调整）它，使模型的输出逐渐接近期望的输出。
    训练阶段的时间可以从毫秒到数天不等，这取决于训练数据的数量，模型架构的复杂性以及硬件的速度。 这种机器学习风格——即使用标记的示例逐渐减少模型输出中的错误——被称为*监督学习*。[[3](#ch01fn3)]
    本书中涵盖的大部分深度学习算法都是监督学习。 一旦我们有了训练好的模型，就可以将学到的规则应用到新数据上——即训练过程从未见过的数据。 这是第二阶段，或*推断阶段*。
    推断阶段的计算负荷比训练阶段小，因为 1）推断通常一次只处理一个输入（例如，一个图像），而训练涉及遍历所有训练数据； 2）在推断期间，模型不需要被改变。
- en: ³
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Another style of machine learning is *unsupervised learning*, in which unlabeled
    data is used. Examples of unsupervised learning are clustering (discovering distinct
    subsets of examples in a dataset) and anomaly detection (determining if a given
    example is sufficiently different from the examples in the training set).
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一种机器学习的风格是*无监督学习*，其中使用未标记的数据。 无监督学习的例子包括聚类（发现数据集中的不同子集）和异常检测（确定给定示例与训练集中的示例是否足够不同）。
- en: Learning representations of data
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习数据的表示
- en: 'Machine learning is about learning from data. But *what* exactly is learned?
    The answer: a way to effectively transform the data or, in other words, to change
    the old representations of the data into a new one that gets us closer to solving
    the problem at hand.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是关于从数据中学习的。 但*究竟*学到了什么？ 答案：一种有效地转换数据的方式，或者换句话说，将数据的旧表示改变为一个新表示，使我们更接近解决手头的问题。
- en: 'Before we go any further, what is a representation? At its core, it is a way
    to look at the data. The same data can be looked at in different ways, leading
    to different representations. For example, a color image can have an RGB or HSV
    (hue-saturation-value) encoding. Here, the words *encoding* and *representation*
    mean essentially the same thing and can be used interchangeably. When encoded
    in these two different formats, the numerical values that represent the pixels
    are completely different, even though they are for the same image. Different representations
    are useful for solving different problems. For example, to find all the red parts
    of an image, the RGB representation is more useful; but to find color-saturated
    parts of the same image, the HSV representation is more useful. This is essentially
    what machine learning is all about: finding an appropriate transformation that
    turns the old representation of the input data into a new one—one that is amenable
    to solving the specific task at hand, such as detecting the location of cars in
    an image or deciding whether an image contains a cat and a dog.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步讨论之前，什么是表示？其核心是一种看待数据的方式。 相同的数据可以以不同的方式来看待，从而导致不同的表示。 例如，彩色图像可以有RGB或HSV（色相-饱和度-值）编码。
    这里，*编码* 和 *表示* 这两个词基本上是指相同的事物，可以互换使用。 当以这两种不同格式进行编码时，代表像素的数值完全不同，即使它们是同一图像的。 不同的表示对于解决不同的问题非常有用。
    例如，要找出图像中所有红色部分，RGB表示更有用； 但是要找出相同图像的色饱和部分，HSV表示更有用。 这基本上就是机器学习的全部内容：找到一种适当的转换，将输入数据的旧表示转换为一个新表示——这个新表示适合解决特定的任务，比如在图像中检测汽车的位置或决定图像中是否包含猫和狗。
- en: To give a visual example, we have a collection of white points and several black
    points in a plane ([figure 1.4](#ch01fig04)). Let’s say we want to develop an
    algorithm that can take the 2D (x, y) coordinates of a point and predict whether
    that point is black or white. In this case,
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给出一个视觉示例，我们在一个平面上有一组白点和几个黑点（[图 1.4](#ch01fig04)）。假设我们想要开发一个算法，可以接受点的二维（x，y）坐标并预测该点是黑色还是白色。在这种情况下，
- en: The input data is the two-dimensional Cartesian coordinates (x and y) of a point.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据是点的二维笛卡尔坐标（x 和 y）。
- en: The output is the predicted color of the point (whether it’s black or white).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是点的预测颜色（是黑色还是白色）。
- en: 'Figure 1.4\. A toy example of the representation transformations that machine
    learning is about. Panel A: the original representation of a dataset consisting
    of black and white points in a plane. Panels B and C: two successive transformation
    steps turn the original representation into one that is more amenable to the color-classification
    task.'
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4\. 机器学习的表示转换的玩具示例。面板A：平面中由黑点和白点组成的数据集的原始表示。面板B和C：两个连续的转换步骤将原始表示转换为更适合颜色分类任务的表示。
- en: '![](01fig04_alt.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig04_alt.jpg)'
- en: The data shows a pattern in panel A of [figure 1.4](#ch01fig04). How would the
    machine decide the color of a point given the x- and y-coordinates? It cannot
    simply compare x with a number, because the range of the x-coordinates of the
    white points overlaps with the range of the x-coordinates of the black ones! Similarly,
    the algorithm cannot rely on the y-coordinate. Therefore, we can see that the
    original representation of the points is not a good one for the black-white classification
    task.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据显示了[图 1.4](#ch01fig04)的面板A中的模式。机器如何根据x和y坐标决定点的颜色呢？它不能简单地将x与一个数字进行比较，因为白点的x坐标范围与黑点的x坐标范围重叠！同样，算法不能依赖于y坐标。因此，我们可以看到点的原始表示不适合黑白分类任务。
- en: What we need is a new representation that separates the two colors in a more
    straightforward way. Here, we transform the original Cartesian x-y representation
    into a polar-coordinate-system representation. In other words, we represent a
    point by 1) its angle—the angle formed by the x-axis and the line that connects
    the origin with the point (see the example in panel A of [figure 1.4](#ch01fig04))
    and 2) its radius—its distance from the origin. After this transformation, we
    arrive at a new representation of the same set of data, as panel B of [figure
    1.4](#ch01fig04) shows. This representation is more amenable to our task, in that
    the angle values of the black and white points are now completely nonoverlapping.
    However, this new representation is still not an ideal one in that the black-white
    color classification cannot be made into a simple comparison with a threshold
    value (like zero).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一种将两种颜色分开的新表示方式。在这里，我们将原始的笛卡尔x-y表示转换为极坐标系统表示。换句话说，我们通过以下方式表示一个点：1）它的角度——x轴和连接原点与点的线之间形成的角度（参见[图1.4](#ch01fig04)的面板A中的示例）和2）它的半径——它到原点的距离。经过这个转换，我们得到了相同数据集的新表示，如[图1.4](#ch01fig04)的面板B所示。这个表示更适合我们的任务，因为黑点和白点的角度值现在完全不重叠。然而，这种新的表示仍然不是理想的，因为黑白颜色分类不能简单地与阈值值（如零）进行比较。
- en: Luckily, we can apply a second transformation to get us there. This transformation
    is based on the simple formula
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以应用第二个转换来实现这一点。这个转换基于简单的公式
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The resulting representation, as shown in panel C, is one-dimensional. Compared
    to the representation in panel B, it throws away the irrelevant information about
    the distance of the points to the origin. But it is a perfect representation in
    that it allows a completely straightforward decision process:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表示，如面板C所示，是一维的。与面板B中的表示相比，它舍弃了关于点到原点的距离的无关信息。但它是一个完美的表示，因为它允许完全直接的决策过程：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, we manually defined a two-step transform of the data representation.
    But if instead we tried automated searching for different possible coordinate
    transforms using feedback about the percentage of points classified correctly,
    then we would be doing machine learning. The number of transformation steps involved
    in solving real machine-learning problems is usually much greater than two, especially
    in deep learning, where it can reach hundreds. Also, the kind of representation
    transformations seen in real machine learning can be much more complex compared
    to those seen in this simple example. Ongoing research in deep learning keeps
    discovering more sophisticated and powerful transformations. But the example in
    [figure 1.4](#ch01fig04) captures the essence of searching for better representations.
    This applies to all machine-learning algorithms, including neural networks, decision
    trees, kernel methods, and so forth.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们手动定义了数据表示的两步转换。但是，如果我们尝试使用关于正确分类百分比的反馈来自动搜索不同可能的坐标转换，那么我们就会进行机器学习。在解决实际机器学习问题时涉及的转换步骤数量通常远远大于两步，特别是在深度学习中，可以达到数百步。此外，实际机器学习中所见到的表示转换类型可能比这个简单示例中所见到的要复杂得多。深度学习中的持续研究不断发现更复杂、更强大的转换方式。但是，[图
    1.4](#ch01fig04) 中的示例捕捉到了寻找更好表示的本质。这适用于所有的机器学习算法，包括神经网络、决策树、核方法等。
- en: 1.1.3\. Neural networks and deep learning
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.3\. 神经网络与深度学习
- en: Neural networks are a subfield of machine learning, one in which the transformation
    of the data representation is done by a system with an architecture loosely inspired
    by how neurons are connected in human and animal brains. How are neurons connected
    to each other in brains? It varies among species and brain regions. But a frequently
    encountered theme of neuronal connection is the layer organization. Many parts
    of the mammalian brain are organized in a layered fashion. Examples include the
    retina, the cerebral cortex, and the cerebellar cortex.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是机器学习的一个子领域，其中数据表示的转换是由一个系统完成的，其架构 loosely 受到人类和动物大脑中神经元连接方式的启发。神经元在大脑中如何连接到彼此？这在物种和脑区之间有所不同。但是神经元连接的一个经常遇到的主题是层组织。许多哺乳动物的大脑部分都是以分层方式组织的。例如视网膜、大脑皮层和小脑皮层。
- en: At least on a superficial level, this pattern is somewhat similar to the general
    organization of *artificial neural networks* (simply called *neural networks*
    in the world of computing, where there is little risk of confusion), in which
    the data is processed in multiple separable stages, aptly named *layers*. These
    layers are usually stacked on top of each other, with connections only between
    adjacent ones. [Figure 1.5](#ch01fig05) shows a simple (artificial) neural network
    with four layers. The input data (an image, in this case) feeds into the first
    layer (on the left side of the figure), then flows sequentially from one layer
    to the next. Each layer applies a new transformation on the representation of
    the data. As the data flows through the layers, the representation becomes increasingly
    different from the original and gets closer and closer to the goal of the neural
    network—namely, applying a correct label to the input image. The last layer (on
    the right side of the figure) emits the neural network’s final output, which is
    the result of the image-classification task.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 至少在表面上，这种模式在某种程度上与*人工神经网络*的一般组织方式相似（在计算机领域简称为*神经网络*，这里几乎没有混淆的风险），其中数据在多个可分隔阶段中进行处理，适当地称为*层*。这些层通常被堆叠在一起，仅在相邻层之间存在连接。[图
    1.5](#ch01fig05) 显示了一个具有四层的简单（人工）神经网络。输入数据（在本例中为图像）流入第一层（图中的左侧），然后依次从一层流向下一层。每个层对数据的表示应用新的转换。随着数据通过层的流动，表示与原始数据越来越不同，并且越来越接近神经网络的目标，即为输入图像应用正确的标签。最后一层（图中的右侧）发出神经网络的最终输出，即图像分类任务的结果。
- en: Figure 1.5\. The schematic diagram of a neural network, organized in layers.
    This neural network classifies images of hand-written digits. In between the layers,
    you can see the intermediate representation of the original data. Reproduced with
    permission from François Chollet, *Deep Learning with Python*, Manning Publications,
    2017.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.5\. 神经网络的示意图，按层组织。这个神经网络对手写数字的图像进行分类。在层之间，你可以看到原始数据的中间表示。经授权转载自 François
    Chollet 的《用 Python 进行深度学习》，Manning 出版社，2017 年。
- en: '![](01fig05_alt.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5](01fig05_alt.jpg)'
- en: A layer of neural networks is similar to a mathematical function in that it
    is a mapping from an input value to an output value. However, neural network layers
    are different from pure mathematical functions in that they are generally *stateful*.
    In other words, they hold internal memory. A layer’s memory is captured in its
    *weights*. What are weights? They are simply a set of numerical values that belong
    to the layer and govern the details of how each input representation is transformed
    by the layer into an output representation. For example, the frequently used *dense*
    layer transforms its input data by multiplying it with a matrix and adding a vector
    to the result of the matrix multiplication. The matrix and the vector are the
    dense layer’s weights. When a neural network is trained through exposure to training
    data, the weights get altered systematically in a way that minimizes a certain
    value called the *loss function*, which we will cover in detail using concrete
    examples in [chapters 2](kindle_split_013.html#ch02) and [3](kindle_split_014.html#ch03).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一层类似于一个数学函数，因为它是从输入值到输出值的映射。然而，神经网络的层与纯粹的数学函数不同，因为它们通常是*有状态的*。换句话说，它们持有内部记忆。一个层的记忆体现在它的*权重*中。什么是权重？它们只是一组属于该层的数值，决定了每个输入表示在该层如何被转换为输出表示。例如，常用的*密集*层通过将其输入数据与矩阵相乘，并将结果加上一个向量来进行转换。矩阵和向量就是密集层的权重。当一个神经网络通过接触训练数据进行训练时，权重会以一种系统化的方式发生变化，目的是最小化一个称为*损失函数*的特定值，我们将在[第2章](kindle_split_013.html#ch02)和[第3章](kindle_split_014.html#ch03)中通过具体例子详细介绍这一点。
- en: Although neural networks are inspired by the brain, we should be careful not
    to overly humanize them. The purpose of neural networks is *not* to study or mimic
    how the brain works. That is the realm of neuroscience, a separate academic discipline.
    Neural networks are about enabling machines to perform interesting practical tasks
    by learning from data. The fact that some neural networks show resemblance to
    some parts of the biological brain, both in structure and in function,^([[4](#ch01fn4)])
    is indeed remarkable. But whether this is a coincidence is beyond the scope of
    this book. In any case, the resemblance should not be overread. Importantly, there
    is no evidence that the brain learns through any form of gradient descent, the
    primary way in which neural networks are trained (covered in the next chapter).
    Many important techniques in neural networks that helped usher in the deep-learning
    revolution were invented and adopted not because they were backed by neuroscience,
    but instead because they helped neural networks solve practical learning tasks
    better and faster.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络的灵感来源于大脑，但我们应该小心不要过度赋予它们人性化。神经网络的目的是*不是*研究或模拟大脑的工作方式。这是神经科学的领域，是一个独立的学术学科。神经网络的目的是通过从数据中学习，让机器执行有趣的实际任务。虽然一些神经网络在结构和功能上与生物大脑的某些部分相似，确实值得注意，但这是否只是巧合超出了本书的范围。无论如何，我们不应过度解读这些相似性。重要的是，没有证据表明大脑是通过任何形式的梯度下降学习的，而梯度下降是训练神经网络的主要方式（在下一章中介绍）。许多神经网络中的重要技术帮助推动了深度学习革命，它们的发明和采用并不是因为得到了神经科学的支持，而是因为它们帮助神经网络更好、更快地解决实际学习任务。
- en: ⁴
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For a compelling example of similarity in functions, see the inputs that maximally
    activate various layers of a convolutional neural network (see [chapter 4](kindle_split_015.html#ch04)),
    which closely resemble the neuronal receptive fields of various parts of the human
    visual system.
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于功能相似性的一个引人注目的例子，请看那些最大化激活卷积神经网络不同层的输入（参见[第4章](kindle_split_015.html#ch04)），这些输入与人类视觉系统不同部分的神经元感受野有着密切的相似之处。
- en: Now that you know what neural networks are, we can tell you what *deep learning*
    is. Deep learning is the study and application of *deep neural networks*, which
    are, quite simply, neural networks with *many layers* (typically, from a dozen
    to hundreds of layers). Here, the word *deep* refers to the idea of a large number
    of successive layers of representations. The number of layers that form a model
    of the data is called the model’s *depth*. Other appropriate names for the field
    could have been “layered representation learning” or “hierarchical representation
    learning.” Modern deep learning often involves tens or hundreds of successive
    layers of representations—and they are all learned automatically from exposure
    to training data. Meanwhile, other approaches to machine learning tend to focus
    on learning only one or two layers of representations of the data; hence, they
    are sometimes called *shallow learning*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道什么是神经网络了，我们可以告诉你什么是*深度学习*。深度学习是研究和应用*深度神经网络*的学科，简单地说，就是具有*许多层次*（通常是从几十到数百层）的神经网络。在这里，*深度*一词指的是大量连续层次的表示法的概念。构成数据模型的层数称为模型的*深度*。该领域的其他合适名称可能是“层次表示学习”或“分层表示学习”。现代深度学习通常涉及数十到数百个连续的表示层次，它们都是自动从训练数据中学习的。与此同时，其他机器学习方法往往集中于仅学习一到两层数据表示；因此，它们有时被称为*浅层学习*。
- en: It is a misconception that the “deep” in deep learning is about any kind of
    deep understanding of data—that is, “deep” in the sense of understanding the meaning
    behind sentences like “freedom is not free” or savoring the contradictions and
    self-references in M.C. Escher’s drawings. That kind of “deep” remains an elusive
    goal for AI researchers.^([[5](#ch01fn5)]) In the future, deep learning may bring
    us closer to this sort of depth, but that will certainly be harder to quantify
    and achieve than adding layers to neural networks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，“深度”一词被误解为对数据的任何深层次理解，即，“深度”意味着理解像“自由不是免费的”这样的句子背后的含义，或者品味 M.C. Escher
    的作品中的矛盾和自指。这种“深度”对于人工智能研究者来说仍然是一个难以捉摸的目标。[[5](#ch01fn5)]未来，深度学习可能会让我们更接近这种深度，但这肯定比给神经网络添加层次更难量化和实现。
- en: ⁵
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Douglas Hofstadter, “The Shallowness of Google Translate,” *The Atlantic*, 30
    Jan. 2018, [http://mng.bz/5AE1](http://mng.bz/5AE1).
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Douglas Hofstadter，《Google 翻译的浅薄》，《大西洋月刊》，2018 年 1 月 30 日，[http://mng.bz/5AE1](http://mng.bz/5AE1)。
- en: '|  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Not just neural networks: Other popular machine-learning techniques**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**不仅仅是神经网络：其他流行的机器学习技术**'
- en: We went directly from the “machine learning” circle of the Venn diagram in [figure
    1.1](#ch01fig01) to the “neural network” circle inside. However, it is worthwhile
    for us to briefly visit the machine-learning techniques that are not neural networks,
    not only because doing so will give us a better historical context but also because
    you may run into some of the techniques in existing code.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接从 [图 1.1](#ch01fig01) 的 Venn 图中的“机器学习”圈子转到内部的“神经网络”圈子。然而，值得我们简要讨论一下不是神经网络的机器学习技术，不仅因为这样做会给我们更好的历史背景，而且因为您可能会在现有代码中遇到一些这样的技术。
- en: The *Naive Bayes classifier* is one of the earliest forms of machine learning.
    Put simply, Bayes’ theorem is about how to estimate the probability of an event
    given 1) the a priori belief of how likely the event is and 2) the observed facts
    (called *features*) relating to the event. This theorem can be used to classify
    observed data points into one of many known categories by choosing the category
    with the highest probability (likelihood) given the observed facts. Naive Bayes
    is based on the assumption that the observed facts are mutually independent (a
    strong and naive assumption, hence the name).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*朴素贝叶斯分类器*是最早的机器学习形式之一。简而言之，贝叶斯定理是关于如何估计事件概率的定理，给定 1) 事件发生的先验信念有多大可能性和 2) 与事件相关的观察事实（称为*特征*）。这个定理可以用来通过选择给定观察事实的最大概率（似然）的类别，将观察到的数据点分类到许多已知类别之一。朴素贝叶斯基于观察到的事实相互独立的假设（一个强假设和天真的假设，因此得名）。'
- en: '*Logistic regression* (or *logreg*) is also a classification technique. Thanks
    to its simple and versatile nature, it is still popular and often the first thing
    a data scientist will try in order to get a feel for the classification task at
    hand.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*逻辑回归*（或 *logreg*）也是一种分类技术。由于它的简单和多才多艺的性质，它仍然很受欢迎，通常是数据科学家在尝试了解手头分类任务的感觉后尝试的第一件事情。'
- en: '*Kernel methods*, of which support vector machines (SVMs) are the best-known
    examples, tackle binary (that is, two-class) classification problems by mapping
    the original data into spaces of higher dimensionality and finding a transformation
    that maximizes a distance (called a *margin*) between two classes of examples.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*核方法*，其中支持向量机（SVM）是最著名的例子，通过将原始数据映射到更高维度的空间，并找到一个最大化两类示例之间距离（称为*边距*）的转换来解决二元（即两类）分类问题。'
- en: '*Decision trees* are flowchart-like structures that let you classify input
    data points or predict output values given inputs. At each step of the flowchart,
    you answer a simple yes/no question, such as, “Is feature X greater than a certain
    threshold?” Depending on whether the answer is yes or no, you advance to one of
    two possible next questions, which is just another yes/no question, and so forth.
    Once you reach the end of the flowchart, you will get the final answer. As such,
    decision trees are easy for humans to visualize and iterpret.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是类似流程图的结构，允许您对输入数据点进行分类或根据输入预测输出值。在流程图的每个步骤中，您需要回答一个简单的是/否问题，例如，“特征X是否大于某个阈值？”根据答案是是还是否，您将前进到两个可能的下一个问题之一，这只是另一个是/否问题，依此类推。一旦到达流程图的末尾，您将得到最终答案。因此，决策树易于人类可视化和解释。'
- en: Random forests and gradient-boosted machines increase the accuracy of decision
    trees by forming an ensemble of a large number of specialized, individual decision
    trees. *Ensembling*, also known as *ensemble learning*, is the technique of training
    a collection (that is, an ensemble) of individual machine-learning models and
    using an aggregate of their outputs during inference. Today, gradient boosting
    may be one of the best algorithms, if not the best, for dealing with nonperceptual
    data (for example, credit card fraud detection). Alongside deep learning, it is
    one of the most commonly used techniques in data science competitions, such as
    those on Kaggle.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和梯度提升机通过形成大量专门的个体决策树的集成来提高决策树的准确性。*集成*，也称为*集成学习*，是训练一组（即集成）个体机器学习模型并在推理过程中使用它们的输出的技术。如今，梯度提升可能是处理非感知数据（例如，信用卡欺诈检测）的最佳算法之一，如果不是最佳算法。与深度学习并列，它是数据科学竞赛（例如Kaggle上的竞赛）中最常用的技术之一。
- en: '|  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The rise, fall, and rise of neural networks, and the reasons behind them
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 神经网络的兴起、衰退和再兴起，以及背后的原因
- en: 'The core ideas of neural networks were formed as early as the 1950s. The key
    techniques for training neural networks, including backpropagation, were invented
    in the 1980s. However, for a long period of time between the 1980s and the 2010s,
    neural networks were almost completely shunned by the research community, partly
    because of the popularity of competing methods such as SVMs and partly because
    of the lack of an ability to train deep (many-layered) neural networks. But around
    2010, a number of people still working on neural networks started to make important
    breakthroughs: the groups of Geoffrey Hinton at the University of Toronto, Yoshua
    Bengio at the University of Montreal, and Yann LeCun at New York University, as
    well as researchers at the Dalle Molle Institute for Artificial Intelligence Research
    (IDSIA) in Switzerland. These groups achieved important milestones, including
    the first practical implementations of deep neural networks on graphics processing
    units (GPUs) and driving the error rate from about 25% down to less than 5% in
    the ImageNet computer vision challenge.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的核心思想早在1950年代就形成了。训练神经网络的关键技术，包括反向传播，是在1980年代发明的。然而，在1980年代到2010年代的很长一段时间里，神经网络几乎完全被研究界忽视，部分原因是由于竞争方法（如SVM）的流行，部分原因是由于缺乏训练深度（多层）神经网络的能力。但是大约在2010年左右，一些仍然在研究神经网络的人开始取得重要的突破：加拿大多伦多大学的Geoffrey
    Hinton小组，蒙特利尔大学的Yoshua Bengio小组，纽约大学的Yann LeCun小组，以及瑞士达勒莫勒人工智能研究所（IDSIA）的研究人员。这些团队取得了重要的里程碑，包括在图形处理单元（GPU）上实现深度神经网络的第一个实际应用，并将ImageNet计算机视觉挑战的错误率从约25%降低到不到5%。
- en: Since 2012, deep *convolutional neural networks* (convnets) have become the
    go-to algorithm for all computer-vision tasks; more generally, they work on all
    perceptual tasks. Examples of non-computer-vision perceptual tasks include speech
    recognition. At major computer vision conferences in 2015 and 2016, it was nearly
    impossible to find presentations that didn’t involve convnets in some form. At
    the same time, deep learning has also found applications in many other types of
    problems, such as natural language processing. It has completely replaced SVMs
    and decision trees in a wide range of applications. For instance, for several
    years, the European Organization for Nuclear Research, CERN, used decision-tree-based
    methods to analyze particle data from the ATLAS detector at the Large Hadron Collider;
    but CERN eventually switched to deep neural networks due to their higher performance
    and ease of training on large datasets.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 自2012年以来，深度*卷积神经网络*（卷积网络）已成为所有计算机视觉任务的首选算法；更一般地，它们适用于所有感知任务。非计算机视觉感知任务的例子包括语音识别。在2015年和2016年的主要计算机视觉会议上，几乎不可能找到不涉及卷积网络的演示。同时，深度学习还在许多其他类型的问题中找到了应用，如自然语言处理。它已经完全取代了支持向量机（SVM）和决策树在广泛应用的范围内。例如，多年来，欧洲核子研究组织（CERN）使用基于决策树的方法来分析大型强子对撞机中的ATLAS探测器的粒子数据；但由于其在大型数据集上的性能更高且更容易训练，CERN最终转向了深度神经网络。
- en: 'So, what makes deep learning stand out from the range of available machine-learning
    algorithms? (See [info box 1.1](#ch01sb01) for a list of some popular machine-learning
    techniques that are not deep neural networks.) The primary reason deep learning
    took off so quickly is that it offered better performance on many problems. But
    that’s not the only reason. Deep learning also makes problem-solving much easier
    because it automates what used to be the most crucial and difficult step in a
    machine-learning workflow: *feature engineering*.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么让深度学习在众多可用的机器学习算法中脱颖而出呢？（请参阅[信息框1.1](#ch01sb01)查看一些不是深度神经网络的流行机器学习技术列表。）深度学习迅速崛起的主要原因之一是它在许多问题上提供了更好的性能。但这并不是唯一的原因。深度学习还使问题解决变得更容易，因为它自动化了曾经是机器学习工作流程中最关键和最困难的步骤：*特征工程*。
- en: 'Previous machine-learning techniques—shallow learning—only involved transforming
    the input data into one or two successive representation spaces, usually via simple
    transformations such as high-dimensional nonlinear projections (kernel methods)
    or decision trees. But the refined representations required by complex problems
    generally can’t be attained by such techniques. As such, human engineers had to
    go to great lengths to make the initial input data more amenable to processing
    by these methods: they had to manually engineer good layers of representations
    for their data. This is called *feature engineering*. Deep learning, on the other
    hand, automates this step: with deep learning, you learn all features in one pass
    rather than having to engineer them yourself. This has greatly simplified machine-learning
    workflows, often replacing sophisticated multistage pipelines with a single, simple,
    end-to-end deep-learning model. Through automating feature engineering, deep learning
    makes machine learning less labor-intensive and more robust—two birds with one
    stone.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的机器学习技术——浅层学习——只涉及将输入数据转换为一个或两个连续的表示空间，通常通过简单的转换，如高维非线性投影（核方法）或决策树。但是，复杂问题所需的精细表示通常无法通过这些技术获得。因此，人工工程师不得不费尽心思地使初始输入数据更容易被这些方法处理：他们不得不手动为其数据设计出良好的表示层。这就是*特征工程*。另一方面，深度学习自动化了这一步骤：通过深度学习，您可以一次学习所有特征，而不是自己设计它们。这极大地简化了机器学习工作流程，通常用单个、简单的端到端深度学习模型替代了复杂的多阶段管道。通过自动化特征工程，深度学习使机器学习变得更少劳动密集和更加健壮——一举两得。
- en: 'These are the two essential characteristics of how deep learning learns from
    data: the incremental, layer-by-layer way in which increasingly complex representations
    are developed; and the fact that these intermediate incremental representations
    are learned jointly, each layer being updated to follow both the representational
    needs of the layer above and the needs of the layer below. Together, these two
    properties have made deep learning vastly more successful than previous approaches
    to machine learning.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习从数据中学习的两个基本特征：逐层增量地开发越来越复杂的表示方式；以及这些中间增量表示是共同学习的，每层都更新以满足其上层和下层的表示需求。这两个属性共同使深度学习比以前的机器学习方法更加成功。
- en: 1.1.4\. Why deep learning? Why now?
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.4\. 为什么深度学习？为什么是现在？
- en: 'If basic ideas and core techniques for neural networks already existed as early
    as the 1980s, why did the deep-learning revolution start to happen only after
    2012? What changed in the two decades in between? In general, three technical
    forces drive advances in machine learning:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经网络的基本思想和核心技术早在1980年代就已存在，为什么深度学习革命直到2012年才开始发生？两者之间发生了什么变化？总的来说，推动机器学习进步的有三个技术力量：
- en: Hardware
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件
- en: Datasets and benchmarks
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集和基准测试
- en: Algorithmic advances
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法进展
- en: Let’s visit these factors one by one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探讨这些因素。
- en: Hardware
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 硬件
- en: Deep learning is an engineering science guided by experimental findings rather
    than by theory. Algorithmic advances become possible only when appropriate hardware
    are available to try new ideas (or to scale up old ideas, as is often the case).
    Typical deep-learning models used in computer vision or speech recognition require
    orders of magnitude more computational power than what your laptop can deliver.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一门由实验结果指导而非理论指导的工程科学。只有当适当的硬件可用于尝试新的想法（或扩大旧想法的规模，通常情况下）时，算法进展才能成为可能。用于计算机视觉或语音识别的典型深度学习模型所需的计算能力比您的笔记本电脑提供的数量级更高。
- en: Throughout the 2000s, companies like NVIDIA and AMD invested billions of dollars
    in developing fast, massively parallel chips (GPUs) to power the graphics of increasingly
    photorealistic video games—cheap, single-purpose supercomputers designed to render
    complex 3D scenes on your screen in real time. This investment came to benefit
    the scientific community when, in 2007, NVIDIA launched CUDA (short for Compute
    Unified Device Architecture), a general-purpose programming interface for its
    line of GPUs. A small number of GPUs started replacing massive clusters of CPUs
    in various highly parallelizable applications, beginning with physics modeling.
    Deep neural networks, consisting mostly of many matrix multiplications and additions,
    are also highly parallelizable.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个21世纪的头十年中，NVIDIA和AMD等公司投入了数十亿美元开发快速、大规模并行芯片——用于提供越来越逼真的视频游戏图形的单一用途超级计算机，旨在实时在您的屏幕上渲染复杂的3D场景。当NVIDIA于2007年推出CUDA（短语Compute
    Unified Device Architecture）时，这些投资开始让科学界受益。CUDA是其GPU产品线的通用编程接口。在各种高度可并行化的应用程序中，一小部分GPU开始取代大型CPU集群，从物理建模开始。由许多矩阵乘法和加法组成的深度神经网络也是高度可并行化的。
- en: Around 2011, some researchers began to write CUDA implementations of neural
    nets—Dan Ciresan and Alex Krizhevsky were among the first. Today, high-end GPUs
    can deliver hundreds of times more parallel computation power when training deep
    neural networks than what a typical CPU is capable of. Without the sheer computational
    power of modern GPUs, it would be impossible to train many state-of-the-art deep
    neural networks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在2011年左右，一些研究人员开始编写神经网络的CUDA实现，丹·希雷赞和亚历克斯·克里兹弗斯基是其中的先驱之一。如今，高端GPU在训练深度神经网络时可以提供比typical
    CPU高出数百倍的并行计算能力。如果没有现代GPU的强大计算能力，许多最先进的深度神经网络的训练是不可能的。
- en: Data and benchmarks
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据和基准测试
- en: 'If hardware and algorithms are the steam engine of the deep-learning revolution,
    then data is its coal: the raw material that powers our intelligent machines,
    without which nothing would be possible. When it comes to data, in addition to
    the exponential progress in storage hardware over the past 20 years (following
    Moore’s law), the game changer has been the rise of the internet, which has made
    it feasible to collect and distribute very large datasets for machine learning.
    Today, large companies work with image datasets, video datasets, and natural language
    datasets that couldn’t have been collected without the internet. User-generated
    image tags on Flickr, for instance, have been a treasure trove of data for computer
    vision. So are YouTube videos. And Wikipedia is a key dataset for natural language
    processing.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果硬件和算法是深度学习革命的蒸汽机，那么数据就是它的煤炭：是驱动我们智能机器的原材料，没有它什么也不可能。在数据方面，除了过去 20 年存储硬件的指数级进步（遵循摩尔定律）之外，游戏变革者是互联网的崛起，使得收集和分发用于机器学习的大型数据集成为可能。今天，大型公司处理图像数据集、视频数据集和自然语言数据集，而这些数据集没有互联网是无法收集的。Flickr
    上用户生成的图像标签，例如，已成为计算机视觉的数据宝库。YouTube 视频也是如此。而维基百科是自然语言处理的关键数据集。
- en: If there’s one dataset that has been a catalyst for the rise of deep learning,
    it’s ImageNet, which consists of 1.4 million images that have been hand annotated
    with 1,000 image categories. What makes ImageNet special isn’t just its large
    size; it is also the yearly competition associated with it. As ImageNet and Kaggle
    have been demonstrating since 2010, public competitions are an excellent way to
    motivate researchers and engineers to push the envelope. Having common benchmarks
    that researchers compete to beat has greatly helped the recent rise of deep learning.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一个数据集促进了深度学习的崛起，那就是 ImageNet，它由 140 万张手动注释的图片组成，涵盖 1000 个分类。使 ImageNet 特殊的不仅仅是其规模庞大；还有与之相关的年度比赛。自
    2010 年以来，像 ImageNet 和 Kaggle 这样的公开竞赛是激励研究人员和工程师去突破极限的极佳方式。拥有共同的基准让研究人员竞争超越已经极大地推动了深度学习的最近崛起。
- en: Algorithmic advances
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 算法进步
- en: In addition to hardware and data, until the late 2000s, we were missing a reliable
    way to train very deep neural networks. As a result, neural networks were still
    fairly shallow, using only one or two layers of representations; thus, they couldn’t
    shine against more refined shallow methods such as SVMs and random forests. The
    key issue was that of gradient propagation through deep stacks of layers. The
    feedback signal used to train neural networks would fade away as the number of
    layers increased.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件和数据之外，在 2000 年代后期之前，我们还缺乏一种可靠的方法来训练非常深的神经网络。因此，神经网络仍然是相当浅的，只使用一两层表示；因此，它们无法与更精细的浅层方法（如
    SVM 和随机森林）相媲美。关键问题在于通过深层堆栈的层传播梯度。用于训练神经网络的反馈信号会随着层数的增加而减弱。
- en: 'This changed around 2009 to 2010 with the advent of several simple but important
    algorithmic improvements that allowed for better gradient propagation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在 2009 年至 2010 年发生了变化，随着几个简单但重要的算法改进的出现，使梯度传播变得更好：
- en: Better activation functions for neural network layers (such as the rectified
    linear unit, or relu)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的激活函数用于神经网络层（如线性整流单元，或 relu）
- en: Better weight-initialization schemes (for example, Glorot initialization)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的权重初始化方案（例如，Glorot 初始化）
- en: Better optimization schemes (for example, RMSProp and ADAM optimizers)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的优化方案（例如，RMSProp 和 ADAM 优化器）
- en: Only when these improvements began to allow for training models with 10 or more
    layers did deep learning start to shine. Finally, in 2014, 2015, and 2016, even
    more advanced ways to help gradient propagation were discovered, such as batch
    normalization, residual connections, and depthwise separable convolutions. Today
    we can train from scratch models that are thousands of layers deep.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当这些改进开始允许训练具有 10 层或更多层的模型时，深度学习才开始发光发热。最终，在 2014 年、2015 年和 2016 年，还发现了更先进的帮助梯度传播的方法，如批归一化、残差连接和深度可分离卷积。今天，我们可以从头开始训练数千层深的模型。
- en: 1.2\. Why combine JavaScript and machine learning?
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 为什么要结合 JavaScript 和机器学习？
- en: 'Machine learning, like other branches of AI and data science, is usually done
    with traditionally backend-focused languages, such as Python and R, running on
    servers or workstations outside the web browser.^([[6](#ch01fn6)]) This status
    quo is not surprising. The training of deep neural networks often requires the
    kind of multicore and GPU-accelerated computation not directly available in a
    browser tab; the enormous amount of data that it sometimes takes to train such
    models is most conveniently ingested in the backend: for example, from a native
    file system of virtually unlimited size. Until recently, many regarded “deep learning
    in JavaScript” as a novelty. In this section, we will present reasons why, for
    many kinds of applications, performing deep learning in the browser environment
    with JavaScript is a wise choice, and explain how combining the power of deep
    learning and the web browser creates unique opportunities, especially with the
    help of TensorFlow.js.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，像人工智能和数据科学的其他分支一样，通常使用传统的后端语言进行，比如 Python 和 R，运行在服务器或工作站上而不是在 web 浏览器中。^([[6](#ch01fn6)])
    这种现状并不令人意外。深度神经网络的训练通常需要多核和 GPU 加速的计算，这在浏览器选项卡中直接不可用；有时需要大量数据来训练这样的模型，最方便的方式是在后端进行摄取：例如，从几乎无限大小的本地文件系统中。直到最近，许多人认为“JavaScript
    中的深度学习”是一种新奇事物。在本节中，我们将阐述为什么对于许多种类的应用来说，在浏览器环境中使用 JavaScript 进行深度学习是一个明智的选择，并解释如何结合深度学习和
    web 浏览器的力量创造独特的机会，特别是在 TensorFlow.js 的帮助下。
- en: ⁶
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Srishti Deoras, “Top 10 Programming Languages for Data Scientists to Learn in
    2018,” *Analytics India Magazine*, 25 Jan. 2018, [http://mng.bz/6wrD](http://mng.bz/6wrD).
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Srishti Deoras，“数据科学家学习的前 10 种编程语言”，《Analytics India Magazine》，2018 年 1 月 25
    日，[http://mng.bz/6wrD](http://mng.bz/6wrD)。
- en: First, once a machine-learning model is trained, it must be deployed somewhere
    in order to make predictions on real data (such as classifying images and text,
    detecting events in audio or video streams, and so on). Without deployment, training
    a model is just a waste of compute power. It is often desirable or imperative
    that the “somewhere” is a web frontend. Readers of this book are likely to appreciate
    the overall importance of the web browser. On desktops and laptops, the web browser
    is the dominant means through which users access content and services on the internet.
    It is how desktop and laptop users spend most of their time using those devices,
    exceeding the second place by a large margin. It is how users get vast amounts
    of their daily work done, stay connected, and entertain themselves. The wide range
    of applications that run in the web browser provide rich opportunities for applying
    client-side machine learning. For the mobile frontend, the web browser trails
    behind native mobile apps in terms of user engagement and time spent. But mobile
    browsers are nonetheless a force to be reckoned with because of their broader
    reach, instant access, and faster development cycles.^([[7](#ch01fn7)]) In fact,
    because of their flexibility and ease of use, many mobile apps, such as Twitter
    and Facebook, drop into a JavaScript-enabled web view for certain types of content.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一旦训练了机器学习模型，就必须将其部署到某个地方，以便对真实数据进行预测（例如对图像和文本进行分类，检测音频或视频流中的事件等）。没有部署，对模型进行训练只是浪费计算资源。通常情况下，希望或必须将“某个地方”设置为
    web 前端。本书的读者可能会意识到 web 浏览器的整体重要性。在台式机和笔记本电脑上，通过 web 浏览器是用户访问互联网上的内容和服务的主要方式。这是用户在使用这些设备时花费大部分时间的方式，远远超过第二名。这是用户完成大量日常工作、保持联系和娱乐自己的方式。运行在
    web 浏览器中的各种应用程序为应用客户端机器学习提供了丰富的机会。对于移动前端来说，Web 浏览器在用户参与度和时间上落后于原生移动应用程序。但是，移动浏览器仍然是一股不可忽视的力量，因为它们具有更广泛的覆盖范围、即时访问和更快的开发周期。^([[7](#ch01fn7)])
    实际上，由于它们的灵活性和易用性，许多移动应用程序，例如 Twitter 和 Facebook，对于某些类型的内容会在启用 JavaScript 的 web
    视图中运行。
- en: ⁷
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Rishabh Borde, “Internet Time Spend in Mobile Apps, 2017–19: It’s 8x than Mobile
    Web,” DazeInfo, 12 Apr. 2017, [http://mng.bz/omDr](http://mng.bz/omDr).'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Rishabh Borde，“移动应用中花费在互联网上的时间，2017–19：比移动网络多了 8 倍”，DazeInfo，2017 年 4 月 12 日，[http://mng.bz/omDr](http://mng.bz/omDr)。
- en: 'Due to this broad reach, the web browser is a logical choice for deploying
    deep-learning models, as long as the kinds of data the models expect are available
    in the browser. But what kinds of data are available in the browser? The answer
    is, many! Take, for example, the most popular applications of deep learning: classifying
    and detecting objects in images and videos, transcribing speech, translating languages,
    and analyzing text content. Web browsers are equipped with arguably the most comprehensive
    technologies and APIs for presenting (and, with user permission, for capturing)
    textual, image, audio, and video data. As a result, powerful machine-learning
    models can be directly used in the browser, for example, with TensorFlow.js and
    straightforward conversion processes. In the later chapters of this book, we will
    cover many concrete examples of deploying deep-learning models in the browser.
    For example, once you have captured images from a webcam, you can use TensorFlow.js
    to run MobileNet to label objects, run YOLO2 to put bounding boxes around detected
    objects, run Lipnet to do lipreading, or run a CNN-LSTM network to apply captions
    to images.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其广泛的覆盖范围，Web 浏览器是部署深度学习模型的一个合理选择，只要模型所需的数据类型在浏览器中可用。但是，浏览器中有哪些类型的数据可用呢？答案是，很多！例如，深度学习的最流行应用：图像和视频中的对象分类和检测、语音转录、语言翻译和文本内容分析。Web
    浏览器配备了可能是最全面的技术和 API，用于呈现（以及在用户许可的情况下捕获）文本、图像、音频和视频数据。因此，强大的机器学习模型可以直接在浏览器中使用，例如，使用
    TensorFlow.js 和简单的转换过程。在本书的后几章中，我们将涵盖许多在浏览器中部署深度学习模型的具体示例。例如，一旦您从网络摄像头捕获了图像，您可以使用
    TensorFlow.js 运行 MobileNet 对对象进行标记，运行 YOLO2 对检测到的对象放置边界框，运行 Lipnet 进行唇读，或者运行 CNN-LSTM
    网络为图像应用标题。
- en: Once you have captured audio from the microphone using the browser’s WebAudio
    API, TensorFlow.js can run models to perform real-time spoken-word recognition.
    There are exciting applications with textual data as well, such as assigning sentiment
    scores to user text like movie reviews ([chapter 9](kindle_split_021.html#ch09)).
    Beyond these data modalities, the modern web browser can access a range of sensors
    on mobile devices. For example, HTML5 provides API access to geolocation (latitude
    and longitude), motion (device orientation and acceleration), and ambient light
    (see [http://mobilehtml5.org](http://mobilehtml5.org)). Combined with deep learning
    and other data modalities, data from such sensors opens doors to many exciting
    new applications.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您使用浏览器的 WebAudio API 从麦克风捕获了音频，TensorFlow.js 就可以运行模型执行实时口语识别。文本数据也有令人兴奋的应用，例如为用户文本（如电影评论）分配情感分数（[第
    9 章](kindle_split_021.html#ch09)）。除了这些数据模态，现代 Web 浏览器还可以访问移动设备上的一系列传感器。例如，HTML5
    提供了对地理位置（纬度和经度）、运动（设备方向和加速度）和环境光（参见 [http://mobilehtml5.org](http://mobilehtml5.org)）的
    API 访问。结合深度学习和其他数据模态，来自这些传感器的数据为许多令人兴奋的新应用打开了大门。
- en: 'Browser-based application of deep learning comes with five additional benefits:
    reduced server cost, lowered inference latency, data privacy, instant GPU acceleration,
    and instant access:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于浏览器的深度学习应用具有五个额外的好处：降低服务器成本、减少推理延迟、数据隐私、即时 GPU 加速和即时访问：
- en: '*Server cost* is often an important consideration when designing and scaling
    web services. The computation required to run deep-learning models in a timely
    manner is often significant, necessitating the use of GPU acceleration. If models
    are not deployed to the client side, they need to be deployed on GPU-backed machines,
    such as virtual machines with CUDA GPUs from Google Cloud or Amazon Web Services.
    Such cloud GPU machines are often costly. Even the most basic GPU machines presently
    cost in the neighborhood of $0.5–1 per hour (see [https://www.ec2instances.info](https://www.ec2instances.info)
    and [https://cloud.google.com/gpu](https://cloud.google.com/gpu)). With increasing
    traffic, the cost of running a fleet of cloud GPU machines gets higher, not to
    mention the challenge of scalability and the added complexity of your server stack.
    All these concerns can be eliminated by deploying the model to the client. The
    overhead of client-side downloading of the model (which is often several megabytes
    or more) can be alleviated by the browser’s caching and local storage capabilities
    ([chapter 2](kindle_split_013.html#ch02)).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务器成本*在设计和扩展 Web 服务时通常是一个重要考虑因素。及时运行深度学习模型所需的计算通常是相当大的，这就需要使用 GPU 加速。如果模型没有部署到客户端，它们就需要部署在支持
    GPU 的机器上，比如来自 Google Cloud 或 Amazon Web Services 的具有 CUDA GPU 的虚拟机。这样的云 GPU 机器通常价格昂贵。即使是最基本的
    GPU 机器目前也在每小时约$0.5–1左右（见 [https://www.ec2instances.info](https://www.ec2instances.info)
    和 [https://cloud.google.com/gpu](https://cloud.google.com/gpu)）。随着流量的增加，运行一系列云
    GPU 机器的成本变得更高，更不用说可伸缩性的挑战以及服务器堆栈的复杂性。所有这些问题都可以通过将模型部署到客户端来消除。客户端下载模型的开销（通常是数兆字节或更多）可以通过浏览器的缓存和本地存储功能来减轻（[第
    2 章](kindle_split_013.html#ch02)）。'
- en: '*Lowered inference latency*—For certain types of applications, the requirement
    for latency is so stringent that the deep-learning models must be run on the client
    side. Any applications that involve real-time audio, image, and video data fall
    into this category. Consider what will happen if image frames need to be transferred
    to the server for inference. Suppose images are captured from a webcam at a modest
    size of 400 × 400 pixels with three color channels (RGB) and an 8-bit depth per
    color channel at a rate of 10 frames per second. Even with JPEG compression, each
    image has a size of about 150 Kb. On a typical mobile network with an approximately
    300-Kbps upload bandwidth, it can take more than 500 milliseconds to upload each
    image, leading to a latency that is noticeable and perhaps unacceptable for certain
    applications (for example, games). This calculation doesn’t take into account
    the fluctuation in (and possible loss of) network connectivity, the additional
    time it takes to download the inference results, and the vast amount of mobile
    data usage, each of which can be a showstopper. Client-side inference addresses
    these potential latency and connectivity concerns by keeping the data and the
    computation on the device. It is impossible to run real-time machine-learning
    applications such as labeling objects and detecting poses in webcam images without
    the model running purely on the client. Even for applications without latency
    requirements, the reduction in model inference latency can lead to greater responsiveness
    and hence an improved user experience.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降低推理延迟*—对于某些类型的应用程序，延迟的要求非常严格，以至于深度学习模型必须在客户端上运行。任何涉及实时音频、图像和视频数据的应用程序都属于此类。考虑一下如果图像帧需要传输到服务器进行推理会发生什么。假设图像以每秒
    10 帧的速率从摄像头捕获，大小适中为 400 × 400 像素，具有三个颜色通道（RGB）和每个颜色通道 8 位深度。即使使用 JPEG 压缩，每个图像的大小也约为
    150 Kb。在具有约 300 Kbps 上传带宽的典型移动网络上，每个图像的上传可能需要超过 500 毫秒，导致一个可察觉且可能不可接受的延迟，对于某些应用程序（例如游戏）来说。这个计算没有考虑到网络连接的波动（和可能的丢失）、下载推理结果所需的额外时间以及大量的移动数据使用量，每一项都可能是一个停滞点。客户端推理通过在设备上保留数据和计算来解决这些潜在的延迟和连接性问题。在没有模型纯粹在客户端上运行的情况下，无法运行实时的机器学习应用程序，比如在网络摄像头图像中标记对象和检测姿势。即使对于没有延迟要求的应用程序，减少模型推理延迟也可以提高响应性，从而改善用户体验。'
- en: '*Data privacy*—Another benefit of leaving the training and inference data on
    the client is the protection of users’ privacy. The topic of data privacy is becoming
    increasingly important today. For certain types of applications, data privacy
    is an absolute requirement. Applications related to health and medical data are
    a prominent example. Consider a “skin disease diagnosis aid” that collects images
    of a patient’s skin from their webcam and uses deep learning to generate possible
    diagnoses of the skin condition. Health information privacy regulations in many
    countries will not allow the images to be transferred to a centralized server
    for inference. By running the model inference in the browser, no data needs to
    ever leave the user’s phone or be stored anywhere, ensuring the privacy of the
    user’s health data. Consider another browser-based application that uses deep
    learning to provide users with suggestions on how to improve the text they write
    in the application. Some users may use this application to write sensitive content
    such as legal documents and will not be comfortable with the data being transferred
    to a remote server via the public internet. Running the model purely in client-side
    browser JavaScript is an effective way to address this concern.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据隐私*——将训练和推断数据留在客户端的另一个好处是保护用户的隐私。数据隐私的话题在今天变得越来越重要。对于某些类型的应用程序，数据隐私是绝对必要的。与健康和医疗数据相关的应用程序是一个突出的例子。考虑一个“皮肤病诊断辅助”应用，它从用户的网络摄像头收集患者皮肤的图像，并使用深度学习生成皮肤状况的可能诊断。许多国家的健康信息隐私法规将不允许将图像传输到集中式服务器进行推断。通过在浏览器中运行模型推断，用户的数据永远不需要离开用户的手机或存储在任何地方，确保用户健康数据的隐私。再考虑另一个基于浏览器的应用程序，它使用深度学习为用户提供建议，以改善他们在应用程序中编写的文本。一些用户可能会使用此应用程序编写诸如法律文件之类的敏感内容，并且不希望数据通过公共互联网传输到远程服务器。在客户端浏览器JavaScript中纯粹运行模型是解决此问题的有效方法。'
- en: '*Instant WebGL acceleration*—In addition to the availability of data, another
    prerequisite for running machine-learning models in the web browser is sufficient
    compute power through GPU acceleration. As mentioned earlier, many state-of-the-art
    deep-learning models are so computationally intensive that acceleration through
    parallel computation on the GPU is a must (unless you are willing to let users
    wait for minutes for a single inference result, which rarely happens in real applications).
    Fortunately, modern web browsers come equipped with the WebGL API, which, even
    though it was originally designed for accelerated rendering of 2D and 3D graphics,
    can be ingeniously leveraged for the kind of parallel computation required for
    accelerating neural networks. The authors of TensorFlow.js painstakingly wrapped
    WebGL-based acceleration of the deep-learning components in the library, so the
    acceleration is available to you through a single line of JavaScript import. WebGL-based
    acceleration of neural networks may not be perfectly on par with native, tailored
    GPU acceleration such as NVIDIA’s CUDA and CuDNN (used by Python deep-learning
    libraries such as TensorFlow and PyTorch), but it still leads to orders of magnitude
    speedup of neural networks and enables real-time inference such as what PoseNet
    extraction of a human-body pose offers. If performing inference on pretrained
    models is expensive, performing training or transfer learning on such models is
    even more so. Training and transfer learning enable exciting applications such
    as personalization of deep-learning models, frontend visualization of deep learning,
    and federated learning (training the same model on many devices, then aggregating
    the results of the training to obtain a good model). The WebGL acceleration of
    TensorFlow.js makes it possible to train or fine-tune neural networks with sufficient
    speed, purely inside the web browser.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*即时的WebGL加速* — 除了数据可用性外，将机器学习模型在Web浏览器中运行的另一个先决条件是通过GPU加速获得足够的计算能力。 正如前面提到的，许多最先进的深度学习模型在计算上是如此密集，以至于通过GPU上的并行计算加速是必不可少的（除非您愿意让用户等待几分钟才能获得单个推断结果，在真实应用中很少发生）。
    幸运的是，现代Web浏览器配备了WebGL API，尽管它最初是为了加速2D和3D图形的渲染而设计的，但可以巧妙地利用它来进行加速神经网络所需的并行计算。
    TensorFlow.js的作者们费尽心思地将基于WebGL的深度学习组件加速包装在库中，因此通过一行JavaScript导入即可为您提供加速功能。 基于WebGL的神经网络加速可能与本机的、量身定制的GPU加速（如NVIDIA的CUDA和CuDNN，用于Python深度学习库，如TensorFlow和PyTorch）不完全相匹配，但它仍然会大大加快神经网络的速度，并实现实时推断，例如PoseNet对人体姿势的提取。
    如果对预训练模型进行推断是昂贵的，那么对这些模型进行训练或迁移学习的成本就更高了。 训练和迁移学习使得诸如个性化深度学习模型、前端可视化深度学习和联邦学习（在许多设备上训练相同的模型，然后聚合训练结果以获得良好的模型）等令人兴奋的应用成为可能。
    TensorFlow.js的WebGL加速使得在Web浏览器中纯粹进行训练或微调神经网络成为可能。'
- en: '*Instant access*—Generally speaking, applications that run in the browser have
    the natural advantage of “zero install:” all it takes to access the app is typing
    a URL or clicking a link. This forgoes any potentially tedious and error-prone
    installation steps, along with possibly risky access control when installing new
    software. In the context of deep learning in the browser, the WebGL-based neural
    network acceleration that TensorFlow.js provides does not require special kinds
    of graphics cards or installation of drivers for such cards, which is often a
    nontrivial process. Most reasonably up-to-date desktop, laptop, and mobile devices
    come with graphics cards available to the browser and WebGL. Such devices, as
    long as they have a TensorFlow.js-compatible web browser installed (a low bar),
    are automatically ready to run WebGL-accelerated neural networks. This is an especially
    attractive feature in places where ease of access is vital—for example, the education
    of deep learning.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*即时访问* — 一般来说，运行在浏览器中的应用程序具有“零安装”的天然优势：访问应用程序所需的全部步骤就是输入URL或单击链接。 这省去了任何可能繁琐和容易出错的安装步骤，以及在安装新软件时可能存在的风险访问控制。
    在浏览器中进行深度学习的背景下，TensorFlow.js提供的基于WebGL的神经网络加速不需要特殊类型的图形卡或为此类卡安装驱动程序，这通常是一个不平凡的过程。
    大多数合理更新的台式机、笔记本电脑和移动设备都配备了供浏览器和WebGL使用的图形卡。 只要安装了与TensorFlow.js兼容的Web浏览器（门槛很低），这些设备就可以自动准备好运行WebGL加速的神经网络。
    这在访问便利至关重要的地方尤为吸引人，例如深度学习的教育。'
- en: '|  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Accelerating computation using GPU and WebGL**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用GPU和WebGL加速计算**'
- en: It takes a massive number of math operations to train machine-learning models
    and use them for inference. For example, the widely used “dense” neural network
    layers involve multiplying a large matrix with a vector and adding the result
    to another vector. A typical operation of this sort involves thousands or millions
    of floating-point operations. An important fact about such operations is that
    they are often *parallelizable*. For instance, adding two vectors can be divided
    into many smaller operations, such as adding two individual numbers. These smaller
    operations do not depend on each other. For example, you don’t need to know the
    sum of the two elements of the two vectors at index 0 to compute the sum of the
    two elements at index 1\. As a result, the smaller operations can be performed
    at the same time, instead of one at a time, no matter how large the vectors are.
    Serial computation, such as a naive CPU implementation of vector addition, is
    known as Single Instruction Single Data (SISD). Parallel computation on the GPU
    is known as Single Instruction Multiple Data (SIMD). It typically takes the CPU
    less time to compute each individual addition than a GPU takes. But the total
    cost over this large amount of data leads the GPU’s SIMD to outperform the CPU’s
    SISD. A deep neural network can contain millions of parameters. For a given input,
    it might take billions of element-by-element math operations to run (if not more).
    The massively parallel computation that GPUs are capable of really shines at this
    scale.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型并将其用于推断需要大量的数学运算。例如，广泛使用的“密集”神经网络层涉及将大矩阵与向量相乘，并将结果添加到另一个向量中。这种类型的典型操作涉及数千或数百万次浮点运算。关于这类操作的一个重要事实是它们通常是*可并行化*的。例如，将两个向量相加可以分解为许多较小的操作，比如将两个单独的数字相加。这些较小的操作不相互依赖。例如，您不需要知道两个向量在索引
    0 处的两个元素的和来计算索引 1 处的两个元素的和。因此，这些较小的操作可以同时进行，而不是一个接一个地进行，无论向量有多大。串行计算，例如向量加法的简单
    CPU 实现，被称为单指令单数据（SISD）。GPU 上的并行计算称为单指令多数据（SIMD）。通常，CPU 计算每个单独的加法所需的时间比 GPU 更少。但是，在这么大量的数据上的总成本导致
    GPU 的 SIMD 胜过 CPU 的 SISD。深度神经网络可以包含数百万个参数。对于给定的输入，可能需要进行数十亿次逐元素数学运算（如果不是更多）。GPU
    能够执行的大规模并行计算在这个规模下表现出色。
- en: 'Task: Add two vectors, element by element:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 任务：逐个元素添加两个向量：
- en: '![](f0022_01_alt.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![在 GPU 上的计算](f0022_01_alt.jpg)'
- en: Computation on a CPU
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 上的计算
- en: '![](f0022_02_alt.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![如何利用 WebGL 加速利用 GPU 的并行计算能力来实现比 CPU 更快的向量运算](f0022_02_alt.jpg)'
- en: Computation on a GPU
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 上的计算
- en: '![](f0022_03.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![在 CPU 上的计算](f0022_03.jpg)'
- en: '**How WebGL acceleration leverages a GPU’s parallel computation capability
    to achieve faster vector operation than a CPU**'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**WebGL 加速如何利用 GPU 的并行计算能力来实现比 CPU 更快的向量操作**'
- en: To be precise, modern CPUs are capable of certain levels of SIMD instructions,
    too. However, a GPU comes with a much greater number of processing units (on the
    order of hundreds or thousands) and can execute instructions on many slices of
    the input data at the same time. Vector addition is a relatively simple SIMD task
    in that each step of computation looks at only a single index, and the results
    at different indices are independent of each other. Other SIMD tasks seen in machine
    learning are more complex. For example, in matrix multiplication, each step of
    computation uses data from multiple indices, and there are dependencies between
    the indices. But the basic idea of acceleration through parallelization remains
    the same.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 精确来说，现代 CPU 也能够执行一定级别的 SIMD 指令。但是，GPU 配备了更多的处理单元（数量在数百到数千之间），可以同时对输入数据的多个片段执行指令。向量加法是一个相对简单的
    SIMD 任务，因为每个计算步骤只查看一个索引，并且不同索引处的结果彼此独立。在机器学习中看到的其他 SIMD 任务更复杂。例如，在矩阵乘法中，每个计算步骤使用多个索引处的数据，并且索引之间存在依赖关系。但是通过并行化加速的基本思想是相同的。
- en: 'It is interesting to note that GPUs were not originally designed for accelerating
    neural networks. This can be seen in the name: *graphics processing unit*. The
    primary purpose of GPUs is processing 2D and 3D graphics. In many graphical applications,
    such as 3D gaming, it is critical that the processing be done in as little time
    as possible so that the images on the screen can be updated at a sufficiently
    high frame rate for smooth gaming experiences. This was the original motivation
    when the creators of the GPU exploited SIMD parallelization. But, as a pleasant
    surprise, the kind of parallel computing GPUs are capable of also suits the needs
    of machine learning.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，GPU 最初并不是为加速神经网络而设计的。这可以从名称中看出：*图形处理单元*。GPU 的主要用途是处理 2D 和 3D 图形。在许多图形应用中，例如
    3D 游戏，至关重要的是要尽可能地快速处理，以便屏幕上的图像可以以足够高的帧率更新，以获得流畅的游戏体验。这是 GPU 的创建者利用 SIMD 并行化时的最初动机。但令人惊喜的是，GPU
    能够进行的并行计算也正适合机器学习的需求。
- en: The WebGL library TensorFlow.js uses for GPU acceleration was originally designed
    for tasks such as rendering textures (surface patterns) on 3D objects in the web
    browser. But textures are just arrays of numbers! Hence, we can pretend that the
    numbers are neural network weights or activations and repurpose WebGL’s SIMD texture
    operations to run neural networks. This is exactly how TensorFlow.js accelerates
    neural networks in the browser.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 GPU 加速的 WebGL 库 TensorFlow.js 最初是为在网络浏览器中渲染 3D 对象上的纹理（表面图案）等任务而设计的。但是，纹理只是一组数字！因此，我们可以假装这些数字是神经网络的权重或激活，并重新利用
    WebGL 的 SIMD 纹理操作来运行神经网络。这正是 TensorFlow.js 在浏览器中加速神经网络的方式。
- en: '|  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'In addition to the advantages we have described, web-based machine-learning
    applications enjoy the same benefits as generic web applications that do not involve
    machine learning:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们描述的优势之外，基于网络的机器学习应用享受与不涉及机器学习的通用网络应用相同的好处：
- en: Unlike native app development, the JavaScript application you write with TensorFlow.js
    will work on many families of devices, ranging from Mac, Windows, and Linux desktops
    to Android and iOS devices.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与原生应用开发不同，使用 TensorFlow.js 编写的 JavaScript 应用程序将在许多设备系列上运行，从 Mac、Windows 和 Linux
    桌面到 Android 和 iOS 设备。
- en: With its optimized 2D and 3D graphical capabilities, the web browser is the
    richest and most mature environment for data visualization and interactivity.
    In places where people would like to present the behavior and internals of neural
    networks to humans, it is hard to think of any environment that beats the browser.
    Take TensorFlow Playground, for example ([https://playground.tensorflow.org](https://playground.tensorflow.org)).
    It is a highly popular web app in which you can interactively solve classification
    problems with neural networks. You can tune the structure and hyperparameters
    of the neural network and observe how its hidden layers and outputs change as
    a result (see [figure 1.6](#ch01fig06)). If you have not played with it before,
    we highly recommend you give it a try. Many have expressed the view that this
    is among the most instructive and delightful educational materials they’ve seen
    on the topic of neural networks. TensorFlow Playground is, in fact, an important
    forebearer of TensorFlow.js. As an offspring of the Playground, TensorFlow.js
    is powered by a far wider range of deep-learning capabilities and far more optimized
    performance. In addition, it is equipped with a dedicated component for visualization
    of deep-learning models (covered in [chapter 7](kindle_split_019.html#ch07) in
    detail). No matter whether you want to build basic educational applications along
    the lines of TensorFlow Playground or present your cutting-edge deep-learning
    research in a visually appealing and intuitive fashion, TensorFlow.js will help
    you go a long way toward your goals (see examples such as real-time tSNE embedding
    visualization^([[8](#ch01fn8)])).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凭借其优化的二维和三维图形能力，网络浏览器是数据可视化和交互性最丰富、最成熟的环境。在人们希望向人类展示神经网络的行为和内部机制时，很难想象有哪种环境能比得上浏览器。以
    TensorFlow Playground 为例（[https://playground.tensorflow.org](https://playground.tensorflow.org)）。这是一个非常受欢迎的
    Web 应用程序，您可以使用其中与神经网络交互式解决分类问题。您可以调整神经网络的结构和超参数，并观察其隐藏层和输出的变化（见[图 1.6](#ch01fig06)）。如果您之前还没有尝试过，我们强烈建议您试试。许多人表示，这是他们在神经网络主题上看到的最具有教育性和愉悦性的教育材料之一。事实上，TensorFlow
    Playground 实际上是 TensorFlow.js 的重要前身。作为 Playground 的衍生品，TensorFlow.js 具有更广泛的深度学习功能和更优化的性能。此外，它还配备了一个专门用于深度学习模型可视化的组件（在[第
    7 章](kindle_split_019.html#ch07)中有详细介绍）。无论您是想构建基本的教育应用程序，还是以视觉上吸引人和直观的方式展示您的前沿深度学习研究，TensorFlow.js
    都将帮助您在实现目标的道路上走得更远（参见诸如实时 tSNE 嵌入可视化的示例^([[8](#ch01fn8)])）。
- en: ⁸
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹⁰
- en: ''
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Nicola Pezzotti, “Realtime tSNE Visualizations with TensorFlow.js,” *googblogs*,
    [http://mng.bz/nvDg](http://mng.bz/nvDg).
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Nicola Pezzotti，“使用 TensorFlow.js 进行实时 tSNE 可视化”，*googblogs*，[http://mng.bz/nvDg](http://mng.bz/nvDg)。
- en: Figure 1.6\. A screenshot of TensorFlow Playground ([https://playground.tensorflow.org](https://playground.tensorflow.org)),
    a popular browser-based UI for teaching how neural networks work from Daniel Smilkov
    and his colleagues at Google. TensorFlow Playground was also an important precursor
    of the later TensorFlow.js project.
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.6\. TensorFlow Playground 的屏幕截图（[https://playground.tensorflow.org](https://playground.tensorflow.org)），这是一个由谷歌的
    Daniel Smilkov 及其同事开发的受欢迎的基于浏览器的用户界面，用于教授神经网络的工作原理。TensorFlow Playground 也是后来
    TensorFlow.js 项目的重要前身。
- en: '![](01fig06_alt.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig06_alt.jpg)'
- en: 1.2.1\. Deep learning with Node.js
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.1\. 使用 Node.js 进行深度学习
- en: For security and performance reasons, the web browser is designed to be a resource-constrained
    environment in terms of limited memory and storage quota. This means that the
    browser is not an ideal environment for training large machine-learning models
    with large amounts of data, despite the fact that it is ideal for many types of
    inference, small-scale training, and transfer-learning tasks, which require fewer
    resources. However, Node.js alters the equation entirely. Node.js enables JavaScript
    to be run outside the web browser, thus granting it access to all the native resources,
    such as RAM and the file system. TensorFlow.js comes with a Node.js version, called
    *tfjs-node*. It binds directly to the native TensorFlow libraries compiled from
    C++ and CUDA code, and so enables users to use the same parallelized CPU and GPU
    operation kernels as used under the hood by TensorFlow (in Python). As can be
    shown empirically, the speed of model training in tfjs-node is on par with the
    speed of Keras in Python. So, tfjs-node is an appropriate environment for training
    large machine-learning models with large amounts of data. In this book, you will
    see examples in which we use tfjs-node to train the kind of large models that
    are beyond the browser’s capability (for example, the word recognizer in [chapter
    5](kindle_split_016.html#ch05) and the text-sentiment analyzer in [chapter 9](kindle_split_021.html#ch09)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全和性能考虑，Web 浏览器被设计为资源受限的环境，具有有限的内存和存储配额。这意味着，尽管浏览器对许多类型的推断、小规模训练和迁移学习任务（需要较少的资源）非常理想，但对于使用大量数据训练大型机器学习模型来说，并不是理想的环境。然而，Node.js完全改变了这一方程。Node.js使JavaScript能够在Web浏览器之外运行，从而赋予它对所有本机资源（如RAM和文件系统）的访问权限。TensorFlow.js带有一个Node.js版本，称为*tfjs-node*。它直接绑定到从C++和CUDA代码编译的本机TensorFlow库，并且使用户能够使用与TensorFlow（在Python中）底层使用的并行化CPU和GPU操作内核相同的内核。正如可以通过实证显示的那样，在tfjs-node中模型训练的速度与Python中Keras的速度相当。因此，tfjs-node是一个适合用于训练大型机器学习模型的环境。在本书中，您将看到一些示例，我们在其中使用tfjs-node来训练超出浏览器能力范围的大型模型（例如，[第5章](kindle_split_016.html#ch05)中的单词识别器和[第9章](kindle_split_021.html#ch09)中的文本情感分析器）。
- en: But what are the possible reasons to choose Node.js over the more established
    Python environment for training machine-learning models? The answers are 1) performance
    and 2) compatibility with existing stack and developer skill sets. First, in terms
    of performance, the state-of-the-art JavaScript interpreters, such as the V8 engine
    Node.js uses, perform just-in-time (JIT) compilation of JavaScript code, leading
    to superior performance over Python. As a result, it is often faster to train
    models in tfjs-node than in Keras (Python), as long as the model is small enough
    for the language interpreter performance to be the determining factor.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，相对于更成熟的Python环境来训练机器学习模型，选择Node.js的可能原因是什么呢？答案是1）性能和2）与现有堆栈和开发人员技能集的兼容性。首先，在性能方面，如Node.js所使用的V8引擎等最新JavaScript解释器对JavaScript代码进行即时（JIT）编译，导致性能优于Python。因此，只要模型足够小以至于语言解释器的性能成为决定因素，通常在tfjs-node中训练模型比在Keras（Python）中快。
- en: Second, Node.js is a very popular environment for building server-side applications.
    If your backend is already written in Node.js, and you would like to add machine
    learning to your stack, using tfjs-node is usually a better choice than using
    Python. By keeping code in a single language, you can directly reuse large portions
    of your code base, including those bits for loading and formatting the data. This
    will help you set up the model-training pipeline faster. By not adding a new language
    to your stack, you also keep its complexity and maintenance costs down, possibly
    saving the time and cost of hiring a Python programmer.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Node.js是构建服务器端应用程序的非常流行的环境。如果您的后端已经是用Node.js编写的，并且您想要向堆栈中添加机器学习，那么使用tfjs-node通常比使用Python更好。通过保持代码在一个语言中，您可以直接重用代码库的大部分代码，包括加载和格式化数据的那些部分。这将帮助您更快地设置模型训练管道。通过不向堆栈添加新语言，您还可以降低其复杂性和维护成本，可能节省雇佣Python程序员的时间和成本。
- en: Finally, the machine-learning code written in TensorFlow.js will work in both
    the browser environment and Node.js, with the possible exception of data-related
    code that relies on browser-only or Node-only APIs. Most of the code examples
    you will encounter in this book will work in both environments. We have made efforts
    to separate the environment-independent, machine-learning-centric part of the
    code from the environment-specific data-ingestion and UI code. The added benefit
    is that you get the ability to do deep learning on both the server and client
    sides by learning only one library.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，用 TensorFlow.js 编写的机器学习代码将在浏览器环境和 Node.js 中都能工作，只有在依赖于仅限于浏览器或仅限于 Node 的 API
    的数据相关代码可能会有例外。您在本书中遇到的大多数代码示例都将在这两种环境中工作。我们努力将代码中与环境无关且以机器学习为中心的部分与与环境相关的数据摄取和
    UI 代码分开。额外的好处是您只需学习一个库，就能在服务器和客户端都进行深度学习。
- en: 1.2.2\. The JavaScript ecosystem
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.2\. JavaScript 生态系统
- en: When assessing the suitability of JavaScript for a certain type of application
    such as deep learning, we should not ignore the factor that JavaScript is a language
    with an exceptionally strong ecosystem. For years, JavaScript has been consistently
    ranked number one among a few dozen programming languages in terms of repository
    count and pull activities on GitHub (see [http://githut.info](http://githut.info)).
    On npm, the de facto public repository of JavaScript packages, there are more
    than 600,000 packages as of July 2018\. This more than quadruples the number of
    packages in PyPI, the de facto public repository of Python packages ([www.modulecounts.com](http://www.modulecounts.com)).
    Despite the fact that Python and R have a better-established community for machine
    learning and data science, the JavaScript community is building up support for
    machine-learning-related data pipelines as well.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估 JavaScript 在某种类型的应用程序（如深度学习）中的适用性时，我们不应忽视 JavaScript 是一种具有异常强大生态系统的语言的因素。多年来，JavaScript
    在 GitHub 上的存储库数量和拉取活动方面一直稳居数十种编程语言中的第一位（参见 [http://githut.info](http://githut.info)）。在
    npm 上，JavaScript 包的事实上公共存储库，截至 2018 年 7 月，已经有超过 600,000 个包。这个数字是 PyPI（Python 包的事实上公共存储库）的包数量的四倍以上（参见
    [www.modulecounts.com](http://www.modulecounts.com)）。尽管 Python 和 R 在机器学习和数据科学领域拥有更成熟的社区，但
    JavaScript 社区也在建立机器学习相关的数据流水线支持。
- en: Want to ingest data from cloud storage and databases? Both Google Cloud and
    Amazon Web Services provide Node.js APIs. Most popular database systems today,
    such as MongoDB and RethinkDB, have first-class support for Node.js drivers. Want
    to wrangle data in JavaScript? We recommend the book *Data Wrangling with JavaScript*
    by Ashley Davis (Manning Publications, 2018, [www.manning.com/books/data-wrangling-with-javascript](http://www.manning.com/books/data-wrangling-with-javascript)).
    Want to visualize your data? There are mature and powerful libraries such as d3.js,
    vega.js, and plotly.js that outshine Python visualization libraries in many regards.
    Once you have your input data ready, TensorFlow.js, the main topic of this book,
    will take it from there and help you create, train, and execute your deep-learning
    models, as well as save, load, and visualize them.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 想要从云存储和数据库获取数据吗？谷歌云和亚马逊 Web 服务都提供 Node.js API。如今最流行的数据库系统，如 MongoDB 和 RethinkDB，都对
    Node.js 驱动程序提供了一流的支持。想要在 JavaScript 中整理数据吗？我们推荐 Ashley Davis 的《使用 JavaScript 进行数据整理》一书（Manning
    Publications，2018 年，[www.manning.com/books/data-wrangling-with-javascript](http://www.manning.com/books/data-wrangling-with-javascript)）。想要可视化您的数据吗？有成熟和强大的库，如
    d3.js、vega.js 和 plotly.js，在许多方面超越了 Python 可视化库。一旦您准备好输入数据，本书的主要内容 TensorFlow.js
    将接手处理，并帮助您创建、训练和执行深度学习模型，以及保存、加载和可视化它们。
- en: Finally, the JavaScript ecosystem is still constantly evolving in exciting ways.
    Its reach is being extended from its traditional strongholds—namely, the web browser
    and Node.js backend environments—to new territories such as desktop applications
    (for example, Electron) and native mobile applications (for instance, React Native
    and Ionic). It is often easier to write UIs and apps for such frameworks than
    to use myriad platform-specific app creation tools. JavaScript is a language that
    has the potential to bring the power of deep learning to all major platforms.
    We summarize the main benefits of combining JavaScript and deep learning in [table
    1.2](#ch01table02).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，JavaScript 生态系统仍在以令人振奋的方式不断发展。它的影响力正在从其传统的强项——即 Web 浏览器和 Node.js 后端环境——扩展到新的领域，例如桌面应用程序（例如
    Electron）和本地移动应用程序（例如 React Native 和 Ionic）。对于这样的框架编写 UI 和应用程序通常比使用各种平台特定的应用程序创建工具更容易。JavaScript
    是一种具有将深度学习的力量带到所有主要平台的潜力的语言。我们在[table 1.2](#ch01table02)中总结了将 JavaScript 和深度学习结合使用的主要优点。
- en: Table 1.2\. A brief summary of the benefits of doing deep learning in JavaScript
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 1.2\. 在 JavaScript 中进行深度学习的利益的简要总结
- en: '| Consideration | Examples |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 考虑因素 | 示例 |'
- en: '| --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Reasons related to the client side |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 与客户端相关的原因 |'
- en: Reduced inference and training latency due to the locality of data
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据局部性而降低推理和训练延迟
- en: Ability to run models when the client is offline
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在客户端脱机时运行模型的能力
- en: Privacy protection (data never leaves the browser)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私保护（数据永远不会离开浏览器）
- en: Reduced server cost
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低服务器成本
- en: Simplified deployment stack
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化的部署栈
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Reasons related to the web browser |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 与 Web 浏览器相关的原因 |'
- en: Availability of multiple modalities of data (HTML5 video, audio, and sensor
    APIs) for inference and training
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于推理和训练的多种数据形式（HTML5 视频、音频和传感器 API）
- en: The zero-install user experience
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零安装用户体验
- en: The zero-install access to parallel computation via the WebGL API on a wide
    range of GPUs
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在广泛范围的 GPU 上通过 WebGL API 进行并行计算的零安装访问
- en: Cross-platform support
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨平台支持
- en: Ideal environment for visualization and interactivity
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想的可视化和交互环境
- en: Inherently interconnected environment opens direct access to various sources
    of machine-learning data and resources
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 固有互联环境开启对各种机器学习数据和资源的直接访问
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Reasons related to JavaScript |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 与 JavaScript 相关的原因 |'
- en: JavaScript is the most popular open source programming language by many measures,
    so there is an abundance of JavaScript talent and enthusiasm.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JavaScript 是许多衡量标准中最受欢迎的开源编程语言，因此有大量的 JavaScript 人才和热情。
- en: JavaScript has a vibrant ecosystem and wide applications at both client and
    server sides.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JavaScript 在客户端和服务器端都有着丰富的生态系统和广泛的应用。
- en: Node.js allows applications to run on the server side without the resource constraints
    of the browser.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Node.js 允许应用程序在服务器端运行，而不受浏览器资源约束。
- en: The V8 engine makes JavaScript code run fast.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V8 引擎使 JavaScript 代码运行速度快。
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 1.3\. Why TensorFlow.js?
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 为什么选择 TensorFlow.js？
- en: To do deep learning in JavaScript, you need to select a library. TensorFlow.js
    is our choice for this book. In this section, we will describe what TensorFlow.js
    is and the reasons we selected it.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 JavaScript 中进行深度学习，您需要选择一个库。TensorFlow.js 是我们这本书的选择。在本节中，我们将描述 TensorFlow.js
    是什么以及我们选择它的原因。
- en: 1.3.1\. A brief history of TensorFlow, Keras, and TensorFlow.js
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1\. TensorFlow、Keras 和 TensorFlow.js 的简要历史
- en: TensorFlow.js is a library that enables you to do deep learning in JavaScript.
    As its name suggests, TensorFlow.js is designed to be consistent and compatible
    with TensorFlow, the Python framework for deep learning. To understand TensorFlow.js,
    we need to briefly examine the history of TensorFlow.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 是一个使您能够在 JavaScript 中进行深度学习的库。顾名思义，TensorFlow.js 旨在与 Python 深度学习框架
    TensorFlow 保持一致和兼容。要了解 TensorFlow.js，我们需要简要介绍 TensorFlow 的历史。
- en: 'TensorFlow was made open source in November 2015 by a team of engineers working
    on deep learning at Google. The authors of this book are members of this team.
    Since its open source debut, TensorFlow has gained immense popularity. It is now
    being used for a wide range of industrial applications and research projects both
    at Google and in the larger technical community. The name “TensorFlow” was coined
    to reflect what happens inside a typical program written with the framework: data
    representations called *tensors* flow through layers and other data-processing
    nodes, allowing inference and training to happen on machine-learning models.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是由谷歌的深度学习团队于 2015 年 11 月开源的。本书的作者是该团队的成员。自从它开源以来，TensorFlow 受到了极大的欢迎。它现在被广泛应用于谷歌和更大的技术社区中的各种工业应用和研究项目中。名称“TensorFlow”是为了反映使用该框架编写的典型程序内部发生的情况：数据表示称为*tensors*（张量）在层和其他数据处理节点之间流动，允许对机器学习模型进行推理和训练。
- en: 'First off, what is a tensor? It is just a computer scientist’s way of saying
    “multidimensional array” concisely. In neural networks and deep learning, every
    piece of data and every computation result is represented as a tensor. For example,
    a grayscale image can be represented as a 2D array of numbers—a 2D tensor; a color
    image is usually represented as a 3D tensor, with the extra dimension being the
    color channels. Sounds, videos, text, and any other types of data can all be represented
    as tensors. Each tensor has two basic properties: the data type (such as float32
    or int32) and the shape. Shape describes the size of the tensor along all its
    dimensions. For instance, a 2D tensor may have the shape `[128, 256]`, and a 3D
    tensor may have the shape `[10, 20, 128]`. Once data is turned into a tensor of
    a given data type and shape, it can be fed into any type of layer that accepts
    that data type and shape, regardless of the data’s original meaning. Therefore,
    the tensor is the lingua franca of deep-learning models.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，什么是张量？这只是计算机科学家简明扼要地说“多维数组”的方式。在神经网络和深度学习中，每个数据和每个计算结果都表示为一个张量。例如，灰度图像可以表示为一个数字的
    2D 数组——一个 2D 张量；彩色图像通常表示为一个 3D 张量，其中额外的维度是颜色通道。声音、视频、文本和任何其他类型的数据都可以表示为张量。每个张量具有两个基本属性：数据类型（例如
    float32 或 int32）和形状。形状描述了张量沿着所有维度的大小。例如，一个 2D 张量可能具有形状`[128, 256]`，而一个 3D 张量可能具有形状`[10,
    20, 128]`。一旦数据被转换为给定数据类型和形状的张量，它就可以被馈送到接受该数据类型和形状的任何类型的层中，而不管数据的原始含义是什么。因此，张量是深度学习模型的通用语言。
- en: But *why* tensors? In the previous section, we learned that the bulk of the
    computations involved in running a deep neural network are performed as massively
    parallelized operations, commonly on GPUs, which require performing the same computation
    on multiple pieces of data. Tensors are containers that organize our data into
    structures that can be processed efficiently in parallel. When we add tensor A
    with shape `[128, 128]` to tensor B with shape `[128, 128]`, it is very clear
    that there are `128 * 128` independent additions that need to take place.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 但是*为什么*使用张量？在前一节中，我们了解到在运行深度神经网络中涉及的大部分计算是作为大规模并行化操作执行的，通常在 GPU 上进行，这需要对多个数据块执行相同的计算。张量是将我们的数据组织成可以在并行中高效处理的结构的容器。当我们将形状为`[128,
    128]`的张量 A 加到形状为`[128, 128]`的张量 B 时，非常清楚需要进行`128 * 128`次独立的加法运算。
- en: 'How about the “flow” part? Imagine a tensor as a kind of fluid that carries
    data. In TensorFlow, it flows through a *graph*—a data structure consisting of
    interconnected mathematical operations (called *nodes*). As [figure 1.7](#ch01fig07)
    shows, the node can be successive layers in a neural network. Each node takes
    tensors as inputs and produces tensors as outputs. The “tensor fluid” gets transformed
    into different shapes and different values as it “flows” through the TensorFlow
    graph. This corresponds to the transformation of representations: that is, the
    crux of what neural networks do, as we have described in previous sections. Using
    TensorFlow, machine-learning engineers can write all kinds of neural networks,
    ranging from shallow ones to very deep ones, from convnets for computer vision
    to recurrent neural networks (RNNs) for sequence tasks. The graph data structure
    can be serialized and deployed to run many types of devices, from mainframes to
    mobile phones.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '那么“流”部分呢？想象一下张量就像一种携带数据的流体。在 TensorFlow 中，它通过一个*图*流动——一个由相互连接的数学操作（称为*节点*）组成的数据结构。如[图1.7](#ch01fig07)所示，节点可以是神经网络中的连续层。每个节点将张量作为输入并产生张量作为输出。随着“张量流体”通过
    TensorFlow 图“流动”，它会被转换成不同的形状和不同的值。这对应于表示的转换：也就是我们在前面的章节中描述的神经网络的要点。使用 TensorFlow，机器学习工程师可以编写各种各样的神经网络，从浅层到非常深层的网络，从用于计算机视觉的卷积网络到用于序列任务的循环神经网络（RNN）。图数据结构可以被序列化并部署到运行许多类型设备上，从大型机到手机。  '
- en: Figure 1.7\. Tensors “flow” through a number of layers, a common scenario in
    TensorFlow and TensorFlow.js.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7\. 张量“流动”通过多个层，这在 TensorFlow 和 TensorFlow.js 中是一个常见情景。
- en: '![](01fig07_alt.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig07_alt.jpg)'
- en: 'At its core, TensorFlow was designed to be very general and flexible: the operations
    can be any well-defined mathematical functions, not just layers of neural networks.
    For example, they can be low-level mathematical operations such as adding and
    multiplying two tensors—the kind of operations that happen *inside* a neural network
    layer. This gives deep-learning engineers and researchers great power to define
    arbitrary and novel operations for deep learning. However, for a large fraction
    of deep-learning practitioners, manipulating such low-level machinery is more
    trouble than it’s worth. It leads to bloated and more error-prone code and longer
    development cycles. Most deep-learning engineers use a handful of fixed layer
    types (for instance, convolution, pooling, or dense, as you will learn in detail
    in later chapters). Rarely do they need to create new layer types. This is where
    the LEGO analogy is appropriate. With LEGOs, there are only a small number of
    block types. LEGO builders don’t need to think about what it takes to make a LEGO
    block. This is different from a toy like, say, Play-Doh, which is analogous to
    TensorFlow’s low-level API. Yet the ability to connect LEGO blocks leads to a
    combinatorially large number of possibilities and virtually infinite power. It
    is possible to build a toy house with either LEGOs or Play-Doh, but unless you
    have very special requirements for the house’s size, shape, texture, or material,
    it is much easier and faster to build it with LEGOs. For most of us, the LEGO
    house we build will stand more stably and look nicer than the Play-Doh house we’d
    make.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的核心设计是非常通用和灵活的：操作可以是任何明确定义的数学函数，不仅仅是神经网络层。例如，它们可以是低级数学操作，比如将两个张量相加和相乘——这种操作发生在神经网络层的*内部*。这使得深度学习工程师和研究人员能够为深度学习定义任意和新颖的操作。然而，对于大部分深度学习从业者来说，操作这样的低级机制比它们值得的麻烦更多。它会导致冗长和更容易出错的代码以及更长的开发周期。大多数深度学习工程师使用少数固定的层类型（例如，卷积、池化或密集层，你将在后面的章节中详细学习）。他们很少需要创建新的层类型。这就是乐高积木的类比适用的地方。使用乐高，只有少数几种积木类型。乐高建筑师不需要考虑制作一块乐高积木需要什么。这与像
    Play-Doh 这样的玩具不同，它类似于 TensorFlow 的低级 API。然而，连接乐高积木的能力导致了组合成千上万种可能性和几乎无限的力量。可以用乐高或
    Play-Doh建造一个玩具房子，但除非你对房子的大小、形状、质地或材料有非常特殊的要求，否则用乐高建造房子会更容易更快。对于大多数人来说，我们建造的乐高房子将更加稳固，看起来更漂亮，而不是用
    Play-Doh 建造的房子。
- en: In the world of TensorFlow, the LEGO equivalent is the high-level API called
    Keras.^([[9](#ch01fn9)]) Keras provides a set of the most frequently used types
    of neural network layers, each with configurable parameters. It also allows users
    to connect the layers together to form neural networks. Furthermore, Keras also
    comes with APIs for
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow的世界中，高级API被称为Keras是对应的乐高积木^([[9](#ch01fn9)])。Keras提供了一组最常用的神经网络层类型，每个层都有可配置的参数。它还允许用户将层连接在一起形成神经网络。此外，Keras还提供了以下API：
- en: ⁹
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, since the introduction of TensorFlow, a number of high-level APIs have
    emerged, some created by Google engineers and others by the open source community.
    Among the most popular ones are Keras, tf.Estimator, tf.contrib.slim, and TensorLayers.
    For the readers of this book, the most relevant high-level API to TensorFlow.js
    is Keras by far, because the high-level API of TensorFlow.js is modeled after
    Keras and because TensorFlow.js provides two-way compatibility in model saving
    and loading with Keras.
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实际上，自从引入TensorFlow以来，出现了许多高级API，一些是由谷歌工程师创建的，一些是由开源社区创建的。其中最受欢迎的是Keras、tf.Estimator、tf.contrib.slim和TensorLayers等。对于本书的读者来说，与TensorFlow.js最相关的高级API无疑是Keras，因为TensorFlow.js的高级API是基于Keras建模的，并且TensorFlow.js在模型保存和加载方面提供了双向兼容性。
- en: Specifying how the neural network will be trained (loss functions, metrics,
    and optimizers)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定神经网络的训练方式（损失函数、度量指标和优化器）
- en: Feeding data to train or evaluate the neural network or use the model for inference
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供数据用于训练或评估神经网络，或使用模型进行推断
- en: Monitoring the ongoing training process (callbacks)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控正在进行的训练过程（回调函数）
- en: Saving and loading models
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: Printing or plotting the architecture of models
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印或绘制模型的架构
- en: 'With Keras, users can perform the full deep-learning workflow with very few
    lines of code. With the flexibility of the low-level API and the usability of
    the high-level API, TensorFlow and Keras form an ecosystem that leads the field
    of deep-learning frameworks in terms of industrial and academic adoption (see
    the tweet at [http://mng.bz/vlDJ](http://mng.bz/vlDJ)). As a part of the ongoing
    deep-learning revolution, their role in making deep learning accessible to a wider
    audience should not be underestimated. Before frameworks such as TensorFlow and
    Keras, only those with CUDA programming skills and extensive experience in writing
    neural networks in C++ were able to do practical deep learning. With TensorFlow
    and Keras, it takes much less skill and effort to create GPU-accelerated deep
    neural networks. But there was one problem: it was not possible to run TensorFlow
    or Keras models in JavaScript or directly in the web browser. To serve trained
    deep-learning models in the browser, we had to do it via HTTP requests to a backend
    server. This is where TensorFlow.js comes into the picture. TensorFlow.js was
    an effort started by Nikhil Thorat and Daniel Smilkov, two experts in deep-learning-related
    data visualization and human-computer interaction^([[10](#ch01fn10)]) at Google.
    As we have mentioned, the highly popular TensorFlow Playground demo of a deep
    neural network planted the initial seed of the TensorFlow.js project. In September
    2017, a library called deeplearn.js was released that has a low-level API analogous
    to the TensorFlow low-level API. Deeplearn.js championed WebGL-accelerated neural
    network operations, making it possible to run real neural networks with low inference
    latencies in the web browser.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，用户只需使用很少的代码即可执行完整的深度学习工作流程。低级API的灵活性和高级API的易用性使得TensorFlow和Keras在工业和学术领域的应用方面领先于其他深度学习框架（请参阅[http://mng.bz/vlDJ](http://mng.bz/vlDJ)上的推文）。作为不断推进的深度学习革命的一部分，不应低估它们让更广泛的人群获得深度学习的作用。在TensorFlow和Keras等框架出现之前，只有那些具有CUDA编程技能并且在C++中有编写神经网络的丰富经验的人能够进行实际的深度学习。通过TensorFlow和Keras，创建基于GPU加速的深度神经网络所需的技能和工作量大大减少。但是有一个问题：无法在JavaScript或直接在Web浏览器中运行TensorFlow或Keras模型。为了在浏览器中提供经过训练的深度学习模型，我们必须通过HTTP请求到后端服务器进行操作。这就是TensorFlow.js的用武之地。TensorFlow.js是由Google的深度学习相关数据可视化和人机交互专家Nikhil
    Thorat和Daniel Smilkov发起的努力^([[10](#ch01fn10)])。正如我们所提到的，深度神经网络的高度流行的TensorFlow
    Playground演示植入了TensorFlow.js项目的最初种子。2017年9月，发布了一个名为deeplearn.js的库，它具有类似于TensorFlow低级API的低级API。
    Deeplearn.js支持WebGL加速的神经网络操作，使得在Web浏览器中以低延迟运行真实的神经网络成为可能。
- en: ^(10)
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an interesting historical note, these authors also played key roles in creating
    TensorBoard, the popular visualization tool for TensorFlow models.
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为一个有趣的历史注释，这些作者还在创建 TensorBoard（TensorFlow 模型的流行可视化工具）方面发挥了关键作用。
- en: Following the initial success of deeplearn.js, more members of the Google Brain
    team joined the project, and it was renamed TensorFlow.js. The JavaScript API
    underwent significant revamping, boosting API compatibility with TensorFlow. In
    addition, a Keras-like high-level API was built on top of the low-level core,
    making it much easier for users to define, train, and run deep-learning models
    in the JavaScript library. Today, what we said earlier about the power and usability
    of Keras is all true for TensorFlow.js as well. To further enhance interoperability,
    converters were built so that TensorFlow.js can import models saved from TensorFlow
    and Keras and export models to them. Since its debut at the worldwide TensorFlow
    Developer Summit and Google I/O in the spring of 2018 (see [www.youtube.com/watch?v=YB-kfeNIPCE](http://www.youtube.com/watch?v=YB-kfeNIPCE)
    and [www.youtube.com/watch?v=OmofOvMApTU](http://www.youtube.com/watch?v=OmofOvMApTU)),
    TensorFlow.js has quickly become a highly popular JavaScript deep-learning library,
    with currently the highest number of stars and forks among similar libraries on
    GitHub.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在 deeplearn.js 初始成功后，谷歌 Brain 团队的更多成员加入了该项目，并将其更名为 TensorFlow.js。JavaScript
    API 经过了重大改进，提高了与 TensorFlow 的 API 兼容性。此外，在底层核心之上构建了一个类似 Keras 的高级 API，使用户更容易在
    JavaScript 库中定义、训练和运行深度学习模型。今天，我们对于 Keras 的力量和易用性所说的一切对于 TensorFlow.js 也完全适用。为了进一步提高互操作性，构建了转换器，使
    TensorFlow.js 可以导入从 TensorFlow 和 Keras 中保存的模型，并将模型导出为它们所用的格式。自从在 2018 年春季全球 TensorFlow
    开发者峰会和 Google I/O 上首次亮相以来（参见[www.youtube.com/watch?v=YB-kfeNIPCE](http://www.youtube.com/watch?v=YB-kfeNIPCE)
    和 [www.youtube.com/watch?v=OmofOvMApTU](http://www.youtube.com/watch?v=OmofOvMApTU)），TensorFlow.js
    快速成为了一个非常受欢迎的 JavaScript 深度学习库，在 GitHub 上类似的库中当前拥有最高的赞数和派生数。
- en: '[Figure 1.8](#ch01fig08) presents an overview of the TensorFlow.js architecture.
    The lowest level is responsible for parallel computing for fast mathematical operations.
    Although this layer is not visible to most users, it is critical that it have
    high performance so that model training and inference in higher levels of the
    API can be as fast as possible. In the browser, it leverages WebGL to achieve
    GPU acceleration (see [info box 1.2](#ch01sb02)). In Node.js, direct binding to
    the multicore CPU parallelization and CUDA GPU acceleration are both available.
    These are the same math backends used by TensorFlow and Keras in Python. Built
    on top of the lowest math level is the *Ops API*, which has good parity with the
    low-level API of TensorFlow and supports loading SavedModels from TensorFlow.
    On the highest level is the Keras-like *Layers API*. The Layers API is the right
    API choice for most programmers using TensorFlow.js and will be the main focus
    of this book. The Layers API also supports two-way model importing/exporting with
    Keras.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1.8](#ch01fig08) 展示了 TensorFlow.js 的架构概述。最底层负责并行计算，用于快速数学运算。尽管大多数用户看不到此层，但它的高性能非常重要，以便在
    API 的更高层级中进行模型训练和推断尽可能地快速。在浏览器中，它利用 WebGL 实现 GPU 加速（参见[信息框 1.2](#ch01sb02)）。在
    Node.js 上，可以直接绑定到多核 CPU 并行化和 CUDA GPU 加速。这些是 TensorFlow 和 Keras 在 Python 中使用的相同的数学后端。在最低的数学层级之上建立了
    *Ops API*，它与 TensorFlow 的低级 API 具有良好的对应性，并支持从 TensorFlow 加载 SavedModels。在最高的层级上是类似
    Keras 的 *Layers API*。对于使用 TensorFlow.js 的大多数程序员来说，Layers API 是正确的 API 选择，也是本书的主要关注点。Layers
    API 还支持与 Keras 的双向模型导入/导出。'
- en: Figure 1.8\. The architecture of TensorFlow.js at a glance. Its relationship
    to Python TensorFlow and Keras is also shown.
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.8\. TensorFlow.js 的架构一览。它与 Python TensorFlow 和 Keras 的关系也显示出来。
- en: '![](01fig08_alt.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig08_alt.jpg)'
- en: '1.3.2\. Why TensorFlow.js: A brief comparison with similar libraries'
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '1.3.2\. 为什么选择 TensorFlow.js: 与类似库的简要比较'
- en: 'TensorFlow.js is not the only JavaScript library for deep learning; neither
    was it the first one to appear (for example, brain.js and ConvNetJS have a much
    longer history). So, why does TensorFlow.js stand out among similar libraries?
    The first reason is its comprehensiveness—TensorFlow.js is the only currently
    available library that supports all key parts of the production deep-learning
    workflow:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 并不是唯一一个用于深度学习的 JavaScript 库；也不是第一个出现的（例如，brain.js 和 ConvNetJS
    的历史要长得多）。那么，为什么 TensorFlow.js 在类似的库中脱颖而出呢？第一个原因是它的全面性——TensorFlow.js 是目前唯一一个支持生产深度学习工作流中所有关键部分的库：
- en: Supports both inference and training
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持推断和训练
- en: Supports web browsers and Node.js
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 Web 浏览器和 Node.js。
- en: Leverages GPU acceleration (WebGL in browsers and CUDA kernels in Node.js)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 GPU 加速（浏览器中的 WebGL 和 Node.js 中的 CUDA 核心）。
- en: Supports definition of neural network model architectures in JavaScript
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持在 JavaScript 中定义神经网络模型架构。
- en: Supports serialization and deserialization of models
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持模型的序列化和反序列化。
- en: Supports conversions to and from Python deep-learning frameworks
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持与 Python 深度学习框架之间的转换。
- en: Compatible in API with Python deep-learning frameworks
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 Python 深度学习框架的 API 兼容。
- en: Equipped with built-in support for data ingestion and with an API for visualization
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配备了内置的数据摄取支持，并提供了可视化 API。
- en: 'The second reason is the ecosystem. Most JavaScript deep-learning libraries
    define their own unique API, whereas TensorFlow.js is tightly integrated with
    TensorFlow and Keras. You have a trained model from Python TensorFlow or Keras
    and want to use it in the browser? No problem. You have created a TensorFlow.js
    model in the browser and want to take it into Keras for access to faster accelerators
    such as Google TPUs? That works, too! Tight integration with non-JavaScript frameworks
    not only boosts interoperability but also makes it easier for developers to migrate
    between the worlds of programming languages and infrastructure stacks. For example,
    once you have mastered TensorFlow.js from reading this book, it will be smooth
    sailing if you want to start using Keras in Python. The reverse journey is as
    easy: someone with good knowledge of Keras should be able to learn TensorFlow.js
    quickly (assuming sufficient JavaScript skills). Last but not least, the popularity
    of TensorFlow.js and the strength of its community should not be overlooked. The
    developers of TensorFlow.js are committed to long-term maintenance and support
    of the library. From GitHub star and fork counts to number of external contributors,
    from the liveliness of the discussion to the number of questions and answers on
    Stack Overflow, TensorFlow.js is shadowed by none of the competing libraries.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是生态系统。大多数 JavaScript 深度学习库定义了自己独特的 API，而 TensorFlow.js 与 TensorFlow 和 Keras
    紧密集成。你有一个来自 Python TensorFlow 或 Keras 的训练模型，想在浏览器中使用它？没问题。你在浏览器中创建了一个 TensorFlow.js
    模型，想将其带入 Keras 以获得更快的加速器，如 Google TPU？也可以！与非 JavaScript 框架的紧密集成不仅提升了互操作性，还使开发人员更容易在编程语言和基础设施堆栈之间迁移。例如，一旦你通过阅读本书掌握了
    TensorFlow.js，如果想开始使用 Python 中的 Keras，将会非常顺利。反向旅程同样轻松：掌握 Keras 的人应该能够快速学会 TensorFlow.js（假设具备足够的
    JavaScript 技能）。最后但同样重要的是，不应忽视 TensorFlow.js 的流行度和其社区的实力。TensorFlow.js 的开发人员致力于长期维护和支持该库。从
    GitHub 的星星和分叉数量到外部贡献者的数量，从讨论的活跃程度到在 Stack Overflow 上的问题和答案的数量，TensorFlow.js 无愧于任何竞争库的阴影。
- en: 1.3.3\. How is TensorFlow.js being used by the world?
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.3\. TensorFlow.js 在世界上是如何被使用的？
- en: 'There is no more convincing testimony to the power and popularity of a library
    than the way in which it is used in real-world applications. A few noteworthy
    applications of TensorFlow.js include the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个库的力量和流行程度，最有说服力的证明莫过于它在真实应用中的使用方式。TensorFlow.js 的几个值得注意的应用包括以下内容：
- en: Google’s Project Magenta uses TensorFlow.js to run RNNs and other kinds of deep
    neural networks to generate musical scores and novel instrument sounds in the
    browser ([https://magenta.tensorflow.org/demos/](https://magenta.tensorflow.org/demos/)).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google 的 Project Magenta 使用 TensorFlow.js 运行 RNN 和其他类型的深度神经网络，在浏览器中生成音乐乐谱和新颖的乐器声音（[https://magenta.tensorflow.org/demos/](https://magenta.tensorflow.org/demos/)）。
- en: Dan Shiffman and his colleagues at New York University built ML5.js, an easy-to-use,
    higher-level API for various out-of-the-box deep-learning models for the browser,
    such as object detection and image style transfer ([https://ml5js.org](https://ml5js.org)).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丹·希夫曼（Dan Shiffman）和他在纽约大学的同事们构建了 ML5.js，这是一个易于使用的、针对浏览器的各种开箱即用的深度学习模型的高级 API，例如目标检测和图像风格转换（[https://ml5js.org](https://ml5js.org)）。
- en: Abhishek Singh, an open source developer, created a browser-based interface
    that translates American Sign Language into speech to help people who can’t speak
    or hear use smart speakers such as Amazon Echo.^([[11](#ch01fn11)])
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源开发者 Abhishek Singh 创建了一个基于浏览器的界面，将美国手语翻译成语音，以帮助不能说话或听力受损的人使用智能扬声器，如亚马逊 Echo。^([[11](#ch01fn11)])
- en: ^(11)
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abhishek Singh, “Getting Alexa to Respond to Sign Language Using Your Webcam
    and TensorFlow.js,” *Medium*, 8 Aug. 2018, [http://mng.bz/4eEa](http://mng.bz/4eEa).
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Abhishek Singh，“使用你的网络摄像头和 TensorFlow.js 让 Alexa 响应手语”，*Medium*，2018 年 8 月 8
    日，[http://mng.bz/4eEa](http://mng.bz/4eEa)。
- en: Canvas Friends is a game-like web app based on TensorFlow.js that helps users
    improve their drawing and artistic skills ([www.y8.com/games/canvas_friends](http://www.y8.com/games/canvas_friends)).
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Canvas Friends 是基于 TensorFlow.js 的类似游戏的网络应用程序，帮助用户提高其绘画和艺术技巧 ([www.y8.com/games/canvas_friends](http://www.y8.com/games/canvas_friends))。
- en: MetaCar, a self-driving car simulator that runs in the browser, uses TensorFlow.js
    to implement reinforcement learning algorithms that are critical to its simulations
    ([www.metacar-project.com](http://www.metacar-project.com)).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetaCar，一个在浏览器中运行的自动驾驶汽车模拟器，使用 TensorFlow.js 实现了对其模拟至关重要的强化学习算法 ([www.metacar-project.com](http://www.metacar-project.com))。
- en: Clinic doctor, a Node.js-based application that monitors the performance of
    server-side programs, implemented a Hidden Markov Model with TensorFlow.js and
    is using it to detect spikes in CPU usage.^([[12](#ch01fn12)])
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊所医生，一个基于 Node.js 的应用程序，用于监视服务器端程序的性能，使用 TensorFlow.js 实现了隐马尔可夫模型，并使用它来检测 CPU
    使用率的峰值。^([[12](#ch01fn12)])
- en: ^(12)
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Andreas Madsen, “Clinic.js Doctor Just Got More Advanced with TensorFlow.js,”
    *Clinic.js* blog, 22 Aug. 2018, [http://mng.bz/Q06w](http://mng.bz/Q06w).
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Andreas Madsen，“Clinic.js Doctor Just Got More Advanced with TensorFlow.js,”
    *Clinic.js* 博客，2018年8月22日，[http://mng.bz/Q06w](http://mng.bz/Q06w)。
- en: See TensorFlow.js’s gallery of other outstanding applications built by the open
    source community at [https://github.com/tensorflow/tfjs/blob/master/GALLERY.md](https://github.com/tensorflow/tfjs/blob/master/GALLERY.md).
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看 TensorFlow.js 的优秀应用程序库，由开源社区构建，地址为 [https://github.com/tensorflow/tfjs/blob/master/GALLERY.md](https://github.com/tensorflow/tfjs/blob/master/GALLERY.md)。
- en: 1.3.4\. What this book will and will not teach you about TensorFlow.js
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.4\. 本书将教授和不会教授你关于 TensorFlow.js 的内容
- en: 'Through studying the materials in this book, you should be able to build applications
    like the following using TensorFlow.js:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习本书中的材料，您应该能够使用 TensorFlow.js 构建如下应用程序：
- en: A website that classifies images uploaded by a user
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个能够对用户上传的图像进行分类的网站
- en: Deep neural networks that ingest image and audio data from browser-attached
    sensors and perform real-time machine-learning tasks, such as recognition and
    transfer learning, on them
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络，从浏览器连接的传感器接收图像和音频数据，并在其上执行实时机器学习任务，例如识别和迁移学习
- en: Client-side natural language AI such as a comment-sentiment classifier to assist
    with comment moderation
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端自然语言人工智能，如评论情感分类器，可辅助评论审核
- en: A Node.js (backend) machine-learning model trainer that uses gigabyte-scale
    data and GPU acceleration
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用千兆字节级别数据和 GPU 加速的 Node.js（后端）机器学习模型训练器
- en: A TensorFlow.js-powered reinforcement learner that can solve small-scale control
    and game problems
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由 TensorFlow.js 提供支持的强化学习器，可以解决小规模控制和游戏问题
- en: A dashboard to illustrate the internals of trained models and the results of
    machine-learning experiments
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仪表板，用于说明经过训练的模型的内部情况和机器学习实验的结果
- en: Importantly, not only will you know how to build and run such applications,
    but you will also understand how they work. For instance, you will have practical
    knowledge of the strategies and constraints involved in creating deep-learning
    models for various types of problems, as well as the steps and gotchas in training
    and deploying such models.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，您不仅会知道如何构建和运行这些应用程序，还将了解它们的工作原理。例如，您将具有创建各种类型问题的深度学习模型所涉及的策略和约束的实际知识，以及训练和部署这些模型的步骤和技巧。
- en: 'Machine learning is a wide field; TensorFlow.js is a versatile library. Therefore,
    some applications are entirely doable with existing TensorFlow.js technology but
    are beyond what is covered in the book. Examples are:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个广泛的领域；TensorFlow.js 是一个多才多艺的库。因此，一些应用程序完全可以使用现有的 TensorFlow.js 技术来完成，但超出了本书的范围。例如：
- en: High-performance, distributed training of deep neural networks that involve
    a huge amount of data (on the order of terabytes) in the Node.js environment
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Node.js 环境中高性能、分布式训练深度神经网络，涉及大量数据（数量级为千兆字节）
- en: Non-neural-network techniques, such as SVMs, decision trees, and random forests
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非神经网络技术，例如 SVM、决策树和随机森林
- en: Advanced deep-learning applications such as text-summarization engines that
    reduce large documents into a few representative sentences, image-to-text engines
    that generate text summary from input images, and generative image models that
    enhance the resolution of input images
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级深度学习应用程序，如将大型文档缩减为几个代表性句子的文本摘要引擎，从输入图像生成文本摘要的图像到文本引擎，以及增强输入图像分辨率的生成图像模型
- en: This book will, however, give you foundational knowledge of deep learning with
    which you will be prepared to learn about the code and articles related to those
    advanced applications.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这本书将为您提供深度学习的基础知识，使您能够学习与这些高级应用相关的代码和文章。
- en: 'Like any other technology, TensorFlow.js has its limits. Some tasks are beyond
    what it can do. Even though these limits are likely to be pushed in the future,
    it is good to be aware of where the boundaries are at the time of writing:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他技术一样，TensorFlow.js 也有其局限性。有些任务超出了它的能力范围。尽管这些限制可能在将来被推动，但了解编写时的边界是很好的：
- en: Running deep-learning models with memory requirements that exceed the RAM and
    WebGL limits in a browser tab. For in-browser inference, this typically means
    a model with a total weight size above ~100 MB. For training, more memory and
    compute power is required, so it is possible that even smaller models will be
    too slow to train in a browser tab. Model training also typically involves larger
    amounts of data than inference, which is another limiting factor that should be
    taken into account when assessing the feasibility of in-browser training.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在浏览器标签页中运行内存需求超出 RAM 和 WebGL 限制的深度学习模型。对于浏览器内推断，通常意味着模型总重量超过 ~100 MB。对于训练，需要更多的内存和计算资源，因此即使是较小的模型在浏览器标签页中进行训练也可能太慢了。模型训练通常还涉及比推断更大量的数据，这是评估浏览器内训练可行性时应考虑的另一个限制因素。
- en: Creating a high-end reinforcement learner, such as the kind that can defeat
    human players at the game of Go.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个高端的强化学习器，例如能够击败人类玩家的围棋游戏。
- en: Training deep-learning models with a distributed (multimachine) setup using
    Node.js.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Node.js 进行分布式（多机器）设置来训练深度学习模型。
- en: Exercises
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'Whether you are a frontend JavaScript developer or a Node.js developer, based
    on what you learned in this chapter, brainstorm a few possible cases in which
    you can apply machine learning to the system you are working on to make it more
    intelligent. For inspiration, refer to [tables 1.1](#ch01table01) and [1.2](#ch01table02),
    as well as [section 1.3.3](#ch01lev2sec9). Some further examples include the following:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无论您是前端 JavaScript 开发人员还是 Node.js 开发人员，根据本章学到的知识，思考一下在您正在开发的系统中应用机器学习以使其更加智能的几种可能情况。可参考
    [表 1.1](#ch01table01) 和 [1.2](#ch01table02)，以及 [第 1.3.3 节](#ch01lev2sec9)。一些进一步的示例包括：
- en: A fashion website that sells accessories such as sunglasses captures images
    of users’ faces using the webcam and detects facial landmark points using a deep
    neural network running on TensorFlow.js. The detected landmarks are then used
    to synthesize an image of the sunglasses overlaid on the user’s face to simulate
    a try-on experience in the web page. The experience is realistic because the simulated
    try-on can run with low latency and at a high frame rate thanks to client-side
    inference. The user’s data privacy is respected because the captured facial image
    never leaves the browser.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个出售眼镜等配件的时尚网站使用网络摄像头捕获用户面部图像，并使用运行在 TensorFlow.js 上的深度神经网络检测面部标记点。然后利用检测到的标记点，在用户面部叠加太阳镜的图像，以在网页中模拟试戴体验。由于客户端推断可实现低延迟和高帧率运行，因此体验效果非常逼真。用户的数据隐私得到尊重，因为捕获的面部图像永远不会离开浏览器。
- en: A mobile sports app written in React Native (a cross-platform JavaScript library
    for creating native mobile apps) tracks users’ exercise. Using the HTML5 API,
    the app accesses real-time data from the phone’s gyroscope and accelerometer.
    The data is run through a TensorFlow.js-powered model that automatically detects
    the user’s current activity type (for example, resting versus walking versus jogging
    versus sprinting).
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个用 React Native（一个用于创建原生移动应用的跨平台 JavaScript 库）编写的移动体育应用程序跟踪用户的运动。使用 HTML5 API，该应用程序从手机的陀螺仪和加速度计获取实时数据。然后将数据传递给由
    TensorFlow.js 驱动的模型，该模型自动检测用户当前的活动类型（例如，休息、步行、慢跑或疾跑）。
- en: A browser extension automatically detects whether the person using the device
    is a child or an adult (by using images captured from the webcam at a frame rate
    of once per 5 seconds and a computer-vision model powered by TensorFlow.js) and
    uses the information to block or grant access to certain websites accordingly.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个浏览器扩展程序会自动检测设备使用者是儿童还是成年人（通过每 5 秒一次的摄像头捕获的图像和由 TensorFlow.js 驱动的计算机视觉模型），并根据这些信息来阻止或允许访问特定网站。
- en: A browser-based programming environment uses a recurrent neural network implemented
    with TensorFlow.js to detect typos in code comments.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于浏览器的编程环境使用TensorFlow.js实现的循环神经网络来检测代码注释中的拼写错误。
- en: A Node.js-based server-side application that coordinates a cargo logistics service
    uses real-time signals such as carrier status, cargo type and quantity, date/time,
    and traffic information to predict the estimated time of arrival (ETA) for each
    transaction. The training and inference pipelines are all written in Node.js,
    using TensorFlow.js, simplifying the server stack.
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于Node.js的服务器端应用，使用实时信号如航空公司状态、货物类型和数量、日期/时间和交通信息等来预测每个交易的预计到达时间（ETA）。所有的训练和推理流水线均使用TensorFlow.js在Node.js中编写，从而简化了服务器堆栈。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: AI is the study of automating cognitive tasks. Machine learning is a subfield
    of AI in which rules for performing a task such as image classification are discovered
    automatically by learning from examples in the training data.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI是自动化认知任务的研究。机器学习是AI的一个子领域，其中通过学习训练数据中的示例来自动发现执行任务（如图像分类）的规则。
- en: A central problem in machine learning is how to transform the original representation
    of data into a representation more amenable to solving the task.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的一个核心问题是如何将原始数据的表示转换为更适合解决任务的表示。
- en: Neural networks are an approach in machine learning wherein the transformation
    of data representation is performed by successive steps (or layers) of mathematical
    operations. The field of deep learning concerns deep neural networks— neural networks
    with many layers.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是机器学习中的一种方法，通过数学运算的不断迭代（或层级）来转换数据表示。深度学习领域则是关注于深度神经网络，即具有多层的神经网络。
- en: Thanks to enhancements in hardware, increased availability of labeled data,
    and advances in algorithms, the field of deep learning has made astonishing progress
    since the early 2010s, solving previously unsolvable problems and creating exciting
    new opportunities.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于硬件的增强、标记数据的可用性和算法的进步，自2010年代初以来，深度学习领域取得了惊人的进展，解决了以前无法解决的问题，创造了令人兴奋的新机会。
- en: JavaScript and the web browser are a suitable environment for deploying and
    training deep neural networks.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JavaScript和Web浏览器是部署和训练深度神经网络的适宜环境。
- en: TensorFlow.js, the focus of this book, is a comprehensive, versatile, and powerful
    open source library for deep learning in JavaScript.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书的重点TensorFlow.js是一款全面、多功能和强大的JavaScript深度学习开源库。
