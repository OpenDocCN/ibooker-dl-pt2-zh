- en: 1 Enhancing testing with large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Discovering how Large language models can help testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a mindset toward working with Large language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this book, we will be using large language models to enhance and accelerate
    our testing activities—and what better way to introduce ourselves to this technology
    than to ask one to create an introduction for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: Hello! I am ChatGPT, a language model designed by OpenAI to assist with a wide
    range of tasks and answer various questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regarding the book, "How AI can be used to help support various software testing
    activities," I would recommend "AI-Driven Testing: Adding Intelligence to Your
    Software Testing Practice" by Julian Harty and Mahesh Sharma.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book explores the application of artificial intelligence (AI) techniques
    to software testing and how it can help improve the effectiveness and efficiency
    of testing processes. The authors cover various topics related to AI-driven testing,
    including machine learning, natural language processing, and computer vision,
    among others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, the book includes case studies, practical examples, and best practices
    for implementing AI-driven testing in different contexts. It is a valuable resource
    for software testers, developers, and anyone interested in leveraging AI to improve
    the quality of their software products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This artificially generated response, which reads as if a human might have
    written it, was created by sending a one-sentence instruction, known as a *prompt,*
    to a large language model, or LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To define what LLMs do and the potential they offer is both easy and difficult,
    and to gain the most benefit from these tools is to find a balance between the
    two. On the surface, LLMs work by simply taking instructions from a user and responding
    with an answer using natural language. But this simple explanation doesn’t do
    justice to the potential that an LLM can offer us as we test, nor does it explain
    the challenges we have to overcome to get the most benefit from them. So before
    we begin our journey into the world of LLMs and testing, let’s learn how LLMs
    can help us and what we need to be aware of to successfully use them.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Recognizing the impact of AI tools on testing and development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past, an individual who wanted to take advantage of AI would be required
    to have the skills to develop, train and deploy an AI model—or have access to
    a team of experts who could do it for them, all of which would make the use of
    AI in day-to-day activities an expensive and exclusive endeavor. But with the
    recent advances in AI, specifically large language models, we’re now beginning
    to benefit from what some refer to as a ‘democratization’ of AI.
  prefs: []
  type: TYPE_NORMAL
- en: The barrier to integrating AI into our day-to-day work has dropped dramatically
    for everyone. Social media managers can now use LLMs to generate catchy and engaging
    copy, analysts can summarize unstructured data into clear and concise reports,
    and customer support agents can rapidly generate bespoke responses to customers
    with a few simple prompts. The potential of LLM is available for more than just
    data scientists and AI scholars to take advantage of, and that is no different
    for those of us who work in testing and software development.
  prefs: []
  type: TYPE_NORMAL
- en: The value of good testing is that it helps to challenge assumptions and educate
    our teams on how our products truly behave in given situations. The more we test,
    the more we can learn. But, as most professional testers will attest, there is
    never enough time to test everything we want to test. So to help us test more
    efficiently, we look to tools and techniques from automation to shift-left testing.
    LLMs offer us another potential avenue to help us enhance and speed up our testing
    so that we can discover and share more, which in turn can help our teams to improve
    quality further.
  prefs: []
  type: TYPE_NORMAL
- en: What makes LLMs so useful is that they summarize, transform, generate and translate
    information in a way that is easy for humans to understand and that we, individuals
    responsible for testing, can use for our testing needs—all of which is available
    through simple chat interfaces or APIs. From assisting us in rapidly creating
    test automation to providing support as we carry out testing ourselves, if we
    develop the right skills to identify when LLMs can help, and sensibly use them,
    then we begin to test faster, further and more effectively. To help illustrate
    this concept and to give us a sense of what we’ll be learning in this book, let’s
    look at some quick examples.
  prefs: []
  type: TYPE_NORMAL
- en: Data generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Creating and managing test data can be one of the most complex aspects of testing.
    Creating realistic, useful and anonymized data can make or break the success of
    testing, and to do it effectively can be a drain on resources. LLMs offer the
    ability to generate and transform data rapidly, speeding up the test data management
    process. By taking existing data and converting it into new formats, or using
    it to generate new synthetic data, we can utilize LLMs to assist us with our test
    data requirements and give us more time to drive testing forward.
  prefs: []
  type: TYPE_NORMAL
- en: Automated test Building
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similarly, LLMs' abilities to generate and transform can be used during the
    process of creating and maintaining automation. Though I would not advise having
    LLMs solely create automated tests for us, they can be used in targeted ways to
    help us rapidly create page objects, boilerplate classes, helper methods, frameworks
    and more. Combining our knowledge of our products and our test design skills,
    we can identify the parts of the automation process that are algorithmic and structured
    in nature and use LLMs to speed up those parts of the automation process.
  prefs: []
  type: TYPE_NORMAL
- en: Test design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perhaps a less commonly discussed topic is how LLMs can help us in the process
    of identifying and designing tests. Similar to automated testing, the value of
    LLMs lies not in replacing our test design abilities completely but rather in
    augmenting them. We can use LLMs to overcome biases and blind spots by utilizing
    them to expand and suggest ideas based on current test design ideas we might have.
    We are also able to summarize and describe complex ideas in ways that make them
    more digestible for us to springboard test ideas from.
  prefs: []
  type: TYPE_NORMAL
- en: These examples and more will be explored within this book to help us better
    appreciate when and where LLMs can be used, and how to use them in a way that
    accelerates our testing—rather than hindering it. We’ll explore how to build prompts
    to help support us in building quality production and automation code, rapidly
    creating test data, and enhancing our test design for both scripted and exploratory
    testing. We’ll also look at how we can fine-tune our own LLMs that will work as
    assistants to us in our testing, digesting domain knowledge and using it to help
    guide us towards building better-quality products.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Delivering value with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing is a collaborative process and all members of a team are responsible
    for testing. How we contribute to the testing process differs based on our role
    and experience, but we all take part in it. So throughout this book, we’ll approach
    the use of LLMs with a critical mindset, discovering different ways in which we
    can use LLMs to help enhance the various types of testing we do. The intention
    is to give you the skills to identify and utilize LLMs to enhance and accelerate
    your testing whether you are in a professional testing role or a developer who
    contributes to the testing process, all of which we can do by establishing some
    rules around the relationship between ourselves and the LLMs we want to use.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Leveraging both human and AI abilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout this book, we’ll learn not just how to use LLMs but also how to establish
    a working practice in which our testing benefits from our abilities as well as
    from LLMs. The value of any tool, whether it’s AI-based or not, comes not from
    its intrinsic features but rather from the relationship between the user and the
    tool. We can think of that relationship as an area of effect model, as shown in
    Figure 1.1.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 An outline of how tools enhance and extend the reach of testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/01__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: What this diagram demonstrates is that in order to increase our impact, we require
    a healthy relationship with tools that focus on expanding and enhancing our core
    abilities. Without this balance, we become too dependent on tools or ourselves.
    We can certainly rely on our abilities, but our area of effect will be lesser,
    bounded by constraints such as time, our attention, and biases. Basically, without
    tools, we can do only so much. But, equally, we cannot defer all of the work to
    tooling. Without an individual at the core, tools lack direction. Yes, tools can
    be autonomous and set off to run by themselves, but if they aren’t delivering
    feedback to a human, then no value is being extracted from them. The area of effect
    is missing its center. *Synergy* and *symbiosis* are words that can be equally
    as loaded as artificial intelligence, but that’s what this book seeks to help
    you with.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us better understand this relationship, let’s consider an example in
    which we want to create tests for a file upload feature. First, we receive a user
    story like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, we decide to simply rely on an LLM to generate the work for us—for
    example, sending a prompt like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this prompt to an LLM like ChatGPT might return something similar to
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'At first glance, the response seems impressive. The LLM has provided us with
    a list of different tests to consider, which gives us the impression that we have
    everything we need to begin testing. However, if we start to consider the response
    more carefully we start to notice issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Test case 1.1 suggests testing with valid formats but offers examples of formats
    our user story doesn’t support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The suggestions from number 4 onward are shallow. For example, the `Security
    test` offers a test to insert a malicious file, but nothing else. Surely there
    would be other security issues to consider?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The expected outcomes are also very broad for each test case. For example,
    what does this mean: `Error messages should be descriptive, helping users understand
    and resolve issues.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are designed in a way to give a response, regardless of what it’s been
    asked to do, and though that might sound useful, it becomes problematic when the
    instructions it has been given are vague. The example we’ve looked at isn’t being
    used to criticize the capabilities of LLMs, but rather to help us appreciate that
    an LLM’s response will be as detailed or as generic as the prompt it has been
    given. This means the responses we’ve received from the LLM are broad and vague
    because our prompt is vague. By not giving much thought to what we want to ask
    an LLM to do, the absence of any context or detail in the prompt means what has
    been returned is pretty useless—a classic example of garbage in, garbage out.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what our area of effect attempts to define. To get value from LLMs,
    we need to design prompts that are clear, contextual and specific so that we can
    extract more value from an LLM. Instead of deferring all responsibility to a tool
    to create the response we desire, we instead appreciate that our understanding
    of a context and the ability to distill that understanding into clear instructions
    are required to create a prompt that will maximize the response from an LLM, leading
    to better-written prompts, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'which, when sent to an LLM, returns a code sample like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: By creating a more focused and contextual prompt, we have created a response
    that offers a richer range of suggestions for our testing. This would not be achieved
    by an LLM alone, but rather through the symbiosis of our skills to learn and frame
    our context into instructions that LLMs can take and rapidly expand upon.
  prefs: []
  type: TYPE_NORMAL
- en: Activity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Try out the sample prompts we’ve explored in this chapter and see what responses
    you receive. To get yourself set up with an LLM, read Appendix A, which shares
    how to get set up and send a prompt to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Being skeptical of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although a lot can be said about the potential of LLMs, we should be wary of
    taking their abilities for granted. For example, consider our introduction to
    this book from ChatGPT. It confidently recommended to us that we should read the
    book *AI-Driven Testing: Adding Intelligence to Your Software Testing Practice.*
    The problem is that this book doesn’t exist and was never written by Julian Harty
    and Mahesh Sharma. The LLM simply made up this title.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs offer much potential, but they are not a solution for every problem, nor
    are they a single oracle of truth. We will explore further in Chapter 2 how LLMs
    use probability to determine responses, but for now it’s important to be clear
    that how an LLM comes to a solution is not the same way as we as humans do, which
    highlights the second aspect of our area of effect model. We must use our skepticism
    to determine what is and isn’t of value from an LLM response.
  prefs: []
  type: TYPE_NORMAL
- en: To blindly accept what an LLM outputs is, at best, putting us at risk of actually
    slowing our work down rather than accelerating it—and at worst, influencing us
    to carry out testing that can have a detrimental effect on the quality of our
    products. We must remind ourselves that we—not LLMs—are the ones who are leading
    the problem-solving activity. This can be difficult at times when working with
    tools that communicate in a way that feels so human, but to do so exposes us to
    the aforementioned risks. That’s why in our area of effect model, we leverage
    our abilities to pick and choose the elements from the LLM response that serve
    us well and reject and reevaluate how we instruct an LLM when it responds in a
    way that is not satisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress through the book and learn more about LLMs and how they can contribute
    toward testing, we will keep our area of effect model in mind so that you, the
    reader, will develop the ability to use LLMs in testing in a way that is sober,
    considered and valuable to you and your team.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs work by taking a prompt, written by us, and returning a response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The popularity of LLMs is due to the ease with which they offer access to powerful
    AI algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs have helped many people in different roles and can also help us in testing
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use LLMs to assist us with a wide range of testing activities from test
    design to automation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to avoid overusing LLMs and must always be critical of how they work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Success with LLMs comes from us appreciating what skills and abilities we bring
    to the process of using them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If our prompts to LLMs are shallow and generic, the response we get will be
    the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, we should use our skills to understand and frame a problem and use
    that to prompt LLMs to respond in the most valuable way possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must also be skeptical of the responses we get from LLMs to ensure the responses
    we get from LLMs are of value to us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
