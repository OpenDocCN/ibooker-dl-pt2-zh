["```py\n>>> re.split(r'[!.?]+[\\s$]+',\n...     \"Hello World.... Are you there?!?! I'm going to Mars!\")\n['Hello World', 'Are you there', \"I'm going to Mars!\"]\n```", "```py\n>>> re.split(\n...    r'[!.?]+[\\s$]+',\n...    \"The author wrote \\\"'It isn't conscious.' Turing said.\\\"\")\n['The author wrote \"\\'It isn\\'t conscious.\\' Turing said.\"']\n```", "```py\n>>> re.split(r'(?<!\\d)\\.|\\.(?!\\d)', \"I went to GT.You?\")\n['I went to GT', 'You?']\n```", "```py\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_md')\n>>> doc = nlp(\"Are you an M.D. Dr. Gebru? either way you are brilliant.\")\n>>> [s.text for s in doc.sents]\n['Are you an M.D. Dr. Gebru?', 'either way you are brilliant.']\n```", "```py\n>>> from nlpia2.text_processing.extractors import extract_lines\n>>> t0 = time.time(); lines = extract_lines(\n...     9, nlp=nlp); t1=time.time()  # #1\n>>> t1 - t0\n15.98...\n>>> t0 = time.time(); lines = extract_lines(9, nlp=None); t1=time.time()\n>>> t1 - t0\n0.022...\n```", "```py\n>>> nlp.pipeline\n[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x...>),\n ('tagger', <spacy.pipeline.tagger.Tagger at 0x7...>),\n ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x...>),\n ('attribute_ruler',\n  <spacy.pipeline.attributeruler.AttributeRuler at 0x...>),\n ('lemmatizer',\n  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x...>),\n ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x...>)]\n>>> nlp = spacy.load(\"en_core_web_md\", exclude=[\n...    'tok2vec', 'parser', 'lemmatizer',  # #1\n...    'ner', 'tagger', 'attribute_ruler'])\n>>> nlp.pipeline  # #2\n[]\n```", "```py\n>>> nlp.enable_pipe('senter')\n>>> nlp.pipeline\n[('senter', <spacy.pipeline.senter.SentenceRecognizer at 0x...>)]\n>>> t0 = time.time(); lines2 = extract_lines(nlp=nlp); t1=time.time()\n>>> t1 - t0\n2.3...\n```", "```py\n>>> df_md = pd.DataFrame(lines)  # #1\n>>> df_fast = pd.DataFrame(lines2)  # #2\n>>> (df_md['sents_spacy'][df_md.is_body]\n...  == df_fast['sents_spacy'][df_fast.is_body]\n...  ).sum() / df_md.is_body.sum()\n0.93\n```", "```py\n>>> df_md['sents_spacy'][df_md.is_body]\n37               [_Transformers_ are changing the world.]\n                              ...\n\n>>> df_fast['sents_spacy'][df_fast.is_body]\n37             [_, Transformers_ are changing the world.]\n                              ...\n```", "```py\n>>> import pandas as pd\n>>> url = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main/'\n>>> url += 'src/nlpia2/data/nlpia_lines.csv'  # #1\n>>> df = pd.read_csv(url, index_col=0)\n>>> df9 = df[df.chapter == 9].copy()\n>>> df9.shape\n(2028, 24)\n```", "```py\n>>> pd.options.display.max_colwidth=25\n>>> df9[['text', 'is_title', 'is_body', 'is_bullet']]\n                           text  is_title  is_body  is_bullet\n19057  = Stackable deep lear...      True    False      False\n...                         ...       ...      ...        ...\n21080  * By keeping the inpu...     False    False       True\n21081  * Transformers combin...     False    False       True\n21082  * The GPT transformer...     False    False       True\n21083  * Despite being more ...     False    False       True\n21084  * If you chose a pret...     False    False       True\n```", "```py\n>>> texts = df9.text[df9.is_body]\n>>> texts.shape\n(672,)\n>>> from sentence_transformers import SentenceTransformer\n>>> minibert = SentenceTransformer('all-MiniLM-L12-v2')\n>>> vecs = minibert.encode(list(texts))\n>>> vecs.shape\n(672, 384)\n```", "```py\n>>> from numpy.linalg import norm\n>>> dfe = pd.DataFrame([list(v / norm(v)) for v in vecs])\n>>> cos_sim = dfe.values.dot(dfe.values.T)\n>>> cos_sim.shape\n(672, 672)\n```", "```py\n>>> labels = list(texts.str[:14].values)\n>>> cos_sim = pd.DataFrame(cos_sim, columns=labels, index=labels)\n                This chapter c  _Transformers_  ...  A personalized\nThis chapter c        1.000000        0.187846  ...        0.073603\n_Transformers_        0.187846        1.000000  ...       -0.010858\nThe increased         0.149517        0.735687  ...        0.064736\n...                        ...             ...  ...             ...\nSo here's a qu 0.124551 0.151740 ... 0.418388\nAnd even if yo        0.093767        0.080934  ...        0.522452\nA personalized        0.073603       -0.010858  ...        1.000000\n```", "```py\n>>> import seaborn as sns\n>>> from matplotlib import pyplot as plt\n>>> sns.heatmap(cos_sim)\n<Axes: >\n>>> plt.xticks(rotation=-35, ha='left')\n>>> plt.show(block=False)\n```", "```py\n>>> from nlpia2 import wikipedia as wiki\n>>> page = wiki.page('Timnit Gebru')\n>>> text = page.content\n```", "```py\n>>> i1 = text.index('Stochastic')\n>>> text[i1:i1+51]\n'Stochastic Parrots: Can Language Models Be Too Big?'\n```", "```py\n>>> import re\n>>> lat = r'([-]?[0-9]?[0-9][.][0-9]{2,10})'\n>>> lon = r'([-]?1?[0-9]?[0-9][.][0-9]{2,10})'\n>>> sep = r'[,/ ]{1,3}'\n>>> re_gps = re.compile(lat + sep + lon)\n\n>>> re_gps.findall('http://...maps/@34.0551066,-118.2496763...')\n[(34.0551066, -118.2496763)]\n\n>>> re_gps.findall(\"https://www.openstreetmap.org/#map=10/5.9666/116.0566\")\n[('5.9666', '116.0566')]\n\n>>> re_gps.findall(\"Zig Zag Cafe is at 45.344, -121.9431 on my GPS.\")\n[('45.3440', '-121.9431')]\n```", "```py\n>>> doc = nlp(text)\n>>> doc.ents[:6]  # #1\n(Timnit Gebru, Amharic, 13, May 1983, Ethiopian, Black in AI)\n```", "```py\n>>> first_sentence = list(doc.sents)[0]\n>>> '  '.join(['{}_{}'.format(tok, tok.pos_) for tok in first_sentence])\n 'Timnit_PROPN Gebru_PROPN (_PUNCT Amharic_PROPN :_PUNCT á‰µáˆáŠ’á‰µ_NOUN áŒˆá‰¥áˆ©_ADV\n ;_PUNCT Tigrinya_PROPN :_PUNCT  _SPACE á‰µáˆáŠ’á‰µ_NOUN áŒˆá‰¥áˆ©_PROPN )_PUNCT\n born_VERB 13_NUM May_PROPN 1983_NUM is_AUX an_DET Eritrean_ADJ\n Ethiopian_PROPN -_PUNCT born_VERB computer_NOUN scientist_NOUN\n who_PRON works_VERB on_ADP algorithmic_ADJ bias_NOUN and_CCONJ\n data_NOUN mining_NOUN ._PUNCT'\n```", "```py\n>>> spacy.explain('CCONJ')\n'coordinating conjunction'\n```", "```py\n>>> '  '.join(['{}_{}'.format(tok, tok.tag_) for tok in first_sentence])\n'Timnit_NNP Gebru_NNP (_-LRB- Amharic_NNP :_: á‰µáˆáŠ’á‰µ_NN áŒˆá‰¥áˆ©_RB ;_:\n Tigrinya_NNP :_:  __SP á‰µáˆáŠ’á‰µ_NN áŒˆá‰¥áˆ©_NNP )_-RRB- born_VBN 13_CD\n May_NNP 1983_CD is_VBZ an_DT Eritrean_JJ Ethiopian_NNP -_HYPH\n born_VBN computer_NN scientist_NN who_WP works_VBZ on_IN\n algorithmic_JJ bias_NN and_CC data_NNS mining_NN ._.'\n```", "```py\n>>> spacy.explain('VBZ')\n'verb, 3rd person singular present'\n```", "```py\n>>> import pandas as pd\n>>> def token_dict(token):\n...    return dict(TOK=token.text,\n...        POS=token.pos_, TAG=token.tag_,\n...        ENT_TYPE=token.ent_type_, DEP=token.dep_,\n...        children=[c for c in token.children])\n>>> token_dict(doc[0])\n{'TOK': 'Gebru', 'POS': 'PROPN', 'TAG': 'NNP',\n 'ENT_TYPE': 'PERSON', 'DEP': 'nsubjpass', 'children': []}\n```", "```py\n>>> def doc2df(doc):\n...    return pd.DataFrame([token_dict(tok) for tok in doc])\n>>> pd.options.display.max_colwidth=20\n>>> doc2df(doc)\n            TOK    POS    TAG ENT_TYPE       DEP\n0        Timnit  PROPN    NNP           compound\n1         Gebru  PROPN    NNP              nsubj  # #1\n2             (  PUNCT  -LRB-              punct\n3       Amharic  PROPN    NNP              appos\n         ...    ...    ...      ...       ...\n3277     Timnit  PROPN    NNP      ORG  compound  # #2\n3278      Gebru  PROPN    NNP      ORG      pobj\n3279         at    ADP     IN               prep\n3280  Wikimedia  PROPN    NNP      FAC  compound  # #3\n3281    Commons  PROPN    NNP      FAC      pobj\n```", "```py\n>>> nlp = spacy.load('en_core_web_lg')\n>>> doc = nlp(text)\n>>> doc2df(doc)\n            TOK    POS    TAG ENT_TYPE       DEP\n0        Timnit  PROPN    NNP   PERSON  compound\n1         Gebru  PROPN    NNP   PERSON     nsubj\n2             (  PUNCT  -LRB-              punct\n3       Amharic  PROPN    NNP     NORP     appos\n4             :  PUNCT      :              punct\n         ...    ...    ...      ...       ...\n3278     Timnit  PROPN    NNP   PERSON  compound\n3279      Gebru  PROPN    NNP   PERSON      pobj\n3280         at    ADP     IN               prep\n3281  Wikimedia  PROPN    NNP      ORG  compound\n3282    Commons  PROPN    NNP      ORG      pobj\n```", "```py\n>>> i0 = text.index('In a six')\n>>> text_gebru = text[i0:i0+308]\n>>> text_gebru\n\"In a six-page mail sent to an internal collaboration list, Gebru \\ describes how she was summoned to a meeting at short notice where \\ she was asked to withdraw the paper and she requested to know the \\ names and reasons of everyone who made that decision, along with \\ advice for how to revise it to Google's liking.\"\n```", "```py\n>>> !python -m spacy.cli download 'en_core_web_trf'  # #1\n>>> import spacy, coreferee  # #2\n>>> nlptrf = spacy.load('en_core_web_trf')\n>>> nlptrf.add_pipe('coreferee')\n<coreferee.manager.CorefereeBroker at 0x...>\n>>> doc_gebru = nlptrf(text_gebru)\n>>> doc_gebru._.coref_chains\n[0: [13], [16], [26], [34], 1: [51], [56]]\n>>> doc_gebru._.coref_chains.print()\n0: Gebru(13), she(16), she(26), she(34)  # #3\n1: advice(51), it(56)\n```", "```py\n>>> text = \"Gebru was unethically fired from her Ethical AI team.\"\n>>> doc = nlp(text)\n>>> doc2df(doc)\n           TOK    POS   TAG ENT_TYPE        DEP\n0        Gebru  PROPN   NNP   PERSON  nsubjpass\n1          was    AUX   VBD             auxpass\n2  unethically    ADV    RB              advmod\n3        fired   VERB   VBN                ROOT\n4         from    ADP    IN                prep\n5          her   PRON  PRP$                poss\n6      Ethical  PROPN   NNP      ORG   compound\n7           AI  PROPN   NNP      ORG   compound\n8         team   NOUN    NN                pobj\n9            .  PUNCT     .               punct\n```", "```py\n>>> def token_dict2(token):\n...    d = token_dict(token)\n...    d['children'] = list(token.children)  # #1\n...    return d\n>>> token_dict2(doc[0])\nOrderedDict([('TOK', 'Gebru'),\n             ('POS', 'PROPN'),\n             ('TAG', 'NNP'),\n             ('ENT_TYPE', 'PERSON'),\n             ('DEP', 'nsubjpass'),\n             ('children', [])])\n```", "```py\n>>> def doc2df(doc):\n...     df = pd.DataFrame([token_dict2(t) for t in doc])\n...     return df.set_index('TOK')\n>>> doc2df(doc)\n               POS   TAG ENT_TYPE        DEP             children\nTOK\nGebru        PROPN   NNP   PERSON  nsubjpass                   []\nwas            AUX   VBD             auxpass                   []\nunethically    ADV    RB              advmod                   []\nfired         VERB   VBN                ROOT  [Gebru, was, une...\nfrom           ADP    IN                prep               [team]\nher           PRON  PRP$                poss                   []\nEthical      PROPN   NNP      ORG   compound                   []\nAI           PROPN   NNP      ORG   compound            [Ethical]\nteam          NOUN    NN                pobj            [her, AI]\n.            PUNCT     .               punct                   []\n```", "```py\n>>> doc2df(doc)['children']['fired']\n[Gebru, was, unethically, from, .]\n```", "```py\n>>> from spacy.displacy import render\n>>> sentence = \"In 1541 Desoto wrote in his journal about the Pascagoula.\"\n>>> parsed_sent = nlp(sentence)\n>>> with open('pascagoula.html', 'w') as f:\n...     f.write(render(docs=parsed_sent, page=True, options=dict(compact=True)))\n```", "```py\n>>> import benepar\n>>> benepar.download('benepar_en3')\n```", "```py\n>>> import spacy\n>>> nlp = spacy.load(\"en_core_web_md\")\n>>> if spacy.__version__.startswith('2'):\n...     nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n... else:\n...     nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n>>> doc = nlp(\"She and five others coauthored a research paper,'On the\n      Dangers of Stochastic Parrots:  Can Language Models Be Too Big?'\")\n>>> sent = list(doc.sents)[0]\n>>> print(sent._.parse_string)\n(S (NP (NP (PRP She)) (CC and) (NP (CD five) (NNS others))) (VP (VBD coauthored) (NP (NP (DT a) (NN research) (NN paper)) (, ,) (`` ') (PP (IN On) (NP (NP (DT the) (NNS Dangers)) (PP (IN of) (NP (NNP Stochastic) (NNPS Parrots))))) (: :) (MD Can) (NP (NN Language) (NNS Models)) (VP (VB Be) (ADJP (RB Too) (JJ Big))))) (. ?) (''  '))\n```", "```py\n('Stanislav Petrov', 'is-a', 'lieutenant colonel')\n```", "```py\n>>> doc_dataframe(nlp(\"In 1541 Desoto met the Pascagoula.\"))\n         ORTH       LEMMA    POS  TAG    DEP\n0          In          in    ADP   IN   prep\n1        1541        1541    NUM   CD   pobj\n2      Desoto      desoto  PROPN  NNP  nsubj\n3         met        meet   VERB  VBD   ROOT\n4         the         the    DET   DT    det\n5  Pascagoula  pascagoula  PROPN  NNP   dobj\n6           .           .  PUNCT    .  punct\n```", "```py\n'PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN'\n```", "```py\n>>> pattern = [\n...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'},\n...     {'IS_ALPHA': True, 'OP': '*'},\n...     {'LEMMA': 'meet'},\n...     {'IS_ALPHA': True, 'OP': '*'},\n...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'}]\n```", "```py\n>>> from spacy.matcher import Matcher\n>>> doc = nlp(\"In 1541 Desoto met the Pascagoula.\")\n>>> matcher = Matcher(nlp.vocab)\n>>> matcher.add(\n...     key='met',\n...     patterns=[pattern])\n>>> matches = matcher(doc)\n>>> matches\n[(12280034159272152371, 2, 6)]  # #1\n>>> start = matches[0][1]\n>>> stop = matches[0][2]\n>>> doc[start:stop]  # #2\nDesoto met the Pascagoula\n```", "```py\n>>> doc = nlp(\"October 24: Lewis and Clark met their\" \\\n...     \"first Mandan Chief, Big White.\")\n>>> m = matcher(doc)[0]\n>>> m\n(12280034159272152371, 3, 11)\n\n>>> doc[m[1]:m[2]]\nLewis and Clark met their first Mandan Chief\n\n>>> doc = nlp(\"On 11 October 1986, Gorbachev and Reagan met at HÃ¶fÃ°i house\")\n>>> matcher(doc)\n[]  # #1\n```", "```py\n>>> doc = nlp(\n...     \"On 11 October 1986, Gorbachev and Reagan met at Hofoi house\"\n...     )\n>>> pattern = [\n...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'},\n...     {'LEMMA': 'and'},\n...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'},\n...     {'IS_ALPHA': True, 'OP': '*'},\n...     {'LEMMA': 'meet'}\n...     ]\n>>> matcher.add('met', None, pattern)  # #1\n>>> matches = matcher(doc)\n>>> pd.DataFrame(matches, columns=)\n[(1433..., 5, 9),\n (1433..., 5, 11),\n (1433..., 7, 11),\n (1433..., 5, 12)]  # #2\n\n>>> doc[m[-1][1]:m[-1][2]]  # #3\nGorbachev and Reagan met at Hofoi house\n```", "```py\n>>> import pandas as pd\n>>> pd.options.display.max_colwidth = 20\n>>> from nlpia2.nell import read_nell_tsv, simplify_names\n>>> df = read_nell_tsv(nrows=1000)\n>>> df[df.columns[:4]].head()\n                entity            relation                value iteration\n0  concept:biotechc...     generalizations  concept:biotechc...      1103\n1  concept:company:...  concept:companyceo  concept:ceo:lesl...      1115\n2  concept:company:...     generalizations  concept:retailstore      1097\n3  concept:company:...     generalizations      concept:company      1104\n4  concept:biotechc...     generalizations  concept:biotechc...      1095\n```", "```py\n>>> pd.options.display.max_colwidth = 40\n>>> df['entity'].str.split(':').str[1:].str.join(':')\n0        biotechcompany:aspect_medical_systems\n1                       company:limited_brands\n2                       company:limited_brands\n3                       company:limited_brands\n4                biotechcompany:calavo_growers\n                        ...\n>>> df['entity'].str.split(':').str[-1]\n0        aspect_medical_systems\n1                limited_brands\n2                limited_brands\n3                limited_brands\n4                calavo_growers\n                 ...\n```", "```py\n>>> df = simplify_names(df)  # #1\n>>> df[df.columns[[0, 1, 2, 4]]].head()\n                   entity relation           value   prob\n0  aspect_medical_systems     is_a  biotechcompany  0.924\n1          limited_brands      ceo   leslie_wexner  0.938\n2          limited_brands     is_a     retailstore  0.990\n3          limited_brands     is_a         company  1.000\n4          calavo_growers     is_a  biotechcompany  0.983\n```", "```py\n>>> islatlon = df['relation'] == 'latlon'\n>>> df[islatlon].head()\n               entity relation                 value\n241          cheveron   latlon      40.4459,-79.9577\n528        licancabur   latlon   -22.83333,-67.88333\n1817             tacl   latlon     13.53333,37.48333\n2967            okmok   latlon  53.448195,-168.15472\n2975  redoubt_volcano   latlon   60.48528,-152.74306\n```", "```py\n>>> def get_wikidata_qid(wikiarticle, wikisite=\"enwiki\"):\n...     WIKIDATA_URL='https://www.wikidata.org/w/api.php'\n...     resp = requests.get(WIKIDATA_URL, timeout=5, params={\n...         'action': 'wbgetentities',\n...         'titles': wikiarticle,\n...         'sites': wikisite,\n...         'props': '',\n...         'format': 'json'\n...     }).json()\n...     return list(resp['entities'])[0]\n\n>>> tg_qid = get_wikidata_qid('Timnit Gebru')\n>>> tg_qid\n'Q59753117'\n```", "```py\n>>> NOTABLE_WORK_PID = 'P800'     # #1\n>>> INSTANCE_OF_PID = 'P31'       # #2\n>>> SCH_ARTICLE_QID= 'Q13442814'  # #3\n>>> query = f\"\"\"  ... SELECT ?article WHERE {{  ... wd:{tg_qid} wdt:{NOTABLE_WORK_PID} ?article.  ... ?article wdt:{INSTANCE_OF_PID} wd:Q13442814.  ...  ... SERVICE wikibase:label {{ bd:serviceParam  ... wikibase:language \"en\". }}  ... }}  ... \"\"\"\n```", "```py\n>>> from SPARQLWrapper import SPARQLWrapper, JSON\n>>>\n>>> endpoint_url = \"https://query.wikidata.org/sparql\"\n>>> sparql = SPARQLWrapper(endpoint_url)\n>>> sparql.setReturnFormat(JSON)  # #1\n```", "```py\n>>> sparql.setQuery(query)\n>>> result = sparql.queryAndConvert()\n>>> result\n{'head': {'vars': ['article', 'articleLabel']},\n 'results': {'bindings': [{'article': {'type': 'uri',\n     'value': 'http://www.wikidata.org/entity/Q105943036'},\n    'articleLabel': {'xml:lang': 'en',\n     'type': 'literal',\n     'value': 'On the Dangers of Stochastic Parrots:\n     Can Language Models Be Too Big?ðŸ¦œ'}}]}}\n```", "```py\n>>> import re\n>>> uri = result['results']['bindings'][0]['article']['value']\n>>> match_id = re.search(r'entity/(Q\\d+)', uri)\n>>> article_qid = match_id.group(1)\n>>> AUTHOR_PID = 'P50'\n>>>\n>>> query = f\"\"\"  ... SELECT ?author ?authorLabel WHERE {{  ... wd:{article_qid} wdt:{AUTHOR_PID} ?author.  ... SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}  ... }}  ... \"\"\"\n>>> sparql.setQuery(query)\n>>> result = sparql.queryAndConvert()['results']['bindings']\n>>> authors = [record['authorLabel']['value'] for record in result]\n>>> authors\n['Timnit Gebru', 'Margaret Mitchell', 'Emily M. Bender']\n```", "```py\n>>> query = \"\"\"  ... SELECT ?author ?authorLabel WHERE {  ... {  ... SELECT ?article WHERE {  ... wd:Q59753117 wdt:P800 ?article.  ... ?article wdt:P31 wd:Q13442814.  ... }  ... }  ... ?article wdt:P50 ?author.  ... SERVICE wikibase:label {  ... bd:serviceParam wikibase:language \"en\".  ... }  ... }  ... \"\"\"\n```"]