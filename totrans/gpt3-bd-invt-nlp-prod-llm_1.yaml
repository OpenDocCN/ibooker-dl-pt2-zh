- en: Chapter 1\. The Era of Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “art is the debris from the collision between the soul and the world” [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “technology is now the myth of the modern world” [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “revolutions begin with a question, but do not end with an answer” [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “nature decorates the world with variety” [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Imagine waking up to a beautiful, sunny morning. It’s Monday and you know the
    week will be hectic. Your company is about to launch a new personal productivity
    app, Taskr, and start a social media campaign to let the world know about your
    ingenious product.
  prefs: []
  type: TYPE_NORMAL
- en: Your main task this week is to write and publish a series of engaging blog posts.
  prefs: []
  type: TYPE_NORMAL
- en: 'You start by making a to-do list:'
  prefs: []
  type: TYPE_NORMAL
- en: Write an informative and fun article about productivity hacks, including Taskr.
    Keep it under five hundred words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a list of five catchy article titles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the visuals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You hit Enter, take a sip of coffee, and watch an article weave itself together
    on your screen, sentence by sentence, paragraph by paragraph. In 30 seconds, you
    have a meaningful, high-quality blog post, a perfect starter for your social media
    series. The visual is fun and attention-grabbing. It’s done! You choose the best
    title and begin the publishing process.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a distant, futuristic fantasy, but a glimpse of the new reality
    made possible by advancements in AI. As we write this book, many such applications
    are being created and deployed to a wider audience.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 is a cutting-edge language model created by OpenAI, a company on the frontier
    of artificial intelligence R&D. OpenAI’s [research paper](https://oreil.ly/PGz0O)
    announcing GPT-3 was released in May 2020, followed by a launch of access to GPT-3
    via the [OpenAI API](https://oreil.ly/I8Bla) in June 2020\. Since the GPT-3 release,
    people around the world from different backgrounds, including technology, art,
    literature, marketing, etc., have already found hundreds of exciting applications
    for the model that have the potential to elevate the ways we communicate, learn,
    and play.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 is capable of performing general language-based tasks, like generating
    and classifying text, with unprecedented ease, moving freely between different
    text styles and purposes. The array of problems it can solve is vast.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we invite you to think of what problems you might solve with GPT-3
    yourself. We’ll show you what it is and how to use it, but first we want to give
    you a bit of context. The rest of this chapter will discuss where this technology
    comes from, how it is built, what tasks it excels at, and the potential risks
    associated with it. Let’s dive right in by looking at the field of natural language
    processing (NLP) and how large language models (LLMs) and GPT-3 fit into it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Natural Language Processing: Under the Hood'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Natural language processing* is a subfield of linguistics, computer science,
    and artificial intelligence concerned with interaction between computer and human
    language. The goal of NLP is to build systems capable of processing human language.
    *Natural language* refers to the way humans communicate with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: NLP combines the field of computational linguistics (rule-based modeling of
    human language) with machine learning to create intelligent machines capable of
    identifying the context and understanding the intent of natural language. *Machine
    learning* (ML) is a subfield of AI that deals with the study of machines capable
    of learning from experience and performing tasks without being explicitly programmed
    to do so. *Deep learning* is a subset of machine learning, inspired by the way
    the human brain works. It is a *neural network*, or a large network of neurons
    that interact with each other to perform significantly complex tasks with minimal
    intervention.
  prefs: []
  type: TYPE_NORMAL
- en: The 2010s saw the advent of deep learning and, with the maturity of the field,
    came large language models consisting of dense neural networks composed of thousands
    or even millions of simple processing units called *artificial neurons*. Neural
    networks became the first major game changer in the field of NLP by making it
    feasible to perform complex natural language tasks, something that had previously
    been possible only in theory. The second major game changer was the introduction
    of pre-trained models (such as GPT-3) that could be fine-tuned on a variety of
    downstream tasks, saving many hours of training. (We discuss pre-trained models
    later in this chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP is at the core of many real-world AI applications, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Spam detection
  prefs: []
  type: TYPE_NORMAL
- en: The spam filtering in your email inbox assigns a percentage of the incoming
    emails to the spam folder, using NLP to evaluate which emails look suspicious.
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation
  prefs: []
  type: TYPE_NORMAL
- en: Google Translate, DeepL, and other machine translation programs use NLP to evaluate
    millions of sentences translated by human speakers of different language pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual assistants and chatbots
  prefs: []
  type: TYPE_NORMAL
- en: All the Alexas, Siris, Google Assistants, and customer support chatbots of the
    world fall into this category. They use NLP to understand, analyze, and prioritize
    user questions and requests, and respond to them quickly and correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Social media sentiment analysis
  prefs: []
  type: TYPE_NORMAL
- en: Marketers collect social media posts about specific brands, conversation subjects,
    and keywords, then use NLP to analyze how users feel about each topic, individually
    and collectively. This helps the brands with customer research, image evaluation,
    and social dynamics detection.
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing a text involves reducing its size while keeping key information
    and the essential meaning. Some everyday examples of text summarization are news
    headlines, movie previews, newsletter production, financial research, legal contract
    analysis, and email summaries, as well as applications delivering news feeds,
    reports, and emails.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search leverages deep neural networks to intelligently search through
    data. You interact with it every time you search on Google. Semantic search is
    helpful when you want to search for something based on the context rather than
    specific keywords.
  prefs: []
  type: TYPE_NORMAL
- en: “The way we interact with other humans is through language,” says [Yannic Kilcher](https://oreil.ly/xrC3p),
    one of the most popular YouTubers and influencers in the NLP space, adding that
    language is part of every interaction humans have with each other and with computers.
    It’s no wonder, then, that NLP as a field has been the site of some of the most
    exciting AI discoveries and implementations of the past decade.
  prefs: []
  type: TYPE_NORMAL
- en: 'Language Models: Bigger and Better'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Language modeling* is the task of assigning a probability to a sequence of
    words in a text in a specific language. Simple language models can look at a word
    and predict the next word (or words) most likely to follow it, based on statistical
    analysis of existing text sequences. To create a language model that successfully
    predicts word sequences, you need to train it on large sets of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Language models are a key component in natural language processing applications.
    You can think of them as statistical prediction machines, where you give text
    as input and get a prediction as the output. You’re probably familiar with this
    from the auto-complete feature on your smartphone. For instance, if you type “good,”
    auto-complete might suggest “morning” or “luck.”
  prefs: []
  type: TYPE_NORMAL
- en: Before GPT-3 there was no general language model that could perform well on
    an *array* of NLP tasks. Language models were designed to perform *one* specific
    NLP task, such as text generation, summarization, or classification, using existing
    algorithms and architectures. In this book, we will discuss GPT-3’s extraordinary
    capabilities as a general language model. We’ll start this chapter by walking
    you through each letter of “GPT” to show what it stands for and what elements
    went into the building of this model. We’ll give a brief overview of the model’s
    history and how the sequence-to-sequence models we see today came into the picture.
    After that, we will walk you through the importance of API access and how it evolved
    over time based on users’ demands. We recommend that you sign up for an OpenAI
    account before you move on to the rest of the chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Generative Pre-Trained Transformer: GPT-3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name GPT-3 stands for “Generative Pre-trained Transformer 3.” Let’s go through
    all these terms one by one to understand the making of GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-3 is a *generative model* because it generates text. Generative modeling
    is a branch of statistical modeling. It is a method for mathematically approximating
    the world.
  prefs: []
  type: TYPE_NORMAL
- en: We are surrounded by an incredible amount of easily accessible information—both
    in the physical world and the digital one. The tricky part is to develop intelligent
    models and algorithms that can analyze and understand this treasure trove of data.
    Generative models are one of the most promising approaches to achieving this goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train a model, you have to prepare and preprocess a *dataset*, which is
    a collection of examples that helps the model learn to perform a given task. Usually
    a dataset is a large amount of data in some specific domain: like millions of
    images of cars to teach a model what a car is, for example. Datasets can also
    take the form of sentences or audio samples. Once you have shown the model many
    examples, you must train it to generate similar data.'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Have you heard of the theory of 10,000 hours? In his book *Outliers*, Malcolm
    Gladwell suggests that practicing any skill for 10,000 hours is sufficient to
    make you an expert.^([1](ch01.xhtml#ch01fn1)) This “expert” knowledge is reflected
    in the connections your human brain develops between its neurons. An AI model
    actually does something similar.
  prefs: []
  type: TYPE_NORMAL
- en: To create a model that performs well, you need to train it using a specific
    set of variables, called *parameters*. The process of determining the ideal parameters
    for your model is called *training*. The model assimilates parameter values through
    successive training iterations.
  prefs: []
  type: TYPE_NORMAL
- en: It takes a lot of time for a deep learning model to find these ideal parameters.
    Training is a lengthy process that, depending on the task, can last from a few
    hours to a few months and requires a tremendous amount of computing power. To
    be able to reuse some of that long learning process for other tasks would be a
    major help. And this is where pre-trained models come in.
  prefs: []
  type: TYPE_NORMAL
- en: A *pre-trained model*, keeping with Gladwell’s 10,000 hours theory, is the first
    skill you develop that can help you acquire another one faster. For example, mastering
    the skill of solving math problems can help you more quickly acquire the skill
    of solving engineering problems. A pre-trained model is trained (by you or someone
    else) for a more general task and is then available to be fine-tuned for different
    tasks. Instead of building a model from scratch to solve your problem, you use
    the model trained on a more general problem as a starting point and give it more
    specific training in the area of your choice using a specially curated dataset.
    A pre-trained model may not be 100% accurate, but it saves you from reinventing
    the wheel, thus saving time and improving performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, a model is trained on a dataset. The size and type of
    data samples vary depending on the task you want to solve. GPT-3 is pre-trained
    on a corpus of text from five datasets: Common Crawl, WebText2, Books1, Books2,
    and Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: Common Crawl
  prefs: []
  type: TYPE_NORMAL
- en: The Common Crawl corpus (collection of texts) comprises petabytes of data including
    raw web page data, metadata, and text data collected over eight years of web crawling.
    OpenAI researchers use a curated, filtered version of this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: WebText2
  prefs: []
  type: TYPE_NORMAL
- en: WebText2 is an expanded version of the WebText dataset, which is an internal
    OpenAI corpus created by scraping web pages of particularly high quality. To vet
    for quality, the authors scraped all outbound links from Reddit that received
    at least three karma (an indicator for whether other users found the link interesting,
    educational, or just funny). WebText2 contains 40 gigabytes of text from these
    45 million links, over 8 million documents.
  prefs: []
  type: TYPE_NORMAL
- en: Books1 and Books2
  prefs: []
  type: TYPE_NORMAL
- en: Books1 and Books2 are two corpora (plural of corpus) that contain the text of
    tens of thousands of books on various subjects.
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: The Wikipedia corpus is a collection including all English-language articles
    from the crowdsourced online encyclopedia [Wikipedia](https://oreil.ly/YBL5o)
    at the time of finalizing the GPT-3’s dataset in 2019\. This dataset has roughly
    [5.8 million](https://oreil.ly/NKIpI) English articles.
  prefs: []
  type: TYPE_NORMAL
- en: This corpus includes nearly a trillion words altogether.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 is capable of generating and successfully working with languages other
    than English as well. [Table 1-1](#top_ten_languages_in_the_gpt_three_data) shows
    the [top 10 languages](https://oreil.ly/Gi1di) within the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-1\. Top ten languages in the GPT-3 dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | Language | Number of documents | % of total documents |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | English | 235,987,420 | 93.68882% |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | German | 3,014,597 | 1.19682% |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | French | 2,568,341 | 1.01965% |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Portuguese | 1,608,428 | 0.63856% |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Italian | 1,456,350 | 0.57818% |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Spanish | 1,284,045 | 0.50978% |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Dutch | 934,788 | 0.37112% |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Polish | 632,959 | 0.25129% |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Japanese | 619,582 | 0.24598% |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Danish | 396,477 | 0.15740% |'
  prefs: []
  type: TYPE_TB
- en: While the gap between English and other languages is dramatic—English is number
    one, with 93% of the dataset; German, at number two, accounts for just 1%—that
    1% is sufficient to create perfect text in German, with style transfer and other
    tasks. The same goes for other languages on the list.
  prefs: []
  type: TYPE_NORMAL
- en: Since GPT-3 is pre-trained on an extensive and diverse corpus of text, it can
    successfully perform a surprising number of NLP tasks without users providing
    any additional example data.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural networks form the core of deep learning. Their name and structure are
    inspired by the human brain, mimicking the way that biological neurons signal
    to one another. A neural network is a network or circuit of neurons working in
    tandem. Neural network innovations can improve the model performance on downstream
    tasks, and so AI scientists continuously work on new architectures for neural
    networks. One such invention revolutionized NLP as we know it today: the transformer.
    A *transformer* is a machine learning model that processes a sequence of text
    all at once (instead of a word at a time), and that has a powerful mechanism to
    understand the connection between the words.'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Researchers at Google and the University of Toronto introduced the idea of
    a transformer model in a 2017 paper:'
  prefs: []
  type: TYPE_NORMAL
- en: We propose a new simple network architecture, the Transformer, based solely
    on attention mechanisms, dispensing with recurrence and convolutions entirely.
    Experiments on two machine translation tasks show these models to be superior
    in quality while being more parallelizable and requiring significantly less time
    to train.^([2](ch01.xhtml#ch01fn2))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The backbone of transformer models is sequence-to-sequence architecture. *Sequence-to-sequence*
    (Seq2Seq) transforms a given sequence of elements, such as words in a sentence,
    into another sequence, such as a sentence in a different language; sentences are
    sequence-dependent since word order is crucial for understanding a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Seq2Seq models are particularly good at translation, where a sequence of words
    from one language is transformed into a sequence of different words in another
    language. Google Translate started using a Seq2Seq-based model in production in
    late 2016.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seq2Seq models consist of two parts: an encoder and a decoder. Imagine the
    encoder and decoder as human translators who can each speak only two languages,
    with each having a different mother tongue. For our example, we’ll say the encoder
    is a native French speaker and the decoder is a native English speaker. The two
    have a second language in common: let’s say it’s Korean. To translate French into
    English, the encoder converts the French sentence into Korean (known as *context*)
    and passes on the context to the decoder. Since the decoder understands Korean,
    he or she can now translate from Korean into English. Working together, they can
    translate the French language to English, as illustrated by [Figure 1-1](#seq_to_seq_model_left_parenthesisneural).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. Seq2Seq model (neural machine translation)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Transformer attention mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformer architecture was invented to improve AI’s performance on machine
    translation tasks. “Transformers started as language models,” Kilcher explains,
    “not even that large, but then they became large.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with transformer models, you need to understand one more technical
    concept: attention. An *attention mechanism* is a technique that mimics cognitive
    attention: it looks at an input sequence, piece by piece and, on the basis of
    probabilities, decides at each step which other parts of the sequence are important.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, look at the sentence “The cat sat on the mat once it ate the mouse.”
    Does “it” in this sentence refer to “the cat” or “the mat”? The transformer model
    can strongly connect “it” with “the cat.” That’s attention.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our encoder and decoder example, imagine that the encoder writes
    down keywords that are important to the semantics of the sentence and gives them
    to the decoder along with the translation. Those keywords make the translation
    much easier for the decoder, who now knows what parts of the sentence are important
    and which terms give the sentence context.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformer model has two types of attention: *self-attention* (connection
    of words within a sentence) and *encoder-decoder attention* (connection between
    words from the source sentence to words from the target sentence).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention mechanism helps the transformer filter out noise and focus on
    what’s relevant: connecting two words in a semantic relationship to each other,
    when the words in themselves do not carry any obvious markers pointing to one
    another.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models benefit from larger architectures and larger quantities of
    data. Training on large datasets and fine-tuning for specific tasks improve results.
    Transformers are better at understanding the context of words in a sentence than
    any other kind of neural network. GPT is just the decoder part of the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what GPT means, let’s talk about that “3”—as well as 1 and
    2.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief History of GPT-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPT-3 was created by, and is a significant milestone for, OpenAI, a San Francisco-based
    pioneer of AI research. OpenAI’s [stated mission](https://oreil.ly/TUwij) is “to
    ensure that artificial general intelligence benefits all of humanity.” Artificial
    *general* intelligence is a type of AI that is not confined to specialized tasks
    but instead performs well at a variety of tasks, just like humans do.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI presented GPT-1 in June 2018\. The developers’ [key finding](https://oreil.ly/21J4S)
    was that combining the transformer architecture with unsupervised pre-training
    yielded promising results. GPT-1, they write, was fine-tuned for specific tasks
    to achieve “strong natural language understanding.”
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1 served as an important stepping stone toward a language model with general
    language-based capabilities. It proved that language models can be effectively
    pre-trained, which could help them generalize well. The architecture could perform
    various NLP tasks with very little fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1 used the [Book Corpus](https://oreil.ly/OQtXS) dataset, which contains
    some seven thousand unpublished books, and the decoder part of the transformer
    with self-attention to train the model. The architecture remained largely the
    same as in the original transformer. The model had 117 million parameters. GPT-1
    opened avenues for future models, which could unleash this potential better with
    larger datasets and more parameters.
  prefs: []
  type: TYPE_NORMAL
- en: One of its achievements was decent zero-shot performance ability on various
    NLP tasks like question answering (Q&A) and sentiment analysis, due to pre-training.
    *Zero-shot learning* is the ability of a model to perform a task without having
    seen any example of that kind in the past; the model is supposed to understand
    the task without looking at any examples. *Zero-shot task transfer* is a setting
    in which the model is presented with few to no examples and asked to understand
    the task based on the examples and an instruction.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In February 2019, OpenAI introduced GPT-2, which was bigger than GPT-1 but otherwise
    very similar. The major difference was that GPT-2 could multitask. It [successfully
    proved](https://oreil.ly/E8IEe) that a language model could perform well on several
    tasks without receiving any training examples for those tasks.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 showed that training on a larger dataset and having more parameters improves
    a language model’s capability to understand tasks and surpass the state of the
    art of many tasks in zero-shot settings. It also showed that even larger language
    models would be even better at natural language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: To create an extensive, high-quality dataset, the authors scraped Reddit and
    pulled data from outbound links of upvoted articles on the platform. The resulting
    dataset, WebText, had 40GB of text data from over eight million documents, far
    larger than GPT-1’s dataset. GPT-2 was trained on the WebText dataset and had
    1.5 billion parameters, 10 times more than GPT-1.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 was evaluated on several datasets of downstream tasks like reading comprehension,
    summarization, translation, and question answering.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the quest to build an even more robust and powerful language model, OpenAI
    built the GPT-3 model. Both its dataset and the model are about two orders of
    magnitude larger than those used for GPT-2: GPT-3 has 175 billion parameters and
    was trained on a mix of five different text corpora, a much bigger dataset than
    was used to train GPT-2\. The architecture of GPT-3 is largely the same as GPT-2\.
    It performs well on downstream NLP tasks in zero-shot and few-shot settings.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 has capabilities like writing articles that are indistinguishable from
    human-written articles. It can also perform on-the-fly tasks for which it was
    never explicitly trained, like summing numbers, writing SQL queries, and even
    writing React and JavaScript code given a plain English description of the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Few-, one-, and zero-shot settings are specialized cases of zero-shot task transfer.
    In a *few-shot setting*, the model is provided with a task description and as
    many examples as fit into the context window of the model. In a *one-shot setting*,
    the model is provided with exactly one example and, in a *zero-shot setting*,
    with no example.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI’s mission statement emphasizes the democratic and ethical aspects of
    AI. The democratic dimension lies in the decision to release the third version
    of the model, GPT-3, via a public API, or application programming interface: a
    software intermediary that sends information back and forth between a website
    or app and a user.'
  prefs: []
  type: TYPE_NORMAL
- en: APIs act as messengers, allowing developers to build new programmatic interactions
    between applications and users. Releasing GPT-3 via an API was a revolutionary
    move. Until 2020, the powerful AI models developed by leading research labs were
    available to only a select few—researchers and engineers working on those projects.
    The OpenAI API gives users all over the world unprecedented access to the world’s
    most powerful language model via a simple sign-in. (OpenAI’s business rationale
    for this move is to create a new paradigm it calls “model-as-a-service” where
    developers can pay per API call; we will take a closer look at this in [Chapter 3](ch03.xhtml#programming_with_gpt_three).)
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI researchers experimented with different model sizes while working on
    GPT-3\. They took the existing GPT-2 architecture and increased the number of
    parameters. What emerged as a result of that experiment is a model with new and
    extraordinary capabilities in the form of GPT-3\. While GPT-2 displayed some zero-shot
    capabilities on downstream tasks, GPT-3 can carry out even more novel tasks when
    presented with example context.
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAI researchers found it remarkable](https://oreil.ly/e22dR) that merely
    scaling the model parameters and the size of the training dataset led to such
    extraordinary advances. They are generally optimistic that these trends will continue
    even for models much larger than GPT-3, enabling ever-stronger learning models
    capable of few-shot or zero-shot learning just by fine-tuning on a small sample
    size.'
  prefs: []
  type: TYPE_NORMAL
- en: As you read this book, [experts estimate](https://oreil.ly/0TE9t) that language
    models based on a trillion parameters are probably being built and deployed. We
    have entered the golden age of large language models, and now it’s time for you
    to become a part of it.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 has captured a lot of public attention. The *MIT Technology Review* considered
    GPT-3 one of the [10 Breakthrough Technologies of 2021](https://oreil.ly/mHAKG).
    Its sheer flexibility in performing a series of generalized tasks with near-human
    efficiency and accuracy is what makes it so exciting, as early adopter Arram Sabeti
    tweeted ([Figure 1-2](#tweet_from_arram_sabeti)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gpt3_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Tweet from [Arram Sabeti](https://oreil.ly/chjI4)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The API release created a paradigm shift in NLP and attracted a huge number
    of beta testers. Innovations and start-ups followed at lightning speed, with commentators
    calling GPT-3 a [“fifth Industrial Revolution”](https://oreil.ly/fZarJ).
  prefs: []
  type: TYPE_NORMAL
- en: Within just nine months of the launch of the API, [according to OpenAI](https://oreil.ly/TiEVy),
    people were building more than three hundred businesses with it. Despite this
    suddenness, some experts argue that the excitement isn’t exaggerated. Bakz Awan
    is a developer turned entrepreneur and influencer, and one of the major voices
    in the OpenAI API developer community. He has a [YouTube channel “Bakz T. Future”](https://oreil.ly/W1cWX)
    and a [podcast](https://oreil.ly/07RBY). Awan argues that GPT-3 and other models
    are actually “underhyped for how usable and friendly and fun and powerful they
    really are. It’s almost shocking.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Daniel Erickson, CEO of Viable, which has a GPT-3-powered product, praises
    the model’s ability to extract insights from large datasets through what he calls
    *prompt-based development*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Companies going down that path cover use cases such as generating copy for
    ads and websites. The design philosophy is relatively simple: the company takes
    your data in, sends it over into a prompt, and displays the API-generated result.
    It solves a task that is easily done by a single API prompt and wraps [a] UI around
    that to deliver it to the users.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The problem Erickson sees with this category of use cases is that it is already
    overcrowded, attracting many ambitious start-up founders competing with similar
    services. Instead, Erickson recommends looking at another category of use cases
    instead, as Viable did. Data-driven use cases are not as crowded as prompt-generation
    use cases, but they are more profitable and allow you to easily create a security
    “moat.”
  prefs: []
  type: TYPE_NORMAL
- en: The key, Erickson says, is to build a large dataset that you can keep adding
    to and that can provide potential insights. GPT-3 will help you extract valuable
    insights from it. At Viable, this was the model that let them monetize easily.
    “People pay a lot more money for data than they do for prompt output,” Erickson
    explains.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that technological revolutions also bring controversies and
    challenges. GPT-3 is a powerful tool in the hands of anyone trying to create a
    narrative. Without great care and benevolent intentions, one such challenge we
    will face is curbing the attempts to use the algorithm to spread misinformation
    campaigns. Another one would be eradicating its use for generating mass quantities
    of low-quality digital content that will then pollute the information available
    on the internet. Yet another one is the limitations of its datasets that are filled
    with various kinds of bias, which can be amplified by this technology. We will
    look closer at these and more challenges in [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and),
    along with discussing the various efforts by OpenAI to address them.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the OpenAI API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of 2021, the market has already produced several proprietary AI models that
    have more parameters than GPT-3\. However, access to them is limited to a handful
    of people within the company’s R&D walls, making it impossible to evaluate their
    performance on real-world NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One factor that makes GPT-3 accessible is its simple and intuitive “text-in,
    text-out” user interface. It doesn’t require complex, gradient fine-tuning or
    updates, and you don’t need to be an expert to use it. This combination of scalable
    parameters and relatively open access makes GPT-3 the most exciting, and arguably
    the most relevant, language model to date.
  prefs: []
  type: TYPE_NORMAL
- en: Due to GPT-3’s extraordinary capabilities, there are significant risks in terms
    of security and misuse associated with making it open source, which we will cover
    in [Chapter 7](ch07.xhtml#democratizing_access_to_ai). Taking that into account,
    OpenAI decided not to release the source code of GPT-3 publicly and came up with
    a unique access sharing model via an API.
  prefs: []
  type: TYPE_NORMAL
- en: The company decided to initially release access to the API in the form of a
    limited beta user list. The application process required people to complete a
    form detailing their background and reasons for requesting API access. Only approved
    users were granted access to a private beta of the API with an interface called
    Playground.
  prefs: []
  type: TYPE_NORMAL
- en: In its early days, the waiting list for GPT-3 beta access consisted of tens
    of thousands of people. OpenAI swiftly managed the applications that started pouring
    in, adding developers in batches but also closely monitoring their activity and
    feedback about the API user experience in order to continuously improve it.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the progress with safeguards, OpenAI removed the waiting list in November
    2021\. GPT-3 is now openly accessible via a [simple sign-in](https://oreil.ly/uSkAH).
    This is a great milestone in the history of GPT-3 and a highly requested move
    by the community. To get API access, simply go to the [sign-up page](https://oreil.ly/e9hFU),
    sign up for a free account, and start experimenting with it right away.
  prefs: []
  type: TYPE_NORMAL
- en: New users initially get a pool of free credits that allows them to freely experiment
    with the API. The number of credits is equivalent to creating text content as
    long as three average-length novels. After the free credits are used, users start
    paying for usage or, if they have a need, they can request additional credits
    from OpenAI API customer support.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI strives to ensure that API-powered applications are built responsibly.
    For that reason, it provides [tools](https://oreil.ly/cHrcg), [best practices](https://oreil.ly/n0xRW),
    and [usage guidelines](https://oreil.ly/HwWSa) to help developers bring their
    applications to production quickly and safely.
  prefs: []
  type: TYPE_NORMAL
- en: The company has also created [content guidelines](https://oreil.ly/o6BLo) to
    clarify what kind of content the OpenAI API can be used to generate. To help developers
    ensure their applications are used for the intended purpose, prevent potential
    misuse, and adhere to the content guidelines, OpenAI offers a free content filter.
    OpenAI policy prohibits the use of the API in ways that do not adhere to the principles
    described in its [charter](https://oreil.ly/nTK3V), including content that promotes
    hate, violence, or self-harm, or that intends to harass, influence political processes,
    spread misinformation, spam content, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have signed up for an OpenAI account, you can move on to [Chapter 2](ch02.xhtml#using_the_openai_api),
    where we will discuss the different components of the API, the GPT-3 Playground,
    and how to use the API to the best of its abilities for different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch01.xhtml#ch01fn1-marker)) Malcolm Gladwell, *Outliers: The Story of
    Success* (Little, Brown, 2008).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch01.xhtml#ch01fn2-marker)) Ashish Vaswani et al., [“Attention Is All
    You Need,”](https://oreil.ly/8rByF) *Advances in Neural Information Processing
    Systems* 30 (2017).
  prefs: []
  type: TYPE_NORMAL
