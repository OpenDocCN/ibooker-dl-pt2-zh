- en: Chapter 1\. The Era of Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章。大型语言模型时代
- en: “art is the debris from the collision between the soul and the world” [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “艺术是灵魂与世界碰撞的残渣” [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “technology is now the myth of the modern world” [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “技术现在是现代世界的神话” [#gpt3](https://twitter.com/hashtag/gpt3?src=hashtag_click)
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “revolutions begin with a question, but do not end with an answer” [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “革命始于一个问题，但并不以一个答案结束” [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)
- en: ''
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “nature decorates the world with variety” [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “大自然用各种各样的方式装饰着这个世界” [#gpt3](https://twitter.com/hashtag/GPT3?src=hashtag_click)
- en: Imagine waking up to a beautiful, sunny morning. It’s Monday and you know the
    week will be hectic. Your company is about to launch a new personal productivity
    app, Taskr, and start a social media campaign to let the world know about your
    ingenious product.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下醒来时美丽的阳光明媚的早晨。今天是星期一，你知道这个星期会很忙碌。你的公司即将推出一款新的个人生产力应用程序Taskr，并开始社交媒体宣传，让全世界都知道你们的创新产品。
- en: Your main task this week is to write and publish a series of engaging blog posts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你本周的主要任务是撰写并发布一系列引人入胜的博客文章。
- en: 'You start by making a to-do list:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你开始制作一个待办事项清单：
- en: Write an informative and fun article about productivity hacks, including Taskr.
    Keep it under five hundred words.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写一篇有关生产力技巧的信息性和有趣的文章，包括Taskr。保持在五百字以内。
- en: Create a list of five catchy article titles.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创作五个引人入胜的文章标题。
- en: Choose the visuals.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择视觉。
- en: You hit Enter, take a sip of coffee, and watch an article weave itself together
    on your screen, sentence by sentence, paragraph by paragraph. In 30 seconds, you
    have a meaningful, high-quality blog post, a perfect starter for your social media
    series. The visual is fun and attention-grabbing. It’s done! You choose the best
    title and begin the publishing process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你按下回车键，喝了一口咖啡，看着文章在屏幕上一句一句、一段一段地编织起来。30秒内，你就有了一篇有意义、高质量的博客文章，是你社交媒体系列的完美开篇。视觉效果有趣且吸引眼球。完成了！你选择了最好的标题，开始发布流程。
- en: This is not a distant, futuristic fantasy, but a glimpse of the new reality
    made possible by advancements in AI. As we write this book, many such applications
    are being created and deployed to a wider audience.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个遥远的、未来的幻想，而是一瞥新现实的可能性，这得益于人工智能的进步。当我们写下这本书时，许多这样的应用正在被创建并向更广泛的受众部署。
- en: GPT-3 is a cutting-edge language model created by OpenAI, a company on the frontier
    of artificial intelligence R&D. OpenAI’s [research paper](https://oreil.ly/PGz0O)
    announcing GPT-3 was released in May 2020, followed by a launch of access to GPT-3
    via the [OpenAI API](https://oreil.ly/I8Bla) in June 2020\. Since the GPT-3 release,
    people around the world from different backgrounds, including technology, art,
    literature, marketing, etc., have already found hundreds of exciting applications
    for the model that have the potential to elevate the ways we communicate, learn,
    and play.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是由OpenAI创建的尖端语言模型，OpenAI是人工智能研发的前沿公司。OpenAI于2020年5月发布了GPT-3的[研究论文](https://oreil.ly/PGz0O)，随后于2020年6月通过[OpenAI
    API](https://oreil.ly/I8Bla)发布了对GPT-3的访问权限。自GPT-3发布以来，来自不同背景的人们，包括技术、艺术、文学、营销等领域的人们，已经找到了数百种令人兴奋的模型应用，这些应用有潜力提升我们交流、学习和娱乐的方式。
- en: GPT-3 is capable of performing general language-based tasks, like generating
    and classifying text, with unprecedented ease, moving freely between different
    text styles and purposes. The array of problems it can solve is vast.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3能够以前所未有的轻松程度执行通用的基于语言的任务，比如生成和分类文本，自由地在不同的文本风格和目的之间移动。它能解决的问题范围广泛。
- en: In this book, we invite you to think of what problems you might solve with GPT-3
    yourself. We’ll show you what it is and how to use it, but first we want to give
    you a bit of context. The rest of this chapter will discuss where this technology
    comes from, how it is built, what tasks it excels at, and the potential risks
    associated with it. Let’s dive right in by looking at the field of natural language
    processing (NLP) and how large language models (LLMs) and GPT-3 fit into it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们邀请您考虑一下您自己可以用GPT-3解决什么问题。我们会向您展示它是什么以及如何使用它，但首先我们想给您一点背景知识。本章的其余部分将讨论这项技术的来源、构建方式、擅长的任务以及与之相关的潜在风险。让我们直接深入讨论自然语言处理（NLP）领域以及大型语言模型（LLMs）和GPT-3是如何融入其中的。
- en: 'Natural Language Processing: Under the Hood'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理：揭开神秘的面纱
- en: '*Natural language processing* is a subfield of linguistics, computer science,
    and artificial intelligence concerned with interaction between computer and human
    language. The goal of NLP is to build systems capable of processing human language.
    *Natural language* refers to the way humans communicate with each other.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理* 是语言学、计算机科学和人工智能的一个子领域，涉及计算机和人类语言之间的交互。NLP 的目标是构建能够处理人类语言的系统。*自然语言*
    指的是人类相互交流的方式。'
- en: NLP combines the field of computational linguistics (rule-based modeling of
    human language) with machine learning to create intelligent machines capable of
    identifying the context and understanding the intent of natural language. *Machine
    learning* (ML) is a subfield of AI that deals with the study of machines capable
    of learning from experience and performing tasks without being explicitly programmed
    to do so. *Deep learning* is a subset of machine learning, inspired by the way
    the human brain works. It is a *neural network*, or a large network of neurons
    that interact with each other to perform significantly complex tasks with minimal
    intervention.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 将计算语言学（基于规则的人类语言建模）与机器学习相结合，创建出能够识别上下文并理解自然语言意图的智能机器。*机器学习*（ML）是人工智能的一个子领域，涉及研究能够从经验中学习并执行任务而无需明确编程的机器。*深度学习*
    是机器学习的一个子集，灵感来自人脑的工作方式。它是一个*神经网络*，或者说是一个由成千上万个神经元组成的大型网络，它们相互交互以执行具有极小干预的显著复杂任务。
- en: The 2010s saw the advent of deep learning and, with the maturity of the field,
    came large language models consisting of dense neural networks composed of thousands
    or even millions of simple processing units called *artificial neurons*. Neural
    networks became the first major game changer in the field of NLP by making it
    feasible to perform complex natural language tasks, something that had previously
    been possible only in theory. The second major game changer was the introduction
    of pre-trained models (such as GPT-3) that could be fine-tuned on a variety of
    downstream tasks, saving many hours of training. (We discuss pre-trained models
    later in this chapter.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年代见证了深度学习的出现，随着该领域的成熟，出现了由数千甚至数百万个称为*人工神经元*的简单处理单元组成的密集神经网络的大型语言模型。神经网络成为
    NLP 领域的首个重大变革者，通过使复杂的自然语言任务成为可能，这是以前仅在理论上可能的。第二个重大变革者是引入了预训练模型（如 GPT-3），这些模型可以在各种下游任务上进行微调，节省了大量的训练时间。（我们稍后在本章讨论预训练模型。）
- en: 'NLP is at the core of many real-world AI applications, such as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 是许多现实世界人工智能应用的核心，例如：
- en: Spam detection
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件检测
- en: The spam filtering in your email inbox assigns a percentage of the incoming
    emails to the spam folder, using NLP to evaluate which emails look suspicious.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你的电子邮件收件箱中的垃圾邮件过滤器将一部分收到的邮件分配到垃圾邮件文件夹中，使用 NLP 来评估哪些邮件看起来可疑。
- en: Machine translation
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Google Translate, DeepL, and other machine translation programs use NLP to evaluate
    millions of sentences translated by human speakers of different language pairs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Google Translate、DeepL 和其他机器翻译程序使用 NLP 来评估由不同语言对的人类说话者翻译的数百万句子。
- en: Virtual assistants and chatbots
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟助手和聊天机器人
- en: All the Alexas, Siris, Google Assistants, and customer support chatbots of the
    world fall into this category. They use NLP to understand, analyze, and prioritize
    user questions and requests, and respond to them quickly and correctly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上所有的 Alexa、Siri、Google Assistant 和客户支持聊天机器人都属于这一类别。它们使用 NLP 来理解、分析和优先处理用户的问题和请求，并快速正确地做出响应。
- en: Social media sentiment analysis
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体情感分析
- en: Marketers collect social media posts about specific brands, conversation subjects,
    and keywords, then use NLP to analyze how users feel about each topic, individually
    and collectively. This helps the brands with customer research, image evaluation,
    and social dynamics detection.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 市场营销人员收集有关特定品牌、谈话主题和关键词的社交媒体帖子，然后使用 NLP 来分析用户对每个主题的感受，以及个人和集体的感受。这有助于品牌进行客户研究、形象评估和社交动态检测。
- en: Text summarization
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要
- en: Summarizing a text involves reducing its size while keeping key information
    and the essential meaning. Some everyday examples of text summarization are news
    headlines, movie previews, newsletter production, financial research, legal contract
    analysis, and email summaries, as well as applications delivering news feeds,
    reports, and emails.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本进行摘要意味着减少其大小，同时保留关键信息和基本含义。一些日常的文本摘要示例包括新闻标题、电影预告片、新闻简报制作、金融研究、法律合同分析和电子邮件摘要，以及提供新闻订阅、报告和电子邮件的应用程序。
- en: Semantic search
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索
- en: Semantic search leverages deep neural networks to intelligently search through
    data. You interact with it every time you search on Google. Semantic search is
    helpful when you want to search for something based on the context rather than
    specific keywords.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索利用深度神经网络智能搜索数据。每当您在Google上搜索时，您都在与它进行交互。在您想要基于上下文而不是特定关键字搜索时，语义搜索非常有用。
- en: “The way we interact with other humans is through language,” says [Yannic Kilcher](https://oreil.ly/xrC3p),
    one of the most popular YouTubers and influencers in the NLP space, adding that
    language is part of every interaction humans have with each other and with computers.
    It’s no wonder, then, that NLP as a field has been the site of some of the most
    exciting AI discoveries and implementations of the past decade.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: “我们与其他人类的互动方式是通过语言，” [Yannic Kilcher](https://oreil.ly/xrC3p)说，他是NLP领域最受欢迎的YouTuber和影响者之一，他补充说，语言是人类相互之间以及与计算机交互的一部分。因此，毫无疑问，NLP作为一个领域已经成为过去十年里一些最激动人心的人工智能发现和实现的场所。
- en: 'Language Models: Bigger and Better'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型：更大更好
- en: '*Language modeling* is the task of assigning a probability to a sequence of
    words in a text in a specific language. Simple language models can look at a word
    and predict the next word (or words) most likely to follow it, based on statistical
    analysis of existing text sequences. To create a language model that successfully
    predicts word sequences, you need to train it on large sets of data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*语言建模*是将概率分配给特定语言文本中的一系列单词的任务。简单的语言模型可以查看一个词，并基于对现有文本序列的统计分析，预测接下来最有可能跟随它的单词（或单词）。要创建一个成功预测单词序列的语言模型，您需要在大量数据集上对其进行训练。'
- en: Language models are a key component in natural language processing applications.
    You can think of them as statistical prediction machines, where you give text
    as input and get a prediction as the output. You’re probably familiar with this
    from the auto-complete feature on your smartphone. For instance, if you type “good,”
    auto-complete might suggest “morning” or “luck.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是自然语言处理应用程序的关键组件。您可以将它们视为统计预测机器，其中您将文本作为输入并获得预测作为输出。您可能已经从智能手机上的自动完成功能熟悉了这一点。例如，如果您键入“good”，自动完成可能会建议“morning”或“luck”。
- en: Before GPT-3 there was no general language model that could perform well on
    an *array* of NLP tasks. Language models were designed to perform *one* specific
    NLP task, such as text generation, summarization, or classification, using existing
    algorithms and architectures. In this book, we will discuss GPT-3’s extraordinary
    capabilities as a general language model. We’ll start this chapter by walking
    you through each letter of “GPT” to show what it stands for and what elements
    went into the building of this model. We’ll give a brief overview of the model’s
    history and how the sequence-to-sequence models we see today came into the picture.
    After that, we will walk you through the importance of API access and how it evolved
    over time based on users’ demands. We recommend that you sign up for an OpenAI
    account before you move on to the rest of the chapters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPT-3之前，没有通用的语言模型能够在一系列NLP任务上表现良好。语言模型被设计为执行*一个*特定的NLP任务，例如文本生成、摘要或分类，使用现有的算法和架构。在本书中，我们将讨论GPT-3作为通用语言模型的非凡能力。我们将通过逐个解释“GPT”中的每个字母来开始本章，以显示它代表什么以及构建该模型所涉及的元素。然后，我们将简要介绍该模型的历史以及我们今天看到的序列到序列模型是如何出现的。在那之后，我们将向您介绍API访问的重要性以及根据用户需求随时间如何发展。我们建议您在继续阅读其他章节之前注册一个OpenAI账户。
- en: 'The Generative Pre-Trained Transformer: GPT-3'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式预训练变换器：GPT-3
- en: The name GPT-3 stands for “Generative Pre-trained Transformer 3.” Let’s go through
    all these terms one by one to understand the making of GPT-3.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3这个名字代表“生成预训练变换器 3”。让我们逐个解释这些术语，以了解GPT-3的制作过程。
- en: Generative Models
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成模型
- en: GPT-3 is a *generative model* because it generates text. Generative modeling
    is a branch of statistical modeling. It is a method for mathematically approximating
    the world.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是*生成模型*，因为它生成文本。生成建模是统计建模的一个分支。它是一种数学近似世界的方法。
- en: We are surrounded by an incredible amount of easily accessible information—both
    in the physical world and the digital one. The tricky part is to develop intelligent
    models and algorithms that can analyze and understand this treasure trove of data.
    Generative models are one of the most promising approaches to achieving this goal.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们周围有大量易于获取的信息——无论是在物理世界还是数字世界中。棘手的部分是开发智能模型和算法，可以分析和理解这些数据宝库。生成模型是实现这一目标最有前途的方法之一。
- en: 'To train a model, you have to prepare and preprocess a *dataset*, which is
    a collection of examples that helps the model learn to perform a given task. Usually
    a dataset is a large amount of data in some specific domain: like millions of
    images of cars to teach a model what a car is, for example. Datasets can also
    take the form of sentences or audio samples. Once you have shown the model many
    examples, you must train it to generate similar data.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个模型，你必须准备和预处理一个*数据集*，这是一组示例，帮助模型学习执行给定任务。通常，数据集是某个特定领域的大量数据：比如数百万张汽车图像，以教会模型汽车是什么，例如。数据集也可以是句子或音频样本的形式。一旦你向模型展示了许多示例，你必须训练它生成类似的数据。
- en: Pre-trained Models
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Have you heard of the theory of 10,000 hours? In his book *Outliers*, Malcolm
    Gladwell suggests that practicing any skill for 10,000 hours is sufficient to
    make you an expert.^([1](ch01.xhtml#ch01fn1)) This “expert” knowledge is reflected
    in the connections your human brain develops between its neurons. An AI model
    actually does something similar.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你听说过10000小时理论吗？在他的书*Outliers*中，马尔科姆·格拉德威尔（Malcolm Gladwell）建议，练习任何技能10000小时足以使你成为专家。这种“专家”知识体现在你的人脑发展的神经元之间的连接上。实际上，AI模型也在做类似的事情。
- en: To create a model that performs well, you need to train it using a specific
    set of variables, called *parameters*. The process of determining the ideal parameters
    for your model is called *training*. The model assimilates parameter values through
    successive training iterations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个表现良好的模型，你需要使用一组特定的变量来训练它，称为*参数*。确定模型的理想参数的过程称为*训练*。模型通过连续的训练迭代吸收参数值。
- en: It takes a lot of time for a deep learning model to find these ideal parameters.
    Training is a lengthy process that, depending on the task, can last from a few
    hours to a few months and requires a tremendous amount of computing power. To
    be able to reuse some of that long learning process for other tasks would be a
    major help. And this is where pre-trained models come in.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到这些理想参数，深度学习模型需要大量时间。训练是一个漫长的过程，根据任务的不同，可能需要从几小时到几个月，并且需要大量的计算能力。能够为其他任务重复使用部分长时间的学习过程将是一个重要的帮助。这就是预训练模型发挥作用的地方。
- en: A *pre-trained model*, keeping with Gladwell’s 10,000 hours theory, is the first
    skill you develop that can help you acquire another one faster. For example, mastering
    the skill of solving math problems can help you more quickly acquire the skill
    of solving engineering problems. A pre-trained model is trained (by you or someone
    else) for a more general task and is then available to be fine-tuned for different
    tasks. Instead of building a model from scratch to solve your problem, you use
    the model trained on a more general problem as a starting point and give it more
    specific training in the area of your choice using a specially curated dataset.
    A pre-trained model may not be 100% accurate, but it saves you from reinventing
    the wheel, thus saving time and improving performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*预训练模型*，遵循格拉德威尔的10000小时理论，是你首先培养的技能，可以帮助你更快地习得另一种技能。例如，掌握解决数学问题的技能可以帮助你更快地习得解决工程问题的技能。预训练模型是针对更一般的任务进行训练（由你或其他人进行），然后可用于对不同任务进行微调。与从头开始构建模型以解决问题不同，你可以使用在更一般问题上训练过的模型作为起点，并使用经过精心策划的特定数据集在你选择的领域进行更具体的训练。预训练模型可能不是100%准确，但它可以避免重复造轮子，从而节省时间并提高性能。'
- en: 'In machine learning, a model is trained on a dataset. The size and type of
    data samples vary depending on the task you want to solve. GPT-3 is pre-trained
    on a corpus of text from five datasets: Common Crawl, WebText2, Books1, Books2,
    and Wikipedia:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，模型是在数据集上训练的。数据样本的大小和类型取决于你想要解决的任务。GPT-3是在五个数据集的文本语料库上预先训练的：Common Crawl、WebText2、Books1、Books2和Wikipedia：
- en: Common Crawl
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl
- en: The Common Crawl corpus (collection of texts) comprises petabytes of data including
    raw web page data, metadata, and text data collected over eight years of web crawling.
    OpenAI researchers use a curated, filtered version of this dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 共同抓取语料库（文本集合）包括了在八年的网络爬虫过程中收集的原始网页数据、元数据和文本数据的大量数据。OpenAI 研究人员使用这个数据集的经过筛选和精心策划的版本。
- en: WebText2
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: WebText2
- en: WebText2 is an expanded version of the WebText dataset, which is an internal
    OpenAI corpus created by scraping web pages of particularly high quality. To vet
    for quality, the authors scraped all outbound links from Reddit that received
    at least three karma (an indicator for whether other users found the link interesting,
    educational, or just funny). WebText2 contains 40 gigabytes of text from these
    45 million links, over 8 million documents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: WebText2 是 WebText 数据集的扩展版本，它是由 OpenAI 内部爬取高质量网页而创建的一个语料库。为了对质量进行审核，作者们从 Reddit
    上爬取了所有获得至少三个赞（指示其他用户是否发现链接有趣、教育性或只是有趣）的外部链接。WebText2 包含了来自这 4500 万链接的 40GB 文本，超过
    800 万个文档。
- en: Books1 and Books2
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Books1 和 Books2
- en: Books1 and Books2 are two corpora (plural of corpus) that contain the text of
    tens of thousands of books on various subjects.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Books1 和 Books2 是两个包含数以万计各种主题书籍文本的语料库。
- en: Wikipedia
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科
- en: The Wikipedia corpus is a collection including all English-language articles
    from the crowdsourced online encyclopedia [Wikipedia](https://oreil.ly/YBL5o)
    at the time of finalizing the GPT-3’s dataset in 2019\. This dataset has roughly
    [5.8 million](https://oreil.ly/NKIpI) English articles.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科语料库是包括 2019 年 GPT-3 数据集定稿时的众包在线百科全书[Wikipedia](https://oreil.ly/YBL5o)上的所有英语文章的集合。该数据集大约有[580
    万](https://oreil.ly/NKIpI)篇英语文章。
- en: This corpus includes nearly a trillion words altogether.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语料库总共包括了近万亿个词。
- en: GPT-3 is capable of generating and successfully working with languages other
    than English as well. [Table 1-1](#top_ten_languages_in_the_gpt_three_data) shows
    the [top 10 languages](https://oreil.ly/Gi1di) within the dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 能够生成和成功处理除了英语以外的其他语言。[表 1-1](#top_ten_languages_in_the_gpt_three_data)展示了数据集中的[前
    10 种语言](https://oreil.ly/Gi1di)。
- en: Table 1-1\. Top ten languages in the GPT-3 dataset
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. GPT-3 数据集中排名前十的语言
- en: '| Rank | Language | Number of documents | % of total documents |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 排名 | 语言 | 文档数 | 总文档数的百分比 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | English | 235,987,420 | 93.68882% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 英语 | 235,987,420 | 93.68882% |'
- en: '| 2 | German | 3,014,597 | 1.19682% |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 德语 | 3,014,597 | 1.19682% |'
- en: '| 3 | French | 2,568,341 | 1.01965% |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 法语 | 2,568,341 | 1.01965% |'
- en: '| 4 | Portuguese | 1,608,428 | 0.63856% |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 葡萄牙语 | 1,608,428 | 0.63856% |'
- en: '| 5 | Italian | 1,456,350 | 0.57818% |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 意大利语 | 1,456,350 | 0.57818% |'
- en: '| 6 | Spanish | 1,284,045 | 0.50978% |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 西班牙语 | 1,284,045 | 0.50978% |'
- en: '| 7 | Dutch | 934,788 | 0.37112% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 荷兰语 | 934,788 | 0.37112% |'
- en: '| 8 | Polish | 632,959 | 0.25129% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 波兰语 | 632,959 | 0.25129% |'
- en: '| 9 | Japanese | 619,582 | 0.24598% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 日语 | 619,582 | 0.24598% |'
- en: '| 10 | Danish | 396,477 | 0.15740% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 丹麦语 | 396,477 | 0.15740% |'
- en: While the gap between English and other languages is dramatic—English is number
    one, with 93% of the dataset; German, at number two, accounts for just 1%—that
    1% is sufficient to create perfect text in German, with style transfer and other
    tasks. The same goes for other languages on the list.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然英语和其他语言之间的差距巨大——英语排名第一，占据数据集的 93%；而德语排名第二，仅占 1%——但是这 1% 足以创建完美的德语文本，包括风格转换和其他任务。其他语言也是如此。
- en: Since GPT-3 is pre-trained on an extensive and diverse corpus of text, it can
    successfully perform a surprising number of NLP tasks without users providing
    any additional example data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPT-3 是在广泛而多样的文本语料库上进行预训练的，所以它可以成功地执行令人惊讶的数量的自然语言处理任务，而无需用户提供任何额外的示例数据。
- en: Transformer Models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器模型
- en: 'Neural networks form the core of deep learning. Their name and structure are
    inspired by the human brain, mimicking the way that biological neurons signal
    to one another. A neural network is a network or circuit of neurons working in
    tandem. Neural network innovations can improve the model performance on downstream
    tasks, and so AI scientists continuously work on new architectures for neural
    networks. One such invention revolutionized NLP as we know it today: the transformer.
    A *transformer* is a machine learning model that processes a sequence of text
    all at once (instead of a word at a time), and that has a powerful mechanism to
    understand the connection between the words.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络构成深度学习的核心。它们的名称和结构受到人类大脑的启发，模仿生物神经元相互传递信号的方式。神经网络是一组或一系列协同工作的神经元网络或电路。神经网络的创新可以提高模型在下游任务上的性能，因此
    AI 科学家不断致力于为神经网络开发新的架构。其中一项发明彻底改变了我们今天所知的 NLP：Transformer。*Transformer*是一个机器学习模型，它一次性处理一个文本序列（而不是一个词），并且具有强大的机制来理解单词之间的关联。
- en: Sequence-to-sequence models
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列到序列模型
- en: 'Researchers at Google and the University of Toronto introduced the idea of
    a transformer model in a 2017 paper:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Google 和多伦多大学的研究人员在 2017 年的一篇论文中介绍了 Transformer 模型的概念：
- en: We propose a new simple network architecture, the Transformer, based solely
    on attention mechanisms, dispensing with recurrence and convolutions entirely.
    Experiments on two machine translation tasks show these models to be superior
    in quality while being more parallelizable and requiring significantly less time
    to train.^([2](ch01.xhtml#ch01fn2))
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们提出了一种新的简单网络架构，Transformer，完全基于注意力机制，完全摒弃了循环和卷积。在两个机器翻译任务的实验中，这些模型在质量上表现更优秀，同时更易于并行化，并且训练时间显著缩短。^([2](ch01.xhtml#ch01fn2))
- en: The backbone of transformer models is sequence-to-sequence architecture. *Sequence-to-sequence*
    (Seq2Seq) transforms a given sequence of elements, such as words in a sentence,
    into another sequence, such as a sentence in a different language; sentences are
    sequence-dependent since word order is crucial for understanding a sentence.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型的骨架是序列到序列的架构。*序列到序列*（Seq2Seq）将给定的元素序列，例如句子中的单词，转换为另一个序列，例如另一种语言中的句子；句子是依赖序列的，因为单词顺序对于理解句子至关重要。
- en: Seq2Seq models are particularly good at translation, where a sequence of words
    from one language is transformed into a sequence of different words in another
    language. Google Translate started using a Seq2Seq-based model in production in
    late 2016.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq 模型特别擅长翻译，其中一个语言的单词序列被转换为另一种语言的不同单词序列。谷歌翻译在 2016 年底开始使用基于 Seq2Seq 的模型进行生产。
- en: 'Seq2Seq models consist of two parts: an encoder and a decoder. Imagine the
    encoder and decoder as human translators who can each speak only two languages,
    with each having a different mother tongue. For our example, we’ll say the encoder
    is a native French speaker and the decoder is a native English speaker. The two
    have a second language in common: let’s say it’s Korean. To translate French into
    English, the encoder converts the French sentence into Korean (known as *context*)
    and passes on the context to the decoder. Since the decoder understands Korean,
    he or she can now translate from Korean into English. Working together, they can
    translate the French language to English, as illustrated by [Figure 1-1](#seq_to_seq_model_left_parenthesisneural).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq 模型由两部分组成：编码器和解码器。想象一下编码器和解码器就像是只能说两种语言的人类翻译员，每个人都有不同的母语。举个例子，我们假设编码器是以法语为母语的人，解码器是以英语为母语的人。他们有一个共同的第二语言：我们假设它是韩语。为了将法语翻译成英语，编码器将法语句子转换为韩语（称为*上下文*）并将上下文传递给解码器。由于解码器理解韩语，他或她现在可以从韩语翻译成英语。通过共同合作，他们可以将法语翻译成英语，如[图1-1](#seq_to_seq_model_left_parenthesisneural)所示。
- en: '![](Images/gpt3_0101.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0101.png)'
- en: Figure 1-1\. Seq2Seq model (neural machine translation)
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. Seq2Seq 模型（神经机器翻译）
- en: Transformer attention mechanisms
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer 注意力机制
- en: Transformer architecture was invented to improve AI’s performance on machine
    translation tasks. “Transformers started as language models,” Kilcher explains,
    “not even that large, but then they became large.”
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构被发明用于改进机器翻译任务中的 AI 性能。“Transformer 最初是作为语言模型的，”基尔切解释说，“甚至不是那么大，但后来它们变得很大。”
- en: 'To work with transformer models, you need to understand one more technical
    concept: attention. An *attention mechanism* is a technique that mimics cognitive
    attention: it looks at an input sequence, piece by piece and, on the basis of
    probabilities, decides at each step which other parts of the sequence are important.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用transformer模型，你需要了解另一个技术概念：attention。*注意机制*是一种模仿认知注意力的技术：它逐步查看输入序列，并根据概率决定每一步哪些序列的其他部分是重要的。
- en: For example, look at the sentence “The cat sat on the mat once it ate the mouse.”
    Does “it” in this sentence refer to “the cat” or “the mat”? The transformer model
    can strongly connect “it” with “the cat.” That’s attention.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看这个句子“猫吃了老鼠之后，坐在垫子上。”这个句子中的“它”指的是“猫”还是“垫子”？transformer模型可以将“它”与“猫”强烈联系起来。这就是注意力。
- en: Going back to our encoder and decoder example, imagine that the encoder writes
    down keywords that are important to the semantics of the sentence and gives them
    to the decoder along with the translation. Those keywords make the translation
    much easier for the decoder, who now knows what parts of the sentence are important
    and which terms give the sentence context.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的编码器和解码器示例，想象一下编码器写下了对句子语义重要的关键词，并将它们连同翻译一起给解码器。这些关键词使解码器的翻译变得更容易，解码器现在知道了句子中哪些部分是重要的，哪些术语给出了句子的上下文。
- en: 'The transformer model has two types of attention: *self-attention* (connection
    of words within a sentence) and *encoder-decoder attention* (connection between
    words from the source sentence to words from the target sentence).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型有两种注意力类型：*自注意力*（句子内单词的连接）和*编码器-解码器注意力*（源句子中的单词与目标句子中的单词之间的连接）。
- en: 'The attention mechanism helps the transformer filter out noise and focus on
    what’s relevant: connecting two words in a semantic relationship to each other,
    when the words in themselves do not carry any obvious markers pointing to one
    another.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制有助于transformer过滤噪声并关注相关内容：将两个单词连接起来，使它们在语义上相互关联，当这些单词本身没有任何明显的标记指向彼此时。
- en: Transformer models benefit from larger architectures and larger quantities of
    data. Training on large datasets and fine-tuning for specific tasks improve results.
    Transformers are better at understanding the context of words in a sentence than
    any other kind of neural network. GPT is just the decoder part of the transformer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型受益于更大的架构和更多的数据。在大型数据集上进行训练并针对特定任务进行微调可以改善结果。transformer比任何其他类型的神经网络更擅长理解句子中词语的上下文。GPT只是transformer的解码器部分。
- en: Now that you know what GPT means, let’s talk about that “3”—as well as 1 and
    2.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了GPT的含义，让我们谈谈那个“3”——以及1和2。
- en: A Brief History of GPT-3
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-3的简要历史
- en: GPT-3 was created by, and is a significant milestone for, OpenAI, a San Francisco-based
    pioneer of AI research. OpenAI’s [stated mission](https://oreil.ly/TUwij) is “to
    ensure that artificial general intelligence benefits all of humanity.” Artificial
    *general* intelligence is a type of AI that is not confined to specialized tasks
    but instead performs well at a variety of tasks, just like humans do.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3由旧金山的AI研究先驱OpenAI创建，并且是一个重要的里程碑。OpenAI的[声明使命](https://oreil.ly/TUwij)是“确保人工通用智能惠及全人类。”人工*通用*智能是一种不局限于专业任务的AI类型，而是在各种任务中表现良好，就像人类一样。
- en: GPT-1
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-1
- en: OpenAI presented GPT-1 in June 2018\. The developers’ [key finding](https://oreil.ly/21J4S)
    was that combining the transformer architecture with unsupervised pre-training
    yielded promising results. GPT-1, they write, was fine-tuned for specific tasks
    to achieve “strong natural language understanding.”
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2018年6月提出了GPT-1。开发者的[关键发现](https://oreil.ly/21J4S)是将transformer架构与无监督预训练相结合产生了有希望的结果。他们写道，GPT-1经过特定任务的微调，实现了“强大的自然语言理解”。
- en: GPT-1 served as an important stepping stone toward a language model with general
    language-based capabilities. It proved that language models can be effectively
    pre-trained, which could help them generalize well. The architecture could perform
    various NLP tasks with very little fine-tuning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1作为通向具有一般语言能力的语言模型的重要里程碑。它证明了语言模型可以有效地进行预训练，这有助于它们良好地泛化。该架构可以进行各种NLP任务，只需进行很少的微调。
- en: GPT-1 used the [Book Corpus](https://oreil.ly/OQtXS) dataset, which contains
    some seven thousand unpublished books, and the decoder part of the transformer
    with self-attention to train the model. The architecture remained largely the
    same as in the original transformer. The model had 117 million parameters. GPT-1
    opened avenues for future models, which could unleash this potential better with
    larger datasets and more parameters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1 使用了 [Book Corpus](https://oreil.ly/OQtXS) 数据集，其中包含约七千本未发表的书籍，以及变压器的解码器部分，其中包含自注意力以训练模型。该体系结构基本保持了与原始变压器相同。该模型有
    1.17 亿个参数。GPT-1 为未来模型打开了道路，这些模型可以利用更大的数据集和更多的参数更好地释放这一潜力。
- en: One of its achievements was decent zero-shot performance ability on various
    NLP tasks like question answering (Q&A) and sentiment analysis, due to pre-training.
    *Zero-shot learning* is the ability of a model to perform a task without having
    seen any example of that kind in the past; the model is supposed to understand
    the task without looking at any examples. *Zero-shot task transfer* is a setting
    in which the model is presented with few to no examples and asked to understand
    the task based on the examples and an instruction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其成就之一是在各种 NLP 任务上具有良好的零样本性能，如问答（Q&A）和情感分析，这归功于预训练。*零样本学习* 是指模型在过去没有看到任何该类示例的情况下执行任务的能力；模型应该在没有查看任何示例的情况下理解任务。*零样本任务转移*
    是一种设置，在该设置中，模型被呈现给少量甚至没有示例，并被要求基于示例和说明理解任务。
- en: GPT-2
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: In February 2019, OpenAI introduced GPT-2, which was bigger than GPT-1 but otherwise
    very similar. The major difference was that GPT-2 could multitask. It [successfully
    proved](https://oreil.ly/E8IEe) that a language model could perform well on several
    tasks without receiving any training examples for those tasks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 2019 年 2 月，OpenAI 推出了 GPT-2，它比 GPT-1 更大，但其他方面非常相似。主要区别在于 GPT-2 具有多任务处理能力。它 [成功地证明](https://oreil.ly/E8IEe)
    了语言模型可以在没有为这些任务接收任何训练示例的情况下在多个任务上表现良好。
- en: GPT-2 showed that training on a larger dataset and having more parameters improves
    a language model’s capability to understand tasks and surpass the state of the
    art of many tasks in zero-shot settings. It also showed that even larger language
    models would be even better at natural language understanding.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 表明，训练更大的数据集并拥有更多参数会提高语言模型理解任务并在零样本设置下超越许多任务的最新技术的能力。它还表明，甚至更大的语言模型在自然语言理解方面表现会更好。
- en: To create an extensive, high-quality dataset, the authors scraped Reddit and
    pulled data from outbound links of upvoted articles on the platform. The resulting
    dataset, WebText, had 40GB of text data from over eight million documents, far
    larger than GPT-1’s dataset. GPT-2 was trained on the WebText dataset and had
    1.5 billion parameters, 10 times more than GPT-1.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个广泛、高质量的数据集，作者们爬取了 Reddit 并从该平台上投票文章的外部链接中获取数据。得到的数据集 WebText 从超过八百万篇文档中提取了
    40GB 的文本数据，远远大于 GPT-1 的数据集。GPT-2 是在 WebText 数据集上训练的，并具有 15 亿个参数，比 GPT-1 大 10 倍。
- en: GPT-2 was evaluated on several datasets of downstream tasks like reading comprehension,
    summarization, translation, and question answering.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 在多个数据集上进行了评估，包括阅读理解、摘要、翻译和问答等下游任务。
- en: GPT-3
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3
- en: 'In the quest to build an even more robust and powerful language model, OpenAI
    built the GPT-3 model. Both its dataset and the model are about two orders of
    magnitude larger than those used for GPT-2: GPT-3 has 175 billion parameters and
    was trained on a mix of five different text corpora, a much bigger dataset than
    was used to train GPT-2\. The architecture of GPT-3 is largely the same as GPT-2\.
    It performs well on downstream NLP tasks in zero-shot and few-shot settings.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建更加健壮和强大的语言模型的过程中，OpenAI 构建了 GPT-3 模型。它的数据集和模型都比用于 GPT-2 的数据集和模型大约大两个数量级：GPT-3
    具有 1750 亿个参数，并且在训练过程中使用了五种不同文本语料库的混合数据，这是比 GPT-2 更大的数据集。GPT-3 的体系结构与 GPT-2 基本相同。它在零样本和少样本设置下在下游
    NLP 任务上表现良好。
- en: GPT-3 has capabilities like writing articles that are indistinguishable from
    human-written articles. It can also perform on-the-fly tasks for which it was
    never explicitly trained, like summing numbers, writing SQL queries, and even
    writing React and JavaScript code given a plain English description of the tasks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 具有撰写与人类撰写的文章无法区分的能力。它还可以执行即时任务，而这些任务从未被明确训练过，例如求和数字、编写 SQL 查询，甚至根据任务的简单英语描述编写
    React 和 JavaScript 代码。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Few-, one-, and zero-shot settings are specialized cases of zero-shot task transfer.
    In a *few-shot setting*, the model is provided with a task description and as
    many examples as fit into the context window of the model. In a *one-shot setting*,
    the model is provided with exactly one example and, in a *zero-shot setting*,
    with no example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本、一样本和零样本设置是零样本任务转移的专业案例。在*少样本设置*中，模型提供了一个任务描述和尽可能多的与模型上下文窗口相匹配的示例。在*一样本设置*中，模型提供了确切的一个示例，而在*零样本设置*中，则没有提供示例。
- en: 'OpenAI’s mission statement emphasizes the democratic and ethical aspects of
    AI. The democratic dimension lies in the decision to release the third version
    of the model, GPT-3, via a public API, or application programming interface: a
    software intermediary that sends information back and forth between a website
    or app and a user.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的使命声明强调了 AI 的民主和道德方面。民主维度在于决定通过公共 API 或应用程序编程接口发布第三版模型 GPT-3：一种软件中介，用于在网站或应用程序与用户之间来回发送信息。
- en: APIs act as messengers, allowing developers to build new programmatic interactions
    between applications and users. Releasing GPT-3 via an API was a revolutionary
    move. Until 2020, the powerful AI models developed by leading research labs were
    available to only a select few—researchers and engineers working on those projects.
    The OpenAI API gives users all over the world unprecedented access to the world’s
    most powerful language model via a simple sign-in. (OpenAI’s business rationale
    for this move is to create a new paradigm it calls “model-as-a-service” where
    developers can pay per API call; we will take a closer look at this in [Chapter 3](ch03.xhtml#programming_with_gpt_three).)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: API 充当着使开发人员能够在应用程序和用户之间建立新的程序化交互的信使。通过 API 发布 GPT-3 是一项革命性的举措。直到2020年，由领先研究实验室开发的强大
    AI 模型只对少数人开放——这些人是在这些项目上工作的研究人员和工程师。OpenAI API 通过简单的登录使全球用户首次获得对世界上最强大的语言模型的前所未有的访问权限。（OpenAI
    采取此举措的商业原因是创建一个称为“模型即服务”的新范式，开发人员可以按 API 调用付费；我们将在[第三章](ch03.xhtml#programming_with_gpt_three)中对此进行更详细的讨论。）
- en: OpenAI researchers experimented with different model sizes while working on
    GPT-3\. They took the existing GPT-2 architecture and increased the number of
    parameters. What emerged as a result of that experiment is a model with new and
    extraordinary capabilities in the form of GPT-3\. While GPT-2 displayed some zero-shot
    capabilities on downstream tasks, GPT-3 can carry out even more novel tasks when
    presented with example context.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 研究人员在研究 GPT-3 时尝试了不同的模型大小。他们采用了现有的 GPT-2 结构并增加了参数数量。作为实验结果，产生了具有新的和非凡能力的模型——GPT-3。虽然
    GPT-2 在下游任务中显示出了一些零样本能力，但当提供示例上下文时，GPT-3 可以执行更多的新任务。
- en: '[OpenAI researchers found it remarkable](https://oreil.ly/e22dR) that merely
    scaling the model parameters and the size of the training dataset led to such
    extraordinary advances. They are generally optimistic that these trends will continue
    even for models much larger than GPT-3, enabling ever-stronger learning models
    capable of few-shot or zero-shot learning just by fine-tuning on a small sample
    size.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI 研究人员发现](https://oreil.ly/e22dR)，仅仅通过扩展模型参数和训练数据集的规模就能取得如此非凡的进展，这令人惊讶。他们普遍乐观地认为，即使是比
    GPT-3 更大的模型，也将延续这些趋势，使得只需在少量样本上进行微调就能实现少样本或零样本学习的更强大的学习模型成为可能。'
- en: As you read this book, [experts estimate](https://oreil.ly/0TE9t) that language
    models based on a trillion parameters are probably being built and deployed. We
    have entered the golden age of large language models, and now it’s time for you
    to become a part of it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读本书时，[专家估计](https://oreil.ly/0TE9t)，基于万亿参数的语言模型可能正在被构建和部署。我们已经进入了大型语言模型的黄金时代，现在是你成为其中一员的时候了。
- en: GPT-3 has captured a lot of public attention. The *MIT Technology Review* considered
    GPT-3 one of the [10 Breakthrough Technologies of 2021](https://oreil.ly/mHAKG).
    Its sheer flexibility in performing a series of generalized tasks with near-human
    efficiency and accuracy is what makes it so exciting, as early adopter Arram Sabeti
    tweeted ([Figure 1-2](#tweet_from_arram_sabeti)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 吸引了大量的公众注意力。*麻省理工科技评论*将 GPT-3 视为[2021年的十大突破性技术之一](https://oreil.ly/mHAKG)。它在执行一系列广义任务时表现出近乎人类的效率和准确性，这种出色的灵活性正是它如此令人兴奋的原因，正如早期采用者
    Arram Sabeti 所推文的（[图1-2](#tweet_from_arram_sabeti)）。
- en: '![](Images/gpt3_0102.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0102.png)'
- en: Figure 1-2\. Tweet from [Arram Sabeti](https://oreil.ly/chjI4)
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 来自[Arram Sabeti](https://oreil.ly/chjI4)的推文
- en: The API release created a paradigm shift in NLP and attracted a huge number
    of beta testers. Innovations and start-ups followed at lightning speed, with commentators
    calling GPT-3 a [“fifth Industrial Revolution”](https://oreil.ly/fZarJ).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: API 的发布在自然语言处理领域引起了一场范式转变，并吸引了大量的测试者。创新和初创企业以闪电般的速度相继而至，评论家称 GPT-3 为[“第五次工业革命”](https://oreil.ly/fZarJ)。
- en: Within just nine months of the launch of the API, [according to OpenAI](https://oreil.ly/TiEVy),
    people were building more than three hundred businesses with it. Despite this
    suddenness, some experts argue that the excitement isn’t exaggerated. Bakz Awan
    is a developer turned entrepreneur and influencer, and one of the major voices
    in the OpenAI API developer community. He has a [YouTube channel “Bakz T. Future”](https://oreil.ly/W1cWX)
    and a [podcast](https://oreil.ly/07RBY). Awan argues that GPT-3 and other models
    are actually “underhyped for how usable and friendly and fun and powerful they
    really are. It’s almost shocking.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 API 推出仅九个月后，[根据 OpenAI 的说法](https://oreil.ly/TiEVy)，已经有超过三百个业务利用它展开发展。尽管如此突然，一些专家认为这种兴奋并不夸张。Bakz
    Awan 曾是一名开发者，现在是一名企业家和意见领袖，也是 OpenAI API 开发者社区的主要声音之一。他有一个 [YouTube 频道“Bakz T.
    Future”](https://oreil.ly/W1cWX) 和一个 [播客](https://oreil.ly/07RBY)。Awan 认为，GPT-3
    和其他模型实际上是“被低估了，它们的可用性、友好性、趣味性和强大性实际上是让人震惊的。”
- en: 'Daniel Erickson, CEO of Viable, which has a GPT-3-powered product, praises
    the model’s ability to extract insights from large datasets through what he calls
    *prompt-based development*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Viable 的 CEO Daniel Erickson，其公司拥有一款由 GPT-3 提供支持的产品，赞扬该模型通过他所称的*提示驱动式开发*从大型数据集中提取见解的能力。
- en: 'Companies going down that path cover use cases such as generating copy for
    ads and websites. The design philosophy is relatively simple: the company takes
    your data in, sends it over into a prompt, and displays the API-generated result.
    It solves a task that is easily done by a single API prompt and wraps [a] UI around
    that to deliver it to the users.'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 涉及这一领域的公司的使用案例包括为广告和网站生成文案。设计理念相对简单：公司将您的数据传入，转换为提示，并显示 API 生成的结果。它解决了一个只需要单个
    API 提示就可以轻松完成的任务，并将 [一个] 用户界面围绕起来，将其交付给用户。
- en: The problem Erickson sees with this category of use cases is that it is already
    overcrowded, attracting many ambitious start-up founders competing with similar
    services. Instead, Erickson recommends looking at another category of use cases
    instead, as Viable did. Data-driven use cases are not as crowded as prompt-generation
    use cases, but they are more profitable and allow you to easily create a security
    “moat.”
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Erickson 认为，这一类使用案例的问题在于已经过度拥挤，吸引了许多雄心勃勃的初创企业创始人竞相提供类似的服务。相反，Erickson 建议转而考虑另一类使用案例，就像
    Viable 所做的那样。基于数据驱动的使用案例没有提示生成使用案例那样拥挤，但它们更有利可图，并且可以轻松地创建一个安全的“壕沟”。
- en: The key, Erickson says, is to build a large dataset that you can keep adding
    to and that can provide potential insights. GPT-3 will help you extract valuable
    insights from it. At Viable, this was the model that let them monetize easily.
    “People pay a lot more money for data than they do for prompt output,” Erickson
    explains.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Erickson 表示，关键在于建立一个大型数据集，并不断添加数据，以提供潜在的见解。GPT-3 将帮助您从中提取有价值的见解。在 Viable，这就是让他们轻松实现盈利的模型。“人们为数据付的钱比为提示输出付的钱要多得多，”Erickson
    解释道。
- en: It should be noted that technological revolutions also bring controversies and
    challenges. GPT-3 is a powerful tool in the hands of anyone trying to create a
    narrative. Without great care and benevolent intentions, one such challenge we
    will face is curbing the attempts to use the algorithm to spread misinformation
    campaigns. Another one would be eradicating its use for generating mass quantities
    of low-quality digital content that will then pollute the information available
    on the internet. Yet another one is the limitations of its datasets that are filled
    with various kinds of bias, which can be amplified by this technology. We will
    look closer at these and more challenges in [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and),
    along with discussing the various efforts by OpenAI to address them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，技术革命也带来了争议和挑战。对于试图塑造叙事的任何人来说，GPT-3 是一个强大的工具。如果没有足够的关怀和善意，我们将面临的一个挑战是遏制试图利用该算法传播虚假信息的企图。另一个挑战将是消除其用于生成大量低质量数字内容的用途，这些内容将污染互联网上可用的信息。还有一个挑战是其数据集的局限性，这些数据集充满了各种各样的偏见，而这些偏见可能会被这项技术放大。我们将在[第六章](ch06.xhtml#challengescomma_controversiescomma_and)中更详细地探讨这些挑战，同时讨论
    OpenAI 为解决这些挑战所做的各种努力。
- en: Accessing the OpenAI API
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问 OpenAI API
- en: As of 2021, the market has already produced several proprietary AI models that
    have more parameters than GPT-3\. However, access to them is limited to a handful
    of people within the company’s R&D walls, making it impossible to evaluate their
    performance on real-world NLP tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2021 年，市场已经产生了几种拥有比 GPT-3 更多参数的专有 AI 模型。然而，对它们的访问仅限于公司研发部门内的少数人，这使得不可能评估它们在现实世界
    NLP 任务中的性能。
- en: One factor that makes GPT-3 accessible is its simple and intuitive “text-in,
    text-out” user interface. It doesn’t require complex, gradient fine-tuning or
    updates, and you don’t need to be an expert to use it. This combination of scalable
    parameters and relatively open access makes GPT-3 the most exciting, and arguably
    the most relevant, language model to date.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使 GPT-3 可访问的一个因素是其简单直观的“文本输入，文本输出”用户界面。它不需要复杂的、渐变微调或更新，你也不需要是专家来使用它。这种可扩展的参数和相对开放的访问结合起来，使
    GPT-3 成为迄今为止最令人兴奋，也可以说是最相关的语言模型。
- en: Due to GPT-3’s extraordinary capabilities, there are significant risks in terms
    of security and misuse associated with making it open source, which we will cover
    in [Chapter 7](ch07.xhtml#democratizing_access_to_ai). Taking that into account,
    OpenAI decided not to release the source code of GPT-3 publicly and came up with
    a unique access sharing model via an API.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPT-3 的非凡能力，与开源相关的安全性和误用方面存在着重大风险，我们将在[第七章](ch07.xhtml#democratizing_access_to_ai)中讨论。考虑到这一点，OpenAI
    决定不公开发布 GPT-3 的源代码，并通过 API 提出了一种独特的访问共享模型。
- en: The company decided to initially release access to the API in the form of a
    limited beta user list. The application process required people to complete a
    form detailing their background and reasons for requesting API access. Only approved
    users were granted access to a private beta of the API with an interface called
    Playground.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 公司决定最初以有限的 beta 用户列表的形式发布对 API 的访问权限。申请过程要求人们填写一份详细说明他们背景和请求 API 访问权限的原因的表格。只有批准的用户才被授予访问名为
    Playground 的 API 的私人 beta 权限。
- en: In its early days, the waiting list for GPT-3 beta access consisted of tens
    of thousands of people. OpenAI swiftly managed the applications that started pouring
    in, adding developers in batches but also closely monitoring their activity and
    feedback about the API user experience in order to continuously improve it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，GPT-3 beta 访问的等待列表中有数万人。OpenAI 迅速处理了开始涌入的申请，分批添加开发者，但也密切监控他们对 API 用户体验的活动和反馈，以持续改进它。
- en: Thanks to the progress with safeguards, OpenAI removed the waiting list in November
    2021\. GPT-3 is now openly accessible via a [simple sign-in](https://oreil.ly/uSkAH).
    This is a great milestone in the history of GPT-3 and a highly requested move
    by the community. To get API access, simply go to the [sign-up page](https://oreil.ly/e9hFU),
    sign up for a free account, and start experimenting with it right away.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于安全保障措施的进展，OpenAI 在 2021 年 11 月取消了等待列表。GPT-3 现在可以通过[简单的登录](https://oreil.ly/uSkAH)来开放访问。这是
    GPT-3 历史上的一个重要里程碑，也是社区强烈要求的一步。要获取 API 访问权限，只需转到[注册页面](https://oreil.ly/e9hFU)，注册一个免费账户，立即开始尝试它。
- en: New users initially get a pool of free credits that allows them to freely experiment
    with the API. The number of credits is equivalent to creating text content as
    long as three average-length novels. After the free credits are used, users start
    paying for usage or, if they have a need, they can request additional credits
    from OpenAI API customer support.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 新用户最初会获得一定数量的免费信用额，可以自由地尝试 API。信用额相当于创建长度为三本普通长度小说的文本内容。免费信用额用完后，用户开始支付使用费用，或者如果有需要，他们可以向
    OpenAI API 客户支持申请额外的信用额。
- en: OpenAI strives to ensure that API-powered applications are built responsibly.
    For that reason, it provides [tools](https://oreil.ly/cHrcg), [best practices](https://oreil.ly/n0xRW),
    and [usage guidelines](https://oreil.ly/HwWSa) to help developers bring their
    applications to production quickly and safely.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 努力确保 API 驱动的应用程序被负责任地构建。因此，它提供[工具](https://oreil.ly/cHrcg)，[最佳实践](https://oreil.ly/n0xRW)，和[使用指南](https://oreil.ly/HwWSa)来帮助开发者快速安全地将他们的应用程序投入生产。
- en: The company has also created [content guidelines](https://oreil.ly/o6BLo) to
    clarify what kind of content the OpenAI API can be used to generate. To help developers
    ensure their applications are used for the intended purpose, prevent potential
    misuse, and adhere to the content guidelines, OpenAI offers a free content filter.
    OpenAI policy prohibits the use of the API in ways that do not adhere to the principles
    described in its [charter](https://oreil.ly/nTK3V), including content that promotes
    hate, violence, or self-harm, or that intends to harass, influence political processes,
    spread misinformation, spam content, and so on.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该公司还制定了[内容指南](https://oreil.ly/o6BLo)，以明确 OpenAI API 可用于生成何种类型的内容。为帮助开发者确保其应用程序用于预期目的，防止潜在的误用，并遵守内容指南，OpenAI
    提供了免费的内容过滤器。OpenAI 政策禁止 API 的使用方式违反其[宪章](https://oreil.ly/nTK3V)中描述的原则，包括宣扬仇恨、暴力或自我伤害的内容，或者意图骚扰、影响政治进程、传播错误信息、发送垃圾内容等行为。
- en: Once you have signed up for an OpenAI account, you can move on to [Chapter 2](ch02.xhtml#using_the_openai_api),
    where we will discuss the different components of the API, the GPT-3 Playground,
    and how to use the API to the best of its abilities for different use cases.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您注册了 OpenAI 账户，您可以继续阅读[第二章](ch02.xhtml#using_the_openai_api)，我们将讨论 API 的不同组件、GPT-3
    Playground，以及如何针对不同的用例最大限度地利用 API 的能力。
- en: '^([1](ch01.xhtml#ch01fn1-marker)) Malcolm Gladwell, *Outliers: The Story of
    Success* (Little, Brown, 2008).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.xhtml#ch01fn1-marker)) 马尔科姆·格拉德威尔，《异类：成功的故事》（小布朗，2008年）。
- en: ^([2](ch01.xhtml#ch01fn2-marker)) Ashish Vaswani et al., [“Attention Is All
    You Need,”](https://oreil.ly/8rByF) *Advances in Neural Information Processing
    Systems* 30 (2017).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.xhtml#ch01fn2-marker)) Ashish Vaswani 等人，《[注意力机制就是你所需要的一切](https://oreil.ly/8rByF)》，*神经信息处理系统进展*
    30（2017）。
