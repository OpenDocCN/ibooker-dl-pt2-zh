- en: 1 Introduction to serverless machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: What serverless machine learning is and why you should care
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between machine learning code and a machine learning platform
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How this book teaches about serverless machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target audience for this book
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you can learn from this book
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Grand Canyon—like gulf separates experimental machine learning code and production
    machine learning systems. The scenic view across the “canyon” is magical: when
    a machine learning system is running successfully in production it can seem prescient.
    The first time I started typing a query into a machine learning—powered autocomplete
    search bar and saw the system anticipate my words, I was hooked. I must have tried
    dozens of different queries to see how well the system worked. So, what does it
    take to trek across the “canyon?”'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: It is surprisingly easy to get started. Given the right data and less than an
    hour of coding time, it is possible to write the experimental machine learning
    code and re-create the remarkable experience I have had using the search bar that
    predicted my words. In my conversations with information technology professionals,
    I find that many have started to experiment with machine learning. Online classes
    in machine learning, such as the one from Coursera and Andrew Ng, have a wealth
    of information about how to get started with machine learning basics. Increasingly,
    companies that hire for information technology jobs expect entry-level experience
    with machine learning.[¹](01.htm#pgfId-1011823)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: While it is relatively easy to experiment with machine learning, building on
    the results of the experiments to deliver products, services, or features has
    proven to be difficult. Some companies have even started to use the word *unicorn*
    to describe the unreasonably hard-to-find machine learning practitioners with
    the skills needed to launch production machine learning systems. Practitioners
    with successful launch experience often have skills that span machine learning,
    software engineering, and many information technology specialties.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'This book is for those who are interested in trekking the journey from experimental
    machine learning code to a production machine learning system. In this book, I
    will teach you how to assemble the components for a machine learning platform
    and use them as a foundation for your production machine learning system. In the
    process, you will learn:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: How to use and integrate public cloud services, including the ones from Amazon
    Web Services (AWS), for machine learning, including data ingest, storage, and
    processing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to assess and achieve data quality standards for machine learning from structured
    data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to engineer synthetic features to improve machine learning effectiveness
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to reproducibly sample structured data into experimental subsets for exploration
    and analysis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement machine learning models using PyTorch and Python in a Jupyter
    notebook environment
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement data processing and machine learning pipelines to achieve both
    high throughput and low latency
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train and deploy machine learning models that depend on data processing
    pipelines
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to monitor and manage the life cycle of your machine learning system once
    it is put in production
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why should you invest the time to learn these skills? They will not make you
    a renowned machine learning researcher or help you discover the next ground-breaking
    machine learning algorithm. However, if you learn from this book, you can prepare
    yourself to deliver the results of your machine learning efforts sooner and more
    productively, and grow to be a more valuable contributor to your machine learning
    project, team, or organization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What is a machine learning platform?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have never heard of the phrase “yak shaving” as it is used in the information
    technology industry,[²](01.htm#pgfId-1011972) here’s a hypothetical example of
    how it may show up during a day in a life of a machine learning practitioner:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: My company wants our machine learning system to launch in a month . . . but
    it is taking us too long to train our machine learning models . . . so I should
    speed things up by enabling graphical processing units (GPUs) for training . .
    . but our GPU device drivers are incompatible with our machine learning framework
    . . . so I need to upgrade to the latest Linux device drivers for compatibility
    . . . which means that I need to be on the new version of the Linux distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: There are many more similar possibilities in which you need to “shave a yak”
    to speed up machine learning. The contemporary practice of launching machine learning—based
    systems in production and keeping them running has too much in common with the
    yak-shaving story. Instead of focusing on the features needed to make the product
    a resounding success, too much engineering time is spent on apparently unrelated
    activities like re-installing Linux device drivers or searching the web for the
    right cluster settings to configure the data processing middleware.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Why is that? Even if you have the expertise of machine learning PhDs on your
    project, you still need the support of many information technology services and
    resources to launch the system. “Hidden Technical Debt in Machine Learning Systems,”
    a peer-reviewed article published in 2015 and based on insights from dozens of
    machine learning practitioners at Google, advises that mature machine learning
    systems “end up being (at most) 5% machine learning code” ([http://mng.bz/01jl](http://mng.bz/01jl)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: This book uses the phrase “machine learning platform” to describe the 95% that
    play a supporting yet critical role in the entire system. Having the right machine
    learning platform can make or break your product.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: If you take a closer look at figure 1.1, you should be able to describe some
    of the capabilities you need from a machine learning platform. Obviously, the
    platform needs to ingest and store data, process data (which includes applying
    machine learning and other computations to data), and serve the insights discovered
    by machine learning to the users of the platform. The less obvious observation
    is that the platform should be able to handle multiple, concurrent machine learning
    projects and enable multiple users to run the projects in isolation from each
    other. Otherwise, replacing only the machine learning code translates to reworking
    95% of the system.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察图1.1，你应该能够描述一些从机器学习平台中所需要的功能。显然，平台需要摄入和存储数据，处理数据（其中包括将机器学习和其他计算应用于数据），并向平台的用户提供由机器学习发现的洞见。不够明显的观察是，平台应该能够处理多个并发的机器学习项目，并使多个用户相互隔离地运行这些项目。否则，仅替换机器学习代码就意味着需要重做系统的95%。
- en: '![01-01](Images/01-01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![01-01](Images/01-01.png)'
- en: Figure 1.1 Although machine learning code is what makes your machine learning
    system stand out, it amounts to only about 5% of the system code according to
    the experiences described in “Hidden Technical Debt in Machine Learning Systems”
    by Google’s Sculley et al. Serverless machine learning helps you assemble the
    other 95% using cloud-based infrastructure.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 尽管机器学习代码是使你的机器学习系统脱颖而出的原因，但根据谷歌的Sculley等人在“机器学习系统中的隐含技术债务”中所描述的经验，它仅占系统代码的5%。无服务器机器学习帮助你使用基于云的基础设施组装其余的95%。
- en: 1.2 Challenges when designing a machine learning platform
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 设计机器学习平台时的挑战
- en: 'How much data should the platform be able to store and process? [AcademicTorrents.com](http://AcademicTorrents.com)
    is a website dedicated to helping machine learning practitioners get access to
    public data sets suitable for machine learning. The website lists over 50 TB of
    data sets, of which the largest are 1—5 TB in size. Kaggle, a website popular
    for hosting data science competitions, includes data sets as large as 3 TB. You
    might be tempted to ignore the largest data sets as outliers and focus on more
    common data sets that are at the scale of gigabytes. However, you should keep
    in mind that successes in machine learning are often due to reliance on larger
    data sets. “The Unreasonable Effectiveness of Data,” by Peter Norvig et al. ([http://mng.bz/5Zz4](http://mng.bz/5Zz4)),
    argues in favor of the machine learning systems that can take advantage of larger
    data sets: “simple models and a lot of data trump more elaborate models based
    on less data.”'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个平台应该能够存储和处理多少数据？[AcademicTorrents.com](http://AcademicTorrents.com)是一个致力于帮助机器学习实践者获取适用于机器学习的公共数据集的网站。该网站列出了超过50TB的数据集，其中最大的数据集大小在1到5TB之间。Kaggle是一个流行的举办数据科学竞赛的网站，包括最大为3TB的数据集。你可能会忽略最大的数据集，并将关注点集中在千兆字节级别的常见数据集上。然而，你应该记住，在机器学习中取得成功通常是基于更大的数据集依赖的。Peter
    Norvig等人在《数据的不合理有效性》中（[http://mng.bz/5Zz4](http://mng.bz/5Zz4)）提出了利用更大的数据集的机器学习系统“简单的模型和大量的数据胜过基于较少数据的更为精细的模型”的观点。
- en: A machine learning platform that is expected to operate on a scale of terabytes
    to petabytes of data for storage and processing must be built as a distributed
    computing system using multiple inter-networked servers in a cluster, each processing
    a part of the data set. Otherwise, a data set with hundreds of gigabytes to terabytes
    will cause out-of-memory problems when processed by a single server with a typical
    hardware configuration. Having a cluster of servers as part of a machine learning
    platform also addresses the input/output bandwidth limitations of individual servers.
    Most servers can supply a CPU with just a few gigabytes of data per second. This
    means that most types of data processing performed by a machine learning platform
    can be sped up by splitting up the data sets in chunks (sometimes called *shards*)
    that are processed in parallel by the servers in the cluster. The distributed
    systems design for a machine learning platform as described is commonly known
    as *scaling out*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: A significant portion of figure 1.1 is the serving part of the infrastructure
    used in the platform. This is the part that exposes the data insights produced
    by the machine learning code to the users of the platform. If you have ever had
    your email provider classify your emails as spam or not spam, or if you have ever
    used a product recommendation feature of your favorite e-commerce website, you
    have interacted as a user with the serving infrastructure part of a machine learning
    platform. The serving infrastructure for a major email or an e-commerce provider
    needs to be capable of making the decisions for millions of users around the globe,
    millions of times a second. Of course, not every machine learning platform needs
    to operate at this scale. However, if you are planning to deliver a product based
    on machine learning, you need to keep in mind that it is within the realm of possibility
    for digital products and services to reach hundreds of millions of users in months.
    For example, Pokemon Go, a machine learning—powered video game from Niantic, reached
    half a billion users in less than two months.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Is it prohibitively expensive to launch and operate a machine learning platform
    at scale? As recently as the 2000s, running a scalable machine learning platform
    would have required a significant upfront investment in servers, storage, networking
    as well as software and the expertise needed to build one. The first machine learning
    platform I worked on for a customer back in 2009 cost over $100,000 USD and was
    built using on-premises hardware and open source Apache Hadoop (and Mahout) middleware.
    In addition to upfront costs, machine learning platforms can be expensive to operate
    due to waste of resources: most machine learning code underutilizes the capacity
    of the platform. As you know, the training phase of machine learning is resource-intensive,
    leading to high utilization of computing, storage, and networking. However, trainings
    are intermittent and are relatively rare for a machine learning system in production,
    translating to low average utilization. Serving infrastructure utilization varies
    based on the specific use case for a machine learning system and fluctuates based
    on factors like time of day, seasonality, marketing events, and more.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在规模上启动和运营机器学习平台成本很高吗？就在 2000 年代，运行一个可扩展的机器学习平台需要显著的前期投资，包括服务器、存储、网络以及构建平台所需的软件和专业知识。我在
    2009 年为一家客户开发的第一个机器学习平台花费超过 10 万美元，采用的是基于本地硬件和开源的 Apache Hadoop（和 Mahout）中间件。除了前期成本之外，机器学习平台的运营成本也可能很高，原因是资源浪费：大多数机器学习代码没有充分利用平台的能力。如您所知，机器学习的训练阶段对计算、存储和网络的利用率要求很高。然而，训练是间歇性的，在生产环境中，机器学习系统相对较少进行训练，平均利用率较低。用于服务的基础设施利用率因机器学习系统的特定用例而异，并根据一天中的时间、季节性、营销活动等因素而波动。
- en: 1.3 Public clouds for machine learning platforms
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 机器学习平台的公共云
- en: 'The good news is that public cloud-computing infrastructure can help you create
    a machine learning platform and address the challenges described in the previous
    section. In particular, the approach described in this book will take advantage
    of public clouds from vendors like Amazon Web Services, Microsoft Azure, or Google
    Cloud to provide your machine learning platform with:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，公共云计算基础设施可以帮助您创建一个机器学习平台，并解决前一节中描述的挑战。特别是，本书介绍的方法将利用像亚马逊网络服务（Amazon Web
    Services）、微软 Azure 或谷歌云这样的公共云，为您的机器学习平台提供以下功能：
- en: Secure isolation so that multiple users of your platform can work in parallel
    with different machine learning projects and code
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安全隔离，使您平台的多个用户能够并行工作，处理不同的机器学习项目和代码
- en: Access to information technologies like data storage, computing, and networking
    when your projects need them and for as long as they are needed
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您的项目需要时，能够获取数据存储、计算和网络等信息技术，并在需要的时间内持续使用
- en: Metering based on consumption so that your machine learning projects are billed
    just for the resources you used
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按消费计量，以便仅为您使用的资源结算机器学习项目的费用
- en: 'This book will teach you how to create a machine learning platform from public
    cloud infrastructure using Amazon Web Services as the primary example. In particular,
    I will teach you:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将教您如何使用公共云基础设施创建一个机器学习平台，以亚马逊网络服务为主要示例。具体而言，我将教您：
- en: How to use public cloud services to cost effectively store data sets regardless
    of whether they are made of kilobytes of terabytes of data
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用公共云服务以高效低成本地存储数据集，无论数据集是由几千字节还是几百万个字节组成
- en: How to optimize the utilization and cost of your machine learning platform computing
    infrastructure so that you are using just the servers you need
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何优化您的机器学习平台计算基础设施的利用率和成本，以便使用所需的服务器
- en: How to elastically scale your serving infrastructure to reduce the operational
    costs of your machine learning platform
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何弹性地扩展您的服务基础设施，以降低机器学习平台的运营成本
- en: 1.4 What is serverless machine learning?
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 什么是无服务器（Serverless）机器学习？
- en: '**Serverless machine learning** is a model for the software development of
    machine learning code written to run on a machine learning platform hosted in
    a cloud-computing infrastructure with consumption-based metering and billing.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**无服务器机器学习**是一种机器学习代码的软件开发模型，旨在在托管在云计算基础设施中的机器学习平台上运行，并采用按使用量计量和计费的模式。'
- en: If a machine learning system runs on a server-based cloud-computing infrastructure,
    why is this book about *serverless* machine learning? The idea of using servers
    from a public cloud for a machine learning platform clearly contradicts the premise
    of server*less*. Machine learning without servers? How is that even possible?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Before you object to the use of the word *serverless* in the definition, keep
    in mind that information technology professionals working with cloud-computing
    platforms have adopted serverless as a moniker to describe an approach to using
    cloud computing, including computing, storage, and networking, as well as other
    cloud resources and services, in a way that helps them spend their time more effectively,
    improve their productivity, and optimize costs. Serverless does not mean without
    servers; it means that when using a serverless approach a developer can ignore
    the existence of servers in a cloud provider and focus on writing code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Serverless, as it is used in this book, describes an approach for building
    machine learning systems that enables the machine learning practitioners to spend
    as much of their time as possible on writing machine learning code and to spend
    as little of their time as possible on managing and maintaining the computing,
    storage, networking, and operating systems; middleware; or any other parts of
    the underlying information technology needed to host and run the machine learning
    platform. Serverless machine learning also delivers on a key idea for cost optimization
    in cloud-computing: consumptive billing. This means that with serverless machine
    learning, you are billed just for the resources and services that you use.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine learning*, as used in academia as well as in the information technology
    industry, covers a broad spectrum of algorithms and systems, including those that
    defeated top human players at the ancient board game of Go, won on the TV show
    *Jeopardy*, and generated deep-fake images of the world’s celebrities and leaders.
    This book focuses on a specific subfield of machine learning known as supervised
    learning with structured (tables of rows and columns) data. If you are worried
    that this subfield is too narrow, note that over 80% of production machine learning
    systems implemented and used at various stages of maturity at Google, arguably
    the leader in adopting machine learning, are built using supervised learning from
    structured data sets.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Why serverless machine learning?
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prior to serverless machine learning, developers involved in getting machine
    learning code to run in production had to either work in concert with team members
    from an operations organization or take on the operations role themselves (this
    is known in the industry as DevOps). The responsibility of the development role
    included writing the machine learning code, for example, the code to perform inference,
    such as estimating a house sales price from real estate property records. Once
    the code was ready, the developers packaged it, typically as part of a machine
    learning framework such as PyTorch (more about PyTorch in part 2) or along with
    external code libraries so that it could be executed as an application (or a microservice)
    on a server, as shown in figure 1.2.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在无服务器机器学习之前，涉及将机器学习代码投入生产的开发人员必须要么与运维组织的团队成员合作，要么自己担任运维角色（这在行业中被称为DevOps）。开发角色的职责包括编写机器学习代码，例如执行推断的代码，例如从房地产物业记录中估算房屋销售价格的代码。一旦代码准备就绪，开发人员将其打包，通常作为机器学习框架（例如第2部分中的PyTorch）的一部分，或与外部代码库一起，以便在服务器上作为应用程序（或微服务）执行，如图1.2所示。
- en: '![01-02](Images/01-02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![01-02](Images/01-02.png)'
- en: 'Figure 1.2 Before serverless platforms, most cloud-based machine learning platforms
    relied on infrastructure-as-a-service (IaaS) or platform-as-a-service (PaaS) service
    models, illustrated in the figure. Both IaaS and PaaS require an operations role
    responsible for instantiating infrastructure: server-based, in the case of IaaS,
    or application-based, in the case of PaaS. Operations are also responsible for
    managing the life cycle of the infrastructure once it is running.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 在无服务器平台之前，大多数基于云的机器学习平台都依赖于基础设施即服务（IaaS）或平台即服务（PaaS）服务模型，如图所示。在IaaS的情况下，由于运维的作用，基础结构是基于服务器的，而在PaaS的情况下，基础结构是基于应用程序的。一旦它们开始运行，运维也需要负责管理基础架构的生命周期。
- en: The operations role involved instantiating the infrastructure required to run
    the code while ensuring the infrastructure had the appropriate capacity (memory,
    storage, bandwidth). The role also was responsible for configuring the server
    infrastructure with the operating system, middleware, updates, security patches,
    and other prerequisites. Next, operations started the execution of the developer’s
    code as one or more application instances. After the code was up and running,
    operations managed the execution of the code, ensuring that requests were serviced
    with high availability (i.e., reliably) and low latency (i.e., responsively).
    Operations were also called to help reduce costs by optimizing infrastructure
    utilization. This meant continuously monitoring the levels of CPU, storage, network
    bandwidth, and service latency in order to change the infrastructure capacity
    (e.g., de-provision servers) and achieve target utilization goals.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 运维角色涉及实例化运行代码所需的基础设施，同时确保基础架构具有适当的容量（内存、存储、带宽）。该角色还负责配置服务器基础架构，包括操作系统、中间件、更新、安全补丁和其他先决条件。接下来，运维人员启动一个或多个应用程序实例来执行开发人员的代码。在代码启动和运行后，操作将管理代码执行，确保请求得到高可用性（即可靠）和低延迟性（即响应迅速）。
    运维还被要求通过优化基础设施利用率来帮助降低成本。这意味着不断监视CPU、存储、网络带宽和服务延迟水平，以改变基础架构容量（例如取消服务器）并实现目标利用率目标。
- en: 'Cloud-computing service models such as IaaS replaced physical servers with
    virtual servers and thus made operations more productive: it took significantly
    less time and effort to provision and de-provision virtual servers than physical
    ones. The operations were further automated in the cloud with features such as
    auto-scaling, which automatically provisioned and de-provisioned virtual servers
    depending on near-real-time measurements of CPU, memory, and other server-level
    metrics. PaaS, a more abstract cloud service model, further reduced the operation
    overhead with virtual servers preconfigured for code execution runtimes, along
    with pre-installed middleware and operating systems.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: IaaS等云计算服务模型用虚拟服务器代替物理服务器，从而使运维更加高效：与物理服务器相比，虚拟服务器的配置和销毁需要更少的时间和精力。云中的运维进一步自动化，具有自动扩展等功能，根据CPU、内存和其他服务器级指标的准实时测量来自动配置和取消配置虚拟服务器。PaaS是一种更抽象的云服务模型，通过为代码执行运行时预配置虚拟服务器，以及预安装中间件和操作系统，进一步减少了操作负担。
- en: While cloud-computing service models like IaaS and PaaS worked well for the
    serving infrastructure part of machine learning platforms, they fell short elsewhere.
    While performing exploratory data analysis as preparation for training, a machine
    learning engineer may execute dozens of different queries against data before
    settling on the right one. In IaaS and PaaS models, this means that the infrastructure
    handling data analysis queries needs to be provisioned (sometimes by the operations
    team) even before the first query can execute. To make matters worse, the utilization
    of the provisioned infrastructure is entirely at a whim of the user. In an extreme
    example, if the machine learning engineer runs just one data analysis query a
    day and it takes 1 hour to execute, the data analysis infrastructure can end up
    idle while still incurring costs for the other 23 hours of the day.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像 IaaS 和 PaaS 这样的云计算服务模型在机器学习平台的服务基础设施部分工作良好，但在其他方面却表现不佳。在进行训练前的探索性数据分析时，机器学习工程师可能需要对数据执行数十个不同的查询才能找到正确的查询。在
    IaaS 和 PaaS 模型中，这意味着数据分析查询的基础设施需要被预配（有时是由运维团队完成）甚至在第一个查询被执行之前就要进行预配。更糟糕的是，预配基础设施的使用完全取决于用户的心情。在极端情况下，如果机器学习工程师每天只运行一个数据分析查询，并且需要一个小时才能执行，那么数据分析基础设施可能在一天的其他
    23 个小时内处于空闲状态，同时仍然产生成本。
- en: 1.5.1 Serverless vs. IaaS and PaaS
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.1 Serverless 与 IaaS 和 PaaS 的比较
- en: In contrast, the serverless approach illustrated in figure 1.3 helps further
    optimize the utilization and costs of the machine learning platform. Serverless
    platforms eliminate the need for performing traditional operations tasks. With
    serverless machine learning, the machine learning platform takes over the entire
    life cycle of the machine learning code, instantiating and managing it. This is
    accomplished by the platform hosting a dedicated runtime for different programming
    languages and functions. For example, a service runtime exists to execute Python
    code for running machine learning model training, another to execute SQL code
    for structured data queries, and more.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，图 1.3 中所示的 Serverless 方法有助于进一步优化机器学习平台的利用率和成本。 Serverless 平台消除了执行传统操作任务的需要。使用
    Serverless 机器学习，机器学习平台接管了整个机器学习代码的生命周期，对其进行实例化和管理。这是通过平台为不同的编程语言和函数提供专用运行时来实现的。例如，存在一个服务运行时来执行
    Python 代码以运行机器学习模型训练，另一个运行时则用于执行结构化数据查询的 SQL 代码，等等。
- en: '![01-03](Images/01-03.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![01-03](Images/01-03.png)'
- en: Figure 1.3 Serverless platforms eliminate the need for operations to manage
    the life cycle of the code infrastructure. The cloud-based platform is responsible
    for instantiating the code in runtime to service requests and for managing the
    infrastructure to ensure high availability, low latency, and other performance
    characteristics.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 Serverless 平台消除了操作管理代码基础设施的需要。基于云的平台负责在运行时中实例化代码以服务请求，并管理基础设施以确保高可用性、低延迟和其他性能特征。
- en: The most impactful consequence of using serverless as opposed to IaaS or PaaS
    models is cost. With both IaaS and PaaS models, public cloud vendors bill based
    on provisioned capacity. In contrast, with serverless models, it is possible to
    optimize machine learning platform costs based on whether the code is actually
    executed on the platform.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Serverless 与 IaaS 或 PaaS 模型相比，最重要的影响是成本。在 IaaS 和 PaaS 模型中，公共云供应商根据预配容量计费。相比之下，在
    Serverless 模型中，可以根据代码是否实际在平台上执行来优化机器学习平台的成本。
- en: Serverless and machine learning exist at the intersection of two information
    technologies. On one hand, machine learning opens the potential for new products,
    new features, or even re-invented industries based on capabilities that previously
    didn’t exist in the marketplace. On the other hand, serverless models strike the
    balance between productivity and customization, enabling developers to focus on
    building differentiating capabilities while reusing existing components from cloud-computing
    platforms. The serverless approach is more than a re-use of black box components.
    It is about rapidly assembling project-specific machine learning platforms that
    can be customized with code to enable the development of new products and services.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.2 Serverless machine learning life cycle
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning–based systems become more valuable when they can operate at
    scale, making frequent and repetitive decisions about data while supporting a
    large quantity of users. To get a sense of machine learning operating at this
    scale, think about your email provider classifying emails as spam or not spam
    for millions of emails every second and for millions of concurrent users around
    the globe. Alternatively, consider product recommendations (“If you bought this,
    you may also like that”) from a major e-commerce website.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'While machine learning–based systems grow more valuable at larger scales, just
    like with any software project, they should still work efficiently when they are
    small, and if successful, scale for growth. Yet most software projects don’t become
    overnight successes and don’t grow to reach billions of users. Although this can
    sound expensive from a cost perspective, the serverless part in serverless machine
    learning, in this book, is about ensuring your project can benefit from the original
    promise of public cloud computing: paying only for what you use, no more and no
    less.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 Who is this book for?
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The serverless machine learning approach described in this book is targeted
    at teams and individuals who are interested in building and implementing a machine
    learning system that may need to be scaled up to a potentially large number of
    users and large quantity of requests and data volumes, but that also needs to
    scale down when necessary to stay cost efficient. Even if you decide against using
    machine learning algorithms in a project, you can still use this book to learn
    about how serverless and cloud computing can help you manage, process, and analyze
    data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 1.6.1 What you can get out of this book
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are planning to put a machine learning system in production, at some
    point you have to decide whether to buy or to build the supporting 95%, in other
    words, the components of a machine learning platform. The examples, such as the
    ones from “Hidden Technical Debt in Machine Learning Systems,” include the serving
    infrastructure, data collection, verification, storage, monitoring, and more.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to build most or all of your machine learning platform, you can
    approach this book as a series of design use cases or inspirational examples from
    a sample machine learning project. The book demonstrates how the platform capabilities
    are implemented in cloud-computing platforms from various public cloud vendors,
    including AWS, Google Cloud, and Microsoft Azure. The book will also teach you
    about the features you will need for the machine learning platform, including
    object storage, data warehousing, interactive querying, and more. Whenever possible,
    the book will highlight the open source projects you can use in your platform
    build-out. While this book will not give you the step-by-step instructions for
    how to build your machine learning platform, you can use it as a case study and
    a guide for the components of the architecture that you should be building.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: If you are planning to acquire most of the machine learning platform capabilities,
    the book gives you the instructions and walks you through the process for how
    to build a sample machine learning project and then put it into production using
    Amazon Web Services. The book will also walk you through the implementation steps
    for a machine learning platform, including the source code needed for the project.
    Whenever possible, the approach in this book relies on portable open source technologies
    such as Docker (more about Docker in appendix B) and PyTorch (more about PyTorch
    in part 2) that will ease the process of porting the project to other cloud providers
    such as Google Cloud and Microsoft Azure.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 1.7 How does this book teach?
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field of machine learning exists at the intersection of computer science
    and statistics. So, it should come as no surprise that there are alternative routes
    for introducing a reader to the applications of machine learning. Many information
    technology professionals began their studies of machine learning with the well-known
    Coursera class by Andrew Ng ([https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)).
    Those with a statistical or academic background often cite *An Introduction to
    Statistical Learning* by James et al. (Springer, 2013), as their first textbook
    on machine learning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: This book takes a software engineering approach to machine learning. This means
    that for the purposes of this book, *machine learning* is the practice of building
    software-based systems with the defining ability to automatically derive answers
    from data in order to augment, and often replace, the need for humans in repetitive
    data-driven decision making. The focus on software engineering also means that
    the details of the machine learning algorithms, techniques, and statistical foundations
    will be covered with less rigor compared to how they are treated by the other
    sources mentioned. Instead, this book will focus on describing how to engineer
    production-ready systems that have machine learning—based features at their core.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 1.8 When is this book not for you?
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on everything you’ve read so far, you may develop a mistaken impression
    that serverless machine learning is suitable to every application of machine learning.
    So, when does it make sense to use serverless machine learning? I will be the
    first to admit that it does not apply in every circumstance. If you are working
    on an experimental, one-of-a-kind project, one that is limited in scope, size,
    or duration, or if your entire working data set is and always will be small enough
    to fit in memory of a virtual server, you should reconsider using serverless machine
    learning. You are probably better off with an approach with a dedicated single
    virtual server (single node) and a Jupyter notebook in an Anaconda installation,
    Google Colaboratory, or a similar Jupyter notebook hosting environment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The serverless approach does help optimize the costs related to running a machine
    learning project on a public cloud; however, this does not mean that re-implementing
    the project from this book is free of charge. To get the most from this book,
    you will want to use your AWS account to reproduce the examples described in the
    upcoming chapters. To do so you will need to spend about $45 USD to re-create
    the project by following the steps described in the book. However, to benefit
    from the book, you don’t need to stick to AWS. Whenever possible, this book will
    make references to alternative capabilities from other vendors such as Google
    Cloud and Microsoft Azure. The good news is that this book’s entire project can
    be completed within the free credit allowances available from the three major
    public cloud vendors. Alternatively, if you choose not to implement code examples
    or the project from this book in a public cloud, you can also rely on the descriptions
    to get a conceptual understanding of what it takes to launch a machine learning
    system at scale.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that you should not use the approach in this book if you are not
    prepared to maintain your system after it is put into production. The reality
    is that the serverless approach integrates with the capabilities of the public
    cloud platforms, such as AWS, and those capabilities, specifically their APIs
    and endpoints, change over time. While the public cloud vendors have an approach
    for providing you with some stability for those endpoints (e.g., managed phaseout
    plans), you should be prepared for vendors to introduce new features and changes
    that, in turn, mean you should be prepared to invest time and effort in maintaining
    your features over time. If you need to minimize and control the extent of maintainability,
    the serverless approach is not for you.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Privacy concerns could give rise to another host of reasons to avoid using a
    public cloud-based infrastructure for your project. Although most public cloud
    providers offer sophisticated encryption key-based data security mechanisms, and
    have features to help meet data privacy needs, in a public cloud you can achieve
    a high degree of certainty in data privacy but not necessarily a complete guarantee
    that your data and processes will be secure. With that said, this book does not
    teach you how to ensure 100% security for your data in the cloud, how to provide
    authentication and authorization, nor how handle other types of security concerns
    for the machine learning systems described in the book. Whenever possible, I provide
    references that can help you with security, but it is outside the scope of this
    book to teach you the security aspects of data and privacy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: From a portability standpoint, the approach described in this book tries to
    strike the balance between ideal code portability and the need to minimize the
    amount of effort needed to deploy a machine learning project. If portability is
    the overriding concern for you, you will be better off attempting a different
    approach. For example, you can rely on complex infrastructure management stacks,
    such as Kubernetes or Terraform, for infrastructure deployment and runtime management.
    You should also not use the serverless machine learning approach if you are determined
    to use a proprietary framework or technology that is incompatible with the stack
    used in this book. The book will attempt to use nonproprietary, portable, and
    open source tools whenever possible.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 1.9 Conclusions
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What problems can this book solve for the reader, and what value can the reader
    get out of it? The contemporary practice of machine learning sucks too much productivity
    out of a machine learning practitioner. This book teaches the reader to work efficiently
    though a sample machine learning project. Instead of navigating the maze of alternatives
    for a machine learning platform, risking mistakes or failure, this book teleports
    the reader right to the well-trodden path of experienced machine learning practitioners.
    Instead of having to rediscover the practices of machine learning yourself, you
    can use this book to take advantage of the capabilities that work well for the
    requirements of the vast majority of machine learning projects.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: This book is for someone who already has some experience with machine learning
    because it does not teach machine learning from scratch. The book focuses on practical,
    pragmatic understanding of machine learning and provides you with just enough
    knowledge to understand and complete the sample project. By the end of the book,
    you will have completed your machine learning project, deployed it to a machine
    learning platform on a public cloud, made your system available as a highly available
    web service accessible to anyone on the internet, and prepared for the next steps
    of ensuring the system’s long-term success.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Successful machine learning systems consist of about 5% machine learning code.
    The rest is the machine learning platform.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Public cloud-computing infrastructure enables cost-effective scalability for
    a machine learning platform.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless machine learning is a model for the software development of machine
    learning code that is written to run on a machine learning platform hosted in
    a cloud-computing infrastructure.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless machine learning can help you develop new products and services by
    rapidly assembling a machine learning system.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book will help you navigate the path from experimental machine learning
    code to a production machine learning system running in a public cloud.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^(1.)If you need or would like a refresher on machine learning basics, there
    is a section about the topic in appendix A.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: ^(2.)The phrase is thought to have originated at the MIT AI Lab in the 1990s
    (see [http://mng.bz/m1Pn](http://mng.bz/m1Pn)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
