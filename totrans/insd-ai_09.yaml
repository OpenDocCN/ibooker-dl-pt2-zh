- en: 9 Technological singularity is absurd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The unlikelihood of the singularity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lack of intelligence in machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoughts about the human brain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nothing in this world is to be feared … only understood.—Marie Curie
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: According to some people, the end of human civilization won’t be due to things
    like climate change, nuclear war, or our sun dying out. Instead, they believe
    that in the not-so-distant future, artificial intelligence could become so advanced
    that it gains its own will and takes control of the planet. This potential catastrophe
    is often referred to as “the singularity,” a hypothetical point in time when AI
    would advance so rapidly that humans couldn’t keep up with its progress. While
    this concept makes for exciting stories in science fiction, it is essential to
    ground such speculations in reality. In this chapter, we aim to demystify the
    notion of technological singularity, arguing that it is fundamentally flawed.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 The genesis of technological singularity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The concept of technological singularity isn’t a new idea; it has its roots
    in discussions that date back to at least 1958\. In that year, physicists John
    von Neumann and Stanislaw Ulam engaged in a conversation where they pondered the
    “ever-accelerating progress of technology” and how it might lead to a profound
    and potentially unpredictable turning point in human history. This turning point,
    referred to as the “singularity,” would mark a moment beyond which human affairs
    could change in ways we couldn’t foresee. A more detailed explanation of this
    notion came in 1965 from I. J. Good in his article titled “Speculations Concerning
    the First Ultra-Intelligent Machine” [1]. In this article, Good explored the idea
    of an “ultra-intelligent machine” that could emerge, potentially surpassing human
    intelligence. This machine, he suggested, could trigger profound changes in our
    society and the way we live:'
  prefs: []
  type: TYPE_NORMAL
- en: Let an ultraintelligent machine be defined as a machine that can far surpass
    all the intellectual activities of any man however clever. Since the design of
    machines is one of these intellectual activities, an ultraintelligent machine
    could design even better machines; there would then unquestionably be an “intelligence
    explosion,” and the intelligence of man would be left far behind. Thus the first
    ultraintelligent machine is the last invention that man need ever make, provided
    that the machine is docile enough to tell us how to keep it under control… . It
    is more probable than not that, within the twentieth century, an ultraintelligent
    machine will be built.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In 1967, Marvin Minsky, who led the AI laboratory at MIT, boldly stated that
    “within a generation, the problem of creating ‘artificial intelligence’ will be
    substantially solved” [cited in 2]. He even went a step further, suggesting that
    “within 10 years, computers won’t even keep us as pets.” Around the same time,
    Herbert Simon, another prominent computer scientist, made a similarly ambitious
    prediction, claiming that by 1985, machines would be capable of doing any work
    that humans could do [3].
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, grandiose claims like these tend to grab more attention than
    reasoned analyses. In recent years, several well-known figures in the scientific
    and tech world, including Bill Gates, Stephen Hawking, and Elon Musk, have raised
    alarm bells about the potential risks associated with AI. Sam Altman, the CEO
    of OpenAI, also subscribes to the idea of the technological singularity [4]:'
  prefs: []
  type: TYPE_NORMAL
- en: Our self-worth is so based on our intelligence that we believe it must be singular
    and not slightly higher than all the other animals on a continuum. Perhaps the
    AI will feel the same way and note that differences between us and bonobos are
    barely worth discussing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A report published in 2015 by the Global Challenges Foundation and Oxford University’s
    Future of Humanity Institute highlighted this concern as one of the “12 risks
    that threaten human civilization” [5]. Specifically, the report referred to the
    risk associated with advanced, intelligent machines:'
  prefs: []
  type: TYPE_NORMAL
- en: Through their advantages in speed and performance, and through their better
    integration with standard computer software, they could quickly become extremely
    intelligent in one or more domains (research, planning, social skills …). If they
    became skilled at computer research, the recursive self-improvement could generate
    what is sometimes called a “singularity,” but is perhaps better described as an
    “intelligence explosion” … . With the AI’s intelligence increasing very rapidly,
    such extreme intelligences could not easily be controlled (either by the groups
    creating them, or by some international regulatory regime), and would probably
    act in a way to boost their own intelligence and acquire maximal resources for
    almost all initial AI motivations. And if these motivations do not detail the
    survival and value of humanity in exhaustive detail, the intelligence will be
    driven to construct a world without humans or without meaningful features of human
    existence. This makes extremely intelligent AIs a unique risk, in that extinction
    is more likely than lesser problems. An AI would only turn on humans if it foresaw
    a likely chance of winning; otherwise, it would remain fully integrated into society.
    And if an AI had been able to successfully engineer a civilisation collapse, for
    instance, then it could certainly drive the remaining humans to extinction.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Rest assured that not everyone involved in the field of AI harbors apocalyptic
    visions. Throughout history, there have been voices of reason who offered more
    measured and rational perspectives on the capabilities and limitations of artificial
    intelligence. One such visionary was Ada Lovelace, recognized for her pioneering
    work in the field of AI as she is often credited with writing the world’s first
    computer program. In the 19th century, Lovelace collaborated with Charles Babbage
    on his proposed Analytical Engine, a mechanical general-purpose computer design.
    Her remarkable insight went beyond the conception of mere calculations; she grasped
    the potential for the machine to manipulate symbols and generate complex sequences,
    essentially laying the groundwork for programming. Lovelace’s groundbreaking notes
    on the Analytical Engine demonstrated the profound idea that machines could be
    used for more than just mathematical computation, envisioning a broader computational
    world. Her work serves as an early example of the theoretical underpinnings of
    modern computer programming and AI. In 1842, Lovelace articulated the following
    perspective regarding the Analytical Engine [6]:'
  prefs: []
  type: TYPE_NORMAL
- en: The Analytical Engine has no pretensions whatever to originate anything. It
    can do whatever we know how to order it to perform. It can follow analysis, but
    it has no power of anticipating any analytical relations or truths.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Similarly, Arthur Samuel, whom we introduced in chapter 1, is one of the pioneers
    of AI. Samuel’s work was instrumental in shaping the foundations of AI and had
    a profound effect on the field. As previously mentioned, he designed a computer
    program that could play checkers and improve its performance over time through
    learning from experience. This concept laid the groundwork for modern machine
    learning and reinforcement learning algorithms. In a seminal 1960 article published
    in *Science* titled “Some Moral and Technical Consequences of Automation—A Refutation”
    [7], Samuel eloquently asserted the critical importance of separating fact from
    fiction in the discourse surrounding automation:'
  prefs: []
  type: TYPE_NORMAL
- en: A machine is not a genie, it does not work by magic, it does not possess a will,
    and … nothing comes out which has not been put in, barring of course, an infrequent
    case of malfunctioning… . The machine will not and cannot do [anything] until
    it has been instructed as to how to proceed… . To believe otherwise is to believe
    in magic. Since the machine does not have a mind of its own, the “conclusions”
    are not “its.” The so-called “conclusions” are only the logical consequences of
    the input program and input data, as revealed by the mechanistic functioning of
    an inanimate assemblage of mechanical and electrical parts. The “intentions” which
    the machine seems to manifest are the intentions of the human programmer, as specified
    in advance, or they are subsidiary intentions derived from these, following rules
    specified by the programmer… . Although I have maintained that “nothing comes
    out that has not gone in,” this does not mean that the output does not possess
    value over and beyond the value to us of the input data. The utility of the computer
    resides in the speed and accuracy with which the computer provides the desired
    transformations of the input data from a form which man may not be able to use
    directly to one which is of direct utility.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Arthur Samuel’s insightful view on AI remains pertinent and accurate in today’s
    context. The concept of an ultra-intelligent, malevolent entity emerging from
    our technology to seize control and create chaos remains firmly in the fictional
    world. While it might be tempting to draw parallels with scenarios depicted in
    the works of science fiction authors like Isaac Asimov, TV series such as *The
    Twilight Zone*, or blockbuster films like *The Terminator* and *The Matrix*, it’s
    essential to recognize that the cognitive faculties, such as intuition, imagination,
    and creativity, which characterize true intelligence, remain elusive for artificial
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 The truth about the evolution of robotics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From Mary Shelley’s 1818 novel depicting Dr. Frankenstein’s creation to the
    animated doll Pinocchio brought to life in Disney’s 1940 film, the idea of human-made
    objects gaining sentience has always held a captivating allure.
  prefs: []
  type: TYPE_NORMAL
- en: The term “robot” achieved widespread recognition and popularity through its
    usage in Karel Cˇapek’s 1920 play, *R.U.R. “Rossum’s Universal Robots*.” This
    word, with its roots in the Czech language, specifically *robota*, which translates
    to “forced labor,” introduced a concept that would shape the future of automation
    and AI. The evolution of robotic terminology continued with the introduction of
    “androids.” While the term itself had ancient Greek roots, it found a new home
    in science fiction during the 1930s, thanks to the imaginative works of authors
    like Edmond Hamilton in his Captain Future series in the early 1940s. These androids,
    with their human-like qualities, began to captivate the imagination of audiences
    worldwide. Fast forward to today, we find ourselves in a world where robots do
    indeed perform labor. However, their capabilities remain constrained to tasks
    of a relatively straightforward and limited nature. Despite our technological
    progress, the concept of an android, as envisioned by early science fiction, remains
    tantalizingly distant.
  prefs: []
  type: TYPE_NORMAL
- en: General Motors made an early venture into robotics in 1961 when it introduced
    a mechanical arm for manipulating hot-cast metal components at a New Jersey plant.
    This venture was met with success, paving the way for further exploration in the
    1970s. The company expanded the use of machines into other meticulously controlled
    tasks, including welding and painting. In the contemporary landscape, robots play
    an indispensable role in auto manufacturing. Nonetheless, they remain specialized
    to particular tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In 1962, MIT’s Henry Ernst conceived the first computer-operated robotic hand,
    designed for the remote manipulation of radioactive materials. A symphony of motors
    orchestrated the movements of a mechanical arm, while light sensors in the fingertips
    discerned shadows. This ingenious system, guided by a computer program, maneuvered
    objects on a table, picking them up and placing them into a container.
  prefs: []
  type: TYPE_NORMAL
- en: Shakey, the subject of research at the SRI Artificial Intelligence Center from
    1966 to 1972, took a monumental leap forward as the inaugural mobile robot equipped
    with the ability to perceive its surroundings (figure 9.1) [8]. A marvel of its
    time, Shakey served as an experimental platform to integrate machine learning,
    computer vision, navigation, and a myriad of AI techniques. Its repertoire included
    tasks demanding planning, route-finding, and the rearrangement of simple objects,
    earning it the moniker of the “first electronic person” by *LIFE* magazine in
    1970\. While Shakey’s accomplishments were undeniably groundbreaking, it’s essential
    to maintain a realistic perspective. The tasks it excelled at were a far cry from
    the cinematic portrayals of robots. A human operator would issue commands from
    a console, instructing Shakey to execute operations like pushing a block from
    a platform. These instructions would then be transmitted via radio, enabling Shakey
    to survey its environment, locate the target, and carry out the task. It was,
    undoubtedly, a notable accomplishment for artificial intelligence, yet one that
    highlights the chasm between the capabilities of contemporary robotics and the
    visionary worlds depicted in fiction. While progress has been substantial, the
    gap between science fiction and real-world robotics remains substantial, emphasizing
    the ongoing challenges in creating highly capable and adaptable robotic systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/9-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 Shakey (image courtesy of the Computer History Museum)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the decades following Shakey’s pioneering research, the field of robotics
    witnessed a gradual yet steady evolution. By the early 1980s, engineers at Waseda
    University in Tokyo achieved a significant breakthrough. They introduced a robot
    powered by a microprocessor chip, a revolutionary departure from the room-sized
    computers that had previously driven robotic systems. This innovative leap paved
    the way for more compact and agile robotic platforms. What set this robot apart
    was its ability to stand on two “legs” and take measured steps at a deliberate
    pace, exemplifying early progress in bipedal locomotion. Shortly thereafter, in
    1981, Shigeo Hirose at the Tokyo Institute of Technology unveiled another milestone
    in robotics with the creation of a quadruped robot. This remarkable machine demonstrated
    the capacity to climb stairs and opened new possibilities for robots to explore
    environments with a combination of stability and mobility. The 1990s brought another
    advancement in the form of an eight-legged robot, developed by researchers at
    Carnegie Mellon University [9]. The use of multiple legs allowed the robot to
    maintain stability and traverse landscapes that would be impassable for wheeled
    or bipedal counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most captivating milestones in the field of robotics emerged with
    NASA’s groundbreaking Pathfinder mission. On the historic date of July 4, 1997,
    the Mars Pathfinder, a remarkable robotic spacecraft, achieved a triumphant exploit
    by successfully deploying a base station and an autonomous rover named Sojourner
    onto the Red Planet [10]. This monumental achievement marked a significant leap
    forward in space exploration. Sojourner, the autonomous rover, played a vital
    role in this groundbreaking mission. Over the course of 83 remarkable days, it
    embarked on an incredible journey, traversing the Martian terrain with its six
    wheels. Equipped with a suite of scientific instruments, Sojourner became an invaluable
    explorer, capturing high-resolution images of the Martian landscape and collecting
    vital atmospheric and geological data. This achievement was an impressive progress
    in the field of robotics and space exploration.
  prefs: []
  type: TYPE_NORMAL
- en: The advent of domestic robotics took a significant step forward during the holiday
    shopping season of 1998, leaving parents and children captivated by the promise
    of technology. Leading this technological wonderland was the Furby, brilliantly
    marketed as an artificially intelligent companion. Resembling a fuzzy, animal-like
    creature, the Furby boasted an array of interactive features, including moving
    eyes and a mouth, as well as voice-recognition technology. It could communicate
    in its own quirky language known as “Furbish” and gradually learn and speak words
    in English and several other common languages. This gradual linguistic development
    was designed to mimic the process of a pet or companion learning its owner’s language.
    The Furby’s irresistible charm and mass appeal led to the staggering sale of 14
    million units in its debut year on the market, making it a must-have item for
    countless households.
  prefs: []
  type: TYPE_NORMAL
- en: While the Furby was captivating headlines and toy stores, Sony Corporation was
    pioneering another remarkable product in the form of AIBO, a robotic pet dog.
    AIBO featured rudimentary computer vision that enabled it to interact with its
    environment and respond to over 100 voice commands. Its movements and interactions
    made it an instant sensation. In a remarkable display of consumer enthusiasm,
    the initial run of 5,000 AIBO units sold out to enthusiastic internet customers
    within 20 minutes. Sony continued to release new models of AIBO annually until
    2006\. In 2017, Sony announced the return of AIBO with a new model that promised
    to form an “emotional bond” with its users [11]. This development marked a significant
    shift in the relationship between humans and robots, as it indicated the potential
    for robots to provide companionship and emotional support.
  prefs: []
  type: TYPE_NORMAL
- en: On Valentine’s Day in 2002, a groundbreaking moment occurred in the world of
    robotics and popular culture when Honda unveiled ASIMO, an acronym that stands
    for “Advanced Step in Innovative Mobility” (figure 9.2). ASIMO represented a significant
    leap forward in the development of humanoid robots, and its introduction was the
    culmination of over 15 years of tireless work by Honda engineers. The journey
    to create ASIMO began way back in 1986 when Honda’s team embarked on the ambitious
    task of constructing a robot that could move and walk just like a real person.
    After years of dedication and innovation, ASIMO emerged as a remarkable robot
    capable of walking and performing a variety of specific tasks with precision.
    One of ASIMO’s most iconic moments was when it rang the bell to open the New York
    Stock Exchange. This event took place on the 25th anniversary of Honda’s stock
    being traded on the market.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/9-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 ASIMO (2000) (image courtesy of Miraikan Museum, Tokyo)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The forefront of robotics technology can often be found within the workshops
    of Boston Dynamics Robotics. Founded in 1992 as a spinoff from MIT, this company,
    headquartered in Waltham, Massachusetts, has undergone ownership changes, with
    Google and SoftBank being previous owners before its acquisition by Hyundai Motor
    Group in 2020\. Boston Dynamics Robotics has gained recognition for its impressive
    robotic creations, including the quadruped military robot known as BigDog, the
    humanoid robot named Atlas, and the versatile material-handling robot Handle.
    A Bloomberg article published in November 2022 featured the CEO of Boston Dynamics,
    who expressed great enthusiasm for Handle. He stated, “Handle, which gracefully
    maneuvers on two large wheels, is designed to automate tasks like moving boxes
    on and off pallets and perhaps even unloading boxes from trucks, a notoriously
    challenging task for a robot” [12].
  prefs: []
  type: TYPE_NORMAL
- en: As we contemplate this statement, it’s important to maintain perspective. In
    the 50 years since the days of Shakey, we have progressed from merely locating
    a block and pushing it off a platform to precisely locating a box and lifting
    it onto or off of a platform. Experts in the field of robotics tend to be candid
    about both the capabilities and limitations of current technology. Instead of
    heeding the words of those who sensationalize the idea of machines taking over,
    we can benefit from their realistic assessments. Even with the most advanced sensors,
    actuators, cameras, and materials, coupled with remarkable talent and resources,
    the world’s leading robotics teams view the prospect of using robots for loading
    and unloading trucks as a crucial milestone in the progress of robotics. While
    there are captivating videos online showcasing robots dancing, jumping, and performing
    acrobatic feats, it is essential to remember that these movements are typically
    preprogrammed. These machines do not autonomously respond to their environment.
    The sight of a human-shaped robot executing a backflip may be impressive, but
    it pales in comparison to the automatic movements of even a clumsy child. Our
    innate ability to regain balance when slipping on ice or when we miss a step on
    the stairs remains far superior to the capabilities of even the most advanced
    robots.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Merging human with machine?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Electronic implants that can decode brain activity and communicate with computers
    have been a subject of research for several decades. As reported by the *Washington
    Post* in 2016, a noteworthy collaboration between the University of Pittsburgh
    and the University of Pittsburgh Medical Center involved the implantation of electrodes
    smaller than a grain of sand into a patient’s sensory cortex. These electrodes
    received signals from a robot arm, allowing the individual to experience tactile
    sensations in their paralyzed right hand, effectively bypassing their damaged
    spinal cord [13]. This development demonstrated a promising application of brain-computer
    interfaces in the field of medical rehabilitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, despite these remarkable advancements, making grandiose claims about
    the imminent possibility of uploading human minds into computers or creating synthetic
    humans and human-machine hybrids is unlikely. Books with titles like *How to Create
    a Mind: The Secret of Human Thought Revealed* [14] may appear evocative of science
    fiction, leaving us to question their scientific validity.'
  prefs: []
  type: TYPE_NORMAL
- en: The challenge lies in the vast disparity between the current state of artificial
    intelligence and our understanding of the complexities of the human brain. Prominent
    neuroscientists themselves admit to lacking a fundamental understanding of how
    the brain truly operates. Therefore, we should be careful not to use or associate
    terms like “brain” or “neuron” with AI. A neuron within a deep learning neural
    network bears about as much resemblance to an actual neuron as a teddy bear does
    to a live bear!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the book *The Singularity Is Near*, Ray Kurzweil boldly predicts the replacement
    of human DNA with designer genes, even providing a somewhat precise timeline,
    suggesting that we’ll have reverse-engineered human brains by the late 2020s [15].
    Such forecasts are partially based on observations such as computers defeating
    champions in games like chess and Moore’s Law, which suggests that computing power
    tends to double approximately every two years. This assertion was somewhat accurate
    a decade ago; however, the chip manufacturers are approaching the physical limitations
    of transistor density on microchips and the challenges of heat dissipation in
    highly compact devices. Even if we assume that computers will continue to become
    faster, smaller, and more affordable indefinitely, it’s crucial to differentiate
    between computing power and intelligence. Machines still operate by executing
    human-coded instructions, fundamentally distinct from the multifaceted processes
    that govern the human mind, involving elements of chemistry, biology, neurology,
    psychology, and more. Neuroscientist Christof Koch pertinently phrases it this
    way in his article “The End of the Beginning for the Brain” [16]:'
  prefs: []
  type: TYPE_NORMAL
- en: One thing is certain. Biology knows nothing of simplicity. Brains are not assembled
    out of billions of identical LEGO blocks but out of hundreds of distinct nerve
    cell types. Each cell type has its own idiosyncratic morphology, signaling, and
    active genes. And they are interconnected with elaborate wiring rules that we
    only discern darkly.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The concept of merging humans and machines often overestimates the sophistication
    of our technological advancements while simultaneously underestimating the astounding
    complexity of living organisms. The human body itself is nothing short of a miraculous
    wonder, comprising 10s of trillions of adaptive and intricately interconnected
    cells. Within this complex web of life, every organ plays a role in sending and
    receiving signals, engaging in complex coordination with every other part. Consider
    one of our many organs, the eyes. Think about the large volume of data processed
    by our eyes with every passing moment, the complex cascade of signals they stimulate,
    the feedback loops that influence their function, and the numerous systems they
    coordinate to provide us with vision.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, our eyes represent just a fraction of the immense complexity within our
    bodies. Even as remarkable as they are, they pale in comparison to the vast complexity
    of our brains, often referred to as the most complex pieces of organized matter
    in the known universe, and for good reason. The human brain, a marvel of biological
    evolution, possesses an unparalleled capacity for creativity, intuition, emotional
    intelligence, and a nuanced understanding of complex, ever-changing environments.
    It operates on a fundamentally different paradigm compared to AI, which, while
    powerful in its own right, lacks the complex interplay of biological neurons and
    the underlying biochemical processes that contribute to human cognition. Those
    who have witnessed AI’s real-world applications are aware that current AI is far
    from achieving anything that bears a resemblance to the depth and complexity of
    human cognition, let alone being able to replicate the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: 'In his highly recommended book *The Biology of Belief* [17], Bruce Lipton eloquently
    describes the concept of living intelligence that operates within our bodies:'
  prefs: []
  type: TYPE_NORMAL
- en: When a measles virus infects a child, an immature immune cell is called in to
    create a protective protein antibody against that virus. In the process, the cell
    must create a new gene to serve as a blueprint in manufacturing the measles antibody
    protein. Activated cells employ an amazing mechanism, called affinity maturation,
    that enables the cell to perfectly adjust the final shape of its antibody protein
    so that it will become a perfect complement to the invading measles virus. The
    new antibody gene is also passed on to all the cell’s progeny when it divides.
    The cell learned about the measles virus; it also creates a memory that will be
    inherited by its daughter cells. This amazing genetic engineering represents an
    inherent intelligence mechanism by which cells evolve.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our cells are complex microcosms of life, intricately designed to receive, process,
    react to, and preserve information in a synchronized ballet. These tiny units
    of life tirelessly perform their roles, coordinating with each other to maintain
    the equilibrium of our bodies. When confronted with external changes, cells exhibit
    remarkable adaptability, adjusting their functions to respond effectively to their
    ever-changing environment. They can sense, make decisions, and execute actions,
    all within fractions of a second. While we have made progress in understanding
    cellular mechanisms, we are still far from uncovering the full extent of their
    complexities.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Science fiction vs. reality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of technological singularity has been a topic of discussion for
    over six decades, often accompanied by enthusiastic hype. While the potential
    threats posed by AI have been frequently highlighted, it’s important to acknowledge
    that AI, in its current state, falls far short of possessing the cognitive abilities
    of even a young child, let alone attributes like creativity or sentiment. It’s
    essential to recognize that machines, whether they are robots or computer programs,
    remain inert and devoid of purpose until humans provide them with instructions
    and guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Computers excel at manipulating numbers and processing words, but they lack
    genuine comprehension of the meaning behind the data they handle. Algorithms,
    operating on binary code, cannot truly “understand” in the way humans do. Complex
    problem-solving, adapting to unforeseen situations, and the formulation of hypotheses
    are still well beyond the capabilities of AI. In practice, most AI applications
    are confined to highly specific tasks, often reliant on extensive sets of meticulously
    curated training data. While it may be tempting to attribute a certain mystique
    to technology that escapes our complete comprehension, this book seeks to dispel
    such illusions. Robots and automatons, despite their seemingly intricate actions,
    are simply executing their preprogrammed instructions. They do not possess self-awareness
    or exhibit characteristics like fear or a desire for conquest, and they certainly
    do not pose a threat to human civilization.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, governments around the world continue to invest heavily
    in AI and autonomous systems. However, even as of 2022, military leaders raise
    questions about their reliability and practicality. A 2020 RAND report [18] on
    military applications of AI offers several illustrative examples of the kind of
    incidents that can give decision makers pause.
  prefs: []
  type: TYPE_NORMAL
- en: One such incident dates back to 1988 when the US Navy accidentally shot down
    an Iranian civilian aircraft, resulting in the tragic loss of 290 lives. While
    the exact cause remains disputed, it’s known that the Aegis weapons system, responsible
    for tracking planes and operating munitions, employed an automated system to assign
    tracking numbers to radar-detected objects. This system periodically recycled
    call numbers, and during the critical time frame, the passenger jet was assigned
    a new number, which happened to be the same as one assigned to a fighter jet just
    110 miles away.
  prefs: []
  type: TYPE_NORMAL
- en: Another disaster occurred during the Gulf War in 1991 when the *USS Missouri*
    mistakenly believed it was under attack from an Iraqi Silkworm missile and deployed
    countermeasures. Simultaneously, the nearby *USS Jarrett*’s Phalanx CIWS system,
    operating in autonomous target-acquisition mode, detected the countermeasures
    and fired at them. Unfortunately, four rounds from the *USS Jarrett* struck the
    *USS Missouri*.
  prefs: []
  type: TYPE_NORMAL
- en: In yet another incident during a multinational training exercise in the Pacific
    in 1996, a Navy A-6E Intruder was towing a target plane intended to be shot down
    by Japanese participants. However, instead of locking onto the target plane, the
    Phalanx system mistakenly targeted the Intruder and opened fire.
  prefs: []
  type: TYPE_NORMAL
- en: The RAND report further describes a fourth accident in 2003 where a US Patriot
    battery mistakenly shot down a Tornado flown by the Royal Air Force due to a misidentification
    by the missile system, resulting in the loss of two crew members. A fifth incident
    occurred later the same year when a US Navy aircraft was mistakenly identified
    as an Iraqi missile, leading to the tragic death of the pilot. These incidents
    serve as reminders of the potential pitfalls of autonomous systems and highlight
    the importance of careful consideration, testing, and oversight in the integration
    of AI and automation into critical functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a candid question-and-answer session held at the prestigious Brookings Institution
    think tank, US General Selva articulated his reservations about contemporary AI
    methodologies, particularly deep learning, due to their inherent inability to
    provide transparent explanations for their decisions [19]. He emphasized: “Our
    belief is AI alone doesn’t actually solve the problems that we’re being asked
    to solve. It can’t be a black box that says just go do X.”'
  prefs: []
  type: TYPE_NORMAL
- en: General Selva further highlighted that the military cannot afford to rely on
    an AI system that lacks both reliability and comprehensibility. In his view, an
    acceptable system must possess the capability to not only undergo rigorous physical
    testing but also intellectual scrutiny.
  prefs: []
  type: TYPE_NORMAL
- en: 'Joseph Weizenbaum, in his seminal 1976 work *Computer Power and Human Reason*
    [20], introduced a profound perspective on the concept of technological singularity
    that remains remarkably valid. He stated:'
  prefs: []
  type: TYPE_NORMAL
- en: Science may also be seen as an addictive drug. Our relentless fascination with
    science has not only made us rely and depend on it, but, similar to many other
    substances taken in escalating doses, science has gradually transformed into a
    slow-acting poison.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Weizenbaum’s observation is quite relevant, as more and more people are getting
    caught up in their devices and spending a lot of time on social media, which can
    have negative effects on their well-being due to harmful content and unhealthy
    behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of technological singularity, while intriguing, begs the question
    of whether we are becoming too enamored with science fiction scenarios at the
    expense of addressing real-world challenges. Humanity faces pressing threats,
    such as climate change, epidemics, and the specter of nuclear and biological warfare.
    In this context, the fascination with the potential for superintelligent AI can
    divert attention and resources from immediate, tangible problems. It is crucial
    to recognize that AI, like any technology, can be used for both benevolent and
    malevolent purposes. While AI can contribute to the development of life-saving
    vaccines, it can also be weaponized for destructive ends. A system designed to
    inform can equally be used to deceive.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, General Selva’s reservations about the limitations of current
    AI techniques, coupled with Weizenbaum’s cautionary insights about the double-edged
    nature of scientific progress, remind us of the importance of thoughtful and ethical
    innovation. Therefore, instead of dwelling solely on dystopian visions of AI,
    we must prioritize the responsible development and safe deployment of these technologies
    to serve humanity’s best interests.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI singularity has been predicted but failed to occur many times over the last
    60 years, and we are no closer to experiencing it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there has been some progress in robotics, machines still have no intelligence
    and no will, so they have no ability to evolve and become a threat.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The faster processing speeds of modern computers do not make them think.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are still unable to explain how the human brain works, so we cannot replicate
    it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
