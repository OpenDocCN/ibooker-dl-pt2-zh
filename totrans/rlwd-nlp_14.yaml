- en: 10 Best practices in developing NLP applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Making neural network inference more efficient by sorting, padding, and masking
    tokens
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying character-based and BPE tokenizationfor splitting text into tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding overfitting via regularization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with imbalanced datasets by using upsampling, downsampling, and loss
    weighting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing hyperparameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve covered a lot of ground so far, including deep neural network models such
    as RNNs, CNNs, and the Transformer, and modern NLP frameworks such as AllenNLP
    and Hugging Face Transformers. However, we’ve paid little attention to the details
    of training and inference. For example, how do you train and make predictions
    efficiently? How do you avoid having your model overfit? How do you optimize hyperparameters?
    These factors could make a huge impact on the final performance and generalizability
    of your model. This chapter covers these important topics that you need to consider
    to build robust and accurate NLP applications that perform well in the real world.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Batching instances
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 2, we briefly mentioned *batching*, a machine learning technique
    where instances are grouped together to form batches and sent to the processor
    (CPU or, more often, GPU). Batching is almost always necessary when training large
    neural networks—it is critical for efficient and stable training. In this section,
    we’ll dive into some more techniques and considerations related to batching.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Padding
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training large neural networks requires a number of linear algebra operations
    such as matrix addition and multiplication, which involve executing basic mathematical
    operations on many, many numbers at once. This is why it requires specialized
    hardware such as GPUs, processors designed to execute such operations in a highly
    parallelized manner. Data is sent to the GPU as tensors, which are just high-dimensional
    arrays of numbers, along with some instructions as to what types of mathematical
    operations it needs to execute. The result is sent back as another tensor.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 2, we likened GPUs to factories overseas that are highly specialized
    and optimized for manufacturing the same type of products in a large quantity.
    Because there is considerable overhead in communicating and shipping products,
    it is more efficient if you make a small number of orders for manufacturing a
    large quantity of products by shipping all the required materials in batches,
    rather than shipping materials on demand.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Materials and products are usually shipped back and forth in standardized containers.
    If you have ever loaded a moving pod or trailer yourself (or observed someone
    else do it), you may know that there are many considerations that are important
    for safe and reliable shipping. You need to put furniture and boxes in tightly
    so that they don’t shift around in transition. You need to wrap them with blankets
    and fix them with ropes to prevent them from being damaged. You need to put heavy
    stuff at the bottom so that lighter stuff won’t get crushed, and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 材料和产品通常在标准化的容器中来回运输。如果你曾经自己装过搬家货舱或观察别人装过，你可能知道有很多需要考虑的因素来确保安全可靠的运输。你需要紧紧地把家具和箱子放在一起，以免在过渡过程中移位。你需要用毯子裹着它们，并用绳子固定它们，以防止损坏。你需要把重的东西放在底部，以免把轻的东西压坏，等等。
- en: Batches in machine learning are similar to containers for shipping stuff in
    the real world. Just like shipping containers are all the same size and rectangular,
    batches in machine learning are just rectangular tensors packed with numbers of
    the same type. If you want to “ship” multiple instances of different shapes in
    a single batch to the GPU, you need to pack them so that the packed numbers form
    a rectangular tensor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的批次类似于现实世界中用于运输物品的容器。就像运输集装箱都是相同的尺寸和矩形形状一样，机器学习中的批次只是装有相同类型数字的矩形张量。如果你想要将不同形状的多个实例在单个批次中“运送”到GPU，你需要将它们打包，使打包的数字形成一个矩形张量。
- en: In NLP, we often deal with sequences of text in different lengths. Because batches
    have to be rectangular, we need to do *padding*, (i.e., append special tokens,
    <PAD>, to each sequence so that each row of the tensor has the same length. You
    need as many padding tokens as necessary to make the sequences the same length,
    which means that you need to pad short sequences until they are all as long as
    the longest sequence in the same batch. This is illustrated in figure 10.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，我们经常处理长度不同的文本序列。因为批次必须是矩形的，所以我们需要进行*填充*（即在每个序列末尾加上特殊标记< PAD >），以便张量的每一行具有相同的长度。你需要足够多的填充标记，以使序列的长度相同，这意味着你需要填充短的序列，直到它们与同一批次中最长的序列一样长。示例见图10.1。
- en: '![CH10_F01_Hagiwara](../Images/CH10_F01_Hagiwara.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH10'
- en: Figure 10.1 Padding and batching. Black squares are tokens, gray ones are EOS
    tokens, and white ones are padding.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 填充和分批。黑色方块表示标记，灰色方块表示EOS（结束）标记，白色方块表示填充。
- en: In reality, each token in natural language text is often represented as a vector
    of length *D*, generated by the word embeddings method. This means that each batched
    tensor is a three-dimensional tensor that has a “depth” of *D*. In many NLP models,
    sequences are represented as batches of size *N* × *L* × *D* (see figure 10.2),
    where *N*, *L*, *D* are the number of instances per batch, the maximum length
    of the sequences, and the dimension of word embeddings, respectively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，自然语言文本中的每个标记通常表示为长度为*D*的向量，由词嵌入方法生成。这意味着每个批次的张量是一个三维张量，其“深度”为*D*。在许多自然语言处理模型中，序列被表示为大小为*N*×*L*×*D*的批次（见图10.2），其中*N*、*L*、*D*分别表示批次中的实例数目、序列的最大长度和词嵌入的维度。
- en: '![CH10_F02_Hagiwara](../Images/CH10_F02_Hagiwara.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F02_Hagiwara](../Images/CH10_F02_Hagiwara.png)'
- en: Figure 10.2 Padding and batching of embedded sequences create rectangular, three-dimensional
    tensors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 嵌入序列的填充和分批创建了三维的矩形张量。
- en: This is starting to look more like real containers!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来越来越像真正的容器了！
- en: 10.1.2 Sorting
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 排序
- en: Because each batch has to be rectangular, if one batch happens to include both
    short sequences and long sequences, you need to add a lot of padding to short
    sequences so that they are as long as the longest sequence in the same batch.
    This often leads to some wasted space in the batch—see “batch 1” in figure 10.3
    for an illustration. The shortest sequence (six tokens) needs to be padded with
    eight more tokens to be equally long as the longest sequence (14 tokens). Wasted
    space in a tensor means wasted memory and computation, so it is best avoided,
    but how?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个批次必须是矩形的，如果一个批次同时包含短序列和长序列，你需要为短序列添加大量填充，使它们与同一批次中最长的序列一样长。这通常会导致批次中存在一些浪费空间——见图10.3中的“batch
    1”示例。最短的序列（六个标记）需要填充八个标记才能与最长的序列（14个标记）长度相等。张量中的浪费空间意味着存储和计算的浪费，所以最好避免这种情况发生，但是怎么做呢？
- en: '![CH10_F03_Hagiwara](../Images/CH10_F03_Hagiwara.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F03_Hagiwara](../Images/CH10_F03_Hagiwara.png)'
- en: Figure 10.3 Sorting instances before batching (right) reduces the total number
    of tensors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: You can reduce the amount of padding by putting instances of similar size in
    the same batch. If shorter instances are batched only with other equally shorter
    ones, they don’t need to get padded with many padding tokens. Similarly, if longer
    instances are batched only with other longer ones, they don’t need a lot of padding
    either, because they are already long. One idea is to sort instances by their
    length and batch accordingly. Figure 10.3 compares two situations—one in which
    the instances are batched in their original order, and the other where instances
    are first sorted before batching. The numbers below each batch indicate how many
    tokens are required to represent the batch, including the padding tokens. Notice
    that the number of total tokens is reduced from 144 to 120 by sorting. Because
    the number of tokens in the original sentences doesn’t change, this is purely
    because sorting reduced the number of padding tokens. Smaller batches require
    less memory to store and less computation to process, so sorting instances before
    batching improves the efficiency of training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'All these techniques sound somewhat complicated, but the good news is, you
    rarely need to write code for sorting, padding, and batching instances yourself
    as long as you use high-level frameworks such as AllenNLP. Recall that we used
    a combination of DataLoader and BucketBatchSampler for building our sentiment
    analysis model back in chapter 2 as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The sorting_keys given to BucketBatchSampler specifies which field to use for
    sorting. As you can guess from its name, by specifying “tokens” you are telling
    the data loader to sort the instances by the number of tokens (which is what you
    want in most cases). The pipeline will take care of padding and batching automatically,
    and the data loader will give you a series of batches you can feed into your model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Masking
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One final detail that you need to pay attention to is *masking*. Masking is
    an operation where you ignore some part of the network that corresponds to padding.
    This becomes relevant especially when you are dealing with a sequential-labeling
    or a language-generation model. To recap, sequential labeling is a task where
    the system assigns a label per token in the input sequence. We built a POS tagger
    with a sequential labeling model (RNN) in chapter 5.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 10.4, sequential-labeling models are trained by minimizing
    the per-token loss aggregated across all tokens in a given sentence. We do this
    because we’d like to minimize the number of “errors” the network makes per token.
    This is fine as long as we are dealing with “real” tokens (“time,” “flies,” and
    “like” in the figure), although it becomes an issue when the input batch includes
    padded tokens. Because they exist just to pad the batch, they should be ignored
    when computing the total loss.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F04_Hagiwara](../Images/CH10_F04_Hagiwara.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 Loss for a sequence is the sum of per-token cross entropy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: We usually do this by creating an extra vector for masking the loss. The vector
    for masking has the same length as the input, whose elements are 1s for “real”
    tokens and 0s for padding. When computing the total loss, you can simply take
    an element-wise product between the per-token loss and the mask and then sum up
    the result.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, as long as you are building standard sequential-labeling models
    with AllenNLP, you rarely need to implement masking yourself. Remember, in chapter
    5, we wrote the forward pass of the POS tagger model as shown in listing 10.1\.
    Here, we get the mask vector from the get_text_field_mask() helper function and
    compute the final loss with sequence_cross_entropy_with_logits().
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 Forward pass of the POS tagger
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you take a peek at what’s inside mask (e.g., by inserting a print statement
    in this forward method), you’ll see the following tensor made of binary (True
    or False) values:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each row of this tensor corresponds to one sequence of tokens, and locations
    with False are where padding occurred. The loss function (sequence_cross_entropy
    _with_logits) receives the prediction, the ground truth (the correct labels),
    and the mask and computes the final loss while ignoring all the elements marked
    as False.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Tokenization for neural models
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 3, we covered the basic linguistic units (words, characters, and
    n-grams) and how to compute their embeddings. In this section, we will go deeper
    and focus on how to analyze texts and obtain these units—a process called *tokenization*.
    Neural network models pose a set of unique challenges on how to deal with tokens,
    and we’ll cover some of the modern models to deal with these challenges.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Unknown words
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A vocabulary is a set of tokens that an NLP model deals with. Many neural NLP
    models operate within a fixed, finite set of tokens. For example, when we built
    a sentiment analyzer in chapter 2, the AllenNLP pipeline first tokenized the training
    dataset and constructed a Vocabulary object that consists of all unique tokens
    that appeared more than, say, three times. The model then uses an embedding layer
    to convert tokens into word embeddings, which are some abstract representation
    of the input tokens.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good, right? But the number of all words in the world is not finite.
    We constantly make up new words that didn’t exist before (I don’t think people
    talked about “NLP” a hundred years ago). What if the model receives a word that
    it has never seen during training? Because the word is not part of the vocabulary,
    the model cannot even convert it to an index, let alone look up its embeddings.
    Such words are called *out-of-vocabulary* (OOV) *words*, and they are one of the
    biggest problems when building NLP applications.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: By far the most common (but not the best) way to deal with this problem is to
    represent all the OOV tokens as a special token, which is conventionally named
    UNK (for “unknown”). The idea is that every time the model sees a token that is
    not part of the vocabulary, it pretends it saw a special token UNK instead and
    proceeds as usual. This means that the vocabulary and the embedding table both
    have a designated “slot” for UNK so that the model can deal with words that it
    has never seen. The embeddings (and any other parameters) for UNK are trained
    in the same manner as other regular tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Do you see any problems with this approach? Treating all OOV tokens with a single
    UNK token means that they are collapsed into a single embedding vector. It doesn’t
    matter if the word is “NLP” or “doggy”—as long as it’s something unseen, it always
    gets treated as a UNK token and assigned the same vector, which becomes a generic,
    catch-all representation of various words. Because of this, the model cannot tell
    the differences among OOV words, no matter what the identity of the words is.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: This may be fine if you are building, for example, a sentiment analyzer. OOV
    words are, by definition, very rare and might not affect the prediction of most
    of the input sentences. However, this becomes a huge problem if you are building
    a machine translation system or a conversational engine. It wouldn’t be a usable
    MT system or a chatbot if it produces “I don’t know” every time it sees new words!
    In general, the OOV problem is more serious for language-generation systems (including
    machine translation and conversational AI) compared to NLP systems for prediction
    (sentiment analysis, POS tagging, and so on).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: How can we do better? OOV tokens are such a big problem in NLP that there has
    been a lot of research work on how to deal with them. In the following subsections,
    we’ll cover character-based and subword-based models, two techniques commonly
    used for building robust neural NLP models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Character models
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest yet effective solution for dealing with the OOV problem is to treat
    characters as tokens. Specifically, we break the input text into individual characters,
    even including punctuation and whitespace, and treat them as if they are regular
    tokens. The rest of the application is unchanged—“word” embeddings are assigned
    to characters, which are further processed by the model. If the model produces
    text, it does so character-by-character.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we used a character-level model in chapter 5 when we built a language
    generator. Instead of generating text word-by-word, the RNN produces text one
    character at a time, as illustrated in figure 10.5\. Thanks to this strategy,
    the model was able to produce words that look like English but actually aren’t.
    Notice a number of peculiar words (*despoit*, *studented*, *redusention*, *distaples*)
    that resemble English words in the output shown in listing 10.2\. If the model
    operated on words, it produces only known words (or UNKs when unsure), and this
    wouldn’t have been possible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们构建语言生成器时，我们在第5章使用了字符级模型。RNN不是一次生成一个单词，而是一次生成一个字符，如图10.5所示。由于这种策略，模型能够生成看起来像英语但实际上不是的单词。请注意10.2列表中显示的输出中类似于英语的许多奇怪的单词（*despoit*，*studented*，*redusention*，*distaples*）.如果模型操作单词，它只会生成已知的单词（或者在不确定时生成UNKs），这是不可能的。
- en: '![CH10_F05_Hagiwara](../Images/CH10_F05_Hagiwara.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F05_Hagiwara](../Images/CH10_F05_Hagiwara.png)'
- en: Figure 10.5 A language-generation model that generates text character-by-character
    (including whitespace)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：生成文本字符级（包括空格）的语言生成模型
- en: Listing 10.2 Generated sentences by a character-based language model
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列10.2：字符级语言模型生成的句子
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Character-based models are versatile and put few assumptions on the structure
    of the language. For languages with small sets of alphabets (like English), it
    effectively eradicates unknown words, because almost any words, no matter how
    rare they are, can be broken down into characters. Tokenizing into characters
    is also an effective strategy for languages with large alphabets (like Chinese),
    although you need to watch out for “unknown character” problems.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符的模型是多功能的，并对语言的结构做出了少量的假设。对于拥有小字母表的语言（比如英语），它有效地消除了未知单词，因为几乎任何单词，无论其多么罕见，都可以被分解为字符。对于拥有大字母表的语言（如中文），将其标记为字符也是一种有效的策略，尽管你需要注意“未知字符”的问题。
- en: However, this strategy is not without drawbacks. The biggest issue is its inefficiency.
    To encode a sentence, the network (be it an RNN or the Transformer) needs to go
    over all the characters in it. For example, a character-based model needs to process
    “t,” “h,” “e,” and “_” (whitespace) to process a single word “the,” whereas a
    word-based model can finish this in a single step. This inefficiency takes its
    biggest toll on the Transformers, where the attention computation increases quadratically
    when the input sequence gets longer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种策略并非没有缺点。最大的问题是效率低下。为了编码一个句子，网络（无论是RNN还是Transformer）都需要处理其中的所有字符。例如，基于字符的模型需要处理“t”，“h”，“e”，和“_”（空格）来处理一个单词“the”，而基于单词的模型可以在一个步骤中完成。这种低效在输入序列变长时对Transformer的影响最大，注意计算的增长是二次方的。
- en: 10.2.3 Subword models
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3：子词模型
- en: So far, we studied two extremes—the word-based approach is efficient but not
    great at dealing with unknown words. The character-based approach is great at
    dealing with unknown words but is inefficient. Is there something in between?
    Can we use some tokenization that is both efficient and robust to unknown words?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们学习了两个极端——基于单词的方法效率很高，但在处理未知词方面表现不佳。基于字符的方法在处理未知词方面表现出色，但效率低下。有没有一种介于两者之间的标记化方法？我们能不能使用一些标记化方法既高效又能很好地处理未知词？
- en: Subword models are a recent invention that addresses this problem for neural
    networks. In subword models, the input text is segmented into a unit called *subwords*,
    which simply means something smaller than words. There is no formal linguistic
    definition as to what subwords actually are, but they roughly correspond to part
    of words that appear frequently. For example, one way to segment “dishwasher”
    is “dish + wash + er,” although some other segmentation is possible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 子词模型是神经网络针对这个问题的最新发明。在子词模型中，输入文本被分割成一个被称为*子词*的单位，这只是意味着比单词小的东西。对于什么是子词，没有正式的语言学定义，但它们大致对应于频繁出现的单词的一部分。例如，“dishwasher”的一种分段方法是“dish
    + wash + er”，尽管也可能有其他的分割方法。
- en: Some varieties of algorithms (such as WordPiece[¹](#pgfId-1107258) and SentencePiece[²](#pgfId-1107261))
    tokenize input into subwords, but by far the most widely used is *byte-pair encoding*
    (BPE).[³](#pgfId-1107267) BPE was originally invented as a compression algorithm,[⁴](#pgfId-1107270)
    but since 2016, it’s been widely used as a tokenization method for neural models,
    particularly in machine translation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法的变体（如 WordPiece[¹](#pgfId-1107258) 和 SentencePiece[²](#pgfId-1107261)）将输入标记化为子词，但迄今为止最广泛使用的是*字节对编码*（BPE）。[³](#pgfId-1107267)
    BPE 最初是作为一种压缩算法发明的，但自 2016 年以来，它已被广泛用作神经模型的标记化方法，特别是在机器翻译中。
- en: The basic concept of BPE is to keep frequent words (such as “the” and “you”)
    and n-grams (such as “-able” and “anti-”) unsegmented, while breaking up rarer
    words (such as “dishwasher”) into subwords (“dish + wash + er”). Keeping frequent
    words and n-grams together helps the model process those tokens efficiently, whereas
    breaking up rare words ensures there are no UNK tokens, because everything can
    be ultimately broken up into individual characters, if necessary. By flexibly
    choosing where to tokenize based on the frequency, BPE achieves the best of two
    worlds—being efficient while addressing the unknown word problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 的基本概念是保持频繁单词（如“the”和“you”）和 n 元组（如“-able”和“anti-”）不分段，同时将较少出现的单词（如“dishwasher”）分解为子词（“dish
    + wash + er”）。将频繁单词和 n 元组放在一起有助于模型高效处理这些标记，而分解稀有单词可以确保没有 UNK 标记，因为一切都最终可以分解为单个字符，如果需要的话。通过根据频率灵活选择标记位置，BPE
    实现了两全其美——既高效又解决了未知词问题。
- en: Let’s see how BPE determines where to tokenize with real examples. BPE is a
    purely statistical algorithm (it doesn’t use any language-dependent information)
    and operates by merging the most frequently occurring pair of consecutive tokens,
    one at a time. First, BPE tokenizes all the input texts into individual characters.
    For example, if your input is four words low, lowest, newer, and wider, it will
    tokenize them into l o w _, l o w e s t _, n e w e r _, w i d e r _. Here, “_”
    is a special symbol that indicates the end of each word. Then, the algorithm identifies
    any two consecutive elements that appear most often. In this example, the pair
    l o appears most often (two times), so these two characters are merged, yielding
    lo w _, lo w e s t _, n e w e r _, w i d e r _. Then, lo w will be merged into
    low, e r into er, er _ into er_, at which time you have low _, low e s t _, n
    e w er_, w i d er_. This process is illustrated in figure 10.6.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 BPE 如何确定在真实示例中进行标记化。BPE 是一种纯统计算法（不使用任何语言相关信息），通过一次合并最频繁出现的一对连续标记来操作。首先，BPE
    将所有输入文本标记化为单个字符。例如，如果您的输入是四个单词 low、lowest、newer 和 wider，则它们将被标记化为 l o w _、l o
    w e s t _、n e w e r _ 和 w i d e r _。在这里，“_”是一个特殊符号，表示每个单词的结尾。然后，算法识别出最频繁出现的任意两个连续元素。在这个例子中，对
    l o 出现最频繁（两次），所以这两个字符被合并，得到 lo w _、lo w e s t _、n e w e r _、w i d e r _。然后，lo
    w 将被合并为 low，e r 将被合并为 er，er _ 将被合并为 er_，此时您有 low _、low e s t _、n e w er_、w i d
    er_。此过程在图 10.6 中有所说明。
- en: '![CH10_F06_Hagiwara](../Images/CH10_F06_Hagiwara.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F06_Hagiwara](../Images/CH10_F06_Hagiwara.png)'
- en: Figure 10.6 BPE learns subword units by iteratively merging consecutive units
    that cooccur frequently.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 BPE 通过迭代地合并频繁出现的连续单元来学习子词单元。
- en: Notice that, after four merge operations, lowest is segmented into low e s t
    where frequent substrings such as low are merged together whereas infrequent ones
    such as est are broken apart. To segment a new input (e.g., lower), the same sequence
    of merge operations is applied in order, yielding low e r _. If you start from
    52 unique letters (26 upper- and lowercase letters), you will end up with 52 +
    N unique tokens in your vocabulary, where N is the number of merge operations
    executed. In this way, you have complete control over the size of the vocabulary.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在四次合并操作之后，lowest 被分割为 low e s t，其中频繁出现的子字符串（如 low）被合并在一起，而不频繁出现的子字符串（如 est）被拆分开来。要对新输入（例如
    lower）进行分割，将按顺序应用相同的合并操作序列，得到 low e r _。如果您从 52 个唯一字母（26 个大写字母和小写字母）开始，执行了 N 次合并操作，则您的词汇表中将有
    52 + N 个唯一标记，其中 N 是执行的合并操作数。通过这种方式，您完全控制了词汇表的大小。
- en: In practice, you rarely need to implement BPE (or any other subword tokenization
    algorithms) yourself. These algorithms are implemented in many open source libraries
    and platforms. Two popular options are Subword-NMT ([https://github.com/rsennrich/subword-nmt](https://github.com/rsennrich/subword-nmt))
    and SentencePiece ([https://github.com/google/sentencepiece](https://github.com/google/sentencepiece))
    (which also supports a variant of subword tokenization using a unigram language
    model). Many of the default tokenizers shipped with NLP frameworks, such as the
    one implemented in Hugging Face Transformers, support subword tokenization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Avoiding overfitting
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Overfitting* is one of the most common and important issues you need to address
    when building any machine learning applications. An ML model is said to overfit
    when it fits the given data so well that it loses its generalization ability to
    unseen data. In other words, the model may capture the training data very well
    and show good performance on it, but it may not be able to capture its inherent
    patterns well and shows poor performance on data that the model has never seen
    before.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Because overfitting is so prevalent in machine learning, researchers and practitioners
    have come up with a number of algorithms and techniques to combat overfitting
    in the past. In this section, we’ll learn two such techniques—regularization and
    early stopping. These are popular in any ML applications (not just NLP) and worth
    getting under your belt.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Regularization
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Regularization* in machine learning refers to techniques that encourage the
    simplicity and the generalization of the model. You can think of it as one form
    of penalty you'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: impose on your ML model to ensure that it is as generic as possible. What does
    it mean? Say you are building an “animal classifier” by training word embeddings
    from a corpus and by drawing a line between animals and other stuff in this embedding
    space (i.e., you represent each word as a multidimensional vector and classify
    whether the word describes an animal based on the coordinates of the vector).
    Let’s simplify this problem a lot and assume that each word is a two-dimensional
    vector, and you end up with the plot shown in figure 10.7\. You can now visualize
    how a machine learning model makes a classification decision by drawing lines
    where the decision flips between different classes (animals and non-animals),
    which is called the *classification boundary*. How would you draw a classification
    boundary so that animals (blue circles) are separated from everything else (triangles)?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F07_Hagiwara](../Images/CH10_F07_Hagiwara.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 Animal vs. non-animal classification plot
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: One simple way to separate animals is to draw one straight line, as in the first
    plot in figure 10.8\. This simple classifier makes several mistakes (in classifying
    words like “hot” and “bat”), but it correctly classifies the majority of data
    points. This sounds like a good start.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F08_Hagiwara](../Images/CH10_F08_Hagiwara.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 Classification boundaries with increasing complexity
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: What if you are told that the decision boundary doesn’t have to be a straight
    line? You may want to draw something like the one shown in the middle in figure
    10.8\. This one looks better—it makes fewer mistakes than the first one, although
    it is still not perfect. It appears tractable for a machine learning model because
    the shape is simple.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: But there’s nothing that stops you here. If you want to make as few errors as
    possible, you can also draw something wiggly like the one shown in the third plot.
    That decision boundary doesn’t even make any classification errors, which means
    that we achieved 100% classification accuracy!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Not so fast—remember that up until here, we’ve been thinking only about the
    training time, but the main purpose of machine learning models is to achieve good
    classification performance *at the test time* (i.e., they need to classify unobserved,
    new instances as correctly as possible). Now let’s think about how the three decision
    boundaries described earlier fare at test time. If we assume the test instances
    are distributed similarly to the training instances we saw in figure 10.8, the
    new “animal” points are most likely to fall in the upper-right region of the plot.
    The first two decision boundaries will achieve decent accuracy by classifying
    the majority of new instances correctly. But how about the third one? Training
    instances such as “hot” shown in the plot are most likely exceptions rather than
    the norm, so the curved sections of the decision boundary that tried to accommodate
    as many training instances as possible may do more harm than good at the test
    time by inadvertently misclassifying test instances. This is exactly what overfitting
    looks like—the model fits the training data so well that it sacrifices its generalization
    ability, which is what’s happening here.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Then, the question is, how can we avoid having your model look like the third
    decision boundary? After all, it is doing a very good job correctly classifying
    the training data. If you looked only at the training accuracy and/or the loss,
    there would be nothing to stop you from choosing it. One way to avoid overfitting
    is to use a separate, held-out dataset (called a *validation set*; see section
    2.2.3) to validate the performance of your model. But can we do this even without
    using a separate dataset?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The third decision boundary just doesn’t look right—it’s overly complex. With
    all other things being equal, we should prefer simpler models, because in general,
    simpler models generalize better. This is also in line with Occam’s razor, which
    states that a simpler solution is preferable to a more complex one. How can we
    balance between the training fit and the simplicity of the model?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: This is where regularization comes into play. Think of regularization as additional
    constraints imposed on the model so that simpler and/or more general models are
    preferred. The model is optimized so that it achieves the best training fit while
    being as generic as possible.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Numerous regularization techniques have been proposed in machine learning because
    overfitting is such an important topic. We are going to introduce only a few of
    the most important ones—L2 regularization (weight decay), dropout, and early stopping.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: L2 Regularization
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization, also called *weight decay*, is one of the most common regularization
    methods not just for NLP or deep learning but for a wide range of ML models. We
    are not going into its mathematical details, but in short, L2 regularization adds
    a penalty for the complexity of a model measured by how large its parameters are.
    To represent a complex classification boundary, an ML model needs to adjust a
    large number of parameters (the “magic constants”) to extreme values, measured
    by the L2 loss, which captures how far away they are from zero. Such models incur
    a larger L2 penalty, which is why L2 encourages simpler models. If you are interested
    in learning more about L2 regularization (and other related topics about NLP in
    general), check out textbooks such as *Speech and Language Processing* by Jurafsky
    and Martin ([https://web.stanford .edu/~jurafsky/slp3/5.pdf](https://web.stanford.edu/~jurafsky/slp3/5.pdf))
    or Goodfellow et al.’s *Deep Learning* ([https://www.deep learningbook.org/contents/regularization.html](https://www.deeplearningbook.org/contents/regularization.html)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '*Dropout* is another popular regularization technique commonly used with neural
    networks. Dropout works by randomly “dropping” neurons during training, where
    a “neuron” is basically a dimension of an intermediate layer and “dropping” means
    to mask it with zeros. You can think of dropout as a penalty to the model’s structural
    complexity and its reliance on particular features and values. As a result, the
    network tries to make the best guess with the remaining smaller number of values,
    which forces it to generalize well. Dropout is easy to implement and effective
    in practice and is used as a default regularization method in many deep learning
    models. For more information on dropout, the regularization chapter of the Goodfellow
    book mentioned earlier provides a good introduction and mathematical details of
    regularization techniques.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Early stopping
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another popular approach for combatting overfitting in machine learning is *early
    stopping*. Early stopping is a relatively simple technique where you stop training
    your model when the model performance stops improving, usually measured by the
    validation set loss. In chapter 6, we plotted learning curves when we built an
    English-Spanish machine translation model (shown again in figure 10.9). Notice
    that the validation loss curve flattens out around the eighth epoch and starts
    to creep up after that, which is a sign of overfitting. Early stopping would detect
    this, stop the training, and use the result from the best epoch when the loss
    is lowest. In general, early stopping has a “patience” parameter, which is the
    number of nonimproving epochs for early stopping to kick in. When patience is
    10 epochs, for example, the training pipeline will wait 10 epochs after the loss
    stops improving to stop the training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F09_Hagiwara](../Images/CH10_F09_Hagiwara.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 The validation loss curve flattens out around the eighth epoch and
    creeps back up.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Why does early stopping help mitigate overfitting? What does it have to do with
    model complexity? Without getting into mathematical details, it takes some time
    (training epochs) for the model to learn complex, overfitted decision boundaries.
    Most models start from something simple (e.g., straight decision lines) and gradually
    increase their complexity over the course of training. By stopping the training
    early, early stopping can prevent the model from becoming overly complex.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Many machine learning frameworks have built-in support for early stopping.
    For example, AllenNLP’s trainer supports early stopping by default. Recall that
    we used the following configuration in section 9.5.3 when we trained a BERT-based
    natural language inference model, where we used early stopping (with patience
    = 10) without paying much attention. This allows the trainer to stop if the validation
    metric doesn’t improve for 10 epochs:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 10.3.3 Cross-validation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Cross-validation* is not exactly a regularization method, but it is one of
    the techniques commonly used in machine learning. A common situation in building
    and validating a machine learning model is this—you have only a couple of hundred
    instances available for training. As we’ve seen so far in this book, you can’t
    train a reliable ML model just on the training set—you need a separate set for
    validation, and preferably another separate set for testing. How much you use
    for validation/testing depends on the task and the data size, but in general,
    it is advised that you set aside 5-20% of your training instances for validation
    and testing. This means that if your training data is small, your model is validated
    and tested on just a few dozen instances, which can make the estimated metrics
    unstable. Also, how you choose these instances has a large impact on the evaluation
    metrics, which is not ideal.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea of cross-validation is to iterate this phase (splitting the dataset
    into training and validation portions) multiple times with different splits to
    improve the stability of the result. Specifically, in a typical setting called
    *k-fold cross validation*, you first split the dataset into *k* different portions
    of equal size called *folds*. You use one of the folds for validation while training
    the model on the rest (*k* - 1 folds), and repeat this process *k* times, using
    a different fold for validation every time. See figure 10.10 for an illustration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F10_Hagiwara](../Images/CH10_F10_Hagiwara.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 In k-fold cross validation, the dataset is split into k equally
    sized folds and one is used for validation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The validation metrics are computed for every fold, and the final metrics are
    averaged over all iterations. This way, you can obtain a more stable estimate
    of the evaluation metrics that are not impacted heavily by the way the dataset
    is split.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The use of cross-validation in deep learning models is not common, because these
    models require a large amount of data, and you don’t need cross-validation if
    you have a large dataset, although its use is more common for more traditional
    and industrial settings where the amount of training data is limited.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Dealing with imbalanced datasets
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll focus on one of the most common problems you may encounter
    in building NLP and ML models—the class imbalance problem. The goal of a classification
    task is to assign one of the classes (e.g., spam or nonspam) to each instance
    (e.g., an email), but these classes are rarely distributed evenly. For example,
    in spam filtering, the number of nonspam emails is usually larger than the number
    of spam emails. In
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: document classification, some topics (such as politics or sports) are usually
    more popular than other topics. Classes are said to be imbalanced when some classes
    have way more instances than others (see figure 10.11 for an example).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F11_Hagiwara](../Images/CH10_F11_Hagiwara.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 Imbalanced dataset
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Many classification datasets have imbalanced classes, which poses some additional
    challenges when you train your classifier. The signals your model gets from smaller
    classes are overwhelmed by larger classes, which causes your model to perform
    poorly on minority classes. In the following subsections, I’m going to discuss
    some techniques you can consider when faced with an imbalanced dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Using appropriate evaluation metrics
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you even begin tweaking your dataset or your model, make sure you are
    validating your model with an appropriate metric. In section 4.3, we discussed
    why it is a bad idea to use accuracy as your evaluation metric when the dataset
    is imbalanced. In one extreme case, if 90% of your instances belong to class A
    and the other 10% belong to class B, even a stupid classifier that assigns class
    A to everything can achieve 90% accuracy. This is called a *majority class baseline*.
    A slightly more clever (but still stupid) classifier that randomly assigns label
    A 90% of the time and label B 10% of the time without even looking at the instance
    will achieve 0.9 * 0.9 + 0.1 * 0.1 = 82% accuracy. This is called a *random baseline*,
    and the more imbalanced your dataset is, the higher the accuracy of these baseline
    models will become.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: But this kind of random baseline is rarely a good model for minority classes.
    Imagine what would happen to class B if you used the random baseline. Because
    it will assign class A 90% of the time no matter what, 90% of the instances belonging
    to class B will be assigned class A. In other words, the accuracy of this random
    baseline for class B is only 10%. If this was a spam filter, it would let 90%
    of spam emails go through, no matter what the content is, just because 90% of
    emails you receive are not spam! This would make a terrible spam filter.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset is imbalanced and you care about the classification performance
    on the minority class, you should consider using metrics that are more appropriate
    for such settings. For example, if your task is a “needle in a haystack” type
    of setting, where the goal is to find a very small number of instances among others,
    you may want to use the F1-measure instead of accuracy. As we saw in chapter 4,
    the F-measure is some sort of average between precision (how hay-free your prediction
    is) and recall (how much of the needle you actually found). Because the F1-measure
    is calculated per class, it does not underrepresent minority classes. If you’d
    like to measure the model’s overall performance including majority classes, you
    can compute the macro-averaged F-measure, which is simply an arithmetic average
    of F-measures computed per class.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Upsampling and downsampling
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s look at concrete techniques that can mitigate the class imbalance
    problem. First of all, if you can collect more labeled training data, you should
    seriously consider doing that first. Unlike academic and ML competition settings
    where the dataset is fixed while you tweak your model, in a real-world setting
    you are free to do whatever is necessary to improve your model (of course, as
    long as it’s lawful and practical). Often, the best thing you can do to improve
    a model’s generalization is expose it to more data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset is imbalanced and the model is making biased predictions, you
    can either *upsample* or *downsample* your data so that classes have roughly equal
    representations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In upsampling (see the second figure in figure 10.12), you artificially increase
    the size of the minority class by copying the instances multiple times. Take the
    scenario we discussed earlier for example—if you duplicate the instances of class
    B and add eight extra copies of each instance to the dataset, they have an equal
    number of instances. This can mitigate the biased prediction issue. More sophisticated
    data augmentation algorithms such as SMOTE[⁵](#pgfId-1107425) are available, although
    they are not widely used in NLP, due to the inherent difficulty in generating
    linguistic examples artificially.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F12_Hagiwara](../Images/CH10_F12_Hagiwara.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 Upsampling and downsampling
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: If your model is biased not because the minority class is too small but because
    the majority class is too large, you can instead choose to downsample (the third
    figure in figure 10.12). In downsampling, you artificially decrease the size of
    the majority class by choosing a subset of the instances belonging to that class.
    For example, if you sample one out of nine instances from class A, you’ll end
    up with the equal number of instances in classes A and B. You can downsample in
    multiple ways—the easiest is to randomly choose the subset. If you would like
    to make sure that the downsampled dataset still preserves the diversity in the
    original data, you can try *stratified sampling*, where you sample some number
    of instances per group defined by some attributes. For example, if you have too
    many nonspam emails and want to downsample, you can group them by the sender’s
    domain first, then sample a fixed number of emails per domain. This will ensure
    that your sampled dataset will contain a diverse set of domains.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Note that neither upsampling nor downsampling is a magic bullet. If you “correct”
    the distribution of classes too aggressively, you risk making unfair predictions
    for the majority class, if that’s what you care about. Always make sure to check
    your model with a held-out validation set with appropriate evaluation metrics.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.3 Weighting losses
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach for mitigating the class imbalance problem is to use weighting
    when computing the loss, instead of making modification to your training data.
    Remember that the loss function is used to measure how “off” the model’s prediction
    for an instance is compared against the ground truth. When you measure how bad
    the model’s prediction is, you can tweak the loss so that it penalizes more when
    the ground truth belongs to the minority class.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at a concrete example. The binary cross-entropy loss, a common
    loss function used for training a binary classifier, looks like the curve shown
    in figure 10.13, when the correct label is 1\. The *x*-axis is the predicted probability
    of the target class, and the *y*-axis is the amount of loss the prediction will
    incur. When the prediction is perfectly correct (probability = 1), there’s no
    penalty, whereas as the prediction gets worse (probability < 1), the loss goes
    up.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F13_Hagiwara](../Images/CH10_F13_Hagiwara.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 Binary cross-entropy loss (when the correct label is 1)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: If you care more about the model’s performance on the minority class, you can
    tweak this loss. Specifically, you can change the shape of this loss (by simply
    multiplying it by a constant number) just for that class so that the model incurs
    a larger loss when it makes mistakes on the minority class. One such tweaked loss
    curve is shown in the figure 10.14 as the top curve. This weighting has the same
    effect as upsampling the minority class, although modifying the loss is computationally
    cheaper because you don’t need to actually increase the amount of training data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F14_Hagiwara](../Images/CH10_F14_Hagiwara.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 Weighted binary cross entropy loss
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy to implement loss weighting in PyTorch and AllenNLP. PyTorch’s binary
    cross-entropy implementation BCEWithLogitLoss already supports different weights
    for different classes. You simply need to pass the weight as the pos_weight parameter
    as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this code snippet, we randomly generate prediction (input) and the ground
    truth (target). There are three instances in total, two of which are of class
    0 (majority) and one belongs to class 1 (minority). We first compute the loss
    without weighting by calling the BCEWithLogitsLoss object, which returns the three
    loss values, one for each instance. We then compute the loss with weighting by
    passing the weight 2—this means that the wrong prediction will be penalized twice
    as much if the target class is positive (class 1). Notice that the third element
    corresponding to class 1 is twice as large as the one returned by the unweighted
    loss function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Hyperparameter tuning
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final section of this chapter, we’ll discuss hyperparameter tuning.
    *Hyperparameters* are parameters about the model and the training algorithm. This
    term is used in contrast with *parameters*, which are numbers that are used by
    the model to make predictions from the input. This is what we’ve been calling
    “magic constants” throughout this book—they work like constants in programming
    languages, although their exact values are automatically adjusted by optimization
    so that the prediction matches the desired output as closely as possible.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Correctly tuning hyperparameters is critical for many machine learning models
    to work properly and achieve their highest potential, and ML practitioners spend
    a lot of time tuning hyperparameters. Knowing how to tune hyperparameters effectively
    has a huge impact on your productivity in building NLP and ML systems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 Examples of hyperparameters
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyperparameters are “meta”-level parameters—unlike model parameters, they are
    used not to make predictions but for controlling the structure of the model and
    how the model is trained. For example, if you are working on word embeddings or
    an RNN, how many hidden units (dimensions) to use for representing words is one
    important hyperparameter. The number of RNN layers to use is another hyperparameter.
    In addition to these two hyperparameters (the number of hidden units and layers),
    the Transformer model we covered in chapter 9 has a number of other parameters,
    such as the number of attention heads and the dimension of the feed-forward network.
    Even the type of architecture you use, such as RNN versus Transformer, can be
    thought of as one hyperparameter.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Besides, the optimization algorithm you use may have hyperparameters, too. For
    example, the learning rate (section 9.3.3), one of the most important hyperparameters
    in many ML settings, determines how much to tweak the model parameters per optimization
    step. The number of epochs (iterations through the training dataset) is also an
    important hyperparameter, too.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been paying little attention to those hyperparameters, let alone
    optimizing them. However, hyperparameters can have a huge impact on the performance
    of machine learning models. In fact, many ML models have a “sweet spot” of hyperparameters
    that makes them most effective, whereas using a set of hyperparameters outside
    of this spot may make the model perform poorly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Many ML practitioners tune hyperparameters by hand. This means that you start
    from a set of hyperparameters that look reasonable and measure the model’s performance
    on a validation set. Then you change one or more of the hyperparameters slightly
    and measure the performance again. You repeat this process several times until
    you hit the “plateau,” where any change of hyperparameters provides only a marginal
    improvement.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: One issue with this manual tuning approach is that it is slow and arbitrary.
    Let’s say you start from one set of hyperparameters. How do you know which ones
    to adjust next, and how much? How do you know when to stop? If you have experience
    tuning a wide range of ML models, you might have some “hunch” about how these
    models respond to certain hyperparameter changes, but if not, it’s like shooting
    in the dark. Hyperparameter tuning is such an important topic that ML researchers
    have been working on better and more organized ways to optimize them.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Grid search vs. random search
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We understand that manual optimization of hyperparameters is inefficient, but
    how should we go about optimizing them, then? We have two more-organized ways
    of tuning hyperparameters—grid search and random search.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In *grid search*, you simply try every possible combination of the hyperparameter
    values you want to optimize. For example, let’s assume your model has just two
    hyperparameters—the number of RNN layers and the embedding dimension. You first
    define reasonable ranges for these two hyperparameters, for example, [1, 2, 3]
    for the number of layers and [128, 256, 512] for the dimensionality. Then grid
    search measures the model’s validation performance for every combination—(1, 128),
    (1, 256), (1, 512), (2, 128), . . . , (3, 512)—and simply picks the best-performing
    combination. If you plot these combinations on a 2-D plot, it looks like a grid
    (see the illustration in figure 10.15), which is why this is called *grid search*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Grid search is a simple and intuitive way to optimize the hyperparameters. However,
    if you have many hyperparameters and/or their ranges are large, this method gets
    out of hand. The number of possible combinations is exponential, which makes it
    impossible to explore all of them in a reasonable amount of time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F15_Hagiwara](../Images/CH10_F15_Hagiwara.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 Grid search vs. random search for hyperparameter tuning. (Adapted
    from Bergstra and Bengio, 2012; [https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf.)](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: A better alternative to grid search is *random search*. In random search, instead
    of trying every possible combination of hyperparameter values, you randomly sample
    the values and measure the model’s performance on a specified number of combinations
    (which are called *trials*). For example, in the previous example, random search
    may choose (2, 87), (1, 339), (2, 101), (3, 254), and so on until it hits the
    specified number of trials. See the illustration in figure 10.15 (right).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Unless your hyperparameter search space is very small (like the first example),
    random search is usually recommended over grid search if you want to optimize
    hyperparameters efficiently. Why? In many machine learning settings, not every
    hyper-parameter is made equal—there are usually only a small number of hyperparameters
    that actually matter for the performance, whereas many others do not. Grid search
    will waste a lot of computation searching for the best combination of hyperparameters
    that do not really matter, while being unable to explore the few hyperparameters
    that do matter in detail (figure 10.15, left). On the other hand, random search
    can explore many possible points on the axis that matters for the performance
    (figure 10.15, right). Notice that random search can find a better model by exploring
    more points on the x-axis with the same number of trials (total of nine trials).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.3 Hyperparameter tuning with Optuna
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OK, we’ve covered some ways to tune hyperparameters including manual, grid,
    and random search, but how should you go about implementing it in practice? You
    can always write your own for-loop (or “for-loops,” in the case of grid search),
    although it would quickly get tiring if you need to write this type of boilerplate
    code for every model and task you work on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization is such a universal topic that many ML researchers
    and engineers have been working on better algorithms and software libraries. For
    example, AllenNLP has its own library called *Allentune* ([https://github.com/allenai/allentune](https://github.com/allenai/allentune))
    that you can easily integrate with your AllenNLP training pipeline. In the remainder
    of this section, however, I’m going to introduce another hyperparameter tuning
    library called *Optuna* ([https://optuna.org/](https://optuna.org/)) and show
    how to use it with AllenNLP to optimize your hyperparameters. Optuna implements
    state-of-the-art algorithms that search for optimal hyperparameters efficiently
    and provides integration with a wide range of machine learning frameworks, including
    TensorFlow, PyTorch, and AllenNLP.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we assume that you have installed AllenNLP (1.0.0+) and the Optuna plugin
    for AllenNLP. These can be installed by running the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, as instructed by the official documentation ([https://github.com/himkt/allennlp
    -optuna](https://github.com/himkt/allennlp-optuna)), you need to register the
    plugin with AllenNLP by running the next code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are going to use the LSTM-based classifier for the Stanford Sentiment Treebank
    dataset we built in chapter 2\. You can find the AllenNLP config file in the book
    repository ([http://www.realworldnlpbook.com/ch10.html#config](http://www.realworldnlpbook.com/ch10.html#config)).
    Note that you need to reference the variables (std.extVar) for Optuna to have
    control over the parameters. Specifically, you need to define them at the beginning
    of the config file:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, you need to tell Optuna which parameters to optimize. You can do this
    by writing a JSON file (hparams.json ([http://www.realworldnlpbook.com/ch10.html#
    hparams](http://www.realworldnlpbook.com/ch10.html#hparams))). You need to specify
    every hyperparameter you want Optuna to optimize with its types and ranges as
    follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, invoke this command to start the optimization:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that we are running 20 trials (—n-trials) with validation accuracy (—metrics
    best_validation_accuracy) as the metric to maximize (—direction maximize). If
    you do not specify the metric and the direction, by default Optuna tries to minimize
    the validation loss.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'This will take a while, but after all the trials are finished you will see
    the following one-line summary of the optimization:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, Optuna supports a wide range of visualization of the optimization
    result, including very nice contour plots ([http://www.realworldnlpbook.com/ch10.html#
    contour)](http://www.realworldnlpbook.com/ch10.html#contour), but here we''ll
    simply use its web-based dashboard to quickly inspect the optimization process.
    All you need to do is invoke its dashboard from the command line as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you can access http:/./localhost:5006/dashboard to see the dashboard, shown
    in figure 10.16.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![CH10_F16_Hagiwara](../Images/CH10_F16_Hagiwara.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 The Optuna dashboard shows the evaluation metrics of the parameters
    for each trial.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'From this dashboard you can quickly see not only that your optimal trial was
    trial #14 but also the optimal hyperparameters at each trial.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instances are sorted, padded, and batched together for more efficient computation.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subword tokenization algorithms such as BPE split words into units smaller than
    words to mitigate the out-of-vocabulary problem in neural network models.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization (such as L2 and dropout) is a technique to encourage model simplicity
    and generalizability in machine learning.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use data upsampling, downsampling, or loss weights for addressing the
    data imbalance issue.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters are parameters about the model or the training algorithm. They
    can be optimized using manual, grid, or random search. Even better, use hyperparameter
    optimization libraries such as Optuna, which integrates easily with AllenNLP.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^(1.)Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap
    between Human and Machine Translation,” (2016). [https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '^(2.)Kudo, “Subword Regularization: Improving Neural Network Translation Models
    with Multiple Subword Candidates,” (2018). [https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: ^(3.)Sennrich et al., “Neural Machine Translation of Rare Words with Subword
    Units,” (2016). [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: ^(4.)See [https://www.derczynski.com/papers/archive/BPE_Gage.pdf](https://www.derczynski.com/papers/archive/BPE_Gage.pdf).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '^(5.)Chawla et al., “SMOTE: Synthetic Minority Over-Sampling Technique,” (2002).
    [https://arxiv.org/abs/1106.1813](https://arxiv.org/abs/1106.1813).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
