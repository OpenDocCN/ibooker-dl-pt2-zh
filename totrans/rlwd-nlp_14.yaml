- en: 10 Best practices in developing NLP applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发自然语言处理应用的十大最佳实践
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Making neural network inference more efficient by sorting, padding, and masking
    tokens
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对令牌进行排序、填充和掩码使神经网络推断更有效率
- en: Applying character-based and BPE tokenizationfor splitting text into tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用基于字符和BPE的分词技术将文本分割成令牌
- en: Avoiding overfitting via regularization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过正则化避免过拟合
- en: Dealing with imbalanced datasets by using upsampling, downsampling, and loss
    weighting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用上采样、下采样和损失加权处理不平衡数据集
- en: Optimizing hyperparameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化超参数
- en: We’ve covered a lot of ground so far, including deep neural network models such
    as RNNs, CNNs, and the Transformer, and modern NLP frameworks such as AllenNLP
    and Hugging Face Transformers. However, we’ve paid little attention to the details
    of training and inference. For example, how do you train and make predictions
    efficiently? How do you avoid having your model overfit? How do you optimize hyperparameters?
    These factors could make a huge impact on the final performance and generalizability
    of your model. This chapter covers these important topics that you need to consider
    to build robust and accurate NLP applications that perform well in the real world.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了很多内容，包括RNN、CNN和Transformer等深度神经网络模型，以及AllenNLP和Hugging Face Transformers等现代NLP框架。然而，我们对训练和推断的细节关注不多。例如，如何高效地训练和进行预测？如何避免模型过拟合？如何优化超参数？这些因素可能会对模型的最终性能和泛化能力产生巨大影响。本章涵盖了您需要考虑的这些重要主题，以构建在实际中表现良好的稳健准确的NLP应用程序。
- en: 10.1 Batching instances
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 实例批处理
- en: In chapter 2, we briefly mentioned *batching*, a machine learning technique
    where instances are grouped together to form batches and sent to the processor
    (CPU or, more often, GPU). Batching is almost always necessary when training large
    neural networks—it is critical for efficient and stable training. In this section,
    we’ll dive into some more techniques and considerations related to batching.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，我们简要提到了*批处理*，这是一种机器学习技术，其中实例被分组在一起形成批次，并发送到处理器（CPU或更常见的GPU）。在训练大型神经网络时，批处理几乎总是必要的——它对于高效稳定的训练至关重要。在本节中，我们将深入探讨与批处理相关的一些技术和考虑因素。
- en: 10.1.1 Padding
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 填充
- en: Training large neural networks requires a number of linear algebra operations
    such as matrix addition and multiplication, which involve executing basic mathematical
    operations on many, many numbers at once. This is why it requires specialized
    hardware such as GPUs, processors designed to execute such operations in a highly
    parallelized manner. Data is sent to the GPU as tensors, which are just high-dimensional
    arrays of numbers, along with some instructions as to what types of mathematical
    operations it needs to execute. The result is sent back as another tensor.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型神经网络需要进行许多线性代数运算，如矩阵加法和乘法，这涉及同时对许多许多数字执行基本数学运算。这就是为什么它需要专门的硬件，如GPU，设计用于高度并行化执行此类操作的处理器。数据被发送到GPU作为张量，它们只是数字的高维数组，以及一些指示，说明它需要执行什么类型的数学运算。结果被发送回作为另一个张量。
- en: In chapter 2, we likened GPUs to factories overseas that are highly specialized
    and optimized for manufacturing the same type of products in a large quantity.
    Because there is considerable overhead in communicating and shipping products,
    it is more efficient if you make a small number of orders for manufacturing a
    large quantity of products by shipping all the required materials in batches,
    rather than shipping materials on demand.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，我们将GPU比作海外高度专业化和优化的工厂，用于大量生产相同类型的产品。由于在通信和运输产品方面存在相当大的开销，因此如果您通过批量运输所有所需材料来进行小量订单以制造大量产品，而不是按需运输材料，则效率更高。
- en: Materials and products are usually shipped back and forth in standardized containers.
    If you have ever loaded a moving pod or trailer yourself (or observed someone
    else do it), you may know that there are many considerations that are important
    for safe and reliable shipping. You need to put furniture and boxes in tightly
    so that they don’t shift around in transition. You need to wrap them with blankets
    and fix them with ropes to prevent them from being damaged. You need to put heavy
    stuff at the bottom so that lighter stuff won’t get crushed, and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 材料和产品通常在标准化的容器中来回运输。如果你曾经自己装过搬家货舱或观察别人装过，你可能知道有很多需要考虑的因素来确保安全可靠的运输。你需要紧紧地把家具和箱子放在一起，以免在过渡过程中移位。你需要用毯子裹着它们，并用绳子固定它们，以防止损坏。你需要把重的东西放在底部，以免把轻的东西压坏，等等。
- en: Batches in machine learning are similar to containers for shipping stuff in
    the real world. Just like shipping containers are all the same size and rectangular,
    batches in machine learning are just rectangular tensors packed with numbers of
    the same type. If you want to “ship” multiple instances of different shapes in
    a single batch to the GPU, you need to pack them so that the packed numbers form
    a rectangular tensor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的批次类似于现实世界中用于运输物品的容器。就像运输集装箱都是相同的尺寸和矩形形状一样，机器学习中的批次只是装有相同类型数字的矩形张量。如果你想要将不同形状的多个实例在单个批次中“运送”到GPU，你需要将它们打包，使打包的数字形成一个矩形张量。
- en: In NLP, we often deal with sequences of text in different lengths. Because batches
    have to be rectangular, we need to do *padding*, (i.e., append special tokens,
    <PAD>, to each sequence so that each row of the tensor has the same length. You
    need as many padding tokens as necessary to make the sequences the same length,
    which means that you need to pad short sequences until they are all as long as
    the longest sequence in the same batch. This is illustrated in figure 10.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，我们经常处理长度不同的文本序列。因为批次必须是矩形的，所以我们需要进行*填充*（即在每个序列末尾加上特殊标记< PAD >），以便张量的每一行具有相同的长度。你需要足够多的填充标记，以使序列的长度相同，这意味着你需要填充短的序列，直到它们与同一批次中最长的序列一样长。示例见图10.1。
- en: '![CH10_F01_Hagiwara](../Images/CH10_F01_Hagiwara.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH10'
- en: Figure 10.1 Padding and batching. Black squares are tokens, gray ones are EOS
    tokens, and white ones are padding.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 填充和分批。黑色方块表示标记，灰色方块表示EOS（结束）标记，白色方块表示填充。
- en: In reality, each token in natural language text is often represented as a vector
    of length *D*, generated by the word embeddings method. This means that each batched
    tensor is a three-dimensional tensor that has a “depth” of *D*. In many NLP models,
    sequences are represented as batches of size *N* × *L* × *D* (see figure 10.2),
    where *N*, *L*, *D* are the number of instances per batch, the maximum length
    of the sequences, and the dimension of word embeddings, respectively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，自然语言文本中的每个标记通常表示为长度为*D*的向量，由词嵌入方法生成。这意味着每个批次的张量是一个三维张量，其“深度”为*D*。在许多自然语言处理模型中，序列被表示为大小为*N*×*L*×*D*的批次（见图10.2），其中*N*、*L*、*D*分别表示批次中的实例数目、序列的最大长度和词嵌入的维度。
- en: '![CH10_F02_Hagiwara](../Images/CH10_F02_Hagiwara.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F02_Hagiwara](../Images/CH10_F02_Hagiwara.png)'
- en: Figure 10.2 Padding and batching of embedded sequences create rectangular, three-dimensional
    tensors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 嵌入序列的填充和分批创建了三维的矩形张量。
- en: This is starting to look more like real containers!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来越来越像真正的容器了！
- en: 10.1.2 Sorting
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 排序
- en: Because each batch has to be rectangular, if one batch happens to include both
    short sequences and long sequences, you need to add a lot of padding to short
    sequences so that they are as long as the longest sequence in the same batch.
    This often leads to some wasted space in the batch—see “batch 1” in figure 10.3
    for an illustration. The shortest sequence (six tokens) needs to be padded with
    eight more tokens to be equally long as the longest sequence (14 tokens). Wasted
    space in a tensor means wasted memory and computation, so it is best avoided,
    but how?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个批次必须是矩形的，如果一个批次同时包含短序列和长序列，你需要为短序列添加大量填充，使它们与同一批次中最长的序列一样长。这通常会导致批次中存在一些浪费空间——见图10.3中的“batch
    1”示例。最短的序列（六个标记）需要填充八个标记才能与最长的序列（14个标记）长度相等。张量中的浪费空间意味着存储和计算的浪费，所以最好避免这种情况发生，但是怎么做呢？
- en: '![CH10_F03_Hagiwara](../Images/CH10_F03_Hagiwara.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F03_Hagiwara](../Images/CH10_F03_Hagiwara.png)'
- en: Figure 10.3 Sorting instances before batching (right) reduces the total number
    of tensors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 在批处理之前对实例进行排序（右侧）可以减少总张量数量。
- en: You can reduce the amount of padding by putting instances of similar size in
    the same batch. If shorter instances are batched only with other equally shorter
    ones, they don’t need to get padded with many padding tokens. Similarly, if longer
    instances are batched only with other longer ones, they don’t need a lot of padding
    either, because they are already long. One idea is to sort instances by their
    length and batch accordingly. Figure 10.3 compares two situations—one in which
    the instances are batched in their original order, and the other where instances
    are first sorted before batching. The numbers below each batch indicate how many
    tokens are required to represent the batch, including the padding tokens. Notice
    that the number of total tokens is reduced from 144 to 120 by sorting. Because
    the number of tokens in the original sentences doesn’t change, this is purely
    because sorting reduced the number of padding tokens. Smaller batches require
    less memory to store and less computation to process, so sorting instances before
    batching improves the efficiency of training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将相似大小的实例放在同一个批次中，可以减少填充的量。如果较短的实例只与其他同样较短的实例一起批处理，则它们不需要用许多填充标记进行填充。同样，如果较长的实例只与其他较长的实例一起批处理，则它们也不需要很多填充，因为它们已经很长了。一个想法是按照它们的长度对实例进行排序，并相应地进行批处理。图
    10.3 比较了两种情况——一种是实例按其原始顺序进行批处理，另一种是在批处理之前对实例进行排序。每个批次下方的数字表示表示批次所需的标记数，包括填充标记。注意，通过排序，总标记数从
    144 降低到 120。因为原始句子中的标记数没有变化，所以这纯粹是因为排序减少了填充标记的数量。较小的批次需要更少的内存来存储和更少的计算来处理，因此在批处理之前对实例进行排序可以提高训练的效率。
- en: 'All these techniques sound somewhat complicated, but the good news is, you
    rarely need to write code for sorting, padding, and batching instances yourself
    as long as you use high-level frameworks such as AllenNLP. Recall that we used
    a combination of DataLoader and BucketBatchSampler for building our sentiment
    analysis model back in chapter 2 as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术听起来有点复杂，但好消息是，只要使用高级框架（如 AllenNLP），你很少需要自己编写排序、填充和批处理实例的代码。回想一下，在第 2 章中构建情感分析模型时，我们使用了
    DataLoader 和 BucketBatchSampler 的组合，如下所示：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The sorting_keys given to BucketBatchSampler specifies which field to use for
    sorting. As you can guess from its name, by specifying “tokens” you are telling
    the data loader to sort the instances by the number of tokens (which is what you
    want in most cases). The pipeline will take care of padding and batching automatically,
    and the data loader will give you a series of batches you can feed into your model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: BucketBatchSampler 中给定的 sorting_keys 指定了要用于排序的字段。从名称可以猜出，通过指定“tokens”，你告诉数据加载器按照标记数对实例进行排序（在大多数情况下是你想要的）。流水线会自动处理填充和批处理，数据加载器会提供一系列批次供您的模型使用。
- en: 10.1.3 Masking
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 掩码
- en: One final detail that you need to pay attention to is *masking*. Masking is
    an operation where you ignore some part of the network that corresponds to padding.
    This becomes relevant especially when you are dealing with a sequential-labeling
    or a language-generation model. To recap, sequential labeling is a task where
    the system assigns a label per token in the input sequence. We built a POS tagger
    with a sequential labeling model (RNN) in chapter 5.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个需要注意的细节是 *掩码*。掩码是一种操作，用于忽略与填充相对应的网络的某些部分。当你处理顺序标记或语言生成模型时，这变得特别重要。回顾一下，顺序标记是一种任务，其中系统为输入序列中的每个标记分配一个标签。我们在第
    5 章中使用了顺序标记模型（RNN）构建了一个词性标注器。
- en: As shown in figure 10.4, sequential-labeling models are trained by minimizing
    the per-token loss aggregated across all tokens in a given sentence. We do this
    because we’d like to minimize the number of “errors” the network makes per token.
    This is fine as long as we are dealing with “real” tokens (“time,” “flies,” and
    “like” in the figure), although it becomes an issue when the input batch includes
    padded tokens. Because they exist just to pad the batch, they should be ignored
    when computing the total loss.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 10.4 所示，顺序标记模型通过最小化给定句子中所有标记的每个标记损失来进行训练。我们这样做是因为我们希望最小化网络每个标记的“错误”数量。只要处理“真实”标记（图中的“time”，“flies”和“like”），这是可以接受的，尽管当输入批次包含填充标记时，这就成为一个问题。因为它们只是为了填充批次而存在，所以在计算总损失时应该忽略它们。
- en: '![CH10_F04_Hagiwara](../Images/CH10_F04_Hagiwara.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F04_Hagiwara](../Images/CH10_F04_Hagiwara.png)'
- en: Figure 10.4 Loss for a sequence is the sum of per-token cross entropy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 序列的损失是每个标记的交叉熵之和。
- en: We usually do this by creating an extra vector for masking the loss. The vector
    for masking has the same length as the input, whose elements are 1s for “real”
    tokens and 0s for padding. When computing the total loss, you can simply take
    an element-wise product between the per-token loss and the mask and then sum up
    the result.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常通过创建一个额外的用于掩码损失的向量来完成这个过程。用于掩码的向量的长度与输入相同，其元素为“真”标记和填充的“假”标记。在计算总损失时，你可以简单地对每个标记的损失和掩码进行逐元素乘积，然后对结果进行求和。
- en: Fortunately, as long as you are building standard sequential-labeling models
    with AllenNLP, you rarely need to implement masking yourself. Remember, in chapter
    5, we wrote the forward pass of the POS tagger model as shown in listing 10.1\.
    Here, we get the mask vector from the get_text_field_mask() helper function and
    compute the final loss with sequence_cross_entropy_with_logits().
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，只要你正在使用 AllenNLP 构建标准的顺序标记模型，你很少需要自己实现掩码。记住，在第五章，我们按照列表 10.1 中所示编写了 POS
    标签器模型的前向传播。在这里，我们从 get_text_field_mask() 辅助函数获取掩码向量，并使用 sequence_cross_entropy_with_logits()
    计算最终损失。
- en: Listing 10.1 Forward pass of the POS tagger
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.1 POS 标签器的前向传播
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you take a peek at what’s inside mask (e.g., by inserting a print statement
    in this forward method), you’ll see the following tensor made of binary (True
    or False) values:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你偷看一下掩码中的内容（比如，在这个前向方法中插入一个打印语句），你会看到以下由二进制（真或假）值组成的张量：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each row of this tensor corresponds to one sequence of tokens, and locations
    with False are where padding occurred. The loss function (sequence_cross_entropy
    _with_logits) receives the prediction, the ground truth (the correct labels),
    and the mask and computes the final loss while ignoring all the elements marked
    as False.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个张量的每一行对应一个标记序列，False 的位置是填充发生的地方。损失函数（sequence_cross_entropy_with_logits）接收预测值、真实标签和掩码，并在忽略所有标记为
    False 的元素时计算最终损失。
- en: 10.2 Tokenization for neural models
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 用于神经模型的标记化
- en: In chapter 3, we covered the basic linguistic units (words, characters, and
    n-grams) and how to compute their embeddings. In this section, we will go deeper
    and focus on how to analyze texts and obtain these units—a process called *tokenization*.
    Neural network models pose a set of unique challenges on how to deal with tokens,
    and we’ll cover some of the modern models to deal with these challenges.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章，我们介绍了基本的语言单位（单词、字符和 n-gram）以及如何计算它们的嵌入。在本节中，我们将更深入地讨论如何分析文本并获取这些单位的过程——称为*标记化*。神经网络模型在处理标记时面临一系列独特的挑战，我们将介绍一些现代模型来解决这些挑战。
- en: 10.2.1 Unknown words
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 未知单词
- en: A vocabulary is a set of tokens that an NLP model deals with. Many neural NLP
    models operate within a fixed, finite set of tokens. For example, when we built
    a sentiment analyzer in chapter 2, the AllenNLP pipeline first tokenized the training
    dataset and constructed a Vocabulary object that consists of all unique tokens
    that appeared more than, say, three times. The model then uses an embedding layer
    to convert tokens into word embeddings, which are some abstract representation
    of the input tokens.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表是一个 NLP 模型处理的标记集合。许多神经网络自然语言处理模型在一组固定、有限的标记中运作。例如，在第二章构建情感分析器时，AllenNLP 管道首先对训练数据集进行标记化，并构造一个
    Vocabulary 对象，该对象包含了所有出现次数超过，比如，三次以上的所有唯一标记。然后模型使用一个嵌入层将标记转换为单词嵌入，这是输入标记的一些抽象表示。
- en: So far, so good, right? But the number of all words in the world is not finite.
    We constantly make up new words that didn’t exist before (I don’t think people
    talked about “NLP” a hundred years ago). What if the model receives a word that
    it has never seen during training? Because the word is not part of the vocabulary,
    the model cannot even convert it to an index, let alone look up its embeddings.
    Such words are called *out-of-vocabulary* (OOV) *words*, and they are one of the
    biggest problems when building NLP applications.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，一切都很顺利，对吧？但是世界上的所有单词数量并不是有限的。我们不断创造以前不存在的新单词（我不认为一百年前人们谈论过“NLP”）。如果模型接收到在训练期间从未见过的单词怎么办？因为这个单词不是词汇表的一部分，所以模型甚至不能将其转换为索引，更不用说查找其嵌入了。这样的单词被称为*词汇外*（OOV）*单词*，它们是构建自然语言处理应用时最大的问题之一。
- en: By far the most common (but not the best) way to deal with this problem is to
    represent all the OOV tokens as a special token, which is conventionally named
    UNK (for “unknown”). The idea is that every time the model sees a token that is
    not part of the vocabulary, it pretends it saw a special token UNK instead and
    proceeds as usual. This means that the vocabulary and the embedding table both
    have a designated “slot” for UNK so that the model can deal with words that it
    has never seen. The embeddings (and any other parameters) for UNK are trained
    in the same manner as other regular tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，处理这个问题最常见（但不是最好）的方法是将所有的 OOV 标记表示为一个特殊的标记，通常称为 UNK（代表“未知”）。想法是每当模型看到一个不属于词汇表的标记时，它都会假装看到了一个特殊的标记
    UNK，并像往常一样继续执行。这意味着词汇表和嵌入表都有一个专门的“插槽”用于 UNK，以便模型可以处理从未见过的词汇。UNK 的嵌入（以及任何其他参数）与其他常规标记一样进行训练。
- en: Do you see any problems with this approach? Treating all OOV tokens with a single
    UNK token means that they are collapsed into a single embedding vector. It doesn’t
    matter if the word is “NLP” or “doggy”—as long as it’s something unseen, it always
    gets treated as a UNK token and assigned the same vector, which becomes a generic,
    catch-all representation of various words. Because of this, the model cannot tell
    the differences among OOV words, no matter what the identity of the words is.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否看到这种方法存在任何问题？将所有的 OOV 标记都用一个单一的 UNK 标记来对待意味着它们被折叠成一个单一的嵌入向量。无论是“NLP”还是“doggy”——只要是未见过的东西，总是被视为一个
    UNK 标记并被分配相同的向量，这个向量成为各种词汇的通用、全能表示。因此，模型无法区分 OOv 词汇之间的差异，无论这些词汇的身份是什么。
- en: This may be fine if you are building, for example, a sentiment analyzer. OOV
    words are, by definition, very rare and might not affect the prediction of most
    of the input sentences. However, this becomes a huge problem if you are building
    a machine translation system or a conversational engine. It wouldn’t be a usable
    MT system or a chatbot if it produces “I don’t know” every time it sees new words!
    In general, the OOV problem is more serious for language-generation systems (including
    machine translation and conversational AI) compared to NLP systems for prediction
    (sentiment analysis, POS tagging, and so on).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建一个情感分析器，这可能是可以接受的。OOV 词汇从定义上来说非常少见，可能不会影响到大部分输入句子的预测。然而，如果你正在构建一个机器翻译系统或一个对话引擎，这将成为一个巨大的问题。如果每次看到新词汇时都产生“我不知道”，那么它就不会是一个可用的
    MT 系统或聊天机器人！一般来说，与用于预测的 NLP 系统（情感分析、词性标注等）相比，对于语言生成系统（包括机器翻译和对话 AI），OOV 问题更为严重。
- en: How can we do better? OOV tokens are such a big problem in NLP that there has
    been a lot of research work on how to deal with them. In the following subsections,
    we’ll cover character-based and subword-based models, two techniques commonly
    used for building robust neural NLP models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如何做得更好？在自然语言处理中，OOV 标记是一个如此严重的问题，以至于已经有很多研究工作在如何处理它们上面。在下面的小节中，我们将介绍基于字符和基于子词的模型，这是两种用于构建强大神经网络自然语言处理模型的常用技术。
- en: 10.2.2 Character models
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 字符模型
- en: The simplest yet effective solution for dealing with the OOV problem is to treat
    characters as tokens. Specifically, we break the input text into individual characters,
    even including punctuation and whitespace, and treat them as if they are regular
    tokens. The rest of the application is unchanged—“word” embeddings are assigned
    to characters, which are further processed by the model. If the model produces
    text, it does so character-by-character.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 处理 OOV 问题最简单但最有效的解决方案是将字符视为标记。具体来说，我们将输入文本分解为单个字符，甚至包括标点符号和空白字符，并将它们视为常规标记。应用程序的其余部分保持不变——“单词”嵌入被分配给字符，然后由模型进一步处理。如果模型生成文本，它是逐字符地生成的。
- en: In fact, we used a character-level model in chapter 5 when we built a language
    generator. Instead of generating text word-by-word, the RNN produces text one
    character at a time, as illustrated in figure 10.5\. Thanks to this strategy,
    the model was able to produce words that look like English but actually aren’t.
    Notice a number of peculiar words (*despoit*, *studented*, *redusention*, *distaples*)
    that resemble English words in the output shown in listing 10.2\. If the model
    operated on words, it produces only known words (or UNKs when unsure), and this
    wouldn’t have been possible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们构建语言生成器时，我们在第5章使用了字符级模型。RNN不是一次生成一个单词，而是一次生成一个字符，如图10.5所示。由于这种策略，模型能够生成看起来像英语但实际上不是的单词。请注意10.2列表中显示的输出中类似于英语的许多奇怪的单词（*despoit*，*studented*，*redusention*，*distaples*）.如果模型操作单词，它只会生成已知的单词（或者在不确定时生成UNKs），这是不可能的。
- en: '![CH10_F05_Hagiwara](../Images/CH10_F05_Hagiwara.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F05_Hagiwara](../Images/CH10_F05_Hagiwara.png)'
- en: Figure 10.5 A language-generation model that generates text character-by-character
    (including whitespace)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：生成文本字符级（包括空格）的语言生成模型
- en: Listing 10.2 Generated sentences by a character-based language model
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列10.2：字符级语言模型生成的句子
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Character-based models are versatile and put few assumptions on the structure
    of the language. For languages with small sets of alphabets (like English), it
    effectively eradicates unknown words, because almost any words, no matter how
    rare they are, can be broken down into characters. Tokenizing into characters
    is also an effective strategy for languages with large alphabets (like Chinese),
    although you need to watch out for “unknown character” problems.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符的模型是多功能的，并对语言的结构做出了少量的假设。对于拥有小字母表的语言（比如英语），它有效地消除了未知单词，因为几乎任何单词，无论其多么罕见，都可以被分解为字符。对于拥有大字母表的语言（如中文），将其标记为字符也是一种有效的策略，尽管你需要注意“未知字符”的问题。
- en: However, this strategy is not without drawbacks. The biggest issue is its inefficiency.
    To encode a sentence, the network (be it an RNN or the Transformer) needs to go
    over all the characters in it. For example, a character-based model needs to process
    “t,” “h,” “e,” and “_” (whitespace) to process a single word “the,” whereas a
    word-based model can finish this in a single step. This inefficiency takes its
    biggest toll on the Transformers, where the attention computation increases quadratically
    when the input sequence gets longer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种策略并非没有缺点。最大的问题是效率低下。为了编码一个句子，网络（无论是RNN还是Transformer）都需要处理其中的所有字符。例如，基于字符的模型需要处理“t”，“h”，“e”，和“_”（空格）来处理一个单词“the”，而基于单词的模型可以在一个步骤中完成。这种低效在输入序列变长时对Transformer的影响最大，注意计算的增长是二次方的。
- en: 10.2.3 Subword models
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3：子词模型
- en: So far, we studied two extremes—the word-based approach is efficient but not
    great at dealing with unknown words. The character-based approach is great at
    dealing with unknown words but is inefficient. Is there something in between?
    Can we use some tokenization that is both efficient and robust to unknown words?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们学习了两个极端——基于单词的方法效率很高，但在处理未知词方面表现不佳。基于字符的方法在处理未知词方面表现出色，但效率低下。有没有一种介于两者之间的标记化方法？我们能不能使用一些标记化方法既高效又能很好地处理未知词？
- en: Subword models are a recent invention that addresses this problem for neural
    networks. In subword models, the input text is segmented into a unit called *subwords*,
    which simply means something smaller than words. There is no formal linguistic
    definition as to what subwords actually are, but they roughly correspond to part
    of words that appear frequently. For example, one way to segment “dishwasher”
    is “dish + wash + er,” although some other segmentation is possible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 子词模型是神经网络针对这个问题的最新发明。在子词模型中，输入文本被分割成一个被称为*子词*的单位，这只是意味着比单词小的东西。对于什么是子词，没有正式的语言学定义，但它们大致对应于频繁出现的单词的一部分。例如，“dishwasher”的一种分段方法是“dish
    + wash + er”，尽管也可能有其他的分割方法。
- en: Some varieties of algorithms (such as WordPiece[¹](#pgfId-1107258) and SentencePiece[²](#pgfId-1107261))
    tokenize input into subwords, but by far the most widely used is *byte-pair encoding*
    (BPE).[³](#pgfId-1107267) BPE was originally invented as a compression algorithm,[⁴](#pgfId-1107270)
    but since 2016, it’s been widely used as a tokenization method for neural models,
    particularly in machine translation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法的变体（如 WordPiece[¹](#pgfId-1107258) 和 SentencePiece[²](#pgfId-1107261)）将输入标记化为子词，但迄今为止最广泛使用的是*字节对编码*（BPE）。[³](#pgfId-1107267)
    BPE 最初是作为一种压缩算法发明的，但自 2016 年以来，它已被广泛用作神经模型的标记化方法，特别是在机器翻译中。
- en: The basic concept of BPE is to keep frequent words (such as “the” and “you”)
    and n-grams (such as “-able” and “anti-”) unsegmented, while breaking up rarer
    words (such as “dishwasher”) into subwords (“dish + wash + er”). Keeping frequent
    words and n-grams together helps the model process those tokens efficiently, whereas
    breaking up rare words ensures there are no UNK tokens, because everything can
    be ultimately broken up into individual characters, if necessary. By flexibly
    choosing where to tokenize based on the frequency, BPE achieves the best of two
    worlds—being efficient while addressing the unknown word problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 的基本概念是保持频繁单词（如“the”和“you”）和 n 元组（如“-able”和“anti-”）不分段，同时将较少出现的单词（如“dishwasher”）分解为子词（“dish
    + wash + er”）。将频繁单词和 n 元组放在一起有助于模型高效处理这些标记，而分解稀有单词可以确保没有 UNK 标记，因为一切都最终可以分解为单个字符，如果需要的话。通过根据频率灵活选择标记位置，BPE
    实现了两全其美——既高效又解决了未知词问题。
- en: Let’s see how BPE determines where to tokenize with real examples. BPE is a
    purely statistical algorithm (it doesn’t use any language-dependent information)
    and operates by merging the most frequently occurring pair of consecutive tokens,
    one at a time. First, BPE tokenizes all the input texts into individual characters.
    For example, if your input is four words low, lowest, newer, and wider, it will
    tokenize them into l o w _, l o w e s t _, n e w e r _, w i d e r _. Here, “_”
    is a special symbol that indicates the end of each word. Then, the algorithm identifies
    any two consecutive elements that appear most often. In this example, the pair
    l o appears most often (two times), so these two characters are merged, yielding
    lo w _, lo w e s t _, n e w e r _, w i d e r _. Then, lo w will be merged into
    low, e r into er, er _ into er_, at which time you have low _, low e s t _, n
    e w er_, w i d er_. This process is illustrated in figure 10.6.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 BPE 如何确定在真实示例中进行标记化。BPE 是一种纯统计算法（不使用任何语言相关信息），通过一次合并最频繁出现的一对连续标记来操作。首先，BPE
    将所有输入文本标记化为单个字符。例如，如果您的输入是四个单词 low、lowest、newer 和 wider，则它们将被标记化为 l o w _、l o
    w e s t _、n e w e r _ 和 w i d e r _。在这里，“_”是一个特殊符号，表示每个单词的结尾。然后，算法识别出最频繁出现的任意两个连续元素。在这个例子中，对
    l o 出现最频繁（两次），所以这两个字符被合并，得到 lo w _、lo w e s t _、n e w e r _、w i d e r _。然后，lo
    w 将被合并为 low，e r 将被合并为 er，er _ 将被合并为 er_，此时您有 low _、low e s t _、n e w er_、w i d
    er_。此过程在图 10.6 中有所说明。
- en: '![CH10_F06_Hagiwara](../Images/CH10_F06_Hagiwara.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F06_Hagiwara](../Images/CH10_F06_Hagiwara.png)'
- en: Figure 10.6 BPE learns subword units by iteratively merging consecutive units
    that cooccur frequently.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 BPE 通过迭代地合并频繁出现的连续单元来学习子词单元。
- en: Notice that, after four merge operations, lowest is segmented into low e s t
    where frequent substrings such as low are merged together whereas infrequent ones
    such as est are broken apart. To segment a new input (e.g., lower), the same sequence
    of merge operations is applied in order, yielding low e r _. If you start from
    52 unique letters (26 upper- and lowercase letters), you will end up with 52 +
    N unique tokens in your vocabulary, where N is the number of merge operations
    executed. In this way, you have complete control over the size of the vocabulary.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在四次合并操作之后，lowest 被分割为 low e s t，其中频繁出现的子字符串（如 low）被合并在一起，而不频繁出现的子字符串（如 est）被拆分开来。要对新输入（例如
    lower）进行分割，将按顺序应用相同的合并操作序列，得到 low e r _。如果您从 52 个唯一字母（26 个大写字母和小写字母）开始，执行了 N 次合并操作，则您的词汇表中将有
    52 + N 个唯一标记，其中 N 是执行的合并操作数。通过这种方式，您完全控制了词汇表的大小。
- en: In practice, you rarely need to implement BPE (or any other subword tokenization
    algorithms) yourself. These algorithms are implemented in many open source libraries
    and platforms. Two popular options are Subword-NMT ([https://github.com/rsennrich/subword-nmt](https://github.com/rsennrich/subword-nmt))
    and SentencePiece ([https://github.com/google/sentencepiece](https://github.com/google/sentencepiece))
    (which also supports a variant of subword tokenization using a unigram language
    model). Many of the default tokenizers shipped with NLP frameworks, such as the
    one implemented in Hugging Face Transformers, support subword tokenization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你很少需要自己实现BPE（或任何其他子词标记化算法）。这些算法在许多开源库和平台上都有实现。两个流行的选择是Subword-NMT（[https://github.com/rsennrich/subword-nmt](https://github.com/rsennrich/subword-nmt)）和SentencePiece（[https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)）（它还支持使用unigram语言模型的子词标记化变体）。许多NLP框架中附带的默认标记器，比如Hugging
    Face Transformers中实现的标记器，都支持子词标记化。
- en: 10.3 Avoiding overfitting
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 避免过拟合
- en: '*Overfitting* is one of the most common and important issues you need to address
    when building any machine learning applications. An ML model is said to overfit
    when it fits the given data so well that it loses its generalization ability to
    unseen data. In other words, the model may capture the training data very well
    and show good performance on it, but it may not be able to capture its inherent
    patterns well and shows poor performance on data that the model has never seen
    before.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*过拟合*是构建任何机器学习应用时需要解决的最常见和最重要的问题之一。当一个机器学习模型拟合给定数据得非常好，以至于失去了对未见数据的泛化能力时，就说该模型过拟合了。换句话说，模型可能在训练数据上表现得非常好，并且在它上面表现良好，但是可能无法很好地捕捉其固有模式，并且在模型从未见过的数据上表现不佳。'
- en: Because overfitting is so prevalent in machine learning, researchers and practitioners
    have come up with a number of algorithms and techniques to combat overfitting
    in the past. In this section, we’ll learn two such techniques—regularization and
    early stopping. These are popular in any ML applications (not just NLP) and worth
    getting under your belt.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因为过拟合在机器学习中非常普遍，研究人员和实践者过去已经提出了许多算法和技术来应对过拟合。在本节中，我们将学习两种这样的技术——正则化和提前停止。这些技术在任何机器学习应用中都很受欢迎（不仅仅是自然语言处理），值得掌握。
- en: 10.3.1 Regularization
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 正则化
- en: '*Regularization* in machine learning refers to techniques that encourage the
    simplicity and the generalization of the model. You can think of it as one form
    of penalty you'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化*在机器学习中指的是鼓励模型的简化和泛化的技术。你可以把它看作是一种惩罚形式之一，你'
- en: impose on your ML model to ensure that it is as generic as possible. What does
    it mean? Say you are building an “animal classifier” by training word embeddings
    from a corpus and by drawing a line between animals and other stuff in this embedding
    space (i.e., you represent each word as a multidimensional vector and classify
    whether the word describes an animal based on the coordinates of the vector).
    Let’s simplify this problem a lot and assume that each word is a two-dimensional
    vector, and you end up with the plot shown in figure 10.7\. You can now visualize
    how a machine learning model makes a classification decision by drawing lines
    where the decision flips between different classes (animals and non-animals),
    which is called the *classification boundary*. How would you draw a classification
    boundary so that animals (blue circles) are separated from everything else (triangles)?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 强加给你的机器学习模型以确保其尽可能通用。这是什么意思呢？假设你正在构建一个“动物分类器”，通过从语料库中训练词嵌入并在这个嵌入空间中为动物和其他东西之间划分一条线（即，你将每个单词表示为一个多维向量，并根据向量的坐标对单词是否描述动物进行分类）。让我们大大简化这个问题，假设每个单词都是一个二维向量，并且你得到了图10.7所示的图。现在你可以可视化一个机器学习模型如何通过在决策翻转不同类别之间的线来做出分类决策，这被称为*分类边界*。你会如何绘制一个分类边界，以便将动物（蓝色圆圈）与其他所有东西（三角形）分开？
- en: '![CH10_F07_Hagiwara](../Images/CH10_F07_Hagiwara.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F07_Hagiwara](../Images/CH10_F07_Hagiwara.png)'
- en: Figure 10.7 Animal vs. non-animal classification plot
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 动物 vs. 非动物分类图
- en: One simple way to separate animals is to draw one straight line, as in the first
    plot in figure 10.8\. This simple classifier makes several mistakes (in classifying
    words like “hot” and “bat”), but it correctly classifies the majority of data
    points. This sounds like a good start.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 分离动物的一个简单方法是绘制一条直线，就像图10.8中的第一个图中所示。这个简单的分类器会犯一些错误（在分类诸如“hot”和“bat”之类的单词时），但是它正确分类了大多数数据点。这听起来是一个不错的开始。
- en: '![CH10_F08_Hagiwara](../Images/CH10_F08_Hagiwara.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F08_Hagiwara](../Images/CH10_F08_Hagiwara.png)'
- en: Figure 10.8 Classification boundaries with increasing complexity
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 随着复杂性增加的分类边界
- en: What if you are told that the decision boundary doesn’t have to be a straight
    line? You may want to draw something like the one shown in the middle in figure
    10.8\. This one looks better—it makes fewer mistakes than the first one, although
    it is still not perfect. It appears tractable for a machine learning model because
    the shape is simple.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果告诉你决策边界不一定是一条直线呢？你可能想画出图10.8中间所示的那样的东西。这个看起来更好一些——它比第一个少犯一些错误，虽然仍然不完美。对于机器学习模型来说，这似乎是可行的，因为形状很简单。
- en: But there’s nothing that stops you here. If you want to make as few errors as
    possible, you can also draw something wiggly like the one shown in the third plot.
    That decision boundary doesn’t even make any classification errors, which means
    that we achieved 100% classification accuracy!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这里没有什么可以阻止你。如果你想要尽可能少地犯错误，你也可以画出像第三个图中所示的那样扭曲的东西。那个决策边界甚至不会犯任何分类错误，这意味着我们实现了100%的分类准确性！
- en: Not so fast—remember that up until here, we’ve been thinking only about the
    training time, but the main purpose of machine learning models is to achieve good
    classification performance *at the test time* (i.e., they need to classify unobserved,
    new instances as correctly as possible). Now let’s think about how the three decision
    boundaries described earlier fare at test time. If we assume the test instances
    are distributed similarly to the training instances we saw in figure 10.8, the
    new “animal” points are most likely to fall in the upper-right region of the plot.
    The first two decision boundaries will achieve decent accuracy by classifying
    the majority of new instances correctly. But how about the third one? Training
    instances such as “hot” shown in the plot are most likely exceptions rather than
    the norm, so the curved sections of the decision boundary that tried to accommodate
    as many training instances as possible may do more harm than good at the test
    time by inadvertently misclassifying test instances. This is exactly what overfitting
    looks like—the model fits the training data so well that it sacrifices its generalization
    ability, which is what’s happening here.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 不要那么快——记住，直到现在，我们只考虑了训练时间，但是机器学习模型的主要目的是在*测试时间*达到良好的分类性能（即，它们需要尽可能正确地分类未观察到的新实例）。现在让我们想一想前面描述的三个决策边界在测试时间表现如何。如果我们假设测试实例的分布与我们在图10.8中看到的训练实例类似，那么新的“动物”点最有可能落在图中的右上区域。前两个决策边界将通过正确分类大多数新实例而实现相当的准确度。但是第三个呢？像图中显示的“热”的训练实例最有可能是例外而不是规则，因此试图适应尽可能多的训练实例的决策边界的曲线部分可能会在测试时间通过无意中错误分类测试实例时带来更多的伤害。这正是过拟合的样子——模型对训练数据拟合得太好，牺牲了其泛化能力，这就是这里发生的事情。
- en: Then, the question is, how can we avoid having your model look like the third
    decision boundary? After all, it is doing a very good job correctly classifying
    the training data. If you looked only at the training accuracy and/or the loss,
    there would be nothing to stop you from choosing it. One way to avoid overfitting
    is to use a separate, held-out dataset (called a *validation set*; see section
    2.2.3) to validate the performance of your model. But can we do this even without
    using a separate dataset?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，问题来了，我们如何避免你的模型看起来像第三个决策边界？毕竟，它在正确分类训练数据方面做得非常好。如果你只看训练准确度和/或损失，那么没有什么能阻止你选择它。避免过拟合的一种方法是使用一个单独的、保留的数据集（称为*验证集*；参见2.2.3节）来验证模型的性能。但是即使不使用单独的数据集，我们能做到吗？
- en: The third decision boundary just doesn’t look right—it’s overly complex. With
    all other things being equal, we should prefer simpler models, because in general,
    simpler models generalize better. This is also in line with Occam’s razor, which
    states that a simpler solution is preferable to a more complex one. How can we
    balance between the training fit and the simplicity of the model?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个决策边界看起来不对劲——它过于复杂。在其他所有条件相同的情况下，我们应该更喜欢简单的模型，因为一般来说，简单的模型更容易泛化。这也符合奥卡姆剃刀原理，即更简单的解决方案优于更复杂的解决方案。我们如何在训练拟合和模型简单性之间取得平衡呢？
- en: This is where regularization comes into play. Think of regularization as additional
    constraints imposed on the model so that simpler and/or more general models are
    preferred. The model is optimized so that it achieves the best training fit while
    being as generic as possible.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是正则化发挥作用的地方。将正则化视为对模型施加的额外限制，以便优选更简单和/或更一般化的模型。该模型被优化，使其能够在获得最佳训练拟合的同时尽可能一般化。
- en: Numerous regularization techniques have been proposed in machine learning because
    overfitting is such an important topic. We are going to introduce only a few of
    the most important ones—L2 regularization (weight decay), dropout, and early stopping.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过拟合是如此重要的话题，因此机器学习中已经提出了许多正则化技术。我们只介绍其中几个最重要的——L2正则化（权重衰减），dropout和提前停止。
- en: L2 Regularization
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化
- en: L2 regularization, also called *weight decay*, is one of the most common regularization
    methods not just for NLP or deep learning but for a wide range of ML models. We
    are not going into its mathematical details, but in short, L2 regularization adds
    a penalty for the complexity of a model measured by how large its parameters are.
    To represent a complex classification boundary, an ML model needs to adjust a
    large number of parameters (the “magic constants”) to extreme values, measured
    by the L2 loss, which captures how far away they are from zero. Such models incur
    a larger L2 penalty, which is why L2 encourages simpler models. If you are interested
    in learning more about L2 regularization (and other related topics about NLP in
    general), check out textbooks such as *Speech and Language Processing* by Jurafsky
    and Martin ([https://web.stanford .edu/~jurafsky/slp3/5.pdf](https://web.stanford.edu/~jurafsky/slp3/5.pdf))
    or Goodfellow et al.’s *Deep Learning* ([https://www.deep learningbook.org/contents/regularization.html](https://www.deeplearningbook.org/contents/regularization.html)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化，也称为*权重衰减*，是不仅用于NLP或深度学习，而且用于广泛的ML模型的最常见的正则化方法之一。我们不会深入探讨它的数学细节，但简单来说，L2正则化为模型的复杂度增加了惩罚，这个复杂度是通过其参数的大小来测量的。为了表示复杂的分类边界，ML模型需要调整大量参数（“魔术常数”）到极端值，这由L2
    loss来衡量，其捕获了它们距离零有多远。这样的模型会承担更大的L2惩罚，这就是为什么L2鼓励更简单的模型。如果你想了解更多关于L2正则化（以及NLP一般的其他相关主题），请查阅类似Jurafsky和Martin的*Speech
    and Language Processing*([https://web.stanford .edu/~jurafsky/slp3/5.pdf](https://web.stanford.edu/~jurafsky/slp3/5.pdf))或Goodfellow等人的*Deep
    Learning*([https://www.deep learningbook.org/contents/regularization.html](https://www.deeplearningbook.org/contents/regularization.html))的教材。
- en: Dropout
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout
- en: '*Dropout* is another popular regularization technique commonly used with neural
    networks. Dropout works by randomly “dropping” neurons during training, where
    a “neuron” is basically a dimension of an intermediate layer and “dropping” means
    to mask it with zeros. You can think of dropout as a penalty to the model’s structural
    complexity and its reliance on particular features and values. As a result, the
    network tries to make the best guess with the remaining smaller number of values,
    which forces it to generalize well. Dropout is easy to implement and effective
    in practice and is used as a default regularization method in many deep learning
    models. For more information on dropout, the regularization chapter of the Goodfellow
    book mentioned earlier provides a good introduction and mathematical details of
    regularization techniques.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout*是另一种常用于神经网络的正则化技术。Dropout通过在训练期间随机“放弃”神经元来工作，其中“神经元”基本上是中间层的一个维度，“放弃”意味着用零掩盖它。你可以将dropout视为对模型结构复杂性的惩罚以及对特定特征和值的依赖性。因此，网络试图通过剩余数量较少的值做出最佳猜测，这迫使它良好地泛化。Dropout易于实现，在实践中非常有效，并且在许多深度学习模型中作为默认正则化方法使用。有关dropout的更多信息，请参考Goodfellow书中提到的正则化章节，其中详细介绍了正则化技术的数学细节。'
- en: 10.3.2 Early stopping
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 提前停止
- en: Another popular approach for combatting overfitting in machine learning is *early
    stopping*. Early stopping is a relatively simple technique where you stop training
    your model when the model performance stops improving, usually measured by the
    validation set loss. In chapter 6, we plotted learning curves when we built an
    English-Spanish machine translation model (shown again in figure 10.9). Notice
    that the validation loss curve flattens out around the eighth epoch and starts
    to creep up after that, which is a sign of overfitting. Early stopping would detect
    this, stop the training, and use the result from the best epoch when the loss
    is lowest. In general, early stopping has a “patience” parameter, which is the
    number of nonimproving epochs for early stopping to kick in. When patience is
    10 epochs, for example, the training pipeline will wait 10 epochs after the loss
    stops improving to stop the training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在机器学习中应对过拟合的流行方法是*提前停止*。提前停止是一种相对简单的技术，当模型性能不再改善时（通常使用验证集损失来衡量），停止训练模型。在第
    6 章中，我们绘制了学习曲线，当我们构建英西机器翻译模型（在图 10.9 中再次显示）时。请注意，验证损失曲线在第八个时期左右变平，在此之后开始上升，这是过拟合的迹象。提前停止会检测到这一点，停止训练，并使用损失最低的最佳时期的结果。一般来说，提前停止具有“耐心”参数，该参数是停止训练的非改善时期的数量。例如，当耐心是
    10 个时期时，训练流程将在损失停止改善后等待 10 个时期才终止训练。
- en: '![CH10_F09_Hagiwara](../Images/CH10_F09_Hagiwara.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F09_Hagiwara](../Images/CH10_F09_Hagiwara.png)'
- en: Figure 10.9 The validation loss curve flattens out around the eighth epoch and
    creeps back up.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 验证损失曲线在第 8 个时期左右变平，并逐渐上升。
- en: Why does early stopping help mitigate overfitting? What does it have to do with
    model complexity? Without getting into mathematical details, it takes some time
    (training epochs) for the model to learn complex, overfitted decision boundaries.
    Most models start from something simple (e.g., straight decision lines) and gradually
    increase their complexity over the course of training. By stopping the training
    early, early stopping can prevent the model from becoming overly complex.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么提前停止有助于减轻过拟合？它与模型复杂度有什么关系？不涉及数学细节，让模型学习复杂的、过拟合的决策边界需要一定的时间（训练时期）。大多数模型从一些简单的东西开始（例如直接的决策线）并逐渐在训练过程中增加其复杂性。通过提前停止训练，可以防止模型变得过于复杂。
- en: 'Many machine learning frameworks have built-in support for early stopping.
    For example, AllenNLP’s trainer supports early stopping by default. Recall that
    we used the following configuration in section 9.5.3 when we trained a BERT-based
    natural language inference model, where we used early stopping (with patience
    = 10) without paying much attention. This allows the trainer to stop if the validation
    metric doesn’t improve for 10 epochs:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习框架都内置了提前停止的支持。例如，AllenNLP 的训练器默认支持提前停止。回忆一下，当我们训练基于 BERT 的自然语言推理模型时，在第
    9.5.3 节使用了以下配置，其中我们使用了提前停止（耐心为 10）而没有过多关注。这使得训练器能够在验证指标在 10 个时期内没有改善时停止：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 10.3.3 Cross-validation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 交叉验证
- en: '*Cross-validation* is not exactly a regularization method, but it is one of
    the techniques commonly used in machine learning. A common situation in building
    and validating a machine learning model is this—you have only a couple of hundred
    instances available for training. As we’ve seen so far in this book, you can’t
    train a reliable ML model just on the training set—you need a separate set for
    validation, and preferably another separate set for testing. How much you use
    for validation/testing depends on the task and the data size, but in general,
    it is advised that you set aside 5-20% of your training instances for validation
    and testing. This means that if your training data is small, your model is validated
    and tested on just a few dozen instances, which can make the estimated metrics
    unstable. Also, how you choose these instances has a large impact on the evaluation
    metrics, which is not ideal.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*交叉验证* 不完全是一种正则化方法，但它是机器学习中常用的技术之一。在构建和验证机器学习模型时，通常情况是只有数百个实例可供训练。正如本书迄今所见，仅依靠训练集是无法训练出可靠的机器学习模型的——您需要一个单独的集合用于验证，最好再有一个单独的集合用于测试。您在验证/测试中使用的比例取决于任务和数据大小，但通常建议将
    5-20% 的训练实例留作验证和测试。这意味着，如果您的训练数据较少，那么您的模型将只有几十个实例用于验证和测试，这可能会使估算的指标不稳定。此外，您选择这些实例的方式对评估指标有很大的影响，这并不理想。'
- en: The basic idea of cross-validation is to iterate this phase (splitting the dataset
    into training and validation portions) multiple times with different splits to
    improve the stability of the result. Specifically, in a typical setting called
    *k-fold cross validation*, you first split the dataset into *k* different portions
    of equal size called *folds*. You use one of the folds for validation while training
    the model on the rest (*k* - 1 folds), and repeat this process *k* times, using
    a different fold for validation every time. See figure 10.10 for an illustration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的基本思想是多次迭代这个阶段（将数据集分成训练和验证部分），使用不同的划分方式来提高结果的稳定性。具体来说，在一个典型的称为*k折交叉验证*的设置中，您首先将数据集分成*k*个不同的相等大小的部分，称为*折叠*。您使用折叠中的一个进行验证，同时在其余部分（*k*
    - 1个折叠）上训练模型，并重复此过程*k*次，每次使用不同的折叠进行验证。详见图10.10的示意图。
- en: '![CH10_F10_Hagiwara](../Images/CH10_F10_Hagiwara.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F10_Hagiwara](../Images/CH10_F10_Hagiwara.png)'
- en: Figure 10.10 In k-fold cross validation, the dataset is split into k equally
    sized folds and one is used for validation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：k折交叉验证中，数据集被分为k个大小相等的折叠，其中一个用于验证。
- en: The validation metrics are computed for every fold, and the final metrics are
    averaged over all iterations. This way, you can obtain a more stable estimate
    of the evaluation metrics that are not impacted heavily by the way the dataset
    is split.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每个折叠的验证指标都会计算，并且最终指标会在所有迭代中取平均。通过这种方式，您可以得到一个对评估指标的更稳定的估计，而不受数据集划分方式的影响。
- en: The use of cross-validation in deep learning models is not common, because these
    models require a large amount of data, and you don’t need cross-validation if
    you have a large dataset, although its use is more common for more traditional
    and industrial settings where the amount of training data is limited.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型中，使用交叉验证并不常见，因为这些模型需要大量数据，如果您有大型数据集，则不需要交叉验证，尽管在传统和工业场景中，训练数据量有限时使用交叉验证更为常见。
- en: 10.4 Dealing with imbalanced datasets
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 处理不平衡数据集
- en: In this section, we’ll focus on one of the most common problems you may encounter
    in building NLP and ML models—the class imbalance problem. The goal of a classification
    task is to assign one of the classes (e.g., spam or nonspam) to each instance
    (e.g., an email), but these classes are rarely distributed evenly. For example,
    in spam filtering, the number of nonspam emails is usually larger than the number
    of spam emails. In
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论在构建自然语言处理（NLP）和机器学习（ML）模型时可能遇到的最常见问题之一——类别不平衡问题。分类任务的目标是将每个实例（例如电子邮件）分配给其中一个类别（例如垃圾邮件或非垃圾邮件），但这些类别很少均匀分布。例如，在垃圾邮件过滤中，非垃圾邮件的数量通常大于垃圾邮件的数量。在
- en: document classification, some topics (such as politics or sports) are usually
    more popular than other topics. Classes are said to be imbalanced when some classes
    have way more instances than others (see figure 10.11 for an example).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分类中，某些主题（如政治或体育）通常要比其他主题更受欢迎。当某些类别的实例数量远远多于其他类别时，类别被称为不平衡（见图10.11中的示例）。
- en: '![CH10_F11_Hagiwara](../Images/CH10_F11_Hagiwara.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F11_Hagiwara](../Images/CH10_F11_Hagiwara.png)'
- en: Figure 10.11 Imbalanced dataset
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：不平衡数据集
- en: Many classification datasets have imbalanced classes, which poses some additional
    challenges when you train your classifier. The signals your model gets from smaller
    classes are overwhelmed by larger classes, which causes your model to perform
    poorly on minority classes. In the following subsections, I’m going to discuss
    some techniques you can consider when faced with an imbalanced dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分类数据集存在不平衡的类别，这在训练分类器时会带来一些额外的挑战。小类别给模型带来的信号会被大类别压倒，导致模型在少数类别上表现不佳。在接下来的小节中，我将讨论一些在面对不平衡数据集时可以考虑的技术。
- en: 10.4.1 Using appropriate evaluation metrics
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 使用适当的评估指标
- en: Before you even begin tweaking your dataset or your model, make sure you are
    validating your model with an appropriate metric. In section 4.3, we discussed
    why it is a bad idea to use accuracy as your evaluation metric when the dataset
    is imbalanced. In one extreme case, if 90% of your instances belong to class A
    and the other 10% belong to class B, even a stupid classifier that assigns class
    A to everything can achieve 90% accuracy. This is called a *majority class baseline*.
    A slightly more clever (but still stupid) classifier that randomly assigns label
    A 90% of the time and label B 10% of the time without even looking at the instance
    will achieve 0.9 * 0.9 + 0.1 * 0.1 = 82% accuracy. This is called a *random baseline*,
    and the more imbalanced your dataset is, the higher the accuracy of these baseline
    models will become.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在您甚至开始调整数据集或模型之前，请确保您正在使用适当的指标验证您的模型。在第4.3节中，我们讨论了在数据集不平衡时使用准确性作为评估指标是一个坏主意的原因。在一个极端情况下，如果您的实例中有90%属于类别A，而其他10%属于类别B，即使一个愚蠢的分类器将类别A分配给一切，它也可以达到90%的准确性。这被称为*多数类基线*。稍微聪明一点（但仍然愚蠢）的分类器，90%的时间随机分配标签A，10%的时间随机分配标签B，甚至不看实例，就可以达到0.9
    * 0.9 + 0.1 * 0.1 = 82%的准确性。这被称为*随机基线*，而数据集越不平衡，这些基线模型的准确性就会越高。
- en: But this kind of random baseline is rarely a good model for minority classes.
    Imagine what would happen to class B if you used the random baseline. Because
    it will assign class A 90% of the time no matter what, 90% of the instances belonging
    to class B will be assigned class A. In other words, the accuracy of this random
    baseline for class B is only 10%. If this was a spam filter, it would let 90%
    of spam emails go through, no matter what the content is, just because 90% of
    emails you receive are not spam! This would make a terrible spam filter.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这种随机基线很少是少数类的良好模型。想象一下，如果您使用随机基线会发生什么事情。因为无论如何，它都会将类别A分配给90%的时间，类别B会发生什么情况。换句话说，属于类别B的90%实例将被分配给类别A。换句话说，这种类别B的随机基线的准确性只有10%。如果这是一个垃圾邮件过滤器，它将让90%的垃圾邮件通过，无论内容是什么，只是因为您收到的邮件中有90%不是垃圾邮件！这会造成一个糟糕的垃圾邮件过滤器。
- en: If your dataset is imbalanced and you care about the classification performance
    on the minority class, you should consider using metrics that are more appropriate
    for such settings. For example, if your task is a “needle in a haystack” type
    of setting, where the goal is to find a very small number of instances among others,
    you may want to use the F1-measure instead of accuracy. As we saw in chapter 4,
    the F-measure is some sort of average between precision (how hay-free your prediction
    is) and recall (how much of the needle you actually found). Because the F1-measure
    is calculated per class, it does not underrepresent minority classes. If you’d
    like to measure the model’s overall performance including majority classes, you
    can compute the macro-averaged F-measure, which is simply an arithmetic average
    of F-measures computed per class.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集不平衡，并且您关心少数类别的分类性能，您应该考虑使用更适合这种情况的指标。例如，如果您的任务是“大海捞针”类型的设置，在这种情况下，目标是在其他实例中找到很少的实例，您可能希望使用F1度量而不是准确性。正如我们在第4章中看到的，F度量是精确度（您的预测有多少是无草的）和召回率（您实际上找到了多少针）之间的某种平均值。因为F1度量是每个类别计算的，所以它不会低估少数类别。如果您想要测量模型的整体性能，包括多数类别，您可以计算宏平均的F度量，它只是每个类别计算的F度量的算术平均值。
- en: 10.4.2 Upsampling and downsampling
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 上采样和下采样
- en: Now let’s look at concrete techniques that can mitigate the class imbalance
    problem. First of all, if you can collect more labeled training data, you should
    seriously consider doing that first. Unlike academic and ML competition settings
    where the dataset is fixed while you tweak your model, in a real-world setting
    you are free to do whatever is necessary to improve your model (of course, as
    long as it’s lawful and practical). Often, the best thing you can do to improve
    a model’s generalization is expose it to more data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看可以缓解类别不平衡问题的具体技术。首先，如果您可以收集更多的标记训练数据，您应该认真考虑首先这样做。与学术和机器学习竞赛设置不同，在这种设置中数据集是固定的，而您调整您的模型，而在现实世界中，您可以自由地做任何必要的事情来改进您的模型（当然，只要合法且实用）。通常，您可以做的最好的事情是让模型暴露于更多的数据。
- en: If your dataset is imbalanced and the model is making biased predictions, you
    can either *upsample* or *downsample* your data so that classes have roughly equal
    representations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集不平衡且模型正在做出偏向的预测，您可以对数据进行*上采样*或*下采样*，以便各类别具有大致相等的表示。
- en: In upsampling (see the second figure in figure 10.12), you artificially increase
    the size of the minority class by copying the instances multiple times. Take the
    scenario we discussed earlier for example—if you duplicate the instances of class
    B and add eight extra copies of each instance to the dataset, they have an equal
    number of instances. This can mitigate the biased prediction issue. More sophisticated
    data augmentation algorithms such as SMOTE[⁵](#pgfId-1107425) are available, although
    they are not widely used in NLP, due to the inherent difficulty in generating
    linguistic examples artificially.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在上采样中（参见图10.12中的第二张图），你通过多次复制实例人工增加少数类的大小。例如，我们之前讨论的场景——如果你复制类B的实例并将每个实例的副本增加八个，它们就会有相等数量的实例。这可以缓解偏见预测的问题。尽管有更复杂的数据增强算法，如SMOTE[⁵](#pgfId-1107425)，但它们在自然语言处理中并不常用，因为人为生成语言示例固有的困难。
- en: '![CH10_F12_Hagiwara](../Images/CH10_F12_Hagiwara.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F12_Hagiwara](../Images/CH10_F12_Hagiwara.png)'
- en: Figure 10.12 Upsampling and downsampling
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 上采样和下采样
- en: If your model is biased not because the minority class is too small but because
    the majority class is too large, you can instead choose to downsample (the third
    figure in figure 10.12). In downsampling, you artificially decrease the size of
    the majority class by choosing a subset of the instances belonging to that class.
    For example, if you sample one out of nine instances from class A, you’ll end
    up with the equal number of instances in classes A and B. You can downsample in
    multiple ways—the easiest is to randomly choose the subset. If you would like
    to make sure that the downsampled dataset still preserves the diversity in the
    original data, you can try *stratified sampling*, where you sample some number
    of instances per group defined by some attributes. For example, if you have too
    many nonspam emails and want to downsample, you can group them by the sender’s
    domain first, then sample a fixed number of emails per domain. This will ensure
    that your sampled dataset will contain a diverse set of domains.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型存在偏见，不是因为少数类太小，而是因为多数类太大，你可以选择进行下采样（图10.12中的第三张图）。在下采样中，你通过选择属于该类的实例的子集人工减少多数类的大小。例如，如果你从类A中随机抽取了九个实例中的一个，你最终会得到类A和类B中相等数量的实例。你可以以多种方式进行下采样——最简单的是随机选择子集。如果你想确保下采样后的数据集仍保留了原始数据的多样性，你可以尝试*分层抽样*，其中你根据某些属性定义的组对实例进行抽样。例如，如果你有太多的非垃圾邮件并想要进行下采样，你可以首先按发件人的域分组，然后在每个域中抽样一定数量的电子邮件。这将确保你的抽样数据集将包含多种域的多样性。
- en: Note that neither upsampling nor downsampling is a magic bullet. If you “correct”
    the distribution of classes too aggressively, you risk making unfair predictions
    for the majority class, if that’s what you care about. Always make sure to check
    your model with a held-out validation set with appropriate evaluation metrics.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，无论是上采样还是下采样都不是灵丹妙药。如果你对类的分布进行了过于激进的“修正”，你会冒着对多数类做出不公平预测的风险，如果这是你关心的话。一定要确保用一个合适的评估指标的验证集检查你的模型。
- en: 10.4.3 Weighting losses
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.3 权重损失
- en: Another approach for mitigating the class imbalance problem is to use weighting
    when computing the loss, instead of making modification to your training data.
    Remember that the loss function is used to measure how “off” the model’s prediction
    for an instance is compared against the ground truth. When you measure how bad
    the model’s prediction is, you can tweak the loss so that it penalizes more when
    the ground truth belongs to the minority class.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解类不平衡问题的另一种方法是在计算损失时使用加权，而不是对训练数据进行修改。请记住，损失函数用于衡量模型对实例的预测与真实情况的“偏离”程度。当你衡量模型的预测有多糟糕时，你可以调整损失，使其在真实情况属于少数类时惩罚更严厉。
- en: Let’s take a look at a concrete example. The binary cross-entropy loss, a common
    loss function used for training a binary classifier, looks like the curve shown
    in figure 10.13, when the correct label is 1\. The *x*-axis is the predicted probability
    of the target class, and the *y*-axis is the amount of loss the prediction will
    incur. When the prediction is perfectly correct (probability = 1), there’s no
    penalty, whereas as the prediction gets worse (probability < 1), the loss goes
    up.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子。二元交叉熵损失是用于训练二元分类器的常见损失函数，当正确标签为 1 时，它看起来像图 10.13 中所示的曲线。 *x* 轴是目标类别的预测概率，*y*
    轴是预测将施加的损失量。当预测完全正确（概率 = 1）时，没有惩罚，而随着预测变得越来越糟糕（概率 < 1），损失增加。
- en: '![CH10_F13_Hagiwara](../Images/CH10_F13_Hagiwara.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F13_Hagiwara](../Images/CH10_F13_Hagiwara.png)'
- en: Figure 10.13 Binary cross-entropy loss (when the correct label is 1)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 二元交叉熵损失（正确标签为 1）
- en: If you care more about the model’s performance on the minority class, you can
    tweak this loss. Specifically, you can change the shape of this loss (by simply
    multiplying it by a constant number) just for that class so that the model incurs
    a larger loss when it makes mistakes on the minority class. One such tweaked loss
    curve is shown in the figure 10.14 as the top curve. This weighting has the same
    effect as upsampling the minority class, although modifying the loss is computationally
    cheaper because you don’t need to actually increase the amount of training data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更关心模型在少数类上的表现，可以调整这个损失。具体而言，您可以更改这个损失的形状（通过简单地将其乘以一个常数），只针对那个类别，以便当模型在少数类上犯错时，它会产生更大的损失。图
    10.14 中的一条调整后的损失曲线就是顶部的那条。这种加权与上采样少数类具有相同的效果，尽管修改损失的计算成本更低，因为您不需要实际增加训练数据量。
- en: '![CH10_F14_Hagiwara](../Images/CH10_F14_Hagiwara.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F14_Hagiwara](../Images/CH10_F14_Hagiwara.png)'
- en: Figure 10.14 Weighted binary cross entropy loss
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 加权二元交叉熵损失
- en: 'It is easy to implement loss weighting in PyTorch and AllenNLP. PyTorch’s binary
    cross-entropy implementation BCEWithLogitLoss already supports different weights
    for different classes. You simply need to pass the weight as the pos_weight parameter
    as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 和 AllenNLP 中实现损失权重很容易。PyTorch 的二元交叉熵实现 BCEWithLogitsLoss 已经支持为不同类别使用不同的权重。您只需要将
    pos_weight 参数作为权重传递，如下所示：
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this code snippet, we randomly generate prediction (input) and the ground
    truth (target). There are three instances in total, two of which are of class
    0 (majority) and one belongs to class 1 (minority). We first compute the loss
    without weighting by calling the BCEWithLogitsLoss object, which returns the three
    loss values, one for each instance. We then compute the loss with weighting by
    passing the weight 2—this means that the wrong prediction will be penalized twice
    as much if the target class is positive (class 1). Notice that the third element
    corresponding to class 1 is twice as large as the one returned by the unweighted
    loss function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码片段中，我们随机生成预测值（input）和真实值（target）。总共有三个实例，其中两个属于类别 0（多数类），一个属于类别 1（少数类）。我们先使用
    BCEWithLogitsLoss 对象计算不加权的损失，这将返回三个损失值，每个实例一个。然后，我们通过传递权重 2 来计算加权损失——这意味着如果目标类别是正类（类别
    1），则错误预测将被惩罚两倍。请注意，对应于类别 1 的第三个元素是非加权损失函数返回值的两倍。
- en: 10.5 Hyperparameter tuning
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 超参数调整
- en: In this final section of this chapter, we’ll discuss hyperparameter tuning.
    *Hyperparameters* are parameters about the model and the training algorithm. This
    term is used in contrast with *parameters*, which are numbers that are used by
    the model to make predictions from the input. This is what we’ve been calling
    “magic constants” throughout this book—they work like constants in programming
    languages, although their exact values are automatically adjusted by optimization
    so that the prediction matches the desired output as closely as possible.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节，我们将讨论超参数调整。*超参数*是有关模型和训练算法的参数。这个术语与*参数*相对，参数是模型用于从输入中作出预测的数字。这就是我们在本书中一直称之为“魔术常数”的内容——它们类似于编程语言中的常数，尽管它们的确切值被优化自动调整，以使预测尽可能接近所需输出。
- en: Correctly tuning hyperparameters is critical for many machine learning models
    to work properly and achieve their highest potential, and ML practitioners spend
    a lot of time tuning hyperparameters. Knowing how to tune hyperparameters effectively
    has a huge impact on your productivity in building NLP and ML systems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正确调整超参数对于许多机器学习模型正常工作并发挥其最高潜力至关重要，机器学习从业者花费大量时间来调整超参数。知道如何有效地调整超参数对于提高在构建自然语言处理和机器学习系统时的生产力有着巨大的影响。
- en: 10.5.1 Examples of hyperparameters
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.1 超参数示例
- en: Hyperparameters are “meta”-level parameters—unlike model parameters, they are
    used not to make predictions but for controlling the structure of the model and
    how the model is trained. For example, if you are working on word embeddings or
    an RNN, how many hidden units (dimensions) to use for representing words is one
    important hyperparameter. The number of RNN layers to use is another hyperparameter.
    In addition to these two hyperparameters (the number of hidden units and layers),
    the Transformer model we covered in chapter 9 has a number of other parameters,
    such as the number of attention heads and the dimension of the feed-forward network.
    Even the type of architecture you use, such as RNN versus Transformer, can be
    thought of as one hyperparameter.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是“元”级别的参数——与模型参数不同，它们不用于进行预测，而是用于控制模型的结构以及模型的训练方式。例如，如果你正在处理词嵌入或者一个 RNN，那么用于表示单词的隐藏单元（维度）的数量就是一个重要的超参数。使用的
    RNN 层数是另一个超参数。除了这两个超参数（隐藏单元和层数）之外，我们在第 9 章中介绍的 Transformer 模型还有一些其他参数，比如注意力头的数量和前馈网络的维度。甚至你使用的架构类型，例如
    RNN 与 Transformer，也可以被视为一个超参数。
- en: Besides, the optimization algorithm you use may have hyperparameters, too. For
    example, the learning rate (section 9.3.3), one of the most important hyperparameters
    in many ML settings, determines how much to tweak the model parameters per optimization
    step. The number of epochs (iterations through the training dataset) is also an
    important hyperparameter, too.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您使用的优化算法也可能有超参数。例如，在许多机器学习设置中最重要的超参数之一——学习率（第 9.3.3 节），确定了每个优化步骤中调整模型参数的程度。迭代次数（通过训练数据集的次数）也是一个重要的超参数。
- en: So far, we have been paying little attention to those hyperparameters, let alone
    optimizing them. However, hyperparameters can have a huge impact on the performance
    of machine learning models. In fact, many ML models have a “sweet spot” of hyperparameters
    that makes them most effective, whereas using a set of hyperparameters outside
    of this spot may make the model perform poorly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对这些超参数几乎没有给予任何关注，更不用说优化它们了。然而，超参数对机器学习模型的性能有着巨大的影响。事实上，许多机器学习模型都有一个“甜蜜点”超参数，使它们最有效，而使用超参数集在这个点之外可能会使模型表现不佳。
- en: Many ML practitioners tune hyperparameters by hand. This means that you start
    from a set of hyperparameters that look reasonable and measure the model’s performance
    on a validation set. Then you change one or more of the hyperparameters slightly
    and measure the performance again. You repeat this process several times until
    you hit the “plateau,” where any change of hyperparameters provides only a marginal
    improvement.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习从业者通过手动调整超参数来调整超参数。这意味着你从一组看起来合理的超参数开始，并在验证集上测量模型的性能。然后，您稍微改变一个或多个超参数，并再次测量性能。您重复这个过程几次，直到达到“高原”，在这里任何超参数的更改都只提供了边际改进。
- en: One issue with this manual tuning approach is that it is slow and arbitrary.
    Let’s say you start from one set of hyperparameters. How do you know which ones
    to adjust next, and how much? How do you know when to stop? If you have experience
    tuning a wide range of ML models, you might have some “hunch” about how these
    models respond to certain hyperparameter changes, but if not, it’s like shooting
    in the dark. Hyperparameter tuning is such an important topic that ML researchers
    have been working on better and more organized ways to optimize them.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种手动调整方法的一个问题是它是缓慢和随意的。假设你从一组超参数开始。你如何知道接下来应该调整哪些参数，以及多少？你如何知道何时停止？如果你有调整广泛的机器学习模型的经验，你可能对这些模型如何响应某些超参数更改有一些“直觉”，但如果没有，那就像在黑暗中射击一样。超参数调整是一个非常重要的主题，机器学习研究人员一直致力于寻找更好和更有组织的方法来优化它们。
- en: 10.5.2 Grid search vs. random search
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.2 网格搜索 vs. 随机搜索
- en: We understand that manual optimization of hyperparameters is inefficient, but
    how should we go about optimizing them, then? We have two more-organized ways
    of tuning hyperparameters—grid search and random search.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们明白手动优化超参数效率低下，但是我们应该如何进行优化呢？我们有两种更有组织的调整超参数的方式——网格搜索和随机搜索。
- en: In *grid search*, you simply try every possible combination of the hyperparameter
    values you want to optimize. For example, let’s assume your model has just two
    hyperparameters—the number of RNN layers and the embedding dimension. You first
    define reasonable ranges for these two hyperparameters, for example, [1, 2, 3]
    for the number of layers and [128, 256, 512] for the dimensionality. Then grid
    search measures the model’s validation performance for every combination—(1, 128),
    (1, 256), (1, 512), (2, 128), . . . , (3, 512)—and simply picks the best-performing
    combination. If you plot these combinations on a 2-D plot, it looks like a grid
    (see the illustration in figure 10.15), which is why this is called *grid search*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在*网格搜索*中，你只需尝试优化的超参数值的每种可能组合。例如，假设你的模型只有两个超参数——RNN层数和嵌入维度。你首先为这两个超参数定义合理的范围，例如，层数为[1,
    2, 3]，维度为[128, 256, 512]。然后，网格搜索会对每种组合进行模型验证性能的测量——(1, 128), (1, 256), (1, 512),
    (2, 128), . . . , (3, 512)——并简单选择表现最佳的组合。如果你将这些组合绘制在二维图上，它看起来像一个网格（见图10.15的示例），这就是为什么称之为*网格搜索*。
- en: Grid search is a simple and intuitive way to optimize the hyperparameters. However,
    if you have many hyperparameters and/or their ranges are large, this method gets
    out of hand. The number of possible combinations is exponential, which makes it
    impossible to explore all of them in a reasonable amount of time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是优化超参数的一种简单直观的方式。然而，如果你有很多超参数和/或它们的范围很大，这种方法就会失控。可能的组合数量是指数级的，这使得在合理的时间内探索所有组合变得不可能。
- en: '![CH10_F15_Hagiwara](../Images/CH10_F15_Hagiwara.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F15_Hagiwara](../Images/CH10_F15_Hagiwara.png)'
- en: Figure 10.15 Grid search vs. random search for hyperparameter tuning. (Adapted
    from Bergstra and Bengio, 2012; [https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf.)](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15 网格搜索与随机搜索的超参数调优比较。（摘自Bergstra和Bengio，2012；[https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf.](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)）
- en: A better alternative to grid search is *random search*. In random search, instead
    of trying every possible combination of hyperparameter values, you randomly sample
    the values and measure the model’s performance on a specified number of combinations
    (which are called *trials*). For example, in the previous example, random search
    may choose (2, 87), (1, 339), (2, 101), (3, 254), and so on until it hits the
    specified number of trials. See the illustration in figure 10.15 (right).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 比*网格搜索*更好的替代方案是*随机搜索*。在随机搜索中，你不是尝试每种可能的超参数值的组合，而是随机抽样这些值，并在指定数量的组合（称为*试验*）上测量模型的性能。例如，在上述示例中，随机搜索可以选择(2,
    87), (1, 339), (2, 101), (3, 254)等，直到达到指定数量的试验为止。请参见图10.15的示例（右侧）。
- en: Unless your hyperparameter search space is very small (like the first example),
    random search is usually recommended over grid search if you want to optimize
    hyperparameters efficiently. Why? In many machine learning settings, not every
    hyper-parameter is made equal—there are usually only a small number of hyperparameters
    that actually matter for the performance, whereas many others do not. Grid search
    will waste a lot of computation searching for the best combination of hyperparameters
    that do not really matter, while being unable to explore the few hyperparameters
    that do matter in detail (figure 10.15, left). On the other hand, random search
    can explore many possible points on the axis that matters for the performance
    (figure 10.15, right). Notice that random search can find a better model by exploring
    more points on the x-axis with the same number of trials (total of nine trials).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你的超参数搜索空间非常小（就像第一个示例一样），如果你想要高效地优化超参数，通常建议使用随机搜索而不是网格搜索。为什么？在许多机器学习设置中，并非每个超参数都是相等的——通常只有少数几个超参数实际上对性能有影响，而其他许多超参数则不然。网格搜索会浪费大量计算资源来寻找并不真正重要的超参数组合，同时无法详细探索那些真正重要的少数超参数（图10.15，左侧）。另一方面，随机搜索可以在性能重要的轴上探索许多可能的点（图10.15，右侧）。请注意，随机搜索可以通过在相同的试验数量下在x轴上探索更多点来找到更好的模型（总共九个试验）。
- en: 10.5.3 Hyperparameter tuning with Optuna
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.3 使用Optuna进行超参数调优
- en: OK, we’ve covered some ways to tune hyperparameters including manual, grid,
    and random search, but how should you go about implementing it in practice? You
    can always write your own for-loop (or “for-loops,” in the case of grid search),
    although it would quickly get tiring if you need to write this type of boilerplate
    code for every model and task you work on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经介绍了一些调整超参数的方法，包括手动、网格和随机搜索，但是在实践中应该如何实现呢？你可以随时编写自己的for循环（或者在网格搜索的情况下是“for-loops”），尽管如果你需要为每个模型和任务编写这种样板代码，这将很快变得令人厌倦。
- en: Hyperparameter optimization is such a universal topic that many ML researchers
    and engineers have been working on better algorithms and software libraries. For
    example, AllenNLP has its own library called *Allentune* ([https://github.com/allenai/allentune](https://github.com/allenai/allentune))
    that you can easily integrate with your AllenNLP training pipeline. In the remainder
    of this section, however, I’m going to introduce another hyperparameter tuning
    library called *Optuna* ([https://optuna.org/](https://optuna.org/)) and show
    how to use it with AllenNLP to optimize your hyperparameters. Optuna implements
    state-of-the-art algorithms that search for optimal hyperparameters efficiently
    and provides integration with a wide range of machine learning frameworks, including
    TensorFlow, PyTorch, and AllenNLP.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化是一个普遍的主题，许多机器学习研究人员和工程师一直在致力于改进算法和软件库。例如，AllenNLP有自己的库叫做*Allentune*（[https://github.com/allenai/allentune](https://github.com/allenai/allentune)），你可以很容易地将其与AllenNLP的训练流程集成起来。然而，在本节的剩余部分中，我将介绍另一个超参数调整库叫做*Optuna*（[https://optuna.org/](https://optuna.org/)），并展示如何将其与AllenNLP一起使用以优化你的超参数。Optuna实现了最先进的算法，可以高效地搜索最优超参数，并与包括TensorFlow、PyTorch和AllenNLP在内的广泛的机器学习框架集成。
- en: 'First, we assume that you have installed AllenNLP (1.0.0+) and the Optuna plugin
    for AllenNLP. These can be installed by running the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设你已经安装了AllenNLP（1.0.0+）和AllenNLP的Optuna插件。你可以通过运行以下命令来安装它们：
- en: '[PRE6]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, as instructed by the official documentation ([https://github.com/himkt/allennlp
    -optuna](https://github.com/himkt/allennlp-optuna)), you need to register the
    plugin with AllenNLP by running the next code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据官方文档的指示（[https://github.com/himkt/allennlp -optuna](https://github.com/himkt/allennlp-optuna)），你需要运行下面的代码来注册AllenNLP的插件：
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are going to use the LSTM-based classifier for the Stanford Sentiment Treebank
    dataset we built in chapter 2\. You can find the AllenNLP config file in the book
    repository ([http://www.realworldnlpbook.com/ch10.html#config](http://www.realworldnlpbook.com/ch10.html#config)).
    Note that you need to reference the variables (std.extVar) for Optuna to have
    control over the parameters. Specifically, you need to define them at the beginning
    of the config file:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用第2章中构建的基于LSTM的分类器对斯坦福情感树库数据集进行分类。你可以在书的代码库中找到AllenNLP的配置文件（[http://www.realworldnlpbook.com/ch10.html#config](http://www.realworldnlpbook.com/ch10.html#config)）。注意，你需要引用变量（std.extVar）以便Optuna可以控制参数。具体来说，你需要在配置文件的开头定义它们：
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, you need to tell Optuna which parameters to optimize. You can do this
    by writing a JSON file (hparams.json ([http://www.realworldnlpbook.com/ch10.html#
    hparams](http://www.realworldnlpbook.com/ch10.html#hparams))). You need to specify
    every hyperparameter you want Optuna to optimize with its types and ranges as
    follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要告诉Optuna要优化哪些参数。你可以通过编写一个JSON文件（hparams.json ([http://www.realworldnlpbook.com/ch10.html#
    hparams](http://www.realworldnlpbook.com/ch10.html#hparams)）来实现这一点。你需要指定你希望Optuna优化的每个超参数及其类型和范围，如下所示：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, invoke this command to start the optimization:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用这个命令来开始优化：
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that we are running 20 trials (—n-trials) with validation accuracy (—metrics
    best_validation_accuracy) as the metric to maximize (—direction maximize). If
    you do not specify the metric and the direction, by default Optuna tries to minimize
    the validation loss.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们正在运行20次试验（—n-trials），以最大化验证准确性（—metrics best_validation_accuracy）作为度量标准（—direction
    maximize）。如果你没有指定度量标准和方向，Optuna默认尝试最小化验证损失。
- en: 'This will take a while, but after all the trials are finished you will see
    the following one-line summary of the optimization:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要一些时间，但是在所有试验完成后，你将看到以下优化的一行摘要：
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, Optuna supports a wide range of visualization of the optimization
    result, including very nice contour plots ([http://www.realworldnlpbook.com/ch10.html#
    contour)](http://www.realworldnlpbook.com/ch10.html#contour), but here we''ll
    simply use its web-based dashboard to quickly inspect the optimization process.
    All you need to do is invoke its dashboard from the command line as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Optuna 支持广泛的优化结果可视化，包括非常好的等高线图（[http://www.realworldnlpbook.com/ch10.html#
    contour)](http://www.realworldnlpbook.com/ch10.html#contour)，但在这里我们将简单地使用其基于 Web
    的仪表板快速检查优化过程。你只需要按照以下命令从命令行调用其仪表板：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you can access http:/./localhost:5006/dashboard to see the dashboard, shown
    in figure 10.16.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以访问 http:/./localhost:5006/dashboard 来查看仪表板，如图 10.16 所示。
- en: '![CH10_F16_Hagiwara](../Images/CH10_F16_Hagiwara.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F16_Hagiwara](../Images/CH10_F16_Hagiwara.png)'
- en: Figure 10.16 The Optuna dashboard shows the evaluation metrics of the parameters
    for each trial.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 Optuna 仪表板显示了每个试验的参数评估指标。
- en: 'From this dashboard you can quickly see not only that your optimal trial was
    trial #14 but also the optimal hyperparameters at each trial.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个仪表板上，你不仅可以迅速看到最优试验是第 14 次试验，而且可以看到每次试验的最优超参数。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Instances are sorted, padded, and batched together for more efficient computation.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例被排序、填充和批量化以进行更有效的计算。
- en: Subword tokenization algorithms such as BPE split words into units smaller than
    words to mitigate the out-of-vocabulary problem in neural network models.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子单词分词算法（如 BPE）将单词拆分成比单词更小的单元，以减轻神经网络模型中的词汇外问题。
- en: Regularization (such as L2 and dropout) is a technique to encourage model simplicity
    and generalizability in machine learning.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化（如 L2 和 dropout）是一种用于鼓励机器学习中模型简单性和可泛化性的技术。
- en: You can use data upsampling, downsampling, or loss weights for addressing the
    data imbalance issue.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用数据上采样、下采样或损失权重来解决数据不平衡问题。
- en: Hyperparameters are parameters about the model or the training algorithm. They
    can be optimized using manual, grid, or random search. Even better, use hyperparameter
    optimization libraries such as Optuna, which integrates easily with AllenNLP.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数是关于模型或训练算法的参数。可以通过手动、网格或随机搜索进行优化。更好的是，使用超参数优化库，如 Optuna，它与 AllenNLP 集成得很容易。
- en: '^(1.)Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap
    between Human and Machine Translation,” (2016). [https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)Wu 等人，“谷歌神经机器翻译系统：填补人机翻译之间的差距”（2016）。[https://arxiv.org/abs/1609.08144](https://arxiv.org/abs/1609.08144)。
- en: '^(2.)Kudo, “Subword Regularization: Improving Neural Network Translation Models
    with Multiple Subword Candidates,” (2018). [https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)Kudo，“Subword Regularization：使用多个子单词提高神经网络翻译模型”（2018）。[https://arxiv.org/abs/1804.10959](https://arxiv.org/abs/1804.10959)。
- en: ^(3.)Sennrich et al., “Neural Machine Translation of Rare Words with Subword
    Units,” (2016). [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)Sennrich 等人，“使用子单词单元进行稀有词的神经机器翻译”（2016）。[https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)。
- en: ^(4.)See [https://www.derczynski.com/papers/archive/BPE_Gage.pdf](https://www.derczynski.com/papers/archive/BPE_Gage.pdf).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)参见[https://www.derczynski.com/papers/archive/BPE_Gage.pdf](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)。
- en: '^(5.)Chawla et al., “SMOTE: Synthetic Minority Over-Sampling Technique,” (2002).
    [https://arxiv.org/abs/1106.1813](https://arxiv.org/abs/1106.1813).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)Chawla 等人，“SMOTE：合成少数类过采样技术”（2002）。[https://arxiv.org/abs/1106.1813](https://arxiv.org/abs/1106.1813)。
