- en: '6 Teaching machines to see: Image classification with CNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 教机器看图像分类和CNN
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Exploratory data analysis on image data in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中对图像数据进行探索性数据分析
- en: Preprocessing and feeding data via image pipelines
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理和通过图像流水线提供数据
- en: Using the Keras functional API to implement a complex CNN model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras功能API实现复杂的CNN模型
- en: Training and evaluating the CNN model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估CNN模型
- en: We have already done a fair bit of work on CNNs. CNNs are a type of network
    that can operate on two-dimensional data, such as images. CNNs use the convolution
    operation to create feature maps of images (i.e., a grid of pixels) by moving
    a kernel (i.e., a smaller grid of values) over the image to produce new values.
    The CNN has several of these layers that generate more and more high-level feature
    maps as they get deeper. You can also use max or average pooling layers between
    convolutional layers to reduce the dimensionality of the feature maps. The pooling
    layers also move a kernel over feature maps to create the smaller representation
    of the input. The final feature maps are connected to a series of fully connected
    layers, where the final layer produces the prediction (e.g., the probability of
    an image belonging to a certain category).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对CNN做了相当多的工作。CNN是一种可以处理二维数据（如图像）的网络类型。CNN使用卷积操作通过在图像上移动一个核（即一个小的值网格）来创建图像（即像素的网格）的特征图，从而产生新的值。CNN具有多个这样的层，随着它们的深入，它们生成越来越高级的特征图。您还可以在卷积层之间使用最大或平均汇聚层来减少特征图的维数。汇聚层也会在特征图上移动核以创建输入的较小表示。最终的特征图连接到一系列完全连接的层，其中最后一层产生预测结果（例如，图像属于某个类别的概率）。
- en: We have implemented CNN using the Keras Sequential API. We used various Keras
    layers such as Conv2D, MaxPool2D, and Dense to easily implement CNNs. We’ve already
    studied various parameters related to the Conv2D and MaxPool2D layers, such as
    window size, stride, and padding.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Keras Sequential API实现了CNN。我们使用了各种Keras层，如Conv2D、MaxPool2D和Dense，以便轻松地实现CNN。我们已经学习了与Conv2D和MaxPool2D层相关的各种参数，如窗口大小、步幅和填充方式。
- en: In this chapter, we will come a step closer to seeing CNNs performing on real-world
    data to solve an exciting problem. There’s more to machine learning than implementing
    a simple CNN to learn from a highly curated data set, as real-world data is often
    messy. You will be introduced to exploratory data analysis, which is at the heart
    of the machine learning life cycle. You will explore an image data set, where
    the objective is to identify the object present in the image (known as *image
    classification*). We will then extensively study one of the state-of-the-art models
    in computer vision, known as the inception model. In deep learning, there are
    widely recognized neural network architectures (or templates) that perform well
    on a given task. The inception model is one such model that has been shown to
    perform well on image data. We will study the architecture of the model and the
    motivations behind several novel design concepts used in it. Finally, we will
    train the model on the data set we explored and analyze model performance by relying
    on metrics such as accuracy on test data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更接近地看到卷积神经网络（CNN）在解决令人兴奋的问题时在真实世界数据上的表现。机器学习不仅仅是实现一个简单的CNN来学习高度策划的数据集，因为真实世界的数据往往是杂乱无序的。您将学习到探索性数据分析，这是机器学习生命周期的核心。您将探索一个图像数据集，目标是识别图像中的对象（称为*图像分类*）。然后，我们将深入研究计算机视觉领域的一个最先进的模型，即inception模型。在深度学习中，广泛认可的神经网络架构（或模板）在特定任务上表现良好。inception模型是一种在图像数据上表现出色的模型之一。我们将研究模型的架构以及其中使用的几个新颖设计概念的动机。最后，我们将训练在我们探索过的数据集上的模型，并依靠准确性等指标分析模型的性能。
- en: We have come a long way. We understand the technical aspects of the main deep
    learning algorithms out there and can be confident in our ability to perform exploratory
    data analysis correctly and thus enter the model stage with confidence. However,
    deep networks can get very large very quickly. Complex networks drag in all sorts
    of computational and performance problems. So, anyone who wants to use these algorithms
    in real-world problems needs to learn existing models that have proven to perform
    well in complex learning tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们走了很长一段路。我们理解了那里存在的主要深度学习算法的技术方面，并且对我们正确执行探索性数据分析的能力充满信心，因此以信心进入模型阶段。然而，深度网络很快就会变得非常庞大。复杂的网络会牵扯到各种计算和性能问题。因此，任何希望在实际问题中使用这些算法的人都需要学习那些在复杂学习任务中已被证明执行良好的现有模型。
- en: '6.1 Putting the data under the microscope: Exploratory data analysis'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 将数据置于显微镜下：探索性数据分析
- en: You are working with a team of data scientists to build a versatile image classification
    model. The end goal is to use this model as a part of an intelligent shopping
    assistant. The user can upload a photo of the inside of their home, and the assistant
    will find suitable products based on their style. The team decided to start out
    with an image classification model. You need to come back to the group with a
    great data set to start with and to explain what the data looks like and why it
    is great. The data set contains day-to-day objects photographed in the real world,
    and you will do exploratory data analysis and look at various attributes of the
    data set (e.g., available classes, data set sizes, image attributes) to understand
    the data and identify and fix potential issues.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在与一组数据科学家合作构建一个多才多艺的图像分类模型。最终目标是将此模型用作智能购物助手的一部分。用户可以上传家里内部的照片，助手将根据他们的风格找到合适的产品。团队决定从图像分类模型开始。你需要回到团队，拿到一个很棒的数据集并解释数据的样子以及为什么这个数据集很棒。数据集包含在现实世界中拍摄的日常物品，你将进行探索性数据分析并查看数据集的各种属性（例如，可用类别，数据集大小，图像属性）来了解数据，并识别和解决潜在问题。
- en: Exploratory data analysis (EDA) is the cornerstone of the technical development
    that you will do in a data science project. The main objective of this process
    is to get a high-quality clean data set by the end of the process by removing
    pesky problems like outliers and noise. In order to have such a data set, you
    need to scrutinize your data and find out if there are
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）是数据科学项目中你将要做的技术发展的基石。该过程的主要目标是通过消除离群值和噪音等烦人问题，最终获得高质量干净的数据集。为了拥有这样的数据集，你需要仔细审查数据，并找出是否存在
- en: Imbalanced classes (in a classification problem)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别不平衡（在分类问题中）
- en: Corrupted data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损坏的数据
- en: Missing features
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失的特征
- en: Outliers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离群值
- en: Features that require various transformations (e.g., normalization, one-hot
    encoding)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要各种转换的特征（例如，标准化，独热编码）
- en: This is by no means an exhaustive list of things to look out for. The more exploration
    you do, the better the quality of the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝不是一份详尽的需要注意的事项清单。你进行的探索越多，数据质量就会越好。
- en: What comes before EDA?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行探索性数据分析之前发生了什么？
- en: 'A machine learning problem always start with a business problem. Once the problem
    is properly identified and understood, you can start thinking about the data:
    What data do we have? What do we train the model to predict? How do these predictions
    translate to actionable insights that deliver benefits to the company? After you
    tick these boxes off, you can retrieve and start playing with the data by means
    of EDA. After all, every single step in a machine learning project needs to be
    done with a purpose.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题总是源于业务问题。一旦问题得到适当的确认和理解，你可以开始考虑数据：我们有什么数据？我们训练模型来预测什么？这些预测如何转化为为公司带来好处的可操作见解？在勾选这些问题之后，你可以通过探索性数据分析来检索并开始处理数据。毕竟，机器学习项目中的每一步都需要有目的性地完成。
- en: You have already spent days researching and have found a data set that is appropriate
    for your problem. To develop an intelligent shopping assistant that can understand
    customers’ style preferences, it should be able to identify as many household
    items as possible from the photos that customers will upload. For this, you are
    planning to use the tiny-imagenet-200 ([https://www.kaggle.com/c/tiny-imagenet](https://www.kaggle.com/c/tiny-imagenet))
    data set.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经花了几天时间研究，找到了一个适合你问题的数据集。为了开发一个能够理解客户风格偏好的智能购物助手，它应该能够从客户上传的照片中识别尽可能多的家居物品。为此，你计划使用tiny-imagenet-200
    ([https://www.kaggle.com/c/tiny-imagenet](https://www.kaggle.com/c/tiny-imagenet))数据集。
- en: The ImageNet data set
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet数据集
- en: Tiny ImageNet is a smaller-scale remake of the original ImageNet data set ([https://www.kaggle.com/competitions/imagenet-object-localization-challenge](https://www.kaggle.com/competitions/imagenet-object-localization-challenge)),
    which is part of the annual ImageNet Large Scale Visual Recognition Challenge
    (ILSVRC) challenge. Each year, research teams all around the world compete to
    come up with state-of-the-art image classification and detection models. This
    data set has around 1.2 million labeled images spread across 1,000 classes and
    has become one of the largest labeled image data sets available in computer vision.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Tiny ImageNet是原始ImageNet数据集（[https://www.kaggle.com/competitions/imagenet-object-localization-challenge](https://www.kaggle.com/competitions/imagenet-object-localization-challenge)）的一个规模较小的重制版，它是年度ImageNet大规模视觉识别挑战(ILSVRC)的一部分。每年，全球各地的研究团队竞争开发最先进的图像分类和检测模型。这个数据集拥有大约1.2百万张标记的图像，分布在1,000个类别中，已成为计算机视觉领域最大的标记图像数据集之一。
- en: This data set has images belonging to 200 different classes. Figure 6.1 depicts
    images for some of the available classes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含属于200个不同类别的图像。图6.1展示了一些可用类别的图像。
- en: '![06-01](../../OEBPS/Images/06-01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![06-01](../../OEBPS/Images/06-01.png)'
- en: Figure 6.1 Some sample images from the tiny-imagenet-200. You can see that these
    images belong to a wide variety of categories.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 tiny-imagenet-200的一些样本图像。你可以看到这些图像属于各种不同的类别。
- en: 'First things first. We need to download the data set. The following code will
    create a directory called data in your working directory, download the zip file
    containing the data, and extract it for you. Finally, you should have a folder
    called tiny-imagenet-200 in the data folder:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要下载数据集。下面的代码将在你的工作目录中创建一个名为data的文件夹，下载包含数据的zip文件，并为你解压缩。最终，你应该在data文件夹中有一个名为tiny-imagenet-200的文件夹：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 6.1.1 The folder/file structure
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 文件夹/文件结构
- en: The data should now be available in the Ch06/data folder. Now it’s time to explore
    the data set. The first thing we will do is manually explore the data in the folders
    provided to us. You’ll note that there are three folders and two files (figure
    6.2). Look around and explore them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在应该在Ch06/data文件夹中可用了。现在是时候探索数据集了。我们将首先手动浏览提供给我们的文件夹中的数据。你会注意到有三个文件夹和两个文件（图6.2）。四处看看并探索一下。
- en: '![06-02](../../OEBPS/Images/06-02.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![06-02](../../OEBPS/Images/06-02.png)'
- en: Figure 6.2 The folders and files found in the tiny-imagenet-200 data set
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 tiny-imagenet-200数据集中找到的文件夹和文件
- en: The file wnids.txt contains a set of 200 IDs (called *wnids* or WordNet IDs,
    based on the lexical database WordNet [[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)];
    figure 6.3). Each of these IDs represents a single class of images (e.g., class
    of gold fish).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文件wnids.txt包含一组200个ID（称为*wnids*或WordNet IDs，基于词汇数据库WordNet [[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)];
    图6.3)。每个ID代表一个图像类别（例如，金鱼类）。
- en: '![06-03](../../OEBPS/Images/06-03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![06-03](../../OEBPS/Images/06-03.png)'
- en: Figure 6.3 Sample content from wnids.txt. It contains wnids (WordNet IDs), one
    per line.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 来自wnids.txt的示例内容。每行包含一个wnid（WordNet ID）。
- en: The file words.txt provides a human touch to these IDs by giving the description
    of each wnid in a tab-separated-value (TSV) format (table 6.1). Note that this
    file contains more than 82,000 lines (well over the 200 classes we have) and comes
    from a much larger data set.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文件words.txt以制表符分隔值（TSV）格式提供了对这些ID的人性化描述（表6.1）。请注意，这个文件包含超过82,000行（远超过我们的200个类别）并来自一个更大的数据集。
- en: Table 6.1 Sample content from words.txt. It contains the wnids and their descriptions
    for the data found in the data set.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 来自words.txt的示例内容。其中包含数据集中的wnids以及它们的描述。
- en: '| n00001740 | entity |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| n00001740 | entity |'
- en: '| n00001930 | physical entity |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| n00001930 | physical entity |'
- en: '| n00002137 | abstraction, abstract entity |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| n00002137 | 抽象，抽象实体 |'
- en: '| n00002452 | thing |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| n00002452 | 东西 |'
- en: '| n00002684 | object, physical object |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| n00002684 | 物体，实物 |'
- en: '| n00003553 | whole, unit |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| n00003553 | 整体，单位 |'
- en: '| n00003993 | congener |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| n00003993 | 同种异体 |'
- en: '| n00004258 | living thing, animate thing |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| n00004258 | 生物，有机物 |'
- en: '| n00004475 | organism, being |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| n00004475 | 有机体，存在 |'
- en: '| n00005787 | benthos |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| n00005787 | 底栖生物 |'
- en: '| n00005930 | dwarf |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| n00005930 | 矮人 |'
- en: '| n00006024 | heterotroph |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| n00006024 | 异养生物 |'
- en: '| n00006150 | parent |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| n00006150 | 父母 |'
- en: '| n00006269 | life |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| n00006269 | 生命 |'
- en: '| n00006400 | biont |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| n00006400 | 生物体 |'
- en: The train folder contains the training data. It contains a subfolder called
    images, and within that, you can find 200 folders, each with a label (i.e., a
    wnid). Inside each of these subfolders, you’ll find a collection of images representing
    that class. Each subfolder having a wnid as its name contains 500 images per class,
    100,000 in total (in all subfolders). Figure 6.4 depicts this structure, as well
    as some of the data found in the train folder.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练文件夹包含训练数据。它包含一个名为 images 的子文件夹，在其中，您可以找到 200 个文件夹，每个都有一个标签（即 wnid）。在每个这些子文件夹中，您将找到代表该类别的一系列图像。每个以其名称作为
    wnid 的子文件夹包含每类 500 张图像，总共有 100,000 张（在所有子文件夹中）。图 6.4 描述了这种结构，以及训练文件夹中找到的一些数据。
- en: '![06-04](../../OEBPS/Images/06-04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![06-04](../../OEBPS/Images/06-04.png)'
- en: Figure 6.4 The overall structure of the tiny-imagenet-200 data set. It has three
    text files (wnids.txt, words.txt, and val/val_annotations.txt) and three folders
    (train, val, and test). We will only use the train and val folders.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 tiny-imagenet-200 数据集的总体结构。它有三个文本文件（wnids.txt、words.txt 和 val/val_annotations.txt）和三个文件夹（train、val
    和 test）。我们只使用 train 和 val 文件夹。
- en: The val folder contains a subfolder called images and a collection of images
    (these are not divided into further subfolders like in the train folder). The
    labels (or wnids) for these images can be found in the val_annotations.txt file
    in the val folder.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: val 文件夹包含一个名为 images 的子文件夹和一组图像（这些图像不像在 train 文件夹中那样被进一步分成子文件夹）。这些图像的标签（或 wnids）可以在
    val 文件夹中的 val_annotations.txt 文件中找到。
- en: The final folder is called the test folder, which we will ignore in this chapter.
    This data set is part of a competition, and the data is used to score the submitted
    models. We don’t have labels for this test set.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个文件夹称为测试文件夹，在本章中我们将忽略它。该数据集是竞赛的一部分，数据用于评分提交的模型。我们没有这个测试集的标签。
- en: 6.1.2 Understanding the classes in the data set
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 理解数据集中的类别
- en: 'We have seen what kind of data we have and where it is available. For the next
    step, let’s identify some of the classes in the data. For that, we will define
    a function called get_tiny_imagenet_classes() that reads the wnids.txt and words.txt
    files and creates a pd.DataFrame (a pandas DataFrame) with two columns: the wnid
    and its corresponding class description (see the next listing).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了我们拥有的数据的类型以及其可用性。接下来，让我们识别一些数据中的类别。为此，我们将定义一个名为 get_tiny_imagenet_classes()
    的函数，该函数读取 wnids.txt 和 words.txt 文件，并创建一个包含两列的 pd.DataFrame（即 pandas DataFrame）：wnid
    及其相应的类别描述（见下一个列表）。
- en: Listing 6.1 Getting class descriptions of the classes in the data set
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 获取数据集中类别的类别描述
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Imports pandas and os packages
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 pandas 和 os 包
- en: ❷ Defines paths of the data directory, wnids.txt, and words.txt files
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义数据目录、wnids.txt 和 words.txt 文件的路径
- en: ❸ Defines a function to read the class descriptions of tiny_imagenet classes
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个函数来读取 tiny_imagenet 类别的类别描述
- en: ❹ Reads wnids.txt and words.txt as CSV files using pandas
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 pandas 读取 wnids.txt 和 words.txt 作为 CSV 文件
- en: ❺ Gets only the classes present in the tiny-imagenet-200 data set
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 仅获取 tiny-imagenet-200 数据集中存在的类别
- en: ❻ Sets the name of the index of the data frame to “wnid”
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将数据框的索引名称设置为“wnid”
- en: ❼ Resets the index so that it becomes a column in the data frame (which has
    the column name “wnid”)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 重置索引，使其成为数据框中的一列（该列的列名为“wnid”）
- en: ❽ Executes the function to obtain the class descriptions
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 执行函数以获取类别描述
- en: ❾ Inspects the head of the data frame (the first 25 entries)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 检查数据框的头部（前 25 个条目）
- en: This function first reads the wnids.txt that contains a list of wnids that correspond
    to the classes available in the data set as a pd.Series (i.e., pandas series)
    object. Next, it reads the words.txt file as a pd.DataFrame (a pandas DataFrame),
    which contains a wnid to class description mapping, and assigns it to words. Then,
    it picks the items from words where the wnid is present in the wnids pandas series.
    This will return a pd.DataFrame with 200 rows (table 6.2). Remember that the number
    of items in words.txt is much larger than the actual data set, so we only need
    to pick the items that are relevant to us.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数首先读取包含wnids列表的wnids.txt文件，该列表对应于数据集中可用的类别，作为pd.Series（即pandas series）对象。
    接下来，它将words.txt文件读取为pd.DataFrame（即pandas DataFrame），其中包含wnid到类别描述的映射，并将其分配给words。
    然后，它选择在wnids pandas系列中存在wnid的项目。 这将返回一个包含200行的pd.DataFrame（表6.2）。 请记住，words.txt中的项数远远大于实际数据集，因此我们只需要选择与我们相关的项。
- en: Table 6.2 Sample of the labels’ IDs and their descriptions that we generate
    using the get_tiny_imagenet_classes() function
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 使用get_tiny_imagenet_classes()函数生成的标签ID及其描述的示例
- en: '|  | **wind** | **class** |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | **风** | **课程** |'
- en: '| 0 | n02124075 | Egyptian cat |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 0 | n02124075 | 埃及猫 |'
- en: '| 1 | n04067472 | reel |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 1 | n04067472 | 卷轴 |'
- en: '| 2 | n04540053 | volleyball |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2 | n04540053 | 排球 |'
- en: '| 3 | n04099969 | rocking chair, rocker |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 3 | n04099969 | 摇椅，摇椅 |'
- en: '| 4 | n07749582 | lemon |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 4 | n07749582 | 柠檬 |'
- en: '| 5 | n01641577 | bullfrog, Rana catesbeiana |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 5 | n01641577 | 牛蛙，美洲牛蛙 |'
- en: '| 6 | n02802426 | basketball |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 6 | n02802426 | 篮球 |'
- en: '| 7 | n09246464 | cliff, drop, drop-off |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 7 | n09246464 | 悬崖，跌落，坠落 |'
- en: '| 8 | n07920052 | espresso |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 8 | n07920052 | 浓缩咖啡 |'
- en: '| 9 | n03970156 | plunger, plumber’s helper |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 9 | n03970156 | 吸盘，管道工的助手 |'
- en: '| 10 | n03891332 | parking meter |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 10 | n03891332 | 停车计时器 |'
- en: '| 11 | n02106662 | German shepherd, German shepherd dog, German p... |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 11 | n02106662 | 德国牧羊犬，德国牧羊犬，德国牧羊犬... |'
- en: '| 12 | n03201208 | dining table, board |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 12 | n03201208 | 餐桌，板 |'
- en: '| 13 | n02279972 | monarch, monarch butterfly, milkweed butterfly |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 13 | n02279972 | 帝王蝴蝶，帝王蝴蝶，小米蝴蝶 |'
- en: '| 14 | n02132136 | brown bear, bruin, Ursus arctos |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 14 | n02132136 | 棕熊，棕熊，北极熊 |'
- en: '| 15 | n041146614 | school bus |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 15 | n041146614 | 校车 |'
- en: 'We will then compute how many data points (i.e., images) there are for each
    class:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将计算每个类别的数据点（即图像）的数量：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code creates a new column called n_train that shows how many data points
    (i.e., images) were found for each wnid. This can be achieved using the pandas
    pd.Series .apply() function, which applies get_image_count() to each item in the
    series labels[“wnid”]. Specifically, get_image_count()takes in a path and returns
    the number of JPEG files found in that folder. When you use this get_image_count()
    function in conjunction with pd.Series.apply(), it goes to every single folder
    within the train folder that has a wnid as its name and counts the number of images.
    Once you run the line labels.head(n=10), you should get the result shown in table
    6.3.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建一个名为n_train的新列，显示每个wnid找到了多少个数据点（即图像）。 这可以通过pandas pd.Series .apply()函数来实现，该函数将get_image_count()应用于系列labels[“wnid”]中的每个项目。
    具体来说，get_image_count()接受一个路径并返回该文件夹中找到的JPEG文件的数量。 当您将此get_image_count()函数与pd.Series.apply()结合使用时，它会进入train文件夹中的每个文件夹，并计算图像的数量。
    一旦运行了标签.head(n=10)行，您应该会得到表6.3中显示的结果。
- en: Table 6.3 Sample of the data where n_train (number of training samples) has
    been calculated
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 计算了n_train（训练样本数）的数据示例
- en: '|  | **wind** | **class** | **n_train** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | **风** | **课程** | **n_train** |'
- en: '| 0 | n02124075 | Egyptian cat | 500 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 0 | n02124075 | 埃及猫 | 500 |'
- en: '| 1 | n04067472 | reel | 500 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1 | n04067472 | 卷轴 | 500 |'
- en: '| 2 | n04540053 | volleyball | 500 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 2 | n04540053 | 排球 | 500 |'
- en: '| 3 | n04099969 | rocking chair, rocker | 500 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 3 | n04099969 | 摇椅，摇椅 | 500 |'
- en: '| 4 | n07749582 | lemon | 500 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 4 | n07749582 | 柠檬 | 500 |'
- en: '| 5 | n01641577 | bullfrog, Rana catesbeiana | 500 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 5 | n01641577 | 牛蛙，美洲牛蛙 | 500 |'
- en: '| 6 | n02802426 | basketball | 500 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 6 | n02802426 | 篮球 | 500 |'
- en: '| 7 | n09246464 | cliff, drop, drop-off | 500 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 7 | n09246464 | 悬崖，跌落，坠落 | 500 |'
- en: '| 8 | n07920052 | espresso | 500 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 8 | n07920052 | 浓缩咖啡 | 500 |'
- en: '| 9 | n03970156 | plunger, plumber’s helper | 500 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 9 | n03970156 | 吸盘，管道工的助手 | 500 |'
- en: Let’s quickly verify that the results are correct. Go into the n02802426 subdirectory
    in the train folder, which should contain images of basketballs. Figure 6.5 shows
    a few sample images.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速验证结果是否正确。 进入train文件夹中的n02802426子目录，其中应该包含篮球的图像。 图6.5显示了几个示例图像。
- en: '![06-05](../../OEBPS/Images/06-05.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![06-05](../../OEBPS/Images/06-05.png)'
- en: Figure 6.5 Sample images for the wnid category n02802426 (i.e., basketball)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 wnid 类别 n02802426（即篮球）的样本图像
- en: 'You might find these images quite the opposite of what you expected. You might
    have expected to see clear and zoomed-in images of basketballs. But in the real
    world, that’s never the case. Real-life data sets are noisy. You can see the following
    images:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现这些图像与你预期的截然不同。你可能期望看到清晰放大的篮球图像。但在现实世界中，永远不会出现这种情况。真实数据集是有噪声的。你可以看到以下图像：
- en: The basketball is hardly visible (top left).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 篮球几乎看不见（左上角）。
- en: The basketball is green (bottom left).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 篮球是绿色的（左下角）。
- en: The basketball is next to a baby (i.e., out of context) (top middle).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 篮球在婴儿旁边（即上下文无关）（中间上方）。
- en: 'This will make you admire deep networks a bit more, as this is a hard problem
    for a bunch of stacked matrix multiplications (i.e., deep networks). Precise scene
    understanding is required to successfully solve this task. Despite the difficulty,
    the reward is significant. The model we develop is ultimately going to be used
    to identify objects in various backgrounds and contexts, such as living rooms,
    kitchens, and outdoors. And that is exactly what this data set trains your model
    for: to understand/detect objects in various contexts. You can probably imagine
    why the modern-day CAPTCHAs are becoming increasingly smarter and can keep up
    with algorithms that are able to classify objects more accurately. It is not difficult
    for a properly trained CNN to recognize a CAPTCHA that has cluttered backgrounds
    or small occlusions.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这会让你更加欣赏深度网络，因为这对一堆堆叠的矩阵乘法（即深度网络）是一个困难的问题。需要精确的场景理解才能成功解决此任务。尽管困难，但奖励很大。我们开发的模型最终将用于识别各种背景和上下文中的物体，例如客厅、厨房和室外。这正是这个数据集为模型训练的目的：在各种情境中理解/检测物体。你可能可以想象为什么现代
    CAPTCHA 越来越聪明，并且可以跟上能够更准确地分类对象的算法。对于受过适当训练的 CNN 来说，识别具有混乱背景或小遮挡的 CAPTCHA 并不困难。
- en: 'You can also quickly check the summary statistics (e.g., mean value, standard
    deviation, etc.) of the n_train column we generated. This provides a more digestible
    summary of the column than having to look through all 200 rows. This is done using
    the pandas describe() function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以快速检查我们生成的 n_train 列的摘要统计数据（例如，平均值、标准差等）。这提供了比查看所有 200 行更容易消化的列的摘要。这是使用 pandas
    描述() 函数完成的：
- en: '[PRE3]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Executing this will return the following series:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作将返回以下系列：
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see that it returns important statistics of the column, such as the
    mean value, standard deviation, minimum, and maximum. Every class has 500 images,
    meaning the data set is perfectly class balanced. This is a handy way to verify
    that we have a class-balanced data set.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到它返回了列的重要统计信息，如平均值、标准差、最小值和最大值。每个类别都有 500 张图像，这意味着数据集完美地平衡了类别。这是验证我们有一个类平衡数据集的有用方法。
- en: 6.1.3 Computing simple statistics on the data set
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 计算数据集上的简单统计量
- en: Analyzing various attributes of the data is also an important step. The type
    of analysis will change depending on the type of data you work with. Here, we
    will find out the average size of images (or even the 25/50/75 percentiles).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据的各种属性也是一个重要步骤。根据你处理的数据类型，分析类型会发生变化。在这里，我们将找出图像的平均大小（甚至是 25/50/75 百分位数）。
- en: Having this information ready by the time you get to the actual model saves
    you a lot of time in making certain decisions. For example, you must know basic
    statistics of the image size (height and width) to crop or pad images to a fixed
    size, as image classification CNNs can only process fixed-sized images (see the
    next listing).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际模型中准备好这些信息可以节省很多时间，因为你必须了解图像大小（高度和宽度）的基本统计信息，以裁剪或填充图像到固定大小，因为图像分类CNN只能处理固定大小的图像（见下一个列表）。
- en: Listing 6.2 Computing image width and height statistics
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 计算图像宽度和高度统计数据
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Importing os, PIL, and pandas packages
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 os、PIL 和 pandas 包
- en: ❷ Defining a list to hold image sizes
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个列表来保存图像大小
- en: ❸ Looping through the first 25 classes in the data set
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在数据集中循环前 25 类
- en: ❹ Defining the image directory for a particular class within the loop
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在循环中为特定类别定义图像目录
- en: ❺ Looping through all the images (ending with the extension JPEG) in that directory
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在该目录中循环所有具有扩展名 JPEG 的图像
- en: ❻ Appending the size of each image (i.e., a tuple of (width, height)) to image_sizes
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将每个图像的大小（即 (宽度、高度) 元组）添加到 image_sizes 中
- en: ❼ Creating a data frame from the tuples in the image_sizes
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 从image_sizes中的元组创建数据框架
- en: ❽ Setting column names appropriately
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 适当设置列名
- en: ❾ Obtaining the summary statistics of width and height for the images we fetched
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 获取我们获取的图像的宽度和高度的摘要统计信息
- en: Here, we take the first 25 wnids from the labels DataFrame we created earlier
    (processing all the wnids would take too much time). Then, for each wnid, we go
    into the subfolder that contains the data belonging to it and obtain the width
    and height information for each image using
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从之前创建的标签DataFrame中获取前25个wnid（处理所有wnid会花费太多时间）。然后，对于每个wnid，我们进入包含属于它的数据的子文件夹，并使用以下方法获取每个图像的宽度和高度信息
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Image.open(<path>).size returns a tuple (width, height) for a given image.
    We record the width and height of all images we come across in the image_sizes
    list. Finally, the image_sizes list looks like the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Image.open(<path>).size`函数返回给定图像的元组（宽度，高度）。我们将遇到的所有图像的宽度和高度记录在`image_sizes`列表中。最后，`image_sizes`列表如下所示：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For data in this format, we can use the pd.DataFrame.from_records() function
    to create a pd.DataFrame out of this list. A single element in image_sizes is
    a record. For example, (image_1.width, image_1.height) is a record. Therefore,
    image_sizes is a list of records. When you create a pd.DataFrame from a list of
    records, each record becomes a row in the pandas DataFrame, where each element
    in each record becomes a column. For example, since we have image width and image
    height as elements in each record, width and height become columns in the pandas
    DataFrame. Finally, we execute img_df.describe() to get the basic statistics on
    the width and height of the images we read (table 6.4).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种格式的数据，我们可以使用`pd.DataFrame.from_records()`函数将此列表创建为`pd.DataFrame`。`image_sizes`中的单个元素是一条记录。例如，`(image_1.width,
    image_1.height)`是一条记录。因此，`image_sizes`是一组记录的列表。当您从记录列表创建`pd.DataFrame`时，每条记录都变为`pandas
    DataFrame`中的一行，其中每条记录中的每个元素都变为列。例如，由于每条记录中都有图像宽度和图像高度作为元素，因此宽度和高度成为`pandas DataFrame`中的列。最后，我们执行`img_df.describe()`以获取我们读取的图像的宽度和高度的基本统计信息（表6.4）。
- en: Table 6.4 Width and height statistics of the images
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4 图像的宽度和高度统计信息
- en: '|  | **width** | **height** |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | **宽度** | **高度** |'
- en: '| count | 12500.0 | 12500.0 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| count | 12500.0 | 12500.0 |'
- en: '| mean | 64.0 | 64.0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| mean | 64.0 | 64.0 |'
- en: '| std | 0.0 | 0.0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| std | 0.0 | 0.0 |'
- en: '| min | 64.0 | 64.0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| min | 64.0 | 64.0 |'
- en: '| 25% | 64.0 | 64.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 25% | 64.0 | 64.0 |'
- en: '| 50% | 64.0 | 64.0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 64.0 | 64.0 |'
- en: '| 75% | 64.0 | 64.0 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 75% | 64.0 | 64.0 |'
- en: '| max | 64.0 | 64.0 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| max | 64.0 | 64.0 |'
- en: Next, we will discuss how we can create a data pipeline to ingest the image
    data we just discussed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何创建数据管道来摄取我们刚刚讨论的图像数据。
- en: Exercise 1
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1
- en: Assume that while browsing through the data set, you came across some corrupted
    images (i.e., they have negative-valued pixels). Assuming you already have a pd.DataFrame()
    called df that has a single column with the image file paths (called filepath),
    use the pandas apply() function to read each image’s minimum value and assign
    it to a column called minimum. To read the image, you can assume from PIL import
    Image and import numpy as np have been completed. You can also use np.array(<Image>)
    to turn a PIL.Image into an array.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在浏览数据集时，您遇到了一些损坏的图像（即，它们具有负值像素）。假设您已经有了一个名为`df`的`pd.DataFrame()`，其中包含一个带有图像文件路径的单列（称为`filepath`），请使用`pandas
    apply()`函数读取每个图像的最小值，并将其分配给名为`minimum`的列。要读取图像，您可以假设已完成`from PIL import Image`和`import
    numpy as np`，您还可以使用`np.array(<Image>)`将`PIL.Image`转换为数组。
- en: 6.2 Creating data pipelines using the Keras ImageDataGenerator
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 使用Keras ImageDataGenerator创建数据管道
- en: 'You have explored the data set well and understand things like how many classes
    there are, what kind of objects are present, and the sizes of the images. Now
    you will create three data generators for three different data sets: training,
    validation, and test. These data generators retrieve data from the disk in batches
    and perform any preprocessing required. This way, the data is readily consumable
    by a model. For this, we will use the convenient tensorflow.keras.preprocessing.image.ImageDataGenerator.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经很好地探索了数据集，并了解了诸如有多少类别、存在什么样的对象以及图像的大小等信息。现在，您将为三个不同的数据集创建三个数据生成器：训练、验证和测试。这些数据生成器以批量从磁盘中检索数据，并执行任何所需的预处理。这样，数据就可以被模型轻松消耗。为此，我们将使用方便的`tensorflow.keras.preprocessing.image.ImageDataGenerator`。
- en: 'We will start by defining a Keras ImageDataGenerator() to feed in data when
    we build the model:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从定义一个Keras ImageDataGenerator()开始，以在构建模型时提供数据：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Setting samplewise_center=True, the images generated will have their values
    normalized. Each image will be centered by subtracting the mean pixel value of
    that image. validation_split argument plays a vital role in training the data.
    This lets us split the training data into two subsets, training and validation,
    by separating a chunk (10% in this example) from the training data. Typically,
    in a machine learning problem, you should have three data sets:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 设置samplewise_center=True，生成的图像将具有归一化的值。每个图像将通过减去该图像的平均像素值来居中。validation_split参数在训练数据中扮演着重要的角色。这让我们将训练数据分成两个子集，训练集和验证集，通过从训练数据中分离出一部分（在本例中为10%）。在机器学习问题中，通常应该有三个数据集：
- en: '*Training data*—Typically the largest data set. We use this to train the model.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练数据*—通常是最大的数据集。我们用它来训练模型。'
- en: '*Validation data*—Held-out data set. It is not used to train the model but
    to monitor the performance of the model during training. Note that this validation
    set must remain fixed (and should not change) during training.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证数据*—保留数据集。它不用于训练模型，而是用于在训练过程中监视模型的性能。请注意，此验证集在训练过程中必须保持固定（不应更改）。'
- en: '*Test data*—Held-out data set. Unlike the validation data set, this is used
    only after the training of the model is completed. This represents how well the
    model will do on unseen real-world data. This is because the model has not interacted
    with the test data set in any way (unlike the training and validation data sets)
    until test time.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*测试数据*—保留数据集。与验证数据集不同，这仅在模型训练完成后使用。这表示模型在未见的真实世界数据上的表现。这是因为模型在测试时间之前没有以任何方式与测试数据集交互（与训练和验证数据集不同）。'
- en: We will also define a random seed and a batch size for later data generation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为稍后的数据生成定义一个随机种子和批量大小。
- en: 'Once you create an ImageDataGenerator, you can use one of its flow functions
    to read data coming from heterogeneous sources. For example, Keras currently offers
    the following methods:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个ImageDataGenerator后，您可以使用其中的一个flow函数来读取来自异构源的数据。例如，Keras目前提供了以下方法：
- en: flow()—Reads data from a NumPy array or a pandas DataFrame
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: flow()—从NumPy数组或pandas DataFrame中读取数据
- en: flow_from_dataframe()—Reads data from a file that contains filenames and their
    associated labels
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: flow_from_dataframe()—从包含文件名和它们关联标签的文件中读取数据
- en: flow_from_directory()—Reads from a folder where images are organized into subfolders
    according to the class where they belong
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: flow_from_directory()—从文件夹中读取数据，该文件夹中的图像根据它们所属的类别组织到子文件夹中。
- en: First, we will look at flow_from_directory(), because our train directory is
    in the exact format flow_from_directory() function expects the data to be in.
    Specifically, flow_from_directory() expects the data to be in the format shown
    in figure 6.6.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看flow_from_directory()，因为我们的训练目录以flow_from_directory()函数期望数据的确切格式存储。具体来说，flow_from_directory()期望数据的格式如图6.6所示。
- en: '![06-06](../../OEBPS/Images/06-06.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![06-06](../../OEBPS/Images/06-06.png)'
- en: Figure 6.6 Folder structure expected by the flow_from_directory() method
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6流从目录方法所预期的文件夹结构
- en: 'The flow methods return data generators, which are Python generators. A generator
    is essentially a function that returns an iterator (called a *generator-iterator*).
    But to keep our discussion simple, we will refer to both the generator and the
    iterator as the generator. You can iterate the generator, just like a list, and
    return items in a sequential manner. Here’s an example of a generator:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 流方法返回数据生成器，这些生成器是Python生成器。生成器本质上是一个返回迭代器（称为*generator-iterator*）的函数。但为了保持我们的讨论简单，我们将生成器和迭代器都称为生成器。您可以像处理列表一样迭代生成器，并以顺序方式返回项目。这里是一个生成器的例子：
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note the use of the keyword yield, which you can treat as you do the return
    keyword. However, unlike return, yield does not exit the function as soon as the
    line is executed. Now you can define the iterator as
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意使用关键字yield，您可以将其视为return关键字。但是，与return不同，yield不会在执行该行后立即退出函数。现在您可以将迭代器定义为
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can treat iterator like a list containing [(0, 0), (1, 2), (2, 4), ...,
    (98, 196), (99, 198)]. However, under the hood, generators are far more memory
    efficient than list objects. In our case, the data generators will return a single
    batch of images and targets in a single iteration (i.e., a tuple of images and
    labels). You can directly feed these generators to a method like tf.keras.models.Model.fit()
    in order to train a model. The flow_from_directory() method is used to retrieve
    data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将迭代器视为包含[(0, 0), (1, 2), (2, 4), ...，(98, 196), (99, 198)]的列表。然而，在幕后，生成器比列表对象更节省内存。在我们的情况下，数据生成器将在单次迭代中返回一批图像和目标（即，图像和标签的元组）。您可以直接将这些生成器提供给像`tf.keras.models.Model.fit()`这样的方法，以训练模型。`flow_from_directory()`方法用于检索数据：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can see numerous arguments that have been set for these functions. The
    most important argument to note is the subset argument, which is set to "training"
    for train_gen and “validation” for valid_gen. The other arguments are as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到已为这些函数设置了许多参数。需要注意的最重要的参数是`subset`参数，对于`train_gen`设置为“training”，对于`valid_gen`设置为“validation”。其他参数如下：
- en: directory (string)—The location of the parent directory, where data is further
    divided into subfolders representing classes.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目录（string）—父目录的位置，在这里数据进一步分成表示类别的子文件夹。
- en: target_size (tuple of ints)—Target size of the images as a tuple of (height,
    width). Images will be resized to the specified height and width.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标大小（int元组）—图像的目标大小，表示为（高度，宽度）的元组。图像将被调整为指定的高度和宽度。
- en: class_mode (string)—The type of targets we are going to provide to the model.
    Because we want the targets to be one-hot encoded vectors representing each class,
    we will set it to 'categorical'. Available types include "categorical" (default
    value), "binary" *(for data sets with two classes, 0 or 1),* "sparse" *(numerical
    label as opposed to a one-hot encoded vector),* "input" or None *(no labels),
    and* "raw" or "multi_output" *(only available in special circumstances).*
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别模式（string）—我们将要提供给模型的目标类型。因为我们希望目标是表示每个类别的独热编码向量，所以我们将其设置为'categorical'。可用类型包括“categorical”（默认值）、“binary”（对于只有两类（0或1）的数据集）、“sparse”（数值标签而不是独热编码向量）、“input”或None（没有标签）、以及“raw”或“multi_output”（仅在特殊情况下可用）。
- en: batch_size (int)—The size of a single batch of data.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小（int）—单个数据批次的大小。
- en: shuffle (bool)—Whether to shuffle the data when fetching.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否在获取时对数据进行洗牌（bool）—是否在获取时对数据进行洗牌。
- en: seed (int)—The random seed for data shuffling, so we get consistent results
    every time we run it.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机种子（int）—数据洗牌的随机种子，因此我们每次运行时都能获得一致的结果。
- en: subset (string)—If validation_split > 0, which subset you need. This needs to
    be set to either "training" or "validation".
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子集（string）—如果`validation_split > 0`，则需要哪个子集。这需要设置为“training”或“validation”之一。
- en: Note that, even though we have 64 × 64 images, we are resizing them to 56 ×
    56\. This is because the model we will use is designed for 224 × 224 images. Having
    an image size that is a factor of 224 × 224 makes adapting the model to our data
    much easier.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使我们有64 × 64的图像，我们也将它们调整为56 × 56。这是因为我们将使用的模型设计用于224 × 224的图像。具有224 × 224尺寸的图像使得将模型适应我们的数据变得更加容易。
- en: 'We can make our solution a bit shinier! You can see that between train_gen
    and valid_gen, there’s a lot of repetition in the arguments used. In fact, all
    the arguments except subset are the same for both generators. This repetition
    clutters the code and creates room for errors (if you need to change arguments,
    you might set one and forget the other). You can use the partial function in Python
    to create a partial function with the repeating arguments and then use that to
    create both train_gen and valid_gen:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以让我们的解决方案变得更加闪亮！您可以看到，在`train_gen`和`valid_gen`之间，使用的参数有很多重复。实际上，除了`subset`之外，所有参数都相同。这种重复会使代码变得凌乱，并为错误留下余地（如果需要更改参数，则可能会设置一个而忘记另一个）。您可以在Python中使用偏函数来创建具有重复参数的偏函数，然后使用它来创建`train_gen`和`valid_gen`：
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we first create a partial_flow_function (a Python function), which is
    essentially the flow_from_directory function with some arguments already populated.
    Then, to create train_gen and valid_gen, we only pass the subset argument. This
    makes the code much cleaner.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们首先创建一个`partial_flow_function`（一个Python函数），它实质上是`flow_from_directory`函数，有一些参数已经填充。然后，为了创建`train_gen`和`valid_gen`，我们只传递了`subset`参数。这样可以使代码更加清晰。
- en: 'Validation data check: Don’t expect the framework to take care of things for
    you'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据检查：不要期望框架为您处理事务
- en: Now that we have a training data generator and validation data generator, we
    shouldn’t blindly commit to using them. We must make sure that our validation
    data, which is randomly sampled from the training data, is consistent every time
    we traverse the training data set. It seems like a trivial thing that should be
    taken care of by the framework itself, but it’s better if you don’t take that
    for granted. And if you do
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个训练数据生成器和一个验证数据生成器，我们不应该盲目地承诺使用它们。我们必须确保我们从训练数据随机采样的验证数据在每次遍历训练数据集时保持一致。这似乎是一个应该由框架本身处理的微不足道的事情，但最好不要认为这是理所当然的。如果你这样做
- en: not do this check, you ultimately pay the price, so it is a good idea to make
    sure that we get consistent results across trials.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不执行此检查，最终你会付出代价，因此最好确保我们在不同试验中获得一致的结果。
- en: For this, you can iterate through the validation data generator’s output multiple
    times for a fixed number of iterations and make sure you get the same label sequence
    in each trial. The code for this is available in the notebook (under the section
    “Validating the consistency of validation data”).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你可以对验证数据生成器的输出进行多次迭代，进行固定次数的迭代，并确保每次试验中都获得相同的标签序列。此代码在笔记本中可用（在“验证验证数据的一致性”部分下）。
- en: 'We’re still not done. We need to do a slight modification to the generator
    returned by the flow_from_directory() function. If you look at an item in the
    data generator, you’ll see that it is a tuple (x, y), where x is a batch of images
    and y is a batch of one-hot-encoded targets. The model we use here has a final
    prediction layer and two additional auxiliary prediction layers. In total, the
    model has three output layers, so instead of a tuple (x, y), we need to return
    (x, (y, y, y)) by replicating y three times. We can fix this by defining a new
    generator data_gen_aux() that takes in the existing generator and modifies its
    output, as shown. This needs to be done for both the train data generator and
    validation data generator:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。我们需要对 flow_from_directory() 函数返回的生成器进行轻微修改。如果你查看数据生成器中的项，你会看到它是一个元组（x，y），其中
    x 是一批图像，y 是一批 one-hot 编码的目标。我们在这里使用的模型有一个最终预测层和两个额外的辅助预测层。总共，该模型有三个输出层，因此我们需要返回（x，（y，y，y））而不是一个元组（x，y），通过三次复制
    y。我们可以通过定义一个新的生成器 data_gen_aux() 来修复这个问题，该生成器接受现有的生成器并修改其输出，如所示。这需要对训练数据生成器和验证数据生成器都进行修复：
- en: '[PRE13]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It’s time to create a data generator for test data. Recall that we said the
    test data we are using (i.e., the val directory) is structured differently than
    the train and tran_val data folders. Therefore, it requires special treatment.
    The class labels are found in a file called val_annotations.txt, and the images
    are placed in a single folder with a flat structure. Not to worry; Keras has a
    function for this situation too. In this case, we will first read the val_annotations.txt
    as a pd.DataFrame using the get_test_labels_df() function. The function simply
    reads the val_annotations.txt file and creates a pd.DataFrame with two columns,
    the filename of an image and the class label:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候为测试数据创建一个数据生成器了。回想一下，我们说过我们正在使用的测试数据（即 val 目录）的结构与训练和 tran_val 数据文件夹不同。因此，它需要特殊处理。类标签存储在一个名为
    val_annotations.txt 的文件中，并且图像放置在一个具有扁平结构的单个文件夹中。不用担心；Keras 也为这种情况提供了一个函数。在这种情况下，我们将首先使用
    get_test_labels_df() 函数将 val_annotations.txt 读取为一个 pd.DataFrame。该函数简单地读取 val_annotations.txt
    文件，并创建一个具有两列的 pd.DataFrame，即图像的文件名和类标签：
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will use the flow_from_dataframe() function to create our test data
    generator. All you need to do is pass the test_df we created earlier (for the
    dataframe argument) and point at the directory where the images can be found (for
    the directory argument). Note that we are setting shuffle=False for test data,
    as we would like feed test data in the same order so that the performance metrics
    we monitor will be the same unless we change the model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 flow_from_dataframe() 函数创建我们的测试数据生成器。你只需要传递我们之前创建的 test_df（作为 dataframe
    参数）和指向图像所在目录的目录参数。请注意，我们为测试数据设置了 shuffle=False，因为我们希望以相同的顺序输入测试数据，以便我们监视的性能指标将保持不变，除非我们更改模型：
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, we are going to define one of the complex computer vision models using
    Keras and eventually train it on the data we have prepared.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Keras 定义一个复杂的计算机视觉模型，并最终在我们准备好的数据上对其进行训练。
- en: Exercise 2
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2
- en: 'As part of the testing process, say you want to see how robust the model is
    against corrupted labels in the training data. For this, you plan to create a
    generator that sets the label to 0 with 50% probability. How would you change
    the following generator for this purpose? You can use np.random.normal() to draw
    a value randomly from a normal distribution with zero mean and unit variance:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为测试过程的一部分，假设你想要查看模型对训练数据中损坏标签的鲁棒性如何。为此，你计划创建一个生成器，以 50% 的概率将标签设置为 0。你将如何修改以下生成器以实现此目的？你可以使用
    **np.random.normal()** 从具有零均值和单位方差的正态分布中随机抽取一个值：
- en: '[PRE16]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '6.3 Inception net: Implementing a state-of-the-art image classifier'
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 Inception net：实现最先进的图像分类器
- en: You have analyzed the data set and have a well-rounded idea of what the data
    looks like. For images, you inarguably turn to CNNs, as they are the best in the
    business. It’s time to build a model to learn customers’ personal tastes. Here,
    we will replicate one of the state-of-the-art CNN models (known as *Inception
    net*) using the Keras functional API.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经分析了数据集，并对数据的外观有了全面的了解。对于图像，你无疑会转向卷积神经网络（**CNNs**），因为它们是业内最好的。现在是构建一个模型来学习客户个人喜好的时候了。在这里，我们将使用
    **Keras functional API** 复制一个最先进的 CNN 模型（称为 *Inception net*）。
- en: Inception net is a complex CNN that has made its mark by delivering state-of-the
    art performance. Inception net draws its name from the popular internet meme “We
    need to go deeper” that features Leonardo De Caprio from the movie *Inception*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网络是一个复杂的 CNN，以其提供的最先进性能而著称。Inception 网络的名字来源于流行的互联网梗“我们需要更深入”，该梗以电影
    *Inception* 中的莱昂纳多·迪卡普里奥为特色。
- en: The Inception model has six different versions that came out over the course
    of a short period of time (approximately 2015-2016). That is a testament to how
    popular the model was among computer vision researchers. To honor the past, we
    will implement the first inception model that came out (i.e., Inception net v1)
    and later compare it to other models. As this is an advanced CNN, a good understanding
    of its architecture and some design decisions is paramount. Let’s look at the
    Inception model, how is it different from a typical CNN, and most importantly,
    why it is different.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 模型在短时间内推出了六个不同版本（大约在 2015-2016 年之间）。这证明了该模型在计算机视觉研究人员中有多受欢迎。为了纪念过去，我们将实现首个推出的
    Inception 模型（即 Inception 网络 v1），并随后将其与其他模型进行比较。由于这是一个高级 CNN，对其架构和一些设计决策的深入了解至关重要。让我们来看看
    Inception 模型，它与典型 CNN 有何不同，最重要的是，它为什么不同。
- en: 'The Inception model (or Inception net) isn’t a typical CNN. Its prime characteristic
    is complexity, as the more complex (i.e., more parameters) the model is, the higher
    the accuracy. For example, Inception net v1 has close to 20 layers. But there
    are two main problems that rear their heads when it comes to complex models:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 模型（或 Inception 网络）不是典型的 CNN。它的主要特点是复杂性，因为模型越复杂（即参数越多），准确率就越高。例如，Inception
    网络 v1 几乎有 20 层。但是当涉及到复杂模型时，会出现两个主要问题：
- en: If you don’t have a big enough data set for a complex model, it is likely the
    model will overfit the training data, leading to poor overall performance on real-world
    data.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你没有足够大的数据集用于一个复杂模型，那么很可能模型会对训练数据过拟合，导致在真实世界数据上的整体性能不佳。
- en: Complex models lead to more training time and more engineering effort to fit
    those models into relatively small GPU memory.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的模型导致更多的训练时间和更多的工程努力来将这些模型适配到相对较小的 GPU 内存中。
- en: This demands a more pragmatic way to approach this problem, such as by answering
    the question “How can we introduce *sparsity* in deep models (i.e., having fewer
    parameters) so that the risk of overfitting as well as the appetite for memory
    goes down?” This is the main question answered in the Inception net model.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这要求以更加务实的方式来解决这个问题，比如回答“我们如何在深度模型中引入*稀疏性*，以减少过拟合风险以及对内存的需求？”这是 Inception 网络模型中回答的主要问题。
- en: What is overfitting?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是过拟合？
- en: Overfitting is an important concept in machine learning and can be notoriously
    hard to avoid. Overfitting refers to the phenomenon where the model learns to
    represent training data very well (i.e., high train accuracy) but performs poorly
    on unseen data (i.e., low test accuracy). This happens when the model tries to
    remember training samples rather than learn generalizable features from the data.
    This is prevalent in deep networks, as they usually have more parameters than
    the amount of data. Overfitting will be discussed in more detail in the next chapter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是机器学习中的一个重要概念，而且常常难以避免。过拟合是指模型学习很好地表示训练数据（即高训练精度），但在未见过的数据上表现不佳（即低测试精度）的现象。当模型试图记住训练样本而不是从数据中学习可泛化的特征时，就会发生这种情况。这在深度网络中很普遍，因为它们通常比数据量更多的参数。过拟合将在下一章中更详细地讨论。
- en: Let’s remind ourselves of the basics of CNNs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾CNN的基础知识。
- en: 6.3.1 Recap on CNNs
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 CNN回顾
- en: 'CNNs are predominantly used to process images and solve computer vision problems
    (e.g., image classification, object detection, etc.). As depicted in figure 6.7,
    a CNN has three components:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: CNN主要用于处理图像和解决计算机视觉问题（例如图像分类、目标检测等）。如图6.7所示，CNN有三个组成部分：
- en: Convolution layers
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layersFully connected layers
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层全连接层
- en: '![06-07](../../OEBPS/Images/06-07.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![06-07](../../OEBPS/Images/06-07.png)'
- en: Figure 6.7 A simple convolutional neural network. First, we have an image with
    height, width, and channel dimensions, followed by a convolution and a pooling
    layer. Finally, the last convolution/pooling layer output is flattened and fed
    to a set of fully connected layers.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7一个简单的卷积神经网络。首先，我们有一个具有高度、宽度和通道维度的图像，然后是一个卷积和一个池化层。最后，最后一个卷积/池化层的输出被展平并馈送到一组全连接层。
- en: 'The convolution operation shifts a small kernel (also called a filter) with
    a fixed size over the width and height dimensions of the input. While doing so,
    it produces a single value at each position. The convolution operation consumes
    an input with some width, height, and a number of channels and produces an output
    that has some width, height, and a single channel. To produce a multichannel output,
    a convolution layer stacks many of these filters, leading to as many outputs as
    the number of filters. A convolution layer has the following important parameters:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作将一个固定大小的小核（也称为过滤器）沿输入的宽度和高度维度移动。在这样做时，它在每个位置产生一个单一值。卷积操作使用具有一定宽度、高度和若干通道的输入，并产生具有一定宽度、高度和单一通道的输出。为了产生多通道输出，卷积层堆叠许多这些过滤器，导致与过滤器数量相同数量的输出。卷积层具有以下重要参数：
- en: '*Number of filters*—Decides the channel depth (or the number of feature maps)
    of the output produced by the convolution layer'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*过滤器数量* — 决定卷积层产生的输出的通道深度（或特征图的数量）'
- en: '*Kernel size*—Also known as the receptive field, it decides the size (i.e.,
    height and width) of the filters. The larger the kernel size, the more of the
    image the model sees at a given time. But larger filters lead to longer training
    times and larger memory requirements.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核大小* — 也称为感受野，它决定了过滤器的大小（即高度和宽度）。核大小越大，模型在一次观察中看到的图像部分就越多。但更大的过滤器会导致更长的训练时间和更大的内存需求。'
- en: '*Stride*—Determines how many pixels are skipped while convolving the image.
    A higher stride leads to a smaller output size (stride is typically used only
    on height and width dimensions).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步长* — 决定在卷积图像时跳过多少像素。更高的步长导致较小的输出大小（步长通常仅用于高度和宽度维度）。'
- en: '*Padding*—Prevents the automatic dimensionality reductions that take place
    during the convolution operation by adding an imaginary border of zeros, such
    that the output has the same height and width as the input.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*填充* — 通过添加零值的虚拟边界来防止卷积操作期间自动降低维度，从而使输出具有与输入相同的高度和宽度。'
- en: Figure 6.8 shows the working of the convolution operation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8展示了卷积操作的工作原理。
- en: '![06-08](../../OEBPS/Images/06-08.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![06-08](../../OEBPS/Images/06-08.png)'
- en: Figure 6.8 The computations that happen in the convolution operation while shifting
    the window
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 在卷积操作中移动窗口时发生的计算
- en: 'The pooling operation exhibits the same behavior as the convolution operation
    when processing an input. However, the exact computations involved are different.
    There are two different types of pooling: max and average. Max pooling takes the
    maximum value found in the dark gray box shown in figure 6.9 as the window moves
    over the input. Average pooling takes the average value of the dark gray box as
    the window moves over the input.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理输入时，池化操作表现出与卷积操作相同的行为。但是，所涉及的确切计算是不同的。池化有两种不同的类型：最大池化和平均池化。最大池化在图 6.9 中显示的深灰色框中找到的最大值作为窗口移过输入时的输出。平均池化在窗口移过输入时取深灰色框的平均值作为输出。
- en: NOTE CNNs use average pooling as the closest to the output and max pooling layers
    everywhere else. This configuration has been found to deliver better performance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 CNNs 在输出处使用平均池化，并在其他地方使用最大池化层。已发现该配置提供了更好的性能。
- en: '![06-09](../../OEBPS/Images/06-09.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![06-09](../../OEBPS/Images/06-09.png)'
- en: Figure 6.9 How the pooling operation computes the output. It looks at a small
    window and takes the maximum of the input in that window as the output for the
    corresponding cell.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 池化操作如何计算输出。它查看一个小窗口，并将该窗口中的输入最大值作为相应单元的输出。
- en: The benefit of the pooling operation is that it makes CNNs translation invariant.
    Translation invariance means that the model can recognize an object regardless
    of where it appears. Due to the way max pooling is computed, the feature maps
    generated are similar, even when objects/features are offset by a small number
    of pixels from what the model was trained on. This means that if you are training
    a model to classify dogs, the network will be resilient against where exactly
    the dog appears (only to a certain extent).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作的好处在于它使得 CNN 具有平移不变性。平移不变性意味着模型可以识别物体，而不管它出现在何处。由于最大池化的计算方式，生成的特征图是相似的，即使对象/特征与模型训练的位置相差几个像素。这意味着，如果你正在训练一个分类狗的模型，网络将对狗出现的确切位置具有弹性（只有在一定程度上）。
- en: Finally, you have a fully connected layer. As we are mostly interested in classification
    models right now, we need to output a probability distribution over the classes
    we have for any given image. We do that by connecting a small number of fully
    connected layers to the end of the CNNs. The fully connected layers will take
    the last convolution/pooling output as the input and produce a probability distribution
    over the classes in the classification problem.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你有一个全连接层。由于我们目前主要关注分类模型，我们需要为任何给定的图像输出一个类别的概率分布。我们通过将少量的全连接层连接到 CNNs 的末尾来实现这一点。全连接层将最后的卷积/池化输出作为输入，并在分类问题中生成类别的概率分布。
- en: As you can see, CNNs have many hyperparameters (e.g., number of layers, convolution
    window size, strides, fully connected hidden layer sizes, etc.). For optimal results,
    they need to be selected using a hyperparameter optimization technique (e.g.,
    grid search, random search).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，CNNs 有许多超参数（例如，层数、卷积窗口大小、步幅、全连接隐藏层大小等）。为了获得最佳结果，需要使用超参数优化技术（例如，网格搜索、随机搜索）来选择它们。
- en: 6.3.2 Inception net v1
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 Inception 网络 v1
- en: Inception net v1 (also called GoogLeNet) ([http://mng.bz/R4GD](http://mng.bz/R4GD))
    takes CNNs to another level. It is not a typical CNN and requires more effort
    to implement compared to a standard CNN. At first glance, Inception net might
    look a bit scary (see figure 6.10). But there are only a handful of new concepts
    that you need to grok to understand this model. It’s mostly the repetitive application
    of those concepts that makes the model sophisticated.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网络 v1（也称为 GoogLeNet）([http://mng.bz/R4GD](http://mng.bz/R4GD)) 将 CNNs
    带入了另一个层次。它不是一个典型的 CNN，与标准 CNN 相比，需要更多的实现工作。乍一看，Inception 网络可能看起来有点可怕（见图 6.10）。但是你只需要理解几个新概念，就可以理解这个模型。主要是这些概念的重复应用使模型变得复杂。
- en: '![06-10](../../OEBPS/Images/06-10.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![06-10](../../OEBPS/Images/06-10.png)'
- en: Figure 6.10 Abstract architecture of Inception net v1\. Inception net starts
    with a stem, which is an ordinary sequence of convolution/pooling layers that
    is found in a typical CNN. Then Inception net introduces a new component known
    as an Inception block. Finally, Inception net also makes use of auxiliary output
    layers.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 Inception 网络 v1 的抽象架构。Inception 网络从一个称为干扰的起始开始，这是一个在典型 CNN 中找到的普通卷积/池化层序列。然后，Inception
    网络引入了一个称为 Inception 块的新组件。最后，Inception 网络还使用了辅助输出层。
- en: Let’s first understand what’s in the Inception model at a macro level, as shown
    in figure 6.10, temporarily ignoring the details, such as layers and their parameters.
    We will flesh these out once we develop a strong macro-level understanding.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先在宏观层面理解 Inception 模型中的内容，如图 6.10 所示，暂时忽略诸如层和它们的参数之类的细节。我们将在开发出强大的宏观水平理解后详细阐述这些细节。
- en: Inception net starts with something called a *stem*. The stem consists of convolution
    and pooling layers identical to the convolution and pooling layers of a typical
    CNN. In other words, the stem is a sequence of convolution and pooling layers
    organized in a specific order.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网络以称为*stem*的东西开始。stem 包含与典型 CNN 的卷积和池化层相同的卷积和池化层。换句话说，stem 是按特定顺序组织的卷积和池化层的序列。
- en: Next you have several *Inception blocks* interleaved by max pooling layers.
    An inception block contains a parallel set of sub-convolution layers with varying
    kernel sizes. This enables the model to look at the input with different-sized
    receptive fields at a given depth. We will study the details and motivations behind
    this in detail.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你有几个*Inception blocks*，这些块被 max pooling 层交错。一个 Inception block 包含一组并行的具有不同核大小的子卷积层。这使得模型能够在给定深度上以不同大小的感受野查看输入。我们将详细研究这背后的细节和动机。
- en: Finally, you have a fully connected layer, which resembles the final prediction
    layer you have in a typical CNN. You can also see that there are two more interim
    fully connected layers. These are known as *auxiliary output layers*. Just like
    the final prediction layer, they consist of fully connected layers and a softmax
    activation that outputs a probability distribution over the classes in the data
    set. Though they have the same appearance as the final prediction layer, they
    do not contribute to the final output of the model but do play an important role
    in stabilizing the model during training. Stable training becomes more and more
    strenuous as the model gets deeper (mostly due to the limited precision of numerical
    values in a computer).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你有一个全连接层，它类似于典型 CNN 中的最终预测层。你还可以看到还有两个更多的临时全连接层。这些被称为*辅助输出层*。与最终预测层一样，它们由全连接层和
    softmax 激活组成，输出数据集中类别的概率分布。尽管它们与最终预测层具有相同的外观，但它们不会对模型的最终输出做出贡献，但在训练过程中起着重要作用，稳定训练变得越来越艰难，因为模型变得越来越深（主要是由于计算机中数值的有限精度）。
- en: Let’s implement a version of the original Inception net from scratch. While
    doing so, we will discuss any new concepts we come across.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始实现原始的 Inception 网络的一个版本。在此过程中，我们将讨论我们遇到的任何新概念。
- en: Caveat! We are going to build a slightly different Inception net v1
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！我们将构建一个略有不同的 Inception 网络 v1。
- en: 'We are implementing something slightly different from the original Inception
    net v1 model to deal with a certain practical limitation. The original Inception
    net is designed to process inputs of size 224 × 224 × 3 belonging to 1,000 classes,
    whereas we have 64 × 64 × 3 inputs belonging to 200 classes, which we will resize
    to 56 × 56 × 3 such that it is a factor of 224 (i.e., 56 × 4 = 224). Therefore,
    we will make some changes to the original Inception net. You can safely ignore
    the details that follow for the moment if you like. But if you are interested,
    we specifically make the following changes:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在实现与原始 Inception 网络 v1 模型略有不同的东西，以应对某种实际限制。原始 Inception 网络设计用于处理尺寸为 224 ×
    224 × 3 的输入，属于 1,000 个类别，而我们有尺寸为 64 × 64 × 3 的输入，属于 200 个类别，我们将其调整为 56 × 56 ×
    3，以便其是 224 的因数（即，56 × 4 = 224）。因此，我们将对原始 Inception 网络进行一些修改。如果你愿意，你可以暂时忽略以下细节。但是如果你感兴趣，我们具体进行以下更改：
- en: Make stride 1 for the first three layers that have stride 2 (in the stem) so
    that we enjoy the full depth of the model for the smaller input images we have.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使前三个具有步长 2 的层（在 stem 中）的步长为 1，以便我们在拥有较小输入图像时享受模型的全部深度。
- en: Change the size of the last fully connected classification layer from 1,000
    to 200 as we only have 200 classes.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将最后一个全连接分类层的大小从 1,000 更改为 200，因为我们只有 200 个类别。
- en: Remove some regularization (i.e., dropout, loss weighting; these will be reintroduced
    in the next chapter).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除一些正则化（即，dropout、loss weighting；这些将在下一章重新引入）。
- en: If you are comfortable with the model discussed here, there will be no issue
    with understanding the original Inception v1 model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这里讨论的模型感到舒适，理解原始的 Inception v1 模型将不会有问题。
- en: First let’s define a function that creates the stem of Inception net v1\. The
    stem is the first few layers of Inception net and looks like nothing more than
    the typical convolution/ pooling layers you find in a typical CNN. But there is
    a new layer (called a *lambda layer*) that performs something known as *local
    response normalization* (LRN). We will discuss the purpose of this layer in more
    detail soon (see the next listing).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个创建 Inception net v1 干部结构的函数。干部结构是 Inception 网络的前几层，看起来不过是典型卷积/池化层，但有一个新的层（称为
    *lambda 层*），执行一些称为 *局部响应归一化*（LRN）的功能。我们将在稍后更详细地讨论该层的目的（请参见下一个清单）。
- en: Listing 6.3 Defining the stem of Inception net
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 6.3 Inception 网络中的干部结构的定义
- en: '[PRE17]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The output of the first convolution layer
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个卷积层的输出
- en: ❷ The output of the first max pooling layer
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一个最大池化层的输出
- en: ❸ The first local response normalization layer. We define a lambda function
    that encapsulates LRN functionality.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第一个局部响应归一化层。我们定义一个封装了 LRN 功能的 lambda 函数。
- en: ❹ Subsequent convolution layers
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 后续的卷积层
- en: ❺ The second LRN layer
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 第二个 LRN 层
- en: ❻ Max pooling layer
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 最大池化层
- en: ❼ Returns the final output (i.e., output of the max pooling layer)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 返回最终输出（即最大池化层的输出）
- en: Most of this code should be familiar to you by now. It is a series of layers,
    starting from an input to produce an output.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这段代码中的大部分应该已经非常熟悉了。它是一系列层，从输入开始生成输出。
- en: Lambda layers (tf.keras.layers.Lambda)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 层（tf.keras.layers.Lambda）
- en: 'Lambda layers in Keras have a similar purpose to standard Python lambda functions.
    They encapsulate some computations that are not typically available as a standard
    layer in Keras when written as a standard lambda function. For example, you can
    define a Keras layer that takes the maximum over axis 1 as follows. However, you
    can only use TensorFlow/Keras computations in the Keras lambda function:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中的 lambda 层与标准的 Python lambda 函数具有相似的目的。当用标准 lambda 函数编写时，它们封装了一些通常不可用作
    Keras 标准层的计算。例如，您可以如下定义一个 Keras 层，该层取轴上的最大值。但是，您只能在 Keras lambda 函数中使用 TensorFlow
    / Keras 计算：
- en: '[PRE18]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You might notice that the purpose of a lambda layer is almost identical to the
    sub-classing API of Keras. Well, yes, but the lambda layer does not require the
    amount of code scaffolding required in the sub-classing API. For layers with complex
    operations (e.g., if-else conditions, for loops, etc.), you might find the sub-classing
    API easier.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到 lambda 层的作用与 Keras 的子类化 API 几乎相同。是的，但是 lambda 层不需要子类化 API 中所需的代码支架。对于具有复杂操作的图层（例如
    if-else 条件，for 循环等），您可能会发现子类化 API 更容易。
- en: 'Specifically, we define the following layers:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们定义了以下层：
- en: A convolution layer
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个卷积层
- en: 64 filters, (7,7) kernel size, (2,2) strides, activation ReLU, same padding
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 64 个过滤器，(7,7) 卷积核大小，(2,2) 步长，激活 ReLU，相同填充
- en: A local response normalization layer
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个局部响应归一化层
- en: This is specified using a tf.keras.layers.Lambda layer. This layer provides
    you an easy way to define a Keras layer that encapsulates TensorFlow/Keras computations
    that are not readily available. Local response normalization is a technique to
    normalize a given input.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是通过使用 tf.keras.layers.Lambda 层来指定的。该层为您提供了一种方便的方法，可以定义一个封装了不容易获得的 TensorFlow
    / Keras 计算的 Keras 层。局部响应归一化是一种归一化给定输入的技术。
- en: Second convolution layer
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个卷积层
- en: 192 filters, (3,3) kernel size, (2, 2) strides, ReLU activation, same padding
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 192 个过滤器，(3,3) 卷积核大小，(2，2) 步长，ReLU 激活，相同填充
- en: A local response normalization layer
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个局部响应归一化层
- en: A max pool layer
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最大池化层
- en: (3,3) kernel size, (2,2) stride, and same padding
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3,3) 卷积核大小，(2,2) 步长以及相同填充
- en: Local response normalization
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 局部响应归一化
- en: Local response normalization (LRN) is an early layer normalization technique
    introduced in the paper “ImageNet Classification with Deep CNNs” ([http://mng.bz/EWPr](http://mng.bz/EWPr)).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 局部响应归一化（LRN）是早期的归一化技术，介绍在论文 “ImageNet Classification with Deep CNNs” ([http://mng.bz/EWPr](http://mng.bz/EWPr))
    中。
- en: This technique is inspired by the lateral inhibition ([http://mng.bz/N6PX](http://mng.bz/N6PX))
    exhibited in a biological system. This refers to the phenomenon where excited
    neurons suppress the activity of neighboring neurons (e.g., observed in retinal
    receptors). Essentially, the LRN layer normalizes each value of a convolution
    output by dividing it by the values found in its neighborhood (the neighborhood
    is parametrized by a radius, which is a hyperparameter of the layer). This normalization
    creates competition among neurons and leads to slightly better performance. We
    will not discuss the exact equation involved in this computation, as this method
    has fallen out of fashion and better and more promising regularization techniques,
    such as batch normalization, have taken over.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术受到了生物系统中表现出的横向抑制（[http://mng.bz/N6PX](http://mng.bz/N6PX)）的启发。这指的是激活的神经元抑制邻近神经元的活动的现象（例如，在视网膜感受器中观察到）。本质上，LRN
    层通过将卷积输出的每个值除以其邻域中的值（邻域由半径参数化，这是该层的超参数）来标准化每个值。这种规范化创建了神经元之间的竞争，并导致略微更好的性能。我们将不讨论涉及此计算的确切方程，因为这种方法已经过时，并且更好、更有前途的正则化技术，如批量标准化，已经取代了它。
- en: Going deeper into the Inception block
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地了解 Inception 块
- en: As stated earlier, one of the main breakthroughs in Inception net is the Inception
    block. Unlike a typical convolution layer that has a fixed kernel size, the Inception
    block is a collection of parallel convolutional layers with different kernel sizes.
    Specifically, the Inception block in Inception v1 contains a 1 × 1 convolution,
    a 3 × 3 convolution, a 5 × 5 convolution, and pooling. Figure 6.11 shows the architecture
    of an Inception block.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，Inception 网中的主要突破之一是 Inception 块。与具有固定核大小的典型卷积层不同，Inception 块是具有不同核大小的并行卷积层的集合。具体来说，在
    Inception v1 中的 Inception 块包含 1 × 1 卷积、3 × 3 卷积、5 × 5 卷积和池化。图 6.11 显示了 Inception
    块的架构。
- en: '![06-11](../../OEBPS/Images/06-11.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![06-11](../../OEBPS/Images/06-11.png)'
- en: Figure 6.11 The computations in the Inception block, which is essentially a
    set of parallel convolution/pooling layers with different kernel sizes
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 Inception 块中的计算，本质上是一组具有不同核大小的并行卷积/池化层
- en: 'Let’s understand why these parallel convolution layers are better than having
    a giant block of convolution filters with the same kernel size. The main advantage
    is that the Inception block is highly parameter efficient compared to a single
    convolution block. We can crunch some numbers to assure ourselves that this is
    the case. Let’s say we have two convolution blocks: one Inception block and a
    standard convolution block. Assume that the Inception block has the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解为什么这些并行卷积层比具有相同核大小的巨型卷积滤波器块更好。主要优势在于 Inception 块与单个卷积块相比具有高度参数效率。我们可以通过一些数字来确保这一点。假设我们有两个卷积块：一个是
    Inception 块，一个是标准卷积块。假设 Inception 块具有以下参数：
- en: A 1 × 1 convolution layer with 32 filters
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 32 个滤波器的 1 × 1 卷积层
- en: A 3 × 3 convolution layer with 16 filters
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 16 个滤波器的 3 × 3 卷积层
- en: A 5 × 5 convolution layer with 16 filters
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 16 个滤波器的 5 × 5 卷积层
- en: If you were to design a standard convolution layer that has the representational
    power of the Inception block, you’d need
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要设计一个具有 Inception 块表示能力的标准卷积层，你会需要
- en: A 5 × 5 convolution layer with 64 filters
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 64 个滤波器的 5 × 5 卷积层
- en: Assuming we’re processing an input with a single channel, the inception block
    has 576 parameters given by
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在处理一个单通道的输入，Inception 块的参数为 576，由以下给出
- en: 1 × 1 × 1 × 32 + 3 × 3 × 1 × 16 + 5 × 5 × 1 × 16 = 576
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 × 1 × 32 + 3 × 3 × 1 × 16 + 5 × 5 × 1 × 16 = 576
- en: 'The standard convolution block has 1,600 parameters:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 标准卷积块具有 1,600 个参数：
- en: 5 × 5 × 1 × 64 = 1,600
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 5 × 5 × 1 × 64 = 1,600
- en: In other words, the Inception block has a 64% reduction in the number of parameters
    compared to a standard convolution layer that has the representational power of
    the Inception block.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，与具有 Inception 块表示能力的标准卷积层相比，Inception 块减少了 64% 的参数数量。
- en: Connection between Inception block and sparsity
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 块与稀疏性之间的联系
- en: 'For the curious minds out there, there might still be a lingering question:
    How does the Inception block introduce sparsity? Think of the following two scenarios
    where you have three convolution filters. In one scenario, you have three 5 ×
    5 convolution filters, whereas in the other you have a 1 × 1, 3 × 3 and 5 × 5
    convolution filter. Figure 6.12 illustrates the difference between the two scenarios.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于好奇的人们，可能还有一个持续存在的问题：Inception 块是如何引入稀疏性的？想象一下以下两种情况，你有三个卷积滤波器。在一个场景中，你有三个
    5 × 5 卷积滤波器，而在另一个场景中，你有一个 1 × 1、3 × 3 和 5 × 5 卷积滤波器。图 6.12 展示了这两种情景之间的差异。
- en: '![06-12](../../OEBPS/Images/06-12.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![06-12](../../OEBPS/Images/06-12.png)'
- en: Figure 6.12 How the Inception block encourages sparsity in the model. You can
    view a 1 × 1 convolution as a highly sparse 5 × 5 convolution.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 Inception 块如何促进模型的稀疏性。你可以将 1 × 1 卷积看作是一个高度稀疏的 5 × 5 卷积。
- en: It is not that hard to see that when you have three 5 × 5 convolution filters,
    it creates a very dense connection between the convolution layer and the input.
    However, when you have a 1 × 1, 3 × 3, and 5 × 5 convolution layer, the connections
    between the input and the layer are sparser. Another way to think about this is
    that a 1 × 1 convolution is essentially a 5 × 5 convolution layer, where all the
    elements are switched off except for the center element. Therefore, a 1 × 1 convolution
    is a highly sparse 5 × 5 convolution layer. Similarly, a 3 × 3 convolution is
    a sparse 5 × 5 convolution layer. And by enforcing sparsity, we make the CNN parameter
    efficient and reduce the chances of overfitting. This explanation is motivated
    by the discussion found at [http://mng.bz/Pn8g](http://mng.bz/Pn8g).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 不难看出，当你有三个 5 × 5 卷积滤波器时，它会在卷积层和输入之间创建非常密集的连接。然而，当你有一个 1 × 1、3 × 3 和 5 × 5 卷积层时，输入和层之间的连接更加稀疏。另一种思考方式是，1
    × 1 卷积本质上是一个 5 × 5 卷积层，其中除了中心元素外，所有元素都关闭了。因此，1 × 1 卷积是一个高度稀疏的 5 × 5 卷积层。类似地，3
    × 3 卷积是一个稀疏的 5 × 5 卷积层。通过引入稀疏性，我们使 CNN 参数高效，并减少了过拟合的可能性。这个解释受到了 [http://mng.bz/Pn8g](http://mng.bz/Pn8g)
    中讨论的启发。
- en: 1 × 1 convolutions as a dimensionality reduction method
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 卷积作为降维方法
- en: Usually, the deeper your model is, the higher the performance (given that you
    have enough data). As we already know, the depth of a CNN comes at a price. The
    more layers you have, the more parameters it creates. Therefore, you need to be
    extra cautious of the parameter count of deep models.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你的模型越深，性能越高（假设你有足够的数据）。正如我们已经知道的，CNN 的深度是有代价的。层数越多，参数就越多。因此，你需要特别注意深度模型的参数数量。
- en: Being a deep model, Inception net leverages 1 × 1 convolution filters within
    Inception blocks to suppress a large increase in parameters. This is done by using
    1 × 1 convolution layers to produce smaller outputs from a larger input and feed
    those smaller outputs as inputs to the convolution sublayers in the Inception
    blocks (figure 6.13). For example, if you have a 10 × 10 × 256-sized input, by
    convolving it with a 1 × 1 convolution layer with 32 filters, you get a 10 × 10
    × 32-sized output. This output is eight times smaller than the original input.
    In other words, a 1 × 1 convolution reduces the channel depth/dimension of large
    inputs.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个深度模型，Inception 网络利用 Inception 块内的 1 × 1 卷积滤波器来抑制参数的大幅增加。通过使用 1 × 1 卷积层，将较大的输入产生较小的输出，并将这些较小的输出作为输入传递给
    Inception 块中的卷积子层（图 6.13）。例如，如果你有一个 10 × 10 × 256 大小的输入，通过将其与具有 32 个滤波器的 1 × 1
    卷积层进行卷积，你将得到一个 10 × 10 × 32 大小的输出。这个输出比原始输入小了八倍。换句话说，1 × 1 卷积减小了大输入的通道深度/维度。
- en: '![06-13](../../OEBPS/Images/06-13.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![06-13](../../OEBPS/Images/06-13.png)'
- en: Figure 6.13 The computations of a 1 × 1 convolution and how it enables reduction
    of a channel dimension of an input
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 1 × 1 卷积的计算以及它如何实现输入通道维度的降维
- en: Thus, it is considered a dimensionality reduction method. The weights of these
    1 × 1 convolutions can be treated just like parameters of the network and let
    the network learn the best values for these filters to solve a given task.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它被认为是一种降维方法。这些 1 × 1 卷积的权重可以被视为网络的参数，并且让网络学习这些滤波器的最佳值来解决给定的任务。
- en: Now it’s time to define a function that represents this new and improved Inception
    block, as shown in the following listing.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候定义一个函数，代表这个新的、改进的 Inception 块了，如下清单所示。
- en: Listing 6.4 Defining the Inception block of the Inception net
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 6.4 定义 Inception 网络的 Inception 块
- en: '[PRE19]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The inception() function takes in some input (four-dimensional: batch, height,
    width, channel, dimensions) and a list of filter sizes for the convolution sublayers
    in the Inception block. This list should have the filter sizes in the following
    format:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The outer loop corresponds to the vertical pillars in the Inception block, and
    the inner loops correspond to the convolution layers in each pillar (*figure 6.14*).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![06-14](../../OEBPS/Images/06-14.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 The Inception block alongside the full architecture of the Inception
    net model
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define the four vertical streams of computations that finally get concatenated
    to one at the end:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The 1 × 1 convolution
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 1 × 1 convolution followed by a 3 × 3 convolution
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 1 × 1 convolution followed by a 5 × 5 convolution
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 3 × 3 pooling layer followed by a 1 × 1 convolution
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematical view of dimensionality reduction using 1 × 1 convolutions
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not a fan of the picturesque method, here’s a more concise and mathematical
    view of how 1 × 1 convolutions reduce dimensions. Say you have an input of size
    10 × 10 × 256\. Say you have a 1 × 1 convolution layer of size 1 × 1 × 32:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Size (input) = 10 × 10 × 256
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size (layer) = 1 × 1 × 32
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can represent your convolution layer as a 1 × 32 matrix. Next, repeat the
    columns on axis = 0 (i.e., row dimension), 256 times which gives us
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Size (input) = 10 × 10 × 256
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size (layer) = 256 × 32
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you can multiply the input with the convolution filter
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Size (output) = (10 × 10 × 256) (256 × 32)
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: which gives us an output of size
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Size (output) = 10 × 10 × 32
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: which is much smaller than the original input.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we concatenate all the outputs of these streams into one on the last
    axis (denoted by axis = -1). Note the last dimension is the channel dimension
    of all the outputs. In other words, we are stacking these outputs on the channel
    dimension. Figure 6.14 illustrates how the Inception block sits in the overall
    Inception net model. Next, we will discuss another component of the Inception
    net model known as auxiliary output layers.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary output layers
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have two auxiliary output layers that help stabilize our deep CNN.
    As mentioned earlier, the auxiliary outputs are there to stabilize the training
    of deep networks. In Inception net, the auxiliary output layer has the following
    (figure 6.15).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: A 5 × 5 average pooling layer
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 1 × 1 convolution layer
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with ReLU activation that consumes the flattened output from the
    1 × 1 convolution layer
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Dense layer with softmax that outputs the probabilities of the classes
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![06-15](../../OEBPS/Images/06-15.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 Auxiliary output layer alongside the full Inception net architecture
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: We define a function that produces the auxiliary output predictions as follows
    (listing 6.5).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Defining the auxiliary output as a Python function
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ The output of the average pooling layer. Note that it uses valid pooling,
    which results in a 4 × 4-sized output for the next layer.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ❷ 1 × 1 convolution layer’s output
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Flattens the output of the convolution layer so that it can be fed to a Dense
    layer
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The first Dense layer’s output
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The final prediction for Dense layer’s output
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: The aux_out() function defines the auxiliary output layer. It starts with an
    average pool layer with a kernel size of (5,5) and strides (3,3) and valid padding.
    This means that the layer does not try to correct for the dimensionality reduction
    introduced while pooling (as done in same padding). Then it’s followed by a convolution
    layer with 128 filters, (1,1) kernel size, ReLU activation, and same padding.
    Then, a Flatten() layer is needed before feeding the output to a Dense layer.
    Remember that the Flatten() layer flattens the height, width, and channel dimension
    to a single dimension. Finally, a Dense layer with 200 nodes and a softmax activation
    is applied. With that, we have all the building blocks to build our very own Inception
    net.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Putting everything together
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have come a long way. Let’s catch our breath and reflect on what we have
    achieved so far:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: The abstract architecture and components of the Inception net model consist
    of a stem, Inception blocks, and auxiliary outputs.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precise details of these components. Stem resembles the stem (everything except
    the fully connected layers) of a standard CNN. The Inception blocks carry sub-convolution
    layers with different kernel sizes that encourage sparsity and reduce overfitting.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The auxiliary outputs make the network training smoother and rid the network
    of any undesirable numerical errors during training.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also defined methods to encapsulate these so that we can call these methods
    and build the full Inception net. Now we can define the full Inception model (see
    the next listing). Additionally, you can find the exact Inception block specifications
    (as per the original paper) summarized in table 6.5.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Defining the full Inception net model
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Defines an input layer. It takes a batch of 64 × 64 × 3-sized inputs.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: ❷ To define the stem, we use the previously defined stem() function.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Defines Inception blocks. Note that each Inception block has different numbers
    of filters.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Defines auxiliary outputs
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The final pooling layer is defined as an Average pooling layer.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: ❻ The Flatten layer flattens the average pooling layer and prepares it for the
    fully connected layers.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: ❼ The final prediction layer that has 200 output nodes (one for each class)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: ❽ When compiling the model, we use categorical cross-entropy loss for all the
    output layers and the optimizer adam.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the model has nine Inception blocks following the original
    paper. In addition, it has the stem, auxiliary outputs, and a final output layer.
    The specifics of the layers are listed in table 6.5.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.5 Summary of the filter counts of the Inception modules in the Inception
    net v1 model. C(nxn) represents a nxn convolution layer, whereas MaxP(mxm) represents
    a mxm max-pooling layer.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '| **Inception layer** | **C(1 × 1)** | **C(1 × 1); before C(3 × 3)** | **C(3
    × 3)** | **C(1 × 1); before C(5 × 5)** | **C(5 × 5)** | **C(1 × 1); after MaxP(3
    × 3)** |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| Inc_3a | 64 | 96 | 128 | 16 | 32 | 32 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| Inc_3b | 128 | 128 | 192 | 32 | 96 | 64 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| Inc_4a | 192 | 96 | 208 | 16 | 48 | 64 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Inc_4b | 160 | 112 | 224 | 24 | 64 | 64 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| Inc_4c | 128 | 128 | 256 | 24 | 64 | 64 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| Inc_4d | 112 | 144 | 288 | 32 | 64 | 64 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| Inc_4e | 256 | 160 | 320 | 32 | 128 | 128 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| Inc_5a | 256 | 160 | 320 | 32 | 128 | 128 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| Inc_5b | 384 | 192 | 384 | 48 | 128 | 128 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: 'The layer definitions will be quite similar to what you have already seen.
    However, the way we define the model and the compilation will be new to some of
    you. As we discussed, Inception net is a multi-output model. You can define the
    Keras model with multiple outputs by passing a list of outputs instead of a single
    output:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When compiling the model, you can define loss as a list of a single string.
    If you define a single string, that loss will be used for all the outputs. We
    compile the model using the categorical cross-entropy loss (for both the final
    output layer and auxiliary outputs) and the optimizer adam, which is a state-of-the-art
    optimizer widely used to optimize models and that can adapt the learning rate
    appropriately as the model trains. In addition, we will inspect the accuracy of
    the model:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With the inception_v1() function defined, you can create a model as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s take a moment to reflect on what we have achieved so far. We have downloaded
    the data, dissected it, and analyzed the data to understand the specifics. Then
    we created an image data pipeline using tensorflow.keras.preprocessing.image.ImageDataGenerator.
    We split the data into three parts: training, validation, and testing. Finally,
    we defined our model, which is a state-of-the art image classifier known as Inception
    net. We will now look at other Inception models that have emerged over the years.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.4 Other Inception models
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We successfully implemented an Inception net model, which covers most of the
    basics we need to understand other Inception models. There have been five more
    Inception nets since the v1 model. Let’s go on a brief tour of the evolution of
    Inception net.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Inception v1
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already discussed Inception net v1 in depth. The biggest breakthroughs
    introduced in Inception net v1 are as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: The concept of an Inception block, which allows the CNN to have different receptive
    fields (i.e., kernels sizes) at the same depth of the model. This encourages sparsity
    in the model, leading to fewer parameters and fewer chances of overfitting.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the 20 layers in the Inception model, the memory of a modern GPU can be
    exhausted if you are not careful. Inception net mitigates this problem by using
    1 × 1 convolution layers to reduce output channel depth whenever it increases
    too much.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deeper the networks are, the more they are prone to having instable gradients
    during model training. This is because the gradients must travel a long way (from
    the top to the very bottom), which can lead to instable gradients. Auxiliary output
    layers introduced in the middle of the network as regularizers alleviate this
    problem, leading to stable gradients.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v2
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Inception net v2 came not long after Inception net v1 was released (“Rethinking
    the Inception Architecture for Computer Vision,” [https://arxiv.org/pdf/1512.00567.pdf](https://arxiv.org/pdf/1512.00567.pdf)).
    The main contributions of this model are as follows.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: A representational bottle neck occurs when a layer does not have enough capacity
    (i.e., parameters) to learn a good representation of the input. This can happen
    if you decrease the size of the layers too fast as you go deep. Inception v2 rejigged
    the architecture to ensure that no representational bottlenecks are present in
    the model. This is mostly achieved by changing the layer sizes while keeping the
    rest of the details the same.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the parameters of the network further to reduce overfitting was reinforced.
    This is done by replacing higher-order convolutions (e.g., 5 × 5 and 7 × 7) with
    3 × 3 convolutions (also known as factorizing large convolution layers). How is
    that possible? Let me illustrate that for you (figure 6.16).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![06-16](../../OEBPS/Images/06-16.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 A 5 × 5 convolution layer (left) with two 3 × 3 convolution layers
    (right)
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: By representing 5 × 5 convolution with two smaller 3 × 3 convolution operations,
    we enjoy a reduction of 28% in parameters. Figure 6.17 contrasts Inception v1
    block with Inception v2 block.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![06-17](../../OEBPS/Images/06-17.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 Inception block in Inception net v1 (left) versus Inception block
    in Inception net v2 (right)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what the TensorFlow code looks like:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: But we don’t have to stop here. We can factorize any n × n convolution operation
    to two 1 × n and n × 1 convolution layers, for example, giving 33% parameter reduction
    for a 3 × 3 convolution layer (figure 6.18). Empirically, it has been found that
    factorizing n × n operation to two 1 × n and n × 1 operations is useful only in
    higher layers. You can refer to the paper to understand when and where these types
    of factorizations are used.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '![06-18](../../OEBPS/Images/06-18.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 A 3 × 3 convolution layer (left) with 3 × 1 and 1 × 3 convolution
    layers (right)
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Inception v3
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Inception v3 was introduced in the same paper as Inception net v2\. The primary
    contribution that sets v3 apart from v2 is the use of batch normalization layers.
    Batch normalization (“Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift,” [http://proceedings.mlr.press/v37/ioffe15.pdf](http://proceedings.mlr.press/v37/ioffe15.pdf))
    normalizes the outputs of a given layer *x* by subtracting the mean (*E*(*x*))
    and standard deviation (√(*Var*(*x*)) from the outputs:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![06_18a](../../OEBPS/Images/06_18a.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'This process helps the network stabilize its output values without letting
    them become too large or too small. Next, it has two trainable parameters, *γ*
    and *β*, that scale and offset the normalized output:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: y = *γ* *x̂* + *β*
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: This way, the network has the flexibility to learn its own variation of the
    normalization by learning optimal *γ* and *β* in case *x̂* is not the optimal
    normalization configuration. At this time, all you need to understand is that
    batch normalization normalizes the output of a given layer in the network. We
    will discuss how batch normalization is used within the Inception net model in
    more detail in the next chapter.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Inception v4
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Inception-v4 was introduced in the paper “Inception-v4, Inception-ResNet and
    the Impact of Residual Connections on Learning” ([http://mng.bz/J28P](http://mng.bz/J28P))
    and does not introduce any new concepts, but rather focuses on making the model
    simpler without sacrificing performance. Mainly, v4 simplifies the stem of the
    network and other elements. As this is mostly grooming the network’s hyperparameters
    for better performance and not introducing any new concepts, we will not dive
    into this model in great detail.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Inception-ResNet v1 and Inception-ResNet v2
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Inception-ResNet v1 and v2 were introduced in the same paper and were its main
    contributions. Inception-ResNet simplifies the Inception blocks that are used
    in the model and removes some cluttering details. More importantly, it introduces
    residual connections. *Residual connections* (or *skip connections*) were introduced
    in the paper by Kaiming He et al. titled “Deep Residual Learning for Image Recognition”
    ([https://arxiv.org/ pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)).
    It’s an elegantly simple concept, yet very powerful, and it has been responsible
    for many of the top-performing models in many different domains.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 6.19, residual connections represent simply adding a lower
    input (close to the input) to a higher-level input (further from the input). This
    creates a shortcut between the lower input and a higher input, essentially creating
    another shortcut from the resulting output to the lower layer. We will not dive
    into too much detail here, as we will discuss Inception-ResNet models in detail
    in the next chapter. Next, we will train the model we just defined on the image
    data we have prepared.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '![06-19](../../OEBPS/Images/06-19.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 How residual connections are introduced to a network. It is a simple
    operation, where you add a lower output (closer to input) of a layer to a higher
    output (further from input). Skip connections can be designed in such a way to
    skip any number of layers you like. The figure also highlights the flow of gradients;
    you can see how skip connections allow gradients to bypass certain layers and
    travel to lower layers.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'As a part of research, you are testing a new technique called *poolception*.
    Conceptually similar to an Inception block, poolception has three parallel pooling
    layers with the following specifications:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: A 3 × 3 max pooling layer with stride 2 and same padding
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 5 × 5 max pooling layer with stride 2 and same padding
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 3 × 3 average pooling layer with stride 2 and same padding
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the outputs of these layers are concatenated on the channel axis. Can
    you implement this as a Python function called poolception that takes the previous
    layer’s input x as an argument?
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Training the model and evaluating performance
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Great work! You have defined one of the state-of-the-art model architectures
    that has delivered great performance on similar (and larger) data sets. Your next
    task is to train this model and analyze its performance.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Model training is an imperative step if you need a model that performs well
    once it’s time to use it. Training the model optimizes (i.e., changes) the parameters
    of the model in such a way that it can produce the correct prediction given an
    input. Typically, model training is done for several epochs, where each epoch
    can consist of thousands of iterations. This process can take anywhere from hours
    to even weeks depending on the size of the data set and the model. As we have
    already discussed, deep neural networks, due to their well-known memory requirements,
    consume data in small batches. A step where the model is optimized with a single
    data batch is known as an *iteration*. When you traverse the full data set in
    such batches, it’s known as an *epoch*.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once the training is done, you need to ensure that the model performs
    well on unseen data. This unseen data must not have had any interaction with the
    model during the training. The most common evaluation metric for deep learning
    networks is accuracy. Therefore, we measure the test accuracy to ensure the model’s
    sturdiness.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to train the model, let us first define a function that computes the
    number of steps or iterations per epoch, given the size of the data set and batch
    size. It’s always a good idea to run for a predefined number of steps for every
    epoch. There can be instances where Keras is unable to figure out the number of
    steps, in which case it can leave the model running until you stop it:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It’s a very simple calculation. The number of steps for an epoch is the number
    of data points (n_data) divided by batch size (batch_size). And if n_data is not
    divisible by batch_size, you need to add 1 to the returned value to make sure
    you’re not leaving any data behind. Now let’s train the model in the following
    listing.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Training the Inception net
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Creates a directory called eval to store the performance results
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: ❷ This is a Keras callback that you pass to the fit() function. It writes the
    metrics data to a CSV file.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: ❸ By fitting the model, you can see that we are passing the train and validation
    data generators to the function.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Saves the model to disk so it can be brought up again if needed
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'When training the model, the following steps are generally followed:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Train the model for a number of epochs.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of every training epoch, measure performance on the validation data
    set.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After all the training epochs have finished, measure the performance on the
    test set.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When model.fit() is called in the code, it takes care of the first two steps.
    We will look at the model.fit() function in a bit more detail. We pass the following
    arguments to the function:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: X—Takes the train data generator to the model, which contains both inputs (x)
    and targets (y).
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y—Typically takes in the targets. Here we do not specify y, as x already contains
    the targets.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: validation_data—Takes the validation data generator.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: steps_per_epoch—Number of steps (iterations) per epoch in training.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: validation_steps—Number of steps (iterations) per epoch in validation.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: epochs—Number of epochs.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: callbacks—Any callbacks that need to be passed to the model (for a full list
    of callbacks visit [http://mng.bz/woEW](http://mng.bz/woEW)).
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should get something like the following after training the model:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 2 hours and 45 minutes. You can reduce the training time by
    cutting down the number of epochs.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will test the trained model on the test data (i.e., the data in
    the val folder). You can easily get the model’s test performance by calling model.evaluate()
    by passing the test data generator (test_gen_aux) and the number of steps (iterations)
    for the test set:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You will get the following output:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can see that the model reaches around 30% validation and test accuracy and
    a whopping ~94% training accuracy. This is a clear indication that we haven’t
    steered clear from overfitting. But this is not entirely bad news. Thirty percent
    accuracy means that the model did recognize around 3,000/10,000 images in the
    validation and test sets. In terms of the sheer data amount, this corresponds
    to 60 classes out of 200.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: NOTE An overfitted model is like a student who memorized all the answers for
    an exam, whereas a generalized model is a student who worked hard to understand
    concepts that will be tested on the exam. The student who memorized answers will
    only perform well in the exam and fail in the real world, whereas the student
    who understood concepts can generalize their knowledge both to the exam and the
    real world.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting can happen for a number of reasons:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: The model architecture is not optimal for the data set we have.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More regularization is needed to reduce overfitting, such as dropout and batch
    normalization.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are not using a pretrained model that has already been trained on similar
    data.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to address each of these concerns in the next chapter, where it
    will be exciting to see how much things improve.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: If you train a model for 10 epochs with a data set that has 50,000 samples with
    a batch size of 250, how many iterations would you train the model for? Assuming
    you are given the inputs as a variable x and labels as a variable y, populate
    the necessary arguments in model.fit() to train the model according to this specification.
    When not using a data generator, you can set the batch size using the batch_size
    argument and ignore the steps_per_epoch argument (automatically inferred) in model.fit().
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA) is a crucial step in the machine learning life
    cycle that must be performed before starting on any modeling.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more aspects of the data you analyze, the better.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras data generator can be used to read images from disk and load them
    into memory to train the model.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception net v1 is one of the state-of-the-art computer vision models for image
    classification designed for reducing overfitting and memory requirements of deep
    models.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception net v1 consists of a stem, several inception blocks, and auxiliary
    outputs.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Inception block is a layer in the Inception net that consists of several
    sub-convolution layers with different kernel sizes, whereas the auxiliary output
    ensures smoothness in model training.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When training a model, there are three data sets: training, validation, and
    test.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, we train the model on training data for several epochs, and at the
    end of every epoch, we measure the performance on the validation set. Finally,
    after the training has finished, we measure performance on the test data set.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model that overfits is like a student who memorized all the answers for an
    exam. It can do very well on the training data but will do poorly in generalizing
    its knowledge to analyze unseen data.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Exercise 2**'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Exercise 3**'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Exercise 4:** Total number of iterations = (data set size/batch_size) * epochs
    = (50,000/250) * 10 = 2,000'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
