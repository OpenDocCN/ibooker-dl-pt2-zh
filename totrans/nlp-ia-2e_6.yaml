- en: 6 Reasoning with word embeddings (word vectors)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词嵌入（词向量）进行推理的6个原因
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Understanding word embeddings or word vectors
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解词嵌入或词向量
- en: Representing meaning with a vector
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用向量表示含义
- en: Customizing word embeddings to create domain-specific nessvectors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制词嵌入以创建特定领域的向量
- en: Reasoning with word embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词嵌入进行推理
- en: Visualizing the meaning of words
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化单词的含义
- en: '*Word embeddings* are perhaps the most approachable and generally useful tools
    in your NLP toolbox. They can give your NLP pipeline a general understanding of
    words. In this chapter you will learn how to apply word embeddings to real world
    applications. And just as importantly you’ll learn where not to use word embeddings.
    And hopefully these examples will help you dream up new and interesting applications
    in business as well as in your personal life.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*词嵌入*可能是你NLP工具箱中最易于接近和普遍有用的工具。它们可以让你的NLP流水线对单词有一个一般的理解。在本章中，你将学习如何将词嵌入应用到现实世界的应用中。同样重要的是，你将学会在哪里不要使用词嵌入。希望这些例子能帮助你在商业和个人生活中构想出新的有趣的应用。'
- en: You can think of word vectors as sorta like lists of attributes for Dota 2 heroes
    or roll playing game (RPG) characters and monsters. Now imagine that there was
    no text on these character sheets or profiles. You would want to keep all the
    numbers in a consistent order so you knew what each number meant. That’s how word
    vectors work. The numbers aren’t labeled with their meaning. They are just put
    in a consistent *slot* or location in the vector. That way when you add or subtract
    or multiply two word vectors together the attribute for "strength" in one vector
    lines up with the strength attribute in another vector. Likewise for "agility"
    and "intelligence" and alignment or philosophy attributes in D&D (Dungeons and
    Dragons).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把单词向量想象成Dota 2英雄或角色扮演游戏（RPG）角色和怪物的属性列表。现在想象一下，这些角色表或简介上没有文字。你会希望保持所有数字的顺序一致，这样你就知道每个数字的含义。这就是词向量的工作方式。这些数字没有标记它们的含义。它们只是放在向量中的一个一致的*位置*或位置。这样，当你将两个单词向量相加、相减或相乘时，一个向量中的“力量”属性就与另一个向量中的力量属性相匹配。同样适用于D&D（龙与地下城）中的“敏捷”、“智力”和阵营或哲学属性。
- en: Thoughtful roll playing games often encourage deeper thinking about philosophy
    and words with subtle combinations of character personalities such "chaotic good"
    or "lawful evil." I am super grateful to my childhood Dungeon Master for opening
    my eyes to the false dichotomies suggested by words like "good" and "evil" or
    "lawful" and "chaotic".^([[1](#_footnotedef_1 "View footnote.")]) The word vectors
    you’ll learn about here have room for every possible quantifiable attribute of
    words you find in almost any text and any language. And the word vector attributes
    or features are intertwined with each other in complex ways that can handle concepts
    like "lawful evil", "benevolent dictator" and "altruistic spite" with ease.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 富有思想的角色扮演游戏经常鼓励对哲学和单词进行更深入的思考，例如"混乱善良"或"法律邪恶"等角色个性的微妙组合。我非常感激我的童年主持人开启了我的眼界，让我看到了像“善”和“恶”或“守法”和“混乱”这样的单词所暗示的错误二分法。[[1](#_footnotedef_1
    "查看脚注。")] 在这里你将学习到的词向量有足够的空间来表达几乎任何文本和任何语言中的单词的每个可能的可量化属性。并且单词向量的属性或特征以复杂的方式相互交织在一起，可以轻松处理诸如“守法邪恶”，“善意的独裁者”和“利他的恶意”等概念。
- en: Learning word embeddings are often categorized as a *representation learning*
    algorithm.^([[2](#_footnotedef_2 "View footnote.")]) The goal of any word embedding
    is to build a compact numerical representation of a word’s "character". These
    numerical representations enable a machine to process your words (or your Dota
    2 character sheet) in a meaningful way.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 学习词嵌入通常被归类为*表示学习*算法。[[2](#_footnotedef_2 "查看脚注。")] 任何词嵌入的目标都是构建一个单词“特征”的紧凑数值表示。这些数值表示使得机器能够以有意义的方式处理你的单词（或你的Dota
    2角色表）。
- en: 6.1 This is your brain on words
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 这是你的单词大脑
- en: Word embeddings are vectors we use to represent meaning. And your brain is where
    meaning is stored. Your brain is "on" words — it is affected by them. Just as
    chemicals affect a brain, so do words. "This is your brain on drugs" was a popular
    slogan of the 80’s anti-narcotics Television advertising campaign that featured
    a pair of eggs sizzling in a frying pan.^([[3](#_footnotedef_3 "View footnote.")])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是我们用来表示含义的向量。而你的大脑是存储含义的地方。你的大脑受到单词的影响。就像化学物质影响大脑一样，单词也会影响大脑。“This is your
    brain on drugs”是80年代反毒品电视广告活动的一句流行口号，其中有一对鸡蛋在煎锅中煎炸。[[3](#_footnotedef_3 "查看脚注。")]
- en: Fortunately words are much more gentle and helpful influencers than chemicals.
    The image of your brain on words shown in figure 6.1 looks a little different
    than eggs sizzling in a frying pan. The sketch gives you one way to imagine the
    neurons sparking and creating thoughts inside your brain as you read one of these
    sentences. Your brain connects the meaning of these words together by firing signals
    to the appropriate neighbor neurons for associated words. Word embeddings are
    vector representations of these connections between words. And so they are also
    a crude representation of the node embeddings for the network of neuron connections
    in your brain.^([[4](#_footnotedef_4 "View footnote.")])
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，文字比化学物质更温和、更有益的影响者。图6.1中显示的文字在你大脑中的形象与鸡蛋在煎锅中滋滋作响有些不同。这张草图为你提供了一种想象的方式，当你阅读这些句子时，你的神经元会火花四溅，创造出你大脑内的思维。你的大脑通过向适当的邻近神经元发送信号将这些词的意义连接在一起。词嵌入是这些单词之间连接的矢量表示。因此，它们也是你大脑中神经元连接网络的节点嵌入的一种粗略表示。^([[4](#_footnotedef_4
    "View footnote.")])
- en: Figure 6.1 Word embeddings in your brain
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 你大脑中的词嵌入
- en: '![word brain embedding drawio](images/word-brain-embedding_drawio.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![word brain embedding drawio](images/word-brain-embedding_drawio.png)'
- en: You can think of a word embedding as a vector representation of the pattern
    of neurons firing in your brain when you think about an individual word. Whenever
    you think of a word, the thought creates a wave of electrical charges and chemical
    reactions in your brain originating at the neurons associated with that word or
    thought. Neurons within your brain fire in waves, like the circluar ripples emanating
    out from a pebble dropped in a pond. But these electrical signals are selectively
    flowing out through some neurons and not others.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当你思考一个单词时，你可以把词嵌入看作是你大脑中神经元触发模式的矢量表示。每当你想到一个词，这个思想就会在你的大脑中引发一波电荷和化学反应，在与该词或思想相关联的神经元开始。你的大脑内部的神经元像水池中扔下的圆形涟漪一样波动。但是，这些电信号只有从某些神经元流出，而不是其他神经元。
- en: As you read the words in this sentence you are sparking flashes of activity
    in your neurons like those in the sketch in figure [6.1](#word_brain_embedding_figure).
    In fact, researchers have found surprising similarity in the patterns of artificial
    neural network weights for word embeddings, and the patterns of activity within
    your brain as you think about words.^([[5](#_footnotedef_5 "View footnote.")])
    ^([[6](#_footnotedef_6 "View footnote.")])
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读这个句子中的词时，你会在你的神经元中引发一连串的活动，就像[图6.1](#word_brain_embedding_figure)中的草图中那样。事实上，研究人员发现人工神经网络权重与词嵌入相似的模式和你思考词语时大脑内部活动的模式。^([[5](#_footnotedef_5
    "View footnote.")]) ^([[6](#_footnotedef_6 "View footnote.")])
- en: Electrons flowing out from neurons are like children running out of a school
    doorway when the school bell rings for recess. The word or thought is like the
    school bell. Of course your thoughts and the electrons in your brain are much
    faster than students. You don’t even have to speak or hear the word to trigger
    its pattern in your brain. You just have to think it. And like kids running out
    to the playground, the electrons never flow along the same paths twice. Just as
    the meaning of a word evolves over time, your embedding of a word is constantly
    evolving. Your brain is a never-ending language learner not too different from
    Cornell’s Never Ending Language Learner system.^([[7](#_footnotedef_7 "View footnote.")])
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元中流出的电子就像学生在放学铃声响起时从学校门口跑出来一样。词语或思想就像学校的铃声。当然，你的思维和大脑中的电子比学生要快得多。你甚至不需要说出或听到这个词，就能在你的大脑中触发它的模式。你只需要想一想它。就像孩子们跑出去玩耍一样，电子永远不会沿着相同的路径流动。正如一个词的意义随着时间的推移而演变，你对一个词的嵌入也在不断演变。你的大脑是一个永不停止的语言学习者，与康奈尔大学的无止境语言学习者系统并没有太大的不同。^([[7](#_footnotedef_7
    "View footnote.")])
- en: Some people have gotten carried away with this idea, and they imagine that you
    can accomplish a form of mind control with words. When I was looking for information
    about NLP research on Reddit I got distracted by the `r/NLP` subreddit. It’s not
    what you think. It turns out that some motivational speakers have name-squatted
    the word "NLP" on reddit for their 70’s era "Neuro-linguistic Programming" money-making
    schemes.^([[8](#_footnotedef_8 "View footnote.")]) ^([[9](#_footnotedef_9 "View
    footnote.")]) ^([[10](#_footnotedef_10 "View footnote.")]) Fortunately word embeddings
    can handle this ambiguity and misinformation just fine.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人对这个想法产生了过多的幻想，他们认为你可以用言辞实现一种形式的心灵控制。当我在 Reddit 上寻找关于 NLP 研究的信息时，我被 `r/NLP`
    子论坛分散了注意力。这不是你想的那样。事实证明，一些励志演讲者在 Reddit 上为他们的70年代的"神经语言编程"赚钱计划占了" NLP"这个词。幸运的是，词向量能够很好地处理这种歧义和错误信息。
- en: You don’t even have to tell the word embedding algorithm what you want the word
    "NLP" to mean. It will figure out the most useful and popular meaning of the acronym
    based on how it is used within the text that you use to train it. The algorithm
    for creating word embeddings is a self-supervised machine learning algorithm.
    This means you will not need a dictionary or thesaurus to feed your algorithm.
    You just need a lot of text. Later in this chapter you will just gather up a bunch
    of Wikipedia articles to use as your training set. But any text in any language
    will do, as long as contains a lot of words that you are interested in.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至不需要告诉词向量算法你希望" NLP"这个词的含义是什么。它将根据你用于训练它的文本中它的用法找出这个缩写词最有用和最流行的含义。创建词向量的算法是一种自监督的机器学习算法。这意味着你不需要词典或同义词词典来喂养你的算法。你只需要很多文本。在本章的后面，你只需收集一堆维基百科文章来用作你的训练集。但任何语言的任何文本都可以，只要它包含了你感兴趣的很多单词。
- en: There’s another "brain on words" to think about. Words not only affect the way
    you think but they affect how you communicate. And you are sorta like a neuron
    in the collective consciousness, the brain of society. That "sorta" word is an
    especially powerful pattern of neural connections for me, because I learned what
    it means from Daniel Dennet’s *Intuition Pumps* book.^([[11](#_footnotedef_11
    "View footnote.")]) It invokes associations with complex ideas and words such
    as the concept of "gradualism" used by Turing to explain how the mechanisms behind
    both AI and a calculator are exactly the same. Darwin used this concept of gradualism
    to explain how language-comprehending human brains can evolve from single cell
    organisms through simple mechanisms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个需要考虑的"大脑上的文字"。文字不仅影响你的思维方式，还影响你的交流方式。而且你有点像是集体意识中的一个神经元，是社会的大脑。对我来说，"有点"这个词是一个特别强大的神经连接模式，因为我是从丹尼尔·丹尼特的《直觉泵》一书中学到了它的含义。它唤起了与复杂思想和词语相关联的联想，比如图灵用来解释
    AI 和计算器背后机制完全相同的概念"渐进主义"。达尔文使用渐进主义这个概念来解释语言理解人类大脑如何通过简单机制从单细胞生物进化而来。
- en: 6.2 Applications
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 应用
- en: 'Well, what are these awesome word embeddings good for? Word embeddings can
    be used anywhere you need a machine to understand words or short N-grams. Here
    are some examples of N-grams where word embeddings haven proven useful in the
    real world:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，这些令人惊叹的词向量有什么用？词向量可以在需要机器理解单词或短 N-gram 的任何地方使用。以下是一些词向量在现实世界中已被证明有用的 N-gram
    的示例：
- en: Hashtags
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签
- en: Tags and Keywords
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签和关键词
- en: Named entities (People, Places, Things)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体（人、地点、事物）
- en: Titles (Songs, Poems , Books, Articles)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题（歌曲、诗歌、书籍、文章）
- en: Job titles & business names
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 职称和企业名称
- en: Web page titles
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网页标题
- en: Web URLs and file paths
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网页 URL 和文件路径
- en: Wikipedia article titles
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wikipedia 文章标题
- en: 'Even there are many practical applications where your NLP pipeline could take
    advantage of the ability to understand these phrases using word embeddings:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至有许多实际应用场景，您的 NLP 流水线可以利用词向量理解这些短语的能力：
- en: Semantic search for jobs, web pages, …​
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义搜索工作、网页等等
- en: Tip-of-your-tongue word finder
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 舌尖上的词查找器
- en: Rewording a title or sentence
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改写标题或句子
- en: Sentiment shaping
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感塑造
- en: Answer word analogy questions
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答词类比问题
- en: Reasoning with words and names
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用文字和名称推理
- en: 'And in the academic world researchers use word embeddings to solve some of
    the 200+ NLP problems: ^([[12](#_footnotedef_12 "View footnote.")])'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术界，研究人员使用词向量解决了200多个 NLP 问题：
- en: Part-of-Speech tagging
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Named Entity Recognition (NER)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）
- en: Analogy querying
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类比查询
- en: Similarity querying
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度查询
- en: Transliteration
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音译
- en: Dependency parsing
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖关系解析
- en: 6.2.1 Search for meaning
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 寻求含义
- en: In the old days (20 years ago) search engines tried to find all the words you
    typed based on their TF-IDF scores in web pages. And good search engines attempted
    would augment your search terms with synonyms. They would sometimes even alter
    your words to guess what you actually "meant" when you typed a particular combination
    of words. So if you searched for "sailing cat" they would change cat to catamaran
    to disambiguate your search for you. Behind the scenes, while ranking your results,
    search engines might even change a query like "positive sum game" to "nonzero
    sum game" to send you to the correct Wikipedia page.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去（20 年前），搜索引擎尝试根据网页中的 TF-IDF 分数找到您输入的所有单词。良好的搜索引擎尝试将您的搜索词与同义词一起增强。它们有时甚至会改变您的单词，以猜测您实际上在输入特定的单词组合时“意味着”什么。因此，如果您搜索“sailing
    cat”，它们会将“cat”更改为“catamaran”，以为您消除歧义。在幕后，在排名结果时，搜索引擎甚至可能将像“positive sum game”的查询更改为“nonzero
    sum game”，以将您发送到正确的维基百科页面。
- en: Then information retrieval researchers discovered how to make latent semantic
    analysis more effective — word embeddings. In fact, the GloVE word embedding algorithm
    is just latent semantic analysis on millions of sentences extracted from web pages.^([[13](#_footnotedef_13
    "View footnote.")]) These new word embeddings (vectors) made it possible for search
    engines to directly match the "meaning" of your query to web pages, without having
    to guess your intent. The embeddings for your search terms provide a direct numerical
    representation of the *intent* of your search based on the average meaning of
    those words on the Internet.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索研究人员随后发现了如何使潜在语义分析更加有效——词嵌入。事实上，GloVE 词嵌入算法只是从网页中提取的数百万句子上的潜在语义分析。[[13]](#_footnotedef_13)
    这些新的词嵌入（向量）使得搜索引擎能够直接将你的查询的“含义”与网页匹配，而不需要猜测你的意图。你的搜索词的嵌入提供了你的搜索意图的直接数值表示，基于这些单词在互联网上的平均含义。
- en: Warning
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Word embeddings do not represent *your* intended interpretation of words. They
    represent the average meaning of those words for everyone that composed the documents
    and pages that were used to train the word embedding language model. This means
    that word embeddings contain all the biases and stereotypes of all the people
    that composed the web pages used to train the model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入不代表*您*对单词的预期解释。它们代表了用于训练词嵌入语言模型的文档和页面的所有人的单词的平均含义。这意味着词嵌入包含了用于训练模型的网页的所有人的偏见和刻板印象。
- en: Search engines no longer need to do synonym substitution, stemming, lemmatization,
    case-folding and disambiguation based on hard-coded rules. They create word embeddings
    based on the text in all the pages in their search index. Unfortunately the dominant
    search engines decided to use this new-found power to match word embeddings with
    products and ads rather than real words. Word embeddings for AdWords and iAds
    are weighted based on how much a marketer has paid to distract you from your intended
    search. Basically, big tech makes it easy for corporations to bribe the search
    engine so that it manipulates you and trains you to become their consumption zombie.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎不再需要进行同义词替换、词干提取、词形还原、大小写转换和基于硬编码规则的消歧义。它们基于搜索索引中所有页面的文本创建词嵌入。不幸的是，主导性的搜索引擎决定利用这种新发现的力量，将词嵌入与产品和广告匹配，而不是真实的单词。AdWords
    和 iAds 的单词嵌入根据营销人员支付的费用加权，以转移你的注意力，使你远离你的真实搜索意图。基本上，大型科技公司使企业能够贿赂搜索引擎，以操纵你并训练你成为他们的消费僵尸。
- en: 'If you use a more honest search engine such as Startpage,^([[14](#_footnotedef_14
    "View footnote.")]) DISROOT,^([[15](#_footnotedef_15 "View footnote.")]) or Wolfram
    Alpha ^([[16](#_footnotedef_16 "View footnote.")]) you will find they give you
    what you’re actually looking for. And if you have some dark web or private pages
    and documents you want to use a as a knowledge base for your organization or personal
    life you can self-host a search engine with cutting edge NLP: Elastic Search,^([[17](#_footnotedef_17
    "View footnote.")]) Meilisearch,^([[18](#_footnotedef_18 "View footnote.")]) SearX,^([[19](#_footnotedef_19
    "View footnote.")]) Apache Solr,^([[20](#_footnotedef_20 "View footnote.")]) Apache
    Lucene,^([[21](#_footnotedef_21 "View footnote.")]), Qwant,^([[22](#_footnotedef_22
    "View footnote.")]) or Sphinx.^([[23](#_footnotedef_23 "View footnote.")]) Even
    PostgreSQL beats the major search engines for full-text search precision. It will
    surprise you how much clearer you see the world when you are using an honest-to-goodness
    search engine.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用一个更诚实的搜索引擎，如Startpage，^([[14](#_footnotedef_14 "查看脚注。")]) DISROOT，^([[15](#_footnotedef_15
    "查看脚注。")]) 或 Wolfram Alpha ^([[16](#_footnotedef_16 "查看脚注。")])，你会发现它们会给你真正想要的东西。如果你有一些黑网页或私人文件，你想将它们用作组织或个人生活的知识库，你可以自主托管一个具备尖端NLP技术的搜索引擎：Elastic
    Search，^([[17](#_footnotedef_17 "查看脚注。")]) Meilisearch，^([[18](#_footnotedef_18
    "查看脚注。")]) SearX，^([[19](#_footnotedef_19 "查看脚注。")]) Apache Solr，^([[20](#_footnotedef_20
    "查看脚注。")]) Apache Lucene，^([[21](#_footnotedef_21 "查看脚注。")]) Qwant，^([[22](#_footnotedef_22
    "查看脚注。")]) 或 Sphinx。^([[23](#_footnotedef_23 "查看脚注。")])即使是PostgreSQL也能在全文搜索精度方面胜过主流搜索引擎。当你使用一个彻头彻尾诚实的搜索引擎时，你会惊讶地发现你对这个世界的看法变得更加清晰。
- en: These semantic search engines use vector search under the hood to query a word
    and document embedding (vector) database.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语义搜索引擎使用向量搜索来查询单词和文档嵌入（向量）数据库。
- en: Open source Python tools such as NBOOST or PynnDescent let you integrate word
    embeddings with into your favorite TF-IDF search algorithm.^([[24](#_footnotedef_24
    "View footnote.")]) Of if you want a scalable way to search your fine tuned embeddings
    and vectors you can use Approximate Nearest Neighbor algorithms to index whatever
    vectors your like.^([[25](#_footnotedef_25 "View footnote.")])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 开源的Python工具，比如 NBOOST 或 PynnDescent，让你可以将单词嵌入与你喜欢的TF-IDF搜索算法集成起来。^([[24](#_footnotedef_24
    "查看脚注。")])如果你想要一种可扩展的方法来搜索你精细调整过的嵌入和向量，你可以使用近似最近邻算法来索引你喜欢的向量。^([[25](#_footnotedef_25
    "查看脚注。")])
- en: That’s the nice thing about word embeddings. All that vector algebra math you
    are used to, such as calculating distance, that will also work for word embeddings.
    Only now that distance represents how far apart the words are in *meaning* rather
    than physical distance. And these new embeddings are much more compact and dense
    with meaning than than the thousands of dimensions you are used to with TF-IDF
    vectors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是单词嵌入的优点。所有你所熟悉的向量代数数学知识，如计算距离，同样适用于单词嵌入。只不过这个距离现在代表的是单词在*意义*上的距离，而不是物理距离。而且这些新的嵌入比你熟悉的
    TF-IDF 向量中的数千个维度更紧凑和更加有意义。
- en: You can use the meaning distance to search a database of words for all job titles
    that are *near* the job title you had in mind for your job search. This may reveal
    additional job titles you hadn’t even thought of. Or your search engine could
    be designed to add additional words to your search query to make sure related
    job titles were returned. This would be like an autocomplete search box that understands
    what words mean - called *semantic search*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用“意义距离”在单词数据库中搜索所有与你心中想要的职业名称*相近*的职位名称，这可能会揭示出一些你没有想到的职业名称。或者你的搜索引擎可以被设计为在搜索查询中添加其他词语，以确保返回相关的职业标题。这就像是一个可以理解词义的自动完成搜索框，被称为*语义搜索*。
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can see that finding the nearest neighbors of a word embedding is kind of
    like looking up a word in a Thesaurus. But this is a much fuzzier and complete
    thesaurus than you’ll find at your local book shop or online dictionary. And you
    will soon see how you can customize this dictionary to work within any domain
    you like. For example you could train it to work with job postings only from the
    UK or perhaps even India or Australia, depending on your region of interest. Or
    you could train it to work better with tech jobs in Silicon Valley rather than
    finance and banking jobs in New York. You can even train it on 2-grams and 3-grams
    if you want it to work on longer job titles like "Software Developer" or "NLP
    Engineer".
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，查找词嵌入的最近邻居有点像在同义词词典中查找一个词。但这是一个比您当地书店或在线词典更模糊且完整的词典。您很快就会看到如何自定义此词典以适应您喜欢的任何领域。例如，您可以训练它只与英国的职位发布一起使用，或者甚至是印度或澳大利亚，具体取决于您的兴趣区域。或者您可以训练它更好地处理硅谷的技术工作而不是纽约的金融和银行工作。您甚至可以训练它处理2元组和3元组，如果您想要它处理更长的职位头衔，比如"软件开发人员"或"NLP工程师"。
- en: 'Another nice thing about word embeddings is that they are *fuzzy*. You may
    have noticed several nearby neighbors of "Engineer" that you’d probably not see
    in a thesaurus. And you can keep expanding the list as far as you like. So if
    you were thinking of a Software Engineer rather than an Architect you might want
    to scan the `get_nearest()` list for another word to do a search for, such as
    "Programmer":'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关于词嵌入的另一个好处是它们是*模糊的*。您可能已经注意到了“工程师”的几个附近邻居，这些邻居在同义词词典中可能找不到。而且您可以根据需要不断扩展列表。所以如果你在考虑一个软件工程师而不是一名建筑师，你可能会想要扫描`get_nearest()`列表以寻找另一个单词进行搜索，比如"程序员"：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Well that’s surprising. It seems that the title "Developer" is often also associated
    with the word "Publisher." I would have never guessed why this would be before
    having worked with the Development Editors, Development Managers, and even a Tech
    Development Editor at Manning Publishing. Just today these "Developers" cracked
    the whip to get me moving on writing this Chapter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这很奇怪。看起来"开发者"这个头衔经常也与"出版商"这个词联系在一起。在与Manning出版公司的开发编辑、开发经理甚至技术开发编辑一起工作之前，我从来没有想过为什么会这样。就在今天，这些"开发者"催促我加紧写这一章节。
- en: 6.2.2 Combining word embeddings
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 合并词嵌入
- en: 'Another nice thing about word embeddings is that you can combine them any way
    you like to create new words! Well, of course, you can combine multiple words
    the old fashioned way just appending the strings together. In Python you do that
    with addition or the `+` operator:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 关于词嵌入的另一个好处是您可以以任何您喜欢的方式将它们组合起来创建新单词！嗯，当然，您可以以传统的方式将多个单词组合在一起，只需将字符串附加在一起。在Python中，您可以使用加法或`+`运算符来实现这一点：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Word embedding math works even better than that. You can add the meanings of
    the words together to try to find a single word that captures the meaning of the
    two words you added together
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入数学运算的效果甚至比这更好。您可以将这些词的含义相加，试图找到一个单词来捕捉您所添加的两个单词的含义
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So if you want to one day become a "Chief Engineer" it looks like "Scientist",
    "Architect", and "Deputy" might also be job titles you’ll encounter along the
    way.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你想有一天成为一名"首席工程师"，看起来"科学家"、"建筑师"和"副手"也可能是你在路上会遇到的职位头衔。
- en: 'What about that tip-of-your-tongue word finder application mentioned at the
    beginning of this chapter? Have you ever tried to recall a famous person’s name
    while only have a general impression of them, like maybe this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在本章开头提到的那个尖端的词语查找应用程序呢？您是否曾经试图回忆起一个著名人士的名字，只是对他们有一个笼统的印象，比如说这样：
- en: She invented something to do with physics in Europe in the early 20th century.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 早在20世纪初，她在欧洲发明了与物理有关的东西。
- en: If you enter that sentence into Google or Bing, you may not get the direct answer
    you are looking for, "Marie Curie." Google Search will most likely only give you
    links to lists of famous physicists, both men and women.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将该句输入Google或Bing，您可能不会得到您正在寻找的直接答案，"玛丽·居里"。Google搜索很可能只会给您一些著名物理学家的列表链接，包括男性和女性。
- en: You would have to skim several pages to find the answer you are looking for.
    But once you found "Marie Curie," Google or Bing would keep note of that. They
    might get better at providing you search results the next time you look for a
    scientist. (At least, that is what it did for us in researching this book. We
    had to use private browser windows to ensure that your search results would be
    similar to ours.)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要浏览几页才能找到你要的答案。但是一旦你找到“Marie Curie”，Google或Bing就会记住它。下次你寻找一个科学家的时候，它们可能会在提供你搜索结果方面变得更加优秀。（至少，在我们研究这本书时，它对我们提供的搜索结果做到了这一点。我们不得不使用私人浏览器窗口来确保您的搜索结果与我们的相似。）
- en: 'With word embeddings, you can search for words or names that combine the meaning
    of the words "woman," "Europe," "physics," "scientist," and "famous," and that
    would get you close to the token "Marie Curie" that you are looking for. And all
    you have to do to make that happen is add up the vectors for each of those words
    that you want to combine:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过词向量，你可以搜索结合了“woman”、“Europe”、“physics”、“scientist”和“famous”的词汇或名称，并且这会让你接近你寻找的“Marie
    Curie”这个词语的记号。而想要实现这一点，你只需将你想要组合的每个单词的向量相加即可：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this chapter, we show you the exact way to do this query. You can even see
    how you might be able to use word embedding math to subtract out some of the gender
    bias within a word:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这章节中，我们会向你展示确切的方法来做这个查询。你甚至可以看到如何使用词向量数学来消除一些单词中的性别偏见：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With word embeddings, you can take the "man" out of "woman"!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过词向量，你可以从“女人”中去掉“男人”！
- en: 6.2.3 Analogy questions
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 类比题
- en: 'What if you could rephrase your question as an analogy question? What if your
    "query" was something like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你把你的问题转化为一个类比题，会怎样？你的“查询”会变成这样：
- en: Who is to nuclear physics what Louis Pasteur is to germs?
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谁与核物理学的关系，如同Louis Pasteur与细菌的关系？
- en: 'Again, Google Search, Bing, and even Duck Duck Go are not much help with this
    one.^([[26](#_footnotedef_26 "View footnote.")]) But with word embeddings, the
    solution is as simple as subtracting "germs" from "Louis Pasteur" and then adding
    in some "physics":'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，Google搜索、Bing甚至Duck Duck Go对这个问题并没有太大帮助^([[26](#_footnotedef_26 "查看脚注。")])。但是用词向量，解决方案就像是从“germs（细菌）”中减去“Louis
    Pasteur”，然后加入一些“physics（物理学）”：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And if you are interested in trickier analogies about people in unrelated fields,
    such as musicians and scientists, you can do that, too.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对无关领域的人，比如音乐家和科学家的笼统类比题感兴趣，你也可以参与其中。
- en: Who is the Marie Curie of music?
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谁是音乐界的玛丽·居里？
- en: OR
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: Marie Curie is to science as who is to music?
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 玛丽·居里与科学的关系，如同谁与音乐的关系？
- en: Can you figure out what the vector space math would be for that question?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你能搞清楚那个问题的向量空间数学会是什么吗？
- en: 'You might have seen questions like these on the English analogy section of
    standardized tests such as SAT, ACT, or GRE exams. Sometimes they are written
    in formal mathematical notation like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在英语类比题的标准化测试中见过这样的问题，比如SAT、ACT或GRE考试。有时它们是用正式的数学符号写成这样的：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Does that make it easier to guess the vector math for these words? One possibility
    is this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是否会更容易猜出这些单词的向量数学？一种可能性是这样的：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And you can answer questions like this for things other than people and occupations,
    like perhaps sports teams and cities:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 而且你可以回答关于人和职业以外的事物的类似问题，比如体育队和城市：
- en: The Timbers are to Portland as what is to Seattle?"
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: The Timbers are to Portland as what is to Seattle?
- en: 'In standardized test form, that is:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准化测试形式中，如下所示：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'But, more commonly, standardized tests use English vocabulary words and ask
    less fun questions, like the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，更常见的是，标准化测试使用英语词汇词汇，并提问一些没有趣味性的问题，例如以下问题：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: OR
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: All those "tip of the tongue" questions are a piece of cake for word embeddings,
    even though they are not multiple choice. It can be difficult to get analogy questions
    right, even when you have multiple choice options to choose from. NLP comes to
    the rescue with word embeddings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些“口头禅”式的问题对于词向量来说是小菜一碟，即使没有多个选项供你选择，回答类比题仍然很困难。自然语言处理会通过词向量来解救你。
- en: Word embeddings can be used to answer even these vague questions and analogy
    problems. Word embeddings can help you remember any word or name on the tip of
    your tongue, as long as the vector for the answer exists in your vocabulary. (For
    Google’s pretrained Word2Vec model, your word is almost certainly within the 100B
    word news feed that Google trained it on, unless your word was created after 2013.)
    And embeddings work well even for questions that you cannot even pose in the form
    of a search query or analogy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入可以用来回答甚至这些模糊的问题和类比问题。 只要答案的向量存在于您的词汇表中，词嵌入就可以帮助您记住您舌尖上的任何单词或名称。（对于谷歌预先训练的Word2Vec模型，您的单词几乎肯定存在于谷歌训练的100亿字的新闻源中，除非您的单词是在2013年之后创建的。）并且嵌入甚至可以处理您甚至无法以搜索查询或类比形式提出的问题。
- en: You can learn about some of the math with embeddings in the "analogical reasoning"
    section later in this chapter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章稍后的“类比推理”部分了解一些嵌入数学知识。
- en: 6.2.4 Word2Vec Innovation
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 Word2Vec 创新
- en: Words that are used near each other sort of pile up on top of each other in
    our minds and eventually define what those words mean within the connections of
    the neurons of our brains. As a toddler you hear people talking about things like
    "soccer balls," "fire trucks," "computers," and "books," and you can gradually
    figure out what each of them is. The surprising thing is that your machine does
    not need a body or brain to understand words as well as a toddler.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的大脑神经元的连接中，彼此靠近使用的单词会堆积在一起，并最终定义这些单词在我们大脑中的含义。 作为一个幼儿，您会听到人们谈论“足球”，“消防车”，“计算机”和“书籍”之类的东西，您可以逐渐弄清楚它们的含义。
    令人惊讶的是，您的机器不需要身体或大脑来理解单词，就像幼儿一样。
- en: A child can learn a word after pointing out objects in the real world or a picture
    book a few times. A child never needs to read a dictionary or thesaurus. Like
    a child, a machine "figures it out" without a dictionary or thesaurus or any other
    supervised machine learning dataset. A machine does not even need to see objects
    or pictures. The machine is completely self-supervised by the way you parse the
    text and set up the dataset. All you need is a lot of text.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 孩子可以在现实世界或图片书中指出几次物体后学会一个单词。 孩子永远不需要阅读字典或同义词词典。 就像一个孩子一样，机器“弄清楚”而不需要字典、同义词词典或任何其他受监督的机器学习数据集。
    机器甚至不需要看到物体或图片。 该机器完全是由您解析文本和设置数据集的方式进行自我监督的。 您只需要大量的文本。
- en: In previous chapters, you could ignore the nearby context of a word. All you
    needed to do was count up the uses of a word within the same *document*. It turns
    out, if you make your documents very very short, these counts of co-occurrences
    become useful for representing the meaning of words themselves. This was the key
    innovation of Tomas Mikolov and his Word2vec NLP algorithm. John Rubert Firth
    popularized the concept that "a word is characterized by the company it keeps."^([[27](#_footnotedef_27
    "View footnote.")]) But to make word embeddings useful required Tomas Mikolov’s
    focus on a very small "company" of words and the computational power of 21st century
    computers as well as massive corpora machine-readable text. You do not need a
    dictionary or thesaurus to train your word embeddings. You only need a large body
    of text.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，您可以忽略单词的附近上下文。 您所需做的只是计算在同一*文档*中使用单词的次数。 结果，如果您使您的文档非常非常短，这些共现次数就变得有用起来，用于表示单词本身的含义。
    这是Tomas Mikolov及其Word2vec NLP算法的关键创新。 John Rubert Firth推广了“一个单词由其周围的公司所特征化”的概念。[[27](#_footnotedef_27
    "查看脚注。")])但要使词嵌入有用，则需要Tomas Mikolov专注于非常小的单词“公司”以及21世纪计算机的计算能力以及大量的机器可读文本语料库。
    您不需要字典或同义词词典来训练您的词嵌入。 您只需要大量的文本。
- en: That is what you are going to do in this chapter. You are going to teach a machine
    to be a sponge, like a toddler. You are going to help machines figure out what
    words mean, without ever explicitly labeling words with their dictionary definitions.
    All you need is a bunch of random sentences pulled from any random book or web
    page. Once you tokenize and segment those sentences, which you learned how to
    do in previous chapters, your NLP pipeline will get smarter and smarter each time
    it reads a new batch of sentences.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你在本章要做的事情。 你要教会机器成为一个海绵，就像一个幼儿一样。 您要帮助机器弄清楚单词的含义，而无需明确标记单词及其词典定义。 您只需要从任意随机书籍或网页中提取一堆随机句子。
    一旦你对这些句子进行了分词和分段，这是您在前几章中学到的，您的NLP流水线将在每次读取新一批句子时变得越来越聪明。
- en: In Chapters 2 and 3 you isolated words from their neighbors and only worried
    about whether they were present or absent in each *document*. You ignored the
    effect the neighbors of a word have on its meaning and how those relationships
    affect the overall meaning of a statement. Our bag-of-words concept jumbled all
    the words from each document together into a statistical bag. In this chapter,
    you will create much smaller bags of words from a "neighborhood" of only a few
    words, typically fewer than ten tokens. You will also ensure that these neighborhoods
    have boundaries to prevent the meaning of words from spilling over into adjacent
    sentences. This process will help focus your word-embedding language model on
    the words that are most closely related to one another.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章和第 3 章中，你将词语与其邻居隔离开，并只关心每个 *文档* 中是否存在。你忽略了一个词的邻居对其意义的影响以及这些关系如何影响陈述的整体意义。我们的词袋概念将每个文档中的所有词混在一起形成一个统计性的袋子。在本章中，你将从只有少于十个标记的“邻域”中创建更小的词袋。你还将确保这些邻域有边界，以防止词语的意义溢出到相邻的句子中去。这个过程将有助于将你的词嵌入语言模型聚焦在彼此最相关的词上。
- en: Word embeddings can help you identify synonyms, antonyms, or words that just
    belong to the same category, such as people, animals, places, plants, names, or
    concepts. We could do that before, with semantic analysis in Chapter 4, but your
    tighter limits on a word’s neighborhood will be reflected in tighter accuracy
    on the word embeddings. Latent semantic analysis (LSA) of words, *n*-grams, and
    documents did not capture all the literal meanings of a word, much less the implied
    or hidden meanings. Some of the connotations of a word are fuzzier for LSA’s oversized
    bags of words.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入可以帮助你识别同义词、反义词，或者只是属于同一类别的词，比如人、动物、地点、植物、名字或概念。在第四章的语义分析中我们以前也可以做到这一点，但是对词语邻近性的更严格的限制将体现在词嵌入的更严格的准确性上。词语的潜在语义分析（LSA）没有捕捉到词语的所有字面意义，更不用说暗示或隐藏的意义了。一些词语的内涵对于
    LSA 过大的词袋来说更加模糊。
- en: Word embeddings
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word embeddings (sometimes called *word vectors*) are high-dimensional numerical
    vector representations of what a word means, including its literal and implied
    meaning. So word embeddings can capture the *connotation* of words. Somewhere
    inside an embedding, there is a score for "peopleness," "animalness," "placeness,"
    "thingness" and even "conceptness." And a word embedding combines all those scores,
    and all the other *ness* of words, into a dense vector (no zeros) of floating
    point values.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入（有时被称为 *词向量*）是词义的高维数值向量表示，包括其字面和隐含的含义。因此，词嵌入可以捕捉词语的 *内涵*。在一个嵌入的某个地方，有一个关于“人性”、“动物性”、“地方性”、“物品性”甚至“概念性”的分数。词嵌入将所有这些分数以及其他词性的分数组合成一个密集的浮点值向量（没有零）。
- en: The density and high (but not too high) dimensionality of word embeddings is
    a source of their power as well as their limitations. This is why dense, high-dimensional
    embeddings are most valuable when you use them in your pipeline along side sparse
    hyper-dimensional TFIDF vectors or discrete bag-of-words vectors.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入的密度和高（但不要太高）维度既是它们的强大之处，也是它们的局限之所在。这就是为什么在你的管道中同时使用密集的、高维度的嵌入和稀疏的超高维 TFIDF
    向量或离散的词袋向量时，密集的、高维度的嵌入最有价值。
- en: 6.3 Artificial Intelligence Relies on Embeddings
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 人工智能依赖于嵌入
- en: Word embeddings were a big leap forward in not only natural language understanding
    accuracy but also a breakthrough in the hope for Artificial General Intelligence,
    or AGI.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入不仅是自然语言理解准确性的一大飞跃，也是对人工通用智能（AGI）希望的一次突破。
- en: Do you think you could tell the difference between intelligent and unintelligent
    messages from a machine? It may not be as obvious as you think. Even the "deep
    minds" at BigTech were fooled by the surprisingly unintelligent answers from their
    latest and greatest chatbots in 2023, Bing and Bard. Simpler, more authentic conversational
    search tools such as you.com and neeva.com and their chat interfaces outperform
    BigTech search on most Internet research tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为你能区分出机器智能和非智能信息的区别吗？这可能没有你想象的那么明显。即使是大科技公司的“深度思维”也被他们最新最伟大的聊天机器人——2023年的
    Bing 和 Bard 给愚弄了。更简单、更真实的对话式搜索工具，如 you.com 和 neeva.com 及其聊天界面，在大多数互联网研究任务中都胜过了大科技公司的搜索。
- en: 'The philosopher Douglas Hofstader pointed out a few things to look out for
    when measuring intelligence. footnote[Douglas R. Hofstadter, "Gödel, Escher, Bach:
    an Eternal Golden Braid (GEB), p. 26]'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 哲学家道格拉斯·霍夫斯塔特在衡量智能时指出了一些要注意的事项。脚注[道格拉斯·R·霍夫斯塔特，《哥德尔、艾舍尔、巴赫：永恒的金边》(GEB)，第26页]
- en: flexibility
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活性
- en: dealing with ambiguity
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理模糊性
- en: ignoring irrelevant details
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略无关细节
- en: finding similarities and analogies
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找相似性和类比
- en: generating new ideas
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成新的想法
- en: You’ll soon see how word embeddings can enable these aspects of intelligence
    within your software. For example, word embeddings make it possible to respond
    with flexibility by giving words fuzziness and nuance that previous representations
    like TF-IDF vectors could not. In previous iterations of your chatbot, you would
    have to enumerate all the possible ways to say "Hi" if you want your bot to be
    flexible in its response to common greetings.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 很快你就会看到词嵌入是如何在你的软件中实现这些智能方面的。例如，词嵌入通过赋予单词模糊性和细微差别，使得以前的表示（如 TF-IDF 向量）无法做到的灵活性得以实现。在你的聊天机器人的以前版本中，如果你想要你的机器人对常见的问候做出灵活的回应，你就必须列举出所有可能的说法。
- en: But with word embeddings you can recognize the **meaning** of the word "hi",
    "hello", and "yo" all with a single embedding vector. And you can create embeddings
    for all the concepts your bot is likely to encounter by just feeding it as much
    text as you can find. There is no need to hand-craft your vocabularies anymore.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 但是使用词嵌入，你可以用一个嵌入向量识别单词"hi"、"hello"和"yo"的**含义**。你可以通过提供尽可能多的文本来为你的机器人创建所有可能遇到的概念的嵌入。不再需要手工制作你的词汇表。
- en: Caution
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 谨慎
- en: Like word embeddings, intelligence itself is a high-dimensional concept. This
    makes *Artificial General Intelligence* (AGI) an elusive target. Be careful not
    to allow your users or your bosses to think that your chatbot is generally intelligent,
    even if it appears to achieve all of Hofstadter’s "essential elements."
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 像词嵌入一样，智能本身是一个高维概念。这使得*人工通用智能*（AGI）成为一个难以捉摸的目标。要小心，不要让你的用户或老板认为你的聊天机器人是普遍具有智能的，即使它似乎实现了霍夫斯塔特的所有"基本要素"。
- en: 6.4 Word2Vec
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 Word2Vec
- en: In 2012, Tomas Mikolov, an intern at Microsoft, found a way to embed the meaning
    of words into vector space. Word embeddings or word vectors typically have 100
    to 500 dimensions, depending on the breadth of information in the corpus used
    to train them. Mikolov trained a neural network to predict word occurrences near
    each target word. Mikolov used a network with a single hidden layer, so almost
    any linear machine learning model will also work. Logistic regression, truncated
    SVD, linear discriminant analysis, or Naive Bayes would all work well and were
    used successfully by others to duplicate Mikolov’s results. In 2013, once at Google,
    Mikolov and his teammates released the software for creating these word vectors
    and called it "Word2Vec."^([[28](#_footnotedef_28 "View footnote.")])
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，微软的实习生托马斯·米科洛夫找到了一种将单词的含义嵌入到向量空间中的方法。词嵌入或单词向量通常具有100到500个维度，取决于用于训练它们的语料库中的信息广度。米科洛夫训练了一个神经网络来预测每个目标单词附近的单词出现次数。米科洛夫使用了一个单隐藏层的网络，因此几乎任何线性机器学习模型都可以工作。逻辑回归、截断的奇异值分解、线性判别分析或朴素贝叶斯都可以很好地工作，并且已经成功地被其他人用来复制米科洛夫的结果。2013年，在谷歌工作时，米科洛夫和他的队友发布了用于创建这些单词向量的软件，并将其称为"Word2Vec"。^([[28](#_footnotedef_28
    "View footnote.")])
- en: The Word2Vec language model learns the meaning of words merely by processing
    a large corpus of unlabeled text. No one has to label the words in the Word2Vec
    vocabulary. No one has to tell the Word2Vec algorithm that "Marie Curie" is a
    scientist, that the "Timbers" are a soccer team, that Seattle is a city, or that
    Portland is a city in both Oregon and Maine. And no one has to tell Word2Vec that
    soccer is a sport, or that a team is a group of people, or that cities are both
    "places" as well as "communities." Word2Vec can learn that and much more, all
    on its own! All you need is a corpus large enough to mention "Marie Curie," "Timbers,"
    and "Portland" near other words associated with science, soccer, or cities.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 语言模型仅通过处理大量未标记的文本语料库来学习单词的含义。没有人需要为 Word2Vec 词汇表中的单词打标签。没有人需要告诉 Word2Vec
    算法"玛丽·居里"是一位科学家，"Timbers" 是一支足球队，西雅图是一个城市，波特兰是俄勒冈州和缅因州的城市。也没有人需要告诉 Word2Vec 足球是一项运动，团队是一群人，城市既是"地方"也是"社区"。Word2Vec
    可以自己学会这一切以及更多！你所需要的只是一个足够大的语料库，以便在与科学、足球或城市相关的其他单词附近提到"玛丽·居里"、"Timbers" 和"波特兰"。
- en: This unsupervised nature of Word2Vec is what makes it so powerful. The world
    is full of unlabeled, uncategorized, and unstructured natural language text.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec的这种无监督性质是使其如此强大的原因。世界充满了未标记、未分类和无结构的自然语言文本。
- en: '*Unsupervised* learning and *supervised* learning are two radically different
    approaches to machine learning.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督*学习和*监督*学习是机器学习的两种根本不同的方法。'
- en: Supervised learning
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 监督学习
- en: In supervised learning, a human or team of humans must label data with the correct
    value for the target variable. An example of a label is the "spam" categorical
    label for an SMS message in chapter 4\. A more difficult label for a human might
    be a percentage score for the hotness connotation of the word "red" or "fire".
    Supervised learning is what most people think of when they think of machine learning.
    A supervised model can only get better if it can measure the difference between
    the expected output (the label) and its predictions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，人类或人类团队必须使用目标变量的正确值标记数据。标签的一个示例是第4章中短信消息的"垃圾邮件"分类标签。人类更难标记的标签可能是单词"红色"或"火"的热度内涵的百分比得分。当人们想到机器学习时，他们大多数时候想到的是监督学习。监督模型只有在它能够衡量期望输出（标签）与其预测之间的差异时才能变得更好。
- en: In contrast, unsupervised learning enables a machine to learn directly from
    data, without any assistance from humans. The training data does not have to be
    organized, structured, or labeled by a human. So unsupervised learning algorithms
    like Word2Vec are perfect for natural language text.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，无监督学习使机器能够直接从数据中学习，而无需任何人类的协助。训练数据不必由人类组织、结构化或标记。因此，像Word2Vec这样的无监督学习算法非常适用于自然语言文本。
- en: Unsupervised learning
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In unsupervised learning, you train the model to perform a task, but without
    any labels, only the raw data. Clustering algorithms such as k-means or DBSCAN
    are examples of unsupervised learning. Dimension reduction algorithms like principal
    component analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE)
    are also unsupervised machine learning techniques. In unsupervised learning, the
    model finds patterns in the relationships between the data points themselves.
    An unsupervised model can get smarter (more accurate) just by throwing more data
    at it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，您训练模型执行一个任务，但没有任何标签，只有原始数据。像k-means或DBSCAN这样的聚类算法是无监督学习的例子。像主成分分析（PCA）和t-分布随机邻居嵌入（t-SNE）这样的降维算法也是无监督机器学习技术。在无监督学习中，模型发现数据点之间的关系模式。无监督模型只需向其提供更多数据，就可以变得更加智能（更准确）。
- en: 'Instead of trying to train a neural network to learn the target word meanings
    directly (on the basis of labels for that meaning) you can teach the network to
    predict words near the target word in your sentences. So in this sense, you do
    have labels: the nearby words you are trying to predict. But because the labels
    are coming from the dataset itself and require no hand-labeling, the Word2Vec
    training algorithm is definitely an unsupervised learning algorithm.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 不是尝试训练神经网络直接学习目标词义（基于该词义的标签），而是教会网络预测句子中目标词附近的词。所以在这个意义上，你确实有标签：你试图预测的附近单词。但是因为标签来自数据集本身，并且不需要手动标记，Word2Vec训练算法绝对是一种无监督学习算法。
- en: Another domain where this unsupervised training technique is used in time series
    modeling. Time series models are often trained to predict the next value in a
    sequence based on a window of previous values. Time series problems are remarkably
    similar to natural language problems in a lot of ways because they deal with ordered
    sequences of values (words or numbers).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个领域是时间序列建模中使用这种无监督训练技术。时间序列模型通常是根据先前数值窗口来预测序列中的下一个值。在很多方面，时间序列问题与自然语言问题非常相似，因为它们处理有序数值（单词或数字）的序列。
- en: And the prediction itself is not what makes Word2Vec work. The prediction is
    merely a means to an end. What you do care about is the internal representation,
    the vector, that Word2Vec gradually builds up to help it generate those predictions.
    This representation will capture much more of the meaning of the target word (its
    semantics) than the word-topic vectors that came out of latent semantic analysis
    (LSA) and latent Dirichlet allocation (LDiA) in chapter 4.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 预测本身并不是Word2Vec起作用的原因。预测只是达到目的的一种手段。你真正关心的是Word2Vec逐渐建立起来帮助它生成这些预测的内部表示，即向量。这种表示将比潜在语义分析（LSA）和潜在狄利克雷分配（LDiA）在第四章中产生的单词-主题向量更多地捕捉到目标单词的含义（其语义）。
- en: Note
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Models that learn by trying to re-predict the input using a lower-dimensional
    internal representation are called *autoencoders*. This may seem odd to you. It
    is like asking the machine to echo back what you just asked them, only they cannot
    write the question down as you are saying it. The machine has to compress your
    question into shorthand. And it has to use the same shorthand algorithm (function)
    for all the questions you ask it. The machine learns a new shorthand (vector)
    representation of your statements.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过尝试使用较低维度的内部表示重新预测输入的模型被称为*自动编码器*。这可能对你来说有些奇怪。这就像要求机器回显你刚刚问过它的内容，只是它们不能在你说话时把问题写下来。机器必须将你的问题压缩成速记。它必须对你提出的所有问题使用相同的速记算法（函数）。机器学习了你的陈述的新速记（向量）表示。
- en: If you want to learn more about unsupervised deep learning models that create
    compressed representations of high-dimensional objects like words, search for
    the term "autoencoder."^([[29](#_footnotedef_29 "View footnote.")]) They are also
    a common way to get started with neural nets, because they can be applied to almost
    any dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于无监督深度学习模型的信息，这些模型可以创建像单词这样的高维对象的压缩表示，请搜索术语“自动编码器”。^([[29](#_footnotedef_29
    "View footnote.")])它们也是开始使用神经网络的常见方式，因为它们几乎可以应用于任何数据集。
- en: Word2Vec will learn about things you might not think to associate with all words.
    Did you know that every word has some geography, sentiment (positivity), and gender
    associated with it? If any word in your corpus has some quality, like "placeness",
    "peopleness", "conceptness" or "femaleness", all the other words will also be
    given a score for these qualities in your word vectors. The meaning of a word
    "rubs off" on the neighboring words when Word2Vec learns word vectors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 将学习关于你可能不会想到与所有单词相关联的事物。你知道每个单词都有一些地理位置、情感（积极性）和性别吗？如果你的语料库中的任何单词具有某种质量，比如“地点性”、“人性”、“概念性”或“女性”，那么所有其他单词在你的单词向量中也会为这些质量得分。当Word2Vec学习单词向量时，单词的含义会传递给相邻的单词。
- en: All words in your corpus will be represented by numerical vectors, similar to
    the word-topic vectors discussed in chapter 4\. Only this time the "topics" mean
    something more specific, more precise. In LSA, words only had to occur in the
    same document to have their meaning "rub off" on each other and get incorporated
    into their word-topic vectors. For Word2Vec word vectors, the words must occur
    near each other — typically fewer than five words apart and within the same sentence.
    And Word2Vec word vector "topic" weights can be added and subtracted to create
    new word vectors that mean something!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你语料库中的所有单词都将由数字向量表示，类似于第 4 章中讨论的单词-主题向量。只是这一次，“主题”意味着更具体、更精确的东西。在LSA中，单词只需出现在同一文档中，它们的含义就会互相“融合”，并被合并到它们的单词-主题向量中。对于Word2Vec单词向量，这些单词必须彼此靠近 — 通常在同一句子中的五个单词之内，并且在同一句子中。并且Word2Vec单词向量的“主题”权重可以相加和相减，以创建新的有意义的单词向量！
- en: A mental model that may help you understand word vectors is to think of word
    vectors as a list of weights or scores. Each weight or score is associated with
    a specific dimension of meaning for that word.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能帮助你理解单词向量的心理模型是将单词向量视为一系列权重或分数的列表。每个权重或分数与该单词的特定含义维度相关联。
- en: Listing 6.1 Compute nessvector
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 6.1 节 计算 nessvector
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can compute "nessvectors" for any word or *n*-gram in the Word2Vec vocabulary
    using the tools from `nlpia` ([https://gitlab.com/tangibleai/nessvec/-/blob/main/src/nessvec/examples/ch06/nessvectors.py](ch06.html)).
    And this approach will work for any "ness" components that you can dream up.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`nlpia`工具（[https://gitlab.com/tangibleai/nessvec/-/blob/main/src/nessvec/examples/ch06/nessvectors.py](ch06.html)）可以为
    Word2Vec 词汇表中的任何单词或*n*-gram 计算“nessvectors”。这种方法适用于你能想到的任何“ness”组件。
- en: Mikolov developed the Word2Vec algorithm while trying to think of ways to numerically
    represent words in vectors. He wasn’t satisfied with the less accurate word sentiment
    math you did in chapter 4\. He wanted to do *analogical reasoning*, like you just
    did in the previous section with those analogy questions. This concept may sound
    fancy, but really it just means that you can do math with word vectors and that
    the answer makes sense when you translate the vectors back into words. You can
    add and subtract word vectors to *reason* about the words they represent and answer
    questions similar to your examples above, like the following. (For those not up
    on sports in the US, the Portland Timbers and Seattle Sounders are Major League
    Soccer teams.)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Mikolov在尝试想出用向量数值表示单词的方法时开发了Word2Vec算法。他对第四章中你在做的不太准确的词情感数学算法不满意。他希望使用类似于你刚刚在前一节中使用类比问题的*类比推理*。这个概念听起来可能很高级，但实际上它只是意味着你可以用词向量进行数学运算，并且当你将向量转换回单词时答案是有意义的。你可以对代表它们的单词进行词向量相加和相减来*推理*，并回答类似于你上面的例子的问题，比如以下问题。（对于不了解美国体育的人来说，波特兰伐木者队和西雅图声浪队是美国职业足球联赛的球队。）
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Ideally, you’d like this math (word vector reasoning) to give you this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你希望这个数学（词向量推理）可以给你这个：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, your analogy question "''Marie Curie'' is to ''physics'' as *__*
    is to ''classical music''?" can be thought about as a math expression like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你的类比问题“'玛丽·居里'与'物理学'相当于 *__*与'古典音乐'相当？”可以被看作是一个数学表达式，如下：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this chapter, we want to improve on the LSA word vector representations we
    introduced in chapter 4\. Topic vectors constructed from entire documents using
    LSA are great for document classification, semantic search, and clustering. But
    the topic-word vectors that LSA produces aren’t accurate enough to be used for
    semantic reasoning or classification and clustering of short phrases or compound
    words. You’ll soon learn how to train the single-layer neural networks required
    to produce these more accurate, more fun, word vectors. And you’ll see why they
    have replaced LSA word-topic vectors for many applications involving short documents
    or statements.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们想要改进在第四章中介绍的LSA词向量表示。使用LSA构建的整个文档的主题向量非常适用于文档分类、语义搜索和聚类。但是，LSA产生的主题-词向量并不足以用于语义推理、短语或复合词的分类和聚类。很快你将学会如何训练单层神经网络以产生这些更准确、更有趣的词向量。并且你将看到，它们已经取代了LSA词-主题向量，用于许多涉及短文档或陈述的应用中。
- en: 6.4.1 Analogy reasoning
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 类比推理
- en: Word2Vec was first presented publicly in 2013 at the ACL conference.^([[30](#_footnotedef_30
    "View footnote.")]) The talk with the dry-sounding title "Linguistic Regularities
    in Continuous Space Word Representations" described a surprisingly accurate language
    model. Word2Vec embeddings were four times more accurate (45%) compared to equivalent
    LSA models (11%) at answering analogy questions like those above.^([[31](#_footnotedef_31
    "View footnote.")]) The accuracy improvement was so surprising, in fact, that
    Mikolov’s initial paper was rejected by the International Conference on Learning
    Representations.^([[32](#_footnotedef_32 "View footnote.")]) Reviewers thought
    that the model’s performance was too good to be true. It took nearly a year for
    Mikolov’s team to release the source code and get accepted to the Association
    for Computational Linguistics.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec首次公开亮相是在2013年的ACL会议上。[^\[30\]] 这场以枯燥标题“在连续空间词表示中的语言规律性”为题的演讲描述了一个非常准确的语言模型。与相应的LSA模型（11%）相比，Word2Vec嵌入更准确（45%），可以回答类似上面的类比问题。[^\[31\]]
    然而，这种准确度的提高如此出乎意料，以至于Mikolov的最初论文被国际学习表示研讨会拒绝接受。[^\[32\]] 评审人员认为该模型的性能太好以至于不可信。Mikolov的团队花了近一年的时间才发布源代码并被计算语言学协会接受。
- en: Suddenly, with word vectors, questions like
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 突然之间，有了词向量，问题就变得像这样：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: can be solved with vector algebra (see figure 6.1).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过向量代数解决（见图6.1）。
- en: Figure 6.2 Geometry of Word2Vec math
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2 Word2Vec数学的几何表示
- en: '![vector add](images/vector_add.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![向量相加](images/vector_add.png)'
- en: The `word2vec` language model "knows" that the terms "Portland" and "Portland
    Timbers" are roughly the same distance apart as "Seattle" and "Seattle Sounders".
    And those vector displacements between the words in each pair are in roughly the
    same direction. So the `word2vec` model can be used to answer your sports team
    analogy question. You can add the difference between "Portland" and "Seattle"
    to the vector that represents the "Portland Timbers". That should get you close
    to the vector for "Seattle Sounders".
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`word2vec` 语言模型“知道”术语“波特兰”和“波特兰木材人”之间的距离大致与“西雅图”和“西雅图音速”相同。并且每对词之间的向量位移大致在同一方向。因此，`word2vec`
    模型可以用来回答您的体育队类比问题。您可以将“波特兰”与“西雅图”的差异添加到代表“波特兰木材人”的向量中。这应该会让您接近“西雅图音速”的向量。'
- en: '**Equation 6.1 Compute the answer to the soccer team question**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程 6.1 计算足球队问题的答案**'
- en: '![equation 6 1](images/equation_6_1.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![equation 6 1](images/equation_6_1.png)'
- en: After adding and subtracting word vectors, your resultant vector will almost
    never exactly equal one of the vectors in your word vector vocabulary. Word2Vec
    word vectors usually have 100s of dimensions, each with continuous real values.
    Nonetheless, the vector in your vocabulary that is closest to the resultant will
    often be the answer to your NLP question. The English word associated with that
    nearby vector is the natural language answer to your question about sports teams
    and cities.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加和减去词向量后，你得到的结果向量几乎永远不会完全等于词向量词汇表中的某个向量。Word2Vec 词向量通常具有数百个维度，每个维度都有连续的实值。尽管如此，在您的词汇表中，与结果最接近的向量通常会成为您的
    NLP 问题的答案。与该附近向量相关联的英文单词是您关于体育队和城市的问题的自然语言答案。
- en: Word2Vec allows you to transform your natural language vectors of token occurrence
    counts and frequencies into the vector space of much lower-dimensional Word2Vec
    vectors. In this lower-dimensional space, you can do your math and then convert
    them back to a natural language space. You can imagine how useful this capability
    is to a chatbot, search engine, question-answering system, or information extraction
    algorithm.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 允许您将自然语言标记发生计数和频率的向量转换为远低维度的 Word2Vec 向量的向量空间。在这个较低维度的空间中，您可以进行数学运算，然后将它们转换回自然语言空间。您可以想象这种能力对于聊天机器人、搜索引擎、问答系统或信息提取算法有多有用。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The initial paper in 2013 by Mikolov and his colleagues was able to achieve
    an answer accuracy of only 40%. But back in 2013, the approach outperformed any
    other semantic reasoning approach by a significant margin. Since its initial publication,
    the performance of Word2Vec has improved further. This was accomplished by training
    it on extremely large corpora. The reference implementation was trained on the
    100 billion words from the Google News Corpus. This is the pre-trained model you’ll
    see used in this book a lot.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 2013 年的初始论文由 Mikolov 和他的同事发表，仅能够实现 40% 的答案准确率。但是在 2013 年，这种方法超过了任何其他语义推理方法。自其最初发表以来，Word2Vec
    的性能进一步提高。这是通过在极大的语料库上进行训练来实现的。参考实现是在 Google 新闻语料库的 1000 亿个词上进行训练的。这是您在本书中经常看到的预训练模型。
- en: 'The research team also discovered that the difference between a singular and
    a plural word is often roughly the same magnitude, and in the same direction:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 研究小组还发现，单数词和复数词之间的差异往往具有大致相同的数量级和方向：
- en: '**Equation 6.2 Distance between the singular and plural versions of a word**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程 6.2 单词的单数和复数版本之间的距离**'
- en: '![equation 6 2](images/equation_6_2.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![equation 6 2](images/equation_6_2.png)'
- en: 'But their discovery didn’t stop there. They also discovered that distance relationships
    go far beyond simple singular versus plural relationships. Distances apply to
    other semantic relationships. The Word2Vec researchers soon discovered they could
    answer questions that involve geography, culture, and demographics, like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，他们的发现并没有止步于此。他们还发现，距离关系远不止简单的单数与复数关系。距离适用于其他语义关系。Word2Vec 研究人员很快发现他们可以回答涉及地理、文化和人口统计学的问题，例如：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: More reasons to use word vectors
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更多使用词向量的理由
- en: Vector representations of words are useful not only for reasoning and analogy
    problems but also for all the other things you use natural language vector space
    models for. From pattern matching to modeling and visualization, your NLP pipeline’s
    accuracy and usefulness will improve if you know how to use the word vectors from
    this chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的向量表示不仅对于推理和类比问题有用，还对于你在自然语言向量空间模型中使用的所有其他事情有用。从模式匹配到建模和可视化，如果你知道如何使用本章的词向量，你的NLP流程的准确性和实用性将会提高。
- en: For example, later in this chapter, we show you how to visualize word vectors
    on 2D "semantic maps" like the one shown in Figure [6.3](#word_vectors_for_ten_us_cities_projected_onto_a_2d_map_figure).
    You can think of this like a cartoon map of a popular tourist destination or one
    of those impressionistic maps you see on bus stop posters. In these cartoon maps,
    things that are close to each other semantically as well as geographically get
    squished together. For cartoon maps, the artist adjusts the scale and position
    of icons for various locations to match the "feel" of the place. With word vectors,
    the machine too can have a feel for words and places and how far apart they should
    be.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在本章后面，我们将向你展示如何在2D“语义地图”上可视化词向量，就像[图6.3](#word_vectors_for_ten_us_cities_projected_onto_a_2d_map_figure)所示的那样。你可以将其视为受欢迎旅游目的地的卡通地图，或者是那些你在公交站海报上看到的印象派地图之一。在这些卡通地图中，语义上和地理上靠近的事物被紧密放在一起。在卡通地图中，艺术家调整了各个位置的图标的比例和位置，以符合地方的“感觉”。借助词向量，机器也可以对单词和地方有所了解，并决定它们之间应该有多远的距离。
- en: So your machine will be able to generate impressionistic maps like the one in
    figure 6.3 using word vectors you are learning about in this chapter.^([[33](#_footnotedef_33
    "View footnote.")])
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，借助你在本章学习的词向量，你的机器就能够生成像图6.3中一样的印象派地图。^([[33](#_footnotedef_33 "View footnote.")])
- en: Figure 6.3 Word vectors for ten US cities projected onto a 2D map
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 十个美国城市的词向量投影到2D地图上
- en: '![us 10 city word vector pca map labeled](images/us-10-city-word-vector-pca-map-labeled.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![us 10 city word vector pca map labeled](images/us-10-city-word-vector-pca-map-labeled.png)'
- en: If you’re familiar with these US cities, you might realize that this isn’t an
    accurate geographic map, but it’s a pretty good semantic map. I, for one, often
    confuse the two large Texas cities, Houston and Dallas, and they have almost identical
    word vectors. And the word vectors for the big California cities make a nice triangle
    of culture in my mind.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉这些美国城市，你可能会意识到这并不是一个准确的地理地图，但它却是一个相当不错的语义地图。我常常把两个德克萨斯州大城市休斯顿和达拉斯搞混，它们的词向量几乎一模一样。而加利福尼亚州的大城市的词向量在我脑海里形成了一个文化三角形。
- en: And word vectors are great for chatbots and search engines too. For these applications,
    word vectors can help overcome some of the rigidity, brittleness of pattern, or
    keyword matching. Say you were searching for information about a famous person
    from Houston, Texas, but didn’t realize they’d moved to Dallas. From Figure [6.3](#word_vectors_for_ten_us_cities_projected_onto_a_2d_map_figure),
    you can see that a semantic search using word vectors could easily figure out
    a search involving city names such as Denver and Houston. And even though character-based
    patterns wouldn’t understand the difference between "tell me about a Denver omelette"
    and "tell me about the Denver Nuggets", a word vector pattern could. Patterns
    based on word vectors would likely be able to differentiate between the food item
    (omelette) and the basketball team (Nuggets) and respond appropriately to a user
    asking about either.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量对于聊天机器人和搜索引擎也非常有用。对于这些应用，词向量可以帮助克服模式的僵硬、脆弱性或关键字匹配的一些问题。比如，你正在搜索关于得克萨斯州休斯顿的著名人物的信息，但你不知道他们搬到了达拉斯。从图[6.3](#word_vectors_for_ten_us_cities_projected_onto_a_2d_map_figure)中，你可以看到，使用词向量进行语义搜索可以轻松处理包含丹佛和休斯顿等城市名称的搜索。即使基于字符的模式无法理解“给我介绍一个丹佛煎蛋”的不同于“给我介绍丹佛掘金队”，但基于词向量的模式可以。基于词向量的模式很可能能够区分食物项（煎蛋）和篮球队（掘金队），并据此恰当地回应用户的任何问题。
- en: 6.4.2 Learning word embeddings
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 学习词向量
- en: 'Word embeddings are vectors that represent the meaning (semantics) of words.
    However, the meaning of words is an elusive, fuzzy thing to capture. An isolated
    individual word has a very ambiguous meaning. Here are some of the things that
    can affect the meaning of a word:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是表示单词意义（语义）的向量。然而，单词的意义是一个难以捉摸的模糊事物。一个孤立的个体单词有一个非常模糊的意义。以下是一些可能影响单词意义的事情：
- en: Whose thought is being communicated with the word
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被传达的思想是谁的
- en: Who is the word intended to be understood by
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该单词的理解对象
- en: The context (where and when) the word is being used
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词被使用的上下文（何地和何时）
- en: The domain knowledge or background knowledge assumed
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假定的领域知识或背景知识
- en: The sense of the word intended
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需表达的单词意义
- en: Your brain will likely understand a word quite differently than mine. And the
    meaning of a word in your brain changes over time. You learn new things about
    a word as you make new connections to other concepts. And as you learn new concepts
    and words, you learn new connections to these new words depending on the impression
    of the new words on your brain. Embeddings are used to represent this evolving
    pattern of neuron connections in your brain created by the new word. And these
    new vectors have 100s of dimensions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你的大脑可能会以与我的大脑完全不同的方式理解一个词。随着时间的推移，你大脑中的一个词的含义也会发生变化。当你将一个词与其他概念建立新联系时，你会对这个词了解到新的东西。随着你学习新概念和新词汇，你会根据新词对你大脑的印象学习到这些新词的新联系。嵌入用于表示你大脑中由新词创建的神经元连接的演变模式。而这些新向量有数百个维度。
- en: Imagine a young girl who says "My mommy is a doctor."^([[34](#_footnotedef_34
    "View footnote.")]) Imagine what the word "doctor" means to her. And then think
    about how her understanding of that word, her NLU processing algorithm, evolves
    as she grows up. Over time she will learn to differentiate between a medical doctor
    (M.D.) and an academic doctor of philosophy (Ph.D.). Imagine what that word means
    to her just a few years later when she herself begins to think about the possibility
    of applying to med school or a Ph.D. program. And imagine what that word means
    to her father or her mother, the doctor. And imagine what that word means to someone
    who doesn’t have access to healthcare.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个年轻的女孩说：“我妈妈是医生。”^([[34](#_footnotedef_34 "View footnote.")]) 想象一下对她来说，“医生”这个词意味着什么。然后想象一下，随着她的成长，她对这个词的理解，她的自然语言理解处理算法是如何演变的。随着时间的推移，她将学会区分医生（M.D.）和哲学博士（Ph.D.）。想象一下，当她自己开始考虑申请医学院或博士项目的可能性时，这个词对她来说意味着什么。想象一下，这个词对她的父亲或母亲，即医生，意味着什么。想象一下，这个词对一个没有医疗保健资源的人来说意味着什么。
- en: Creating useful numerical representations of words is tricky. The meaning you
    want to encode or embed in the vector depends not only on whose meaning you want
    to represent but also on when and where you want your machine to process and understand
    that meaning. In the case of GloVe, Word2Vec and other early word embeddings the
    goal was to represent the "average" or most popular meaning. The researchers creating
    these representations were focused on analogy problems and other benchmark tests
    that measure human and machine understanding of words. For example, we used pretrained
    fastText word embeddings for the code snippets earlier in this chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 创建有用的单词数值表示是棘手的。你想要编码或嵌入向量中的含义不仅取决于你想要代表的含义，还取决于何时何地你希望你的机器处理和理解这个含义。在GloVe、Word2Vec和其他早期单词嵌入的情况下，目标是表示“平均”或最流行的含义。创建这些表示的研究人员关注的是类比问题和其他衡量人类和机器对单词理解的基准测试。例如，我们在本章前面的代码片段中使用了预训练的fastText单词嵌入。
- en: Tip
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Pretrained word vector representations are available for corpora like Wikipedia,
    DBPedia, Twitter, and Freebase.^([[35](#_footnotedef_35 "View footnote.")]) These
    pretrained models are great starting points for your word vector applications.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的单词向量表示可用于诸如维基百科、DBPedia、Twitter和Freebase等语料库。^([[35](#_footnotedef_35 "View
    footnote.")]) 这些预训练模型是你的单词向量应用的绝佳起点。
- en: Google provides a pretrained `word2vec` model based on English Google News articles.^([[36](#_footnotedef_36
    "View footnote.")])
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌提供了基于英文Google新闻文章的预训练`word2vec`模型。^([[36](#_footnotedef_36 "View footnote.")])
- en: Facebook published their word models, called *fastText*, for 294 languages.^([[37](#_footnotedef_37
    "View footnote.")])
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook发布了他们的词模型，称为*fastText*，涵盖了294种语言。^([[37](#_footnotedef_37 "View footnote.")])
- en: Fortunately, once you’ve decided your "audience" or "users" for the word embeddings,
    you only need to gather up example usages of those words. Word2Vec, GloVe, and
    fastText are all unsupervised learning algorithms. All you need is some raw text
    from the *domain* that you and your users are interested in. If you are mainly
    interested in medical doctors you can train your embeddings on a collection of
    texts from medical journals. Or if you want the most general understanding of
    words represented in your vectors, ML engineers often use Wikipedia and online
    news articles to capture the meaning of words. After all, Wikipedia represents
    our collective understanding of everything in the world.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一旦你决定了词嵌入的“受众”或“用户”，你只需要收集这些单词的用法示例。Word2Vec、GloVe和fastText都是无监督学习算法。你只需要一些来自你和你的用户感兴趣领域的原始文本。如果你主要关注医生，你可以训练你的嵌入在医学期刊的一系列文本上。或者，如果你想要最普遍的单词表示的理解，机器学习工程师通常使用维基百科和在线新闻文章来捕捉单词的含义。毕竟，维基百科代表了我们对世界上一切的集体理解。
- en: 'Now that you have your corpus how exactly do you create a training set for
    your word embedding language model? In the early days there were two main approaches:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你拥有你的语料库时，你如何为你的词嵌入语言模型创建训练集呢？在早期，主要有两种方法：
- en: '*Continuous bag-of-words* (CBOW)'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*连续词袋模型*（CBOW）'
- en: Continuous *skip-gram*
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续的*skip-gram*
- en: The *continuous bag-of-words* (CBOW) approach predicts the target word (the
    output or "target" word) from the nearby context words (input words). The only
    difference with the bag-of-words (BOW) vectors you learned about in chapter 3
    is that a CBOWs are created for a continuously sliding window of words within
    each document. So you will have almost as many CBOW vectors as you have words
    in the sequence of words from all of your documents. Whereas for the BOW vectors
    you only had one vector for each document. This gives your word embedding training
    set a lot more information to work with so it will produce more accurate embedding
    vectors. With the CBOW approach, you create a huge number of tiny synthetic documents
    from every possible phrase you can extract from your original documents.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*连续的词袋模型*（CBOW）方法从附近的上下文单词（输入单词）预测目标单词（输出或“目标”单词）。与第3章中学习的词袋（BOW）向量唯一的区别在于，CBOW是针对每个文档内的连续滑动窗口的单词创建的。因此，你将有几乎与所有文档中的单词序列中的单词数量相同的CBOW向量。而对于BOW向量，你只有每个文档一个向量。这为你的词嵌入训练集提供了更多信息以使其生成更准确的嵌入向量。使用CBOW方法，你可以从原始文档中提取的每个可能短语创建大量的微小合成文档。'
- en: Figure 6.4 CBOW neural network architecture
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 CBOW神经网络架构
- en: '![word2vec cbow whatever affects one drawio](images/word2vec-cbow-whatever-affects-one_drawio.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![word2vec cbow whatever affects one drawio](images/word2vec-cbow-whatever-affects-one_drawio.png)'
- en: For the *skip-gram* approach you also create this huge number of synthetic documents.
    You just reverse the prediction target so that you’re using the CBOW targets to
    predict the CBOW features. predicts the context words ("target" words) from a
    word of interest (the input word). Though these may seem like your pairs of words
    are reversed, you will see soon that the results are almost mathematically equivalent.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于skip-gram方法，在这种方法中，你也会创建大量合成文档。你只需反转预测目标，这样就可以使用CBOW目标来预测CBOW特征。它预测与感兴趣的单词（输入单词）相关的上下文单词（“目标”单词）。尽管这些看起来像是你的词对被颠倒了，但你很快就会发现结果几乎是数学上等价的。
- en: Figure 6.5 Skip-gram neural network architecture
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5 Skip-gram神经网络架构
- en: '![word2vec skip gram whatever affects one drawio](images/word2vec-skip-gram-whatever-affects-one_drawio.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![word2vec skip gram whatever affects one drawio](images/word2vec-skip-gram-whatever-affects-one_drawio.png)'
- en: You can see how the two neural approaches produce the same number of training
    examples and create the same number of training examples for both the skip-gram
    and CBOW approach.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，两种神经网络方法产生相同数量的训练样本，并为skip-gram和CBOW方法创建相同数量的训练样本。
- en: Skip-gram approach
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Skip-gram方法
- en: In the skip-gram training approach, you predict a word in the neighborhood of
    the context word. Imagine your corpus contains this wise rejection of individualism
    by Bayard Rustin and Larry Dane Brimner.^([[38](#_footnotedef_38 "View footnote.")])
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在skip-gram训练方法中，你预测上下文单词附近的单词。想象一下，你的语料库包含了Bayard Rustin和Larry Dane Brimner关于个人主义的明智拒绝。^([[38](#_footnotedef_38
    "查看脚注。")])
- en: We are all one. And if we don’t know it, we will find out the hard way.
  id: totrans-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们都是一个整体。如果我们不知道这一点，那我们将会以艰难的方式发现它。
- en: — Bayard Rustin
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ——Bayard Rustin
- en: '_We Are One: The Story of Bayard Rustin_, 2007, p.46_ by Larry Dane Brimner'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '_《We Are One: The Story of Bayard Rustin》，2007，第 46 页_，Larry Dane Brimner'
- en: Definition
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: A *skip-gram* is a 2-gram or pair of grams where each gram is within the neighborhood
    of each other. As usual, the grams can be whatever chunks of text your tokenizer
    is designed to predict - usually words.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*skip-gram* 是一个 2-gram 或两个 gram，其中每个 gram 都在彼此的附近。通常情况下，这些 grams 可以是您的分词器设计为预测的文本块
    - 通常是单词。'
- en: For the continuous skip-gram training approach, skip-grams are word pairs that
    skip over zero to four words to create the skip-gram pair. When training word
    embeddings using the Word2Vec skip-gram method, the first word in a skip-gram
    is called the "context" word. The context word is the input to the Word2Vec neural
    network. The second word in the skip-gram pair is often called the "target" word.
    The target word is the word that the language model and embedding vector is being
    trained to predict - the output.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续的 skip-gram 训练方法，skip-grams 是跳过零到四个单词以创建 skip-gram 对的单词对。使用 Word2Vec skip-gram
    方法训练单词嵌入时，skip-gram 中的第一个单词称为“上下文”单词。上下文单词是输入到 Word2Vec 神经网络中的。skip-gram 对中的第二个单词通常被称为“目标”单词。目标单词是语言模型和嵌入向量被训练以预测的单词
    - 输出。
- en: Figure 6.6 Training input and output example for the skip-gram approach
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.6 跳过-gram 方法的训练输入和输出示例
- en: '![we are all one drawio](images/we-are-all-one_drawio.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![we are all one drawio](images/we-are-all-one_drawio.png)'
- en: In Figure [6.6](#training_input_and_output_example_for_the_skip-gram_approach_figure),
    you can see how the neural network architecture looks like for the skip-gram approach
    to creating word embeddings.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[6.6](#training_input_and_output_example_for_the_skip-gram_approach_figure)中，您可以看到
    skip-gram 方法创建单词嵌入的神经网络架构是什么样子的。
- en: What is softmax?
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: softmax 是什么？
- en: The *softmax function* is often used as the activation function in the output
    layer of neural networks when the network’s goal is to learn classification problems.
    The softmax will squash the output results between 0 and 1, and the sum of all
    output notes will always add up to 1\. That way, the results of an output layer
    with a softmax function can be considered as probabilities.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 函数通常用作神经网络输出层的激活函数，当网络的目标是学习分类问题时。softmax 将输出结果压缩在 0 到 1 之间，所有输出节点的总和始终加起来为
    1。因此，具有 softmax 函数的输出层的结果可以被视为概率。
- en: 'For each of the *K* output nodes, the softmax output value of the can be calculated
    using the normalized exponential function:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个*K* 输出节点，可以使用归一化指数函数计算 softmax 输出值：
- en: '![equation 6 3](images/equation_6_3.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![equation 6 3](images/equation_6_3.png)'
- en: 'If your output vector of a three-neuron output layer looks like this:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的三神经元输出层的输出向量如下所示：
- en: '**Equation 6.3 Example 3D vector**'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程 6.3 例子 3D 向量**'
- en: '![equation 6 4](images/equation_6_4.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![equation 6 4](images/equation_6_4.png)'
- en: 'The "squashed" vector after the softmax activation would look like this:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 激活后的“压缩”向量将如下所示：
- en: '**Equation 6.4 Example 3D vector after softmax**'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程 6.4 例子 3D 向量经过 softmax 后**'
- en: '![equation 6 5](images/equation_6_5.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![equation 6 5](images/equation_6_5.png)'
- en: Notice that the sum of these values (rounded to 3 significant digits) is approximately
    1.0, like a probability distribution.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些值的总和（四舍五入到 3 个有效数字）约为 1.0，就像概率分布一样。
- en: Figure 6.4 shows the numerical network input and output for the first two surrounding
    words. In this case, the input word is "Monet", and the expected output of the
    network is either "Claude" or "painted", depending on the training pair.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 显示了前两个周围单词的数字网络输入和输出。在这种情况下，输入单词是“Monet”，网络的预期输出是“Claude”或“painted”，这取决于训练对。
- en: Figure 6.7 Network example for the skip-gram training
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.7 跳过-gram 训练的网络示例
- en: '![skipgram](images/skipgram.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![skipgram](images/skipgram.png)'
- en: Note
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: When you look at the structure of the neural network for word embedding, you’ll
    notice that the implementation looks similar to what you discovered in chapter
    5.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 当您查看单词嵌入的神经网络结构时，您会注意到实现看起来与您在第 5 章中发现的内容相似。
- en: 6.4.3 Learning meaning without a dictionary
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 学习没有字典的含义
- en: For this Word2Vec training example you won’t need to use a dictionary, such
    as `wiktionary.org` to explicitly define the meaning of words. Instead you can
    just have Word2Vec read text that contains meaningful sentences. You’ll use the
    WikiText2 corpus that comes with PyTorch in the `torchtext` package.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个 Word2Vec 训练示例，您不需要使用字典，比如 `wiktionary.org` 来明确定义单词的含义。相反，您可以只让 Word2Vec
    读取包含有意义的句子的文本。您将使用 PyTorch 中 `torchtext` 包中提供的 WikiText2 语料库。
- en: '[PRE19]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To make it even less mysterious you can look at the text file you just created
    with about 10,000 paragraphs of from the `WikiText2` dataset:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让它更不神秘，您可以查看您刚刚从`WikiText2`数据集中创建的包含约10,000个段落的文本文件：
- en: '[PRE20]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The 99,998th paragraph just happens to contain the abbreviation "Dr.". In this
    case the abbreviation is for the word "doctor." You can use this to practice your
    "Mommy is a doctor" intuition pump. So you’ll soon find out whether Word2Vec can
    learn what a doctor really is. Or maybe it will get confused by street addresses
    that use "Dr." to mean "drive".
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 第99,998段碰巧包含缩写"Dr."。在这种情况下，缩写是为了单词"doctor"。您可以利用这个来练习您的"妈妈是一名医生"直觉泵。因此，您很快就会发现Word2Vec是否能学会什么是医生。或者它可能会因为使用"Dr."表示"drive"的街道地址而感到困惑。
- en: Conveniently, the WikiText2 dataset has already tokenized the text into words
    for you. Words are delimited with a single space (`" "`) character. So your pipeline
    doesn’t have to decide whether "Dr." is the end of a sentence or not. If the text
    was not tokenized, your NLP pipeline would need to remove periods from tokens
    at the ends of all sentences. Even the heading delimiter text `"=="` has been
    split into two separate tokens `"="` and `"="`. And paragraphs are delimited by
    a newline (`"\n"`) character. And many "paragraphs" will be created for Wikipedia
    headings such as "== Reception ==" as well as retaining all empty lines between
    paragraphs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，WikiText2数据集已经将文本分词成单词。单词之间用单个空格(`" "`)字符分隔。因此，您的管道不必决定"Dr."是否是句子的结尾。如果文本没有被分词，您的NLP管道将需要删除所有句子末尾的句号。甚至标题分隔符文本`"=="`也已经被拆分为两个独立的标记`"="`和`"="`。段落由换行(`"\n"`)字符分隔。对于维基百科标题如`"==
    接待 =="`，将创建许多"段落"，同时保留段落之间的所有空行。
- en: You can utilize a sentence boundary detector or sentence segmenter such as SpaCy
    to split paragraphs into sentences. This would prevent your training pairs of
    words from spilling over from one sentence to the other. Honoring sentence boundaries
    with your Word2Vec can improve the accuracy of your word embeddings. But we’ll
    leave that to you to decide if you need the extra boost in accuracy.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以利用像SpaCy这样的句子边界检测器或句子分割器将段落分割成句子。这将防止您的Word2Vec训练对从一个句子溢出到另一个句子。尊重句子边界的Word2Vec可以提高词嵌入的准确性。但是我们将把这个决定留给您，看您是否需要额外的准确性提升。
- en: 'One critical piece of infrastructure that your pipeline here can handle is
    the memory management for large corpora. If you were training your word embeddings
    on millions of paragraphs you will need to use a dataset object that manages the
    text on disk, only loading into RAM or the GPU what is needed. The Hugging Face
    Hub `datasets` package can handle this for you:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的管道可以处理的一个关键基础设施是大型语料库的内存管理。如果您正在对数百万段落进行词嵌入的训练，您将需要使用一个管理磁盘上文本的数据集对象，只加载需要的部分到RAM或GPU中。Hugging
    Face Hub的`datasets`包可以为您处理这个问题：
- en: '[PRE21]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: But you still need to tell Word2Vec what a word is. This is the only "supervising"
    of Word2Vec dataset that you need to worry about. And you can use the simplest
    possible tokenizer from Chapter 2 to achieve good results. For this space-delimited
    tokenized text, you can just use the `str.split()` method. And you can use case
    folding with `str.lower()` to cut your vocabulary size in half. Surprisingly,
    this is enough for Word2Vec to learn the meaning and connotation of words sufficiently
    well for the magic of analogy problems like you might see on an SAT test and even
    reason about the real-world objects and people.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您仍然需要告诉Word2Vec什么是单词。这是您需要担心的唯一"监督"Word2Vec数据集。您可以使用第2章中最简单的分词器来实现良好的结果。对于这种空格分词的文本，您只需使用`str.split()`方法。您可以使用`str.lower()`进行大小写折叠，将您的词汇表大小减半。令人惊讶的是，这已经足够让Word2Vec学会单词的含义和内涵，以至于能够解决类似SAT测试中可能会看到的类比问题，并且甚至能够推理现实世界的对象和人。
- en: '[PRE22]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now you can use your tokenizer on the torchtext dataset that contains this iterable
    sequence of rows of data, each with a "text" key for the WikiText2 data.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在包含数据行迭代序列的torchtext数据集上使用您的分词器，每行数据都有一个用于WikiText2数据的"text"键。
- en: '[PRE23]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You’ll need to compute the vocabulary for your dataset to handle the one-hot
    encoding and decoding for your neural network.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要为数据集计算词汇表，以处理神经网络的一热编码和解码。
- en: '[PRE24]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The one remaining feature engineering step is to create the skip-gram pairs
    using by windowizing the token sequences and then pairing up the skip-grams within
    those windows.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的特征工程步骤是通过对令牌序列进行窗口化，然后在这些窗口内配对跳字来创建跳字对。
- en: '[PRE25]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Once you apply the windowizer to your dataset it will have a 'window' key where
    the windows of tokens will be stored.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将 windowizer 应用于你的数据集，它将有一个 'window' 键，其中将存储标记的窗口。
- en: '[PRE26]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here’s your skip_gram generator function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你的 skip_gram 生成函数：
- en: '[PRE27]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Your neural network only needs the pairs of skip-grams from the windowed data:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 你的神经网络只需要窗口化数据中的跳字对：
- en: '[PRE28]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: And your DataLoader will take care of memory management for you. This will ensure
    your pipeline is reusable for virtually any size corpus, even all of Wikipedia.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 而且你的 DataLoader 会为你处理内存管理。这将确保你的管道可重用于几乎任何大小的语料库，甚至是整个维基百科。
- en: '[PRE29]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You need a one-hot encoder to turn your word pairs into one-hot vector pairs:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个独热编码器将你的词对转换成独热向量对：
- en: '[PRE30]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: To dispell some of the magic of the examples you saw earlier, you’ll train the
    network from scratch, just as you did in chapter 5\. You can see that a Word2Vec
    neural network is almost identical to your single-layer neural network from the
    previous chapter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了揭示你之前看到的示例的一些魔力，你将从头开始训练网络，就像你在第五章中所做的一样。你可以看到，Word2Vec 神经网络几乎与你之前章节中的单层神经网络相同。
- en: '[PRE31]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once you instantiate your Word2Vec model you are ready to create 100-D embeddings
    for the more than 20 thousand words in your vocabulary:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实例化你的 Word2Vec 模型，你就可以为你词汇表中的 20000 多个词创建 100 维嵌入：
- en: '[PRE32]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If you have a GPU you can send your model to the GPU to speed up the training:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有 GPU，你可以将模型发送到 GPU 来加快训练速度：
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Don’t worry if you do not have a GPU. On most modern CPUs this Word2Vec model
    will train in less than 15 minutes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有 GPU，不用担心。在大多数现代 CPU 上，这个 Word2Vec 模型将在不到 15 分钟内训练完毕。
- en: '[PRE34]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now is the fun part! You get to watch as Word2Vec quickly learns the meaning
    of "Dr." and thousands of other tokens, just by reading a lot of text. You can
    go get a tea or some chocolate or just have a 10 minute meditation to contemplate
    the meaning of life while your laptop contemplates the meaning of words. First,
    let’s define some training parameters
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分！你可以看到 Word2Vec 快速地学习了“Dr.”等成千上万个标记的含义，只是通过阅读大量的文本。你可以去泡杯茶或吃些巧克力，或者只是冥想
    10 分钟，思考生命的意义，而你的笔记本电脑则在思考单词的意义。首先，让我们定义一些训练参数
- en: '[PRE35]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 6.4.4 Computational tricks of Word2Vec
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 Word2Vec 的计算技巧
- en: 'After the initial publication, the performance of `word2vec` models have been
    improved through various computational tricks. In this section, we highlight the
    three key improvements that help word embeddings achieve greater accuracy with
    less computational resources or training data:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的出版之后，通过各种计算技巧提高了 `word2vec` 模型的性能。在本节中，我们重点介绍了三个关键改进，这些改进有助于词嵌入在更少的计算资源或训练数据下实现更高的准确性：
- en: Add frequent bigrams to the vocabulary
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将频繁的二元组添加到词汇表中
- en: Undersampling (subsampling) frequent tokens
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 欠采样（子采样）频繁的标记
- en: Undersampling of negative examples
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负例的欠采样
- en: Frequent bigrams
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 频繁的二元组
- en: 'Some words often occur in combination with other words creating a compound
    word. For example "Aaron" is often followed by "Swartz" and "AI" is often followed
    by "Ethics". Since the word "Swartz" would follow the word "Aaron" with an above
    average probability you probably want to create a single word vector for "Aaron
    Swartz" as a single compound proper noun. In order to improve the accuracy of
    the Word2Vec embedding for their applications involving proper nouns and compound
    words, Mikolov’s team included some bigrams and trigrams in their Word2Vec vocabulary.
    The team ^([[39](#_footnotedef_39 "View footnote.")]) used co-occurrence frequency
    to identify bigrams and trigrams that should be considered single terms using
    the following scoring function:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一些词经常与其他词组合在一起形成复合词。例如，“Aaron”经常跟在“Swartz”后面，“AI”经常跟在“Ethics”后面。由于单词“Swartz”跟在单词“Aaron”后面的概率高于平均水平，你可能想为“Aaron
    Swartz”创建一个单一的复合专有名词的单词向量。为了提高 Word2Vec 嵌入在涉及专有名词和复合词的应用中的准确性，Mikolov 的团队在他们的
    Word2Vec 词汇表中包括了一些二元组和三元组。团队使用共现频率来识别应该被视为单个术语的二元组和三元组，使用以下评分函数：
- en: '**Equation 6.5 Bigram scoring function**'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程 6.5 大二元组评分函数**'
- en: '![equation 6 5 score](images/equation_6_5_score.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![方程 6 5 分数](images/equation_6_5_score.png)'
- en: The bigram score is the number times the two words occur occur together divided
    by the counts of their occurrences separately. When words occur often enough next
    to each other, they will be included in the Word2Vec vocabulary as a paired token
    by replace the space with an underscore, as in `"ice_cream"`. You’ll notice that
    the vocabulary of many word embeddings models such as Word2vec contains terms
    like "New_York" or "San_Francisco". That way, these terms will be represented
    as a single vector instead of two separate ones, such as "San" and "Francisco".
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Bigram 分数是两个单词一起出现的次数除以它们分别出现的次数。当单词在一起出现的次数足够多时，它们将作为一对标记包含在 Word2Vec 词汇中，通过用下划线替换空格，比如
    `"ice_cream"`。你会注意到许多词嵌入模型的词汇，比如 Word2vec，包含诸如 "New_York" 或 "San_Francisco" 的术语。这样，这些术语将被表示为单个向量，而不是两个单独的向量，比如
    "San" 和 "Francisco"。
- en: Another effect of the word pairs is that the word combination often represents
    a different meaning than the sum of the vectors for the individual words. For
    example, the MLS soccer team "Portland Timbers" has a different meaning than the
    individual words "Portland" or "Timbers". But by adding oft-occurring bigrams
    to the Word2vec model, their embeddings can be included in your vocabulary of
    embeddings used in your model without you having to train custom embeddings for
    bigrams in your text.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 单词对的另一个影响是，单词组合通常表示的意义与单个单词的向量之和不同。例如，MLS 足球队 "Portland Timbers" 与单词 "Portland"
    或 "Timbers" 的单词具有不同的含义。但是通过将经常出现的二元组添加到 Word2vec 模型中，它们的嵌入可以包含在您的模型中使用的嵌入词汇中，而无需您为文本中的二元组训练自定义嵌入。
- en: A nice way to visualize word embeddings is to reduce their dimensionality to
    2-D using an algorithm such as Principal Component Analysis (PCA). This gives
    you a *map* of the word embeddings that you can plot to see their relationship
    to each other. When the words you are plotting are place names, such as US cities,
    this can reveal an interesting geographic dimension to words. Word embeddings
    provide you with the word’s "northness", "southness", "eastness", and "westness"
    quality. There’s even a bit of altitude in words that can be measured with a word
    embedding.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化词嵌入的一种好方法是使用诸如主成分分析（PCA）之类的算法将它们的维度降至二维。这样你就可以得到一个*地图*，展示词嵌入之间的关系。当你要绘制的词是地名，比如美国城市时，这可以揭示词语的有趣地理维度。词嵌入为你提供了词语的
    "北部性"、"南部性"、"东部性" 和 "西部性" 特性。甚至在词语中还有一点高度可以通过词嵌入来衡量。
- en: Figure 6.8 US city word map
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.8 美国城市词图
- en: '![us city word vector pca map](images/us-city-word-vector-pca-map.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![美国城市词向量PCA地图](images/us-city-word-vector-pca-map.png)'
- en: If you plotted words such as "New" and "York" in a 2-D plot such [6.8](#figure-city-map),
    they would not appear anywhere near the embedding for the term "New York."
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 2-D 图中绘制诸如 "New" 和 "York" 这样的词语，如[6.8](#figure-city-map)，它们不会出现在 "New York"
    一词的嵌入附近。
- en: Undersampling frequent tokens
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对频繁标记进行欠采样
- en: Another accuracy improvement to the original algorithm was to undersample (subsample)
    frequent words. This is also referred to as "undersampling the majority classes"
    in order to balance the class weights. Common words such as the articles "the"
    and "a" often don’t contain a lot of information and meaning relevant to most
    NLP problems so they are referred to as stop words. Mikolov and others often chose
    to *subsample* these words. Subsampling just means you randomly ignore them during
    your sampling of the corpus of continuous skip-grams or CBOWs. Many blogger will
    take this to the extreme and completely remove them from the corpus during prepossessing.
    Though subsampling or filtering stopwords may help your word embedding algorithm
    train faster, it can sometimes be counterproductive. And with modern computers
    and applications, a 1% improvement in training time is not likely to be worth
    the loss in precision of your word vectors. And the co-occurrence of stop words
    with other "words" in the corpus might create less meaningful connections between
    words muddying the Word2Vec representation with this false semantic similarity
    training.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始算法的另一个准确性改进是对频繁词语进行欠采样（子采样）。这也被称为“欠采样多数类”，以平衡类别权重。常见词语，如“the”和“a”，通常不包含大多数自然语言处理问题相关的信息和含义，因此被称为停用词。Mikolov
    和其他人经常选择*对这些词语进行子采样*。子采样意味着在连续 skip-gram 或 CBOW 的语料抽样过程中随机忽略它们。许多博主会把这一做法发挥到极致，在预处理过程中完全删除它们。虽然进行子采样或过滤停用词可能有助于让您的词向量算法训练得更快，但有时可能产生反效果。而且，在现代计算机和应用中，训练时间提高
    1% 不太可能抵消词向量精度的损失。而且停用词与语料库中其他“词语”的共现可能会导致词向量表示中出现含糊不清的词语之间的较不有意义的连接，从而通过错误的语义相似性训练来混淆
    Word2Vec 表示。
- en: Important
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: All words carry meaning, including stop words. So stop words should not be completely
    ignored or skipped while training your word vectors or composing your vocabulary.
    In addition, because word vectors are often used in generative models (like the
    model Cole used to compose sentences in this book), stop words and other common
    words must be included in your vocabulary and are allowed to affect the word vectors
    of their neighboring words.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 所有词语都有意义，包括停用词。因此，在训练词向量或组成词汇表时，不应完全忽略或跳过停用词。此外，由于词向量常用于生成模型（例如 Cole 在本书中用于组合句子的模型），停用词和其他常见词语必须包含在词汇表中，并允许其影响其相邻词语的词向量。
- en: 'To reduce the emphasis on frequent words like stop words, words are sampled
    during training in inverse proportion to their frequency. The effect of this is
    similar to the IDF affect on TF-IDF vectors. Frequent words are given less influence
    over the vector than the rarer words. Tomas Mikolov used the following equation
    to determine the probability of sampling a given word. This probability determines
    whether or not a particular word is included in a particular skip-gram during
    training:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少像停用词这样频繁出现的词语的强调，训练过程中对词语进行抽样，抽样概率与其频率成反比。这种影响类似于 IDF 对 TF-IDF 向量的影响。频繁出现的词语对向量的影响要小于罕见的词语。Tomas
    Mikolov 使用以下方程确定抽样给定词语的概率。该概率决定了在训练期间是否包含特定词语在特定 skip-gram 中：
- en: '**Equation 6.6 Subsampling probability in Mikolov’s Word2Vec paper**'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程式 6.6 中 Mikolov 的 Word2Vec 论文中的子采样概率**'
- en: '![equation 6 7](images/equation_6_7.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6.7](images/equation_6_7.png)'
- en: 'The `word2vec` C++ implementation uses a slightly different sampling probability
    than the one mentioned in the paper, but it has the same effect:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`word2vec` 的 C++ 实现使用了与论文中提到的略有不同的抽样概率，但效果相同：'
- en: '**Equation 6.7 Subsampling probability in Mikolov’s `word2vec` code**'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**方程式 6.7 中 Mikolov 的 `word2vec` 代码中的子采样概率**'
- en: '![equation 6 8](images/equation_6_8.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6.8](images/equation_6_8.png)'
- en: In the preceding equations, `f(w[i])` represents the frequency of a word across
    the corpus, and `t` represents a frequency threshold above which you want to apply
    the subsampling probability. The threshold depends on your corpus size, average
    document length, and the variety of words used in those documents. Values between
    `10^(-5)` and `10^(-6)` are often found in the literature.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，`f(w[i])` 表示语料库中词语的频率，`t` 表示希望在其上应用子采样概率的频率阈值。阈值取决于语料库大小、平均文档长度以及这些文档中使用的词语种类。文献中通常使用
    `10^(-5)` 和 `10^(-6)` 之间的值。
- en: If a word shows up 10 times across your entire corpus, and your corpus has a
    vocabulary of one million distinct words, and you set the subsampling threshold
    to `10^(-6)`, the probability of keeping the word in any particular *n*-gram is
    68%. You would skip it 32% of the time while composing your *n*-grams during tokenization.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词在整个语料库中出现了 10 次，而你的语料库有一百万个不同的单词，并且你将子采样阈值设为 `10^(-6)`，那么在任何特定的 *n*-gram
    中保留该单词的概率为 68%。在分词过程中，你将在 32% 的时间内跳过它们。
- en: Mikolov showed that subsampling improves the accuracy of the word vectors for
    tasks such as answering analogy questions.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Mikolov 表明，子采样提高了词向量的准确性，例如回答类比问题。
- en: Negative sampling
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负采样
- en: One last trick that Mikolov came up with was the idea of negative sampling.
    If a single training example with a pair of words is presented to the network
    it will cause all weights for the network to be updated. This changes the values
    of all the vectors for all the words in your vocabulary. But if your vocabulary
    contains thousands or millions of words, updating all the weights for the large
    one-hot vector is inefficient. To speed up the training of word vector models,
    Mikolov used negative sampling.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Mikolov 提出的最后一个技巧是负采样的概念。如果向网络提供了一对词的单个训练示例，它将导致网络的所有权重被更新。这会改变词汇表中所有单词的所有向量的值。但是如果你的词汇表包含数千个或数百万个单词，更新大型的独热向量的所有权重是低效的。为了加速词向量模型的训练，Mikolov
    使用了负采样。
- en: Instead of updating all word weights that weren’t included in the word window,
    Mikolov suggested sampling just a few negative samples (in the output vector)
    to update their weights. Instead of updating all weights, you pick *n* negative
    example word pairs (words that don’t match your target output for that example)
    and update the weights that contributed to their specific output. That way, the
    computation can be reduced dramatically and the performance of the trained network
    doesn’t decrease significantly.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Mikolov 提出，与其更新未包含在词窗口中的所有单词权重，不如只对几个负样本（在输出向量中）进行采样以更新它们的权重。不是更新所有权重，而是选择 *n*
    个负例词对（不匹配该示例的目标输出的单词）并更新导致其特定输出的权重。这样，计算量可以大大减少，并且训练网络的性能不会显著下降。
- en: Note
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If you train your word model with a small corpus, you might want to use a negative
    sampling rate of 5 to 20 samples. For larger corpora and vocabularies, you can
    reduce the negative sample rate to as low as two to five samples, according to
    Mikolov and his team.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用小语料库训练你的词模型，你可能想使用 5 到 20 个样本的负采样率。对于更大的语料库和词汇表，你可以将负采样率降低到两到五个样本，根据 Mikolov
    及其团队的说法。
- en: 6.4.5 Using the gensim.word2vec module
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 gensim.word2vec 模块
- en: If the previous section sounded too complicated, don’t worry. Various companies
    provide their pretrained word vector models, and popular NLP libraries for different
    programming languages allow you to use the pretrained models efficiently. In the
    following section, we look at how you can take advantage of the magic of word
    vectors. For word vectors you’ll use the popular `gensim` library, which you first
    saw in chapter 4.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的部分听起来太复杂，别担心。各种公司提供了预训练的词向量模型，而不同编程语言的流行 NLP 库允许你高效地使用这些预训练模型。在下一节中，我们将看看如何利用词向量的魔力。对于词向量，你将使用流行的
    `gensim` 库，这是你在第四章中首次看到的。
- en: 'If you’ve already installed the `nlpia` package,^([[40](#_footnotedef_40 "View
    footnote.")]) you can download a pretrained `word2vec` model with the following
    command:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了 `nlpia` 包，你可以使用以下命令下载预训练的 `word2vec` 模型：
- en: '[PRE37]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If that doesn’t work for you, or you like to "roll your own," you can do a
    Google search for `word2vec` models pretrained on Google News documents.^([[41](#_footnotedef_41
    "View footnote.")]) After you find and download the model in Google’s original
    binary format and put it in a local path, you can load it with the `gensim` package
    like this:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这对你不起作用，或者你喜欢自己动手，你可以搜索 `word2vec` 在 Google News 文档上预训练的模型。在找到并下载了 Google
    的原始二进制格式模型并将其放在本地路径后，你可以像这样使用 `gensim` 包加载它：
- en: '[PRE38]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Working with word vectors can be memory intensive. If your available memory
    is limited or if you don’t want to wait minutes for the word vector model to load,
    you can reduce the number of words loaded into memory by passing in the `limit`
    keyword argument. In the following example, you’ll load the 200k most common words
    from the Google News corpus:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单词向量可能会消耗大量内存。如果您的可用内存有限，或者如果您不想等待几分钟才能加载单词向量模型，您可以通过传递`limit`关键字参数来减少加载到内存中的单词数量。在以下示例中，您将从Google新闻语料库中加载前20万个最常见的单词：
- en: '[PRE39]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: But keep in mind that a word vector model with a limited vocabulary will lead
    to a lower performance of your NLP pipeline if your documents contain words that
    you haven’t loaded word vectors for. Therefore, you probably only want to limit
    the size of your word vector model during the development phase. For the rest
    of the examples in this chapter, you should use the complete Word2Vec model if
    you want to get the same results we show here.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，具有有限词汇量的单词向量模型会导致您的NLP流水线性能较低，如果您的文档包含尚未加载单词向量的单词。因此，在开发阶段，您可能只想限制单词向量模型的大小。对于本章中的其余示例，如果您想获得我们在此处展示的相同结果，则应使用完整的Word2Vec模型。
- en: The `gensim.KeyedVectors.most_similar()` method provides an efficient way to
    find the nearest neighbors for any given word vector. The keyword argument `positive`
    takes a list of the vectors to be added together, similar to your soccer team
    example from the beginning of this chapter. Similarly, you can use the `negative`
    argument for subtraction and to exclude unrelated terms. The argument `topn` determines
    how many related terms should be provided as a return value.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.KeyedVectors.most_similar()`方法提供了一种有效的方式来找到任何给定词向量的最近邻居。关键字参数`positive`接受一个要相加的向量列表，类似于本章开头的足球队示例。类似地，您可以使用`negative`参数进行减法操作并排除不相关的术语。参数`topn`确定应作为返回值提供多少相关术语。'
- en: Unlike a conventional thesaurus, Word2Vec synonomy (similarity) is a continuous
    score, a distance. This is because Word2Vec itself is a continuous vector space
    model. Word2Vec high dimensionality and continuous values for each dimension enable
    it to capture the full range of meaning for any given word. That’s why analogies
    and even zeugmas, odd juxtopositions of multiple meanings within the same word,
    are no problem. Handling analogies and zeugmas is a really big deal. Understanding
    analogies and zeugmas takes human-level understanding of the world, including
    common sense knowledge and reasoning.^([[42](#_footnotedef_42 "View footnote.")])
    Word embeddings are enough to give machines at least a passing understanding on
    the kinds of analogies you might see on an SAT quiz.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的同义词词典不同，Word2Vec的同义词（相似性）是一个连续的分数，一个距离。这是因为Word2Vec本身是一个连续的向量空间模型。Word2Vec高维度和每个维度的连续值使其能够捕捉任何给定单词的完整含义范围。这就是为什么类比甚至是zeugma，同一个词内多个意义的奇怪的并列，都不成问题。处理类比和zeugma是一件很重要的事情。理解类比和zeugma需要对世界的人类水平的理解，包括常识知识和推理[^42]。词嵌入足以让机器至少能够对您可能在SAT测验中看到的类比有一定了解。
- en: '[PRE40]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Word vector models also allow you to determine unrelated terms. The `gensim`
    library provides a method called `doesnt_match`:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量模型还允许您确定不相关的术语。`gensim`库提供了一个名为`doesnt_match`的方法：
- en: '[PRE41]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: To determine the most unrelated term of the list, the method returns the term
    with the highest distance to all other list terms.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定列表中最不相关的术语，该方法返回与所有其他列表术语的距离最大的术语。
- en: 'If you want to perform calculations (such as the famous example *king + woman
    - man = queen*, which was the example that got Mikolov and his advisor excited
    in the first place), you can do that by adding a `negative` argument to the `most_similar`
    method call:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想执行计算（例如著名的例子*king + woman - man = queen*，这是最初引起Mikolov和他的顾问兴奋的例子），您可以通过向`most_similar`方法调用添加`negative`参数来实现：
- en: '[PRE42]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `gensim` library also allows you to calculate the similarity between two
    terms. If you want to compare two words and determine their cosine similarity,
    use the method `.similarity()`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`库还允许您计算两个术语之间的相似度。如果您想比较两个单词并确定它们的余弦相似度，请使用方法`.similarity()`：'
- en: '[PRE43]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: If you want to develop your own functions and work with the raw word vectors,
    you can access them through Python’s square bracket syntax (`[]`) or the `get()`
    method on a `KeyedVector` instance. You can treat the loaded model object as a
    dictionary where your word of interest is the dictionary key. Each float in the
    returned array represents one of the vector dimensions. In the case of Google’s
    word model, your numpy arrays will have a shape of 1x300.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要开发自己的函数并使用原始单词向量进行工作，您可以通过 Python 的方括号语法（`[]`）或`KeyedVector`实例上的`get()`方法访问它们。您可以将加载的模型对象视为字典，其中您感兴趣的单词是字典键。返回的数组中的每个浮点数代表一个向量维度。在谷歌的词模型中，您的
    numpy 数组的形状将为 1x300。
- en: '[PRE44]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If you’re wondering what all those numbers *mean*, you can find out. But it
    would take a lot of work. You would need to examine some synonyms and see which
    of the 300 numbers in the array they all share. Alternatively you can find the
    linear combination of these numbers that make up dimensions for things like "placeness"
    and "femaleness", like you did at the beginning of this chapter.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想知道所有这些数字*意味着*什么，您可以找到答案。但这需要大量的工作。您需要检查一些同义词，并查看它们在数组中共享的 300 个数字中的哪些。或者，您可以找到这些数字的线性组合，构成像“位置”和“女性”之类的维度，就像您在本章的开头所做的那样。
- en: 6.4.6 Generating your own Word vector representations
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.6 生成自己的词向量表示
- en: In some cases you may want to create your own domain-specific word vector models.
    Doing so can improve the accuracy of your model if your NLP pipeline is processing
    documents that use words in a way that you wouldn’t find on Google News before
    2006, when Mikolov trained the reference `word2vec` model. Keep in mind, you need
    a *lot* of documents to do this as well as Google and Mikolov did. But if your
    words are particularly rare on Google News, or your texts use them in unique ways
    within a restricted domain, such as medical texts or transcripts, a domain-specific
    word model may improve your model accuracy. In the following section, we show
    you how to train your own `word2vec` model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望创建自己的特定领域的单词向量模型。这样做可以提高您的模型准确性，如果您的 NLP 管道正在处理使用词汇方式与 Google News
    中 2006 年以前 Mikolov 训练的参考`word2vec`模型不同的文档，则会更加如此。请记住，您需要大量的文档来做到这一点，就像 Google
    和 Mikolov 一样。但是，如果您的词在 Google News 上特别罕见，或者您的文本在受限领域内以独特的方式使用它们，比如医学文本或转录文本，则特定于领域的单词模型可能会提高您的模型准确性。在接下来的部分中，我们将向您展示如何训练您自己的`word2vec`模型。
- en: For the purpose of training a domain-specific `word2vec` model, you’ll again
    turn to `gensim`, but before you can start training the model, you’ll need to
    preprocess your corpus using tools you discovered in Chapter 2.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个特定于领域的`word2vec`模型，您将再次转向`gensim`，但在您开始训练模型之前，您需要使用第二章中发现的工具对语料库进行预处理。
- en: Preprocessing steps
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预处理步骤
- en: 'First you need to break your documents into sentences and the sentences into
    tokens. The `gensim` `word2vec` model expects a list of sentences, where each
    sentence is broken up into tokens. This prevents word vectors learning from irrelevant
    word occurrences in neighboring sentences. Your training input should look similar
    to the following structure:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要将文档分成句子，然后将句子分成标记。`gensim`的`word2vec`模型期望得到一个句子列表，其中每个句子被分成标记。这可以防止单词向量学习邻近句子中的无关单词出现。您的训练输入应该类似于以下结构：
- en: '[PRE45]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'To segment sentences and then convert sentences into tokens, you can apply
    the various strategies you learned in chapter 2\. Let’s add another one: Detector
    Morse is a sentence segmenter that improves upon the accuracy segmenter available
    in NLTK and `gensim` for some applications.^([[43](#_footnotedef_43 "View footnote.")])
    It has been pretrained on sentences from years of text in the Wall Street Journal.
    So if your corpus includes language similar to that in the WSJ, Detector Morse
    is likely to give you the highest accuracy currently possible. You can also retrain
    Detector Morse on your own dataset if you have a large set of sentences from your
    domain. Once you’ve converted your documents into lists of token lists (one for
    each sentence), you’re ready for your `word2vec` training.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 要将句子分割成标记，然后将句子转换为标记，您可以应用第二章中学到的各种策略。让我们再增加一个：Detector Morse 是一个句子分段器，它改进了
    NLTK 和`gensim`中提供的准确性分段器的某些应用场景。[[43](#_footnotedef_43 "查看脚注。")] 它已经在《华尔街日报》的多年文本中进行了预训练。因此，如果您的语料库包含与《华尔街日报》类似的语言，那么
    Detector Morse 很可能会为您提供目前可能的最高准确性。如果您拥有来自您领域的大量句子集合，还可以在自己的数据集上重新训练 Detector Morse。一旦您将文档转换为标记列表（每个句子一个列表），您就可以开始进行`word2vec`训练了。
- en: Train your domain-specific `word2vec` model
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练您的领域特定的`word2vec`模型
- en: 'Get started by loading the *word2vec* module:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加载*word2vec*模块开始：
- en: '[PRE46]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The training requires a few setup details.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 训练需要一些设置细节。
- en: Listing 6.2 Parameters to control word2vec model training
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2 控制word2vec模型训练的参数
- en: '[PRE47]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now you’re ready to start your training.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以开始培训了。
- en: Listing 6.3 Instantiating a word2vec model
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.3 实例化word2vec模型
- en: '[PRE48]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Depending on your corpus size and your CPU performance, the training will take
    a significant amount of time. For smaller corpora, the training can be completed
    in minutes. But for a comprehensive word model, the corpus will contain millions
    of sentences. You need to have several examples of all the different ways all
    the different words in your corpus are used. If you start processing larger corpora,
    such as the Wikipedia corpus, expect a much longer training time and a much larger
    memory consumption.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的语料库大小和CPU性能，训练将需要相当长的时间。对于较小的语料库，训练可以在几分钟内完成。但是对于一个全面的词模型，语料库将包含数百万句子。您需要有关语料库中所有不同单词的所有不同用法的几个示例。如果开始处理较大的语料库，例如维基百科语料库，预期训练时间会更长，并且内存消耗量会更大。
- en: 'In addition, Word2Vec models can consume quite a bit of memory. But remember
    that only the weight matrix for the hidden layer is of interest. Once you’ve trained
    your word model, you can reduce the memory footprint by about half if you freeze
    your model and discard the unnecessary information. The following command will
    discard the unneeded output weights of your neural network:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，Word2Vec模型可能会消耗大量内存。但请记住，只有隐藏层的权重矩阵才感兴趣。一旦训练了您的词模型，如果您冻结模型并丢弃不必要的信息，您可以将内存占用减少约一半。以下命令将丢弃神经网络的不需要的输出权重：
- en: '[PRE49]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `init_sims` method will freeze the model, storing the weights of the hidden
    layer and discarding the output weights that predict word co-ocurrences. The output
    weights aren’t part of the vector used for most Word2Vec applications. But the
    model cannot be trained further once the weights of the output layer have been
    discarded.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`init_sims`方法将冻结模型，存储隐藏层的权重并丢弃预测单词共现的输出权重。输出权重不是大多数Word2Vec应用程序所使用的向量的一部分。但是一旦丢弃了输出层的权重，模型就无法再进一步训练。'
- en: 'You can save the trained model with the following command and preserve it for
    later use:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令保存已训练的模型，并将其保留以供以后使用：
- en: '[PRE50]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: If you want to test your newly trained model, you can use it with the same method
    you learned in the previous section.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要测试您新训练的模型，可以使用与前一节学到的相同方法。
- en: Listing 6.4 Loading a saved `word2vec` model
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.4 加载已保存的`word2vec`模型
- en: '[PRE51]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 6.5 Word2Vec alternatives
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 Word2Vec替代方案
- en: Word2Vec was a breakthrough, but it relies on a neural network model that must
    be trained using backpropagation. Since Mikolov first popularized word embeddings,
    researchers have come up with increasingly more accurate and efficient ways to
    embed the meaning of words in a vector space.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一项突破，但它依赖于必须使用反向传播进行训练的神经网络模型。自Mikolov首次推广词嵌入以来，研究人员提出了越来越准确和高效的方式来将单词的含义嵌入向量空间。
- en: Word2vec
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Word2vec
- en: GloVE
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GloVE
- en: fastText
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: fastText
- en: Stanford NLP researchers ^([[44](#_footnotedef_44 "View footnote.")]) led by
    Jeffrey Pennington set about to understand the reason why Word2Vec worked so well
    and to find the cost function that was being optimized. They started by counting
    the word co-occurrences and recording them in a square matrix. They found they
    could compute the singular value decomposition (SVD)^([[45](#_footnotedef_45 "View
    footnote.")]) of this co-occurrence matrix, splitting it into the same two weight
    matrices that Word2Vec produces.^([[46](#_footnotedef_46 "View footnote.")]) The
    key was to normalize the co-occurrence matrix the same way. But in some cases,
    the Word2Vec model failed to converge to the same global optimum that the Stanford
    researchers were able to achieve with their SVD approach. It’s this direct optimization
    of the global vectors of word co-occurrences (co-occurrences across the entire
    corpus) that gives GloVe its name.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: GloVe can produce matrices equivalent to the input weight matrix and output
    weight matrix of Word2Vec, producing a language model with the same accuracy as
    Word2Vec but in much less time. GloVe speeds the process by using text data more
    efficiently. GloVe can be trained on smaller corpora and still converge.^([[47](#_footnotedef_47
    "View footnote.")]) And SVD algorithms have been refined for decades, so GloVe
    has a head start on debugging and algorithm optimization. Word2Vec relies on backpropagation
    to update the weights that form the word embeddings. Neural network backpropagation
    is less efficient than more mature optimization algorithms such as those used
    within SVD for GloVe.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though Word2Vec first popularized the concept of semantic reasoning with
    word vectors, your workhorse should probably be GloVe to train new word vector
    models. With GloVe you’ll be more likely to find the global optimum for those
    vector representations, giving you more accurate results. And spaCy utilizes it
    as its default embedding algorithm, so that when you run:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The results are computed using GloVe under the hood!
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages of GloVe:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Faster training
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better RAM/CPU efficiency (can handle larger documents)
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More efficient use of data (helps with smaller corpora)
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More accurate for the same amount of training
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.5.1 fastText
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Researchers from Facebook took the concept of Word2Vec one step further ^([[48](#_footnotedef_48
    "View footnote.")]) by adding a new twist to the model training. The new algorithm,
    which they named fastText, predicts the surrounding *n*-*character* grams rather
    than just the surrounding words, like Word2Vec does. For example, the word "whisper"
    would generate the following 2- and 3-character grams:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: fastText is then training a vector representation for every *n*-character gram
    (called "subwords"), which includes words, misspelled words, partial words, and
    even single characters. The advantage of this approach is that it handles rare
    or new words much better than the original Word2Vec approach.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: The fastText tokenizer will create vectors for two halves of a longer word if
    the longer word used much less often than the subwords that make it up. For example,
    fastText might create vectors for "super" and "woman" if your corpus only mentions
    "Superwoman" once or twice but uses "super" and "woman" thousands of times. And
    if your fastText language model encounters the word "Superwoman" in the real world
    after training is over, it sums the vectors for "Super" and "woman" together to
    create a vector for the word "Superwoman". This reduces the number of words that
    fastText will have to assign the generic Out of Vocabulary (OOV) vector to. In
    the "mind" of your NLU pipeline, the OOV word vector looks like "Unkown Word".
    It has the same effect as if you heard a foreign word in a completely unfamiliar
    language. While Word2vec only "knows" how to embed words it has seen before, fastText
    is much more flexible due to its subword approach. It is also relatively lightweight
    and operates faster.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 分词器将为较长的单词的两半创建向量，如果该较长的单词的使用频率远低于构成它的子词。例如，如果您的语料库只提到 "Superwoman"
    一两次，但却成千上万次使用 "super" 和 "woman"，那么 fastText 可能会为 "super" 和 "woman" 创建向量。如果您的 fastText
    语言模型在训练结束后在现实世界中遇到单词 "Superwoman"，它将合并 "Super" 和 "woman" 的向量以创建单词 "Superwoman"
    的向量。这减少了 fastText 需要分配通用的 Out of Vocabulary（OOV）向量的单词数量。在您的 NLU 流水线的 "思维" 中，OOV
    单词向量看起来像 "未知单词"。它的效果与您在完全陌生的语言中听到一个外来单词的效果相同。虽然 Word2vec 只能 "知道" 如何嵌入它以前见过的单词，但
    fastText 由于其子词方法而更加灵活。它还相对轻量且运行速度更快。
- en: As part of the fastText release, Facebook published pretrained fastText models
    for 294 languages. On the Github page of Facebook research, ^([[49](#_footnotedef_49
    "View footnote.")]) you can find models ranging from *Abkhazian* to *Zulu*. The
    model collection even includes rare languages such as *Saterland Frisian*, which
    is only spoken by a handful of Germans. The pretrained fastText models provided
    by Facebook have only been trained on the available Wikipedia corpora. Therefore
    the vocabulary and accuracy of the models will vary across languages.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 fastText 发布的一部分，Facebook 发布了 294 种语言的预训练 fastText 模型。在 Facebook 研究的 Github
    页面^([[49](#_footnotedef_49 "View footnote.")])上，您可以找到从 *阿布哈兹语* 到 *祖鲁语* 的模型。模型集合甚至包括罕见的语言，如
    *萨特兰弗里西亚语*，该语言仅由少数德国人使用。Facebook 提供的预训练 fastText 模型仅在可用的维基百科语料库上进行了训练。因此，模型的词汇量和准确性会因语言而异。
- en: We’ve included the fastText logic for creating new vectors for OOV words in
    the `nessvec` package. We’ve also added an enhancement to the fastText pipeline
    to handle misspellings and typos using Peter Norvig’s famously elegant spelling
    corrector algorithm.^([[50](#_footnotedef_50 "View footnote.")]) This will give
    you the best of both worlds, an understandable training algorithm and a robust
    inference or prediction model when you need to use your trained vectors in the
    real world.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `nessvec` 包中包含了用于为 OOV 单词创建新向量的 fastText 逻辑。我们还通过 Peter Norvig 的著名的优雅拼写纠正算法^([[50](#_footnotedef_50
    "View footnote.")])增强了 fastText 流水线，以处理拼写错误和错别字。这将为您提供最佳选择，即一个易于理解的训练算法和当您需要在现实世界中使用您训练过的向量时，一个强大的推断或预测模型。
- en: Power up your NLP with pretrained model
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用预训练模型增强您的 NLP
- en: Supercharge your NLP pipeline by taking advantage of the open source pretrained
    embeddings from the most powerful corporations on the planet. Pretrained fastText
    vectors are available in almost every language conceivable. An If you want to
    see the all the options available for your word embeddings check out the fastText
    model repository ([https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md](docs.html)).
    And for multilingual power you can find combined models for many of the 157 languages
    supported in the Common Crawl version of fastText embeddings ([https://fasttext.cc/docs/en/crawl-vectors.html](en.html)).
    If you want you can download all the different versions of the embeddings for
    your language using the *bin+text* links on the fastText pages. But if you want
    to save some time and just download the 1 million
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用来自地球上最强大的公司的开源预训练嵌入来提升您的 NLP 流水线的能力。预训练的 fastText 向量几乎可以在任何可想象的语言中找到。如果您想查看您的词嵌入的所有可用选项，请查看
    fastText 模型存储库（[https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md](docs.html)）。而对于多语言支持，您可以在
    fastText 嵌入的 Common Crawl 版本中找到许多支持的 157 种语言的组合模型（[https://fasttext.cc/docs/en/crawl-vectors.html](en.html)）。如果您愿意，您可以使用
    fastText 页面上的 *bin+text* 链接下载您语言的所有不同版本的嵌入。但如果您想节省一些时间，只需下载 100 万...
- en: Warning
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: The *bin+text* `wiki.en.zip` file ([https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip](vectors-wiki.html))
    is *9.6 GB*. The text-only `wiki.en.vec` file is *6.1 GB* ([https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec](vectors-wiki.html)).
    If you use the `nessvec` package rather than `gensim` it will download just the
    600MB `wiki-news-300d-1M.vec.zip` file ([https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip](vectors-english.html)).
    That `wiki-news-300d-1M.vec.zip` contains the 300-D vectors for the 1 million
    most popular words (case-insensitive) from Wikipedia and news web pages.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '*bin+text* `wiki.en.zip` 文件（[https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip](vectors-wiki.html)）大小为
    *9.6 GB*。仅文本 `wiki.en.vec` 文件大小为 *6.1 GB*（[https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec](vectors-wiki.html)）。如果您使用的是
    `nessvec` 包而不是 `gensim`，它将仅下载大小为 600MB 的 `wiki-news-300d-1M.vec.zip` 文件（[https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip](vectors-english.html)）。该
    `wiki-news-300d-1M.vec.zip` 包含来自维基百科和新闻网页中最受欢迎的 100 万个单词（不区分大小写）的 300-D 向量。'
- en: The `nessvec` package will create a memory-mapped `DataFrame` of all your pretrained
    vectors. The memory-mapped file (`.hdf5`) keeps you from running out of memory
    (RAM) on your computer by lazy-loading just the vectors you need, when you need
    them.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`nessvec` 包将创建一个内存映射的 `DataFrame`，其中包含所有预训练向量。内存映射文件（`.hdf5`）通过按需惰性加载你所需的向量，可以防止你的计算机内存（RAM）不足。'
- en: '[PRE54]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: To turbocharge your word embedding pipeline you can use Bloom embeddings. Bloom
    embeddings aren’t a new algorithm for creating embeddings, but a faster more accurate
    indexing approach for storing and retrieving a high dimensional vector. The vectors
    in a Bloom embedding table each represent the meaning of two or more words combined
    together. The trick is to subtract out the words you don’t need in order to recreate
    the original embedding that you’re looking for. Fortunately SpaCy has implemented
    all this efficiency under the hood with its v2.0 language model. This is how SpaCy
    can create word embeddings for millions of words while storing only 20k unique
    vectors.^([[51](#_footnotedef_51 "View footnote.")])
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速您的词嵌入流程，您可以使用 Bloom 嵌入。Bloom 嵌入不是用于创建嵌入的新算法，而是用于存储和检索高维向量的更快、更精确的索引方法。Bloom
    嵌入表中的向量每个都代表两个或更多单词的组合含义。诀窍是减去您不需要的单词，以便重新创建您正在寻找的原始嵌入。幸运的是，SpaCy 在其 v2.0 语言模型中已经实现了所有这些效率。这就是
    SpaCy 如何能够为数百万个单词创建单词嵌入，同时仅存储 20k 个唯一的向量。
- en: 6.5.2 Word2Vec vs LSA
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 Word2Vec vs LSA
- en: You might now be wondering how word embeddings compare to the LSA topic-word
    vectors of chapter 4\. These are word embeddings that you created using PCA (principal
    component analysis) on your TF-IDF vectors. And LSA also gives you topic-document
    vectors which you used as embeddings of entire documents. LSA topic-document vectors
    are the sum of the topic-word vectors for all the words in whatever document you
    create the embedding for. If you wanted to get a word vector for an entire document
    that is analogous to topic-document vectors, you’d sum all the word vectors for
    your document. That’s pretty close to how Doc2vec document vectors work.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可能想知道词嵌入与第 4 章的 LSA 主题-单词向量相比如何。这些是使用 PCA（主成分分析）在您的 TF-IDF 向量上创建的词嵌入。LSA
    还为您提供了主题-文档向量，您可以将其用作整个文档的嵌入。LSA 主题-文档向量是您为任何文档创建嵌入的所有单词的主题-单词向量的总和。如果您想为整个文档获取一个与主题-文档向量类似的单词向量，您将为您的文档中的所有单词求和。这非常接近
    Doc2vec 文档向量的工作原理。
- en: If your LSA matrix of topic vectors is of size `N[words] × N[topics]`, the LSA
    word vectors are the rows of that LSA matrix. These row vectors capture the meaning
    of words in a sequence of around 200 to 300 real values, like Word2Vec does. And
    LSA topic-word vectors are just as useful as Word2Vec vectors for finding both
    related and unrelated terms. As you learned in the GloVe discussion, Word2Vec
    vectors can be created using the exact same SVD algorithm used for LSA. But Word2Vec
    gets more use out of the same number of words in its documents by creating a sliding
    window that overlaps from one document to the next. This way it can reuse the
    same words five times before sliding on.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的主题向量的 LSA 矩阵的大小为 `N[words] × N[topics]`，则 LSA 单词向量是该 LSA 矩阵的行。这些行向量捕捉了单词的含义，其中有大约
    200 到 300 个实值，就像 Word2Vec 一样。而 LSA 主题-单词向量和 Word2Vec 向量一样有用，可以用于查找相关和不相关的术语。正如您在
    GloVe 讨论中了解到的那样，Word2Vec 向量可以使用用于 LSA 的相同 SVD 算法来创建。但是，Word2Vec 通过创建一个从一个文档到下一个文档重叠的滑动窗口，从而更多地利用了文档中相同数量的单词。这样它可以在滑动之前重复使用相同的单词五次。
- en: What about incremental or online training? Both LSA and Word2Vec algorithms
    allow adding new documents to your corpus and adjusting your existing word vectors
    to account for the co-occurrences in the new documents. But only the existing
    "bins" in your lexicon can be updated. Adding completely new words would change
    the total size of your vocabulary and therefore your one-hot vectors would change.
    That requires starting the training over if you want to capture the new word in
    your model.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 那么增量或在线训练呢？LSA 和 Word2Vec 算法都允许将新文档添加到你的语料库中，并调整现有的单词向量以考虑新文档中的共现。但只能更新你词汇表中的现有
    "桶"。添加全新的词汇会改变你词汇表的总大小，因此你的独热向量也会改变。如果你想在模型中捕捉新词，那就需要重新开始训练。
- en: LSA trains faster than Word2Vec does. And for long documents, it does a better
    job of discriminating and clustering those documents. In fact, Stanford researchers
    used this faster PCA-based method to train the GloVE vectors. You can compare
    the three most popular word embeddings using the `nessvec` package.^([[52](#_footnotedef_52
    "View footnote.")])
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: LSA 比 Word2Vec 训练更快。对于长文档，它在区分和聚类这些文档方面表现更好。事实上，斯坦福的研究人员使用了这种更快的基于 PCA 的方法来训练
    GloVE 向量。你可以使用 `nessvec` 包比较三种最流行的词嵌入。^([[52](#_footnotedef_52 "View footnote.")])
- en: The "killer app" for Word2Vec is the semantic reasoning that it made possible.
    LSA topic-word vectors can do that, too, but it usually isn’t accurate. You’d
    have to break documents into sentences and then only use short phrases to train
    your LSA model if you want to approach the accuracy and "wow" factor of Word2Vec
    reasoning. With Word2Vec you can determine the answer to questions like *Harry
    Potter + "University = Hogwarts* As a great example for domain-specific `word2vec`
    models, check out the models for words from Harry Potter, the Lord of the Rings
    by ^([[53](#_footnotedef_53 "View footnote.")]).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 的 "杀手级应用" 是它可能的语义推理。LSA 主题-词向量也可以做到，但通常不够准确。如果你想要接近 Word2Vec 推理的准确性和
    "哇" 因素，你需要将文档分成句子，然后只使用短语来训练你的 LSA 模型。使用 Word2Vec，你可以回答像 *哈利·波特 + "大学" = 霍格沃茨*
    这样的问题。作为领域特定 `word2vec` 模型的绝佳示例，请查看哈利·波特、指环王的单词模型，参见 ^([[53](#_footnotedef_53
    "View footnote.")])。
- en: 'Advantages of LSA:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: LSA 的优势：
- en: Faster training
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快的训练
- en: Better discrimination between longer documents
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好地区分长文档
- en: 'Advantages of Word2Vec and GloVe:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 和 GloVe 的优势：
- en: More efficient use of large corpora
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更有效地利用大型语料库
- en: More accurate reasoning with words, such as answering analogy questions
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更准确的词语推理，比如回答类比问题
- en: 6.5.3 Static vs contextualized embeddings
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.3 静态 vs 上下文化的嵌入
- en: 'There are two kinds of word embeddings you may encounter in the real world:
    static and contextualized.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，你可能会遇到两种类型的词嵌入：静态和上下文化的。
- en: Static word embeddings can be used on individual words or N-Grams in isolation.
    And once the training is completed, the vectors remain fixed. These are the kinds
    of word embeddings you’ll use for analogy and other word vector reasoning problems
    you want to solve. You’ll train a language model to create static word embeddings
    here. The context of a word will only be used to train the model. Once your word
    embeddings are trained, you will not use the context of a word’s usage to adjust
    your word embeddings at all as you are *using* your trained word embeddings. This
    means that the different senses or meanings of a word are all smushed together
    into a single static vector. All the embeddings we have seen so far - Word2Vec,
    GloVe and fasttext - are static embeddings. Word2Vec will return the same embedding
    for the word "bank" in the name "World Bank" and in the expression "river bank".
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 静态词嵌入可以单独用于单词或独立的 N-Grams。一旦训练完成，向量就保持不变。这些是你用于类比和其他想解决的词向量推理问题的词嵌入类型。你将在这里训练一个语言模型来创建静态词嵌入。单词的上下文只会用来训练模型。一旦你的词嵌入训练完成，你将不会使用单词使用的上下文来调整你的词嵌入，因为你
    *使用* 的是你训练好的词嵌入。这意味着一个词的不同含义或意思都被压缩成一个单一的静态向量。到目前为止我们见过的所有嵌入 - Word2Vec、GloVe
    和 fasttext - 都是静态嵌入。Word2Vec 在 "World Bank" 这个名字和 "river bank" 这个表达中都会返回相同的嵌入。
- en: In contrast, contextualized word embeddings can be updated or refined based
    on the embeddings and words that come before or after. And the order a word appears
    relative to other words matters for contextualized word embeddings. This means
    that for NLU of the bigram "not happy" it would have an embedding much closer
    to the embedding of "unhappy" for contextualized word embeddings than for static
    word embeddings.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，上下文化的词嵌入可以根据前后的嵌入和单词进行更新或细化。单词出现的顺序相对于其他单词的顺序对于上下文化的词嵌入很重要。这意味着对于“not happy”这个二元组的自然语言理解（NLU），在上下文化的词嵌入中，它的嵌入会比静态词嵌入更接近于“unhappy”的嵌入。
- en: As you can imagine, contextualized embeddings can be much more useful for a
    variety of applications, such as semantic search. A huge breakthrough in creating
    them came with the introduction of bi-directional transformer neural networks,
    such as BERT (Bi-directional Encoder Representations for Transformers), which
    we’re going to cover in depth in Chapter 9\. BERT embeddings outperformed older
    algorithms such as World2Vec and GloVe because it takes into account not only
    the context to the right and to the left of the word it embeds but also the order
    of the words in the sentence. As such, it became a popular choice for many NLP
    applications.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所想象的那样，上下文化的嵌入对各种应用非常有用，例如语义搜索。在创建它们方面取得了巨大的突破，这是因为引入了双向变压器神经网络，例如 BERT（双向编码器变换器），我们将在第
    9 章深入讨论。BERT 嵌入的性能优于旧的算法，例如 World2Vec 和 GloVe，因为它不仅考虑了嵌入的单词右侧和左侧的上下文，还考虑了句子中单词的顺序。因此，它成为了许多自然语言处理应用的热门选择。
- en: 6.5.4 Visualizing word relationships
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.4 可视化单词关系
- en: The semantic word relationships can be powerful and their visualizations can
    lead to interesting discoveries. In this section, we demonstrate steps to visualize
    the word vectors in 2D.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 语义单词关系可能非常强大，它们的可视化可以导致有趣的发现。在本节中，我们演示了将单词向量可视化为 2D 的步骤。
- en: To get started, let’s load all the word vectors from the Google Word2Vec model
    of the Google News corpus. As you can imagine, this corpus included a lot of mentions
    of "Portland" and "Oregon" and a lot of other city and state names. You’ll use
    the `nlpia` package to keep things simple, so you can start playing with Word2Vec
    vectors quickly.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们从 Google 新闻语料库的 Google Word2Vec 模型中加载所有单词向量。您可以想象，这个语料库包含了大量关于“Portland”和“Oregon”的提及，以及许多其他城市和州的名称。您将使用
    `nlpia` 包来保持简单，这样您就可以快速开始使用 Word2Vec 向量。
- en: Listing 6.5 Load a pretrained `FastText` language model using `nlpia`
  id: totrans-403
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.5 使用 `nlpia` 加载预训练的 `FastText` 语言模型
- en: '[PRE55]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Warning
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: 'The Google News `word2vec` model is huge: 3 million words with 300 vector dimensions
    each. The complete word vector model requires 3 GB of available memory. If your
    available memory is limited or you quickly want to load a few most frequent terms
    from the word model, check out chapter 13.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: Google 新闻的`word2vec`模型非常庞大：300个向量维度的300万个单词。完整的单词向量模型需要3 GB 的可用内存。如果您的可用内存有限，或者您想快速从单词模型中加载一些最常见的术语，请查看第
    13 章。
- en: 'This `KeyedVectors` object in `gensim` now holds a table of 3 million Word2Vec
    vectors. We loaded these vectors from a file created by Google to store a Word2Vec
    model that they trained on a large corpus based on Google News articles. There
    should definitely be a lot of words for states and cities in all those news articles.
    The following listing shows just a few of the words in the vocabulary, starting
    at the 1 millionth word:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 `gensim` 中的这个 `KeyedVectors` 对象保存了一个包含 300 万个 Word2Vec 向量的表。我们从 Google
    创建的文件中加载了这些向量，该文件用于存储他们在大量 Google 新闻文章上训练的 Word2Vec 模型。在所有这些新闻文章中，肯定有很多州和城市的单词。以下列表仅显示了词汇表中的一些单词，从第一百万个词开始：
- en: Listing 6.6 Examine word2vec vocabulary frequencies
  id: totrans-408
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.6 检查 word2vec 词汇频率
- en: '[PRE56]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Notice that compound words and common *n*-grams are joined together with an
    underscore character ("\_"). Also notice that the "value" in the key-value mapping
    is a `gensim` `Vocab` object that contains not only the index location for a word,
    so you can retrieve the Word2Vec vector, but also the number of times it occurred
    in the Google News corpus.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，复合词和常见的 *n*-gram 用下划线字符（"_"）连接在一起。还请注意，键值映射中的“值”是一个包含词的索引位置的 `gensim` `Vocab`
    对象，因此您可以检索 Word2Vec 向量，但还包括它在 Google 新闻语料库中出现的次数。
- en: 'As you’ve seen earlier, if you want to retrieve the 300-D vector for a particular
    word, you can use the square brackets on this `KeyedVectors` object to `*getitem*`
    any word or *n*-gram:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前所看到的，如果你想检索特定词的 300-D 向量，你可以在这个 `KeyedVectors` 对象上使用方括号 `*getitem*` 任何词或
    *n*-gram：
- en: '[PRE57]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The reason we chose the 1 millionth word (in lexical alphabetic order) is because
    the first several thousand "words" are punctuation sequences like "#\##\#\#" and
    other symbols that occurred a lot in the Google News corpus. We just got lucky
    that "Illini" showed up in your list. The word "Illini" refers to a group of people,
    usually football players and fans, rather than a single geographic region like
    "Illinois" — where most fans of the "Fighting Illini" live. Let’s see how close
    this "Illini" vector is to the vector for "Illinois":'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择第 100 万个词（按词典字母顺序）的原因是因为前几千个 "词" 是类似 "#\##\#\#" 和其他符号的标点符号，在谷歌新闻语料库中经常出现。我们只是幸运地发现了
    "Illini" 出现在你的列表中。单词 "Illini" 指的是一群人，通常是足球运动员和球迷，而不是像 "Illinois" 那样的单一地理区域 ——
    大多数 "Fighting Illini" 的球迷生活在那里。让我们看看这个 "Illini" 向量跟 "Illinois" 向量有多接近：
- en: Listing 6.7 Distance between "Illinois" and "Illini"
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.7 "Illinois" 和 "Illini" 之间的距离
- en: '[PRE58]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: These distances mean that the words "Illini" and "Illinois" are only moderately
    close to one another in meaning.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这些距离意味着单词 "Illini" 和 "Illinois" 在含义上只是适度接近的。
- en: Now let’s retrieve all the Word2Vec vectors for US cities so you can use their
    distances to plot them on a 2D map of meaning. How would you find all the cities
    and states in that Word2Vec vocabulary in that `KeyedVectors` object? You could
    use cosine distance like you did in the previous listing to find all the vectors
    that are close to the words "state" or "city".
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检索所有美国城市的 Word2Vec 向量，这样你就可以使用它们的距离将它们绘制到一个意义的二维地图上。你将如何在那个 `KeyedVectors`
    对象中找到所有城市和州的词汇表？你可以像在上一个清单中一样使用余弦距离来找到所有接近 "state" 或 "city" 的词的向量。
- en: But rather than reading through all 3 million words and word vectors, lets load
    another dataset containing a list of cities and states (regions) from around the
    world.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，与其阅读全部 300 万个词和词向量，不如加载另一个数据集，其中包含来自世界各地的城市和州（地区）列表。
- en: Listing 6.8 Some US city data
  id: totrans-419
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.8 一些美国城市数据
- en: '[PRE59]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This dataset from Geocities contains a lot of information, including latitude,
    longitude, and population. You could use this for some fun visualizations or comparisons
    between geographic distance and Word2Vec distance. But for now you’re just going
    to try to map that Word2Vec distance on a 2D plane and see what it looks like.
    Let’s focus on just the United States for now:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Geocities 的这个数据集包含大量信息，包括纬度、经度和人口。你可以用它进行一些有趣的可视化或地理距离与 Word2Vec 距离的比较。但是现在你只是尝试将
    Word2Vec 距离映射到二维平面上，并看看它是什么样子的。现在让我们专注于美国：
- en: Listing 6.9 Some US state data
  id: totrans-422
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.9 一些美国州数据
- en: '[PRE60]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now you have a full state name for each city in addition to its abbreviation.
    Let’s check to see which of those state names and city names exist in your Word2Vec
    vocabulary:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，除了缩写外，你还可以为每个城市提供完整的州名。让我们检查一下那些州名和城市名是否存在于你的 Word2Vec 词汇表中：
- en: '[PRE61]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Even when you only look at United States cities, you’ll find a lot of large
    cities with the same name, like Portland, Oregon and Portland, Maine. So let’s
    incorporate into your city vector the essence of the state where that city is
    located. To combine the meanings of words in Word2Vec, you add the vectors together.
    That’s the magic of "Analogy reasoning."
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你只看美国城市，你也会发现很多同名的大城市，比如俄勒冈州的波特兰和缅因州的波特兰。所以让我们将该城市所在州的本质合并到你的城市向量中。要结合 Word2Vec
    中词的含义，你需要将向量相加。这就是 "类比推理" 的魔力。
- en: Here’s one way to add the Word2Vecs for the states to the vectors for the cities
    and put all these new vectors in a big DataFrame. We use either the full name
    of a state or just the abbreviations (whichever one is in your Word2Vec vocabulary).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将各州的 Word2Vec 向量添加到城市向量中并将所有这些新向量放入一个大的 DataFrame 的一种方法。我们使用州的全名或缩写（在你的 Word2Vec
    词汇表中的任何一个）。
- en: Listing 6.10 Augment city word vectors with US state word vectors
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 6.10 用美国州词向量扩充城市词向量
- en: '[PRE62]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Depending on your corpus, your word relationship can represent different attributes,
    such as geographical proximity or cultural or economic similarities. But the relationships
    heavily depend on the training corpus, and they will reflect the corpus.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的语料库，你的词关系可以代表不同的属性，例如地理接近性或文化或经济相似性。但是这些关系在很大程度上取决于训练语料库，并且它们会反映出语料库。
- en: Word vectors are biased!
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 词向量是有偏的！
- en: Word vectors learn word relationships based on the training corpus. If your
    corpus is about finance then your "bank" word vector will be mainly about businesses
    that hold deposits. If your corpus is about geology the your "bank" word vector
    will be trained on associations with rivers and streams. And if you corpus is
    mostly about a matriarchal society with women bankers and men washing clothes
    in the river, then your word vectors would take on that gender bias.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量是根据训练语料库中的词关系进行学习的。如果你的语料库是关于金融的，那么你的"银行"词向量主要是关于进行存款的企业。如果你的语料库是关于地质学的，那么你的"银行"词向量将会与河流和溪流有关。如果你的语料库主要是关于母系社会，有女性银行家和男性在河边洗衣服，那么你的词向量会带有性别偏见。
- en: The following example shows the gender bias of a word model trained on Google
    News articles. If you calculate the distance between "man" and "nurse" and compare
    that to the distance between "woman" and "nurse", you’ll be able to see the bias.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子展示了在Google新闻文章上训练的词模型的性别偏见。如果你计算"男人"和"护士"之间的距离，并将其与"女人"和"护士"之间的距离进行比较，你将能够看到这种偏见。
- en: '[PRE63]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Identifying and compensating for biases like this is a challenge for any NLP
    practitioner that trains her models on documents written in a biased world.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 辨识和补偿这样的偏差对任何在偏见世界中训练模型的NLP从业者来说都是一个挑战。
- en: The news articles used as the training corpus share a common component, which
    is the semantical similarity of the cities. Semantically similar locations in
    the articles seems to interchangeable and therefore the word model learned that
    they are similar. If you would have trained on a different corpus, your word relationship
    might have differed.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 作为训练语料库使用的新闻文章共享一个共同组件，那就是城市的语义相似性。文章中语义上相似的地点似乎是可以互换的，因此词模型学习到它们是相似的。如果你的训练语料库不同，你的词关系可能会有所不同。
- en: Cities that are similar in size and culture are clustered close together despite
    being far apart geographically, such as San Diego and San Jose, or vacation destinations
    such as Honolulu and Reno.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在地理上相距很远的城市，却因为大小和文化相似而聚集在一起，比如圣地亚哥和圣荷塞，或者类似度假地点的火奴鲁鲁和里诺。
- en: Fortunately you can use conventional algebra to add the vectors for cities to
    the vectors for states and state abbreviations. As you discovered in chapter 4,
    you can use tools such as the principal components analysis (PCA) to reduce the
    vector dimensions from your 300 dimensions to a human-understandable 2D representation.
    PCA enables you to see the projection or "shadow" of these 300D vectors in a 2D
    plot. Best of all, the PCA algorithm ensures that this projection is the best
    possible view of your data, keeping the vectors as far apart as possible. PCA
    is like a good photographer that looks at something from every possible angle
    before composing the optimal photograph.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你可以使用传统的代数方法将城市的向量加到州和州的缩写的向量中。就像你在第4章中发现的那样，你可以使用主成分分析（PCA）等工具，将你的300维向量维度减少为人类可理解的2D表示。PCA使你能够在2D图中看到这些300D向量的投影或"阴影"。最重要的是，PCA算法确保了这个投影是你的数据的最佳视图，尽可能地保持向量相隔较远。PCA就像一位优秀的摄影师，在构图之前从各个可能的角度查看某物。
- en: You don’t even have to normalize the length of the vectors after summing the
    city + state + abbrev vectors, because PCA takes care of that for you.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在对城市+州+缩写向量求和之后，甚至都不需要对向量长度进行归一化，因为PCA会为你处理这一切。
- en: We saved these "augmented" city word vectors in the `nlpia` package so you can
    load them to use in your application. In the following code, you use PCA to project
    them onto a 2D plot.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些"增强的"城市词向量保存在`nlpia`软件包中，这样你就可以加载它们在你的应用程序中使用。在下面的代码中，你可以使用PCA将它们投影到一个2D图中。
- en: Listing 6.11 Bubble chart of US cities
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.11 美国城市的泡泡图
- en: '[PRE64]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Figure 6.8 shows the 2D projection of all these 300-D word vectors for US cities:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8显示了所有这些美国城市的300维词向量的2D投影：
- en: Figure 6.9 Google News Word2Vec 300-D vectors projected onto a 2D map using
    PCA
  id: totrans-444
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9 Google新闻Word2Vec 300-D向量使用PCA投影到2D地图上
- en: '![us city word vector pca map](images/us-city-word-vector-pca-map.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![美国城市词向量PCA图](images/us-city-word-vector-pca-map.png)'
- en: Note
  id: totrans-446
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Low semantic distance (distance values close to zero) represent high similarity
    between words. The semantic distance, or "meaning" distance, is determined by
    the words occurring nearby in the documents used for training. The Word2Vec vectors
    for two terms are *close* to each other in word vector space if they are often
    used in similar contexts (used with similar words nearby). For example "San Francisco"
    is *close* to "California" because they often occur nearby in sentences and the
    distribution of words used near them are similar. A large distance between two
    terms expresses a low likelihood of shared context and shared meaning (they are
    semantically dissimilar), such as "cars" and "peanuts".
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to explore the city map shown in figure 6.8, or try your hand
    at plotting some vectors of your own, listing 6.12 shows you how. We built a wrapper
    for Plotly’s offline plotting API that should help it handle DataFrames where
    you’ve "denormalized" your data. The Plotly wrapper expects a DataFrame with a
    row for each sample and column for features you’d like to plot. These can be categorical
    features (such as time zones) and continuous real-valued features (such as city
    population). The resulting plots are interactive and useful for exploring many
    types of machine learning data, especially vector-representations of complex things
    such as words and documents.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Bubble plot of US city word vectors
  id: totrans-449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: To produce the 2D representations of your 300-D word vectors, you need to use
    a dimension reduction technique. We used PCA. To reduce the amount of information
    lost during the compression from 300-D to 2D, reducing the range of information
    contained in the input vectors helps. So you limited your word vectors to those
    associated with cities. This is like limiting the domain or subject matter of
    a corpus when computing TF-IDF (term frequency - inverse document frequency) or
    BOW vectors.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: For a more diverse mix of vectors with greater information content, you’ll probably
    need a nonlinear embedding algorithm such as t-SNE (t-Distributed Stochastic Neighbor
    Embedding). We talk about t-SNE and other neural net techniques in later chapters.
    t-SNE will make more sense once you’ve grasped the word vector embedding algorithms
    here.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.5 Making Connections
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we are going to construct what is known as a *graph*.^([[54](#_footnotedef_54
    "View footnote.")]) The *graph* data structure is ideal for representing relations
    in data. At its core, a *graph* can be characterized as having *entities* (*nodes*
    or *vertices*) that are connected together through *relationships* or *edges*.
    Social networks are great examples of where the *graph* data structure is ideal
    to store the data. We will be using a particular type of *graph* in this section,
    an *undirected graph*. This type of *graph* is one where the *relationships* do
    not have a direction. An example of this non-directed relationship could be a
    friend connection between two people on Facebook, since neither can be the friend
    of the other without reciprocation. Another type of *graph* is the *directed graph*.
    This type of *graph* has relationships that go one way. This type of relationship
    can be seen in the example of Followers or Following on Twitter. You can follow
    someone without them following you back, and thus you can have Followers without
    having to reciprocate the relationship.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将构建所谓的**图（graph）**。图数据结构非常适合表示数据中的关系。从本质上讲，一个图可以被描述为具有通过关系（edges）连接在一起的实体（节点或顶点）。社交网络是最适合存储数据的图数据结构的典型例子。在本节中，我们将使用一种特定类型的图，即无向图。这种类型的图是指关系没有方向。一个非定向关系的例子可以是两个人之间在Facebook上的朋友关系，因为彼此之间没有互相认可的话，其中一个人不能成为另一个人的朋友。另一种类型的图是定向图。这种类型的图具有单向关系。在Twitter上的粉丝或关注者就是这种关系的一个例子。你可以关注某人而不被他们关注回来，因此你可以有粉丝而不必互相认可这种关系。
- en: To visualize the relationships between ideas and thoughts in this chapter you
    can create an *undirected graph* with connections (edges) between sentences that
    have similar meaning. You’ll use a force-directed layout engine to push all the
    similar concepts nodes together into clusters. But first you need some sort of
    embedding for each sentence. Sentences are designed to contain a single thought,
    how would you use word embeddings to create an embedding for a sentence?
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本章中可视化思想和思考之间的关系，您可以创建一个*无向图*，其中连句具有相似含义的句子之间有连接（边缘）。您将使用一个力导向布局引擎，将所有相似的概念节点推到一起形成簇。但首先，您需要为每个句子创建某种嵌入。句子旨在包含一个单一的思想，您将如何使用词嵌入为句子创建一个嵌入？
- en: You can apply what you learned about word embeddings from previous sections
    to create sentence embeddings. You will just average all the embeddings for each
    word in a sentence to create a single 300-D embedding for each sentence.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将在前几节中学到的关于词嵌入的知识应用到创建句子嵌入。您只需对句子中的每个单词的嵌入求平均，就可以为每个句子创建一个单一的300维嵌入。
- en: Extreme summarization
  id: totrans-457
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 极端概括
- en: What does a sentence embedding or thought vector actually contain? That depends
    on how you create it. You’ve already seen how to use SpaCy and `nessvec` to create
    word embeddings. You can create sentence embeddings by averaging all the word
    embeddings for a sentence;
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 句子嵌入或思维向量实际上包含了什么？这取决于您如何创建它。您已经知道如何使用SpaCy和nessvec创建词嵌入。您可以通过对句子中所有单词的嵌入求平均来创建句子嵌入；
- en: '[PRE66]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Extract natural language from the NLPiA manuscript
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从NLPiA手稿中提取自然语言
- en: You can download any of the chapters of this book in `adoc` format from the
    `src/nlpia2/data/manuscript` directory in the `nlpia2` project ([https://gitlab.com/tangibleai/nlpia2/](nlpia2.html)).
    The examples here will use the `adoc` manuscript for chapter 6\. If you ever write
    a book or software documentation yourself don’t do this. The recursive loop of
    testing and editing code within the text that you are processing with that code
    will break your brain. But you can now enjoy the fruits of all those headaches
    by processing the words your reading right now.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从nlpia2项目（[https://gitlab.com/tangibleai/nlpia2/](nlpia2.html)）的`src/nlpia2/data/manuscript`目录中以adoc格式下载本书的任何章节。这里的示例将使用第6章的adoc手稿。如果您自己编写一本书或软件文档，请不要这样做。在文本中嵌入代码并用该代码处理您正在处理的文本的递归循环会让您的大脑崩溃。但现在您可以享受所有这些头痛的成果，通过处理您现在正在阅读的文字。
- en: Listing 6.13 Download the adoc text from the `nlpia2` repo
  id: totrans-462
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6.13 从`nlpia2`仓库下载adoc文本
- en: '[PRE67]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Now you need to save that text to an `.adoc` file so that you can use a commandline
    tool to render it to html.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要将该文本保存到一个.adoc文件中，以便您可以使用命令行工具将其呈现为html。
- en: Listing 6.14 Write the adoc string to disk
  id: totrans-465
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6.14 将adoc字符串写入磁盘
- en: '[PRE68]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Now you will want to render the `adoc` text into HTML to make it easier to separate
    out the natural language text from the formatting characters and other "unnatural"
    text. You can use the Python package called `Asciidoc3` to convert any *AsciiDoc*
    (.adoc) text file into HTML.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您希望将`adoc`文本转换为HTML，以便更容易将自然语言文本与格式化字符和其他“不自然”文本分开。您可以使用名为`Asciidoc3`的Python软件包将任何*AsciiDoc*（.adoc）文本文件转换为HTML。
- en: Listing 6.15 Convert AsciiDoc file to HTML
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.15 将 AsciiDoc 文件转换为 HTML
- en: '[PRE69]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now that you have an HTML text file, you can use the *BeautifulSoup* package
    to extract the text.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经有了一个HTML文本文件，可以使用*BeautifulSoup*软件包提取文本。
- en: '[PRE70]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Now that you have the text for this chapter, you can run the small English language
    model from *spaCy* to get the sentence embedding vectors. SpaCy will average the
    *token* vectors within a `Doc` object.^([[55](#_footnotedef_55 "View footnote.")])
    In addition to getting the sentence vectors, you also want to retrieve the *noun
    phrases* ^([[56](#_footnotedef_56 "View footnote.")]) ^([[57](#_footnotedef_57
    "View footnote.")]) from each sentence that will be the labels for our sentence
    vectors.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经有了本章的文本，可以运行*spaCy*的小型英语语言模型以获得句子嵌入向量。*spaCy*将在`Doc`对象内对*token*向量进行平均。^([[55](#_footnotedef_55
    "查看脚注")]) 除了获取句子向量外，您还希望获取每个句子的*名词短语*^([[56](#_footnotedef_56 "查看脚注.")])^([[57](#_footnotedef_57
    "查看脚注.")])，这些短语将成为句子向量的标签。
- en: Listing 6.16 Getting Sentence Embeddings and Noun Phrases with spaCy
  id: totrans-473
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.16 使用 spaCy 获取句子嵌入和名词短语
- en: '[PRE71]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Now that you have sentence vectors and noun phrases, you should normalize ^([[58](#_footnotedef_58
    "View footnote.")]) the sentence vectors so that all your vectors have a length
    (or *2-norm*) of 1\. The 2-norm is computed the same way you compute the length
    of the diagonal across a right triangle, you add up the square of the length of
    the dimensions and then you take the square root of the sum of those squares.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经有了句子向量和名词短语，您应该对句子向量进行标准化^([[58](#_footnotedef_58 "查看脚注")])，以使所有向量的长度（或*2-范数*）为1。计算
    2-范数的方式与计算直角三角形对角线的长度相同，您将各个维度长度的平方相加，然后对这些平方和取平方根。
- en: '[PRE72]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Normalizing the data in the 300-dimensional vector gets all the values on the
    same scale while still retaining what differentiates them. ^([[59](#_footnotedef_59
    "View footnote.")])
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 对300维向量中的数据进行标准化可以将所有值调整到相同的尺度，同时保留它们之间的差异。^([[59](#_footnotedef_59 "查看脚注.")])
- en: Listing 6.17 Normalize the Sentence Vector Embeddings with NumPy
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.17 使用 NumPy 对句子向量嵌入进行标准化
- en: '[PRE73]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: With the sentence vectors normalized, you can get the similarity between all
    those vectors and each other. When you compute the pairwise similarity between
    all the possible pairs of objects in a list of object that creates a square matrix
    called a *similarity matrix* or *affinity matrix*. ^([[60](#_footnotedef_60 "View
    footnote.")]) If you use the dot product of each vector with all the others your
    are computing the cosine similarity that you are familiar with from previous chapters.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在将句子向量进行标准化后，您可以计算所有这些向量之间以及与彼此的相似性。当您计算列表中所有可能的对象对之间的成对相似性时，会生成一个称为*相似性矩阵*或*关联矩阵*的方阵。^([[60](#_footnotedef_60
    "查看脚注")]) 如果您使用每个向量与其他所有向量的点积进行计算，则计算的是您在之前章节中熟悉的余弦相似度。
- en: Listing 6.18 Getting the Similarity/Affinity Matrix
  id: totrans-481
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.18 获取相似性/关联矩阵
- en: '[PRE74]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The similarity matrix is calculated by taking the *dot product* between the
    normalized matrix of sentence embeddings (*N* by 300 dimensions) with the transpose
    of itself. This gives a matrix with shape *N* by *N* matrix, one row and column
    for each sentence in this chapter. The upper diagonal half of the matrix has the
    exact same values as the lower diagonal half. This is because of the commutative
    property of multiplication. The similarity between one vecgtor and another is
    the same no matter which direction you do the multiplication or the similarity
    computation.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度矩阵通过将句子嵌入的标准化矩阵（*N*维，300个维度）与其自身的转置进行点积计算得到。这样就得到了一个形状为 *N* 乘 *N* 的矩阵，矩阵的每一行和列都对应本章中的一个句子。矩阵的上半对角线和下半对角线的值完全相同。这是由于乘法的交换律。无论你如何进行乘法或相似度计算，一个向量与另一个的相似度是相同的。
- en: With the similarity matrix, you can now create an undirected graph using the
    similarities between sentence vectors to create graph edges between those sentences
    that are similar. The code below uses a library called `NetworkX` ^([[61](#_footnotedef_61
    "View footnote.")]) to create the *undirected graph* data structure. Internally
    the data is stored in an in nested dictionaries — a dictionary of dictionaries
    of dictionaries…​ — "[dictionaries] all the way down". ^([[62](#_footnotedef_62
    "View footnote.")]) Like a linked list, the nested dictionaries allow for quick
    lookups of sparse data. You computed the similarity matrix as a dense matrix with
    the dot product, but you will need to make it sparse because you don’t want every
    sentence to be connected to every other sentence in your graph. You are going
    to break the links between any sentence pairs that are far apart (have low similarity).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相似度矩阵，您现在可以创建一个无向图，使用句子向量之间的相似性来创建那些相似的句子之间的图边。下面的代码使用一个名为`NetworkX`的库来创建*无向图*数据结构。在内部，数据存储在嵌套的字典中
    - 一种字典的字典的字典... - "一直到底的字典"。就像链表一样，嵌套的字典允许快速查找稀疏数据。您使用点积计算了相似度矩阵作为密集矩阵，但需要将其变为稀疏矩阵，因为您不希望图中的每个句子都与其他句子相连接。您将会打破那些相距很远（相似度低）的句子对之间的连接。
- en: Listing 6.19 Creating the Undirected Graph
  id: totrans-485
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.19 创建无向图
- en: '[PRE75]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: With the shiny new graph (network) you’ve assembled, you can now use `matplotlib.pyplot`
    to visualize it.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 有了全新的图（网络），您现在可以使用`matplotlib.pyplot`来可视化它。
- en: Listing 6.20 Plot an undirected graph
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.20 绘制无向图
- en: '[PRE76]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Finally, you can see how your *undirected graph* shows the clusters of concepts
    in the natural language of this book! The springs in the force-directed graph
    have pulled similar concepts together based on their connections to the other
    concepts. Each *node* represents the average word embedding for a sentence in
    this chapter. And the *edges* (or lines) represent the connections between the
    meaning of those sentences that mean similar things. Looking at the plot, you
    can see the central big cluster of nodes (sentences) are connected the most. And
    there are other smaller clusters of nodes further out from the central cluster
    for topics such as sports and cities.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以看到您的*无向图*如何展示了本书自然语言中的概念集群！力导向图中的弹簧根据它们与其他概念的连接将相似的概念拉在一起。每个*节点*代表本章句子的平均词嵌入。而*边*（或线）表示那些意思相似的句子之间的连接。从绘图中可以看出，中心的大集群节点（句子）连接得最多。而中心集群外还有其他更小的集群节点，如体育和城市等主题。
- en: Figure 6.10 Connecting concepts to each other with word embeddings
  id: totrans-491
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.10 用词嵌入连接概念
- en: '![adjacency graph ch 6 with labels bold](images/adjacency_graph_ch_6_with_labels_bold.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![带有加粗标签的第 6 章邻接图](images/adjacency_graph_ch_6_with_labels_bold.png)'
- en: The dense cluster of concepts in the center should contain some information
    about the central ideas of this chapter and how they are related. Zooming in you
    can see these passages are mostly about words and numbers to represent words,
    because that’s what this chapter is about.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 中心密集的概念集群应包含关于本章主要思想及其关系的一些信息。放大后，您会看到这些段落大多关于用词和数字来表示词汇，因为这正是本章所讨论的内容。
- en: Figure 6.11 Undirected Graph Plot of Chapter 6 Center Zoom-in
  id: totrans-494
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.11 第 6 章中心放大的无向图绘制
- en: '![adjacency graph ch 6 zoom in center with labels bold](images/adjacency_graph_ch_6_zoom_in_center_with_labels_bold.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![带有加粗标签的第 6 章中心放大邻接图](images/adjacency_graph_ch_6_zoom_in_center_with_labels_bold.png)'
- en: The end of this chapter includes some exercises that you can do to practice
    what we have covered in this section.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结尾包括一些练习，供您练习本节所涵盖的内容。
- en: 6.5.6 Unnatural words
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.6 不自然的词汇
- en: Word embeddings such as Word2Vec are useful not only for English words but also
    for any sequence of symbols where the sequence and proximity of symbols is representative
    of their meaning. If your symbols have semantics, embeddings may be useful. As
    you may have guessed, word embeddings also work for languages other than English.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 Word2Vec 之类的词嵌入不仅对英语单词有用，而且对于任何表示符号序列的符号都有用，其中符号序列和符号的接近程度代表它们的含义。如果您的符号具有语义，那么嵌入可能会有用。正如您可能已经猜到的那样，词嵌入也适用于英语以外的其他语言。
- en: Embedding works also for pictorial languages such as traditional Chinese and
    Japanese (Kanji) or the mysterious hieroglyphics in Egyptian tombs. Embeddings
    and vector-based reasoning even works for languages that attempt to obfuscate
    the meaning of words. You can do vector-based reasoning on a large collection
    of "secret" messages transcribed from "Pig Latin" or any other language invented
    by children or the Emperor of Rome. A *Caesar cipher* ^([[63](#_footnotedef_63
    "View footnote.")]) such as RO13 or a *substitution cipher* ^([[64](#_footnotedef_64
    "View footnote.")]) are both vulnerable to vector-based reasoning with Word2Vec.
    You don’t even need a decoder ring (shown in figure 6.9). You just need a large
    collection of messages or *n*-grams that your Word2Vec embedder can process to
    find co-occurrences of words or symbols.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入还适用于象形文字，比如中国传统文字和日本汉字，以及埃及墓穴中神秘的象形文字。嵌入和基于向量的推理甚至适用于试图混淆词义的语言。您可以对一个大量收集而来的从“Pig
    Latin”转录的“秘密”消息或由儿童或罗马帝王发明的任何其他语言进行基于向量的推理。像RO13这样的*凯撒密码*^([[63](#_footnotedef_63
    "View footnote.")])或*替换密码*^([[64](#_footnotedef_64 "View footnote.")]) 都容易受到Word2Vec的基于向量的推理的攻击。你甚至不需要一个解码器环（如图6.9所示）。您只需要一个可以处理的大量消息或*n*-grams的Word2Vec嵌入器，以找到单词或符号的共现。
- en: Figure 6.12 Decoder rings
  id: totrans-500
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.12 解码器环
- en: '![decoder rings](images/decoder-rings.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![解码器环](images/decoder-rings.png)'
- en: Word2Vec has even been used to glean information and relationships from unnatural
    words or ID numbers such as college course numbers (CS-101), model numbers (Koala
    E7270 or Galaga Pro), and even serial numbers, phone numbers, and zip codes. ^([[65](#_footnotedef_65
    "View footnote.")]) To get the most useful information about the relationship
    between ID numbers like this, you’ll need a variety of sentences that contain
    those ID numbers. And if the ID numbers often contain a structure where the position
    of a symbol has meaning, it can help to tokenize these ID numbers into their smallest
    semantic packet (such as words or syllables in natural languages).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec甚至可以从非自然的词或ID号码（如大学课程号码（CS-101）、型号号码（Koala E7270 或 Galaga Pro）以及序列号、电话号码和邮政编码）中获取信息和关系。^([[65](#_footnotedef_65
    "View footnote.")]) 要获取关于此类ID号码间关系的最有用信息，您需要一系列包含这些ID号码的语句。如果ID号码经常包含具有意义的符号位置结构，将这些ID号码标记为最小的语义包（例如自然语言中的词或音节）可以提供帮助。
- en: 6.6 Summary
  id: totrans-503
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 小结
- en: Word vectors and vector-oriented reasoning can solve some surprisingly subtle
    problems like analogy questions and nonsynonomy relationships between words.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词向量和面向向量的推理可以解决一些令人惊讶的微妙问题，如类比问题和单词之间的非同位关系。
- en: To keep your word vectors current and improve their relevance to the current
    events and concepts you are interested in you can retrain and fine tune your word
    embeddings with `gensim` or PyTorch.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使您的词向量保持最新并提高其与您感兴趣的当前事件和概念的相关性，可以使用`gensim`或PyTorch重新训练和微调您的词嵌入。
- en: The `nessvec` package is a fun new tool for helping you find that word on the
    tip of your tongue or visualize the "character sheet" of a word.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nessvec`软件包是一个有趣的新工具，可以帮助您找到舌尖上的词或可视化一个单词的“角色卡”。'
- en: Word embeddings can reveal some surprising hidden meanings of the names of people,
    places, businesses and even occupations.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入可以揭示人名、地名、商号甚至职业名称中一些意外的隐藏含义。
- en: A PCA projection of word vectors for cities and countries can reveal the cultural
    closeness of places that are geographically far apart.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对城市和国家的词向量进行PCA投射，可以揭示地理上相距遥远的地方之间的文化亲近程度。
- en: The key to turning latent semantic analysis vectors into more powerful word
    vectors is to respect sentence boundaries when creating your *n*-grams.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将潜在语义分析向量转化为更强大的词向量的关键在于在创建*n*-grams时尊重句子边界。
- en: Machines can easily pass the word analogies section of standardized tests with
    nothing more than pretrained word embeddings.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器只需使用预训练的词嵌入就能轻松通过标准化测试中的词类比部分。
- en: 6.7 Test yourself
  id: totrans-511
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 自我测试
- en: Use pretrained word embeddings to compute the strength, agility, and intelligence
    of Dota 2 heroes based only on the natural language summary.^([[66](#_footnotedef_66
    "View footnote.")])
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的词嵌入，仅根据自然语言摘要，计算Dota 2英雄的力量、敏捷性和智力。^([[66](#_footnotedef_66 "View footnote.")])
- en: Visualize the graph of connections between concepts in another chapter of this
    book (or any other text) that you’d like to understand better.
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本书的另一章节（或任何其他文本）中可视化概念之间的连接图，从而更好地理解它们。
- en: Try combining graph visualizations of the word embeddings for all the chapters
    of this book.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试结合这本书所有章节的词嵌入图形可视化。
- en: Give examples for how word vectors enable at least two of Hofstadter’s eight
    elements of intelligence
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给出示例，说明词向量如何使得霍夫斯塔德的智能八要素中至少两个成为可能。
- en: Fork the nessvec repository and create your own visualizations or nessvector
    "character sheets" for your favorite words or famous people. Perhaps the "mindfulness",
    "ethicalness", "kindness", or "impactfulness" of your heros? Humans are complex
    and the words used to describe them are multidimensional.
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fork nessvec存储库并创建您自己喜欢的词或著名人物的可视化或nessvector“角色卡”。也许是您的英雄的“正念”、“道德”、“善意”或“影响力”？人类是复杂的，用来描述他们的词是多维的。
- en: Use PCA and word embeddings to create a 2-D map of some cities or words for
    objects that have a location near you. Try to include bigrams together as as a
    single point and then as two separate points for each word. Do the locations of
    the geographic words correspond in some way to their geographic location? What
    about non-geographic words?
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PCA和词嵌入创建一些城市或靠近您位置的物体的二维地图。尝试将二元词组合作为一个单独的点，然后分别作为每个词的两个单独的点。地理词的位置是否在某种程度上对应其地理位置？非地理词呢？
- en: '[[1]](#_footnoteref_1) Thank you Marc Foster for your chaotic good influence
    on me and the world!'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#_footnoteref_1) 感谢马克·福斯特对我的混乱善意的影响以及对世界的影响！'
- en: '[[2]](#_footnoteref_2) Representation learning methodology on Papers With Code
    ( [https://paperswithcode.com/area/methodology/representation-learning](methodology.html))'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#_footnoteref_2) Papers With Code上的表示学习方法（ [https://paperswithcode.com/area/methodology/representation-learning](methodology.html)）'
- en: '[[3]](#_footnoteref_3) "This is your brain on drugs" ( [https://en.wikipedia.org/wiki/This_Is_Your_Brain_on_Drugs](wiki.html)
    )'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#_footnoteref_3) "This is your brain on drugs"（ [https://en.wikipedia.org/wiki/This_Is_Your_Brain_on_Drugs](wiki.html)）'
- en: '[[4]](#_footnoteref_4) See "Recap: Node Embeddings" by Ted Kye for San Diego
    Machine Learning Book Club ( [https://github.com/SanDiegoMachineLearning/bookclub/blob/master/graph/graphml-05-GNN1.pdf](graph.html)
    )'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4]](#_footnoteref_4) 请查看《Node Embeddings》作者泰德·凯为圣地亚哥机器学习读书会所做的总结（ [https://github.com/SanDiegoMachineLearning/bookclub/blob/master/graph/graphml-05-GNN1.pdf](graph.html)）'
- en: '[[5]](#_footnoteref_5) "Robust Evaluation of Language-Brain Encoding Experiments"
    ( [https://arxiv.org/abs/1904.02547](abs.html) ) by Lisa Beinborn ( [https://beinborn.eu/](beinborn.eu.html))'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5]](#_footnoteref_5) 由Lisa Beinborn编写的《语言-大脑编码实验的稳健评估》（ [https://arxiv.org/abs/1904.02547](abs.html)）（
    [https://beinborn.eu/](beinborn.eu.html)）'
- en: '[[6]](#_footnoteref_6) footnote:["Linkng human cognitive patterns to NLP models"
    ( [https://soundcloud.com/nlp-highlights/130-linking-human-cognitive-patterns-to-nlp-models-with-lisa-beinborn](nlp-highlights.html))
    interview of Lisa Beinborn ( [https://beinborn.eu/](beinborn.eu.html))'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#_footnoteref_6) 脚注：[“将人类认知模式与NLP模型联系起来”]( [https://soundcloud.com/nlp-highlights/130-linking-human-cognitive-patterns-to-nlp-models-with-lisa-beinborn](nlp-highlights.html))
    采访了Lisa Beinborn（ [https://beinborn.eu/](beinborn.eu.html)）'
- en: '[[7]](#_footnoteref_7) "Never-Ending Language Learning" by T. Mitchell et al
    at Cornell ( [http://proai.org/NELL_aaai15.pdf](proai.org.html))'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#_footnoteref_7) 康奈尔大学的T.米切尔等人撰写的《永不止步的语言学习》（ [http://proai.org/NELL_aaai15.pdf](proai.org.html)）'
- en: '[[8]](#_footnoteref_8) ''Neuro-linguistic programming'' explanation on Wikipedia
    ( [https://en.wikipedia.org/wiki/Neuro-linguistic_programming](wiki.html) )'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#_footnoteref_8) 维基百科上关于“神经语言编程”的解释（ [https://en.wikipedia.org/wiki/Neuro-linguistic_programming](wiki.html)）'
- en: '[[9]](#_footnoteref_9) ''r/NLP'' subreddit ( [https://www.reddit.com/r/NLP](r.html)
    )'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#_footnoteref_9) ''r/NLP'' subreddit（ [https://www.reddit.com/r/NLP](r.html)）'
- en: '[[10]](#_footnoteref_10) An authentic NLP subreddit at ''r/NaturalLanguage/''
    ( [https://www.reddit.com/r/NaturalLanguage](r.html) )'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#_footnoteref_10) 在''r/NaturalLanguage/''上有一个真正的NLP subreddit（ [https://www.reddit.com/r/NaturalLanguage](r.html)）'
- en: '[[11]](#_footnoteref_11) *Intuition Pumps and Other Tools for Thinking* by
    Daniel Dennett p.96'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#_footnoteref_11) 丹尼尔·丹尼特的《直觉泵及其他思维工具》第96页'
- en: '[[12]](#_footnoteref_12) Papers With Code topic "Word Embeddings" ( [https://paperswithcode.com/task/word-embeddings](task.html))'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#_footnoteref_12) Papers With Code上有关“词嵌入”的主题（ [https://paperswithcode.com/task/word-embeddings](task.html)）'
- en: '[[13]](#_footnoteref_13) Standford’s open source GloVE algorithm in C ( [https://github.com/stanfordnlp/GloVe](stanfordnlp.html))
    and Python ( [https://github.com/lapis-zero09/compare_word_embedding/blob/master/glove_train.py](master.html))'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#_footnoteref_13) 斯坦福开源的GloVE算法的C版本（ [https://github.com/stanfordnlp/GloVe](stanfordnlp.html)）和Python版本（
    [https://github.com/lapis-zero09/compare_word_embedding/blob/master/glove_train.py](master.html)）'
- en: '[[14]](#_footnoteref_14) Startpage proviacy-protecting web search ( [https://www.startpage.com/](www.startpage.com.html))'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#_footnoteref_14) Startpage隐私保护网络搜索（[https://www.startpage.com/](www.startpage.com.html)）'
- en: '[[15]](#_footnoteref_15) DISROOT nonprofit search engine ( [https://search.disroot.org](.html))'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15]](#_footnoteref_15) DISROOT非营利搜索引擎（[https://search.disroot.org](.html)）'
- en: '[[16]](#_footnoteref_16) Wolfram Alpha uses state-of-the art NLP ( [https://wolframalpha.com/](wolframalpha.com.html))'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[[16]](#_footnoteref_16) Wolfram Alpha采用最先进的自然语言处理技术（[https://wolframalpha.com/](wolframalpha.com.html)）'
- en: '[[17]](#_footnoteref_17) ElasticSearch backend source code ( [https://github.com/elastic/elasticsearch](elastic.html))
    and frontend SearchKit demo ( [https://demo.searchkit.co/type/all?query=prosocial%20AI](type.html))'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '[[17]](#_footnoteref_17) ElasticSearch后端源代码（[https://github.com/elastic/elasticsearch](elastic.html)）和前端SearchKit演示（[https://demo.searchkit.co/type/all?query=prosocial%20AI](type.html)）'
- en: '[[18]](#_footnoteref_18) Meilisearch source code and self-hosting docker images
    ( [https://github.com/meilisearch/meilisearch](meilisearch.html)) and managed
    hosting ( [https://www.meilisearch.com/](www.meilisearch.com.html))'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18]](#_footnoteref_18) Meilisearch源代码和自托管Docker镜像（[https://github.com/meilisearch/meilisearch](meilisearch.html)）以及托管服务（[https://www.meilisearch.com/](www.meilisearch.com.html)）'
- en: '[[19]](#_footnoteref_19) SearX git repository ( [https://github.com/searx/searx](searx.html))'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '[[19]](#_footnoteref_19) SearX git仓库（[https://github.com/searx/searx](searx.html)）'
- en: '[[20]](#_footnoteref_20) Apache Solr home page and Java source code ( [https://solr.apache.org/](solr.apache.org.html))'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[[20]](#_footnoteref_20) Apache Solr主页和Java源代码（[https://solr.apache.org/](solr.apache.org.html)）'
- en: '[[21]](#_footnoteref_21) Apache Lucene home page ( [https://lucene.apache.org/](lucene.apache.org.html))'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '[[21]](#_footnoteref_21) Apache Lucene主页（[https://lucene.apache.org/](lucene.apache.org.html)）'
- en: '[[22]](#_footnoteref_22) Qwant web search engine is based in Europe where regulations
    protect you from manipulation and deception ( [https://www.qwant.com/](www.qwant.com.html))'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '[[22]](#_footnoteref_22) Qwant网络搜索引擎位于法规保护您免受操纵和欺骗的欧洲（[https://www.qwant.com/](www.qwant.com.html)）'
- en: '[[23]](#_footnoteref_23) Sphinx home page and C source code ( [http://sphinxsearch.com/](sphinxsearch.com.html))'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '[[23]](#_footnoteref_23) Sphinx主页和C源代码（[http://sphinxsearch.com/](sphinxsearch.com.html)）'
- en: '[[24]](#_footnoteref_24) "How to Build a Semantic Search Engine in 3 minutes"
    by Cole Thienes and Jack Pertschuk ( [http://mng.bz/yvjG](mng.bz.html))'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24]](#_footnoteref_24) Cole Thienes 和 Jack Pertschuk 的《如何在3分钟内构建语义搜索引擎》（[http://mng.bz/yvjG](mng.bz.html)）'
- en: '[[25]](#_footnoteref_25) PynnDescent Python package ( [https://pypi.org/project/pynndescent/](pynndescent.html))'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '[[25]](#_footnoteref_25) PynnDescent Python软件包（[https://pypi.org/project/pynndescent/](pynndescent.html)）'
- en: '[[26]](#_footnoteref_26) Try them all if you don’t believe us.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '[[26]](#_footnoteref_26) 如果你不相信我们，请尝试它们全部。'
- en: '[[27]](#_footnoteref_27) See wikipedia article ( [https://en.wikipedia.org/wiki/John_Rupert_Firth](wiki.html))'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '[[27]](#_footnoteref_27) 查看维基百科文章（[https://en.wikipedia.org/wiki/John_Rupert_Firth](wiki.html)）'
- en: '[[28]](#_footnoteref_28) "Efficient Estimation of Word Representations in Vector
    Space" Sep 2013, Mikolov, Chen, Corrado, and Dean ( [https://arxiv.org/pdf/1301.3781.pdf](pdf.html)).'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '[[28]](#_footnoteref_28) “在向量空间中高效估计词表示”2013年9月，Mikolov、Chen、Corrado和Dean（[https://arxiv.org/pdf/1301.3781.pdf](pdf.html)）。'
- en: '[[29]](#_footnoteref_29) See the web page titled "Unsupervised Feature Learning
    and Deep Learning Tutorial" ( [http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/](Autoencoders.html)).'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '[[29]](#_footnoteref_29) 查看名为“无监督特征学习和深度学习教程”的网页（[http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/](Autoencoders.html)）。'
- en: '[[30]](#_footnoteref_30) See the PDF "Linguistic Regularities in Continuous
    Space Word Representations" by Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig
    ( [https://www.aclweb.org/anthology/N13-1090](anthology.html)).'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '[[30]](#_footnoteref_30) 查看Tomas Mikolov、Wen-tau Yih和Geoffrey Zweig的《连续空间词表示中的语言学规律》PDF（[https://www.aclweb.org/anthology/N13-1090](anthology.html)）。'
- en: '[[31]](#_footnoteref_31) See Radim Řehůřek’s interview of Tomas Mikolov ( [https://rare-technologies.com/rrp#episode_1_tomas_mikolov_on_ai](rare-technologies.com.html)).'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '[[31]](#_footnoteref_31) 查看Radim Řehůřek对Tomas Mikolov的采访（[https://rare-technologies.com/rrp#episode_1_tomas_mikolov_on_ai](rare-technologies.com.html)）。'
- en: '[[32]](#_footnoteref_32) See "ICRL2013 open review" ( [https://openreview.net/forum?id=idpCdOWtqXd60&noteId=C8Vn84fqSG8qa](openreview.net.html)).'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '[[32]](#_footnoteref_32) 查看“ICRL2013开放评论”（[https://openreview.net/forum?id=idpCdOWtqXd60&noteId=C8Vn84fqSG8qa](openreview.net.html)）。'
- en: '[[33]](#_footnoteref_33) You can find the code for generating these interactive
    2D word plots in [http://mng.bz/M5G7](mng.bz.html)'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '[[33]](#_footnoteref_33) 您可以在[http://mng.bz/M5G7](mng.bz.html)找到生成这些交互式二维词图的代码。'
- en: '[[34]](#_footnoteref_34) See Part III. "Tools for thinking about Meaning or
    Content" p 59 and chapter 15 "Daddy is a doctor" p. in the book "Intuition Pumps
    and Other Tools for Thinking" by Daniel C. Dennett'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) See the web page titled "GitHub - 3Top/word2vec-api:
    Simple web service providing a word embedding model" ( [https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-model](3Top.html)).'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) Original Google 300D Word2Vec model on Google Drive
    ( [https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM](d.html))'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '[[37]](#_footnoteref_37) See the web page titled "GitHub - facebookresearch/fastText:
    Library for fast text representation and classification." ( [https://github.com/facebookresearch/fastText](facebookresearch.html)).'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '[[38]](#_footnoteref_38) Wikipedia on Bayard Rustin ( [https://en.wikipedia.org/wiki/Bayard_Rustin](wiki.html))
    a civil right leader and Larry Dane Brimner ( [https://en.wikipedia.org/wiki/Larry_Dane_Brimner](wiki.html))
    an author of more than 150 children’s books'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: '[[39]](#_footnoteref_39) The publication by the team around Tomas Mikolov (
    [https://arxiv.org/pdf/1310.4546.pdf](pdf.html)) provides more details.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: '[[40]](#_footnoteref_40) See the README file at [http://gitlab.com/tangibleai/nlpia2](tangibleai.html)
    for installation instructions.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '[[41]](#_footnoteref_41) Google hosts the original model trained by Mikolov
    on Google Drive [here](bit.ly.html)'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: '[[42]](#_footnoteref_42) *Surfaces and Essences: Analogy as the Fuel and Fire
    of Thinking* by Douglas Hoffstadter and Emmanuel Sander.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '[[43]](#_footnoteref_43) Detector Morse, by Kyle Gorman and OHSU on pypi and
    at [https://github.com/cslu-nlp/DetectorMorse](cslu-nlp.html)'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '[[44]](#_footnoteref_44) Stanford GloVe Project ( [https://nlp.stanford.edu/projects/glove/](glove.html)).'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '[[45]](#_footnoteref_45) See chapter 5 and Appendix C for more details on SVD.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: '[[46]](#_footnoteref_46) *GloVe: Global Vectors for Word Representation* by
    Jeffrey Pennington, Richard Socher, and Christopher D. Manning: [https://nlp.stanford.edu/pubs/glove.pdf](pubs.html)'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '[[47]](#_footnoteref_47) Gensim’s comparison of Word2Vec and GloVe performance:
    [https://rare-technologies.com/making-sense-of-Word2Vec/#glove_vs_word2vec](making-sense-of-Word2Vec.html)'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '[[48]](#_footnoteref_48) Enriching Word Vectors with Subword Information, Bojanowski
    et al.: [https://arxiv.org/pdf/1607.04606.pdf](pdf.html)'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '[[49]](#_footnoteref_49) See the web page titled "fastText/pretrained-vectors.md"
    ( [https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md](docs.html)).'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '[[50]](#_footnoteref_50) Spelling corrector code and explanation by Peter Norvig
    ( [https://norvig.com/spell-correct.html](norvig.com.html))'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: '[[51]](#_footnoteref_51) SpaCy medium language model docs ( [https://spacy.io/models/en#en_core_web_md](models.html))'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '[[52]](#_footnoteref_52) Nessvec source code ( [https://gitlab.com/tangibleai/nessvec](tangibleai.html))
    and tutorial videos ( [https://proai.org/nessvec-videos](proai.org.html))'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '[[52]](#_footnoteref_52) Nessvec 源代码（[https://gitlab.com/tangibleai/nessvec](tangibleai.html)）和教程视频（[https://proai.org/nessvec-videos](proai.org.html)）'
- en: '[[53]](#_footnoteref_53) Niel Chah’s `word2vec4everything` repository ( [https://github.com/nchah/word2vec4everything](nchah.html))'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[[53]](#_footnoteref_53) Niel Chah 的 `word2vec4everything` 仓库（[https://github.com/nchah/word2vec4everything](nchah.html)）'
- en: '[[54]](#_footnoteref_54) See this Wiki page titled, ''Graph (abstract data
    type''): [https://en.wikipedia.org/wiki/Graph_(abstract_data_type](wiki.html))'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '[[54]](#_footnoteref_54) 请查看标题为 ''图（抽象数据类型）'' 的 Wiki 页面：[https://en.wikipedia.org/wiki/Graph_(abstract_data_type](wiki.html))'
- en: '[[55]](#_footnoteref_55) spaCy’s vector attribute for the Span object defaults
    to the average of the token vectors ( [https://spacy.io/api/span#vector](api.html))'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '[[55]](#_footnoteref_55) spaCy 的 Span 对象的向量属性默认为标记向量的平均值（[https://spacy.io/api/span#vector](api.html)）'
- en: '[[56]](#_footnoteref_56) See the Wiki page titled, ''Noun phrase'': [https://en.wikipedia.org/wiki/Noun_phrase](wiki.html)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '[[56]](#_footnoteref_56) 请查看标题为 ''名词短语'' 的 Wiki 页面：[https://en.wikipedia.org/wiki/Noun_phrase](wiki.html)'
- en: '[[57]](#_footnoteref_57) spaCy’s Span.noun_chunks: [https://spacy.io/api/span#noun_chunks](api.html)'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '[[57]](#_footnoteref_57) spaCy 的 Span.noun_chunks：[https://spacy.io/api/span#noun_chunks](api.html)'
- en: '[[58]](#_footnoteref_58) See the Wiki page title, ''Norm (mathematics) — Euclidean
    norm'': [https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm](wiki.html)'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '[[58]](#_footnoteref_58) 请查看标题为 ''范数（数学）— 欧几里得范数'' 的 Wiki 页面：[https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm](wiki.html)'
- en: '[[59]](#_footnoteref_59) See the web page titled, ''Why Data Normalization
    is necessary for Machine Learning models'': [http://mng.bz/aJ2z](mng.bz.html)'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '[[59]](#_footnoteref_59) 请查看标题为 ''为什么数据标准化对机器学习模型是必要的'' 的网页：[http://mng.bz/aJ2z](mng.bz.html)'
- en: '[[60]](#_footnoteref_60) See this web page titled, ''Affinity Matrix'': [https://deepai.org/machine-learning-glossary-and-terms/affinity-matrix](machine-learning-glossary-and-terms.html)'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '[[60]](#_footnoteref_60) 请查看标题为 ''亲和矩阵'' 的网页：[https://deepai.org/machine-learning-glossary-and-terms/affinity-matrix](machine-learning-glossary-and-terms.html)'
- en: '[[61]](#_footnoteref_61) See the NetworkX web page for more information: [https://networkx.org/](networkx.org.html)'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '[[61]](#_footnoteref_61) 欲获取更多信息，请查看 NetworkX 网页：[https://networkx.org/](networkx.org.html)'
- en: '[[62]](#_footnoteref_62) NetworkX docs have more detail ( [https://networkx.org/documentation/stable/reference/introduction.html#data-structure](reference.html))'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '[[62]](#_footnoteref_62) NetworkX 文档有更多细节（[https://networkx.org/documentation/stable/reference/introduction.html#data-structure](reference.html)）'
- en: '[[63]](#_footnoteref_63) See the web page titled "Caesar cipher - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Caesar_cipher](wiki.html)).'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '[[63]](#_footnoteref_63) 欲查看标题为 "凯撒密码 - 维基百科" 的网页，请访问（[https://en.wikipedia.org/wiki/Caesar_cipher](wiki.html)）'
- en: '[[64]](#_footnoteref_64) See the web page titled "Substitution cipher - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Substitution_cipher](wiki.html)).'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '[[64]](#_footnoteref_64) 欲查看标题为 "替换密码 - 维基百科" 的网页，请访问（[https://en.wikipedia.org/wiki/Substitution_cipher](wiki.html)）'
- en: '[[65]](#_footnoteref_65) See article "A non-NLP application of Word2Vec – Towards
    Data Science" by Kwyk ( [https://archive.ph/n5yw3](archive.ph.html)).'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '[[65]](#_footnoteref_65) 请查看 Kwyk 撰写的文章 "Word2Vec 的非 NLP 应用 - Towards Data
    Science"（[https://archive.ph/n5yw3](archive.ph.html)）。'
- en: '[[66]](#_footnoteref_66) The `nessvec` and `nlpia2` packages contain FastText,
    GloVE and Word2vec loaders ( [https://gitlab.com/tangibleai/nessvec](tangibleai.html)),
    and `nlpia2` contains `ch06_dota2_wiki_heroes.hist.py` ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/etl/ch06_dota2_wiki_heroes.hist.py](etl.html))
    for downloading Dota 2 character sheets.'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '[[66]](#_footnoteref_66) `nessvec` 和 `nlpia2` 包包含 FastText、GloVE 和 Word2vec
    加载器（[https://gitlab.com/tangibleai/nessvec](tangibleai.html)），而 `nlpia2` 包含用于下载
    Dota 2 角色卡的 `ch06_dota2_wiki_heroes.hist.py`（[https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/etl/ch06_dota2_wiki_heroes.hist.py](etl.html)）。'
