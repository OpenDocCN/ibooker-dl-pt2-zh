- en: 1 What is transfer learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 什么是迁移学习？
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: What exactly transfer learning is, both generally in artificial intelligence
    (AI) and in the context of natural language processing (NLP)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习在人工智能（AI）中的普遍含义，以及在自然语言处理（NLP）的上下文中的含义
- en: Typical NLP tasks and the related chronology of NLP transfer learning advances
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型NLP任务及NLP迁移学习进展的相关年表
- en: An overview of transfer learning in computer vision
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉中的迁移学习概述
- en: The reason for the recent popularity of NLP transfer learning techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近年来NLP迁移学习技术日益普及的原因
- en: Artificial intelligence (AI) has transformed modern society in a dramatic way.
    Machines now perform tasks that human used to do, and they do them faster, cheaper,
    and, in some cases, more effectively. Popular examples include computer vision
    applications, which teach computers how to understand images and videos, such
    as for the detection of criminals in closed-circuit television camera feeds. Other
    computer vision applications include the detection of diseases from images of
    patients’ organs and the defining of plant species from plant leaves. Another
    important branch of AI, natural language processing (NLP), deals particularly
    with the analysis and processing of human natural language data. Examples of NLP
    applications include speech-to-text transcription and translation between various
    languages.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）已经以戏剧性的方式改变了现代社会。现在，机器执行了人类曾经做过的任务，而且它们做得更快、更便宜，有些情况下甚至更有效。流行的例子包括计算机视觉应用，教会计算机如何理解图像和视频，例如监控摄像头视频中的罪犯检测。其他计算机视觉应用包括从患者器官图像中检测疾病以及从植物叶片中定义植物物种。人工智能的另一个重要分支，自然语言处理（NLP），特别涉及人类自然语言数据的分析和处理。NLP应用的例子包括语音转文本转录和各种语言之间的翻译。
- en: The most recent incarnation of the technical revolution in AI robotics and automation—which
    some refer to as the Fourth Industrial Revolution[¹](#pgfId-1081153)—was sparked
    by the intersection of algorithmic advances for training large neural networks,
    the availability of vast amounts of data via the internet, and the ready availability
    of massively parallel capabilities via graphical processing units (GPUs), which
    were initially developed for the personal gaming market. The recent rapid advances
    in the automation of tasks relying on human perception, specifically computer
    vision and NLP, required these strides in neural network theory and practice to
    happen. The growth of this area enabled the development of sophisticated representations
    of input data and desired output signals to handle these difficult problems.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI机器人技术和自动化的技术革命最新演变—一些人将其称为第四次工业革命[¹](#pgfId-1081153)—是由几个因素的交汇引发的：针对训练大型神经网络的算法进步，通过互联网获取大量数据的可行性，以及最初是为个人游戏市场开发的大规模并行能力通过图形处理单元（GPUs）的可获得性。最近对依赖人类感知的任务自动化的快速进步，特别是计算机视觉和NLP，需要这些神经网络理论和实践的进步。这一领域的增长促进了对输入数据和所需输出信号的复杂表示的开发，以处理这些困难问题。
- en: At the same time, projections of what AI will be able to accomplish have significantly
    exceeded what has been achieved in practice. We are warned of an apocalyptic future
    that will erase most human jobs and replace us all, potentially even posing an
    existential threat to us. NLP is not excluded from this speculation, as it is
    today one of the most active research areas within AI. It is my hope that reading
    this book will contribute to helping you gain a better understanding of what is
    realistically possible to expect from AI, machine learning, and NLP in the near
    future. However, the main purpose of this book is to arm readers with a set of
    actionable skills related to a recent paradigm that has become important in NLP—transfer
    learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，人们对AI能够实现的预期大大超出了实践中所取得的成就。我们被警告说，将来可能会有一个末日般的未来，将消灭大部分人类工作并取代我们所有人，甚至可能对我们构成存在威胁。NLP并没有被排除在这种猜测之外，因为它今天是AI内最活跃的研究领域之一。我希望阅读本书能帮助你更好地了解从AI、机器学习和NLP中现实可能期待的东西。然而，本书的主要目的是向读者提供一组与最近在NLP中变得重要的范式相关的可行技能—迁移学习。
- en: Transfer learning aims to leverage prior knowledge from different settings—be
    it a different task, language, or domain—to help solve a problem at hand. It is
    inspired by the way in which humans learn, because we typically do not learn things
    from scratch for any given problem but rather build on prior knowledge that may
    be related. For instance, learning to play a musical instrument is considered
    easier when one already knows how to play another instrument. Obviously, the more
    similar the instruments—an organ versus a piano, for example—the more useful prior
    knowledge is and the easier learning the new instrument will be. However, even
    if the instruments are vastly different—such as the drum versus the piano—some
    prior knowledge can still be useful, even if less so, such as the practice of
    adhering to a rhythm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习旨在利用不同设置的先前知识——无论是不同的任务、语言还是领域——来帮助解决手头的问题。它受到人类学习的启发，因为我们通常不会为了任何给定的问题从头学习事物，而是建立在可能相关的先前知识上。例如，学习演奏一种乐器，在已经知道如何演奏另一种乐器的情况下被认为更容易。显然，乐器越相似——比如风琴与钢琴——先前的知识越有用，学习新乐器也会更容易。然而，即使乐器非常不同——如鼓和钢琴——一些先前知识仍然有用，虽然作用较小，比如遵循节奏的练习。
- en: Large research laboratories, such as Lawrence Livermore National Laboratories
    or Sandia National Laboratories, and large internet companies, such as Google
    and Facebook, are able to learn large sophisticated models by training deep neural
    networks on billions of words and millions of images. For instance, Google's NLP
    model BERT (*Bidirectional Encoder Representations from Transformers*), which
    will be introduced in the next chapter, was pretrained on the English version
    of Wikipedia (2.5 billion words) and the BookCorpus (0.8 billion words).[²](#pgfId-1081161)
    Similarly, deep convolutional neural networks (CNNs) have been trained on more
    than 14 million images of the ImageNet dataset, and the learned parameters have
    been widely outsourced by a number of organizations. The amounts of resources
    required to train such models from scratch are not typically available to the
    average practitioner of neural networks today, such as NLP engineers working at
    smaller businesses or students at smaller schools. Does this mean that the smaller
    players are locked out of being able to achieve state-of-the-art results on their
    problems? Most definitely not—thankfully, the concept of transfer learning promises
    to alleviate this concern if applied correctly.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型研究实验室，如劳伦斯利弗莫尔国家实验室或桑迪亚国家实验室，以及大型互联网公司，如谷歌和Facebook，能够通过在数十亿字和数百万图片上训练深层神经网络来学习大规模复杂模型。例如，谷歌的NLP模型BERT（*双向编码器表示转换*），将在下一章介绍，是在英文版本的维基百科（25亿字）和BookCorpus（8亿字）上进行了预训练。[²](#pgfId-1081161)
    同样，深度卷积神经网络（CNNs）已经在ImageNet数据集的超过1400万张图片上进行了训练，学习的参数已经被许多组织广泛应用。从头开始训练这样的模型需要的资源量通常不会被普通的神经网络从业者所使用，比如在较小企业工作的NLP工程师或在较小学校读书的学生。这是否意味着较小的参与者无法取得其问题的最先进成果？绝对不是——值得庆幸的是，如果正确应用，迁移学习的概念承诺解决这个问题。
- en: Why is transfer learning important?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么迁移学习如此重要？
- en: Transfer learning enables you to adapt or transfer the knowledge acquired from
    one set of tasks and/or domains to a different set of tasks and/or domains. What
    this means is that a model trained with massive resources—including data, computing
    power, time, and cost—which were once open sourced can be fine-tuned and reused
    in new settings by the wider engineering community at a fraction of the original
    resource requirements. This represents a big step forward for the democratization
    of NLP and, more widely, AI. This paradigm is illustrated in figure 1.1, using
    the act of learning how to play a musical instrument as an example. It can be
    observed from the figure that information sharing between the different tasks/domains
    can lead to a reduction in data required to achieve the same performance for the
    later, or *downstream**,* task B.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习使您能够将从一组任务和/或领域中获得的知识调整或转移到另一组任务和/或领域。这意味着，曾经开源的、经过大量资源包括数据、计算能力、时间和成本训练的模型可以通过更广泛的工程社区进行微调和重复使用，而使用的资源成本仅为原始资源成本的一小部分。这对于NLP甚至更广泛的AI的民主化代表了一个重要进步。图1.1说明了这种范式，以学习如何演奏乐器为例。从图中可以看出，不同任务/领域之间的信息共享可以使后续任务B所需的数据量减少，以实现相同的性能，或者*下游*任务B。
- en: '![01_01](../Images/01_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![01_01](../Images/01_01.png)'
- en: Figure 1.1 An illustration of the advantages of the transfer learning paradigm—shown
    in the bottom panel—where information is shared between systems trained for different
    tasks/domains, versus the traditional paradigm—shown in the top panel—where training
    occurs in parallel between tasks/domains. In the transfer learning paradigm, reduction
    in data and computing requirements can be achieved via the information/knowledge
    sharing. For instance, we expect a person to learn to play the drums more easily
    if they know how to play the piano first.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 转移学习范式优势的示意图—显示在底部面板—在不同任务/领域训练的系统之间共享信息，与传统范式—显示在顶部面板—其中任务/领域之间同时进行训练相比。在转移学习范式中，通过信息/知识共享可以实现减少数据和计算需求。例如，如果一个人先学会弹钢琴，我们预期他们学会打鼓会更容易些。
- en: 1.1 Overview of representative NLP tasks
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 代表性自然语言处理任务概述
- en: 'The goal of NLP is to enable computers to understand natural human language.
    You can think of it as a process of systematically encoding natural language text
    into numerical representations that accurately portray its meaning. Although various
    taxonomies of typical NLP tasks exist, the following nonexhaustive list provides
    a framework for thinking about the scope of the problem and framing appropriately
    the various examples that will be addressed by this book. Note that some of these
    tasks may (or may not, depending on the specific algorithm selected) be required
    by other, more difficult, tasks on the list:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的目标是使计算机能够理解自然人类语言。您可以将其视为将自然语言文本系统地编码为准确反映其含义的数值表示的过程。尽管存在各种典型自然语言处理任务的分类法，但以下非尽述性列表提供了一个框架，用于思考问题的范围，并适当地构建本书将讨论的各种示例。请注意，其中一些任务可能需要（或不需要，具体取决于所选择的特定算法）列表中其他更难的任务：
- en: '*Part-of-speech (POS) tagging*—Tagging a word in text with its part of speech;
    potential tags include verb, adjective, and noun.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词性标注（POS）*—在文本中标记词语的词性；可能的标记包括动词、形容词和名词。'
- en: '*Named entity recognition (NER)*—Detecting entities in unstructured text, such
    as PERSON, ORGANIZATION, and LOCATION. Note that POS tagging could be part of
    an NER pipeline.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*命名实体识别（NER）*—在非结构化文本中检测实体，如人名、组织名和地名。请注意，词性标注可能是NER流水线的一部分。'
- en: '*Sentence/document classification*—Tagging sentences or documents with predefined
    categories, such as sentiments {“positive,” “negative”}, various topics {“entertainment,”
    “science,” “history”}, or some other predefined set of categories.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*句子/文档分类*—使用预定义的类别对句子或文档进行标记，例如情感{“积极”，“消极”}、各种主题{“娱乐”，“科学”，“历史”}或一些其他预定义的类别集。'
- en: '*Sentiment analysis*—Assigning to a sentence or document the sentiment expressed
    in it, for example, {“positive,” “negative”}. Indeed, you can arguably view this
    as a special case of sentence/document classification.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*情感分析*—为一个句子或文档分配其中表达的情感，例如，{“积极”，“消极”}。事实上，您可以将其视为句子/文档分类的特例。'
- en: '*Automatic summarization*—Summarizing the content of a collection of sentences
    or documents, usually in a few sentences or keywords.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动摘要*—总结一系列句子或文档的内容，通常用几句话或关键词概括。'
- en: '*Machine translation*—Translating sentences/documents from one language into
    another language or a collection of languages.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器翻译*—将句子/文档从一种语言翻译成另一种语言或一系列语言。'
- en: '*Question answering*—Determining an appropriate answer to a question posed
    by a human; for example, Question: What is the capital of Ghana? Answer: Accra.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问答系统*—确定对人类提出的问题的合适答案；例如，问题：加纳的首都是什么？答案：阿克拉。'
- en: '*Chatterbot/chatbot*—Carrying out a conversation with a human convincingly,
    potentially aiming to accomplish some goal, such as maximizing the length of the
    conversation or extracting some specific information from the human. Note that
    a chatbot can be formulated as a question-answering system.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*闲聊机器人/聊天机器人*—与人类进行一段有说服力的对话，可能旨在实现某个目标，例如最大化对话长度或从人类那里提取某些特定信息。请注意，闲聊机器人可以被构建为问答系统。'
- en: '*Speech recognition*—Converting the audio of human speech into its text representation.
    Although a lot of effort has been and continues to be spent making speech recognition
    systems more reliable, in this book it is assumed that a text representation of
    the language of interest is already available.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语音识别*—将人类语音的音频转换为其文本表示。尽管已经投入了大量的工作使语音识别系统更加可靠，但在本书中，假设已经存在了语言感兴趣的文本表示。'
- en: '*Language modeling*—Determining the probability distribution of a sequence
    of words in human language, where knowing the most likely next word in a sequence
    is particularly important for language generation—predicting the next word or
    sentence.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言建模* —— 确定人类语言中一系列单词的概率分布，其中知道一个序列中最有可能的下一个单词对于语言生成——预测下一个单词或句子——尤为重要。'
- en: '*Dependency parsing*—Splitting a sentence into a *dependency tree* that represents
    its grammatical structure and the relationships between its words. Note that POS
    tagging can be important here.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*依赖解析* —— 将一句话分成一个表示其语法结构和单词之间关系的*依赖树*。请注意，POS标记在这里可能很重要。'
- en: 1.2 Understanding NLP in the context of AI
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 在AI背景下理解NLP
- en: Before proceeding with the rest of this book, it is important to understand
    the term natural language processing and to correctly situate it with respect
    to other commonly encountered terms, such as artificial intelligence, machine
    learning, and deep learning. The popular media often assign meanings to these
    terms that do not match their use by machine learning scientists and engineers.
    As such, it is important to kick off our journey by defining precisely what we
    mean when we use these terms, as shown in the Venn diagram in figure 1.2.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本书的其余部分之前，了解自然语言处理这个术语，并正确地将其与其他常见术语，如人工智能、机器学习和深度学习相联系非常重要。流行媒体经常将这些术语赋予的含义与机器学习科学家和工程师使用它们的含义不匹配。因此，在我们使用这些术语时，通过图1.2中的部分图解精确定义这些术语非常重要。
- en: '![01_02](../Images/01_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![01_02](../Images/01_02.png)'
- en: Figure 1.2 A Venn diagram visualization of the terms natural language processing
    (NLP), artificial intelligence (AI), machine learning, and deep learning relative
    to each other. Symbolic AI is also shown.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：自然语言处理（NLP）、人工智能（AI）、机器学习和深度学习相互关系的维恩图解。具有符号AI的其他相关内容也在图中显示。
- en: As you can see, deep learning is a subset of machine learning, which in turn
    is a subset of AI. NLP is a subset of AI as well, with a nonempty intersection
    with deep learning and machine learning. This figure expands on the one presented
    by François Chollet.[³](#pgfId-1081209) Please see chapter 6 and section 8.1 of
    his book for a good overview of the application of neural nets to text. Symbolic
    AI is also shown in the figure and will be described in the next subsection.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，深度学习是机器学习的子集，而机器学习又是AI的子集。NLP也是AI的子集，与深度学习和机器学习有非空交集。本图扩展了François Chollet提出的图表[³](#pgfId-1081209)。请参阅他的书中的第6章和第8.1节，了解神经网络在文本中的应用综述。符号AI也在图表中显示，并将在下一小节中描述。
- en: 1.2.1 Artificial intelligence (AI)
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 人工智能（AI）
- en: Artificial intelligence as a field came about in the middle of the 20th century,
    as a broad effort to make computers mimic and perform tasks typically carried
    out by human beings. Initial approaches focused on manually deriving and hard-coding
    explicit rules for manipulating input data for each circumstance of interest.
    This paradigm is typically referred to as *symbolic AI*. It worked for well-defined
    problems such as chess but notably stumbled when encountering problems from the
    perception category, such as vision and speech recognition. A new paradigm was
    needed, one where the computer could learn new rules from data, rather than having
    a human supervisor specify them explicitly. This led to the rise of machine learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能这一领域起源于20世纪中叶，旨在使计算机模仿和执行人类通常执行的任务。最初的方法侧重于手动推导和硬编码显式规则，以处理每种感兴趣情况的输入数据。这个范式通常被称为*符号AI*。它适用于像棋类这样明确定义的问题，但在遇到属于感知类别的问题，如视觉和语音识别时，明显遇到了困难。需要一种新的范式，其中计算机可以从数据中学习新规则，而不是让人类主管明确指定它们。这导致了机器学习的崛起。
- en: 1.2.2 Machine learning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 机器学习
- en: In the 1990s, the paradigm of machine learning became the dominant trend in
    AI. Instead of explicitly programming a computer for every possible scenario,
    the computer would now be *trained* to associate input to output signals by seeing
    many examples of such corresponding input-output pairs. Machine learning employs
    heavy mathematical and statistical machinery, but because it tends to deal with
    large and complex datasets, the field relies more on experimentation, empirical
    observations, and engineering than mathematical theory.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪90年代，机器学习的范式成为了人工智能的主导趋势。计算机不再为每种可能的情景明确地编程，而是通过看到许多相应的输入输出对的示例来*训练*计算机将输入与输出信号关联起来。机器学习使用了大量的数学和统计机制，但由于它往往涉及大型和复杂的数据集，该领域更多地依赖于实验、经验观察和工程，而不是数学理论。
- en: A machine learning algorithm learns a representation of input data that transforms
    it into appropriate output. For that, it needs a collection of data, such as a
    set of sentence inputs in a sentence classification task, and a set of corresponding
    outputs, for example, tags such as {“positive,” “negative”} for sentence classification.
    Also needed is a *loss function**,* which measures how far the current output
    of the machine learning model is from the expected output of the dataset. To aid
    in understanding, consider a binary classification task, where the goal of machine
    learning might be to pick a function called the *decision boundary* that will
    cleanly separate data points of the different types, as shown in figure 1.3\.
    This decision boundary should *generalize* beyond training data to unseen examples.
    To make this boundary easier to find, you might want to first preprocess or transform
    the data into a form more amenable for separation. We seek such transformations
    from the allowable set of functions called the *hypothesis set*. Automatically
    determining such a transformation, which makes the machine learning end goal easier
    to accomplish, is specifically what is referred to as *learning*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法学习一种将输入数据转换为适当输出的表示。为此，它需要一组数据，例如句子分类任务中的一组句子输入，以及一组相应的输出，例如句子分类的标签，如{“positive”，“negative”}。还需要一个*损失函数*，它衡量机器学习模型当前输出与数据集预期输出的距离有多远。为了帮助理解，考虑一个二元分类任务，其中机器学习的目标可能是选择一个名为*决策边界*的函数，它将清晰地将不同类型的数据点分开，如图1.3所示。这个决策边界应该*泛化*到超出训练数据的未见示例。为了使这个边界更容易找到，您可能希望首先对数据进行预处理或转换，使其更易于分离。我们从被允许的一组称为*假设集*的函数中寻求这样的转换。自动确定这样一个转换，使得机器学习的最终目标更容易实现，具体来说就是所谓的*学习*。
- en: '![01_03](../Images/01_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![01_03](../Images/01_03.png)'
- en: 'Figure 1.3 An illustrative example of a major motivating task in machine learning:
    finding a decision boundary in the hypothesis set to effectively separate different
    types of points from each other. In the case shown in this figure, the hypothesis
    set may be the set of arcs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 机器学习中一个重要的激励任务的示例：在假设集中找到一个决策边界，以有效地将不同类型的点彼此分开。在本图所示的情况下，假设集可能是弧的集合。
- en: Machine learning automates this process of searching for the best input-output
    transformation inside some predefined hypothesis set, using guidance from some
    feedback signal embodied by the loss function. The nature of the hypothesis set
    determines the class of algorithms under consideration, as we outline next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习自动化了在一些预定义的假设集中搜索最佳输入输出转换的过程，利用损失函数所体现的一些反馈信号的指导。假设集的性质确定了考虑的算法类别，我们接下来会概述。
- en: '*Classical machine learning* is initiated with probabilistic modeling approaches
    such as *naive Bayes*. Here, we make a *naive* assumption that the input data
    features are all independent. *Logistic regression* is a related method and typically
    the first one a data scientist will try on a dataset to baseline it. The hypothesis
    sets for both of these classes of methods are sets of linear functions.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*经典机器学习*是以概率建模方法为起点，例如*朴素贝叶斯*。在这里，我们做出一个*朴素*假设，即输入数据特征都是相互独立的。*逻辑回归*是一个相关方法，通常是数据科学家在数据集上尝试的第一个方法，以其为基准。这两类方法的假设集都是线性函数的集合。'
- en: '*Neural networks* were initially developed in the 1950s, but it was not until
    the 1980s that an efficient way to train large networks was discovered—backpropagation
    coupled with the stochastic gradient descent algorithm. While backpropagation
    provides a way to compute gradients for the network, stochastic gradient descent
    uses these gradients to train the network. We review these concepts briefly in
    appendix B. The first successful practical application occurred in 1989, when
    Yann LeCun of Bell Labs built a system for recognizing handwritten digits, which
    was then used heavily by the US Postal Service.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络*最初是在20世纪50年代发展起来的，但直到20世纪80年代才发现了训练大型网络的有效方法——反向传播算法与随机梯度下降算法相结合。虽然反向传播提供了计算网络梯度的方法，但随机梯度下降则利用这些梯度来训练网络。我们在附录B中简要回顾了这些概念。第一个成功的实际应用发生在1989年，当时贝尔实验室的Yann
    LeCun构建了一个识别手写数字的系统，这个系统后来被美国邮政部门大量使用。'
- en: '*Kernel methods* rose in popularity in the 1990s. These methods attempt to
    solve classification problems by finding good decision boundaries between sets
    of points, as was conceptualized in figure 1.3\. The most popular such method
    is the *support vector machine* (SVM). Attempts to find a good decision boundary
    proceed by mapping the data to a new high-dimensional representation where hyperplanes
    are valid boundaries. The distance between the hyperplane and the closest data
    points in each class is then maximized. The high computational cost of operating
    in the high-dimensional space is alleviated using the *kernel trick**.* Instead
    of computing high-dimensional data representations explicitly, a *kernel function*
    is used to compute distances between points at a fraction of the computing cost.
    This class of methods is backed by solid theory and is amenable to mathematical
    analysis, which is linear when the kernel is a linear function—attributes that
    made these methods extremely popular. However, performance on perceptual machine
    learning problems left much to be desired, because these methods first required
    a manual *feature engineering* step, which was brittle and prone to error.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*核方法* 在20世纪90年代变得流行起来。这些方法试图通过找到好的决策边界来解决分类问题，就像在图1.3中概念化的那样。最受欢迎的方法是*支持向量机*（SVM）。通过将数据映射到一个新的高维表示，然后在这个表示中超平面就是有效的边界。然后最大化每个类中最近数据点与超平面之间的距离。在高维空间中操作的高计算成本通过*核技巧*来减轻。这个方法类别受到坚实的理论支持，并且可以进行数学分析，当核是线性函数时，这样的分析是线性的。然而，在感知机器学习问题上的表现仍有待改善，因为这些方法首先需要手动进行*特征工程*，这使方法变得脆弱且容易出错。'
- en: '*Decision trees* and related methods are another class of algorithms that is
    still widely used. A decision tree is a decision support aid that models decisions
    and their consequences as *trees*, that is, a graph where any two nodes are connected
    by exactly one path. Alternatively, a tree can be defined as a flowchart that
    transforms input values into output categories. The popularity of decision trees
    rose in the 2010s, when methods relying on them began to be preferred over kernel
    methods. This popularity benefited from their ease of visualization, comprehension,
    and explainability. To aid in understanding, figure 1.4 shows an example decision
    tree structure that classifies the input {A,B} in category 1 if A<10, category
    2 if A>=10 while B<25, and category 3 otherwise.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*及相关方法是另一类仍然被广泛使用的算法类别。决策树是一种决策支持辅助工具，可以将决策及其后果建模为*树*，即一个两个节点之间只有一条路径连接的图。另外，可以将树定义为将输入值转换为输出类别的流程图。决策树的流行度在2010年代上升，当依赖它们的方法开始被更喜欢于核方法时。这种流行度得益于它们易于可视化、理解和解释。为了帮助理解，图1.4展示了一个示例决策树结构，如果
    A<10 则将输入 {A,B} 分类为类别 1，如果 A>=10 且 B<25 则分类为类别 2，否则分类为类别 3。'
- en: '![01_04](../Images/01_04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![01_04](../Images/01_04.png)'
- en: Figure 1.4 Example decision tree structure that classifies the input {A,B} in
    category 1 if A<10, category 2 if A>=10 while B<25, and category 3 otherwise
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 示例决策树结构，如果 A<10 则将输入 {A,B} 分类为类别 1，如果 A>=10 且 B<25 则分类为类别 2，否则分类为类别 3
- en: '*Random forests* provide a practical machine learning method for applying decision
    trees. This method involves generating a large number of specialized trees and
    combining their outputs. Random forests are extremely flexible and widely applicable,
    making them often the second algorithm to try after logistic regression for baselining.
    When the Kaggle open competition platform started out in 2010, random forests
    quickly became the most widely used algorithm on the platform. In 2014, *gradient-boosting
    machines* took over. They iteratively learn new decision-tree-based models that
    address weak points of models from the previous iterations. At the time of this
    writing, they are widely considered to be the best class of methods for addressing
    nonperceptual machine learning problems. They are still extremely popular on Kaggle.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林*为应用决策树提供了一种实用的机器学习方法。该方法涉及生成大量的专门树并结合它们的输出。随机森林非常灵活且广泛适用，通常在逻辑回归之后作为基线的第二种算法尝试。当Kaggle开放竞赛平台在2010年启动时，随机森林迅速成为该平台上最广泛使用的算法。在2014年，*梯度提升机*接管了这一地位。它们迭代地学习新的基于决策树的模型，解决了上一轮迭代中模型的弱点。在撰写本文时，它们被普遍认为是解决非感知机器学习问题的最佳类方法。它们在Kaggle上仍然非常受欢迎。'
- en: Around 2012, GPU-trained deep convolutional neural networks (CNNs) began to
    win the yearly ImageNet competition, marking the beginning of the current deep
    learning “golden age.” CNNs started to dominate all major image-processing tasks,
    such as object recognition and object detection. Similarly, we can find applications
    in the processing of human natural language, that is, NLP. Neural networks learn
    via a succession of increasingly meaningful, layered representations of the input
    data. The number of these *layers* specifies the *depth* of the model. This is
    where the term *deep learning*—the process of training deep neural networks—comes
    from. To distinguish them from deep learning, all aforementioned machine learning
    methods are often referred to as *shallow* or *traditional* learning methods.
    Note that neural networks with a small depth would also be classified as shallow
    but not traditional. Deep learning has come to dominate the field of machine learning,
    being a clear favorite for perceptual problems and sparking a revolution in the
    complexity of problems that can be handled.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2012年，GPU训练的深度卷积神经网络（CNNs）开始在每年的ImageNet比赛中获胜，标志着当前深度学习“黄金时代”的开始。CNNs开始主导所有主要的图像处理任务，如对象识别和对象检测。同样，我们可以在处理人类自然语言，即NLP方面找到应用。神经网络通过对输入数据的一系列越来越有意义的分层表示进行学习。这些*层*的数量指定了模型的*深度*。这就是术语*深度学习*——训练深度神经网络的过程来自哪里。为了区分它们与深度学习，所有前述的机器学习方法通常被称为*浅层*或*传统*学习方法。请注意，深度较小的神经网络也将被分类为浅层，但不是传统的。深度学习已经主导了机器学习领域，成为感知问题的明显首选，并引发了能够处理的问题复杂度的革命。
- en: Although neural networks were inspired by neurobiology, they are not direct
    models of how our nervous system works. Every layer of a neural network is parameterized
    by a set of numbers, referred to as the layer’s weights, specifying exactly how
    it transforms the input data. In deep neural networks, the total number of parameters
    can easily reach into the millions. The already-mentioned backpropagation algorithm
    is the algorithmic engine used to find the right set of parameters, that is, to
    *learn* the network. A visualization of a simple neural network with two fully
    connected hidden layers is shown in figure 1.5\. Also shown on the right is a
    summarized visualization of the same, which we will often employ. A deep neural
    network would have many such layers. A notable neural network architecture that
    does not conform to such a *feedforward* nature is the *long short-term memory*
    (LSTM) recurrent neural network (RNN) architecture. Unlike the feedforward architecture
    in figure 1.5, which accepts a fixed-length input of length 2, LSTMs can process
    input sequences with arbitrary lengths.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络受到神经生物学的启发，但它们并不是我们神经系统工作方式的直接模型。神经网络的每一层都由一组数字参数化，称为该层的权重，准确指定了它如何转换输入数据。在深度神经网络中，参数的总数可以轻易达到百万级别。前面提到的反向传播算法是用来找到正确参数集的算法引擎，也就是*学习*网络的过程。图1.5展示了一个具有两个全连接隐藏层的简单神经网络的可视化。右侧还显示了同样的总结性可视化，我们经常会使用。一个深度神经网络可能有许多这样的层。一个显著的神经网络架构，不符合*前馈*性质的是*长短期记忆*（LSTM）循环神经网络（RNN）架构。与图1.5中的前馈架构不同，该架构接受长度为2的固定长度输入，而LSTMs可以处理任意长度的输入序列。
- en: '![01_05](../Images/01_05.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![01_05](../Images/01_05.png)'
- en: Figure 1.5 Visualization of a simple feedforward neural network with two fully
    connected hidden layers (left). On the right is a summarized equivalent representation,
    which we will often employ to simplify diagrams.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 显示了一个具有两个全连接隐藏层的简单前馈神经网络的可视化（左）。右侧是一个总结性的等效表示，我们经常会用来简化图表。
- en: As previously touched on, what sparked the most recent interest in deep learning
    was spanned hardware, the availability of vast amounts of data, and algorithmic
    progress. GPUs had been developed for the video gaming market, and the internet
    matured to begin providing the field with unprecedented quality and quantity of
    data. Wikipedia, YouTube, and ImageNet are specific examples of data sources,
    the availability of which has driven many advances in computer vision and NLP.
    The ability of neural networks to eliminate the need for expensive manual feature
    engineering—which is needed to apply shallow learning methods to perceptual data
    successfully—is arguably the factor that influenced the ease of adoption of deep
    learning. Because NLP is a perceptual problem, it will also be the most important
    class of machine learning algorithms addressed in this book, albeit not the only
    one.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，引发深度学习最近兴趣的因素是硬件的跨度，大量数据的可用性以及算法的进步。GPU最初是为视频游戏市场开发的，互联网的成熟开始为该领域提供前所未有的质量和数量的数据。维基百科、YouTube和ImageNet是数据源的具体例子，其可用性推动了计算机视觉和自然语言处理的许多进步。神经网络消除了昂贵的手工特征工程的需求——这是将浅层学习方法成功应用于感知数据所需的——这可以说是影响了深度学习易于采纳的因素。由于自然语言处理是一个感知问题，它也将是本书中讨论的最重要的机器学习算法类别之一，尽管不是唯一的。
- en: Next, we aim to get some insight into the history and progression of advances
    in NLP.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们的目标是深入了解自然语言处理（NLP）领域的历史和进展。
- en: 1.2.3 Natural language processing (NLP)
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 自然语言处理（NLP）
- en: Language is one of the most important aspects of human cognition. It stands
    to reason that in order to create true artificial intelligence, a machine needs
    to be taught how to interpret, understand, process, and act on human language.
    This underlines the importance of NLP to the fields of AI and machine learning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是人类认知中最重要的方面之一。毋庸置疑的是，为了创建真正的人工智能，机器需要被教导如何解释、理解、处理和作出对人类语言的反应。这强调了自然语言处理对人工智能和机器学习领域的重要性。
- en: Just like the other subfields of AI, initial approaches to handling NLP problems,
    such as sentence classification and sentiment analysis, were based on explicit
    rules, or symbolic AI. Such systems typically could not generalize to new tasks
    and would break down easily. Since the advent of kernel methods in the 1990s,
    human effort has been channeled toward feature engineering—transforming the input
    data manually into a form that the shallow learning methods could use to produce
    useful predictions. This method is extremely time-consuming, task-specific, and
    inaccessible to a nonexpert. The advent of deep learning, around 2012, sparked
    a true revolution in NLP. The ability of neural networks to automatically engineer
    appropriate features in some of their layers lowered the bar for the applicability
    of these methods to new tasks and problems. Human effort then focused on designing
    the appropriate neural network architecture for any given task, as well as tuning
    various hyperparameter settings during training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人工智能的其他子领域一样，处理自然语言处理问题的初始方法，如句子分类和情感分析，都是基于显式规则或符号型人工智能。这种系统通常无法推广到新的任务，并且很容易崩溃。自从20世纪90年代核方法诞生以来，人们一直致力于特征工程——将输入数据手动转化为浅层学习方法可以用来产生有用预测的形式。这种方法非常耗时、任务特定且对非专家来说难以接触。深度学习的出现（大约在2012年）引发了自然语言处理的真正革命。神经网络能够在其某些层自动设计合适的特征，降低了这些方法对新任务和问题的适用性门槛。然后，人们将精力集中在为特定任务设计适当的神经网络架构，以及调整训练过程中的各种超参数设置上。
- en: The standard way to train NLP systems involves collecting a large set of data
    points, each reliably annotated with output labels, such as “positive” or “negative,”
    in a sentiment analysis task of sentences or documents. These data points are
    then supplied to the machine learning algorithm to learn the best representation
    or transformation of input to output signals that could potentially generalize
    to new data points. Both within NLP and in other subfields of machine learning,
    this process is often referred to as the paradigm of *supervised learning*. The
    labeling process, which is typically done manually, provides the “supervision
    signal” for learning the representative transformation. Learning representations
    from unlabeled data, on the other hand, is referred to as *unsupervised learning*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自然语言处理系统的标准方式是收集一组大量的数据点，每个数据点都可靠地注释了输出标签，比如情感分析任务中的“积极”或“消极”的句子或文档。然后将这些数据点提供给机器学习算法，学习最佳的输入到输出信号的表示或转换，可以推广到新的数据点。在自然语言处理和机器学习的其他子领域中，这个过程通常被称为“监督学习”范式。标注过程通常是手动完成的，为学习代表性转换提供“监督信号”。另一方面，从无标签数据中学习表示转换被称为“无监督学习”。
- en: Although today’s machine learning algorithms and systems are not a direct replica
    of biological learning systems and should not be considered models of such systems,
    some of their aspects are inspired by evolutionary biology, and, in the past,
    inspirations drawn from biology have guided significant advances. Based on this,
    it seems flawed that for each new task, language, or application domain, the supervised
    learning process has traditionally been repeated from scratch. This process is
    somewhat antithetical to the way natural systems learn—building on and reusing
    previously acquired knowledge. Despite this, significant advances have been achieved
    in learning for perceptual tasks from scratch, notably in machine translation,
    question-answering systems, and chatbots, although some drawbacks remain. In particular,
    today’s systems are not robust in handling significant changes in the sample distribution
    from which the input signals are drawn. In other words, the systems learn how
    to perform well on inputs of a certain kind or type. If we change the input type,
    it can lead to a significant degradation in performance and sometimes absolute
    failure. Moreover, to fully democratize AI and make NLP accessible to the average
    engineer at a small business—or to anyone without the resources possessed by major
    internet companies—it would be extremely helpful to be able to download and reuse
    knowledge acquired elsewhere. This is also important to anyone living in a country
    where the lingua franca may differ from English or other popular languages for
    which pretrained models exist, as well as anyone working on tasks that may be
    unique to their part of the world or tasks that no one has ever explored. Transfer
    learning provides a way to address some of these issues.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然今天的机器学习算法和系统并非生物学习系统的直接复制品，也不应被认为是这种系统的模型，但它们的某些方面受到了进化生物学的启发，而在过去，从生物学中汲取的灵感引导了显著的进步。基于这一点，似乎不合逻辑的是，对于每个新的任务、语言或应用领域，监督学习过程传统上都是从零开始重复。这一过程在某种程度上与自然系统学习的方式背道而驰——建立在之前获得的知识之上并进行再利用。尽管如此，从零开始学习感知任务仍取得了重大进展，特别是在机器翻译、问答系统和聊天机器人领域，虽然其中仍存在一些缺点。尤其是，当样本分布发生重大变化时，现有系统在处理时的稳定性较差。换句话说，系统学会了在特定类型的输入上表现良好。如果我们改变输入类型，这可能导致性能显著下降，甚至完全失效。此外，为了完全民主化人工智能，并使自然语言处理对小型企业的普通工程师——或对没有大型互联网公司所拥有的资源的人——变得更易获得，能够下载和重复使用其他地方获得的知识将是极其有益的。这对于生活在官方语言可能与英语或其他流行语言不同的国家的人，以及从事可能在他们所在地区独特的任务或从未有人探索过的任务的人来说，也非常重要。迁移学习提供了一种解决这些问题的方法。
- en: Transfer learning enables one to literally transfer knowledge from one *setting*—which
    we define as a combination of a particular task, domain, and language—to a different
    setting. The original setting is naturally referred to as the *source setting*,
    and the final setting is referred to as the *target setting*. The ease and success
    of the transfer process hinges on the similarity of the source and target settings.
    Quite naturally, a target setting that is “similar” to the source in some sense,
    which we will define later on in this book, leads to an easier and more successful
    transfer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习使人们能够从一个*环境*——我们定义为特定任务、领域和语言的组合——转移知识到另一个不同的环境。原始环境自然被称为*源环境*，而最终的环境称为*目标环境*。迁移过程的难易程度和成功程度取决于源环境和目标环境的相似性。很自然地，如果目标环境在某种意义上与源环境“相似”，在这本书的后面我们将对此做出定义，那么迁移将会更加容易且成功。
- en: Transfer learning has been in implicit use in NLP for much longer than most
    practitioners realize, because it is a common practice to vectorize words using
    pretrained embeddings such as *word2vec* or *sent2vec* (more on these in the next
    section). Shallow learning methods have typically been applied to these vectors
    as features. We cover both of these techniques in more detail in the next section
    and in chapter 4 and apply them in various ways throughout the book. This popular
    approach relies on an unsupervised preprocessing step, which is used to first
    train these embeddings without any labels. Knowledge from this step is then transferred
    to the specific application in a supervised setting, where the said knowledge
    is refined and specialized to the problem at hand using a shallow learning algorithm
    on a smaller set of labeled examples. Traditionally, this paradigm of combining
    unsupervised and supervised learning steps has been referred to as *semisupervised
    learning*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习在自然语言处理中的隐式使用时间比大多数从业者意识到的要长得多，因为常见做法是使用预训练的嵌入，如*word2vec*或*sent2vec*（在下一节中会更详细介绍）对单词进行向量化。
    浅层学习方法通常被应用于这些向量作为特征。 我们将在接下来的章节和第四章中更详细地介绍这两种技术，并在整本书中以各种方式应用它们。 这种流行的方法依赖于一个无监督的预处理步骤，首先用于训练这些嵌入而不需要任何标签。
    然后，从这一步中获取的知识被转移到特定的应用程序中，在监督设置中，通过使用浅层学习算法在一小部分标记示例上对所说的知识进行改进和专业化，以解决手头的问题。
    传统上，将无监督和监督学习步骤相结合的这种范式被称为*半监督学习*。
- en: We next expand on the historical progression of advances in NLP, with a particular
    focus on the role transfer learning has played recently in this important subfield
    of AI and machine learning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细介绍自然语言处理进展的历史进程，特别关注转移学习最近在这一重要的人工智能和机器学习子领域中所起的作用。
- en: 1.3 A brief history of NLP advances
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 自然语言处理进展简史
- en: To frame your understanding of the state and importance of transfer learning
    in NLP, it can be helpful to first gain a better sense of the kinds of tasks and
    techniques that have historically been important for this subfield of AI. This
    section covers these tasks and techniques and culminates in a brief overview of
    recent advances in NLP transfer learning. This overview will help you appropriately
    contextualize the impact of transfer learning in NLP and understand why it is
    more important now than ever before.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要框架化你对自然语言处理中转移学习的状态和重要性的理解，首先了解历史上对这个人工智能子领域重要的任务和技术可以是有帮助的。 本节介绍了这些任务和技术，并以自然语言处理转移学习最近的进展概述告终。
    这个概述将帮助你适当地将转移学习在自然语言处理中的影响放入背景，并理解为什么它现在比以往任何时候都更重要。
- en: 1.3.1 General overview
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 概述
- en: NLP was born in the middle of the 20th century, alongside AI. A major historical
    NLP landmark was the Georgetown Experiment of 1954, in which a set of approximately
    60 Russian sentences was translated into English. In the 1960s, the Massachusetts
    Institute of Technology (MIT) NLP system ELIZA convincingly simulated a psychotherapist.
    Also in the 1960s, the vector space model for information representation was developed,
    where words came to be represented by vectors of real numbers, which were amenable
    to computation. The 1970s saw the development of a number of chatterbot/ chatbot
    concepts based on sophisticated sets of handcrafted rules for processing the input
    information.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理诞生于20世纪中叶，与人工智能同时出现。 自然语言处理的一个重要历史里程碑是1954年的乔治城实验，在该实验中，大约60个俄语句子被翻译成英语。
    在20世纪60年代，麻省理工学院（MIT）的自然语言处理系统ELIZA成功模拟了一名心理医生。 同样在20世纪60年代，信息表示的向量空间模型被开发出来，其中单词被表示为实数向量，这些向量可进行计算。
    20世纪70年代，基于处理输入信息的复杂手工规则集的一系列闲聊机器人/聊天机器人概念被开发出来。
- en: In the 1980s and 1990s, we saw the advent of the application of systematic machine
    learning methodologies to NLP, where rules were discovered by the computer versus
    being crafted by humans. This advance coincided with the explosion in the wider
    popularity of machine learning during that time, as we have already discussed
    earlier in this chapter. The late 1980s witnessed the application of *singular
    value decomposition* (SVD) to the vector space model, leading to *latent semantic
    analysis*—an unsupervised technique for determining the relationship between words
    in language.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在1980年代和1990年代，我们看到了将系统化的机器学习方法应用于自然语言处理的出现，计算机发现了规则，而不是人类制定了规则。这一进步与当时机器学习的普及爆炸同时发生，正如我们在本章前面已经讨论过的那样。1980年代末，将*奇异值分解*（SVD）应用于向量空间模型，导致*潜在语义分析*—一种无监督的确定语言中单词关系的技术。
- en: In the early 2010s, the rise of neural networks and deep learning in the field
    dramatically transformed NLP. Such techniques were shown to achieve state-of-the-art
    results for the most difficult NLP tasks, such as machine translation and text
    classification. The mid-2010s witnessed the development of the word2vec model,[⁴](#pgfId-1081308)
    and its variants sent2vec,[⁵](#pgfId-1081311) doc2vec,[⁶](#pgfId-1081314) and
    so on. These neural-network-based techniques vectorize words, sentences, and documents
    (respectively) in a way that ensures the distance between vectors in the generated
    vector space is representative of the difference in meaning between the corresponding
    entities, that is, words, sentences, and documents. Indeed, some interesting properties
    of such embeddings allowed analogies to be handled—the distance between the words
    *Man* and *King* are approximately equal to the distance between the words *Woman*
    and *Queen* in the induced vector space, for instance. The metric used to train
    these neural-network-based models was derived from the field of linguistics, more
    specifically *distributional semantics*, and did not require labeled data. The
    meaning of a word was assumed to be tied to its context, that is, the words surrounding
    it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代初，神经网络和深度学习在该领域的崛起，彻底改变了自然语言处理。这些技术被证明在最困难的自然语言处理任务中取得了最先进的结果，例如机器翻译和文本分类。2010年代中期见证了word2vec模型的发展，以及其变种sent2vec、doc2vec等等。这些基于神经网络的技术将单词、句子和文档（分别）向量化，以一种确保生成的向量空间中向量之间距离代表相应实体之间的差异的方式，即单词、句子和文档。事实上，这些嵌入的一些有趣属性允许处理类比—在诱导的向量空间中，单词*Man*和*King*之间的距离大约等于单词*Woman*和*Queen*之间的距离，例如。用于训练这些基于神经网络的模型的度量来自语言学领域，更具体地说是*分布语义学*，不需要标记数据。一个单词的含义被假定与其上下文相关联，即周围的单词。
- en: The variety of methods for embedding various units of text, such as words, sentences,
    paragraphs, and documents, became a key cornerstone of modern NLP. Once text samples
    are embedded into an appropriate vector space, analysis can often be reduced to
    the application of a well-known shallow statistical/machine learning technique
    for real vector manipulation, including clustering and classification. This can
    be viewed as a form of *implicit transfer learning*, and a semisupervised machine
    learning pipeline—the embedding step is unsupervised and the learning step is
    typically supervised. The unsupervised pretraining step essentially reduces the
    requirements for labeled data and, thereby, computing resources required to achieve
    a given performance—something we will learn to leverage transfer learning to do
    for us for a broader range of scenarios in this book.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 各种嵌入文本单元的方法，例如单词、句子、段落和文档，成为现代自然语言处理的关键基石。一旦文本样本被嵌入到适当的向量空间中，分析通常可以简化为对真实向量操作的众所周知的浅层统计/机器学习技术的应用，包括聚类和分类。这可以看作是一种*隐式迁移学习*的形式，以及一种半监督机器学习流水线—嵌入步骤是无监督的，学习步骤通常是监督的。无监督的预训练步骤实质上降低了标记数据的要求，从而减少了实现给定性能所需的计算资源—我们将在本书中学习如何利用迁移学习来为更广泛的情景提供服务。
- en: Around 2014, *sequence-to-sequence* models[⁷](#pgfId-1081321) were developed
    and achieved a significant improvement in difficult tasks such as machine translation
    and automatic summarization. In particular, whereas pre-neural network NLP pipelines
    consist of several explicit steps, such as POS tagging, dependency parsing, and
    language modeling, it was shown that machine translation could be carried out
    “sequence to sequence.” Here the various layers of a deep neural network automate
    all of these intermediate steps. These models learn to associate an input sequence,
    such as a source sentence in one language, with an output sequence—for example,
    that sentence’s translation into another language—via an encoder that converts
    inputs into a context vector and a decoder that converts it into the target sequence.
    Both the encoder and decoder were typically designed to be *recurrent neural networks*
    (RNNs). These are able to encode order information in the input sentence, something
    earlier models, such as the bag-of-words model, couldn’t do, leading to significant
    improvements in performance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2014年，*序列到序列*模型[⁷](#pgfId-1081321)被开发出来，并在困难任务，如机器翻译和自动摘要中取得显著改进。特别是，尽管在神经网络之前的NLP流水线由几个明确的步骤组成，例如词性标注、依存句法分析和语言建模，但后来表明机器翻译可以进行“序列到序列”的处理。在这里，深度神经网络的各个层自动执行了所有这些中间步骤。这些模型学会了通过一个将输入序列（例如一种语言中的源句子）与一个输出序列（例如该句子的另一种语言的翻译）相关联的方法，通过将输入转换成上下文向量的编码器和将其转换成目标序列的解码器。编码器和解码器通常被设计为*循环神经网络*（RNNs）。这些能够在输入句子中编码顺序信息，这是早期模型（如词袋模型）无法做到的，从而显著提高了性能。
- en: It was discovered, however, that long input sequences were harder to deal with,
    which motivated the development of the technique known as *attention*. This technique
    significantly improved the performance of machine translation sequence-to-sequence
    models by allowing the model to focus on the parts of the input sequence that
    were most relevant for the output. A model called *the transformer* [⁸](#pgfId-1081329)
    took this a step further by defining a *self-attention layer* for both the encoder
    and decoder, allowing both to build better context for text segments with respect
    to other text segments in the input sequence. Significant improvements in machine
    translation were achieved with this architecture, and it was observed to be better
    suited for training on massively parallel hardware than prior models, speeding
    up training by up to an order of magnitude.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人们发现，长输入序列更难处理，这促使了被称为*注意力*的技术的发展。这一技术通过让模型关注输入序列中最相关的部分，显著改善了机器翻译序列模型的性能。一个叫做*transformer*的模型进一步定义了*自注意力层*，用于编码器和解码器，使两者都能相对于输入序列中的其他文本段构建更好的上下文。这种架构在机器翻译方面取得了显著的改进，并且观察到它更适合在大规模并行硬件上进行训练，将训练速度提高了一个数量级。
- en: Up until about 2015, most practical methods for NLP focused on the *word level*,
    which means that the whole word was treated as an indivisible atomic entity and
    assigned a feature vector. This approach has several disadvantages, notably how
    to treat never-before-seen or *out-of-vocabulary* words. When the model encountered
    such words—for instance, if a word was misspelled—the method would fail because
    it could not vectorize it. In addition, the rise of social media changed the definition
    of what was considered natural language. Now, billions of people express themselves
    online using emoticons, newly invented slang, and deliberately misspelled words.
    It was not long until it was realized that the solution to many of these issues
    came naturally from treating language at the character level. In this paradigm,
    every character would be vectorized, and as long as the human was expressing themself
    with allowable characters, vector features could be generated successfully, and
    the algorithm could be successfully applied. Zhang et al.[⁹](#pgfId-1081337) showed
    this in the context of character-level CNNs for text classification and demonstrated
    a remarkable robustness to misspellings.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2015年左右，大多数自然语言处理的实用方法都集中在*词级别*，这意味着整个单词被视为不可分割的原子实体，并被赋予一个特征向量。这种方法有几个缺点，尤其是如何处理从未见过或*词汇外*的单词。当模型遇到这样的单词时，比如单词拼写错误时，该方法会失败，因为无法对其进行向量化。此外，社交媒体的兴起改变了什么被视为自然语言的定义。现在，数十亿人通过表情符号、新发明的俚语和故意拼错的单词在线表达自己。不久之后，人们意识到，许多这些问题的解决方案自然地来自于以字符级别处理语言。在这个范式中，每个字符都将被向量化，只要人类使用可接受的字符表达自己，就可以成功生成向量特征，并成功应用算法。Zhang等人[⁹](#pgfId-1081337)在字符级别CNN用于文本分类的背景下展示了这一点，并展示了对拼写错误的显著鲁棒性。
- en: 1.3.2 Recent transfer learning advances
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 最近的迁移学习进展
- en: Traditionally, learning has proceeded in either a fully supervised or fully
    unsupervised fashion for any given problem setting—a particular combination of
    task, domain, and language—from scratch. As previously alluded to, semisupervised
    learning was recognized as early as 1999, in the context of SVMs, as a way to
    address potentially limited labeled data availability. An initial unsupervised
    pretraining step on larger collections of unlabeled data made downstream supervised
    learning easier. Variants of this were studied to address potentially noisy—possibly
    incorrect—labels, which is an approach sometimes referred to as *weakly supervised
    learning*. However, it was often assumed that the same sampling distribution held
    for both the labeled and unlabeled datasets.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，针对任何给定的问题设置——任务、领域和语言的特定组合——学习都是以完全监督或完全无监督的方式进行的，从头开始。如前所述，半监督学习早在1999年就在SVM的背景下被认识到，作为一种解决可能有限标记数据可用性的方式。对更大规模的未标记数据集进行初始无监督预训练步骤使下游监督学习更容易。对此的变体被研究用于解决可能存在噪声——可能不正确——标签的情况，这种方法有时被称为*弱监督学习*。然而，通常假设标记数据集和未标记数据集的采样分布是相同的。
- en: Transfer learning relaxes these assumptions. In 1995, at the Conference on Neural
    Information Processing Systems (NeurIPS), one of the biggest conferences on machine
    learning, transfer learning was popularly recognized as “learning to learn.” Essentially,
    it was stipulated that intelligent machines need to possess lifelong learning
    capabilities that reuse learned knowledge for new tasks. This has since been studied
    under a few different names, including *learning to learn*, *knowledge transfer*,
    *inductive bias*, and *multitask learning*. In multitask learning, an algorithm
    is trained to perform well on multiple tasks simultaneously, thereby uncovering
    features that may be more generally useful. However, it wasn’t until around 2018
    that practical and scalable methods were developed to achieve it in NLP for the
    hardest perceptual problems.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习放宽了这些假设。1995年，在神经信息处理系统会议（NeurIPS）上，迁移学习被普遍认为是“学习学习”。基本上，它规定智能机器需要具有终身学习能力，以重复利用学到的知识进行新任务。此后，这一点已经在几个不同的名称下进行了研究，包括*学习学习*、*知识转移*、*归纳偏差*和*多任务学习*。在多任务学习中，算法被训练以在多个任务上同时表现良好，从而发现可能更普遍有用的特征。然而，直到2018年左右，才开发出了实用且可扩展的方法来解决NLP中最困难的感知问题。
- en: The year 2018 saw nothing short of a revolution in the field of NLP. The understanding
    in the field of how to best represent collections of text as vectors evolved dramatically.
    Moreover, it became widely recognized that open source models could be fine-tuned
    or transferred to different tasks, languages, and domains. At the same time, several
    of the big internet companies released even more and bigger NLP models for computing
    such representations and also specified well-defined procedures for fine-tuning
    them. All of a sudden, the ability to attain state-of-the-art results in NLP became
    accessible to the average practitioner, even an independent one. Some called it
    NLP’s “ImageNet moment,” referencing the explosion in computer vision applications
    witnessed post-2012, when a GPU-trained neural network won the ImageNet computer
    vision competition. Just as was the case for the original ImageNet moment, for
    the first time, a library of pretrained models became available for a large subset
    of NLP data, together with well-defined techniques for fine-tuning them to particular
    tasks at hand with labeled datasets of a size significantly smaller than would
    be needed otherwise. This book’s purpose is to describe, elucidate, evaluate,
    demonstrably apply, compare, and contrast the various techniques that fall into
    this category. We briefly overview these techniques next.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年可谓是自然语言处理领域的一场革命。对于如何最好地将文本集合表示为向量的理解发生了巨大变革。此外，人们普遍认识到开源模型可以进行微调或转移到不同的任务、语言和领域。与此同时，一些大型互联网公司发布了更多、更大的自然语言处理模型，用于计算这些表示，并且指定了明确定义的微调程序。突然之间，即使是普通从业者，甚至是独立从业者，也能够获得自然语言处理方面的最新成果。有人称之为自然语言处理的“ImageNet时刻”，这是在2012年之后看到的计算机视觉应用的爆发，当时一个GPU训练的神经网络赢得了ImageNet计算机视觉竞赛。就像最初的ImageNet时刻一样，预训练模型库首次为大量的自然语言处理数据提供了支持，以及对使用标记数据集微调到特定任务的明确定义技术，其数据集大小明显小于否则所需的大小。本书的目的是描述、阐明、评估、可证明地应用、比较和对比属于此类别的各种技术。我们接下来简要概述这些技术。
- en: Early explorations of transfer learning for NLP focused on analogies to computer
    vision, where it has been used successfully for over a decade. One such model—
    Semantic Inference for the Modeling of Ontologies (SIMOn)[^(10)](#pgfId-1081357)—employed
    character-level convolutional neural networks (CNNs) combined with bidirectional
    LSTMs for structural semantic text classification. The SIMOn approach demonstrated
    NLP transfer learning methods directly analogous to those that have been used
    in computer vision. The rich body of knowledge on transfer learning for computer
    vision applications motivated this approach. The features learned by this model
    were shown to be useful for unsupervised learning tasks and to work well on social
    media language data, which can be somewhat idiosyncratic and very different from
    the kind of language on Wikipedia and other large book-based datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 早期对自然语言处理的迁移学习的探索主要集中在类比于计算机视觉，后者在过去十多年中已经成功使用了。其中一种模型——本体建模语义推理（SIMOn）[^10]——采用了字符级卷积神经网络（CNN）与双向LSTM结合的结构语义文本分类。SIMOn方法展示了直接类比于计算机视觉的自然语言处理迁移学习方法。计算机视觉应用的丰富知识库激发了这种方法。该模型学到的特征被证明对无监督学习任务有用，并且在社交媒体语言数据上表现良好，这种语言有些特殊，与维基百科和其他大型基于书籍的数据集上的语言非常不同。
- en: One notable weakness of the original formulation of word2vec was disambiguation.
    There was no way to distinguish between various uses of a word that may have different
    meanings depending on context, such as the case of homographs—duck (posture) versus
    duck (bird) or fair (a gathering) versus fair (just). In some sense, the original
    word2vec formulation represents each such word by the average vector of the vectors
    representing each of these distinct meanings of the homograph. *Embeddings from
    Language Models*[^(11)](#pgfId-1081362)—abbreviated ELMo after the popular *Sesame
    Street* character—is an attempt to develop contextualized embeddings of words
    using bidirectional LSTMs. The embedding of a word in this model depends very
    much on its context, with the corresponding numerical representation being different
    for each such context. ELMo did this by being trained to predict the next word
    in a sequence of words, which is very much related to the concept of language
    modeling that was introduced at the beginning of the chapter. Huge datasets, like
    Wikipedia and various datasets of books, are readily available for training in
    this framework.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 word2vec 公式中一个显著的弱点是消歧。无法区别在不同上下文中可能具有不同含义的单词的各种用法，例如同音异形词的情况——鸭子（姿势）与鸭子（鸟类）或公平（一次集会）与公平（有正义）。在某种意义上，原始的
    word2vec 公式通过单词的平均向量表示来代表一个单词中这些不同同音异形词的向量的平均值。*从语言模型中嵌入*([^(11)](#pgfId-1081362)
    ELMo)——以受欢迎的*Sesame Street*角色命名-试图使用双向 LSTM 开发单词的上下文化嵌入。在这个模型中，一个单词的嵌入非常依赖于它的上下文，相应的数值表示对于每个这样的上下文是不同的。ELMo
    通过训练来预测单词序列中的下一个词，这与本章开头介绍的语言建模概念有很大关系。大型数据集，如维基百科和各种书籍数据集，可用于此框架的训练。
- en: '*The Universal Language Model Fine-Tuning* [^(12)](#pgfId-1081366) (ULMFiT)
    method was proposed to fine-tune any neural-network-based language model for any
    particular task and was initially demonstrated in the context of text classification.
    A key concept behind this method is *discriminative fine-tuning*, where the different
    layers of the network are trained at different rates. The OpenAI *Generative Pretrained
    Transformer* (GPT) modified the encoder-decoder architecture of the transformer
    to achieve a fine-tunable language model for NLP. It discarded the encoders and
    retained the decoders and their self-attention sublayers. Bidirectional Encoder
    Representations from Transformers[^(13)](#pgfId-1081373) (BERT) did the opposite,
    modifying the transformer architecture by preserving the encoders and discarding
    the decoders and also relying on *masking* of words, which would then need to
    be predicted accurately as the training metric. These concepts will be discussed
    in detail in the upcoming chapters.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*通用语言模型微调*（Universal Language Model Fine-Tuning, ULMFiT ）[^(12)](#pgfId-1081366)
    方法被提出来为了微调任何一种基于神经网络的语言模型以适应特定任务，并在文本分类的情况下被初步证明。这种方法背后的一个重要概念是*有区别的微调*，其中网络的不同层以不同的速率进行训练。OpenAI
    的*生成式预训练变换器*（Generative Pretrained Transformer, GPT）改变了变换器的编码器-解码器架构，以实现 NLP 微调语言模型。它放弃了编码器，并保留了解码器及其自我注意力子层。来自变形金刚的双向编码器表征[^(13)](#pgfId-1081373)
    (Bidirectional Encoder Representations from Transformers, BERT) 则相反，修改了变换器的结构，保留了编码器并丢弃了解码器，还依赖于单词*掩蔽*，需要准确预测训练指标。这些概念将在接下来的章节中详细讨论。'
- en: 'In all of these language-model-based methods—ELMo, ULMFiT, GPT, and BERT—it
    was shown that generated embeddings could be fine-tuned for specific downstream
    NLP tasks with relatively few labeled data points. The focus on language models
    was deliberate: it was hypothesized that the hypothesis set induced by them would
    be generally useful, and the data for massive training was known to be readily
    available.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些基于语言模型的方法中——ELMo、ULMFiT、GPT 和BERT，都表明生成的嵌入可以针对特定的下游NLP任务进行微调，只需相对较少的标记数据点即可。对语言模型的关注是有意义的：假设它们诱导的假设集是普遍有用的，并且已知为大规模训练准备了数据。
- en: Next, we highlight key aspects of transfer learning in computer vision to even
    better frame transfer learning in NLP and to see if anything can be learned and
    borrowed for our purposes. This knowledge will become a rich source of analogies
    that will be used to drive our exploration of NLP transfer learning in the remainder
    of the book.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重点介绍计算机视觉中的迁移学习的关键方面，以更好地理解在 NLP 中的迁移学习，并看看是否可以为我们的目的学到和借鉴一些知识。这些知识将成为本书剩余部分中驱动我们对
    NLP 迁移学习探索的丰富类比的来源。
- en: 1.4 Transfer learning in computer vision
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 计算机视觉中的迁移学习
- en: Although the target of this book is NLP, it is helpful to frame NLP transfer
    learning in the context of computer vision transfer learning. One reason for doing
    this is that neural network architectures from the two subfields of AI may share
    some similar features, so techniques from computer vision can be borrowed, or,
    at the very least, be used to inform, techniques for NLP. Indeed, the availability
    of such techniques in computer vision is arguably a large driver behind recent
    NLP transfer learning research. Researchers can access a library of well-defined
    computer vision methods to experiment with in the relatively unexplored domain
    of NLP. The extent to which such techniques are directly transferable is, however,
    an open question, and it is important to remain mindful of a number of important
    differences. One such difference is that NLP neural networks tend to be shallower
    than those used in computer vision.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的目标是自然语言处理，但将NLP迁移学习放在计算机视觉迁移学习的背景下进行框架化有助于理解。这样做的原因之一是，来自AI的这两个子领域的神经网络架构可能具有某些相似的特征，因此可以借鉴计算机视觉的方法，或者至少用它们来指导NLP的技术。事实上，计算机视觉领域中这些技术的可用性被认为是最近NLP迁移学习研究的一个重要驱动因素。研究人员可以访问一个定义良好的计算机视觉方法库，以在相对未被探索的NLP领域进行实验。然而，这些技术直接可转移的程度是一个开放的问题，有几个重要的区别需要注意。一个这样的区别是，NLP神经网络通常比计算机视觉中使用的神经网络要浅。
- en: 1.4.1 General overview
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 总体概述
- en: The goal of computer vision or machine vision is to enable computers to understand
    digital images and/or videos, including methods for acquiring, processing, and
    analyzing image data and making decisions based on their derived representation.
    Video analysis can typically be carried out by splitting videos into frames, which
    can then be viewed as an image analysis problem. Thus, computer vision can be
    posed as an image analysis problem theoretically without the loss of generality.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉或视觉机器人的目标是使计算机理解数字图像和/或视频，包括获取、处理和分析图像数据，并根据它们的派生表示做出决策。视频分析通常可以通过将视频分成帧来进行，然后可以将其视为图像分析问题。因此，理论上计算机视觉可以被提出为图像分析问题而不失一般性。
- en: Computer vision was born along with AI in the middle of the 20th century. Vision,
    obviously, is an important part of cognition, so researchers seeking to build
    intelligent robots recognized it as being important early on. Initial methods
    in the 1960s attempted to mimic the human visual system, whereas focus on extracting
    edges and modeling of shapes in scenes rose in popularity in the 1970s. The 1980s
    witnessed more mathematically robust methods developed for various aspects of
    computer vision, notably facial recognition and image segmentation, with mathematically
    rigorous treatments emerging by the 1990s. This move coincided with the rise in
    popularity of machine learning during that time, as we already touched on. The
    following couple of decades saw focus and effort spent on developing better feature-extraction
    methods for images, prior to the application of a shallow machine learning technique.
    The “ImageNet moment” of 2012, when GPU-accelerated neural networks won the prominent
    ImageNet competition by a wide margin for the very first time, marked a revolution
    in the field.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉诞生于20世纪中期，与人工智能一起出现。显然，视觉是认知的重要部分，因此致力于建造智能机器人的研究人员早期就认识到它的重要性。上世纪六十年代，首批方法试图模仿人类视觉系统，而上世纪七十年代人们更加关注提取边缘和场景中形状建模。上世纪八十年代，各个方面的计算机视觉方法越来越成熟，尤其是人脸识别和图像分割，到了上世纪九十年代出现了数学严谨的方法。这个时期正值机器学习流行的时期，正如我们前面所提到的。接下来的几十年，致力于为图像开发更好的特征提取方法。在应用浅层机器学习技术之前，进行努力和重心在此。2012年的“ImageNet时刻”，当GPU加速的神经网络第一次在广受关注的ImageNet比赛中大幅领先时，标志着该领域的革命。
- en: ImageNet[^(14)](#pgfId-1081394) was originally published in 2009 and rapidly
    became the basis of a competition for testing the best methods for object recognition.
    The famed 2012 neural network entry pointed to deep learning as the way forward
    for computer vision in particular and perceptual problems in machine learning
    in general. Importantly for us, a number of researchers quickly realized that
    neural network weights from pretrained ImageNet models could be used to initialize
    neural network models for other, sometimes seemingly unrelated, tasks and achieve
    a significant improvement in performance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '- ImageNet[^(14)](#pgfId-1081394) 最初于 2009 年发布，并迅速成为测试目标识别最佳方法的竞赛基础。著名的 2012
    年神经网络条目指出了深度学习作为计算机视觉特别是机器学习中感知问题的前进之路。对我们来说，一些研究人员很快意识到，来自预训练的 ImageNet 模型的神经网络权重可以用于初始化其他有时看似无关的任务的神经网络模型，并显著提高性能。'
- en: 1.4.2 Pretrained ImageNet models
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '- 1.4.2 预训练的 ImageNet 模型'
- en: The various teams that have won the hallmark ImageNet yearly competition have
    been very generous with sharing their pretrained models. Notable examples of such
    CNN models follow.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在 ImageNet 每年的标志性比赛中获胜的各个团队非常慷慨地共享了他们的预训练模型。以下是一些值得注意的 CNN 模型示例。'
- en: The VGG architecture was initially introduced in 2014, with variants VGG16 (a
    depth of 16) and VGG19 (a depth of 19 layers). To make the deeper network converge
    during training, the shallower network needed to be trained until convergence
    first and its parameters used to initialize the deeper network. This architecture
    has been found to be somewhat slow to train and relatively large in terms of overall
    number of parameters—130 million to 150 million parameters in size.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '- VGG 架构最初是在 2014 年引入的，具有 VGG16（深度为 16）和 VGG19（深度为 19 层）两个变种。为了使更深的网络在训练过程中收敛，需要首先训练较浅的网络直至收敛，然后使用它的参数初始化更深的网络。该架构被发现在训练过程中有些慢，而且参数总数相对较大——约为
    1.3 亿至 1.5 亿个参数。'
- en: Some of these issues were addressed by the ResNet architecture in 2015\. Despite
    being substantially deeper, the number of parameters was significantly reduced—the
    smallest variant, ResNet50, is 50 layers deep with approximately 50 million parameters.
    A key to achieving this reduction was regularization via a technique called *max
    pooling* and a modular design out of subbuilding blocks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '- 2015 年 ResNet 架构解决了其中一些问题。尽管更深层，但参数数量显著减少——最小的变种 ResNet50 深 50 层，约有 5000
    万个参数。实现这种减少的关键是通过一种称为 *最大池化* 的技术进行正则化，并通过子构建块的模块化设计。'
- en: Other notable examples include Inception and its extension Xception, proposed
    in 2015 and 2016, respectively, which aim to create multiple levels of extracted
    features by stacking multiple convolutions within the same network module. Both
    of these models achieved further significant reduction in model size.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '- 其他值得注意的例子包括 Inception 及其扩展 Xception，分别于 2015 年和 2016 年提出，旨在通过在同一网络模块中堆叠多个卷积来创建多个级别的特征提取。这两个模型都进一步显著减小了模型大小。'
- en: 1.4.3 Fine-tuning pretrained ImageNet models
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '- 1.4.3 微调预训练的 ImageNet 模型'
- en: Due to the existence of the pretrained CNN ImageNet models that have been presented,
    it is uncommon for practitioners to train computer vision models from scratch.
    By far the more common approach is to download one of these open source models
    and either use it to initialize a similar architecture prior to learning on limited
    labeled data, such as *fine-tuning* a subset of the layers, or to use it as a
    fixed feature extractor.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '- 由于已经提出了预训练的 CNN ImageNet 模型，因此从头开始训练计算机视觉模型是不常见的。目前更常见的方法是下载其中一个这些开源模型，并在有限的标记数据上使用它来初始化类似的架构，例如
    *微调* 一部分层，或者将其用作固定的特征提取器。'
- en: A visualization of how a subset of layers to be fine-tuned is typically selected
    in a feedforward neural network is shown in figure 1.6\. A threshold is moved
    away from the output (and toward the input) as more data becomes available in
    the target domain, with layers between the threshold and output retrained. This
    change occurs because the increased amount of data can be used to train more parameters
    effectively than could be done otherwise. Additionally, movement of the threshold
    must happen in the right-to-left direction, that is, away from the output and
    toward the input. This movement direction allows us to retain layers encoding
    general features that are close to the input, while retraining layers closer to
    the output, which encode features specific to the source domain. Moreover, when
    the source and target are highly dissimilar, some of the more specific parameters/layers
    to the right of the threshold can be discarded.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.6中显示了在前馈神经网络中选择要微调的一部分层的可视化。随着目标领域中的数据量增加，阈值从输出（向输入）移动，阈值和输出之间的层被重新训练。这种变化是因为增加的数据量可以有效地用于训练更多的参数，而否则是无法完成的。此外，阈值的移动方向必须是从右到左，即远离输出端，接近输入端。这种移动方向使我们能够保留编码接近输入端的一般特征的层，同时重新训练接近输出端的层，它们编码源领域特定特征。而且，当源领域和目标领域高度不同的时候，一些阈值右侧的更具体的参数/层可以被丢弃。
- en: Feature extraction, on the other hand, involves removing only the last layer
    of the network, which, instead of producing data labels, will now produce a set
    of numerical vectors on which a shallow machine learning method, such as the support
    vector machine (SVM), can be trained as before.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，特征提取涉及仅移除网络的最后一层，该层不再产生数据标签，而是产生一组数值向量，可以通过浅层机器学习方法（如支持向量机SVM）进行训练，就像以前一样。
- en: In the retraining or *fine-tuning* approach, the prior pretrained weights do
    not all stay fixed, but a subset of them can be allowed to change based on the
    new labeled data. However, it is important to make sure that the number of parameters
    being trained does not lead to overfitting on limited new data, which motivates
    us to freeze some to reduce the number of parameters being trained. Picking the
    number of layers to freeze has typically been done empirically, with the heuristics
    in figure 1.6 guiding it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新训练或*微调*方法中，先前的预训练权重并不全部保持不变，而是允许其中的一个子集根据新的标记数据进行改变。然而，重要的是要确保在有限的新数据上训练的参数数量不会导致过度拟合，这促使我们冻结一些参数以减少正在训练的参数的数量。通常是以经验的方式来选择要冻结的层数，图1.6中的启发式方法指导了这一点。
- en: '![01_06](../Images/01_06.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![01_06](../Images/01_06.png)'
- en: Figure 1.6 Visualization of the various transfer learning heuristics applicable
    in computer vision for feedforward neural network architectures, which we will
    draw on in NLP whenever possible. A threshold is moved to the left, with more
    availability of training data in the target domain, and all parameters to the
    right of it are retrained, with the exception of those that are discarded due
    to increasing dissimilarity between source and target domains.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6表现了在计算机视觉中适用于前馈神经网络架构的各种迁移学习启发式方法的可视化，在NLP中我们将尽可能利用它。随着目标领域中的训练数据的增加，阈值向左移动，它右侧的所有参数都被重新训练，除了那些由于源领域和目标领域越来越不同而被丢弃的参数。
- en: It has been established in CNNs that the early layers—those closer to the input
    layer—perform functions more general to the task of image processing, such as
    detecting any edges in the image. Later layers—those closer to the output layer—perform
    functions more specific to the task at hand, such as mapping final numerical outputs
    to specific labels. This arrangement leads us to unfreeze and fine-tune layers
    closer to the output layer first and then incrementally unfreeze and fine-tune
    layers closer to the input layer if performance is found to be unsatisfactory.
    This process can continue as long as the available labeled dataset for the target
    task can support the increase in training parameters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中已经确定，靠近输入层的早期层—执行更一般的图像处理任务的功能，例如检测图像中的任何边缘。 靠近输出层的后期层—执行更特定于手头任务的功能，例如将最终的数值输出映射到特定标签。
    这种安排导致我们首先解冻和微调靠近输出层的层，然后逐渐解冻和微调接近输入层的层，如果发现性能不满意，这个过程将继续，只要目标任务的可用标记数据集能够支持训练参数的增加。
- en: A corollary of this process is that if the labeled dataset for the target task
    is very large, the whole network should probably be fine-tuned. If the target
    dataset is small, on the other hand, one needs to think carefully about how similar
    the target dataset is to the source dataset. If it is very similar, the model
    architecture can be directly initialized to pretrained weights when fine-tuning.
    If very different, it may be beneficial to discard the pretrained weights in some
    of the later layers of the network when initializing, because they may not have
    any relevance to the target task. Moreover, because the dataset is not large,
    only a small set of the remaining later layers should be unfrozen while fine-tuning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的一个推论是，如果目标任务的标记数据集非常大，整个网络可能都需要被微调。另一方面，如果目标数据集很小，就需要仔细考虑目标数据集与源数据集的相似程度。如果非常相似，模型体系结构可以直接初始化为预训练权重进行微调。如果非常不同，当初始化时，放弃一些网络的后续层的预训练权重可能会对目标任务没有任何相关性。此外，由于数据集不是很大，在微调时应该只解冻剩余后续层的一小部分。
- en: We will conduct computational experiments to explore these heuristics further
    in the subsequent chapters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行计算实验，以进一步探索这些启发式方法。
- en: 1.5 Why is NLP transfer learning an exciting topic to study now?
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 为什么NLP迁移学习是一个令人兴奋的研究课题？
- en: Now that we have framed the current state of NLP in the context of the general
    artificial intelligence and machine learning landscapes, we are in a good position
    to summarize why the key theme of this book is important and why you, the reader,
    should care very much about it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在整体人工智能和机器学习领域的背景下框定了NLP的当前状态，我们可以很好地总结为什么本书的主题重要，以及为什么您作为读者应该非常关心这个主题。
- en: By now it should be clear that recent years have seen a rapid acceleration in
    advances in this field. A number of pretrained language models have been made
    available, along with well-defined procedures, for the very first time, for fine-tuning
    them to more specific tasks or domains. It was discovered that analogies could
    be made to the way transfer learning had been conducted in computer vision for
    a decade, and a number of research groups were able to rapidly draw on a body
    of existing computer vision techniques to push forward the state of our understanding
    of NLP transfer learning. This work has achieved the important advantage of reducing
    computing and training time requirements for these problems for the average practitioner
    without access to massive resources.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，显而易见的是，近年来这一领域的进展迅速加速。许多预训练语言模型首次提供，同时也提供了明确定义的程序，用于对其进行更具体的任务或领域的微调。人们发现可以类比于计算机视觉领域进行迁移学习的方式，一些研究小组能够迅速借鉴现有的计算机视觉技术，推动我们对NLP迁移学习的了解的进展。这项工作取得了重要的优势，即为那些没有大量资源的普通从业者减少了这些问题的计算和训练时间要求。
- en: A lot of excitement exists in the field right now, and droves of researchers
    are working on this problem area. A lot of outstanding questions in a subject
    this novel present an opportunity for machine learning researchers to make a name
    for themselves by helping move the state of knowledge forward. Simultaneously,
    social media, which has become an increasingly significant factor in human interaction,
    presents new challenges not seen in NLP before. These challenges include slang/jargon
    and emoticon use, which may not be found in the more formal language that is typically
    used to train language models. A demonstrative example is the severe vulnerabilities
    discovered with the social media natural language ecosystem—notably with regard
    to election interference claims by sovereign democracies against other foreign
    governments, such as the Cambridge Analytica scandal.[^(15)](#pgfId-1081444) In
    addition, the general sense of the worsening of the “fake news” problem has increased
    interest in the field and has driven discussions of the ethical considerations
    that should be made when building these systems. All this, coupled with the proliferation
    of increasingly sophisticated chatbots in a variety of domains, and associated
    cybersecurity threats, implies that the problem of transfer learning in NLP is
    poised to continue growing in significance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 目前该领域存在着大量的激动人心的研究，并且大量的研究人员正在从事这个问题领域的研究。在这个新颖的学科中存在许多未解决的问题，这为机器学习研究人员通过帮助推动知识水平的提高而使自己出名提供了机会。同时，社交媒体已经成为人类互动中越来越重要的因素，它带来了在自然语言处理中以前未曾见过的新挑战。这些挑战包括俚语/行话和表情符号的使用，这些在通常用于训练语言模型的更正式语言中可能找不到。一个示例是在社交媒体自然语言生态系统中发现的严重漏洞——尤其是关于主权民主国家针对其他外国政府的选举干预指控，比如剑桥分析丑闻。此外，对“假新闻”问题恶化的一般感觉增加了人们对该领域的兴趣，并推动了在构建这些系统时应考虑的道德问题的讨论。所有这些，加上在各个领域不断增长的越来越复杂的聊天机器人的增加，以及相关的网络安全威胁，意味着自然语言处理中的迁移学习问题有望继续增长其重要性。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Artificial intelligence (AI) holds the promise of fundamentally transforming
    our society. To democratize the benefits of this transformation, we must ensure
    that state-of-the-art advances are accessible to everyone, regardless of language,
    access to massive computing resources, and country of origin.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能（AI）承诺着从根本上改变我们的社会。为了使这种转变的好处普及化，我们必须确保最新的进展对每个人都是可访问的，无论其语言、获取大规模计算资源的能力和出生国是什么。
- en: Machine learning is the dominant modern paradigm in AI, which, rather than explicitly
    programming a computer for every possible scenario, *trains* it to associate input
    to output signals by seeing many examples of such corresponding input-output pairs.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是人工智能中主要的现代范式，它不是为每种可能的情况明确地编程计算机，而是通过看到许多这样对应的输入-输出对的例子，*训练*它将输入与输出信号关联起来。
- en: Natural language processing (NLP), the subfield of AI we will be discussing
    in this book, deals with the analysis and processing of human natural language
    data and is one of the most active areas of AI research today.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是我们将在本书中讨论的人工智能的子领域，它涉及对人类自然语言数据的分析和处理，是当今人工智能研究中最活跃的领域之一。
- en: A recently popularized paradigm in NLP, transfer learning, enables you to adapt
    or transfer the knowledge acquired from one set of tasks or domains to a different
    set of tasks or domains. This is a big step forward for the democratization of
    NLP and, more widely, AI, allowing knowledge to be reused in new settings at a
    fraction of the previously required resources, which may not be available to everyone.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近年来在自然语言处理领域中流行的一种范式，迁移学习，使你能够将从一个任务或领域中获得的知识适应或迁移到另一个任务或领域。这对于自然语言处理的民主化以及更广泛地说是人工智能，是一个重要的进步，使得知识可以在新环境中以前所需资源的一小部分重新使用，而这些资源可能并不是所有人都能得到的。
- en: Key modeling frameworks enabling transfer learning in NLP include ELMo and BERT.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键的建模框架，使得在自然语言处理中实现迁移学习成为可能，包括 ELMo 和 BERT。
- en: The recent rise in the importance of social media has changed the definition
    of what is considered natural language. Now, billions of people express themselves
    online using emoticons, newly invented slang, and deliberately misspelled words.
    All these present new challenges, which we must take into account when developing
    new transfer learning techniques for NLP.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体重要性的近期上升改变了什么被认为是自然语言的定义。现在，数十亿人在网上使用表情符号、新创造的俚语和故意拼写错误的单词来表达自己。所有这些都提出了新的挑战，在开发新的自然语言处理迁移学习技术时我们必须考虑到这些挑战。
- en: Transfer learning is relatively well understood in computer vision, and whenever
    possible, we should draw on this body of knowledge when experimenting with new
    transfer techniques for NLP.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算机视觉中，迁移学习相对较为成熟，因此在尝试新的自然语言处理迁移技术时，我们应尽可能借鉴这一知识体系。
- en: '1. K. Schwab, The Fourth Industrial Revolution (Geneva: World Economic Forum,
    2016).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 1. K. Schwab，《第四次工业革命》（日内瓦：世界经济论坛，2016年）。
- en: '2. J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding,” arXiv (2018).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '2. J. Devlin 等人，“BERT: 深度双向转换器的预训练”，arXiv (2018)。'
- en: '3. F. Chollet, Deep Learning with Python (New York: Manning Publications, 2018).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 3. F. Chollet，《Python 深度学习》（纽约：Manning Publications，2018年）。
- en: 4. T. Mikolov et al., “Efficient Estimation of Word Representations in Vector
    Space,” *arXiv* (2013).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 4. T. Mikolov 等人，“词表示在向量空间中的高效估计”，*arXiv* (2013)。
- en: 5. M. Pagliardini et al., “Unsupervised Learning of Sentence Embeddings Using
    Compositional n-Gram Features,” Proc. of NAACL-HLT (2018).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 5. M. Pagliardini 等人，“使用组合 n-Gram 特征的句子嵌入的无监督学习”，NAACL-HLT 论文集 (2018)。
- en: 6. Q. V. Le et al., “Distributed Representations of Sentences and Documents,”
    *arXiv* (2014).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 6. Q. V. Le 等人，“句子和文档的分布式表示”，*arXiv* (2014)。
- en: 7. I. Sutskever et al., “Sequence to Sequence Learning with Neural Networks,”
    NeurIPS Proceedings (2014).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 7. I. Sutskever 等人，“序列到序列学习的神经网络”，NeurIPS 论文集 (2014)。
- en: 8. A. Vaswani et al., “Attention Is All You Need,” NeurIPS Proceedings (2017).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 8. A. Vaswani 等人，“注意力就是一切”，NeurIPS 论文集 (2017)。
- en: 9. X. Zhang et al., “Character-Level Convolutional Networks for Text Classification,”
    NeurIPS Proceedings (2015).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 9. X. Zhang 等人，“用于文本分类的字符级卷积网络”，NeurIPS 论文集 (2015)。
- en: 10. P. Azunre et al., “Semantic Classification of Tabular Datasets via Character-Level
    Convolutional Neural Networks,” arXiv (2019).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 10. P. Azunre 等人，“基于字符级卷积神经网络的表格数据集的语义分类”，arXiv (2019)。
- en: 11. M. E. Peters et al., “Deep Contextualized Word Representations,” Proc. of
    NAACL-HLT (2018).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 11. M. E. Peters 等人，“深层上下文化词表示”，NAACL-HLT 论文集 (2018)。
- en: 12. J. Howard et al., “Universal Language Model Fine-Tuning for Text Classification,”
    Proc. of the 56th Annual Meeting of the Association for Computational Linguistics
    (2018).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 12. J. Howard 等人，“通用语言模型微调用于文本分类”，第56届计算语言学年会论文集 (2018)。
- en: '13. J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding,” Proc. of NAACL-HLT (2019).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '13. J. Devlin 等人，“BERT: 深度双向转换器的预训练”，NAACL-HLT 论文集 (2019)。'
- en: '14. J. Deng et al., “ImageNet: A Large-Scale Hierarchical Image Database,”
    Proc. of NAACL-HLT (2019).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 14. J. Deng 等人，“ImageNet：一个大规模分层图像数据库”，NAACL-HLT 论文集 (2019)。
- en: '15. K. Schaffer, Data versus Democracy:- How Big Data Algorithms Shape Opinions
    and Alter the Course of History (New York: Apress, 2019).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 15. K. Schaffer，《数据与民主：大数据算法如何塑造观点并改变历史进程》（纽约：Apress，2019年）。
