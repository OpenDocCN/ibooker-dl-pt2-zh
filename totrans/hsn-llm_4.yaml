- en: Chapter 5\. Multimodal Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章\. 多模态大型语言模型
- en: When you think about Large Language Models (LLMs), multimodality might not be
    the first thing that comes to mind. After all, they are *Language*Models!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想到大型语言模型（LLMs）时，多模态可能不是首先想到的事情。毕竟，它们是*语言*模型！
- en: We have seen all manner of emerging behaviors rising from LLMs, from generalization
    capabilities and reasoning to arithmetic and linguistics. As models grow larger
    and smarter, so do their skill sets.^([1](ch05.html#id292))
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到各种新兴行为从LLMs中涌现，包括概括能力、推理、算术和语言学。随着模型变得越来越大和智能，它们的技能集也随之增加。^([1](ch05.html#id292))
- en: The ability to receive and reason with multimodal input might further increase
    and help emerge capabilities that were previously locked. In practice, Language
    does not solely live in a vacuum. As an example, your body language, facial expressions,
    intonation, etc. are all methods of communication that enhance the spoken word.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接收和推理多模态输入的能力可能进一步增强，并帮助涌现出以前被锁定的能力。实际上，语言并不单独存在于真空中。例如，你的肢体语言、面部表情、语调等都是增强口语交流的沟通方式。
- en: The same thing applies to Large Language Models, if we can enable them to reason
    about multimodal information, their capabilities might increase.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型同样适用，如果我们能够使它们对多模态信息进行推理，它们的能力可能会增强。
- en: In this chapter, we will explore a number of different LLMs that have multimodal
    capabilities and what that means for practical use cases. We will start by exploring
    how images are converted to numerical representations using an adaption of the
    original transformer technique. Then, we will show how LLMs can be extended to
    include vision tasks using this transformer.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一些具有多模态能力的不同LLMs，以及这对实际应用的意义。我们将首先探讨如何使用原始变换器技术的适应版将图像转换为数值表示。然后，我们将展示如何将LLMs扩展以包括视觉任务。
- en: Transformers for Vision
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉的变换器
- en: Throughout the chapters of this book, we have seen the success of using transformer-based
    models for a variety of language modeling tasks, from classification and clustering
    to search and generative modeling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的各章中，我们已经看到使用基于变换器的模型在各种语言建模任务中的成功，从分类和聚类到搜索和生成建模。
- en: So it might not be surprising that researchers have been looking at a way to
    generalize some of the transformer’s success to the field of computer vision.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员一直在寻找将某些变换器的成功推广到计算机视觉领域的方法，这一点可能并不令人惊讶。
- en: The method they came up with is called the Vision Transformer (ViT) which has
    been shown to do tremendously well on image recognition tasks compared to the
    previously default Convolutional Neural Networks (CNNs).^([2](ch05.html#id293))
    Like the original transformer, ViT is used to transform unstructured data, an
    image, into representations that can be used for a variety of tasks, like classification
    as illustrated in [Figure 5-1](#fig_1_both_the_original_transformer_as_well_as_the_visio).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出的方法被称为视觉变换器（ViT），与之前的默认卷积神经网络（CNNs）相比，它在图像识别任务上表现非常出色。^([2](ch05.html#id293))
    像原始变换器一样，ViT用于将非结构化数据（图像）转换为可用于各种任务的表示，例如分类，如[图 5-1](#fig_1_both_the_original_transformer_as_well_as_the_visio)所示。
- en: '![Both the original transformer as well as the vision transformer take unstructured
    data  convert it to numerical representations  and finally use that for tasks
    like classification. ](assets/multimodal_large_language_models_406548_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![原始变换器和视觉变换器都处理非结构化数据，将其转换为数值表示，最终用于分类等任务。](assets/multimodal_large_language_models_406548_01.png)'
- en: Figure 5-1\. Both the original transformer as well as the vision transformer
    take unstructured data, convert it to numerical representations, and finally use
    that for tasks like classification.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 原始变换器和视觉变换器都处理非结构化数据，将其转换为数值表示，最终用于分类等任务。
- en: ViT relies on an important component of the transformer architecture, namely
    the encoder. As we saw in Chapter 1, the encoder is responsible for converting
    textual input into numerical representations before being passed to the decoder.
    However, before the encoder can perform its duties, the textual input needs to
    be tokenized first as is illustrated in [Figure 5-2](#fig_2_text_is_passed_to_one_or_multiple_encoders_by_firs).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ViT依赖于变换器架构中的一个重要组成部分，即编码器。正如我们在第一章看到的，编码器负责将文本输入转换为数字表示，然后传递给解码器。然而，在编码器执行其职能之前，文本输入需要首先进行标记化，如[图5-2](#fig_2_text_is_passed_to_one_or_multiple_encoders_by_firs)所示。
- en: '![Text is passed to one or multiple encoders by first tokenizing it using a
    tokenizer.](assets/multimodal_large_language_models_406548_02.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![文本通过使用标记器首先进行标记化，然后传递给一个或多个编码器。](assets/multimodal_large_language_models_406548_02.png)'
- en: Figure 5-2\. Text is passed to one or multiple encoders by first tokenizing
    it using a tokenizer.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 文本通过使用标记器首先进行标记化，然后传递给一个或多个编码器。
- en: Since an image does not consist of words this tokenization process cannot be
    used for visual data. Instead, the authors of ViT came up with a method for tokenizing
    images into “words” which allowed them to use the original encoder structure.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像不是由单词组成，因此这个标记化过程不能用于视觉数据。相反，ViT的作者提出了一种将图像标记为“单词”的方法，这使他们能够使用原始的编码器结构。
- en: Imagine that you have an image of a cat. This image is represented by a number
    of pixels, let’s say 512 by 512 pixels. Each individual pixel does not convey
    much information but when you combine patches of pixels, you slowly start to see
    more information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你有一张猫的图片。这张图片由许多像素表示，假设为512x512像素。每个单独的像素并没有传达太多信息，但当你组合像素块时，你会逐渐看到更多的信息。
- en: ViT uses a principle much like that. Instead of splitting text up into tokens,
    it converts the original image into patches of images. In other words, it cuts
    the image into a number of pieces horizontally and vertically as illustrated in
    [Figure 5-3](#fig_3_the_tokenization_process_for_image_input_it_con).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ViT使用的原理与此类似。它不是将文本拆分成标记，而是将原始图像转换为图像块。换句话说，它将图像水平和垂直切割成多个部分，如[图5-3](#fig_3_the_tokenization_process_for_image_input_it_con)所示。
- en: '![The  tokenization  process for image input. It converts an image into patches
    of sub images. ](assets/multimodal_large_language_models_406548_03.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图像输入的标记化过程。它将图像转换为子图像的块。](assets/multimodal_large_language_models_406548_03.png)'
- en: Figure 5-3\. The “tokenization” process for image input. It converts an image
    into patches of sub-images.
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3\. 图像输入的“标记化”过程。它将图像转换为子图像的块。
- en: Just like we are converting text into tokens of text, we are converting an image
    into patches of images. The flattened input of image patches can be thought of
    as the tokens in a piece of text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们将文本转换为文本标记一样，我们也将图像转换为图像块。图像块的扁平化输入可以被视为文本中的标记。
- en: However, unlike tokens, we cannot just assign each patch with an ID since these
    patches will rarely be found in other images, unlike the vocabulary of a text.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与标记不同，我们不能仅仅给每个块分配一个ID，因为这些块在其他图像中很少出现，这与文本的词汇不同。
- en: Instead, the patches are linearly embedded to create numerical representations,
    namely embeddings. These can then be used as the input of a transformer model.
    That way, the patches of images are treated the same way as tokens. The full process
    is illustrated in [Figure 5-4](#fig_4_the_main_algorithm_behind_vit_after_patching_the).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，块被线性嵌入以创建数字表示，即嵌入。这些可以用作变换器模型的输入。这样，图像块被视为标记。整个过程如[图5-4](#fig_4_the_main_algorithm_behind_vit_after_patching_the)所示。
- en: '![The main algorithm behind ViT. After patching the images and linearly projecting
    them  the patch embeddings are passed to the encoder and treated as if they were
    textual tokens. ](assets/multimodal_large_language_models_406548_04.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![ViT背后的主要算法。在对图像进行分块和线性投影后，块嵌入被传递给编码器，并被视为文本标记。](assets/multimodal_large_language_models_406548_04.png)'
- en: Figure 5-4\. The main algorithm behind ViT. After patching the images and linearly
    projecting them, the patch embeddings are passed to the encoder and treated as
    if they were textual tokens.
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4\. ViT背后的主要算法。在对图像进行分块和线性投影后，块嵌入被传递给编码器，并被视为文本标记。
- en: For illustrative purposes, the images in the examples were patched into 3 by
    3 patches but the original implementation used 16 by 16 patches. After all, the
    paper is called “An image is worth 16x16 words”.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，示例中的图像被分割成3x3的补丁，但原始实现使用的是16x16的补丁。毕竟，这篇论文的标题是“图像价值16x16个词”。
- en: What is so interesting about this approach is that the moment the embeddings
    are passed to the encoder, they are treated as if they were textual tokens. From
    that point forward, there is no difference in how a textual or image trains and
    their outputs
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有趣之处在于，当嵌入被传递给编码器时，它们被视为文本标记。从那时起，文本和图像的训练及其输出没有区别。
- en: Due to their similarities, the ViT is often used to make all kinds of language
    models multimodal. One of the most straightforward ways to use them is during
    the training of embedding models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们的相似性，ViT通常用于使各种语言模型实现多模态。其中一种最简单的方法是在嵌入模型的训练过程中使用它们。
- en: Multimodal Embedding Models
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态嵌入模型
- en: In previous chapters, like Chapters X, X, and X, we used embedding models to
    capture the semantic content of textual representations, such as books and documents.
    We saw that we could use these embeddings or numerical representations to find
    similar documents, apply classification tasks, and even perform topic modeling.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，如章节X、X和X，我们使用嵌入模型来捕捉文本表示的语义内容，例如书籍和文档。我们看到可以利用这些嵌入或数值表示找到相似文档，应用分类任务，甚至进行主题建模。
- en: As we have seen many times before, embeddings often are an important driver
    behind LLM applications. They are an efficient method for capturing large-scale
    information and searching for the needle in the haystack of information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前多次看到的，嵌入通常是LLM应用程序的重要驱动因素。它们是一种有效的方法，可以捕获大规模信息并在信息的海洋中寻找针。
- en: That said, we have only looked at monomodal embedding models thus far. Embedding
    models that only focus on generating embeddings for textual representations. Although
    embedding models exist for solely embedding imagery, we will look at embedding
    models that can capture both textual as well as vision representations. We illustrate
    this in [Figure 5-5](#fig_5_multimodal_embedding_models_can_create_embeddings).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们到目前为止只关注了单模态嵌入模型。嵌入模型仅专注于生成文本表示的嵌入。虽然存在仅用于嵌入图像的嵌入模型，但我们将关注能够同时捕捉文本和视觉表示的嵌入模型。我们在[图5-5](#fig_5_multimodal_embedding_models_can_create_embeddings)中对此进行了说明。
- en: '![Multimodal embedding models can create embeddings for multiple modalities
    in the same vector space. ](assets/multimodal_large_language_models_406548_05.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![多模态嵌入模型可以在同一向量空间中为多种模态创建嵌入。](assets/multimodal_large_language_models_406548_05.png)'
- en: Figure 5-5\. Multimodal embedding models can create embeddings for multiple
    modalities in the same vector space.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5\. 多模态嵌入模型可以在同一向量空间中为多种模态创建嵌入。
- en: A big advantage is that it allows for comparing multimodal representations since
    the resulting embeddings lie in the same vector space, as illustrated in [Figure 5-6](#fig_6_tmultimodal_embedding_models_can_create_embeddings).
    For instance, using such a multimodal embedding model, we can find images based
    on input text. What images would we find if we search for images similar to “pictures
    of a puppy”? Vice versa would also be possible. Which documents are best related
    to this question?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大优势是，它允许比较多模态表示，因为结果嵌入位于同一向量空间中，如[图5-6](#fig_6_tmultimodal_embedding_models_can_create_embeddings)所示。例如，使用这样的多模态嵌入模型，我们可以根据输入文本找到图像。如果我们搜索与“狗狗的照片”相似的图像，会找到什么？反之亦然，哪些文档与这个问题最相关？
- en: '![tMultimodal embedding models can create embeddings for multiple modalities
    in the same vector space. ](assets/multimodal_large_language_models_406548_06.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![多模态嵌入模型可以在同一向量空间中为多种模态创建嵌入。](assets/multimodal_large_language_models_406548_06.png)'
- en: Figure 5-6\. Multimodal embedding models can create embeddings for multiple
    modalities in the same vector space.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 多模态嵌入模型可以在同一向量空间中为多种模态创建嵌入。
- en: There are a number of multimodal embedding models out there but the most well-known
    and currently most-used model is CLIP (Contrastive Language-Image Pre-Training).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多多模态嵌入模型，但最著名且目前使用最广泛的模型是CLIP（对比语言-图像预训练）。
- en: 'CLIP: Connecting Text and Images'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIP：连接文本和图像
- en: CLIP is an embedding model that can compute embeddings of both images and texts.
    The resulting embeddings lie in the same vector space which means that the embeddings
    of images can be compared with the embeddings of text.^([3](ch05.html#id294))
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP是一个嵌入模型，可以计算图像和文本的嵌入。生成的嵌入位于同一向量空间，这意味着图像的嵌入可以与文本的嵌入进行比较。^([3](ch05.html#id294))
- en: 'This capability of comparison makes CLIP, and similar models, usable for tasks
    such as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种比较能力使得CLIP及类似模型可用于诸如以下任务：
- en: Zeroshot classification
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本分类
- en: We can compare the embedding of an image with that of the description of its
    possible classes to find which class is most similar
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将图像的嵌入与其可能类别的描述进行比较，以找出哪个类别最相似。
- en: Clustering
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类
- en: Cluster both images and a collection of keywords to find which keywords belong
    to which sets of images
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像和一组关键词进行聚类，以找出哪些关键词属于哪些图像集合。
- en: Search
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索
- en: Across billions of texts or images, we can quickly find what relates to an input
    text or image
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在数十亿的文本或图像中，我们可以快速找到与输入文本或图像相关的内容。
- en: Generation
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 生成
- en: Use multimodal embeddings to drive the generation of images (e.g., stable diffusion)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多模态嵌入来驱动图像生成（例如，稳定扩散）。
- en: How can CLIP generate multimodal embeddings?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CLIP如何生成多模态嵌入？
- en: The procedure of CLIP is actually quite straightforward. Imagine that you have
    a dataset with millions of images alongside captions as we illustrate in [Figure 5-7](#fig_7_the_type_of_data_that_is_needed_to_train_a_multimo).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的过程实际上相当简单。想象一下，你有一个包含数百万张图片及其说明的数据集，如我们在[图5-7](#fig_7_the_type_of_data_that_is_needed_to_train_a_multimo)中所示。
- en: '**![The type of data that is needed to train a multimodal embedding model.
    ](assets/multimodal_large_language_models_406548_07.png)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**![训练多模态嵌入模型所需的数据类型。](assets/multimodal_large_language_models_406548_07.png)'
- en: Figure 5-7\. The type of data that is needed to train a multimodal embedding
    model.**  **This dataset can be used to create two representations for each pair,
    the image and its caption. To do so, CLIP uses a text encoder to embed text and
    an image encoder to embed images. As is shown in [Figure 5-8](#fig_8_in_the_first_step_of_training_clip_both_images_an),
    the result is an embedding for both the image and its corresponding caption.
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7。训练多模态嵌入模型所需的数据类型。**  **这个数据集可以为每对数据创建两个表示，即图像及其说明。为此，CLIP使用文本编码器来嵌入文本，使用图像编码器来嵌入图像。如[图5-8](#fig_8_in_the_first_step_of_training_clip_both_images_an)所示，结果是图像及其对应说明的嵌入。
- en: '![In the first step of training CLIP  both images and text are embedded using
    an image and text encoder respectively. ](assets/multimodal_large_language_models_406548_08.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![在训练CLIP的第一步中，图像和文本分别使用图像编码器和文本编码器进行嵌入。](assets/multimodal_large_language_models_406548_08.png)'
- en: Figure 5-8\. In the first step of training CLIP, both images and text are embedded
    using an image and text encoder respectively.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8。在训练CLIP的第一步中，图像和文本分别使用图像编码器和文本编码器进行嵌入。
- en: The pair of embeddings that are generated are compared through cosine similarity.
    As we saw in Chapter 2, cosine similarity is the cosine of the angle between vectors
    which is calculated through the dot product of the embeddings and divided by the
    product of their lengths.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的嵌入对通过余弦相似度进行比较。正如我们在第二章中看到的，余弦相似度是向量之间角度的余弦，通过嵌入的点积计算，并除以它们长度的乘积。
- en: When we start training, the similarity between the image embedding and text
    embedding will be low as they are not yet optimized to be within the same vector
    space. During training, we optimize for the similarity between the embeddings
    and want to maximize them for similar image/caption pairs and minimize them for
    dissimilar image/caption pairs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始训练时，图像嵌入和文本嵌入之间的相似度较低，因为它们尚未优化到同一向量空间。在训练过程中，我们优化嵌入之间的相似度，希望对相似的图像/说明对最大化，而对不相似的图像/说明对最小化。
- en: '**![In the second step of training CLIP  the similarity between the sentence
    and image embedding is calculated using cosine similarity. ](assets/multimodal_large_language_models_406548_09.png)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**![在训练CLIP的第二步中，句子和图像嵌入之间的相似度使用余弦相似度进行计算。](assets/multimodal_large_language_models_406548_09.png)'
- en: Figure 5-9\. In the second step of training CLIP, the similarity between the
    sentence and image embedding is calculated using cosine similarity.**  **After
    calculating their similarity, the model is updated and the process starts again
    with new batches of data and updated representations. This method is called contrastive
    learning and we will go in-depth into its inner workings in Chapter 13 where we
    will create our own embedding model.
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 在训练 CLIP 的第二步中，句子和图像嵌入之间的相似性使用余弦相似度进行计算。**  **在计算它们的相似性后，模型会更新，流程再次开始，使用新的数据批次和更新后的表示。此方法称为对比学习，我们将在第13章深入探讨其内部工作原理，并创建自己的嵌入模型。
- en: '![In the third step of training CLIP  the text and image encoders are updated
    to match what the intended similarity should be. This updates the embeddings such
    that they are closer in vector space if the inputs are similar. ](assets/multimodal_large_language_models_406548_10.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![在训练 CLIP 的第三步中，文本和图像编码器会更新，以匹配预期的相似性。这会更新嵌入，使其在向量空间中更接近，如果输入相似的话。](assets/multimodal_large_language_models_406548_10.png)'
- en: Figure 5-10\. In the third step of training CLIP, the text and image encoders
    are updated to match what the intended similarity should be. This updates the
    embeddings such that they are closer in vector space if the inputs are similar.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. 在训练 CLIP 的第三步中，文本和图像编码器会更新，以匹配预期的相似性。这会更新嵌入，使其在向量空间中更接近，如果输入相似的话。
- en: Eventually, we expect the embedding of an image of a cat would be similar to
    the embedding of the sentence “a picture of a cat”. As we will see in Chapter
    13, to make sure the representations are as accurate as possible, negative examples
    of images and captions that are not related should also be included in the training
    process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们期望猫的图像的嵌入与句子“猫的图片”的嵌入相似。正如我们将在第13章看到的，为确保表示尽可能准确，训练过程中还应包括与之无关的图像和标题的负例。
- en: Modeling similarity is not only knowing what makes things similar to one another
    but also what makes them different and dissimilar.****  ****### OpenCLIP
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 建模相似性不仅是了解事物之间相似的原因，也包括它们不同和不相似的原因。****  ****### OpenCLIP
- en: For this example, we are going to be using models from the open-source variant
    of CLIP, namely OpenCLIP ([https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用开源版本的 CLIP 模型，即 OpenCLIP（[https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)）。
- en: Using OpenCLIP, or any CLIP model, boils down to two things, processing the
    textual and image inputs before passing them to the main model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OpenCLIP 或任何 CLIP 模型，归结为两件事：在将文本和图像输入传递给主模型之前，先对其进行处理。
- en: 'Before doing so, let’s take a look at a small example where we will be using
    one of the images we have seen before. Namely, an AI-generated image (though stable-diffusion)
    of a puppy playing in the snow as illustrated in [Figure 5-11](#fig_11_an_ai_generated_image_of_a_puppy_playing_in_the_sn):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，让我们看看一个小示例，我们将使用之前看到的其中一张图像。即，一张 AI 生成的图像（通过稳定扩散）显示一只小狗在雪地里玩耍，如[图 5-11](#fig_11_an_ai_generated_image_of_a_puppy_playing_in_the_sn)所示：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![An AI generated image of a puppy playing in the snow.](assets/multimodal_large_language_models_406548_11.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![一只小狗在雪地里玩耍的 AI 生成图像。](assets/multimodal_large_language_models_406548_11.png)'
- en: Figure 5-11\. An AI-generated image of a puppy playing in the snow.
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-11\. 一只小狗在雪地里玩耍的 AI 生成图像。
- en: Since we have a caption for this image, we can use OpenCLIP to generate embeddings
    for both.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有这张图像的标题，我们可以使用 OpenCLIP 为两者生成嵌入。
- en: 'To do so, we load in three models:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们加载三个模型：
- en: A tokenizer for tokenizing the textual input
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于对文本输入进行分词的分词器。
- en: A preprocessor to preprocess and resize the image
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个预处理器，用于预处理和调整图像大小。
- en: The main model that converts the previous outputs to embeddings
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将之前的输出转换为嵌入的主模型。
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After having loaded in the models, preprocessing our input is straightforward.
    Let’s start with the tokenizer and see what happens if we preprocess our input:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载模型后，对输入进行预处理是简单明了的。让我们从分词器开始，看看预处理输入时会发生什么：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our input text has been converted to input ids. To see what those represent,
    let’s convert them to tokens:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入文本已被转换为输入 ID。为了查看它们所表示的内容，让我们将其转换为标记：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we often have seen before, the text is split up into tokens. Additionally,
    we now also see that the start and end of the text is indicated to separate it
    from a potential image embedding. You might also notice that the `[CLS]` token
    is missing. In CLIP, the `[CLS]` token is actually used to represent the image
    embedding.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们以前经常看到的，文本被拆分成了多个标记。此外，我们现在还可以看到，文本的开始和结束被标记出来，以将其与潜在的图像嵌入分开。你可能还会注意到`[CLS]`标记缺失。在CLIP中，`[CLS]`标记实际上用于表示图像嵌入。
- en: 'Now that we have preprocessed our caption, next up is to create the embedding:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预处理了标题，接下来是创建嵌入：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Before we can create our image embedding, like the text embedding, we will need
    to preprocess it as the model expects the input image to have certain characteristics,
    like its size and shape.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建图像嵌入之前，与文本嵌入一样，我们需要对其进行预处理，因为模型期望输入图像具有某些特征，如大小和形状。
- en: 'To do so, we can use the processor that we created before:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用之前创建的处理器：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The original image was 512 by 512 pixels. Notice that the preprocessing of this
    image reduced its size to 224 by 224 pixels as that is its expected size.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像为512×512像素。注意，这张图像的预处理将其大小减少到224×224像素，因为这是其预期的大小。
- en: 'Let’s visualize, in [Figure 5-12](#fig_12_the_preprocessed_input_image_by_clip),
    the preprocessed image to see what it actually is doing:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在[图5-12](#fig_12_the_preprocessed_input_image_by_clip)中可视化处理过的图像，以查看它实际上在做什么：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![The preprocessed input image by CLIP.](assets/multimodal_large_language_models_406548_12.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![CLIP处理过的输入图像。](assets/multimodal_large_language_models_406548_12.png)'
- en: Figure 5-12\. The preprocessed input image by CLIP.
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-12\. CLIP处理过的输入图像。
- en: 'To convert this preprocessed image into embeddings, we can call the `model`
    as we did before:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个预处理过的图像转换为嵌入，我们可以像之前那样调用`model`：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that the shape of the resulting image embedding is exactly the same as
    that of the text embedding. This is important as it allows us to compare their
    embeddings and see whether they actually are similar.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成的图像嵌入的形状与文本嵌入的形状完全相同。这一点很重要，因为它使我们能够比较它们的嵌入并查看它们是否确实相似。
- en: 'We can use these embeddings to calculate the probability that the caption belongs
    to the image by calculating their dot product and taking the softmax:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些嵌入来计算标题属于图像的概率，通过计算它们的点积并进行softmax：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It gives us back a score of 1 indicating that the model is certain that the
    caption belongs to the image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它给我们返回了1的得分，表明模型确定该标题属于该图像。
- en: 'We can extend this example by calculating the similarity between the embeddings.
    By normalizing the embeddings first before calculating the dot product, we get
    a value that lies between 0 and 1:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算嵌入之间的相似性来扩展这个例子。通过在计算点积之前先对嵌入进行归一化，我们得到一个介于0和1之间的值：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We get a similarity score of 0.33 which is difficult to interpret considering
    we do not know what the model considers a low versus a high similarity score.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的相似性得分是0.33，这很难解释，因为我们不知道模型认为低相似性和高相似性得分的标准是什么。
- en: Instead, let’s extend the example with more images and captions as illustrated
    in [Figure 5-13](#fig_13_the_similarity_matrix_between_three_images_and_thr).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们通过在[图5-13](#fig_13_the_similarity_matrix_between_three_images_and_thr)中展示更多图像和标题来扩展这个例子：
- en: '![The similarity matrix between three images and three captions.](assets/multimodal_large_language_models_406548_13.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![三幅图像与三条标题之间的相似性矩阵。](assets/multimodal_large_language_models_406548_13.png)'
- en: Figure 5-13\. The similarity matrix between three images and three captions.
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-13\. 三幅图像与三条标题之间的相似性矩阵。
- en: It seems that a score of 0.33 is indeed high considering the similarities with
    other images are quite a bit lower.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到其他图像的相似性要低得多，0.33的得分似乎确实很高。
- en: Tip
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'In sentence-transformers, there are a few CLIP-based models implemented that
    make it much easier to create embeddings. It only takes a few lines of code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在sentence-transformers中，有几个基于CLIP的模型实现，使得创建嵌入变得更加容易。只需几行代码：
- en: '[PRE10]****  ****# Making Text Generation Models Multimodal'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE10]****  ****# 使文本生成模型多模态化'
- en: Traditionally, text generation models have been, as you might expect, models
    that interpret textual representations. Models like Llama 2 and ChatGPT excel
    at reasoning about textual information and responding with natural language.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，文本生成模型，如你所期望的，都是解释文本表示的模型。像Llama 2和ChatGPT这样的模型在推理文本信息和用自然语言响应方面表现出色。
- en: They are, however, limited to the modality they were trained in, namely text.
    As we have seen before with multimodal embedding models, the addition of vision
    can enhance the capabilities of a model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们限于所训练的模态，即文本。正如我们之前看到的多模态嵌入模型，添加视觉可以增强模型的能力。
- en: In the case of text generation models, we would like it to reason about certain
    input images. For example, we could give it an image of a pizza and ask it what
    ingredients it contains. You could show it a picture of the Eiffel Tower and ask
    it when it was built or where it is located. This conversational ability is further
    illustrated in [Figure 5-14](#fig_14_a_multimodal_text_generation_model_that_can_reason).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本生成模型的情况下，我们希望它能够对某些输入图像进行推理。例如，我们可以给它一张披萨的图片，问它包含哪些配料。你可以向它展示一张埃菲尔铁塔的照片，问它建于何时或位于何处。这种对话能力在[图5-14](#fig_14_a_multimodal_text_generation_model_that_can_reason)中进一步说明。
- en: '![A multimodal text generation model that can reason about input images. ](assets/multimodal_large_language_models_406548_14.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![一个可以对输入图像进行推理的多模态文本生成模型。](assets/multimodal_large_language_models_406548_14.png)'
- en: Figure 5-14\. A multimodal text generation model that can reason about input
    images.
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-14\. 一个可以对输入图像进行推理的多模态文本生成模型。
- en: 'To bridge the gap between these two domains, attempts have been made to introduce
    a form of multimodality to existing models. One such method is called BLIP-2:
    *Bootstrapping Language Image Pre-training for unified vision-language understanding
    and generation 2*. BLIP-2 introduces an easy-to-use and modular technique that
    allows for introducing vision capabilities to existing language models.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥合这两个领域之间的差距，已经尝试向现有模型引入一种多模态形式。其中一种方法被称为BLIP-2：*用于统一视觉-语言理解和生成的引导语言图像预训练2*。BLIP-2引入了一种易于使用的模块化技术，允许将视觉能力引入现有语言模型。
- en: 'BLIP-2: Bridging the Modality Gap'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLIP-2：弥合模态差距
- en: Creating a multimodal language model from scratch requires significant computing
    power and data. We would have to use billions of images, text, and image-text
    pairs to create such a model. As you can imagine, this is not easily feasible!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始创建一个多模态语言模型需要大量的计算能力和数据。我们必须使用数十亿张图像、文本和图像-文本对来创建这样的模型。可以想象，这并不容易实现！
- en: Instead of building the architecture from scratch, BLIP-2 bridges the vision-language
    gap by building a bridge, named the Q-former, that connects a pre-trained image
    encoder and a pre-trained LLM.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP-2通过构建一座名为Q-former的桥梁，连接一个预训练的图像编码器和一个预训练的LLM，从而弥合视觉-语言之间的差距，而不是从头构建架构。
- en: By leveraging pre-trained models, BLIP-2 only needs to train the bridge without
    needing to train the image encoder and LLM from scratch. It makes great use of
    the technology and models that are already out there! This bridge is illustrated
    in [Figure 5-15](#fig_15_the_querying_transformer_is_the_bridge_between_vis).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用预训练模型，BLIP-2只需训练这座桥，而无需从头开始训练图像编码器和LLM。它充分利用了现有的技术和模型！这座桥在[图5-15](#fig_15_the_querying_transformer_is_the_bridge_between_vis)中有所示。
- en: '![The Querying Transformer is the bridge between vision  ViT  and text  LLM  which
    is the only trainable component of the pipeline.](assets/multimodal_large_language_models_406548_15.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![查询变换器是视觉ViT和文本LLM之间的桥梁，是管道中唯一可训练的组件。](assets/multimodal_large_language_models_406548_15.png)'
- en: Figure 5-15\. The Querying Transformer is the bridge between vision (ViT) and
    text (LLM) which is the only trainable component of the pipeline.
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-15\. 查询变换器是视觉（ViT）和文本（LLM）之间的桥梁，是管道中唯一可训练的组件。
- en: 'To connect the two pre-trained models, the Q-Former, also known as the Querying
    Transformer, mimics their architectures. It has two modules that share their attention
    layers:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了连接这两个预训练模型，Q-Former，也称为查询变换器，模仿了它们的架构。它有两个共享注意力层的模块：
- en: An image transformer to interact with the frozen vision transformer for feature
    extraction
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个图像变换器，与冻结的视觉变换器进行特征提取互动
- en: A text transformer that can interact with the LLM
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以与LLM互动的文本变换器
- en: The Q-Former is trained in two stages, one for each modality as illustrated
    in [Figure 5-16](#fig_16_in_step_1_representation_learning_is_applied_to_l).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Former分两个阶段进行训练，每个阶段针对一种模态，如[图5-16](#fig_16_in_step_1_representation_learning_is_applied_to_l)所示。
- en: '![In step 1  representation learning is applied to learn representations for
    vision and language simultaneously. In step 2  these representations are converted
    to soft visual prompts to feed the LLM. ](assets/multimodal_large_language_models_406548_16.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![在步骤1中，表示学习用于同时学习视觉和语言的表示。在步骤2中，这些表示被转换为软视觉提示，以供LLM使用。](assets/multimodal_large_language_models_406548_16.png)'
- en: Figure 5-16\. In step 1, representation learning is applied to learn representations
    for vision and language simultaneously. In step 2, these representations are converted
    to soft visual prompts to feed the LLM.
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-16。在步骤1中，表示学习用于同时学习视觉和语言的表示。在步骤2中，这些表示被转换为软视觉提示，以供LLM使用。
- en: In step 1, a number of image-document pairs are used to train the Q-Former to
    represent both images and text. These pairs are generally captions of images,
    as we have seen before with training CLIP.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，使用多个图像-文档对来训练Q-Former，以表示图像和文本。这些对通常是图像的说明，正如我们之前在训练CLIP时看到的那样。
- en: The images are fed to the frozen vision transformer to extract vision embeddings.
    These embeddings are used as the input of Q-Former’s vision transformer. The captions
    are used as the input of Q-Former’s text transformer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被馈送到冻结的视觉变换器以提取视觉嵌入。这些嵌入作为Q-Former视觉变换器的输入，说明作为Q-Former文本变换器的输入。
- en: 'With these inputs, the Q-Former is then trained on three tasks:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些输入，Q-Former随后在三个任务上进行训练：
- en: Image-Text Contrastive Learning
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像-文本对比学习
- en: Image-Text Matching
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像-文本匹配
- en: Image-grounded Text Generation
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于图像的文本生成
- en: These three objectives are jointly optimized to improve the visual representations
    that are extracted from the frozen vision transformer. In a way, we are trying
    to inject textual information into the embeddings of the frozen vision transformer
    so that we can use them in the LLM. This first step of BLIP-2 is illustrated in
    [Figure 5-17](#fig_17_in_step_1_the_output_of_the_frozen_vision_transfo).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个目标被联合优化，以改善从冻结视觉变换器提取的视觉表示。在某种程度上，我们试图将文本信息注入冻结视觉变换器的嵌入中，以便可以在LLM中使用它们。BLIP-2的第一步在[图5-17](#fig_17_in_step_1_the_output_of_the_frozen_vision_transfo)中进行了说明。
- en: '![In step 1  the output of the frozen vision transformer is used together with
    its caption and trained on three contrastive like tasks to learn visual text representations.](assets/multimodal_large_language_models_406548_17.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![在步骤1中，冻结视觉变换器的输出与其说明一起使用，并在三个对比性任务上进行训练，以学习视觉文本表示。](assets/multimodal_large_language_models_406548_17.png)'
- en: Figure 5-17\. In step 1, the output of the frozen vision transformer is used
    together with its caption and trained on three contrastive-like tasks to learn
    visual-text representations.
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-17。在步骤1中，冻结的视觉变换器的输出与其说明一起使用，并在三个对比性任务上进行训练，以学习视觉-文本表示。
- en: In step 2, the learnable embeddings derived from step 1 now contain visual information
    in the same dimensional space as its corresponding textual information.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，来自步骤1的可学习嵌入现在在与其相应的文本信息相同的维度空间中包含视觉信息。
- en: The learnable embeddings are then passed to the LLM as a soft prompt. In a way,
    these embeddings contain textual representations of the input image.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将可学习的嵌入作为软提示传递给LLM。在某种程度上，这些嵌入包含输入图像的文本表示。
- en: The learnable embeddings are then passed to the LLM. In a way, these embeddings
    serve as soft visual prompts that condition the LLM on the visual representations
    that were extracted by the Q-Former.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将可学习的嵌入传递给LLM。在某种程度上，这些嵌入作为软视觉提示，使LLM基于Q-Former提取的视觉表示进行条件化。
- en: There is also a fully connected linear layer in between them to make sure that
    the learnable embeddings have the same shape as the LLM expects. This second step
    of converting vision to language is represented in [Figure 5-18](#fig_18_in_step_2_the_learned_embeddings_from_the_q_forme).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它们之间还有一个全连接线性层，以确保可学习的嵌入具有LLM期望的相同形状。将视觉转换为语言的第二步在[图5-18](#fig_18_in_step_2_the_learned_embeddings_from_the_q_forme)中表示。
- en: '![In step 2  the learned embeddings from the Q Former are passed to the LLM
    through a projection layer. The projected embeddings serve as a soft visual prompt.](assets/multimodal_large_language_models_406548_18.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![在步骤2中，来自Q-Former的学习嵌入通过投影层传递给LLM。投影后的嵌入作为软视觉提示。](assets/multimodal_large_language_models_406548_18.png)'
- en: Figure 5-18\. In step 2, the learned embeddings from the Q-Former are passed
    to the LLM through a projection layer. The projected embeddings serve as a soft
    visual prompt.
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-18。在第2步中，Q-Former学习到的嵌入通过投影层传递给LLM。投影嵌入作为软视觉提示。
- en: When we put these steps together, they make it possible for the Q-Former to
    learn visual and textual representations in the same dimensional space which can
    be used as a soft prompt to the LLM. As a result, the LLM will be given information
    about the image and is similar to the context you would provide an LLM when prompting.
    The full in-depth process is illustrated in [Figure 5-19](#fig_19_the_full_procedure_of_blip_2).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些步骤结合起来时，它们使得Q-Former能够在相同的维度空间中学习视觉和文本表示，这可以作为对LLM的软提示。因此，LLM将获得关于图像的信息，类似于你在提示LLM时提供的上下文。完整的深入过程在[图5-19](#fig_19_the_full_procedure_of_blip_2)中进行了说明。
- en: '![The full procedure of BLIP 2.](assets/multimodal_large_language_models_406548_19.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![BLIP 2的完整过程。](assets/multimodal_large_language_models_406548_19.png)'
- en: Figure 5-19\. The full procedure of BLIP-2.
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-19。BLIP-2的完整过程。
- en: Preprocessing Multimodal Inputs
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理多模态输入
- en: Now that we know how BLIP-2 is created, there are a number of interesting use
    cases for which you can use such a model. Not limited to captioning images, answering
    visual questions, and even performing prompting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了BLIP-2是如何创建的，有许多有趣的用例可以使用这样的模型。这不仅限于对图像进行注释、回答视觉问题，甚至进行提示。
- en: 'Before we go through some use cases, let’s first load the model and explore
    how you can use it:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论一些用例之前，让我们先加载模型并探索如何使用它：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Using `model.vision_model` and `model.language_model` we can see which vision
    transformer and large language model are respectively used in the BLIP-2 model
    that we loaded.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`model.vision_model`和`model.language_model`，我们可以看到在我们加载的BLIP-2模型中，分别使用了哪个视觉变换器和大型语言模型。
- en: We loaded two components that make up our full pipeline, a `processor` and a
    `model`. The `processor` can be compared to the tokenizer of language models.
    It converts unstructured input, such as images and text, to representations that
    the model generally expects.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了组成完整管道的两个组件，一个是`processor`，另一个是`model`。`processor`可以与语言模型的分词器进行比较。它将非结构化输入（如图像和文本）转换为模型通常期望的表示形式。
- en: Preprocessing Images
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理图像
- en: 'Let’s start by exploring what the `processor` does to images. We start by loading
    the picture of a very wide image for illustration purposes:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来探讨一下`processor`对图像的处理。我们首先加载一张非常宽的图像用于说明：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Caption to come](assets/multimodal_large_language_models_406548_20.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![待添加说明](assets/multimodal_large_language_models_406548_20.png)'
- en: Figure 5-20\. Caption to come
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-20。待添加说明
- en: The image has 520 by 492 pixels which is generally an unusual format. So let’s
    see what our processor does to it.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的尺寸为520乘492像素，这通常是一种不寻常的格式。那么让我们看看我们的`processor`对它做了什么。
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When we check its shape after converting it to Numpy, it shows us an additional
    dimension that is of size 3\. This represents the RGB coding of each pixel, namely
    its color.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将其转换为Numpy格式后检查形状时，显示出一个额外的维度，其大小为3。这代表每个像素的RGB编码，即其颜色。
- en: 'Next, we pass the original image to the processor so that the image can be
    processed to the shape the model expects:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将原始图像传递给处理器，以便图像可以被处理成模型所期望的形状：
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result is a 224 by 224 sized image. Quite a bit smaller than we initially
    had! This also means that all different shapes of images will be processed into
    squares. So be careful inputting very wide or tall images as they might get distorted.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个224乘224像素的图像。比我们最初的图像要小得多！这也意味着所有不同形状的图像都将被处理为正方形。因此，输入非常宽或高的图像时要小心，因为它们可能会变形。
- en: Preprocessing Text
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理文本
- en: 'Let’s continue this exploration of the `processor` with text instead. First,
    we can access the tokenizer used to tokenize the input text:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探索`processor`对文本的处理。首先，我们可以访问用于对输入文本进行分词的分词器：
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The BLIP-2 model that we are using uses a GPT2Tokenizer. Most tokenizers work
    very similarly but have slight differences in when and how they tokenize the input
    text.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的BLIP-2模型使用的是GPT2Tokenizer。大多数分词器的工作方式非常相似，但在何时及如何对输入文本进行分词方面有细微差别。
- en: 'To explore how this GPT2Tokenizer works, we can try it out with a small sentence.
    We start by converting the sentence to token ids before converting them back to
    tokens:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这个GPT2Tokenizer是如何工作的，我们可以用一句小句子进行尝试。我们先将句子转换为token ids，然后再转换回tokens：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When we inspect the tokens, you might notice a strange symbol at the beginning
    of some tokens. Namely, the Ġ symbol. This is actually supposed to be a space.
    However, an internal function takes characters in certain code points and moves
    them up by 256 to make them printable. As a result, the space (code point 32)
    becomes Ġ (code point 288).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查标记时，可能会注意到某些标记开头有一个奇怪的符号，即 Ġ 符号。这实际上应该是一个空格。然而，一个内部函数将特定代码点中的字符向上移动256，使其可打印。因此，空格（代码点32）变成了
    Ġ（代码点288）。
- en: 'We will convert them to underscores for illustrative purposes:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们转换为下划线以便于说明：
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output shows that the underscore indicates the beginning of a word. That
    way, words that are made up of multiple tokens can be recognized.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示下划线表示一个单词的开始。这样，由多个标记组成的单词就可以被识别。
- en: 'Use Case 1: Image Captioning'
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例1：图像标注
- en: The most straightforward usage of a model like BLIP-2 is to create captions
    of images that you have in your data. You might be a store that wants to create
    descriptions of its clothing or perhaps you are a photographer that does not have
    the time to manually label its 1000+ pictures of a wedding.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 BLIP-2 这样的模型最直接的用法是为您数据中的图像创建标题。您可能是一家想要为其服装创建描述的商店，或者您是一位没有时间手动标注其1000多张婚礼照片的摄影师。
- en: The process of captioning an image closely follows the processing. An image
    is converted to pixel values that the model can read. These pixel values are passed
    to BLIP-2 to be converted into soft visual prompts that the LLM can use to decide
    on a proper caption.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图像标注的过程紧密跟随处理过程。图像被转换为模型可以读取的像素值。这些像素值被传递给 BLIP-2，以转换成 LLM 可用于决定适当标题的软视觉提示。
- en: 'Let’s take the image of a supercar and process it using the processor to derive
    pixels in the expected shape:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理一张超跑的图像，并使用处理器推导出预期形状的像素：
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The next step is converting the image into token IDs using the BLIP-2 model.
    After doing so, we can convert the IDs into text which is the generated caption:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用 BLIP-2 模型将图像转换为标记ID。完成后，我们可以将这些ID转换为生成的标题文本：
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When we print out the `generated_text`, we can take a look at the caption:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印出 `generated_text` 时，可以看看标题：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: “An orange supercar driving on the road at sunset” seems like a perfect description
    for this image!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: “一辆橙色超跑在日落时的公路上行驶”似乎是对这张图像的完美描述！
- en: Image captioning is a great way to get to learn this model before stepping into
    more complex use cases. Try it out with a few images yourself and see where it
    performs well and where it performs poorly.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图像标注是学习这个模型的好方法，在进入更复杂的用例之前，自己尝试几张图像，看看它表现良好和不佳的地方。
- en: Domain specific images, like pictures of specific cartoon characters or imaginary
    creations may fail as the model was trained on largely public data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域的图像，如特定卡通角色或虚构创作的图片，可能会失败，因为该模型主要是基于公开数据进行训练的。
- en: Let’s end this use case with a fun example, namely an image of the Rorschach
    which is illustrated in [Figure 5-21](#fig_21_an_image_from_the_rorschach_test_what_do_you_see).
    This test is an old psychological test which tests the individual’s perception
    of inkblots.^([4](ch05.html#id295)) What someone sees in such an inkblot supposedly
    tells you something about a person’s personality characteristics.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个有趣的例子结束这个用例，即[Rorschach测验中的图像](#fig_21_an_image_from_the_rorschach_test_what_do_you_see)，这个测试是一个古老的心理测试，用来测试个体对墨水斑点的感知。^([4](ch05.html#id295))
    人们在这样的墨水斑点中看到的东西据说能揭示出一个人的性格特征。
- en: It is quite a subjective test but that just makes it more fun!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项相当主观的测试，但这正使其更加有趣！
- en: '![An image from the Rorschach test. What do you see in it ](assets/Rorschach.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![Rorschach测验中的图像。你在里面看到了什么](assets/Rorschach.png)'
- en: Figure 5-21\. An image from the Rorschach test. What do you see in it?
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-21\. Rorschach测验中的图像。你在里面看到了什么？
- en: 'Let’s take the image illustrated in Figure 7-X and use that as our input:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以图7-X中的图像作为输入：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As before, when we print out the `generated_text`, we can take a look at the
    caption:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，当我们打印出 `generated_text` 时，可以看看标题：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: “A black and white ink drawing of a bat”. I can definitely see how the model
    would caption this image using such a description. Since this is a Rorscharch
    test, what do you think it says about the model?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: “一幅黑白的蝙蝠墨水画”。我完全能理解模型会用这样的描述为这张图像标注。由于这是一个Rorschach测试，你认为这对模型意味着什么？
- en: 'Use Case 2: Multimodal Chat-based Prompting'
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例2：多模态基于聊天的提示
- en: Although captioning is an important task, we can extend its use case even further.
    In that example, we showed going from one modality, vision (image), to another,
    text (caption).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管标题生成是一项重要任务，我们可以进一步扩展其应用案例。在那个示例中，我们展示了如何从一种模态（视觉，即图像）转向另一种模态（文本，即标题）。
- en: Instead of following this linear structure, we can try to present both modalities
    simultaneously by performing what is called visual question answering. In this
    particular use case, we give the model an image along with a question about that
    specific image for it to answer. The model would need to process both the image
    as well as the question as once.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试通过执行所谓的视觉问答，来同时呈现这两种模态，而不是遵循这种线性结构。在这个特定的用例中，我们给模型提供一张图像以及关于该特定图像的问题，让它回答。模型需要同时处理图像和问题。
- en: 'To demonstrate, let’s start with the picture of a car and ask BLIP-2 to describe
    the image. To do so, we first need to preprocess the image as we did a few times
    before:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们从一张汽车的图片开始，请BLIP-2描述该图像。为此，我们首先需要对图像进行预处理，就像我们之前做过几次一样：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To perform our visual question answering we need to give BLIP-2 more than just
    the image, namely the prompt. Without it the model would generate a caption as
    it did before.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行视觉问答，我们需要给BLIP-2的不仅仅是图像，还有提示语。如果没有提示，模型会像之前一样生成一个标题。
- en: 'We will ask the model to describe the image we just processed:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将请求模型描述我们刚刚处理的图像：
- en: '[PRE24]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When we print out the `generated_text`, we can explore the answer it has given
    to the question we asked it:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印出`generated_text`时，可以探索它对我们提问的回答：
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It correctly describes the image. However, this is a rather simple example since
    our question is essentially asking the model to create a caption. Instead, we
    can ask it follow-up questions in a chat-based manner.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 它正确地描述了图像。然而，这是一个相对简单的例子，因为我们的问题本质上是在让模型创建一个标题。相反，我们可以以聊天的方式问它后续问题。
- en: To do so, we can give the model our previous conversation, including its answer
    to our question. We then ask it a follow-up question.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以将之前的对话，包括它对我们问题的回答，提供给模型。然后我们问它一个后续问题。
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: $1,000,000 is highly specific! This shows a more chat-like behavior from BLIP-2
    which allows for some interesting conversations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: $1,000,000 是高度具体的！这展示了BLIP-2更像聊天的行为，允许进行一些有趣的对话。
- en: Finally, we can make this process a bit smoother by creating an interactive
    chat-bot using ipywidgets, an extension for Jupyter Notebooks that allows us to
    make interactive buttons, input text, etc.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过使用ipywidgets（Jupyter Notebooks的一个扩展，允许我们制作交互按钮、输入文本等）来使这个过程变得更加顺畅。
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Figure Caption to come](assets/multimodal_large_language_models_406548_23.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图例说明待补充](assets/multimodal_large_language_models_406548_23.png)'
- en: Figure 5-22\. Figure Caption to come
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-22\. 图例说明待补充
- en: It seems that we can continue the conversation and ask it a bunch of questions.
    Using this chat-based approach, we essentially created a chatbot that can reason
    about images!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 看来我们可以继续对话，问它一堆问题。通过这种基于聊天的方式，我们实际上创建了一个可以推理图像的聊天机器人！
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored two methods making language models multimodal.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了两种使语言模型多模态的方法。
- en: ^([1](ch05.html#id292-marker)) Wei, J., Tay, Y., Bommasani, R., Raffel, C.,
    Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., & others
    (2022). Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#id292-marker)) Wei, J., Tay, Y., Bommasani, R., Raffel, C.,
    Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., & others
    (2022). 大语言模型的涌现能力。arXiv预印本arXiv:2206.07682。
- en: '^([2](ch05.html#id293-marker)) Dosovitskiy, A., Beyer, L., Kolesnikov, A.,
    Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold,
    G., Gelly, S., & others (2020). An image is worth 16x16 words: Transformers for
    image recognition at scale. arXiv preprint arXiv:2010.11929.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.html#id293-marker)) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., & others (2020). 一张图像的价值相当于16x16个词：用于大规模图像识别的变换器。arXiv预印本arXiv:2010.11929。
- en: ^([3](ch05.html#id294-marker)) Radford, A., Kim, J., Hallacy, C., Ramesh, A.,
    Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., & others
    (2021). Learning transferable visual models from natural language supervision.
    In *International conference on machine learning* (pp. 8748–8763).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.html#id294-marker)) Radford, A., Kim, J., Hallacy, C., Ramesh, A.,
    Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., & others
    (2021). 从自然语言监督中学习可转移的视觉模型。在 *国际机器学习会议* (第8748–8763页)。
- en: '^([4](ch05.html#id295-marker)) Schafer, R. (1954). Psychoanalytic interpretation
    in Rorschach testing: theory and application.****'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.html#id295-marker)) Schafer, R. (1954). 罗夏克测试中的精神分析解读：理论与应用。****
