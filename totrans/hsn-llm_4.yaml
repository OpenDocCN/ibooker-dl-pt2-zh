- en: Chapter 5\. Multimodal Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you think about Large Language Models (LLMs), multimodality might not be
    the first thing that comes to mind. After all, they are *Language*Models!
  prefs: []
  type: TYPE_NORMAL
- en: We have seen all manner of emerging behaviors rising from LLMs, from generalization
    capabilities and reasoning to arithmetic and linguistics. As models grow larger
    and smarter, so do their skill sets.^([1](ch05.html#id292))
  prefs: []
  type: TYPE_NORMAL
- en: The ability to receive and reason with multimodal input might further increase
    and help emerge capabilities that were previously locked. In practice, Language
    does not solely live in a vacuum. As an example, your body language, facial expressions,
    intonation, etc. are all methods of communication that enhance the spoken word.
  prefs: []
  type: TYPE_NORMAL
- en: The same thing applies to Large Language Models, if we can enable them to reason
    about multimodal information, their capabilities might increase.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore a number of different LLMs that have multimodal
    capabilities and what that means for practical use cases. We will start by exploring
    how images are converted to numerical representations using an adaption of the
    original transformer technique. Then, we will show how LLMs can be extended to
    include vision tasks using this transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers for Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the chapters of this book, we have seen the success of using transformer-based
    models for a variety of language modeling tasks, from classification and clustering
    to search and generative modeling.
  prefs: []
  type: TYPE_NORMAL
- en: So it might not be surprising that researchers have been looking at a way to
    generalize some of the transformer’s success to the field of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: The method they came up with is called the Vision Transformer (ViT) which has
    been shown to do tremendously well on image recognition tasks compared to the
    previously default Convolutional Neural Networks (CNNs).^([2](ch05.html#id293))
    Like the original transformer, ViT is used to transform unstructured data, an
    image, into representations that can be used for a variety of tasks, like classification
    as illustrated in [Figure 5-1](#fig_1_both_the_original_transformer_as_well_as_the_visio).
  prefs: []
  type: TYPE_NORMAL
- en: '![Both the original transformer as well as the vision transformer take unstructured
    data  convert it to numerical representations  and finally use that for tasks
    like classification. ](assets/multimodal_large_language_models_406548_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Both the original transformer as well as the vision transformer
    take unstructured data, convert it to numerical representations, and finally use
    that for tasks like classification.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ViT relies on an important component of the transformer architecture, namely
    the encoder. As we saw in Chapter 1, the encoder is responsible for converting
    textual input into numerical representations before being passed to the decoder.
    However, before the encoder can perform its duties, the textual input needs to
    be tokenized first as is illustrated in [Figure 5-2](#fig_2_text_is_passed_to_one_or_multiple_encoders_by_firs).
  prefs: []
  type: TYPE_NORMAL
- en: '![Text is passed to one or multiple encoders by first tokenizing it using a
    tokenizer.](assets/multimodal_large_language_models_406548_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Text is passed to one or multiple encoders by first tokenizing
    it using a tokenizer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since an image does not consist of words this tokenization process cannot be
    used for visual data. Instead, the authors of ViT came up with a method for tokenizing
    images into “words” which allowed them to use the original encoder structure.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you have an image of a cat. This image is represented by a number
    of pixels, let’s say 512 by 512 pixels. Each individual pixel does not convey
    much information but when you combine patches of pixels, you slowly start to see
    more information.
  prefs: []
  type: TYPE_NORMAL
- en: ViT uses a principle much like that. Instead of splitting text up into tokens,
    it converts the original image into patches of images. In other words, it cuts
    the image into a number of pieces horizontally and vertically as illustrated in
    [Figure 5-3](#fig_3_the_tokenization_process_for_image_input_it_con).
  prefs: []
  type: TYPE_NORMAL
- en: '![The  tokenization  process for image input. It converts an image into patches
    of sub images. ](assets/multimodal_large_language_models_406548_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. The “tokenization” process for image input. It converts an image
    into patches of sub-images.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just like we are converting text into tokens of text, we are converting an image
    into patches of images. The flattened input of image patches can be thought of
    as the tokens in a piece of text.
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike tokens, we cannot just assign each patch with an ID since these
    patches will rarely be found in other images, unlike the vocabulary of a text.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the patches are linearly embedded to create numerical representations,
    namely embeddings. These can then be used as the input of a transformer model.
    That way, the patches of images are treated the same way as tokens. The full process
    is illustrated in [Figure 5-4](#fig_4_the_main_algorithm_behind_vit_after_patching_the).
  prefs: []
  type: TYPE_NORMAL
- en: '![The main algorithm behind ViT. After patching the images and linearly projecting
    them  the patch embeddings are passed to the encoder and treated as if they were
    textual tokens. ](assets/multimodal_large_language_models_406548_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. The main algorithm behind ViT. After patching the images and linearly
    projecting them, the patch embeddings are passed to the encoder and treated as
    if they were textual tokens.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For illustrative purposes, the images in the examples were patched into 3 by
    3 patches but the original implementation used 16 by 16 patches. After all, the
    paper is called “An image is worth 16x16 words”.
  prefs: []
  type: TYPE_NORMAL
- en: What is so interesting about this approach is that the moment the embeddings
    are passed to the encoder, they are treated as if they were textual tokens. From
    that point forward, there is no difference in how a textual or image trains and
    their outputs
  prefs: []
  type: TYPE_NORMAL
- en: Due to their similarities, the ViT is often used to make all kinds of language
    models multimodal. One of the most straightforward ways to use them is during
    the training of embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Embedding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, like Chapters X, X, and X, we used embedding models to
    capture the semantic content of textual representations, such as books and documents.
    We saw that we could use these embeddings or numerical representations to find
    similar documents, apply classification tasks, and even perform topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen many times before, embeddings often are an important driver
    behind LLM applications. They are an efficient method for capturing large-scale
    information and searching for the needle in the haystack of information.
  prefs: []
  type: TYPE_NORMAL
- en: That said, we have only looked at monomodal embedding models thus far. Embedding
    models that only focus on generating embeddings for textual representations. Although
    embedding models exist for solely embedding imagery, we will look at embedding
    models that can capture both textual as well as vision representations. We illustrate
    this in [Figure 5-5](#fig_5_multimodal_embedding_models_can_create_embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: '![Multimodal embedding models can create embeddings for multiple modalities
    in the same vector space. ](assets/multimodal_large_language_models_406548_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Multimodal embedding models can create embeddings for multiple
    modalities in the same vector space.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A big advantage is that it allows for comparing multimodal representations since
    the resulting embeddings lie in the same vector space, as illustrated in [Figure 5-6](#fig_6_tmultimodal_embedding_models_can_create_embeddings).
    For instance, using such a multimodal embedding model, we can find images based
    on input text. What images would we find if we search for images similar to “pictures
    of a puppy”? Vice versa would also be possible. Which documents are best related
    to this question?
  prefs: []
  type: TYPE_NORMAL
- en: '![tMultimodal embedding models can create embeddings for multiple modalities
    in the same vector space. ](assets/multimodal_large_language_models_406548_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Multimodal embedding models can create embeddings for multiple
    modalities in the same vector space.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a number of multimodal embedding models out there but the most well-known
    and currently most-used model is CLIP (Contrastive Language-Image Pre-Training).
  prefs: []
  type: TYPE_NORMAL
- en: 'CLIP: Connecting Text and Images'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CLIP is an embedding model that can compute embeddings of both images and texts.
    The resulting embeddings lie in the same vector space which means that the embeddings
    of images can be compared with the embeddings of text.^([3](ch05.html#id294))
  prefs: []
  type: TYPE_NORMAL
- en: 'This capability of comparison makes CLIP, and similar models, usable for tasks
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Zeroshot classification
  prefs: []
  type: TYPE_NORMAL
- en: We can compare the embedding of an image with that of the description of its
    possible classes to find which class is most similar
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs: []
  type: TYPE_NORMAL
- en: Cluster both images and a collection of keywords to find which keywords belong
    to which sets of images
  prefs: []
  type: TYPE_NORMAL
- en: Search
  prefs: []
  type: TYPE_NORMAL
- en: Across billions of texts or images, we can quickly find what relates to an input
    text or image
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs: []
  type: TYPE_NORMAL
- en: Use multimodal embeddings to drive the generation of images (e.g., stable diffusion)
  prefs: []
  type: TYPE_NORMAL
- en: How can CLIP generate multimodal embeddings?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The procedure of CLIP is actually quite straightforward. Imagine that you have
    a dataset with millions of images alongside captions as we illustrate in [Figure 5-7](#fig_7_the_type_of_data_that_is_needed_to_train_a_multimo).
  prefs: []
  type: TYPE_NORMAL
- en: '**![The type of data that is needed to train a multimodal embedding model.
    ](assets/multimodal_large_language_models_406548_07.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-7\. The type of data that is needed to train a multimodal embedding
    model.**  **This dataset can be used to create two representations for each pair,
    the image and its caption. To do so, CLIP uses a text encoder to embed text and
    an image encoder to embed images. As is shown in [Figure 5-8](#fig_8_in_the_first_step_of_training_clip_both_images_an),
    the result is an embedding for both the image and its corresponding caption.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![In the first step of training CLIP  both images and text are embedded using
    an image and text encoder respectively. ](assets/multimodal_large_language_models_406548_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. In the first step of training CLIP, both images and text are embedded
    using an image and text encoder respectively.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The pair of embeddings that are generated are compared through cosine similarity.
    As we saw in Chapter 2, cosine similarity is the cosine of the angle between vectors
    which is calculated through the dot product of the embeddings and divided by the
    product of their lengths.
  prefs: []
  type: TYPE_NORMAL
- en: When we start training, the similarity between the image embedding and text
    embedding will be low as they are not yet optimized to be within the same vector
    space. During training, we optimize for the similarity between the embeddings
    and want to maximize them for similar image/caption pairs and minimize them for
    dissimilar image/caption pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '**![In the second step of training CLIP  the similarity between the sentence
    and image embedding is calculated using cosine similarity. ](assets/multimodal_large_language_models_406548_09.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5-9\. In the second step of training CLIP, the similarity between the
    sentence and image embedding is calculated using cosine similarity.**  **After
    calculating their similarity, the model is updated and the process starts again
    with new batches of data and updated representations. This method is called contrastive
    learning and we will go in-depth into its inner workings in Chapter 13 where we
    will create our own embedding model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![In the third step of training CLIP  the text and image encoders are updated
    to match what the intended similarity should be. This updates the embeddings such
    that they are closer in vector space if the inputs are similar. ](assets/multimodal_large_language_models_406548_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. In the third step of training CLIP, the text and image encoders
    are updated to match what the intended similarity should be. This updates the
    embeddings such that they are closer in vector space if the inputs are similar.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Eventually, we expect the embedding of an image of a cat would be similar to
    the embedding of the sentence “a picture of a cat”. As we will see in Chapter
    13, to make sure the representations are as accurate as possible, negative examples
    of images and captions that are not related should also be included in the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling similarity is not only knowing what makes things similar to one another
    but also what makes them different and dissimilar.****  ****### OpenCLIP
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we are going to be using models from the open-source variant
    of CLIP, namely OpenCLIP ([https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)).
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenCLIP, or any CLIP model, boils down to two things, processing the
    textual and image inputs before passing them to the main model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before doing so, let’s take a look at a small example where we will be using
    one of the images we have seen before. Namely, an AI-generated image (though stable-diffusion)
    of a puppy playing in the snow as illustrated in [Figure 5-11](#fig_11_an_ai_generated_image_of_a_puppy_playing_in_the_sn):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![An AI generated image of a puppy playing in the snow.](assets/multimodal_large_language_models_406548_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. An AI-generated image of a puppy playing in the snow.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since we have a caption for this image, we can use OpenCLIP to generate embeddings
    for both.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we load in three models:'
  prefs: []
  type: TYPE_NORMAL
- en: A tokenizer for tokenizing the textual input
  prefs: []
  type: TYPE_NORMAL
- en: A preprocessor to preprocess and resize the image
  prefs: []
  type: TYPE_NORMAL
- en: The main model that converts the previous outputs to embeddings
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After having loaded in the models, preprocessing our input is straightforward.
    Let’s start with the tokenizer and see what happens if we preprocess our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our input text has been converted to input ids. To see what those represent,
    let’s convert them to tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As we often have seen before, the text is split up into tokens. Additionally,
    we now also see that the start and end of the text is indicated to separate it
    from a potential image embedding. You might also notice that the `[CLS]` token
    is missing. In CLIP, the `[CLS]` token is actually used to represent the image
    embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have preprocessed our caption, next up is to create the embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Before we can create our image embedding, like the text embedding, we will need
    to preprocess it as the model expects the input image to have certain characteristics,
    like its size and shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we can use the processor that we created before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The original image was 512 by 512 pixels. Notice that the preprocessing of this
    image reduced its size to 224 by 224 pixels as that is its expected size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize, in [Figure 5-12](#fig_12_the_preprocessed_input_image_by_clip),
    the preprocessed image to see what it actually is doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![The preprocessed input image by CLIP.](assets/multimodal_large_language_models_406548_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-12\. The preprocessed input image by CLIP.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To convert this preprocessed image into embeddings, we can call the `model`
    as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the shape of the resulting image embedding is exactly the same as
    that of the text embedding. This is important as it allows us to compare their
    embeddings and see whether they actually are similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use these embeddings to calculate the probability that the caption belongs
    to the image by calculating their dot product and taking the softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It gives us back a score of 1 indicating that the model is certain that the
    caption belongs to the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extend this example by calculating the similarity between the embeddings.
    By normalizing the embeddings first before calculating the dot product, we get
    a value that lies between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We get a similarity score of 0.33 which is difficult to interpret considering
    we do not know what the model considers a low versus a high similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, let’s extend the example with more images and captions as illustrated
    in [Figure 5-13](#fig_13_the_similarity_matrix_between_three_images_and_thr).
  prefs: []
  type: TYPE_NORMAL
- en: '![The similarity matrix between three images and three captions.](assets/multimodal_large_language_models_406548_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-13\. The similarity matrix between three images and three captions.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It seems that a score of 0.33 is indeed high considering the similarities with
    other images are quite a bit lower.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In sentence-transformers, there are a few CLIP-based models implemented that
    make it much easier to create embeddings. It only takes a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]****  ****# Making Text Generation Models Multimodal'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, text generation models have been, as you might expect, models
    that interpret textual representations. Models like Llama 2 and ChatGPT excel
    at reasoning about textual information and responding with natural language.
  prefs: []
  type: TYPE_NORMAL
- en: They are, however, limited to the modality they were trained in, namely text.
    As we have seen before with multimodal embedding models, the addition of vision
    can enhance the capabilities of a model.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of text generation models, we would like it to reason about certain
    input images. For example, we could give it an image of a pizza and ask it what
    ingredients it contains. You could show it a picture of the Eiffel Tower and ask
    it when it was built or where it is located. This conversational ability is further
    illustrated in [Figure 5-14](#fig_14_a_multimodal_text_generation_model_that_can_reason).
  prefs: []
  type: TYPE_NORMAL
- en: '![A multimodal text generation model that can reason about input images. ](assets/multimodal_large_language_models_406548_14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-14\. A multimodal text generation model that can reason about input
    images.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To bridge the gap between these two domains, attempts have been made to introduce
    a form of multimodality to existing models. One such method is called BLIP-2:
    *Bootstrapping Language Image Pre-training for unified vision-language understanding
    and generation 2*. BLIP-2 introduces an easy-to-use and modular technique that
    allows for introducing vision capabilities to existing language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BLIP-2: Bridging the Modality Gap'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a multimodal language model from scratch requires significant computing
    power and data. We would have to use billions of images, text, and image-text
    pairs to create such a model. As you can imagine, this is not easily feasible!
  prefs: []
  type: TYPE_NORMAL
- en: Instead of building the architecture from scratch, BLIP-2 bridges the vision-language
    gap by building a bridge, named the Q-former, that connects a pre-trained image
    encoder and a pre-trained LLM.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging pre-trained models, BLIP-2 only needs to train the bridge without
    needing to train the image encoder and LLM from scratch. It makes great use of
    the technology and models that are already out there! This bridge is illustrated
    in [Figure 5-15](#fig_15_the_querying_transformer_is_the_bridge_between_vis).
  prefs: []
  type: TYPE_NORMAL
- en: '![The Querying Transformer is the bridge between vision  ViT  and text  LLM  which
    is the only trainable component of the pipeline.](assets/multimodal_large_language_models_406548_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-15\. The Querying Transformer is the bridge between vision (ViT) and
    text (LLM) which is the only trainable component of the pipeline.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To connect the two pre-trained models, the Q-Former, also known as the Querying
    Transformer, mimics their architectures. It has two modules that share their attention
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: An image transformer to interact with the frozen vision transformer for feature
    extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A text transformer that can interact with the LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Q-Former is trained in two stages, one for each modality as illustrated
    in [Figure 5-16](#fig_16_in_step_1_representation_learning_is_applied_to_l).
  prefs: []
  type: TYPE_NORMAL
- en: '![In step 1  representation learning is applied to learn representations for
    vision and language simultaneously. In step 2  these representations are converted
    to soft visual prompts to feed the LLM. ](assets/multimodal_large_language_models_406548_16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-16\. In step 1, representation learning is applied to learn representations
    for vision and language simultaneously. In step 2, these representations are converted
    to soft visual prompts to feed the LLM.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In step 1, a number of image-document pairs are used to train the Q-Former to
    represent both images and text. These pairs are generally captions of images,
    as we have seen before with training CLIP.
  prefs: []
  type: TYPE_NORMAL
- en: The images are fed to the frozen vision transformer to extract vision embeddings.
    These embeddings are used as the input of Q-Former’s vision transformer. The captions
    are used as the input of Q-Former’s text transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these inputs, the Q-Former is then trained on three tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Image-Text Contrastive Learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image-Text Matching
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image-grounded Text Generation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These three objectives are jointly optimized to improve the visual representations
    that are extracted from the frozen vision transformer. In a way, we are trying
    to inject textual information into the embeddings of the frozen vision transformer
    so that we can use them in the LLM. This first step of BLIP-2 is illustrated in
    [Figure 5-17](#fig_17_in_step_1_the_output_of_the_frozen_vision_transfo).
  prefs: []
  type: TYPE_NORMAL
- en: '![In step 1  the output of the frozen vision transformer is used together with
    its caption and trained on three contrastive like tasks to learn visual text representations.](assets/multimodal_large_language_models_406548_17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-17\. In step 1, the output of the frozen vision transformer is used
    together with its caption and trained on three contrastive-like tasks to learn
    visual-text representations.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In step 2, the learnable embeddings derived from step 1 now contain visual information
    in the same dimensional space as its corresponding textual information.
  prefs: []
  type: TYPE_NORMAL
- en: The learnable embeddings are then passed to the LLM as a soft prompt. In a way,
    these embeddings contain textual representations of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: The learnable embeddings are then passed to the LLM. In a way, these embeddings
    serve as soft visual prompts that condition the LLM on the visual representations
    that were extracted by the Q-Former.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a fully connected linear layer in between them to make sure that
    the learnable embeddings have the same shape as the LLM expects. This second step
    of converting vision to language is represented in [Figure 5-18](#fig_18_in_step_2_the_learned_embeddings_from_the_q_forme).
  prefs: []
  type: TYPE_NORMAL
- en: '![In step 2  the learned embeddings from the Q Former are passed to the LLM
    through a projection layer. The projected embeddings serve as a soft visual prompt.](assets/multimodal_large_language_models_406548_18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-18\. In step 2, the learned embeddings from the Q-Former are passed
    to the LLM through a projection layer. The projected embeddings serve as a soft
    visual prompt.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When we put these steps together, they make it possible for the Q-Former to
    learn visual and textual representations in the same dimensional space which can
    be used as a soft prompt to the LLM. As a result, the LLM will be given information
    about the image and is similar to the context you would provide an LLM when prompting.
    The full in-depth process is illustrated in [Figure 5-19](#fig_19_the_full_procedure_of_blip_2).
  prefs: []
  type: TYPE_NORMAL
- en: '![The full procedure of BLIP 2.](assets/multimodal_large_language_models_406548_19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-19\. The full procedure of BLIP-2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Preprocessing Multimodal Inputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know how BLIP-2 is created, there are a number of interesting use
    cases for which you can use such a model. Not limited to captioning images, answering
    visual questions, and even performing prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go through some use cases, let’s first load the model and explore
    how you can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using `model.vision_model` and `model.language_model` we can see which vision
    transformer and large language model are respectively used in the BLIP-2 model
    that we loaded.
  prefs: []
  type: TYPE_NORMAL
- en: We loaded two components that make up our full pipeline, a `processor` and a
    `model`. The `processor` can be compared to the tokenizer of language models.
    It converts unstructured input, such as images and text, to representations that
    the model generally expects.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by exploring what the `processor` does to images. We start by loading
    the picture of a very wide image for illustration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Caption to come](assets/multimodal_large_language_models_406548_20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-20\. Caption to come
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The image has 520 by 492 pixels which is generally an unusual format. So let’s
    see what our processor does to it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When we check its shape after converting it to Numpy, it shows us an additional
    dimension that is of size 3\. This represents the RGB coding of each pixel, namely
    its color.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we pass the original image to the processor so that the image can be
    processed to the shape the model expects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result is a 224 by 224 sized image. Quite a bit smaller than we initially
    had! This also means that all different shapes of images will be processed into
    squares. So be careful inputting very wide or tall images as they might get distorted.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s continue this exploration of the `processor` with text instead. First,
    we can access the tokenizer used to tokenize the input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The BLIP-2 model that we are using uses a GPT2Tokenizer. Most tokenizers work
    very similarly but have slight differences in when and how they tokenize the input
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore how this GPT2Tokenizer works, we can try it out with a small sentence.
    We start by converting the sentence to token ids before converting them back to
    tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: When we inspect the tokens, you might notice a strange symbol at the beginning
    of some tokens. Namely, the Ġ symbol. This is actually supposed to be a space.
    However, an internal function takes characters in certain code points and moves
    them up by 256 to make them printable. As a result, the space (code point 32)
    becomes Ġ (code point 288).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will convert them to underscores for illustrative purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the underscore indicates the beginning of a word. That
    way, words that are made up of multiple tokens can be recognized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case 1: Image Captioning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most straightforward usage of a model like BLIP-2 is to create captions
    of images that you have in your data. You might be a store that wants to create
    descriptions of its clothing or perhaps you are a photographer that does not have
    the time to manually label its 1000+ pictures of a wedding.
  prefs: []
  type: TYPE_NORMAL
- en: The process of captioning an image closely follows the processing. An image
    is converted to pixel values that the model can read. These pixel values are passed
    to BLIP-2 to be converted into soft visual prompts that the LLM can use to decide
    on a proper caption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the image of a supercar and process it using the processor to derive
    pixels in the expected shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is converting the image into token IDs using the BLIP-2 model.
    After doing so, we can convert the IDs into text which is the generated caption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When we print out the `generated_text`, we can take a look at the caption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: “An orange supercar driving on the road at sunset” seems like a perfect description
    for this image!
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning is a great way to get to learn this model before stepping into
    more complex use cases. Try it out with a few images yourself and see where it
    performs well and where it performs poorly.
  prefs: []
  type: TYPE_NORMAL
- en: Domain specific images, like pictures of specific cartoon characters or imaginary
    creations may fail as the model was trained on largely public data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s end this use case with a fun example, namely an image of the Rorschach
    which is illustrated in [Figure 5-21](#fig_21_an_image_from_the_rorschach_test_what_do_you_see).
    This test is an old psychological test which tests the individual’s perception
    of inkblots.^([4](ch05.html#id295)) What someone sees in such an inkblot supposedly
    tells you something about a person’s personality characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite a subjective test but that just makes it more fun!
  prefs: []
  type: TYPE_NORMAL
- en: '![An image from the Rorschach test. What do you see in it ](assets/Rorschach.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-21\. An image from the Rorschach test. What do you see in it?
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s take the image illustrated in Figure 7-X and use that as our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, when we print out the `generated_text`, we can take a look at the
    caption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: “A black and white ink drawing of a bat”. I can definitely see how the model
    would caption this image using such a description. Since this is a Rorscharch
    test, what do you think it says about the model?
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Case 2: Multimodal Chat-based Prompting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although captioning is an important task, we can extend its use case even further.
    In that example, we showed going from one modality, vision (image), to another,
    text (caption).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of following this linear structure, we can try to present both modalities
    simultaneously by performing what is called visual question answering. In this
    particular use case, we give the model an image along with a question about that
    specific image for it to answer. The model would need to process both the image
    as well as the question as once.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate, let’s start with the picture of a car and ask BLIP-2 to describe
    the image. To do so, we first need to preprocess the image as we did a few times
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: To perform our visual question answering we need to give BLIP-2 more than just
    the image, namely the prompt. Without it the model would generate a caption as
    it did before.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will ask the model to describe the image we just processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When we print out the `generated_text`, we can explore the answer it has given
    to the question we asked it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It correctly describes the image. However, this is a rather simple example since
    our question is essentially asking the model to create a caption. Instead, we
    can ask it follow-up questions in a chat-based manner.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, we can give the model our previous conversation, including its answer
    to our question. We then ask it a follow-up question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: $1,000,000 is highly specific! This shows a more chat-like behavior from BLIP-2
    which allows for some interesting conversations.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can make this process a bit smoother by creating an interactive
    chat-bot using ipywidgets, an extension for Jupyter Notebooks that allows us to
    make interactive buttons, input text, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure Caption to come](assets/multimodal_large_language_models_406548_23.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-22\. Figure Caption to come
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It seems that we can continue the conversation and ask it a bunch of questions.
    Using this chat-based approach, we essentially created a chatbot that can reason
    about images!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored two methods making language models multimodal.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#id292-marker)) Wei, J., Tay, Y., Bommasani, R., Raffel, C.,
    Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., & others
    (2022). Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch05.html#id293-marker)) Dosovitskiy, A., Beyer, L., Kolesnikov, A.,
    Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold,
    G., Gelly, S., & others (2020). An image is worth 16x16 words: Transformers for
    image recognition at scale. arXiv preprint arXiv:2010.11929.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#id294-marker)) Radford, A., Kim, J., Hallacy, C., Ramesh, A.,
    Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., & others
    (2021). Learning transferable visual models from natural language supervision.
    In *International conference on machine learning* (pp. 8748–8763).
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch05.html#id295-marker)) Schafer, R. (1954). Psychoanalytic interpretation
    in Rorschach testing: theory and application.****'
  prefs: []
  type: TYPE_NORMAL
