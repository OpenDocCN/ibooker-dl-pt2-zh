- en: 8 Attention and Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Using attention to produce summaries of the input and improve the quality of
    Seq2Seq models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing RNN-style loops with self-attention, a mechanism for the input to
    summarize itself
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving machine translation systems with the Transformer model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a high-quality spell-checker using the Transformer model and publicly
    available datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our focus so far in this book has been recurrent neural networks (RNNs), which
    are a powerful model that can be applied to various NLP tasks such as sentiment
    analysis, named entity recognition, and machine translation. In this chapter,
    we will introduce an even more powerful model—the *Transformer*[¹](#pgfId-1106840)—a
    new type of encoder-decoder neural network architecture based on the concept of
    self-attention. It is without a doubt the most important NLP model since it appeared
    in 2017\. Not only is it a powerful model itself (for machine translation and
    various Seq2Seq tasks, for example), but it is also used as the underlying architecture
    that powers numerous modern NLP pretrained models, including GPT-2 (section 8.4.3)
    and BERT (section 9.2). The developments in modern NLP since 2017 can be best
    summarized as “the era of the Transformer.”
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we start with attention, a mechanism that made a breakthrough
    in machine translation, then move on to introducing self-attention, the concept
    that forms the foundation of the Transformer model. We will build two NLP applications—a
    Spanish-to-English machine translator and a high-quality spell-checker—and learn
    how to apply the Transformer model to your everyday applications. As we’ll see
    later, the Transformer models can improve the quality of NLP systems over RNNs
    by a large margin and achieve almost human-level performance in some tasks, such
    as translation and generation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 What is attention?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 6, we covered Seq2Seq models—NLP models that transform one sequence
    to another using an encoder and a decoder. Seq2Seq is a versatile and powerful
    paradigm with many applications, although the “vanilla” Seq2Seq models are not
    without limitation. In this section, we discuss the Seq2Seq models’ bottleneck
    and motivate the use of an attention mechanism.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Limitation of vanilla Seq2Seq models
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s remind ourselves how Seq2Seq models work. Seq2Seq models consist of an
    encoder and a decoder. The decoder takes a sequence of tokens in the source language
    and runs it through an RNN, which produces a fixed-length vector at the end. This
    fixed-length vector is a representation of the input sentence. The decoder, which
    is another RNN, takes this vector and produces a sequence in the target language,
    token by token. Figure 8.1 illustrates how Spanish sentences are translated into
    English with a vanilla Seq2Seq model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![CH08_F01_Hagiwara](../Images/CH08_F01_Hagiwara.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 A bottleneck in a vanilla Seq2Seq model
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: This Seq2Seq architecture is quite simple and powerful, but it is known that
    its vanilla version (shown in figure 8.1) does not translate sentences as well
    as other traditional machine translation algorithms (such as phrase-based statistical
    machine translation models). You may be able to guess why this is the case if
    you look at its structure carefully—its encoder is trying to “compress” all the
    information in the source sentence into the sentence representation, which is
    a vector of some fixed length (e.g., 256 floating-point numbers), and the decoder
    is trying to restore the entire target sentence just from that vector. The size
    of the vector is fixed no matter how long (or how short) the source sentence is.
    The intermediate vector is a huge bottleneck. If you think of how humans actually
    translate between languages, this sounds quite difficult and somewhat unusual.
    Professional translators do not just read the source sentence and write down its
    translation in one breath. They refer to the source sentences as many times as
    necessary to translate the relevant parts in the target sentence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Seq2Seq 架构非常简单而强大，但众所周知，它的基本版本（如图 8.1 所示）在翻译句子方面不如其他传统的机器翻译算法（如基于短语的统计机器翻译模型）。如果你仔细观察它的结构，可能就能猜出其中的原因——它的编码器试图将源句子中的所有信息“压缩”到句子表示中，这是一个固定长度的向量（例如，256个浮点数），而解码器则试图仅从该向量中恢复整个目标句子。无论源句子有多长（或多短），向量的大小都是固定的。中间向量是一个巨大的瓶颈。如果你考虑一下人类实际上如何在语言之间进行翻译，这听起来相当困难且有些不寻常。专业的翻译人员不会一口气读完源句子然后把它的翻译写下来。他们会根据需要多次参考源句子，以翻译目标句子中的相关部分。
- en: Compressing all the information into one vector may (and does) work for short
    sentences, as we’ll see later in section 8.2.2, but it becomes increasingly difficult
    as the sentences get longer and longer. Studies have shown that the translation
    quality of a vanilla Seq2Seq model gets worse as the sentence gets longer.[²](#pgfId-1106861)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有信息压缩成一个向量可能（并且确实）对短句子有效，正如我们稍后在8.2.2节中将看到的那样，但随着句子变得越来越长，这种方法变得越来越困难。研究表明，基本
    Seq2Seq 模型的翻译质量随着句子变得越来越长而变差。[²](#pgfId-1106861)
- en: 8.1.2 Attention mechanism
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 注意力机制
- en: Instead of relying on a single, fixed-length vector to represent all the information
    in a sentence, the decoder would have a much easier time if there was a mechanism
    where it can refer to some specific part of the encoder as it generates the target
    tokens. This is similar to how human translators (the decoder) reference the source
    sentence (the encoder) as needed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器如果能够在生成目标标记时参考编码器的某个特定部分，将会容易得多。这类似于人类翻译员（解码器）根据需要参考源句子（编码器）。
- en: This can be achieved by using *attention*, which is a mechanism in neural networks
    that focuses on a specific part of the input and computes its context-dependent
    summary. It is like having some sort of key-value store that contains all of the
    input’s information and then looking it up with a query (the current context).
    The stored values are not just a single vector but usually a list of vectors,
    one for each token, associated with corresponding keys. This effectively increases
    the size of the “memory” the decoder can refer to when it’s making a prediction.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过使用 *注意力* 来实现，注意力是神经网络中的一种机制，它专注于输入的特定部分并计算其上下文相关的摘要。这就像拥有某种包含输入所有信息的键值存储，然后用查询（当前上下文）查找它一样。存储的值不仅仅是单个向量，而通常是一个向量列表，每个标记关联一个相应的键。这有效地增加了解码器在进行预测时可以参考的“内存”大小。
- en: 'Before we discuss how the attention mechanism works for Seq2Seq models, let’s
    see it in action in a general form. Figure 8.2 illustrates a generic attention
    mechanism with the following features:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论注意力机制如何在 Seq2Seq 模型中工作之前，让我们先看一下它以一般形式的运行情况。图 8.2 描绘了一个具有以下特征的通用注意力机制：
- en: The inputs to an attention mechanism are the values and their associated keys.
    The input values can take many different forms, but in NLP, they are almost always
    lists of vectors. For Seq2Seq models, the keys and values here are the hidden
    states of the encoder, which represent token-by-token encoding of the input sentence.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力机制的输入是值及其相关的键。输入值可以采用许多不同的形式，但在自然语言处理中，它们几乎总是向量列表。对于 Seq2Seq 模型，这里的键和值是编码器的隐藏状态，它们代表了输入句子的标记编码。
- en: Each key associated with a value is compared against the query using an attention
    function f. By applying f to the query and each one of the keys, you get a set
    of scores, one per key-value pair, which are then normalized to obtain a set of
    attention weights. The specific function f depends on the architecture (more on
    this later). For Seq2Seq models, this gives you a distribution over the input
    tokens. The more relevant an input token is, the larger the weight it gets.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个与值关联的键都使用注意力函数 f 与查询进行比较。通过将 f 应用于查询和每个键之一，您会得到一组分数，每个键值对一个，然后将其归一化以获得一组注意力权重。特定的函数
    f 取决于体系结构（稍后会详细介绍）。对于 Seq2Seq 模型，这会给出一个输入令牌的分布。输入令牌越相关，其权重越大。
- en: The input values are weighted by their corresponding weights obtained in step
    2 and summed up to compute the final summary vector. For Seq2Seq models, this
    summary vector is appended to the decoder hidden states to aid the translation
    process.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入值由第 2 步中获得的相应权重加权，并相加以计算最终摘要向量。对于 Seq2Seq 模型，此摘要向量附加到解码器隐藏状态以辅助翻译过程。
- en: '![CH08_F02_Hagiwara](../Images/CH08_F02_Hagiwara.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F02_Hagiwara](../Images/CH08_F02_Hagiwara.png)'
- en: Figure 8.2 Using an attention mechanism to summarize the input
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 使用注意力机制对输入进行总结
- en: Because of step 3, the output of an attention mechanism is always a weighted
    sum of the input vectors, but how they are weighted is determined by the attention
    weights, which are in turn are calculated from the keys and the query. In other
    words, what an attention mechanism computes is *a context (query)-dependent summary
    of the input*. Downstream components of a neural network (e.g., the decoder of
    an RNN-based Seq2Seq model, or the upper layers of a Transformer model) use this
    summary to further process the input.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第 3 步，注意力机制的输出始终是输入向量的加权和，但它们如何加权是由注意力权重确定的，而注意力权重又是从键和查询计算得出的。换句话说，注意力机制计算的是
    *上下文（查询）相关的输入摘要*。神经网络的下游组件（例如基于 RNN 的 Seq2Seq 模型的解码器，或者 Transformer 模型的上层）使用此摘要进一步处理输入。
- en: In the following sections, we will learn the two most commonly used types of
    attention mechanisms in NLP—encoder-decoder attention (also called *cross-attention*;
    used in both RNN-based Seq2Seq models and the Transformer) and self-attention
    (used in the Transformer).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将学习 NLP 中两种最常用的注意力机制类型 —— 编码器-解码器注意力（也称为 *交叉注意力*；在基于 RNN 的 Seq2Seq
    模型和 Transformer 中都使用）和自注意力（在 Transformer 中使用）。
- en: 8.2 Sequence-to-sequence with attention
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 带有注意力的序列到序列
- en: In this section, we’ll learn how the attention mechanism is applied to an RNN-based
    Seq2Seq model for which the attention mechanism was first invented. We’ll study
    how it works with specific examples, and then we’ll experiment with Seq2Seq models
    with and without the attention mechanism using fairseq to observe how it affects
    the translation quality.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何将注意力机制应用于 RNN 基础的 Seq2Seq 模型，注意力机制是首次发明的。我们将研究它如何与具体示例一起工作，然后我们将使用
    fairseq 实验带有和不带有注意力机制的 Seq2Seq 模型，以观察它对翻译质量的影响。
- en: 8.2.1 Encoder-decoder attention
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 编码器-解码器注意力
- en: As we saw earlier, attention is a mechanism for creating a summary of the input
    under a specific context. We used a key-value store and a query as an analogy
    for how it works. Let’s see how an attention mechanism is used with RNN-based
    Seq2Seq models using the concrete examples that follow.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，注意力是在特定上下文下创建输入摘要的机制。我们使用了一个键值存储和一个查询作为它如何工作的类比。让我们看看注意力机制如何在基于 RNN
    的 Seq2Seq 模型中使用，使用随后的具体示例。
- en: '![CH08_F03_Hagiwara](../Images/CH08_F03_Hagiwara.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F03_Hagiwara](../Images/CH08_F03_Hagiwara.png)'
- en: Figure 8.3 Adding an attention mechanism to an RNN-based Seq2Seq model (the
    lightly shaded box)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 在基于 RNN 的 Seq2Seq 模型中添加注意力机制（浅色阴影框）
- en: 'Figure 8.3 illustrates a Seq2Seq model with attention. It looks complex at
    first, but it is just an RNN-based Seq2Seq model with some extra “things” added
    on top of the encoder (the lightly shaded box in top left corner of the figure).
    If you ignore what’s inside and see it as a black box, all it does is simply take
    a query and return some sort of summary created from the input. The way it computes
    this summary is just a variant of the generic form of attention we covered in
    section 8.1.2\. It proceeds as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3展示了一个带有注意力机制的Seq2Seq模型。一开始看起来很复杂，但实际上它只是一个基于RNN的Seq2Seq模型，在编码器顶部左上角的浅色阴影框中添加了一些额外的“东西”。如果你忽略里面的内容，将其视为黑匣子，它所做的就是简单地接受一个查询并从输入中返回某种摘要。它计算这个摘要的方式只是我们在8.1.2节中介绍的通用注意力形式的一个变体。它的执行步骤如下：
- en: The input to the attention mechanism is the list of hidden states computed by
    the encoder. These hidden states are used as both keys and values (i.e., the keys
    and the values are identical). The encoder hidden state at a certain token (e.g.,
    at token “no”) reflects the information about that token and all the tokens leading
    up to it (if the RNN is unidirectional) or the entire sentence (if the RNN is
    bidirectional).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力机制的输入是编码器计算的隐藏状态列表。这些隐藏状态既用作键也用作值（即，键和值是相同的）。某个令牌（例如，“no”令牌）的编码器隐藏状态反映了关于该令牌及其之前所有令牌的信息（如果RNN是单向的），或整个句子（如果RNN是双向的）。
- en: Let’s say you finished decoding up to “Mary did.” The hidden states of the decoder
    at that point are used as the query, which is compared against every key using
    function f. This produces a list of attention scores, one per each key-value pair.
    These scores determine which part of the input the decoder should attend to when
    it’s trying to generate a word that follows “Mary did.”
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你已经解码到“Mary did.”。此时解码器的隐藏状态被用作查询，与每个键使用函数f进行比较。这会产生一个注意力分数列表，每个键值对应一个分数。这些分数确定了解码器在尝试生成跟在“Mary
    did.”后面的单词时应该关注输入的哪个部分。
- en: These scores are converted to a probability distribution (a set of positive
    values that sum to one), which is used to determine which vectors should get the
    most attention. The return value from this attention mechanism is the sum of all
    values, weighted by the attention scores after normalizing with softmax.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些分数被转换为概率分布（一组正值，总和为1），用于确定哪些向量应该得到最多的关注。这个注意力机制的返回值是所有值的加权和，加权值为注意力分数经过softmax归一化后的值。
- en: You may be wondering what the attention function f looks like. A couple of variants
    of f are possible, depending on how it computes the attention scores between the
    key and the query, but these details do not matter much here. One thing to note
    is that in the original paper proposing the attention mechanism,[³](#pgfId-1106909)
    the authors used a “mini” neural network to calculate attention scores from the
    key and the query.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道注意力函数f是什么样的。f的几个变体是可能的，这取决于它如何计算键和查询之间的注意力分数，但这些细节在这里并不重要。值得注意的一点是，在提出注意力机制的原始论文中，作者使用了一个“迷你”神经网络来计算键和查询之间的注意力分数。
- en: This “mini” network-based attention function is not something you just plug
    in to an RNN model post hoc and expect it to work. It is optimized as part of
    the entire network—that is, as the entire network gets optimized by minimizing
    the loss function, the attention mechanism also gets better at generating summaries
    because doing so well also helps the decoder generate better translation and lower
    the loss function. In other words, the entire network, including the attention
    mechanism, is trained end to end. This usually means that, as the network is optimized,
    the attention mechanism starts to learn to focus only on the relevant part of
    the input, which is usually where the target tokens are aligned with the source
    tokens. In other words, attention is calculating some sort of “soft” word alignment
    between the source and the target tokens.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基于“迷你”网络的注意力函数不是你只需事后将其插入到RNN模型中就能让其正常工作的东西。它是作为整个网络的一部分进行优化的—也就是说，当整个网络通过最小化损失函数进行优化时，注意力机制也会变得更好，因为这样做也有助于解码器生成更好的翻译并降低损失函数。换句话说，整个网络，包括注意力机制，都是端到端训练的。这通常意味着，随着网络的优化，注意力机制开始学习只关注输入的相关部分，这通常是目标标记与源标记对齐的地方。换句话说，注意力计算了源标记和目标标记之间某种“软”单词对齐。
- en: 8.2.2 Building a Seq2Seq machine translation with attention
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 使用注意力构建Seq2Seq机器翻译
- en: In section 6.3, we built our first machine translation (MT) system using fairseq,
    an NMT toolkit developed by Facebook. Using the parallel dataset from Tatoeba,
    we built an LSTM-based Seq2Seq model to translate Spanish sentences into English.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6.3节中，我们使用由Facebook开发的NMT工具包fairseq构建了我们的第一个机器翻译（MT）系统。 使用来自Tatoeba的平行数据集，我们构建了一个基于LSTM的Seq2Seq模型，将西班牙语句子翻译成英语。
- en: In this section, we are going to experiment with a Seq2Seq machine translation
    system and see how attention affects the translation quality. We assume that you’ve
    already gone through the steps we took when we built the MT system by downloading
    the dataset and running the fairseq-preprocess and fairseq-train commands (section
    6.3). After that, you ran the fairseq-interactive command to interactively translate
    Spanish sentences into English. You might have noticed that the translation you
    get from this MT system that took you just 30 minutes to build was actually decent.
    In fact, the model architecture we used (—arch lstm) has an attention mechanism
    built in by default. Notice when you ran the following fairseq-train command
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试使用Seq2Seq机器翻译系统，并看看注意力如何影响翻译质量。 我们假设您已经按照我们构建MT系统的步骤操作，通过下载数据集并运行fairseq-preprocess和fairseq-train命令（第6.3节）。
    之后，您运行了fairseq-interactive命令以将西班牙语句子交互式地翻译成英语。 您可能已经注意到，从这个仅花了您30分钟构建的MT系统得到的翻译实际上相当不错。
    实际上，我们使用的模型架构（—arch lstm）默认内置了注意力机制。 请注意，当您运行以下fairseq-train命令时
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'you should have seen the dump of what your model looks like in your terminal
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该已经在终端中看到了您的模型的输出，如下所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This tells you that your model has an encoder and a decoder, but the decoder
    also has a component called attention (which is of type AttentionLayer), shown
    in bold in the code snippet. This is exactly the “mini-network” that we covered
    in section 8.2.1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉您，您的模型有一个编码器和一个解码器，但解码器还有一个称为注意力的组件（类型为AttentionLayer），如代码片段中的粗体所示。 这正是我们在第8.2.1节中讨论过的“小型网络”。
- en: 'Now let’s train the same model, but without attention. You can add —decoder-attention
    0 to fairseq-train to disable the attention mechanism, while keeping everything
    else the same, as shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练相同的模型，但是不使用注意力机制。您可以在fairseq-train中添加—decoder-attention 0来禁用注意力机制，同时保持其他所有内容不变，如下所示：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When you run this, you’ll see a similar dump, shown next, that shows the architecture
    of the model but without attention:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此命令时，您将看到类似的输出，接下来显示了模型的架构，但没有注意力机制：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we saw in section 6.3.2, the training process alternates between training
    and validation. In the training phase, the parameters of the neural network are
    optimized by the optimizer. In the validation phase, these parameters are fixed,
    and the model is run on a held-out portion of the dataset called the *validation
    set*. In addition to making sure the training loss decreases, you should be looking
    at the validation loss during training, because it better represents how well
    the model generalizes outside the training data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第6.3.2节中看到的，训练过程在训练和验证之间交替进行。 在训练阶段，神经网络的参数通过优化器进行优化。 在验证阶段，这些参数被固定，并且模型在称为*验证集*的数据集的一个保留部分上运行。
    除了确保训练损失下降外，您还应该在训练过程中查看验证损失，因为它更好地表示了模型在训练数据之外的泛化能力。
- en: 'During this experiment, you should observe that the lowest validation loss
    achieved by the attention model is around 1.727, whereas that for the attention-less
    model is around 2.243\. Lower loss values mean the model is fitting the dataset
    better, so this indicates the attention is helping improve the translation. Let’s
    see if this is actually the case. As we’ve done in section 6.3.2, you can generate
    translations interactively by running the following fairseq-interactive command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，您应该观察到由注意力模型实现的最低验证损失约为1.727，而无注意力模型的最低验证损失约为2.243。 较低的损失值意味着模型更好地适应了数据集，因此这表明注意力有助于改善翻译。
    让我们看看这是否属实。 正如我们在第6.3.2节中所做的，您可以通过运行以下fairseq-interactive命令来交互地生成翻译：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In table 8.1, we compare the translations generated by the model with and without
    attention. The translations you get from the attention-based model are the same
    as the ones we saw in section 6.3.3\. Notice that the translations you get from
    the attention-less model are a lot worse than those from the attention model.
    If you look at the translations for “¿Hay habitaciones libres?” and “Maria no
    daba una bofetada a la bruja verde,” you see unfamiliar tokens “<unk>” (for “unknown”)
    in them. What’s happening here?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 8.1 中，我们比较了带有和不带有注意力的模型生成的翻译。基于注意力的模型得到的翻译与我们在第 6.3.3 节中看到的一样。请注意，基于没有注意力的模型得到的翻译比具有注意力的模型要糟糕得多。如果您看一下“¿Hay
    habitaciones libres？”和“Maria no daba una bofetada a la bruja verde”的翻译，您会看到其中的陌生令牌“<unk>”（表示“未知”）。这里发生了什么？
- en: Table 8.1 Translation generated by the model with and without attention
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 模型带有和不带有注意力的翻译比较
- en: '| **Spanish (input)** | **With attention** | **Without attention** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **西班牙语（输入）** | **带有注意力** | **没有注意力** |'
- en: '| ¡Buenos días! | Good morning! | Good morning! |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ¡Buenos días! | 早上好！ | Good morning! |'
- en: '| ¡Hola! | Hi! | Hi! |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| ¡Hola! | 你好！ | Hi! |'
- en: '| ¿Dónde está el baño? | Where’s the restroom? | Where’s the toilet? |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ¿Dónde está el baño? | 厕所在哪里？ | Where’s the toilet? |'
- en: '| ¿Hay habitaciones libres? | Is there free rooms? | Are there <unk> rooms?
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ¿Hay habitaciones libres? | 有空房间吗？ | Are there <unk> rooms? |'
- en: '| ¿Acepta tarjeta de crédito? | Do you accept credit card? | Do you accept
    credit card? |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ¿Acepta tarjeta de crédito? | 你们接受信用卡吗？ | Do you accept credit card? |'
- en: '| La cuenta, por favor. | The bill, please. | Check, please. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| La cuenta, por favor. | 请结账。 | Check, please. |'
- en: '| Maria no daba una bofetada a la bruja verde. | Maria didn’t give the green
    witch. | Mary wasn’t a <unk> of the pants. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Maria no daba una bofetada a la bruja verde. | Maria 没有打绿色女巫。 | Mary wasn’t
    a <unk> of the pants. |'
- en: These are special tokens that are assigned to out-of-vocabulary (OOV) words.
    We touched upon OOV words in section 3.6.1 (when we introduced the concept of
    subwords used for FastText). Most NLP applications operate within a fixed vocabulary,
    and whenever they encounter or try to produce words that are outside that predefined
    set, the words are replaced with a special token, <unk>. This is akin to a special
    value (such as None in Python) returned when a method doesn’t know what to do
    with the input. Because these sentences contain certain words (I suspect they
    are “libres” and “bofetada”), the Seq2Seq model without attention, whose memory
    is limited, didn’t know what to do with them and simply fell back on a safest
    thing to do, which is to produce a generic, catch-all symbol, <unk>. On the other
    hand, you can see that attention prevents the system from producing these symbols
    and helps improve the overall quality of the produced translations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是分配给词汇表之外（OOV）词汇的特殊令牌。我们在第 3.6.1 节中提及了 OOV 词汇（当我们介绍用于 FastText 的子词概念时）。大多数自然语言处理应用都在一个固定的词汇表中操作，每当它们遇到或尝试生成超出预定义集合的词汇时，这些词汇都会被替换为一个特殊的令牌
    `<unk>`。这类似于当方法不知道如何处理输入时返回的特殊值（例如 Python 中的 None）。因为这些句子包含某些词汇（我怀疑它们是“libres”和“bofetada”），没有注意力的
    Seq2Seq 模型，其内存是有限的，不知道该如何处理它们，简单地回归到最安全的操作，即生成一个通用的、捕获所有的符号 `<unk>`。另一方面，您可以看到注意力防止系统生成这些符号，并有助于提高生成的翻译的整体质量。
- en: 8.3 Transformer and self-attention
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 Transformer 和自注意力
- en: In this section, we are going to learn how the Transformer model works and,
    specifically, how it generates high-quality translations by using a new mechanism
    called *self-attention*. Self-attention creates a summary of the entire input,
    but it does this for each token using the token as the context.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将学习 Transformer 模型的工作原理，具体来说，是它如何利用一种称为*自注意力*的新机制生成高质量的翻译。自注意力机制使用每个令牌作为上下文，为每个令牌创建了整个输入的摘要。
- en: 8.3.1 Self-attention
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 自注意力
- en: As we’ve seen before, attention is a mechanism that creates a context-dependent
    summary of the input. For RNN-based Seq2Seq models, the input is the encoder hidden
    states, whereas the context is the decoder hidden states. The core idea of the
    Transformer, self-attention, also creates a summary of the input, except for one
    key difference—the context in which the summary is created is also the input itself.
    See figure 8.4 for a simplified illustration of a self-attention mechanism.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，注意力是一种创建输入的上下文相关摘要的机制。对于基于 RNN 的 Seq2Seq 模型，输入是编码器隐藏状态，而上下文是解码器隐藏状态。Transformer
    的核心思想，自注意力，也创建了输入的摘要，除了一个关键的区别——创建摘要的上下文也是输入本身。请参见图 8.4，了解自注意力机制的简化示例。
- en: '![CH08_F04_Hagiwara](../Images/CH08_F04_Hagiwara.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F04_Hagiwara](../Images/CH08_F04_Hagiwara.png)'
- en: Figure 8.4 Self-attention transforms the input into summaries.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 自注意力将输入转化为摘要。
- en: 'Why is this a good thing? Why does it even work? As we discussed in chapter
    4, RNNs can also create a summary of the input by looping over the input tokens
    while updating an internal variable (hidden states). This works—we previously
    saw that RNNs can generate good translations when combined with attention, but
    they have one critical issue: because RNNs process the input sequentially, it
    becomes progressively more difficult to deal with long-range dependencies between
    tokens as the sentence gets longer.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很好？为什么它有效？正如我们在第4章中讨论的那样，RNN也可以通过循环遍历输入标记并更新内部变量（隐藏状态）来创建输入的摘要。这是有效的-我们之前看到当RNN与注意力结合时可以生成良好的翻译，但是它们有一个关键问题：因为RNN按顺序处理输入，随着句子变得越来越长，处理标记之间的远程依赖关系变得越来越困难。
- en: Let’s look at a concrete example. If the input sentence is “The Law will never
    be perfect, but its application should be just,” understanding what the pronoun
    “its” refers to (“The Law”) is important for understanding what the sentence means
    and for any subsequent tasks (such as translating the sentence accurately). However,
    if you use an RNN to encode this sentence, to learn this coreference relationship,
    the RNN needs to learn to remember the noun “The Law” in the hidden states first,
    then wait until the loop encounters the target pronoun (“its”) while learning
    to ignore everything unrelated in between. This sounds like a complicated trick
    for a neural network to learn.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子。如果输入句子是“The Law will never be perfect, but its application should
    be just”，了解代词“its”指的是什么（“The Law”）对于理解句子的含义以及任何后续任务（如准确翻译句子）都很重要。然而，如果您使用RNN来编码这个句子，要学习这个代词的共指关系，RNN需要先学习在隐藏状态中记住名词“The
    Law”，然后等待循环遇到目标代词（“its”），同时学会忽略之间的所有无关内容。对于神经网络来说，这听起来像是一种复杂的技巧。
- en: But things shouldn’t be that complicated. Singular possessive pronouns like
    “its” usually refer to their nearest singular nouns that appear before them, regardless
    of the words in between, so simple rules like “replace it with the nearest noun
    that appeared before” will suffice. In other words, such “random access” is better
    suited in this situation than “sequential access” is. Self-attention is better
    at learning such long-range dependencies, as we’ll see later.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但事情不应该那么复杂。像“its”这样的单数所有格代词通常指的是它们前面最近的单数名词，而与它们之间的词无关，因此简单的规则“用最近出现的名词替换它”就足够了。换句话说，在这种情况下，“随机访问”比“顺序访问”更适合。自注意力更擅长学习这种远程依赖关系，稍后我们将会看到。
- en: Let’s walk through how self-attention works with an example. Let’s assume we
    are translating Spanish into English and would like to encode the first few words,
    “Maria no daba,” in the input sentence. Let’s also focus on one specific token,
    “no,” and how its embeddings are computed from the entire input. The first step
    is to compare the target token against all tokens in the input. Self-attention
    does this by converting the target into a query by using projection W[Q] as well
    as converting all the tokens into keys using projection W[K] and computing attention
    weights using function f. The attention weights computed by f are normalized and
    converted to a probability distribution by the softmax function. Figure 8.5 illustrates
    these steps where attention weights are computed. As with the encoder-decoder
    attention mechanism we covered in section 8.2.1, these weights determine how to
    “mix” values we obtained from the input tokens. For words like “its,” we expect
    that the weight will be higher for related words such as “Law” in the example
    shown earlier.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来了解自注意力是如何工作的。假设我们要将西班牙语翻译成英语，并且想要编码输入句子中的前几个单词“Maria no daba”。我们还将关注一个特定的标记“no”，以及如何从整个输入计算其嵌入。第一步是将目标标记与输入中的所有标记进行比较。自注意力通过使用投影W[Q]将目标转换为查询，使用投影W[K]将所有标记转换为键，并使用函数f计算注意力权重来完成这一步骤。由f计算得到的注意力权重通过softmax函数进行归一化和转换为概率分布。图8.5说明了这些步骤，注意力权重如何计算。与我们在8.2.1节中涵盖的编码器-解码器注意力机制一样，注意力权重决定了我们从输入标记中获得的值如何“混合”。对于像“its”这样的词，我们希望相关词的权重会更高，比如之前的例子中的“Law”。
- en: '![CH08_F05_Hagiwara](../Images/CH08_F05_Hagiwara.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F05_Hagiwara](../Images/CH08_F05_Hagiwara.png)'
- en: Figure 8.5 Computing attention weights from keys and queries
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 从键和查询计算注意力权重
- en: In the next step, the vector corresponding to each input token is converted
    to a value vector by projection W[V]. Each projected value is weighted by the
    corresponding attention weight and is summed up to produce a summary vector. See
    figure 8.6 for an illustration.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，将每个输入令牌对应的向量通过投影 W[V] 转换为值向量。每个投影值都由相应的注意权重加权，并加总以生成摘要向量。请参见图 8.6 进行说明。
- en: '![CH08_F06_Hagiwara](../Images/CH08_F06_Hagiwara.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F06_Hagiwara](../Images/CH08_F06_Hagiwara.png)'
- en: Figure 8.6 Calculating the sum of all values weighted by attention weights
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 计算所有值的加权和
- en: This would be it if this were the “regular” encoder-decoder attention mechanism.
    You need only one summary vector per each token during decoding. However, one
    key difference between encoder-decoder attention and self-attention is the latter
    repeats this process for every single token in the input. As shown in figure 8.7,
    this produces a new set of embeddings for the input, one for each token.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是“常规”的编码器-解码器注意机制，那就是这样了。在解码期间，每个令牌只需要一个摘要向量。然而，编码器-解码器注意力和自注意力之间的一个关键区别是后者会针对输入中的每个令牌重复此过程。如图
    8.7 所示，这会为输入产生一组新的嵌入，每个令牌一个。
- en: '![CH08_F07_Hagiwara](../Images/CH08_F07_Hagiwara.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F07_Hagiwara](../Images/CH08_F07_Hagiwara.png)'
- en: Figure 8.7 Producing summaries for the entire input sequence (details are omitted)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 为整个输入序列生成摘要（细节被省略）
- en: Each summary produced by self-attention takes all the tokens in the input sequence
    into consideration, but with different weights. It is, therefore, straightforward
    for words like “its” to incorporate some information from related words, such
    as “The Law,” no matter how far apart these two words are. Using an analogy, self-attention
    produces summaries through random access over the input. This is in contrast to
    RNNs, which allow only sequential access over the input, and is one of the key
    reasons why the Transformer is such a powerful model for encoding and decoding
    natural language text.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力产生的每个摘要都考虑了输入序列中的所有令牌，但权重不同。因此，对于诸如“its”之类的词，它可以融入一些来自相关词语的信息，例如“法律”，无论这两个词有多远。使用类比，自注意力通过对输入进行随机访问产生摘要。这与RNN形成对比，后者只允许对输入进行顺序访问，并且这也是
    Transformer 之所以是编码和解码自然语言文本的强大模型之一的关键原因之一。
- en: We need to cover one final piece of detail to fully understand self-attention.
    As it is, the self-attention mechanism illustrated previously can use only one
    aspect of the input sequence to generate summaries. For example, if you want self-attention
    to learn which word each pronoun refers to, it can do that—but you may also want
    to “mix in” information from other words based on some other linguistic aspects.
    For example, you may want to refer to some other words that the pronoun modifies
    (“applications,” in this case). The solution is to have multiple sets of keys,
    values, and queries per token and compute multiple sets of attention weights to
    “mix” values that focus on different aspects of the input. The final embeddings
    are a combination of summaries generated this way. This mechanism is called *multihead
    self-attention* (figure 8.8).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要解释自注意力的最后一个细节才能完全理解它。现在，前面介绍的自注意机制只能使用输入序列的一个方面来生成摘要。例如，如果您希望自注意力学习每个代词指代哪个单词，它可以做到这一点——但您也可能希望根据其他一些语言学方面“混合”其他单词的信息。例如，您可能希望参考代词修改的其他单词（在这种情况下是“应用”）。解决方案是为每个令牌计算多组密钥、值和查询，并计算多组注意权重以“混合”关注不同输入方面的值。最终的嵌入是以这种方式生成的摘要的组合。这种机制被称为*多头自注意力*（图
    8.8）。
- en: '![CH08_F08_Hagiwara](../Images/CH08_F08_Hagiwara.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F08_Hagiwara](../Images/CH08_F08_Hagiwara.png)'
- en: Figure 8.8 Multihead self-attention produces summaries with multiple keys, values,
    and queries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 多头自注意力生成具有多个密钥、值和查询的摘要。
- en: You would need to learn some additional details if you were to fully understand
    how a Transformer layer works, but this section has covered the most important
    concepts. If you are interested in more details, check out *The Illustrated Transformer*
    *(*[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)),
    a well-written guide for understanding the Transformer model with easy-to-understand
    illustrations. Also, if you are interested in implementing the Transformer model
    from scratch in Python, check out “The Annotated Transformer” ([http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要完全理解 Transformer 层的工作原理，你需要学习一些额外的细节，但本节已经涵盖了最重要的概念。如果你对更多细节感兴趣，请查看*《图解Transformer》*（[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)），这是一个写得很好的指南，用易于理解的插图解释了
    Transformer 模型。此外，如果你有兴趣用 Python 从零开始实现 Transformer 模型，请查看*《注释版Transformer》*（[http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)）。
- en: 8.3.2 Transformer
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 Transformer
- en: The Transformer model doesn’t just use a single step of self-attention to encode
    or decode natural language text. It applies self-attention repeatedly to the inputs
    to gradually transform them. As with multilayer RNNs, the Transformer also groups
    a series of transformation operations into a layer and applies it repeatedly.
    Figure 8.9 shows one layer of the Transformer encoder.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型不仅仅使用单步自注意力来编码或解码自然语言文本。它重复应用自注意力到输入中，逐渐转换它们。与多层 RNN 一样，Transformer
    还将一系列转换操作分组到一个层中，并重复应用它。图 8.9 显示了 Transformer 编码器的一个层。
- en: A lot is going on within each layer, and it’s not our goal to explain every
    bit of its detail—you need to understand only that the multihead self-attention
    is at its core, followed by transformation by a feed-forward neural network (“FF”
    in figure 8.9). Residual connections and normalization layers are introduced to
    make it easier to train the model, although the details of these operations are
    outside the scope of this book. The Transformer model applies this layer repeatedly
    to transform the input from something literal (raw word embeddings) to something
    more abstract (the “meaning” of the sentence). In the original Transformer paper,
    Vaswani et al. used six layers for machine translation, although it is not uncommon
    for larger models to use 10-20 layers these days.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每个层内都有很多操作，我们的目标不是解释每一个细节——你只需要理解多头自注意力是其核心，后跟通过前馈神经网络的转换（图 8.9 中的“FF”）。引入了残差连接和归一化层，以使模型更容易训练，尽管这些操作的细节超出了本书的范围。Transformer
    模型反复应用这个层，将输入从文字的形式（原始词嵌入）转换为更抽象的东西（句子的“含义”）。在原始的 Transformer 论文中，Vaswani 等人用了六层进行机器翻译，尽管如今更大的模型通常使用
    10-20 层。
- en: '![CH08_F09_Hagiwara](../Images/CH08_F09_Hagiwara.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F09_Hagiwara](../Images/CH08_F09_Hagiwara.png)'
- en: Figure 8.9 A Transformer encoder layer with self-attention and a feed-forward
    layer
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 一个具有自注意力和前馈层的 Transformer 编码器层
- en: At this point, you may have noticed that the self-attention operation is completely
    independent of positions. In other words, the embedded results of self-attention
    would be completely identical even if, for example, we flipped the word order
    between “Maria” and “daba,” because the operation looks only at the word itself
    and the aggregated embeddings from other words, regardless of where they are.
    This is obviously very limiting—what a natural language sentence means depends
    a lot on how its words are ordered. How does the Transformer encode word order,
    then?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一步，你可能已经注意到自注意力操作完全独立于位置。换句话说，即使我们颠倒“Maria”和“daba”之间的单词顺序，自注意力的嵌入结果也完全相同，因为该操作只关注单词本身和来自其他单词的聚合嵌入，而不考虑它们的位置。这显然非常限制——自然语言句子的意义很大程度上取决于单词的顺序。那么，Transformer
    如何编码单词顺序呢？
- en: The Transformer model solves this problem by generating some artificial embeddings
    that differ from position to position and adding them to word embeddings before
    they are fed to the layers. These embeddings, called *positional encoding* *and
    shown in figure 8.10*, are either generated by some mathematical function (such
    as sine curves) or learned during training per position. This way, the Transformer
    can distinguish between “Maria” at the first position and “Maria” at the third
    position, because they have different positional encoding.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型通过生成一些人工嵌入来解决这个问题，这些嵌入在位置之间不同，并在将它们馈送到层之前添加到词嵌入中。这些嵌入被称为*位置编码*，*如图8.10所示*，可以由某些数学函数（如正弦曲线）生成，或者在训练过程中根据位置学习。这样，Transformer可以区分第一个位置的“Maria”和第三个位置的“Maria”，因为它们具有不同的位置编码。
- en: '![CH08_F10_Hagiwara](../Images/CH08_F10_Hagiwara.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F10_Hagiwara](../Images/CH08_F10_Hagiwara.png)'
- en: Figure 8.10 Adding positional encoding to the input to represent word order
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 将位置编码添加到输入中以表示词序
- en: Figure 8.11 shows the Transformer decoder. Although a lot is going on, make
    sure to notice two important things. First, you’ll notice one extra mechanism
    called cross-attention inserted between the self-attention and feed-forward networks.
    This cross-attention mechanism is similar to the encoder-decoder attention mechanism
    we covered in section 8.2\. This works exactly the same as self-attention, except
    that the values for the attention come from the encoder, not the decoder, summarizing
    the information extracted from the encoder.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 显示了Transformer解码器。虽然很多事情正在进行，但一定要注意两个重要的事情。首先，你会注意到一个额外的机制，称为交叉注意力，插入在自注意力和前馈网络之间。这个交叉注意力机制类似于我们在第8.2节介绍的编码器-解码器注意力机制。它的工作方式与自注意力完全相同，唯一的区别是注意力的值来自编码器，而不是解码器，总结了从编码器提取的信息。
- en: '![CH08_F11_Hagiwara](../Images/CH08_F11_Hagiwara.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F11_Hagiwara](../Images/CH08_F11_Hagiwara.png)'
- en: Figure 8.11 A Transformer decoder layer with self- and cross-attention
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 Transformer解码器层，具有自注意力和交叉注意力
- en: Finally, the Transformer model generates the target sentence in exactly the
    same way as RNN-based Seq2Seq models we’ve previously learned in section 6.4\.
    The decoder is initialized by a special token <START> and produces a probability
    distribution over possible next tokens. From here, you can proceed by choosing
    the token with the maximum probability (greedy decoding, as shown in section 6.4.3)
    or keeping a few tokens with the highest probability while searching for the path
    that maximizes the total score (beam search, as shown in section 6.4.4). In fact,
    if you look at the Transformer decoder as a black box, the way it produces the
    target sequence is exactly the same as RNNs, and you can use the same set of decoding
    algorithms. In other words, the decoding algorithms covered in section 6.4 are
    generic ones that are agnostic of the underlying decoder architecture.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Transformer模型以与我们之前在第6.4节学习的基于RNN的Seq2Seq模型完全相同的方式生成目标句子。解码器由特殊标记<START>初始化，并生成可能的下一个标记的概率分布。从这里，你可以选择具有最大概率的标记（贪婪解码，如第6.4.3节所示），或者在寻找最大化总分数的路径时保留一些具有最高概率的标记（波束搜索，如第6.4.4节所示）。事实上，如果你把Transformer解码器看作一个黑匣子，它生成目标序列的方式与RNN完全相同，你可以使用相同的一组解码算法。换句话说，第6.4节介绍的解码算法是一种通用的算法，不受底层解码器架构的影响。
- en: 8.3.3 Experiments
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 实验
- en: 'Now that we know how the Transformer model works, let’s build a machine translation
    system with it. The good news is the sequence-to-sequence toolkit, Fairseq, already
    supports the Transformer-based models (along with other powerful models), which
    can be specified by the —arch transformer option when you train the model. Assuming
    that you have already preprocessed the dataset we used to build the Spanish-to-English
    machine translation, you need to tweak only the parameters you give to fairseq-train,
    as shown next:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了Transformer模型的工作原理，让我们用它构建一个机器翻译系统。好消息是，序列到序列工具包Fairseq已经支持基于Transformer的模型（以及其他强大的模型），可以在训练模型时通过`--arch
    transformer`选项指定。假设你已经预处理了我们用于构建西班牙语到英语机器翻译的数据集，你只需要调整给予`fairseq-train`的参数，如下所示：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that this might not even run on your laptop. You really need GPUs to train
    the Transformer models. Also note that training can take hours even with GPUs.
    See section 11.5 for more information on using GPUs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这甚至可能在你的笔记本电脑上都无法运行。你真的需要GPU来训练Transformer模型。还要注意，即使有GPU，训练也可能需要几个小时。更多关于使用GPU的信息请参见第11.5节。
- en: A number of cryptic parameters appear here, but you don’t need to worry about
    them. You can see the model structure when you run this command. The entire model
    dump is quite long, so we are omitting some intermediate layers in listing 8.1\.
    If you look carefully, you’ll see that the structure of the layers corresponds
    to the figures we showed earlier.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现了一些神秘的参数，但您不需要担心。当您运行此命令时，您可以看到模型结构。整个模型转储相当长，因此我们在清单8.1中省略了一些中间层。仔细观察，您会发现层次结构与我们之前显示的图形相对应。
- en: Listing 8.1 Transformer model dump from Fairseq
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 清单8.1 Fairseq的Transformer模型转储
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Self-attention of the encoder
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 编码器的自注意力
- en: ❷ Feed-forward network of the encoder
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码器的前馈网络
- en: ❸ Self-attention of the decoder
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 解码器的自注意力
- en: ❹ Encoder-decoder of the decoder
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 解码器的编码器-解码器
- en: ❺ Feed-forward network of the decoder
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 解码器的前馈网络
- en: 'When I ran this, the validation loss converges after around epoch 30, at which
    point you can stop the training. The result I got by translating the same set
    of Spanish sentences into English follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行时，验证损失在大约第30个时期后收敛，此时您可以停止训练。我将同一组西班牙语句子翻译成英文的结果如下：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can see most of these English translations here are almost perfect. It is
    quite surprising that the model translated the most difficult sentence (“Maria
    no daba . . .”) almost perfectly. This is probably enough to convince us that
    the Transformer is a powerful translation model. After its advent, this model
    became the de facto standard in research and commercial machine translation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数英文翻译几乎完美。令人惊讶的是，模型几乎完美地翻译了最困难的句子（“Maria no daba . . .”）。这可能足以说服我们，Transformer是一个强大的翻译模型。在它的出现之后，这个模型成为了研究和商业机器翻译的事实标准。
- en: 8.4 Transformer-based language models
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 基于Transformer的语言模型
- en: In section 5.5, we introduced language models, which are statistical models
    that give a probability to a piece of text. By decomposing text into a sequence
    of tokens, language models can estimate how “probable” the given text is. In section
    5.6, we demonstrated that by leveraging this property, language models can also
    be used to generate new texts out of thin air!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5.5节中，我们介绍了语言模型，这是一种给文本赋予概率的统计模型。通过将文本分解为令牌序列，语言模型可以估计给定文本的“概率”。在第5.6节中，我们演示了通过利用这一特性，语言模型也可以用于“凭空”生成新的文本！
- en: The Transformer is a powerful model that achieves impressive results in Seq2Seq
    tasks (such as machine translation), although its architecture can also be used
    for modeling and generating language. In this section, we learn how to use the
    Transformer for modeling language and generating realistic texts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一个强大的模型，在Seq2Seq任务（如机器翻译）中取得了令人印象深刻的结果，尽管它的架构也可以用于语言建模和生成。在本节中，我们将学习如何使用Transformer进行语言建模和生成真实文本。
- en: 8.4.1 Transformer as a language model
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 Transformer作为语言模型
- en: In section 5.6, we built a language-generation model based on a character LSTM-RNN.
    To recap, given a prefix (a partial sentence generated so far), the model uses
    an LSTM-based RNN (a neural network with a loop) to produce a probability distribution
    over possible next tokens, as shown in figure 8.12.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5.6节中，我们建立了基于字符LSTM-RNN的语言生成模型。简而言之，给定一个前缀（到目前为止生成的部分句子），模型使用基于LSTM的RNN（一个带有循环的神经网络）来生成可能的下一个令牌的概率分布，如图8.12所示。
- en: '![CH08_F12_Hagiwara](../Images/CH08_F12_Hagiwara.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F12_Hagiwara](../Images/CH08_F12_Hagiwara.png)'
- en: Figure 8.12 Generating text using an RNN
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 使用RNN生成文本
- en: We noted earlier that, by regarding the Transformer decoder as a black box,
    you can use the same set of decoding algorithms (greedy, beam search, and so on)
    as we introduced earlier for RNNs. This is also the case for language generation—by
    thinking of the neural network as a black box that produces some sort of score
    given a prefix, you can use the same logic to generate texts, no matter the underlying
    model. Figure 8.13 shows how an architecture similar to the Transformer can be
    used for language generation. Except for a few minor differences (such as lack
    of cross-attention), the structure is almost identical to the Transformer decoder.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们早些时候指出，通过将Transformer解码器视为黑匣子，您可以使用与我们之前介绍的RNN相同的一组解码算法（贪婪、束搜索等）。对于语言生成也是如此——通过将神经网络视为在给定前缀的情况下产生某种分数的黑匣子，您可以使用相同的逻辑生成文本，而不管底层模型如何。图8.13显示了类似Transformer的架构如何用于语言生成。除了一些细微差别（如缺乏交叉注意力）之外，结构几乎与Transformer解码器相同。
- en: '![CH08_F13_Hagiwara](../Images/CH08_F13_Hagiwara.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F13_Hagiwara](../Images/CH08_F13_Hagiwara.png)'
- en: Figure 8.13 Using the Transformer for language generation
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 使用 Transformer 进行语言生成
- en: 'The following snippet shows Python-like pseudocode for generating text with
    the Transformer model. Here, model() is the main function where the model computation
    happens—it takes the tokens, converts them to embeddings, adds positional encoding,
    and passes them through all the Transformer layers, returning the final hidden
    states back to the caller. The caller then passes them through a linear layer
    to convert them to logits, which in turn get converted to a probability distribution
    by softmax:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下片段显示了使用 Transformer 模型生成文本的类似 Python 的伪代码。在这里，model() 是主要的函数，模型计算发生在这里——它接受标记，将它们转换为嵌入，添加位置编码，并将它们传递到所有的
    Transformer 层，将最终的隐藏状态返回给调用者。调用者然后将它们通过线性层传递，将它们转换为 logits，然后通过 softmax 转换为概率分布：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In fact, decoding for Seq2Seq models and language generation with language models
    are very similar tasks, where the output sequence is produced token by token,
    feeding itself back to the network, as shown in the previous code snippet. The
    only difference is that the former has some form of input (the source sentence)
    whereas the latter does not (the model feeds itself). These two tasks are also
    called *unconditional* and *conditional generation*, respectively. Figure 8.14
    illustrates these three components (network, task, and decoding) and how they
    can be combined to solve a specific problem.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Seq2Seq 模型的解码和语言模型的语言生成是非常相似的任务，输出序列是逐标记生成的，将自身反馈给网络，就像前面的代码片段所示。唯一的区别在于，前者有某种形式的输入（源句子），而后者没有（模型自我反馈）。这两个任务也分别称为*无条件生成*和*有条件生成*。图
    8.14 描绘了这三个组件（网络、任务和解码）以及它们如何结合起来解决特定问题。
- en: '![CH08_F14_Hagiwara](../Images/CH08_F14_Hagiwara.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F14_Hagiwara](../Images/CH08_F14_Hagiwara.png)'
- en: Figure 8.14 Three components of language generation and Seq2Seq tasks
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 语言生成和 Seq2Seq 任务的三个组件
- en: In the rest of this section, we are going to experiment with some Transformer-based
    language models and generate natural language texts using them. We’ll be using
    the transformers library ([https://huggingface.co/transformers/](https://huggingface.co/transformers/))
    developed by Hugging Face, which has become a standard, go-to library for NLP
    researchers and engineers working with Transformer models in the past few years.
    It comes with a number of state-of-the-art model implementations including GPT-2
    (this section) and BERT (next chapter), along with pretrained model parameters
    that you can load and use right away. It also provides a simple, consistent interface
    through which you can interact with powerful NLP models.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将尝试使用一些基于 Transformer 的语言模型，并使用它们生成自然语言文本。我们将使用由 Hugging Face 开发的
    transformers 库（[https://huggingface.co/transformers/](https://huggingface.co/transformers/)），这个库在过去几年已经成为了
    NLP 研究人员和工程师使用 Transformer 模型的标准库。它提供了一些最先进的模型实现，包括 GPT-2（本节）和 BERT（下一章），以及预训练模型参数，您可以立即加载和使用。它还提供了一个简单、一致的接口，通过这个接口您可以与强大的
    NLP 模型进行交互。
- en: 8.4.2 Transformer-XL
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 Transformer-XL
- en: In many cases, you want to load and use pretrained models provided by third
    parties (most often the developer of the model), instead of training them from
    scratch. Recent Transformer models are fairly complex (usually with hundreds of
    millions of parameters) and are trained with huge datasets (tens of gigabytes
    of text). This would require GPU resources that only large institutions and tech
    giants can afford. It is not completely uncommon that some of these models take
    days to train, even with more than a dozen GPUs! The good news is the implementation
    and pretrained model parameters for these huge Transformer models are usually
    made publicly available by their creators so that anyone can integrate them into
    their NLP applications.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，您希望加载并使用由第三方提供的预训练模型（通常是模型的开发者），而不是从头开始训练它们。最近的 Transformer 模型相当复杂（通常具有数亿个参数），并且使用大量的数据集进行训练（数十吉字节的文本）。这将需要只有大型机构和科技巨头才能承受得起的
    GPU 资源。甚至有些模型在训练时需要数天的时间，即使有十几个 GPU！好消息是，这些庞大的 Transformer 模型的实现和预训练模型参数通常由它们的创建者公开提供，以便任何人都可以将它们集成到他们的
    NLP 应用程序中。
- en: In this section, we’ll first check out Transformer-XL, a variant of the Transformer
    developed by the researchers at Google Brain. Because there is no inherent “loop”
    in the original Transformer model, unlike RNNs, the original Transformer is not
    good at dealing with super-long context. In training language models with the
    Transformer, you first split long texts into shorter chunks of, say, 512 words,
    and feed them to the model separately. This means the model is unable to capture
    dependencies longer than 512 words. Transformer-XL[⁴](#pgfId-1107292) addresses
    this issue by making a few improvements over the vanilla Transformer model (“XL”
    means extra-long). Although the details of these changes are outside the scope
    of this book, in a nutshell, the model reuses its hidden states from the previous
    segment, effectively creating a loop that passes information between different
    segments of texts. It also improves the positional encoding scheme we touched
    on earlier to make it easier for the model to deal with longer texts.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们首先将介绍Transformer-XL，这是由Google Brain的研究人员开发的Transformer的一个变种。由于原始的Transformer模型中没有固有的“循环”，不像RNNs，所以原始的Transformer不擅长处理超长的上下文。在用Transformer训练语言模型时，你首先将长文本分割成较短的块，比如512个单词，并将它们分别馈送到模型中。这意味着模型无法捕获超过512个单词的依赖关系。Transformer-XL[⁴](#pgfId-1107292)通过对原始Transformer模型进行一些改进来解决这个问题（“XL”表示额外长）。尽管这些改变的细节超出了本书的范围，在简单地说，该模型重复使用前一个段落的隐藏状态，有效地创建了一个在不同文本段之间传递信息的循环。它还改进了我们之前提到的位置编码方案，使得模型更容易处理更长的文本。
- en: You can install the transformers library just by running pip install transformers
    from the command line. The main abstractions you’ll be interacting with are tokenizers
    and models. The tokenizers split a raw string into a sequence of tokens, whereas
    the model defines the architecture and implements the main logic. The model and
    the pretrained weights usually depend on a specific tokenization scheme, so you
    need to make sure you are using the tokenizer that is compatible with the model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需在命令行中运行pip install transformers即可安装transformers库。您将与主要抽象进行交互的是分词器和模型。分词器将原始字符串拆分为一系列标记，而模型定义了架构并实现了主要逻辑。模型和预训练权重通常取决于特定的标记化方案，因此您需要确保您使用的分词器与模型兼容。
- en: 'The easiest way to initialize a tokenizer and a model with some specified pretrained
    weights is use the AutoTokenizer and AutoModelWithLMHead classes and call their
    from_pretrained() methods as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个分词器和一个模型，并使用一些指定的预训练权重的最简单方法是使用AutoTokenizer和AutoModelWithLMHead类，并调用它们的from_pretrained()方法如下所示：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The parameter to from_pre-trained() is the name of the model/pretrained weights.
    This is a Transformer-XL model trained on a dataset called wt103 (WikiText103).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: from_pre-trained()函数的参数是模型/预训练权重的名称。这是一个在名为wt103（WikiText103）的数据集上训练的Transformer-XL模型。
- en: You may be wondering what this “LMHead” part in AutoModelWithLMHead means. An
    LM (language model) head is a specific layer added to a neural network that converts
    its hidden states to a set of scores that determine which tokens to generate next.
    These scores (also called logits) are then fed to a softmax layer to obtain a
    probability distribution over possible next tokens (figure 8.15). We would like
    a model with an LM head because we are interested in generating text by using
    the Transformer as a language model. However, depending on the task, you may also
    want a Transformer model without an LM head and just want to use its hidden states.
    That’s what we’ll do in the next chapter.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道AutoModelWithLMHead中的“LMHead”部分是什么意思。LM（语言模型）头是添加到神经网络中的特定层，它将其隐藏状态转换为一组分数，这些分数确定要生成的下一个标记。然后，这些分数（也称为logits）被馈送到softmax层以获得可能的下一个标记的概率分布（图8.15）。我们希望一个带有LM头的模型，因为我们有兴趣通过将Transformer作为语言模型来生成文本。但是，根据任务的不同，您可能还想要一个没有LM头的Transformer模型，并且只想使用其隐藏状态。这将是我们在下一章中要做的事情。
- en: '![CH08_F15_Hagiwara](../Images/CH08_F15_Hagiwara.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F15_Hagiwara](../Images/CH08_F15_Hagiwara.png)'
- en: Figure 8.15 Using a language model head with the Transformer
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 使用Transformer的语言模型头
- en: 'The next step is to initialize the prefix for which you would like your language
    model to write the rest of the story. You can use tokenizer.encode() to convert
    a string into a list of token IDs, which are then converted to a tensor. We’ll
    also initialize a variable past for caching the internal states and making the
    inference faster, as shown next:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是初始化前缀，用于让语言模型生成故事的其余部分。可以使用 tokenizer.encode() 方法将字符串转换为标记 ID 列表，然后将其转换为张量。我们还将初始化变量
    past，用于缓存内部状态并加速推理过程，如下所示：
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now you are ready to generate the rest of the text. Notice the next code is
    similar to the pseudocode we showed earlier. The idea is simple: get the output
    from the model, sample a token using the output, and feed it back to the model.
    Rinse and repeat.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已准备好生成文本的其余部分了。请注意，下面的代码与我们之前显示的伪代码相似。思路很简单：从模型获取输出，使用输出随机采样一个标记，并将其输入模型。反复这个过程。
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You need to do some housekeeping to make the shape of the tensors compatible
    with the model, which we can ignore for now. The sample_token() method here takes
    the output of the model, converts it to a probability distribution, and samples
    a single token from it. I’m not showing the entire code for the method, but you
    can check the Google Colab notebook ([http://realworldnlpbook.com/ch8.html#xformer-nb](http://realworldnlpbook.com/ch8.html#xformer-nb))
    for more details. Also, here we wrote the generation algorithm from scratch, but
    if you need more full-fledged generation (such as beam search), check out the
    official example script from the developers of the library: [http://mng.bz/wQ6q](http://mng.bz/wQ6q).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进行一些清理工作，以使张量的形状与模型兼容，我们暂时可以忽略此步骤。此处的 sample_token() 方法将模型的输出转换为概率分布，并从中随机采样一个标记。我没有显示该方法的完整代码，但您可以查看
    Google Colab 笔记本（[http://realworldnlpbook.com/ch8.html#xformer-nb](http://realworldnlpbook.com/ch8.html#xformer-nb)）了解更多细节。此外，虽然我们在此处从零编写了生成算法，但如果您需要更全面的生成方式（如波束搜索），请查看该库开发者的官方示例脚本：[http://mng.bz/wQ6q](http://mng.bz/wQ6q)。
- en: 'After finishing the generation, you can convert the token IDs back into a raw
    string by calling tokenizer.decode()as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成完成后，您可以通过调用 tokenizer.decode() 将标记 ID 转换为原始字符串，如下所示：
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following “story” is what I got when I ran this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后我得到了以下“故事”：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is not a bad start. I like the way the story is trying to be consistent
    by sticking with the concept of “group.” However, because the model is trained
    on Wikipedia text only, its generation is not realistic and looks a little bit
    too formal.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个坏的开始。我喜欢这个故事试图通过坚持“群体”概念来保持一致性的方式。然而，由于该模型仅训练于维基百科文本，其生成的结果并不真实，看起来有点过于正式。
- en: 8.4.3 GPT-2
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 GPT-2
- en: GPT-2 (which stands for generative pretraining), developed by OpenAI, is probably
    the most famous language model to date. You may have heard the story about a language
    model generating natural language texts that are so realistic that you cannot
    tell them from those written by humans. Technically, GPT-2 is just a huge Transformer
    model, just like the one we introduced earlier. The main difference is its size
    (the largest model has 48 layers!) and the fact that the model is trained on a
    huge amount of natural language text collected from the web. The OpenAI team publicly
    released the implementation and the pretrained weights, so we can easily try out
    the model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2（代表生成预训练）是由 OpenAI 开发的迄今为止最著名的语言模型。你可能听说过关于一种语言模型生成如此真实无缝的自然语言文本，以至于你无法分辨其与人类写作的文本。从技术上讲，GPT-2
    只是一个庞大的 Transformer 模型，就像我们之前介绍的那个一样。主要区别在于其规模（最大模型有 48 层！）以及该模型是通过从网络上收集到的大量自然语言文本进行训练的。OpenAI
    团队公开发布了实现和预训练权重，因此我们可以轻松尝试这个模型。
- en: 'Initialize the tokenizer and the model for GPT-2 as you have done for Transformer-XL,
    as shown next:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化标记器和 GPT-2 模型，方法与 Transformer-XL 相同，如下所示：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then generate text using the next code snippet:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下代码片段生成文本：
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You may have noticed how little this code snippet changed from the one for Transformer-XL.
    In many cases, you don’t need to make any modifications when switching between
    different models. This is why the transformers library is so powerful—you can
    try out and integrate a variety of state-of-the-art Transformer-based models into
    your application with a simple, consistent interface. As we’ll see in the next
    chapter, this library is also integrated into AllenNLP, which makes it easy to
    build powerful NLP applications with state-of-the-art models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到这段代码与Transformer-XL的代码几乎没有变化。在许多情况下，当切换不同的模型时，您不需要进行任何修改。这就是为什么transformers库如此强大的原因
    - 您可以尝试并集成各种最先进的基于Transformer的模型到您的应用程序中，只需使用一个简单且一致的界面。正如我们将在下一章中看到的那样，这个库还集成到AllenNLP中，这使得使用最先进的模型构建强大的自然语言处理应用程序变得容易。
- en: 'When I tried this, the GPT-2 generated the following beautifully written passage:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当我尝试这个代码时，GPT-2生成了以下精美的段落：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice how naturally it reads. Also, the GPT-2 model is good at staying consistent—you
    can see the name of the island, “A,” is consistently used throughout the passage.
    As far as I checked, there is no real island named A in the world, meaning that
    this is something the model simply made up. It is a great feat that the model
    remembered the name it just coined and successfully wrote a story around it!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意它和自然的阅读感。此外，GPT-2模型擅长保持一致性-您可以看到“A”这个岛的名字在整个段落中始终使用。就我所知，世界上没有一个真正名为A的岛屿，这意味着这是模型简单地编造的。这是一个伟大的成就，模型记住了它刚刚创造的名字，并成功围绕它写了一个故事！
- en: 'Here’s another passage that GPT-2 generated with a prompt: ''Real World Natural
    Language Processing'' is the name of the book:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是GPT-2根据提示生成的另一段话：'Real World Natural Language Processing'是这本书的名字：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As of February 2019, when GPT-2 was released, I had barely begun writing this
    book, so I doubt GPT-2 knew anything about it. For a language model that doesn’t
    have any prior knowledge about the book, this is an amazing job, although I have
    to note that it got the price and the name of the author wrong.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到2019年2月，当GPT-2发布时，我几乎刚开始写这本书，所以我怀疑GPT-2对此一无所知。对于一个没有关于这本书的任何先验知识的语言模型来说，这是一项惊人的工作，尽管我必须指出它价格和作者的错误。
- en: 8.4.4 XLM
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.4 XLM
- en: Finally, as an interesting example, we will experiment with multilingual language
    generation. XLM (cross-lingual language model), proposed by researchers at Facebook
    AI Research, is a Transformer-based cross-lingual language model that can generate
    and encode texts in multiple languages.[⁵](#pgfId-1107402) By learning how to
    encode multilingual texts, the model can be used for transfer learning between
    different languages. We’ll cover transfer learning in chapter 9.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为一个有趣的例子，我们将尝试多语言语言生成。XLM（跨语言语言模型）是由Facebook AI Research的研究人员提出的基于Transformer的跨语言语言模型，可以生成和编码多种语言的文本。通过学习如何编码多语言文本，模型可以用于不同语言之间的迁移学习。我们将在第9章介绍迁移学习。
- en: 'You can start by initializing the tokenizer and the model and initialize it
    with the pretrained weights as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下方式初始化分词器和模型，并使用预训练权重进行初始化：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we are loading an XLM model (xlm), trained with a *causal language modeling*
    (CLM) *objective* (clm) in English and French (enfr). CLM is just a fancier way
    to describe what we’ve been doing in this chapter—predicting the next token based
    on a prefix. XLM is usually used for encoding multilingual texts for some downstream
    tasks such as text classification and machine translation, but we are simply using
    it as a language model to generate texts. See listing 8.2 for the code snippet
    for generating multilingual text with XLM. You can again reuse most of the earlier
    code snippet, although you also need to specify what language you are working
    in (note the lang = 0 line). Also, here we are generating text from scratch by
    supplying just the BOS token (whose index is zero).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们加载一个使用英语和法语（enfr）进行训练的XLM模型（xlm），使用*因果语言模型*（CLM）*目标*（clm）进行训练。CLM只是以更高级的方式描述我们在本章中所做的内容-根据前缀预测下一个标记。XLM通常用于对多语言文本进行编码，用于一些下游任务，如文本分类和机器翻译，但我们只是将其用作生成文本的语言模型。有关使用XLM生成多语言文本的代码片段，请参见清单8.2。您可以再次重用大部分之前的代码片段，尽管您还需要指定您正在使用的语言（请注意lang
    = 0行）。此外，在这里，我们通过仅提供BOS标记（其索引为零）从头开始生成文本。
- en: Listing 8.2 Generating multilingual text with XLM
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 清单8.2 生成多语言文本与XLM
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When I ran this, I got the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个代码后，我得到了以下结果：
- en: '[PRE20]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, let’s change lang to 1 (which means French) and run the same snippet
    again, which gives you the next bit of text:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们将语言更改为1（表示法语），并再次运行相同的代码片段，这将给出下一段文本：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Although the quality of generation is not as great as GPT-2, which we experimented
    with earlier, it is refreshing to see that a single model can produce texts both
    in English and French. These days, it is increasingly common to build multilingual
    Transformer-based NLP models to solve NLP problems and tasks in multiple languages
    at the same time. This also became possible thanks to the Transformer’s powerful
    capacity to model the complexity of language.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种生成质量不如我们之前实验的GPT-2那么好，但是看到一种单一模型可以同时生成英语和法语的文本非常令人耳目一新。如今，构建基于Transformer的多语言NLP模型以解决多种语言的NLP问题和任务越来越普遍。这也得益于Transformer对语言复杂性建模的强大能力。
- en: '8.5 Case study: Spell-checker'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 案例研究：拼写检查器
- en: In the final section of this chapter, we will build a practical NLP application—a
    spell-checker—with the Transformer. In the modern world, spell-checkers are everywhere.
    Chances are your web browser is equipped with a spell-checker that tells you when
    you make a spelling mistake by underlining misspelled words. Many word processors
    and editors also run spell-checkers by default. Some applications (including Google
    Docs and Microsoft Word) even point out simple grammatical errors, too. Ever wondered
    how they work? We’ll learn how to formulate this as an NLP problem, prepare the
    dataset, train, and improve the model next.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们将使用Transformer构建一个实用的NLP应用——拼写检查器。在现代世界中，拼写检查器无处不在。你的Web浏览器可能装备有一个拼写检查器，它会在拼写错误的单词下划线提示你。许多字处理器和编辑器也默认运行拼写检查器。一些应用程序（包括Google
    Docs和Microsoft Word）甚至指出简单的语法错误。你是否想知道它们是如何工作的？我们将学习如何将其作为NLP问题进行规划、准备数据集、训练和改进模型。
- en: 8.5.1 Spell correction as machine translation
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 拼写纠正作为机器翻译
- en: 'Spell-checkers receive a piece of text such as “tisimptant too spll chck ths
    dcment,” detect spelling and grammatical errors, if any, and fix all errors: “It’s
    important to spell-check this document.” How can you solve this task with NLP
    technologies? How can such systems be implemented?'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写检查器接收这样一个文本：“tisimptant too spll chck ths dcment”，检测任何拼写和语法错误，并修复所有错误：“It's
    important to spell-check this document.” 如何使用自然语言处理技术解决这个任务？这些系统如何实现？
- en: The simplest thing you could do is tokenize the input text into words and check
    if each word is in a dictionary. If it’s not, you look for the closest valid word
    in the dictionary according to some measure such as the edit distance and replace
    with that word. You repeat this until there are no words to fix. This word-by-word
    fixing algorithm is widely used by many spell-checkers due to its simplicity.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是将输入文本分词为单词，并检查每个单词是否在字典中。如果不在，你可以查找距离最近的有效单词并替换它。可以使用一些度量（如编辑距离）来计算距离。重复这个过程，直到没有需要更正的单词。这种逐个单词修正的算法被许多拼写检查器广泛使用，因为它很简单。
- en: However, this type of spell-checker has several issues. First, just like the
    first word in the example, “tisimptant,” how do you know which part of the sentence
    is actually a word? The default spell-checker for my copy of Microsoft Word indicates
    it’s a misspelling of “disputant,” although it would be obvious to any English
    speakers that it is actually a misspelling of two (or more) words. The fact that
    users can also misspell punctuation (including whitespace) makes everything complicated.
    Second, just because some word is in a dictionary doesn’t mean it’s not an error.
    For example, the second word in the example, “too” is a misspelling of “to,” but
    both are valid words that are in any English dictionary. How can you tell if the
    former is wrong in this context? Third, all these decisions are made out of context.
    One of the spell-checkers I tried shows “thus” as a candidate to replace “ths”
    in this example. However, from this context (before a noun), it is obvious that
    “this” is a more appropriate candidate, although both “this” and “thus” are one
    edit distance away from “ths,” meaning they are equally valid options according
    to the edit distance.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: You would be able to solve some of these issues by adding some heuristic rules.
    For example, “too” is more likely a misspelling of “to” before a verb, and “this”
    is more likely before a noun than “thus.” But this method is obviously not scalable.
    Remember the poor junior developer from section 1.1.2? Language is vast and full
    of exceptions. You cannot just keep writing such rules to deal with the full complexity
    of language. Even if you are able to write rules for such simple words, how would
    you tell that “tisimptant” is actually two words? Would you try to split this
    word at every possible position to see if split words resemble existing words?
    What if the input was in a language that is written without whitespace, like Chinese
    and Japanese?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you may realize this “split and fix” approach is going nowhere.
    In general, when designing an NLP application, you should think in terms of the
    following three aspects:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '*Task*—What is the task being solved? Is it a classification, sequential-labeling,
    or sequence-to-sequence problem?'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model*—What model are you going to use? Is it a feed-forward network, an RNN,
    or the Transformer?'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dataset*—Where are you obtaining the dataset to train and validate your model?'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on my experience, a vast majority of NLP applications nowadays can be
    solved by combining these aspects. How about spell-checkers? Because they take
    a piece of text as the input and produce the fixed string, it’d be most straightforward
    if we solve this as a Seq2Seq task using the Transformer model. In other words,
    we will be building a machine translation system that translates noisy inputs
    with spelling/grammatical errors into clean, errorless outputs as shown in figure
    8.16\. You can regard these two sides as two different “languages” (or “dialects”
    of English).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![CH08_F16_Hagiwara](../Images/CH08_F16_Hagiwara.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 Training a spell-checker as an MT system that translates “noisy”
    sentences into “clean” ones
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may be wondering where we are obtaining the dataset. This
    is often the most important (and the most difficult) part in solving real-world
    NLP problems. Fortunately, we can use a public dataset for this task. Let’s dive
    in and start building a spell-checker.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.2 Training a spell-checker
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will be using GitHub Typo Corpus ([https://github.com/mhagiwara/github-typo
    -corpus](https://github.com/mhagiwara/github-typo-corpus)) as the dataset to train
    a spell-checker. The dataset, created by my collaborator and me, consists of hundreds
    of thousands of “typo” edits automatically harvested from GitHub. It is the largest
    dataset of spelling mistakes and their corrections to date, which makes it a perfect
    choice for training a spell-checker.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: One decision we need to make before preparing the dataset and training a model
    is what to use as the atomic linguistic unit on which the model operates. Many
    NLP models use tokens as the smallest unit (i.e., RNN/Transformer is fed a sequence
    of tokens), but a growing number of NLP models use *word or sentence pieces* as
    the basic units (section 10.4). What should we use as the smallest unit for spelling
    correction? As with many other NLP models, using words as the input sounds like
    a good “default” thing to do at first. However, as we saw earlier, the concept
    of tokens is not well suited for spelling correction—users can mess up with punctuation,
    which makes everything overly complex if you are dealing with tokens. More importantly,
    because NLP models need to operate on a fixed vocabulary, the spell-corrector
    vocabulary would need to include every single misspelling of every single word
    it encountered during the training. This would make it unnecessarily expensive
    to train and maintain such an NLP model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, we will be using *characters* as the basic unit for our spell-checker,
    as we did in section 5.6\. Using characters has several advantages—it can keep
    the size of the vocabulary quite small (usually less than one hundred for a language
    with a small set of alphabets such as English). You don’t need to worry about
    bloating your vocabulary, even with a noisy dataset full of typos, because typos
    are just different arrangements of characters. You can also treat punctuation
    marks (even whitespace) as one of the characters in the vocabulary. This makes
    the preprocessing step extremely easy because you don’t need any linguistic toolkits
    (such as tokenizers) for doing this.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Using characters is not without disadvantages. One main issue is using
    them will increase the length of sequences, because you need to break everything
    up into characters. This makes the model large and slower to train.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s prepare the dataset for training a spell-checker. All the necessary
    data and code for building a spell-checker are included in this repository: [https://github.com/mhagiwara/xfspell](https://github.com/mhagiwara/xfspell).
    The tokenized and split datasets are located under data/gtc (as train.tok.fr,
    train.tok.en, dev.tok.fr, dev.tok.en). The suffixes en and fr are a commonly used
    convention in machine translation—“fr” means “foreign language” and “en” means
    English, because many MT research projects were originally motivated by people
    wanting to translate some foreign language into English. Here, we are using “fr”
    and “en” to mean just “noisy text before spelling correction” and “clean text
    after spelling correction.”'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为训练拼写检查器准备数据集。构建拼写检查器所需的所有必要数据和代码都包含在此代码库中：[https://github.com/mhagiwara/xfspell](https://github.com/mhagiwara/xfspell)。经过分词和拆分的数据集位于
    data/gtc 目录下（如 train.tok.fr、train.tok.en、dev.tok.fr、dev.tok.en）。后缀 en 和 fr 是机器翻译中常用的约定，其中“fr”表示“外语”，“en”表示英语，因为许多机器翻译研究项目最初是由希望将某种外语翻译为英语的人发起的。这里，我们将“fr”和“en”仅仅解释为“拼写纠错前的嘈杂文本”和“拼写纠错后的纠正文本”。
- en: Figure 8.17 shows an excerpt from the dataset for spelling correction created
    from GitHub Typo Corpus. Notice that text is segmented into individual characters,
    even whitespaces (replaced by “_”). Any characters outside common alphabets (upper-
    and lowercase letters, numbers, and some common punctuation marks) are replaced
    with “#.” You can see that the dataset contains diverse corrections, including
    simple typos (pubilc -> public on line 670, HYML -> HTML on line 672), trickier
    errors (mxnet as not -> mxnet is not on line 681, 22th -> 22nd on line 682), and
    even lines without any corrections (line 676). This looks like a good resource
    to use for training a spell-checker.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 显示了根据 GitHub Typo Corpus 创建的拼写纠错数据集的摘录。请注意，文本被分割成单个字符，甚至包括空格（由“_”替换）。所有不在通用字母表（大写字母、小写字母、数字和一些常见标点符号）内的字符都被替换为“#”。您可以看到数据集包含各种纠正，包括简单的拼写错误（pubilc->public
    在第 670 行，HYML->HTML 在第 672 行），更复杂的错误（mxnet 一词替换成 mxnet is not 在第 681 行，22th->22nd
    在第 682 行），甚至不带任何更正的行（第 676 行）。这看起来是训练拼写检查器的一个好资源。
- en: '![CH08_F17_Hagiwara](../Images/CH08_F17_Hagiwara.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F17_Hagiwara](../Images/CH08_F17_Hagiwara.png)'
- en: Figure 8.17 Training data for spelling correction
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 拼写纠错的训练数据
- en: 'The first step for training a spell-checker (or any other Seq2Seq model) is
    to preprocess the datasets. Because the dataset is already split and formatted,
    all you need to do is run fairseq-preprocess to convert the datasets into a binary
    format as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 训练拼写检查器（或任何其他 Seq2Seq 模型）的第一步是对数据集进行预处理。因为数据集已经分割和格式化，你只需要运行 fairseq-preprocess
    将数据集转换为二进制格式，操作如下：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Then you can start training your model right away using the following code.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下代码立即开始训练模型。
- en: Listing 8.3 Training a spell-checker
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 训练拼写检查器
- en: '[PRE23]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You don’t need to worry about most of the hyperparameters here—this set of
    parameters worked fairly well for me, although some other combinations of parameters
    may work better. However, you may want to pay attention to some of the parameters
    related to the size of the model, namely:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要担心这里的大多数超参数——这组参数对我来说效果还不错，尽管可能还有其他参数组合效果更好。但是，您可能想注意一些与模型大小相关的参数，即：
- en: Number of layers (—[encoder|decoder]-layers)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数（-[encoder|decoder]-layers）
- en: Embedding dimension of self-attention (—[encoder|decoder]-embed-dim)
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力的嵌入维度（-[encoder|decoder]-embed-dim）
- en: Embedding dimension of feed-forward layers (—[encoder/decoder]-ffn-embed-dim)
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈层的嵌入维度（-[encoder/decoder]-ffn-embed-dim）
- en: Number of attention heads (—[encoder|decoder]-attention-heads)
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力头数（-[encoder|decoder]-attention-heads）
- en: These parameters determine the capacity of the model. In general, the larger
    these parameters are, the larger capacity the model would have, although as a
    result the model would also require more data, time, and GPU resources to train.
    Another important parameter is —max-tokens, which specifies the number of tokens
    loaded onto a single batch. If you are experiencing out-of-memory errors on a
    GPU, try adjusting this parameter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数决定了模型的容量。一般来说，这些参数越大，模型的容量就越大，尽管作为结果，模型也需要更多的数据、时间和 GPU 资源来进行训练。另一个重要的参数是—max-token，它指定加载到单个批次中的标记数。如果在
    GPU 上遇到内存不足错误，请尝试调整此参数。
- en: 'After the training is finished, you can run the following command to make predictions
    using the trained model:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，您可以运行以下命令使用训练好的模型进行预测：
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Because the fairseq-interactive interface can also take source text from the
    standard input, we are directly providing the text using the echo command. The
    Python script src/format_fairseq_output.py, as its name suggests, formats the
    output from fairseq-interactive and shows the predicted target text. When I ran
    this, I got the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因为fairseq-interactive界面也可以从标准输入接收源文本，所以我们直接使用echo命令提供文本。Python脚本src/format_fairseq_output.py，顾名思义，格式化来自fairseq-interactive的输出，并显示预测的目标文本。当我运行这个脚本时，我得到了以下结果：
- en: '[PRE25]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This is rather disappointing. The spell-checker learned to somehow fix “imptant”
    to “implement,” although it failed to correct any other words. I suspect a couple
    of reasons for this. The training data used, GitHub Typo Corpus, is heavily biased
    toward software-related language and corrections, which might have led to the
    wrong correction (imptant -> implement). Also, the training data might have just
    been too small for the Transformer to be effective. How could we improve the model
    so that it can fix spellings more accurately?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当令人失望。拼写检查器学会了如何将“imptant”修正为“implement”，尽管它未能纠正任何其他单词。我怀疑有几个原因。使用的训练数据，GitHub
    Typo Corpus，严重偏向于软件相关的语言和纠正，这可能导致了错误的更正（imptant -> implement）。此外，训练数据可能对于Transformer来说太小了。我们如何改进模型，使其能够更准确地纠正拼写错误呢？
- en: 8.5.3 Improving a spell-checker
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.3 改进拼写检查器
- en: As we discussed earlier, one main reason the spell-checker is not working as
    expected might be because the model wasn’t exposed to a more diverse, larger amount
    of misspellings during training. But as far as I know, no such large datasets
    of diverse misspellings are publicly available for training a general-domain spell-checker.
    How could we obtain more data for training a better spell-checker?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，拼写检查器不如预期工作的一个主要原因可能是因为模型在训练过程中没有暴露给更多种类、更大数量的拼写错误。但据我所知，没有这样的大型数据集公开可用于训练一个通用领域的拼写检查器。我们如何获取更多的数据来训练一个更好的拼写检查器呢？
- en: This is where we need to be creative. One idea is to artificially generate noisy
    text from clean text. If you think of it, it is very difficult (especially for
    a machine learning model) to fix misspellings, whereas it is very easy to “corrupt”
    clean text to simulate how people make typos, even for a computer. For example,
    we can take some clean text (which is available from, for example, scraped web
    text almost indefinitely) and replace some letters at random. If you pair artificially
    generated noisy text created this way with the original, clean text, this will
    effectively create a new, larger dataset on which you can train an even better
    spell-checker!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要有创造性的地方。一个想法是从干净的文本中人工生成带有噪音的文本。如果你想一想，这是非常困难的（尤其对于一个机器学习模型）来纠正拼写错误，但很容易“破坏”干净的文本，以模拟人们如何打字错误，即使对于计算机也是如此。例如，我们可以从一些干净的文本（例如，几乎无限的从网页抓取的文本）中随机替换一些字母。如果你将以这种方式创建的人工生成的带噪音的文本与原始的干净文本配对，这将有效地创建一个新的、更大的数据集，你可以在其上训练一个更好的拼写检查器！
- en: The remaining issue we need to address is how to “corrupt” clean text to generate
    realistic spelling errors that look like the ones made by humans. You can write
    a Python script that, for example, replaces, deletes, and/or swaps letters at
    random, although there is no guarantee that typos made this way are similar to
    those made by humans and that the resulting artificial dataset will provide useful
    insights for the Transformer model. How can we model the fact that, for example,
    humans are more likely to type “too” in place of “to” than they do “two”?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要解决的剩下的问题是如何“破坏”干净的文本以生成看起来像人类所做的真实拼写错误。你可以编写一个Python脚本，例如，随机替换、删除和/或交换字母，虽然不能保证以这种方式生成的拼写错误与人类所做的拼写错误相似，也不能保证生成的人工数据集能为Transformer模型提供有用的见解。我们如何建模这样一个事实，例如，人们更有可能在“too”的地方输入“to”，而不是“two”呢？
- en: This is starting to sound familiar again. We can use the data to simulate the
    typos! But how? This is where we need to be creative again—if you “flip” the direction
    of the original dataset we used to train the spell-checker, you can observe how
    humans make typos. If you treat the clean text as the source language and the
    noisy text as the target and train a Seq2Seq model for that direction, you are
    effectively training a “spell-corruptor”—a Seq2Seq model that inserts realistic-looking
    spelling errors into clean text. See Figure 8.18 for an illustration.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![CH08_F18_Hagiwara](../Images/CH08_F18_Hagiwara.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 Using back-translation to generate artificial noisy data
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: This technique of using the “inverse” of the original training data to artificially
    generate a large amount of data in the source language from a real corpus in the
    target language is called *back-translation* in the machine learning literature.
    It is a popular technique to improve the quality of machine translation systems.
    As we’ll show next, it is also effective for improving the quality of spell-checkers.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily train a spell corruptor just by swapping the source and the
    target languages. You can do this by supplying “en” (clean text) as the source
    language and “fr” (noisy text) as the target language when you run fairseq-preprocess
    as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We are not going over the training process again—you can use almost the same
    fairseq-train command to start the training. Just don’t forget to specify a different
    directory for —save-dir. After you finish training, you can check whether the
    spelling corrupter can indeed corrupt the input text as expected:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note the extra options that I added earlier, which are shown in bold. It means
    that the fairseq-interactive command uses sampling (from top 10 tokens with largest
    probabilities) instead of beam search. When corrupting clean text, it is often
    better to use sampling instead of beam search. To recap, sampling picks the next
    token randomly according to the probability distribution after the softmax layer,
    whereas beam search tries to find the “best path” that maximizes the score of
    the output sequence. Although beam search can find better solutions when translating
    some text, we want noisy, more diverse output when corrupting clean text. Past
    research[⁶](#pgfId-1107565) has also shown that sampling (instead of beam search)
    works better for augmenting data via back-translation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: From here, the sky’s the limit. You can collect as much clean text as you want,
    generate noisy text from it using the corruptor you just trained, and increase
    the size of the training data. There is no guarantee that the artificial errors
    look like the real ones made by humans, but this is not a big deal because 1)
    the source (noisy) side is used only for encoding, and 2) the target (clean) side
    data is always “real” data written by humans, from which the Transformer can learn
    how to generate real text. The more text data you collect, the more confident
    the model will get about what error-free, real text looks like.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: I won’t go over every step I took to increase the size of the data, but here’s
    the summary of what I did and what you can also do. Collect as much clean and
    diverse text data from publicly available datasets, such as Tatoeba and Wikipedia
    dumps. My favorite way to do this is to use OpenWebTextCorpus ([https://skylion007.github.io/OpenWebTextCorpus/](https://skylion007.github.io/OpenWebTextCorpus/)),
    an open source project to replicate the dataset on which GPT-2 was originally
    trained. It consists of a huge amount (40 GB) of high-quality web text crawled
    from all outbound links from Reddit. Because the entire dataset would take days,
    if not weeks, just to preprocess and run the corruptor on, you can take a subset
    (say, 1/1000th) and add it to the dataset. I took 1/100th of the dataset, preprocessed
    it, and ran the corruptor to obtain the noisy-clean parallel dataset. This 1/100th
    subset alone added more than five million pairs (in comparison, the original training
    set contains only ~240k pairs). Instead of training from scratch, you can download
    the pretrained weights and try the spell-checker from the repository.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The training took several days, even on multiple GPUs, but when it was done,
    the result was very encouraging. Not only can it accurately fix spelling errors,
    as shown here
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'but the spell-checker also appears to understand the grammar of English to
    some degree, as shown here:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This example alone may not prove that the model really understands the grammar
    (namely, using the correct verb depending on the number of the subject). It might
    just be learning some association between consecutive words, which can be achieved
    by any statistical NLP model, such as n-gram language models. However, even after
    you make the sentences more complicated, the spell-checker shows amazing resilience,
    as shown in the next code snippet:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: From these examples, it is clear that the model learned how to ignore irrelevant
    noun phrases (such as “Tom and Jerry” and “yellow desk”) and focus on the noun
    (“book(s)”) that determines the form of the verb (“was” versus “were”). We are
    more confident that it understands the basic sentence structure. All we did was
    collect a large amount of clean text and trained the Transformer model on it,
    combined with the original training data and the corruptor. Hopefully through
    these experiments, you were able to feel how powerful the Transformer model can
    be!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attention is a mechanism in neural networks that focuses on a specific part
    of the input and computes its context-dependent summary. It works like a “soft”
    version of a key-value store.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder-decoder attention can be added to Seq2Seq models to improve their translation
    quality.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-attention is an attention mechanism that produces the summary of the input
    by summarizing itself.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Transformer model applies self-attention repeatedly to gradually transform
    the input.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-quality spell-checkers can be built using the Transformer and a technique
    called back-translation.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^(1.)Vaswani et al., “Attention Is All You Need,” (2017). [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: ^(2.)Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align
    and Translate,” (2014). [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: ^(3.)Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align
    and Translate,” (2014). [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '^(4.)Dai et al., “Transformer-XL: Attentive Language Models Beyond a Fixed-Length
    Context,” (2019). [https://arxiv.org/abs/1901.02860](https://arxiv.org/abs/1901.02860).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: ^(5.)Lample and Conneau, “Cross-Lingual Language Model Pretraining,” (2019).
    [https://arxiv.org/abs/1901 .07291](https://arxiv.org/abs/1901.07291).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: ^(6.)Edunov et al.,”Understanding Back-Translation at Scale,” (2018). [https://arxiv.org/abs/1808.09381](https://arxiv.org/abs/1808.09381).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
