- en: '7 Teaching machines to see better: Improving CNNs and making them confess'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教会机器更好地看：改善 CNNs 并让它们承认
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Reducing overfitting of image classifiers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少图像分类器的过拟合
- en: Boosting model performance via better model architectures
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过更好的模型架构提升模型性能
- en: Image classification using pretrained models and transfer learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型和迁移学习进行图像分类
- en: Modern ML explainability techniques to dissect image classifiers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代 ML 解释技术来解剖图像分类器
- en: We have developed and trained a state-of-the-art image classifier known as the
    Inception net v1 on an object classification data set. Inception net v1 is a well-recognized
    image classification model in computer vision. You learned how Inception blocks
    are created by aggregating convolution windows at multiple scales, which encourages
    sparsity in the model. You further saw how 1 × 1 convolutions are employed to
    keep the dimensionality of layers to a minimum. Finally, we observed how Inception
    net v1 uses auxiliary classification layers in the middle of the network to stabilize
    and maintain the gradient flow throughout the network. However, the results didn’t
    really live up to the reputation of the model, which was heavily overfit with
    ~30% validation and test accuracies and a whopping ~94% training accuracy. In
    this chapter, we will discuss improving the model by reducing overfitting and
    improving validation and test accuracies, which will ultimately leave us with
    a model that reaches ~80% accuracy (equivalent to being able to accurately identify
    160/200 classes of objects) on validation and test sets. Furthermore, we will
    look at techniques that allow us to probe the model’s brain to gain insights.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开发并训练了一款名为 Inception net v1 的最先进的图像分类器，它在一个物体分类数据集上进行了训练。Inception net v1
    是计算机视觉中一个广为人知的图像分类模型。你学习了 Inception 块是如何通过在多个尺度上聚合卷积窗口来创建的，这鼓励了模型中的稀疏性。你还看到了如何使用
    1 × 1 卷积来保持层的维度最小。最后，我们观察到 Inception net v1 如何在网络中部使用辅助分类层来稳定和维持整个网络中的梯度流。然而，结果并没有真正达到模型的声誉，它在验证和测试准确性上过度拟合，验证和测试准确率约为
    30%，而训练准确率则高达约 94%。在本章中，我们将讨论通过减少过拟合和提高验证和测试准确率来改善模型，最终将使我们得到一个在验证和测试集上达到约 80%
    准确率（相当于能够准确识别 160/200 类物体）的模型。此外，我们还将研究允许我们探索模型思维的技术，以获得洞察。
- en: 'This chapter takes you through an exciting journey where we turn a suboptimal
    machine learning model to a significantly superior model. This process will be
    reminiscent of what we did in the previous chapter. We will add an additional
    step to explain/interpret the decisions the model made. We will use special techniques
    to see which part of the image the model paid most attention to in order to make
    a prediction. This helps us to build trust in the model. During this process,
    we identify lurking issues in the model and systematically fix them to increase
    performance. We will discuss several important techniques, including the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带你走过一个激动人心的旅程，在这个旅程中，我们将把一个次优的机器学习模型变成一个显著优秀的模型。这个过程将使人联想起我们在上一章所做的事情。我们将增加一个额外的步骤来解释/解读模型所做的决策。我们将使用特殊的技术来看看模型在做出预测时对图像的哪个部分支付了最多的注意力。这有助于我们对模型建立信任。在这个过程中，我们识别出模型中潜藏的问题，并系统地修复它们以提高性能。我们将讨论几种重要的技术，包括以下内容：
- en: Augmenting the data by using various image transformation techniques such as
    brightness/contrast adjustment, rotation, and translation to create more labeled
    data for the model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用各种图像变换技术如亮度/对比度调整、旋转和平移来增广数据，为模型创建更多标记数据
- en: Implementing a variant of Inception net that is more suited for the size and
    type of data used
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个更适合所使用的数据大小和类型的 Inception net 变体
- en: Using transfer learning to leverage a model already trained on a larger data
    set and fine-tuning it to perform well on the data set we have
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习来利用已经在更大数据集上训练过的模型，并对其进行微调，以在我们拥有的数据集上表现良好
- en: This chapter might deeply resonate with you if you have ever had to implement
    a deep learning solution to an unfamiliar problem. Typically, implementing “some”
    deep network will not place you on the apex of success. The novelty of the problem
    or the bespoke nature of the problem at hand can impede your progress if you are
    not careful. Such problems send you into uncharted territory where you need to
    tread carefully to find a solution without exhausting yourself. This chapter will
    provide guidance for anyone who might come face-to-face with such situations in
    computer vision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经需要为不熟悉的问题实施深度学习解决方案，那么这一章可能会与你产生共鸣。通常，仅仅实施“某些”深度网络并不会让你登上成功的顶峰。如果问题的新颖性或手头问题的定制性质没有处理好，它会阻碍你的进展。这样的难题会把你带入未知领域，你需要小心行事，找到解决方案而不至于筋疲力尽。本章将为任何可能在计算机视觉领域面对类似情况的人提供指导。
- en: 7.1 Techniques for reducing overfitting
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 减少过拟合的技术
- en: We are pursuing the ambitious goal of developing an intelligent shopping assistant
    app that will use an image/object classifier as a vital component. For this, we
    will use the data set tiny-imagenet-200, which is a smaller version of the large
    ImageNet image classification data set, and consists of images and a class that
    represents the object present in that image. The data set has a training subset
    and a testing subset. You split the training subset further into a training set
    (90% of the original) and a validation set (10% of the original).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在追求一个雄心勃勃的目标，即开发一个智能购物助手应用程序，其中会使用图像/物体分类器作为重要组件。为此，我们将使用数据集 tiny-imagenet-200，它是大型
    ImageNet 图像分类数据集的一个较小版本，由图像和表示该图像中存在的对象类别组成。该数据集有一个训练子集和一个测试子集。你进一步将训练子集分成训练集（原始的90%）和验证集（原始的10%）。
- en: You have developed a model based on the famous Inception net model, but it is
    overfitting heavily. Overfitting needs to be alleviated, as it leads to models
    that perform exceptionally well on training data but poorly on test/real-world
    data. You know several techniques to reduce overfitting, namely data augmentation
    (creating more data out of existing data; for images this includes creating variants
    of the same image by introducing random brightness/contrast adjustments, translations,
    rotations, etc.), dropout (i.e., turning nodes randomly in the network during
    training), and early stopping (i.e., terminating model training before overfitting
    takes place). You wish to leverage these methods with the Keras API to reduce
    overfitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经基于著名的 Inception 网络模型开发了一个模型，但它严重过拟合。需要缓解过拟合，因为它会导致模型在训练数据上表现非常好，但在测试/真实世界数据上表现不佳。你知道几种减少过拟合的技术，即数据增强（从现有数据中创建更多数据；对于图像，这包括通过引入随机亮度/对比度调整、平移、旋转等方式创建相同图像的变体）、随机失活（即在训练期间随机关闭网络中的节点）以及提前停止（即在过拟合发生前终止模型训练）。你希望利用
    Keras API 来减少过拟合。
- en: Usually, reducing overfitting requires close scrutiny of the machine learning
    pipeline end to end. This involves looking at the data fed in, the model structure,
    and the model training. In this section, we will look at all these aspects and
    see how we can place guards against overfitting. The code for this is available
    at Ch07-Improving-CNNs-and-Explaining/7.1.Image_Classification_Advance.ipynb.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，减少过拟合需要仔细检查整个机器学习流程。这涉及到输入的数据、模型结构和模型训练。在这一节中，我们将看看所有这些方面，并了解如何防止过拟合。此部分的代码可在
    Ch07-Improving-CNNs-and-Explaining/7.1.Image_Classification_Advance.ipynb 中找到。
- en: 7.1.1 Image data augmentation with Keras
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 使用 Keras 进行图像数据增强
- en: 'First in line is augmenting data in the training set. Data augmentation is
    a prevalent method for increasing the amount of data available to deep learning
    networks without labeling new data. For example, in an image classification problem,
    you can create multiple data points from a single image by creating various transformed
    versions of the same image (e.g., shift the image, change brightness) and having
    the same label as for the original image (figure 7.1). As previously stated, more
    data conduces the strength of deep learning models by increasing generalizability
    (and reducing overfitting), leading to reliable performance in the real world.
    For image data, there are many different augmentation techniques you can use:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是在训练集中增加数据。 数据增强是一种常见的方法，可以增加深度学习网络可用的数据量，而无需对新数据进行标记。 例如，在图像分类问题中，您可以通过创建同一图像的多个变换版本（例如，移动图像，更改亮度）并具有与原始图像相同的标签（图7.1）来从单个图像创建多个数据点。
    如前所述，更多数据通过增加泛化能力（减少过拟合）来增强深度学习模型的强度，从而在实际应用中实现可靠的性能。 对于图像数据，您可以使用许多不同的增强技术：
- en: Randomly adjusting brightness, contrast, and so on
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机调整亮度、对比度等
- en: Randomly zooming in/out, rotations, translations, and so on
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机缩放、旋转、平移等
- en: '![07-01](../../OEBPS/Images/07-01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![07-01](../../OEBPS/Images/07-01.png)'
- en: Figure 7.1 Difference between training data and validation data after the augmentation
    step. The figure clearly shows various transformations applied on the training
    data and not on the validation data, as we expected.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 在增强步骤之后的训练数据和验证数据之间的差异。 图清楚地显示了对训练数据应用的各种转换，而对验证数据未应用，正如我们所预期的那样。
- en: Such augmentation can be easily applied by providing several additional parameters
    to the ImageDataGenerator that we used earlier. Let’s define a new Keras ImageDataGenerator
    with data augmentation capability. In Keras you can perform most of these augmentations,
    and there’s hardly a need to look elsewhere. Let’s look at various options an
    ImageDataGenerator provides (only the most important parameters are shown). Figure
    7.2 illustrates the effects of the different parameters listed here.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向我们之前使用的ImageDataGenerator提供几个额外参数，可以轻松地实现这种增强。 让我们定义一个新的Keras ImageDataGenerator，具有数据增强功能。
    在Keras中，您可以执行大多数这些增强，几乎不需要去其他地方寻找。 让我们看看ImageDataGenerator提供的各种选项（仅显示了最重要的参数）。
    图7.2说明了此处列出的不同参数的效果。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: featurewise_center specifies whether the images are centered by subtracting
    the mean value of the whole data set (e.g., True/False).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: featurewise_center指定是否通过减去整个数据集的平均值来使图像居中（例如，True/False）。
- en: samplewise_center specifies whether the images are centered by subtracting individual
    mean values of each image (e.g., True/False).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: samplewise_center指定是否通过减去每个图像的单个平均值来使图像居中（例如，True/False）。
- en: featurewise_std_normalization is the same as featurewise_center, but instead
    of subtracting, the mean images are divided by the standard deviation (True/False).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: featurewise_std_normalization与featurewise_center相同，但是将图像除以标准偏差而不是减去平均值（True/False）。
- en: samplewise_std_normalization is the same as samplewise_center, but instead of
    subtracting, the mean images are divided by the standard deviation (True/ False).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: samplewise_std_normalization与samplewise_center相同，但是将图像除以标准偏差而不是减去平均值（True/False）。
- en: zca_whitening is a special type of image normalization that is geared toward
    reducing correlations present in the image pixels (see [http://mng.bz/DgP0](http://mng.bz/DgP0))
    (True/False).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: zca_whitening是一种特殊类型的图像归一化，旨在减少图像像素中存在的相关性（请参阅[http://mng.bz/DgP0](http://mng.bz/DgP0)）（True/False）。
- en: rotation_range specifies the bounds of the random image rotations (in degrees)
    done during data augmentation. There is a float with values between (0, 360);
    for example, 30 means a range of -30 to 30; 0 is disabled.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rotation_range指定在数据增强期间进行的随机图像旋转的边界（以度为单位）。 具有值在(0, 360)之间的浮点数； 例如，30表示-30到30的范围；
    0禁用。
- en: width_shift_range specifies the bounds for random shifts (as proportions or
    pixels) done on the width axis during data augmentation.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: width_shift_range指定在数据增强期间在宽度轴上进行的随机移位的边界（作为比例或像素）。
- en: A tuple with values between (-1, 1) is considered as a proportion of the width
    (e.g., (-0.4, 0.3)).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值在(-1, 1)之间的元组被视为宽度的比例（例如，(-0.4, 0.3)）。
- en: A tuple with values between (-inf, inf) is considered as pixels (e.g., (-150,
    250)).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素的值在(-inf, inf)之间的元组被视为像素（例如，(-150, 250)）。
- en: height_shift_range is the same as width_shift_range except for the height dimension.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: height_shift_range与width_shift_range相同，只是针对高度维度。
- en: brightness_range specifies the bounds of the random brightness adjustments made
    to data during data augmentation.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`brightness_range`指定在数据增强期间对数据进行的随机亮度调整的范围。'
- en: A tuple with values between (-inf, inf) is, for example, (-0.2, 0.5) or (-5,
    10); 0 is disabled.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元组中的值介于（-inf，inf）之间，例如，（-0.2，0.5）或（-5，10）；0表示禁用。
- en: shear_range is the same as brightness_range but for shearing (i.e., skewing)
    images during data augmentation
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shear_range`与`brightness_range`相同，但用于在数据增强期间剪切（即倾斜）图像。'
- en: A float in degrees is, for example, 30.0.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以度为单位的浮点数，例如，30.0。
- en: zoom_range is the same as brightness_range except for scaling the images during
    data augmentation.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoom_range`与`brightness_range`相同，除了在数据增强期间对图像进行缩放。'
- en: horizontal_flip specifies whether to randomly flip images horizontally during
    data augmentation (True/False).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizontal_flip`指定在数据增强期间是否随机水平翻转图像（是/否）。'
- en: vertical_flip is the same as horizontal_flip but flips vertically (True/False)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vertical_flip`与`horizontal_flip`相同，但垂直翻转（是/否）。'
- en: fill_mode defines how the empty spaces created by various image transformations
    (e.g., translating the image to the left creates an empty space on the right)
    are handled. Possible options are “reflect,” “nearest,” and “constant.” The last
    row of figure 7.2 depicts the differences.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill_mode`定义了通过各种图像变换（例如，将图像向左移动会在右侧创建空白空间）创建的空白空间如何处理。可能的选项是“reflect”，“nearest”和“constant”。图
    7.2 的最后一行显示了差异。'
- en: rescale rescales the inputs by a constant value.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale`通过常量值重新缩放输入。'
- en: preprocessing_function takes a Python function that can be used to introduce
    additional data augmentation/preprocessing steps that are not readily available.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocessing_function`接受一个Python函数，该函数可用于引入额外的数据增强/预处理步骤，这些步骤不容易获得。'
- en: validation_split addresses how much data should be used as validation data.
    We don’t use this parameter, as we create a data generator for the validation
    set separately because we do not want an augmentations app. A float is, for example,
    0.2.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_split`解决了应该将多少数据用作验证数据的问题。我们不使用此参数，因为我们单独为验证集创建数据生成器，因为我们不希望有增强应用。一个浮点数，例如，0.2。'
- en: '![07-02](../../OEBPS/Images/07-02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![07-02](../../OEBPS/Images/07-02.png)'
- en: Figure 7.2 Effects of different augmentation parameters and their values of
    the ImageDataGenerator
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 不同增强参数及其`ImageDataGenerator`的值的效果。
- en: 'With a good understanding of different parameters, we will define two image
    data generators: one with data augmentation (training data) and the other without
    (testing data). For our project, we will augment the data in the following ways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对不同参数有良好的理解，我们将定义两个图像数据生成器：一个用于数据增强（训练数据），另一个不用于数据增强（测试数据）。对于我们的项目，我们将以以下方式增强数据：
- en: Randomly rotate images
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机旋转图像。
- en: Randomly translate on width dimension
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在宽度维度上随机平移。
- en: Randomly translate on height dimension
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高度维度上随机平移。
- en: Randomly adjust brightness
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机调整亮度。
- en: Randomly shear
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机剪切。
- en: Randomly zoom
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机缩放。
- en: Randomly flip images horizontally
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机水平翻转图像。
- en: Random gamma correct (custom implementation)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机伽马校正（自定义实现）。
- en: Random occlude (custom implementation)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机遮挡（自定义实现）。
- en: The following listing shows how theImageDataGenerator is defined with a validation
    split.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了如何使用验证分割定义`ImageDataGenerator`。
- en: Listing 7.1 Defining the ImageDataGenerator with validation split
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 定义了具有验证分割的`ImageDataGenerator`。
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Defines the ImageDataGenerator for training/validation data
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义用于训练/验证数据的`ImageDataGenerator`。
- en: ❷ We will switch off samplewise_center temporarily and reintroduce it later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们将暂时关闭`samplewise_center`并稍后重新引入它。
- en: ❸ Various augmentation arguments previously discussed (set empirically)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 先前讨论的各种增强参数（经验设置）。
- en: ❹ Uses a 10% portion of training data as validation data
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将训练数据的 10% 部分用作验证数据。
- en: ❺ Defines a separate ImageDataGenerator for testing data
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义了用于测试数据的单独`ImageDataGenerator`。
- en: We chose the parameters for these arguments empirically. Feel free to experiment
    with other arguments and see the effect they have on the model’s performance.
    One important thing to note is that, unlike previous examples, we set samplewise_center=False.
    This is because we are planning to do few custom preprocessing steps before the
    normalization. Therefore, we will turn off the normalization in the ImageDataGenerator
    and reintroduce it later (through a custom function). Next, we will define the
    training and testing data generators (using a flow function). Following a similar
    pattern as the previous chapter, we will get the training and validation data
    generators through the same data generator (using the validation_split and subset
    arguments; see the following listing).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经验地选择了这些参数的参数。随意尝试其他参数，并查看它们对模型性能的影响。一个重要的事情要注意的是，与以前的例子不同，我们设置了samplewise_center=False。这是因为我们计划在标准化之前进行少量自定义预处理步骤。因此，我们将关闭ImageDataGenerator中的标准化，并稍后重新引入它（通过自定义函数）。接下来，我们将定义训练和测试数据生成器（使用流函数）。与上一章类似的模式，我们将通过同一数据生成器（使用validation_split和subset参数）获取训练和验证数据生成器（参见下一个列表）。
- en: Listing 7.2 Defining the data generators for training, validation, and testing
    sets
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2：定义训练、验证和测试集的数据生成器
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Define a partial function that has all the arguments fixed except for the
    subset argument.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个偏函数，除了子集参数之外所有参数都已固定。
- en: ❷ Get the training data subset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取训练数据子集。
- en: ❸ Get the validation data subset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取验证数据子集。
- en: ❹ Read in the test labels stored in a txt file.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 读取存储在txt文件中的测试标签。
- en: ❺ Define the test data generator.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义测试数据生成器。
- en: 'To refresh our memory, the flow_from_directory(...) has the following function
    signature:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了恢复我们的记忆，flow_from_directory(...)具有以下函数签名：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The train_gen and valid_gen uses image_gen_aug (with data augmentation) to retrieve
    data. train_gen and valid_gen are defined as partial functions of the original
    image_gen.flow_from_directory(), where they share all the arguments except for
    the subset argument. However, it is important to keep in mind that augmentation
    is only applied to training data and must not be applied on the validation subset.
    This is the desired behavior we need, as we want the validation data set to remain
    fixed across epochs. Next, test_gen uses image_gen (without data augmentation).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: train_gen和valid_gen使用image_gen_aug（进行数据增强）来获取数据。train_gen和valid_gen被定义为原始image_gen.flow_from_directory()的偏函数，它们共享除子集参数之外的所有参数。但是，重要的是要记住，增强仅应用于训练数据，不得应用于验证子集。这是我们需要的期望行为，因为我们希望验证数据集跨多个周期保持固定。接下来，test_gen使用image_gen（无数据增强）。
- en: Why should we not augment validation/test data?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不应该增强验证/测试数据？
- en: When augmenting data, you should only augment the training data set and not
    the validation and test sets. Augmentation on validation and test sets will lead
    to inconsistent results between trials/runs (due to the random modifications introduced
    by data augmentation). We want to keep our validation and testing data sets consistent
    from the start to the end of training. Therefore, data augmentation is only done
    to the training data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数据增强时，应该只对训练数据集进行增强，不要对验证和测试集进行增强。在验证和测试集上进行增强会导致不同测试/运行之间结果不一致（因为数据增强引入了随机修改）。我们希望保持验证和测试数据集在训练期间始终保持一致。因此，数据增强只针对训练数据进行。
- en: Remember that Inception Net v1 has three output layers; therefore, the output
    of the generator needs to be a single input and three outputs. We do this by defining
    a new Python generator off the Keras generator that modifies the content accordingly
    (see the next listing).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Inception Net v1有三个输出层；因此，生成器的输出需要是一个输入和三个输出。我们通过定义一个新的Python生成器，修改内容以实现这一点（见下一个列表）。
- en: Listing 7.3 Defining the data generator with several modifications
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3：定义带有几个修饰的数据生成器
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Define a new function that introduces two new augmentation techniques and
    modifies the format of the final output.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个新的函数，引入两种新的增强技术，并修改最终输出的格式。
- en: ❷ Check if the Gamma correction augmentation is needed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查是否需要伽马校正增强。
- en: ❸ Perform Gamma correction-related augmentation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行伽马校正相关的数据增强。
- en: ❹ Check if random occlusion augmentation is needed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 检查是否需要随机遮挡数据增强。
- en: ❺ Defines the starting x/y pixels randomly for occlusion
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 随机定义遮挡的起始x/y像素。
- en: ❻ Apply a white/gray/black color randomly to the occlusion.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 随机为遮挡覆盖添加白色/灰色/黑色。
- en: ❼ Perform the sample-wise centering that was switched off earlier.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对之前关闭的样本居中进行样本级居中。
- en: ❽ Makes sure we replicate the target (y) three times
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 确保我们复制目标（y）三次
- en: ❾ Training data is augmented with random gamma correction and occlusions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 训练数据使用随机gamma校正和遮挡进行增强。
- en: ❿ Validation/testing sets are not augmented.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 验证/测试集不进行增强。
- en: 'You can see how the data_gen_augmented_inceptionnet_v1 returns a single input
    (x) and three replicas of the same output (y). In addition to modifying the format
    of the output, this data_gen_augmented_inceptionnet_v1 will include two extra
    data augmentation steps using a custom implementation (which are not available
    as built-ins):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到`data_gen_augmented_inceptionnet_v1`返回单个输入（x）和相同输出的三个副本（y）。除了修改输出的格式外，`data_gen_augmented_inceptionnet_v1`还将使用自定义实现包括两个额外的数据增强步骤（这些步骤不是内置的）：
- en: '*Gamma correction*—A standard computer vision transformation performed by raising
    the pixel values to the power of some value ([http://mng.bz/lxdz](http://mng.bz/lxdz)).
    In our case, we chose this value randomly between 0.9 and 1.08.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gamma校正*—标准的计算机视觉转换，通过将像素值提高到某个值的幂次方来执行（[http://mng.bz/lxdz](http://mng.bz/lxdz)）。在我们的情况下，我们在0.9和1.08之间随机选择这个值。'
- en: '*Random occlusions*—We will occlude a random patch on the image (10 × 10) with
    white, gray, or black pixels (chosen randomly).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机遮挡*—我们将在图像上随机遮挡一个随机的补丁（10 × 10），用白色、灰色或黑色像素（随机选择）。'
- en: You also need to center the images, as we set the samplewise_center argument
    to False when we defined the ImageDataGenerator. This is done by subtracting the
    mean pixel value of each image from its pixels. With the data_gen_augmented_inceptionnet_v1
    function defined, we can create the modified data generators train_gen_aux, valid_
    gen_aux, and test_gen_aux for training/validation/testing data, respectively.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义ImageDataGenerator时，也需要对图像进行居中处理，因为我们将samplewise_center参数设置为False。这通过从每个图像的像素中减去其平均像素值来完成。定义了data_gen_augmented_inceptionnet_v1函数后，我们可以为训练/验证/测试数据分别创建修改后的数据生成器train_gen_aux、valid_gen_aux和test_gen_aux。
- en: Check, check, check to avoid model performance defects
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 检查，检查，检查以避免模型性能缺陷
- en: If you don’t check to see if just the training data is augmented, you can be
    in trouble. If it doesn’t work as intended, it can easily fly under the radar.
    Technically, your code is working and free of functional bugs. But this will leave
    you scratching your head for days trying to figure out why the model is not performing
    as intended in the real world.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不检查只有训练数据是否被增强，那你可能会陷入麻烦。如果它不能按预期工作，它很容易被忽视。从技术上讲，你的代码是正常工作的，并且没有功能性的bug。但这会让你在实际情况下不断琢磨为什么模型没有按预期执行。
- en: Finally, the most important step in this process is verifying that the data
    augmentation is working as we expect and not corrupting the images in unexpected
    ways, which would impede the learning of the model. For that, we can plot some
    of the samples generated by the train data generator as well as the validation
    data generator. Not only do we need to make sure that the data augmentation is
    working properly, but we also need to make sure that data augmentation is not
    present in the validation set. Figure 7.3 ensures that this is the case.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个过程中最重要的步骤是验证数据增强是否按照我们的期望进行，而不会以意想不到的方式破坏图像，这会妨碍模型的学习。为此，我们可以绘制由训练数据生成器生成的一些样本以及验证数据生成器生成的样本。我们不仅需要确保数据增强正常工作，还需要确保验证集中不存在数据增强。图7.3确保了这一点。
- en: '![07-03](../../OEBPS/Images/07-03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![07-03](../../OEBPS/Images/07-03.png)'
- en: Figure 7.3 Difference between training data and validation data after the augmentation
    step. The figure clearly shows various transformations applied on the training
    data and not on the validation data, as we expected.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 在增强步骤之后训练数据和验证数据之间的差异。该图清楚地显示了应用于训练数据但未应用于验证数据的各种变换，正如我们所预期的那样。
- en: Next, we discuss another regularization technique called dropout.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论另一种正则化技术称为dropout。
- en: '7.1.2 Dropout: Randomly switching off parts of your network to improve generalizability'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 Dropout：随机关闭网络的部分以提高泛化能力
- en: We will now learn a technique called *dropout* to reduce further overfitting.
    Dropout was part of Inception net v1, but we avoided dropout in the previous chapter
    to improve clarity.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习一种称为*dropout*的技术，以进一步减少过拟合。 Dropout是Inception net v1的一部分，但在前一章中，我们避免使用dropout以提高清晰度。
- en: Dropout is a regularization technique for deep networks. A regularization technique’s
    job is to control the deep network in such a way that the network is rid of numerical
    errors during training or troublesome phenomena like overfitting. Essentially,
    regularization keeps the deep network well behaved.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一种用于深度网络的正则化技术。正则化技术的作用是控制深度网络，使其在训练过程中摆脱数值错误或者像过拟合这样的麻烦现象。本质上，正则化使深度网络行为良好。
- en: Dropout switches off output neurons randomly during each training iteration.
    This helps the model learn redundant features during training as it will not always
    have the previously learned features at its disposal. In other words, the network
    only has a subset of parameters of the full network to learn at a given time,
    and it forces the network to learn multiple (i.e., redundant) features to classify
    objects. For example, if the network is trying to identify cats, in the first
    iteration it might learn about whiskers. Then, if the nodes that correspond to
    the knowledge on whiskers are switched off, it might learn about cats’ pointy
    ears (figure 7.4). This leads to a network that learns redundant/different features
    like whiskers, two pointed ears, and so on, leading to better performance at test
    time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 辍学在每次训练迭代期间随机关闭输出神经元。这有助于模型在训练期间学习冗余特征，因为它不总是能够使用先前学到的特征。换句话说，网络在任何给定时间只有部分参数的全网络可学习，并迫使网络学习多个（即冗余的）特征来分类对象。例如，如果网络试图识别猫，那么在第一次迭代中它可能学习关于胡须的知识。然后，如果与胡须知识相关的节点被关闭，它可能学习关于猫耳朵的知识（见图
    7.4）。这导致网络学习了冗余/不同的特征，如胡须、两只尖耳朵等，从而在测试时间表现更好。
- en: '![07-04](../../OEBPS/Images/07-04.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![07-04](../../OEBPS/Images/07-04.png)'
- en: Figure 7.4 How dropout might change the network when learning to classify cat
    images. In the first iteration, it might learn about whiskers. In the second iteration,
    as the part containing information about whiskers is turned off, the network might
    learn about pointy ears. This leads the network to having knowledge about both
    whiskers and ears when it’s time for testing. That’s good in this case, because
    in the test image, you cannot see the cat’s whiskers!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 当学习分类猫图像时，辍学如何改变网络。在第一次迭代中，它可能学习有关胡须的知识。在第二次迭代中，由于包含有关胡须信息的部分被关闭，网络可能学习有关尖耳朵的知识。这使网络在测试时具有关于胡须和耳朵的知识。在这种情况下是好的，因为在测试图像中，你看不到猫的胡须！
- en: The nodes are switched off by applying a random mask of 1s and 0s on each layer
    you want to apply dropout on (figure 7.5). There is also a vital normalization
    step you perform on the active nodes during training. Let’s assume we are training
    a network with 50% dropout (i.e., dropping half of the nodes on every iteration).
    When 50% of your network is switched off, conceptually your network’s total output
    is reduced by half, compared to having the full network on. Therefore, you need
    to multiply the output by a factor of 2 to make sure the total output remains
    constant. Such computational details of dropout are highlighted in figure 7.5\.
    The good news is that you don’t have to implement any of the computational details,
    as dropout is provided as a layer in TensorFlow.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个要应用辍学的层上应用随机的 1 和 0 掩码关闭节点（见图 7.5）。在训练过程中，您还需要对活动节点进行重要的规范化步骤。假设我们正在训练一个辍学率为
    50% 的网络（即在每次迭代中关闭一半的节点）。当你的网络关闭了 50% 时，从概念上讲，你的网络总输出会减少一半，与完整网络相比。因此，您需要将输出乘以一个因子
    2，以确保总输出保持不变。辍学的这些计算细节在图 7.5 中突出显示。好消息是，您不必实现任何计算细节，因为 TensorFlow 中提供了辍学作为一个层。
- en: '![07-05](../../OEBPS/Images/07-05.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![07-05](../../OEBPS/Images/07-05.png)'
- en: Figure 7.5 A computational perspective on how dropout works. If dropout is set
    to 50%, then half the nodes in every layer (except for the last layer) will be
    turned off. But at testing time, all the nodes are switched on.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 辍学如何运作的计算视角。如果辍学设置为 50%，则每个层中的一半节点（除了最后一层）将被关闭。但在测试时，所有节点都被打开。
- en: 'Inception net v1 (figure 7.6) only has dropout for fully connected layers and
    the last average pooling layer. Remember not to use dropout on the last layer
    (i.e., the layer that provides final predictions). There are two changes to perform:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网 v1（见图 7.6）只对全连接层和最后一个平均池化层应用辍学。记住不要在最后一层（即提供最终预测的层）上使用辍学。要执行两个更改：
- en: Apply 70% dropout to the intermediate fully connected layer in the auxiliary
    outputs.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在辅助输出中的中间全连接层应用 70% 的辍学。
- en: Apply 40% dropout to the output of the last average pooling layer.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对最后一个平均池化层的输出应用 40% 的 dropout。
- en: '![07-06](../../OEBPS/Images/07-06.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![07-06](../../OEBPS/Images/07-06.png)'
- en: Figure 7.6 Abstract architecture of Inception net v1\. Inception net starts
    with a stem, which is an ordinary sequence of convolution/pooling layers that
    you will find in a typical CNN. Then Inception net introduces a new component
    known as Inception block. Finally, Inception net also makes use of auxiliary output
    layers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 Inception 网络 v1 的抽象架构。Inception 网络以一个称为干线的组件开始，这是一个典型 CNN 中会找到的普通的卷积/池化层序列。然后
    Inception 网络引入了一个称为 Inception 块的新组件。最后，Inception 网络还利用了辅助输出层。
- en: In TensorFlow, applying dropout is as easy as writing a single line. Once you
    get the output of the fully connected layer, dense1, you can apply dropout with
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，应用 dropout 就像写一行代码一样简单。一旦你得到了全连接层 dense1 的输出，你就可以使用以下方法应用 dropout：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we’re using a 70% dropout rate (as suggested in the original Inception
    net v1 paper) for the auxiliary output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 70% 的 dropout 率（正如原始 Inception 网络 v1 论文中建议的）用于辅助输出。
- en: Dropout on convolution layers
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层上的 dropout
- en: Dropout is mostly applied to dense layers, so one cannot help but wonder, “Why
    are we not applying dropout on convolution layers?” It is still an open debate.
    For example, the original dropout paper by Nitish Srivastava et al. ([http://mng.bz/o2Nv](http://mng.bz/o2Nv))
    argues that using dropout on lower convolution layers provides a performance boost.
    In contrast, the paper “Bayesian CNNs with Bernoulli Approximate Variational Inference”
    by Yarin Gal et al. ([https://arxiv.org/pdf/1506.02158v6.pdf](https://arxiv.org/pdf/1506.02158v6.pdf))
    argues that dropout on convolution layers doesn’t help much as, due to their low
    number of parameters (compared to a dense layer), they are already regularized
    well. Consequentially, dropout can hinder the learning in convolution layers.
    One thing you need to take into account is the time of publication. The dropout
    paper was written two years before the Bayesian CNN paper. Regularization and
    other improvements introduced in that duration could have had a major impact on
    improving deep networks, so the benefit of having dropout in convolution layers
    could become negligible. You can find a more casual discussion on [http://mng.bz/nNQ4](http://mng.bz/nNQ4).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 主要应用在密集层上，所以人们不禁会想，“为什么我们不在卷积层上应用 dropout 呢？”这仍然是一个争论未决的问题。例如，Nitish
    Srivastava 等人的原始 dropout 论文（[http://mng.bz/o2Nv](http://mng.bz/o2Nv)）认为，在低卷积层上使用
    dropout 可以提高性能。相反，Yarin Gal 等人的论文“具有伯努利近似变分推断的贝叶斯 CNN”（[https://arxiv.org/pdf/1506.02158v6.pdf](https://arxiv.org/pdf/1506.02158v6.pdf)）认为，在卷积层上应用
    dropout 并不会有太大帮助，因为它们的参数数量较低（与密集层相比），已经很好地被正则化了。因此，dropout 可以阻碍卷积层的学习。你需要考虑的一件事是出版时间。dropout
    论文是在贝叶斯 CNN 论文之前两年写的。在那段时间内引入的正则化和其他改进可能对改进深度网络产生了重大影响，因此，在卷积层中使用 dropout 的好处可能变得微不足道。你可以在
    [http://mng.bz/nNQ4](http://mng.bz/nNQ4) 找到更多非正式的讨论。
- en: The final code for the auxiliary output is shown in the following listing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助输出的最终代码如下列表所示。
- en: Listing 7.4 Modifying the auxiliary output of Inception net
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 修改了 Inception 网络的辅助输出
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Applying a dropout layer with 70% dropout
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用了 70% 的 dropout 层
- en: Next, we will apply dropout to the output of the last average pooling layer
    before the final prediction layer. We must flatten the output of the average pooling
    layer (flat_out) before feeding into a fully connected (i.e., dense) layer. Then
    dropout is applied on flat_out using
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在最后一个平均池化层的输出上应用 dropout，然后是最后的预测层。在将平均池化层的输出（flat_out）馈送到全连接（即密集）层之前，我们必须将其展平。然后，使用以下方法在
    flat_out 上应用 dropout：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are using a dropout rate of 40% for this layer, as prescribed by the paper.
    The final code (starting from the average pooling layer) looks like this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一层，我们使用了 40% 的 dropout 率，正如论文中所建议的一样。最终的代码（从平均池化层开始）如下所示：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This concludes the discussion on dropout. One final note to keep in mind is
    that you should not naively set the dropout rate. It should be chosen via a hyperparameter
    optimization technique. A very high dropout rate can leave your network severely
    crippled, whereas a very low dropout rate will not contribute to reducing overfitting.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对 dropout 的讨论。要牢记的最后一点是，你不应该简单地设置 dropout 率。应该通过超参数优化技术来选择。非常高的 dropout
    率会严重削弱你的网络，而非常低的 dropout 率则不会有助于减少过拟合。
- en: '7.1.3 Early stopping: Halting the training process if the network starts to
    underperform'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 早停：如果网络开始表现不佳，则停止训练过程
- en: The final technique we will be looking at is called early stopping. As the name
    suggests, early stopping stops training the model when the validation accuracy
    stops increasing. You may be thinking, “What? I thought the more training we do
    the better.” Until you reach a certain point, more training is better, but then
    training starts to reduce the model’s generalizability. Figure 7.7 depicts the
    typical training accuracy and validation accuracy curves you will obtain over
    the course of training a model. As you can see, after a point, the validation
    accuracy stops increasing and starts dropping. This is the start of overfitting.
    You can see that the training accuracy keeps going up, regardless of the validation
    accuracy. This is because modern deep learning models have more than enough parameters
    to “remember” data instead of learning features and patterns present in the data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的最后一种技术叫做早停（early stopping）。顾名思义，早停会在验证准确度不再提高时停止模型训练。你可能会想：“什么？我以为训练越多越好。”在达到某一点之前，训练得越多越好，但是之后，训练开始降低模型的泛化能力。图7.7展示了在训练模型过程中你会获得的典型训练准确度和验证准确度曲线。你可以看到，在某一点之后，验证准确度停止提高并开始下降。这标志着过拟合的开始。你可以看到，无论验证准确度如何，训练准确度都在持续上升。这是因为现代深度学习模型具有足够多的参数来“记住”数据，而不是学习数据中存在的特征和模式。
- en: '![07-07](../../OEBPS/Images/07-07.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![07-07](../../OEBPS/Images/07-07.png)'
- en: Figure 7.7 An illustration of overfitting. At the start, as the number of training
    iterations increases, both training and validation accuracies increase. But after
    a certain point, the validation accuracy plateaus and starts to go down, while
    the training accuracy keeps going up. This behavior is known as overfitting and
    should be avoided.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：过拟合的示意图。在开始时，随着训练迭代次数的增加，训练和验证准确度都会提高。但是在某个时刻之后，验证准确度会趋于平稳并开始下降，而训练准确度则持续上升。这种行为称为过拟合，应该避免。
- en: The early stopping procedure is quite simple to understand. First, you define
    a maximum number of epochs to train for. Then the model is trained for one epoch.
    After the training, the model is evaluated on the validation set using an evaluation
    metric (e.g., accuracy). If the validation accuracy has gone up and hasn’t reached
    the maximum epoch, the training is continued. Otherwise, training is stopped,
    and the model is finalized. Figure 7.8 depicts the early stopping workflow.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 早停过程非常简单易懂。首先，你定义一个最大的训练轮数。然后模型训练一轮。训练之后，使用评估指标（例如准确度）在验证集上评估模型。如果验证准确度提高了并且还没有达到最大epoch，则继续训练。否则，停止训练，并完成模型。图7.8描述了早停的工作流程。
- en: '![07-08](../../OEBPS/Images/07-08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![07-08](../../OEBPS/Images/07-08.png)'
- en: Figure 7.8 The workflow followed during early stopping. First the model is trained
    for one epoch. Then, the validation accuracy is measured. If the validation accuracy
    has increased and the training hasn’t reached maximum epoch, training is continued.
    Otherwise, training is halted.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：早停期间的工作流程。首先，模型训练一轮。然后，测量验证准确度。如果验证准确度提高了并且训练还没有达到最大epoch，则继续训练。否则，停止训练。
- en: 'Implementing early stopping requires minimal changes to your code. First, as
    before, we will set up a function that computes the number of steps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 实施早停需要对你的代码进行最小的更改。首先，和之前一样，我们将建立一个计算步数的函数：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will use the EarlyStopping callback provided by Keras ([http://mng.bz/v6lr](http://mng.bz/v6lr))
    to enable early stopping during the training. A Keras callback is an easy way
    to make something happen at the end of each epoch during training. For example,
    for early stopping, all we need to do is analyze the validation accuracy at the
    end of each epoch and, if it hasn’t shown any improvement, terminate the training.
    Callbacks are ideal for achieving this. We have already used the CSVLogger callback
    to log the metric quantities over the epochs. The EarlyStopping callback has several
    arguments:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Keras提供的EarlyStopping回调（[http://mng.bz/v6lr](http://mng.bz/v6lr)）来在训练过程中启用早停。Keras回调是在每个epoch结束时让某些事情发生的简单方法。例如，对于早停，我们只需在每个epoch结束时分析验证准确度，如果没有显示任何改善，就终止训练。回调是实现这一目标的理想选择。我们已经使用了CSVLogger回调来记录每个epoch的指标数量。EarlyStopping回调有几个参数：
- en: monitor—Which metric needs to be monitored in order to terminate the training.
    You can get the list of defined metric names using the model.metric_names attribute
    of a Keras model. In our example, this will be set to val_loss (i.e., the loss
    value computed on the validation data).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: monitor—需要监测的指标以终止训练。可以使用Keras模型的model.metric_names属性获取定义的指标名称列表。在我们的示例中，这将设置为val_loss（即在验证数据上计算的损失值）。
- en: min_delta—The minimum change required in the monitored metric to be considered
    an improvement (i.e., any improvement < min_delta will be considered a “no improvement”
    [defaults to zero]).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: min_delta—被监测指标所需的最小改变，以被视为改进（即任何改进<min_delta将被视为“没有改进” [默认为零]）。
- en: patience—If there’s no improvement after this many epochs, training will stop
    (defaults to zero).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: patience—如果在这么多个epochs之后没有改进，则训练将停止（默认为零）。
- en: mode—Can be auto/min/max. In min, training will stop if the metric has stopped
    decreasing (e.g., loss). In max, training will stop if the metric has stopped
    increasing (e.g., accuracy). The mode will be automatically inferred from the
    metric name (defaults to auto).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mode—可以是auto/min/max。在min中，如果指标停止减少（如损失），则训练将停止。在max中，如果指标停止增加（如准确度），则训练将停止。该模式将自动从指标名称中推断（默认为auto）。
- en: baseline—Baseline value for the metric. If the metric doesn’t improve beyond
    the baseline, training will stop (defaults to none).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: baseline—指标的基准值。如果指标未超出基准值，则训练将停止（默认为无）。
- en: restore_best_weights—Restores the best weight result in between the start of
    the training and the termination that showed the best value for the chosen metric
    (defaults to false).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: restore_best_weights—在训练开始和终止之间恢复显示选择指标的最佳权重结果（默认为false）。
- en: 'First, we will create a directory called eval if it doesn’t exist. This will
    be used to store the CSV, returned by the CSVLogger:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果不存在，我们将创建一个名为eval的目录。这将用于存储由CSVLogger返回的CSV文件：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we define the EarlyStopping callback. We chose val_loss as the metric
    to monitor and a patience of five epochs. This means the training will tolerate
    a “no improvement” for five epochs. We will leave the other parameters in their
    defaults:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义EarlyStopping回调函数。我们选择val_loss作为监测的指标，以及五个epochs的耐心。这意味着在五个epochs内训练将容忍“没有改进”。我们将保留其他参数为默认值：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally call model.fit() with the data and the appropriate callbacks. Here,
    we use the previously defined train_gen_aux and valid_gen_aux as the training
    and validation data (respectively). We also set epochs to 50 and the training
    steps and the validations steps using the get_steps_per_epoch function. Finally,
    we provide the EarlyStopping and CSVLogger callbacks, so the training stops when
    there’s no improvement under the specified conditions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后使用数据和适当的回调函数调用model.fit()。在这里，我们使用先前定义的train_gen_aux和valid_gen_aux作为训练和验证数据（分别）。我们还将epochs设置为50，并使用get_steps_per_epoch函数设置训练步数和验证步数。最后，我们提供EarlyStopping和CSVLogger回调函数，所以在指定条件下没有改进时训练停止：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The next listing shows a summary of the training logs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个清单展示了训练日志的摘要。
- en: Listing 7.5 Training logs provided during training the model
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 在训练模型期间提供的训练日志
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Because we used a high dropout rate of 70% for some layers, TensorFlow warns
    us about it, as unintended high dropout rates can hinder model performance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 因为我们对一些层使用了高达70%的丢失率，TensorFlow会对此进行警告，因为意外的高丢失率可能会影响模型的性能。
- en: It seems the model doesn’t see a benefit in training the model for 50 epochs.
    After epoch 38, it has decided to terminate the training. This is evident by the
    fact that training stopped before reaching epoch 50 (as shown in the line Epoch
    38/50). The other important observation is that you can see that the training
    accuracy doesn’t explode to large values, as we saw in the last chapter. The training
    accuracy has remained quite close to the validation accuracy (~30%). Though we
    don’t see much of a performance increase, we have managed to reduce overfitting
    significantly. With that, we can focus on getting the accuracy higher.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来模型没有在50个epochs中训练到利益。在第38个epoch之后，它决定终止训练。这在于训练在达到第50个epoch之前停止（如第38/50行所示）。另一个重要的观察结果是，你可以看到训练准确度没有像我们在上一章中看到的那样激增到很高的值。训练准确度一直与验证准确度（~30%）相当接近。尽管我们没有看到很大的性能提升，但我们成功地显著减少了过拟合。因此，我们可以着重提高准确度。
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 1 hour and 30 minutes to run 38 epochs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will revisit our model. We will dig into some research and implement
    a model that has proven to work well for this specific classification problem.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'You have the following model presented to you, and you see that it is heavily
    underfitting. Underfitting occurs when your model is not approximating the distribution
    of the data closely enough. Suggest how you can change the dropout layer to reduce
    underfitting. You can choose between 20%, 50%, and 80% as dropout rates:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Exercise 2
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Define an early stopping callback to terminate the training if the validation
    loss value (i.e., val_loss) has not increased by 0.01 after five epochs. Use tf.keras.callbacks.EarlyStopping
    callback for this purpose.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '7.2 Toward minimalism: Minception instead of Inception'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have a model where overfitting is almost nonexistent. However, test performance
    of the model is still not where we want it to be. You feel like you need a fresh
    perspective on this problem and consult a senior data scientist on your team.
    You explain how you have trained an Inception net v1 model on the tiny-imagenet-200
    image classification data set, as well as the poor performance of the model. He
    mentions that he recently read a paper ([cs231n.stanford.edu/reports/2017/pdfs/930.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/930.pdf))
    that uses a modified version of the Inception net that’s motivated by Inception-ResNet
    v2 and has achieved better performance on the data set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: He further explains two new techniques, batch normalization and residual connections
    (that are used in the modified inception net as well as Inception-ResNet v2),
    and the significant impact they have in helping the model training, especially
    in deep models. Now you will implement this new modified model and see if it will
    improve performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: We have seen a slight increase in the validation and test accuracies. But we
    still have barely scratched the surface when it comes to performance. For example,
    there are reports of ~85% test accuracy for this data set ([http://mng.bz/44ev](http://mng.bz/44ev)).
    Therefore, we need to look for other ways to improve the model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: That session you had with the senior data scientist on your team couldn’t have
    been more fruitful. We are going to try the new network he read about.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'This network is predominantly inspired by the Inception-Resnet-v2 network that
    was briefly touched on in the previous chapter. This new network (which we will
    call Minception) leverages all the state-of-the-art components used in the Inception-ResNet
    v2 model and modifies them to suit the problem at hand. In this section, you will
    learn this new model in depth. Particularly, Minception net has the following
    elements:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: A stem
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception-ResNet block A
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception-ResNet block B
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction blocks (a new type of block to reduce output size)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average pooling layer
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final prediction layer
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like other Inception models, this has a stem and Inception blocks. However,
    Minception differs from Inception Net v1 because it does not have auxiliary outputs,
    as they have other techniques to stabilize the training. Another notable difference
    is that Minception has two types of Inception blocks, whereas Inception Net v1
    reuses the same format throughout the network. While discussing the different
    aspects of Minception, we will compare it to Inception Net v1 (which we implemented)
    in more detail. In a later section, we will discuss the architecture of the Inception-ResNet
    v2 model in more detail and compare that to Minception. The code for this is available
    at Ch07-Improving-CNNs-and-Explaining/7.1.Image_Classification_Advance.ipynb.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Implementing the stem
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First and foremost, we should focus on the stem of the model. To refresh our
    knowledge, a stem is a sequence of convolution and pooling layers and resembles
    a typical CNN. Minception, however, has a more complex layout, as shown in figure
    7.9.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![07-09](../../OEBPS/Images/07-09.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 Comparing the stems of Minception and Inception-v1\. Note how Minception
    separates the nonlinear activation of convolution layers. This is because batch
    normalization must be inserted in between the convolution output and the nonlinear
    activation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: You can see that it has parallel streams of convolution layers spread across
    the stem. The stem of the Minception is quite different from Inception Net v1\.
    Another key difference is that Minception does not use local response normalization
    (LRN) but something far more powerful known as *batch normalization*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization: A versatile normalization technique to stabilize and accelerate
    the training of deep networks'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization (BN) was introduced in the paper “Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift” by Sergey
    Ioffe et al. ([http://proceedings.mlr.press/v37/ioffe15.pdf](http://proceedings.mlr.press/v37/ioffe15.pdf)).
    As the name suggests, it is a normalization technique that normalizes the intermediate
    outputs of deep networks.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '“Why is this important?” you might ask. It turns out deep networks can cause
    massive headaches if not properly cared for. For example, a batch of improperly
    scaled/ anomalous inputs during training or incorrect weight initialization can
    lead to a poor model. Furthermore, such problems can amplify along the depth of
    the network or over time, leading to changes to the distribution of the inputs
    received by each layer over time. The phenomenon where the distribution of the
    inputs is changed over time is known as a *covariate shift*. This is very common,
    especially in streaming data problems. Batch normalization was invented to solve
    this problem. Let’s understand how BN solves this problem. The batch normalization
    layer does the following things:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Normalize *x*^((k)), the outputs of the *k*^(th) layer of the network using
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![07_09a](../../OEBPS/Images/07_09a.png)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Here, *E*[*x*^((k))] represents the mean of the output, and *Var*[*x*^((k))]
    is the variance of the output. Both *E*[*x*^((k))] and *Var*[*x*^((k))] are vectors.
    For a fully connected layer with n nodes, both *E*[*x*^((k))] and *Var*[*x*^((k))]
    are n-long vectors (computed by taking mean-over-batch dimension). For a convolutional
    layer with f filters/kernels, *E*[*x*^((k))] and *Var*[*x*^((k))] will be f-long
    vectors (computed by taking mean over batch, height, and width dimensions).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale and offset the normalized output using two trainable hyperparameters,
    γ and β (defined separately for each layer), as
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*^((k)) = *γ*^((k))*x̂*^((k)) + *β^((k))*'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this process, computing *E*(*x*) and *Var*(*x*) gets a bit tricky, as these
    need to be treated differently in the training and testing phases.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, following the stochastic (i.e., looking at a random batch of
    data instead of the full data set at a given time) nature of the training for
    each batch, *E*(*x*) and *Var*(*x*) are computed using only that batch of data.
    Therefore, for each batch, you can compute *E*(*x*) (mean) and *Var*(*x*) (variance)
    without worrying about anything except the current batch.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, using each *E*(*x*) and *Var*(*x*) computed for each batch of data, we
    estimate *E*(*x*) and *Var*(*x*) for the population. This is achieved by computing
    the running mean of *E*(*x*) and *Var*(*x*). We will not discuss how the running
    mean works. But you can imagine the running mean as an efficiently computed approximate
    representation of the true mean for a large data set.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the testing phase, we use the population-based *E*(*x*) and *Var*(*x*)
    that we computed earlier and perform the earlier defined computations to get *y*^((k)).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the complex steps involved in the batch normalization, it will take quite
    some effort to implement this from the scratch. Luckily, you don’t have to. There
    is a batch normalization layer provided in TensorFlow ([http://mng.bz/Qv0Q](http://mng.bz/Qv0Q)).
    If you have the output of some dense layer (let’s call it dense1) to inject batch
    normalization, all you need to do is
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Then TensorFlow will automatically take care of all the complex computations
    that need to happen under the hood for batch normalization to work properly. Now
    it’s time to use this powerful technique in our Minception model. In the next
    listing, you can see the implementation of the stem of Minception net. We will
    write a function called stem, which allows us to turn on/off batch normalization
    at will.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Defining the stem of Minception
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Defines the function. Note that we can switch batch normalization on and off.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The first part of the stem until the first split
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Note that first batch normalization is applied before applying the nonlinear
    activation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Nonlinear activation is applied to the layer after the batch normalization
    step.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The two parallel streams of the first split
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Concatenates the outputs of the two parallel streams in the first split
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 连接第一个分割的两个并行流的输出
- en: ❼ First stream of the second split
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 第二个分割的第一个流
- en: ❽ Second stream of the second split
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 第二个分割的第二个流
- en: ❾ Concatenates the outputs of the two streams in the second split
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 连接第二个分割的两个流的输出
- en: ❿ The third (final split) and the concatenation of the outputs
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 第三个（最终分割）和输出的连接
- en: A key change you should note is that the nonlinear activation of each layer
    is separated from the layers. This is so that batch normalization can be inserted
    in between the output of the layer and the nonlinear activation. This is the original
    way to apply batch normalization, as discussed in the original paper. But whether
    BN should come before or after the nonlinearity is an ongoing discussion. You
    can find a casual discussion on this topic at [http://mng.bz/XZpp](http://mng.bz/XZpp).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键变化需要注意，即每一层的非线性激活与层本身分开。这样做是为了能够在层的输出和非线性激活之间插入批量归一化。这是应用批量归一化的原始方式，正如原始论文中所讨论的那样。但是批量归一化应该在非线性激活之前还是之后是一个持续讨论的问题。您可以在[http://mng.bz/XZpp](http://mng.bz/XZpp)上找到关于这个主题的非正式讨论。
- en: 7.2.2 Implementing Inception-ResNet type A block
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 实现Inception-ResNet类型A块
- en: With the stem of the network behind us, let’s move forward to see what the Inception
    blocks look like in Minception net. Let’s quickly revisit what the Inception block
    is and why it was developed. The Inception block was developed to maximize the
    representational power of convolution layers while encouraging sparsity in model
    parameters and without shooting the memory requirements through the roof. It does
    this by having several parallel convolution layers with varying receptive field
    sizes (i.e., kernel sizes). The Inception block in Minception net uses mostly
    the same framework. However, it introduces one novel concept, known as *residual
    connections*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们已经讨论了网络的干部后，让我们继续看看在Minception网络中Inception块是什么样子的。让我们快速回顾一下Inception块是什么以及为什么会开发它。Inception块的开发旨在最大化卷积层的表示能力，同时鼓励模型参数的稀疏性，而不会使内存需求激增。它通过具有不同感知域大小（即内核大小）的几个并行卷积层来实现这一点。Minception网络中的Inception块主要使用相同的框架。但是，它引入了一个新概念，称为*残差连接*。
- en: 'Residual/skip connections: Shortcuts to stable gradients'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 残差/跳过连接：稳定梯度的捷径
- en: 'We have already touched lightly on residual connections, which introduce one
    of the simplest operations you can think of in mathematics: element-wise adding
    of an input to an output. In other words, you take a previous output of the network
    (call it x) and add it to the current output (call it y), so you get the final
    output z as z = x + y.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经简要讨论了残差连接，它引入了数学中可以想象的最简单的操作之一：将输入逐元素添加到输出中。换句话说，您取网络的前一个输出（称为x）并将其添加到当前输出（称为y）中，因此您得到最终输出z为z
    = x + y。
- en: '![07-09-unnumb-1](../../OEBPS/Images/07-09-unnumb-1.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![07-09-unnumb-1](../../OEBPS/Images/07-09-unnumb-1.png)'
- en: How skip/residual connections are added between convolution layers
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在卷积层之间添加跳过/残差连接
- en: One thing to be aware of when implementing residual connections is to make sure
    their sizes match, as this is an element-wise addition.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现残差连接时需要注意的一点是确保它们的尺寸匹配，因为这是逐元素的加法。
- en: What is residual about residual connections? Mathematical view
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接的数学观点是什么？
- en: It might not be obvious at first, but it’s not clear what is residual about
    skip connections. Assume the following scenario. You have an input x; next you
    have some layer, F(x) = y, that takes an input x and maps it to y. You implement
    the following network.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 起初可能不太明显，但是跳过连接中的残差之处并不清楚。假设以下情景。您有一个输入x；接下来您有一些层，F(x) = y，它将输入x映射到y。您实现了以下网络。
- en: '![07-09-unnumb-2](../../OEBPS/Images/07-09-unnumb-2.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![07-09-unnumb-2](../../OEBPS/Images/07-09-unnumb-2.png)'
- en: Mathematical view of residual connections
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接的数学观点
- en: y[k] = F(x)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] = F(x)
- en: y[k] [+ 1] = F(y[k])
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 1] = F(y[k])
- en: y[k] [+ 2] = y[k] [+ 1] + x
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 2] = y[k] [+ 1] + x
- en: y[k] [+ 2] = y[k] [+ 1] + G(x); let us consider the residual connections as
    a layer that does identity mapping and call it G.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 2] = y[k] [+ 1] + G(x); 让我们将残差连接视为一个执行恒等映射的层，并将其称为G。
- en: y[k] [+ 2] - y[k] [+ 1] = G(x) or
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 2] - y[k] [+ 1] = G(x) 或
- en: G(x) = y[k] [+ 2] - y[k] [+ 1]; G, in fact, represents the residual between
    the final output and the previous output.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: G(x) = y[k] [+ 2] - y[k] [+ 1]; 实际上，G代表了最终输出和前一个输出之间的残差。
- en: 'By considering final output as a layer H that takes x and y[k] [+ 1] as inputs,
    we obtain the following equation:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: G(x) = H(x, y[k] [+ 1]) - F(y[k])
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: You can see how the residual enters the picture. Essentially, G(x) is a residual
    between the final layer output and the previous layer’s output
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'It could not be easier to implement residual connections. Assume you have the
    following network:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You’d like to create a residual connection from d1 to d3. Then all you need
    to do is
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: or, if you want to use a Keras layer (equivalent to the previous operation),
    you can do
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'There you have it: d4 is the output of a residual connection. You might remember
    that I said the output sizes must match in order for the residual connections
    to be added. Let’s try adding two incompatible shapes. For example, let’s change
    the Dense layer to have 30 nodes instead of 20:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you try to run this code, you’ll get the following error:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see, TensorFlow is complaining that it was not able to broadcast
    (in this context, this means performing element-wise addition) two tensors with
    shapes 30 and 20\. This is because TensorFlow doesn’t know how to add a (batch_size,
    20) tensor to (batch_size, 30). If you see a similar error when trying to implement
    residual connections, you should go through the network outputs and make sure
    they match. To get rid of this error, all you need to do is change the code as
    follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Minception has two types of Inception blocks (type A and type B). Now let’s
    write Inception-ResNet block (type A) as a function inception_resnet_a. Compared
    to the Inception block you implemented earlier, this new inception block has the
    following additions:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Uses batch normalization
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses a residual connection from the input to the final output of the block
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 7.10 compares Inception-ResNet block type A of Minception to Inception
    Net v1\. An obvious difference is that Inception Net v1 does not harness the power
    of residual connections.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![07-10](../../OEBPS/Images/07-10.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 Comparison between Inception-ResNet block A (Minception) and Inception
    net v1’s Inception block
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement the Minception-ResNet block A. Figure 7.11 shows the type
    of computations and their connectivity that need to be implemented (listing 7.7).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![07-11](../../OEBPS/Images/07-11.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 Illustration of the Minception-ResNet block A with annotations from
    code listing 7.7
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Implementation of Minception-ResNet block A
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ The first parallel stream in the block
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The second parallel stream in the block
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The third parallel stream in the block
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Concatenate the outputs of the three separate streams.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Incorporate the residual connection (which is multiplied by a factor to improve
    the gradient flow).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Though the function appears long, it is mostly playing Legos with convolution
    layers. Figure 7.11 provides you the mental map between the visual inception layer
    and the code. A key observation is how the batch normalization and the nonlinear
    activation (ReLU) are applied in the top part of the block. The last 1 × 1 convolution
    uses batch normalization, not nonlinear activation. Nonlinear activation is only
    applied after the residual connections.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: We are now going to see how to implement the Inception-ResNet B block.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3 Implementing the Inception-ResNet type B block
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next up is the Inception-ResNet type B block in the Minception network. We will
    not talk about this at length as it is very similar to the Inception-ResNet A
    block. Figure 7.12 depicts the Inception-ResNet B block and compares it to Inception-ResNet
    A block. Block B looks relatively simpler than block A, with only two parallel
    streams. The code-related annotations help you map the mental model of the Inception
    block to the code, as shown in the following listing.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![07-12](../../OEBPS/Images/07-12.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 Minception’s Inception-ResNet block B (left) and Minception’s Inception-ResNet
    block A (right) side by side
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 The implementation of Minception-ResNet block B
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ The first parallel stream in the block
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The second parallel stream in the block
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Concatenate the results from the two parallel streams.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The final convolution layer on top of the concatenated result
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Applies the weighted residual connection
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: This is quite similar to the function inception_resnet_a(...), with two parallel
    streams and residual connections. The differences to note are that the type A
    block has a larger number of convolution layers than the type B block. In addition,
    the type A block uses a 5 × 5 convolution (factorized to two 3 × 3 convolution
    layers) and type B uses a 7 × 7 convolution (factorized to 1 × 7 and 7 × 1 convolution
    layers). I will leave it up to the reader to explore the function in detail.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4 Implementing the reduction block
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inspired by Inception-ResNet models, Minception also uses reduction blocks.
    Reduction blocks are quite similar to Resnet blocks, with the exception of not
    having residual connections in the blocks (see the next listing).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 Implementation of the reduction block of Minception
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ First parallel stream of convolutions
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Second parallel stream of convolutions
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Third parallel stream of pooling
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Concatenates all the outputs
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: I will let figure 7.13 speak for itself in terms of explaining listing 7.9\.
    But as you can see, at an abstract level it uses the same types of connections
    and layers as the Inception blocks we discussed.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![07-13](../../OEBPS/Images/07-13.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 Illustration of the reduction block
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re going to see how we can complete the puzzle of Minception by collating
    all the different elements we have implemented thus far.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.5 Putting everything together
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Great work so far. With all the basic blocks ready, our Minception model is
    taking shape. Next, it’s a matter of putting things where they belong. The final
    model uses the following components:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: A single stem
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1x Inception-ResNet block A
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2x Inception-ResNet block B
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average pooling
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final prediction layer with 200 nodes and softmax activation
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, we will make a few more changes to the inputs of the model. According
    to the original paper, the model takes in a 56 × 56 × 3-sized input instead of
    a 64 × 64 × 3-sized input. This is done by the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '*Training phase*—Randomly cropping a 56 × 56 × 3-sized image from the original
    64 × 64 × 3-sized image'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Validation/testing phase*—Center cropping a 56 × 56 × 3-sized image from the
    original image'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, we will introduce another augmentation step to randomly contrast
    images during the training (as used in the paper). Unfortunately, you cannot achieve
    either of these steps with the ImageDataGenerator. The good news is that since
    TensorFlow 2.2, there have been several new image preprocessing layers introduced
    ([http://mng.bz/yvzy](http://mng.bz/yvzy)). We can incorporate these layers just
    like any other layer in the model. For example, we start with the input just like
    before:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then you import the RandomCrop and RandomContrast layers and use them as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The final model looks like the following listing.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 The final Minception model
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Define the 64 × 64 Input layer.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Perform random cropping on the input (randomness is only activated during
    training).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Perform random contrast on the input (randomness is only activated during
    training).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Define the output of the stem.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Define the Inception-ResNet block (type A).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Define a reduction layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Define 2 Inception-ResNet block (type B).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Define the final prediction layer.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Define the model.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Compile the model with categorical crossentropy loss and the adam optimizer.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Finally, our Minception model is ready for battle. It takes in a 64 × 64 × 3-sized
    input (like the other models we implemented). It then randomly (during training)
    or center (during validation/testing) crops the image and applies random contrast
    adjustments (during training). This is taken care of automatically. Next, the
    processed input goes into the stem of the network, which produces the output stem_out,
    which goes into an Inception-ResNet block of type A and flows into a reduction
    block. Next, we have two Inception-ResNet type B blocks, one after the other.
    This is followed by an average pooling layer, a Flatten layer that squashes all
    dimensions except the batch dimension to 1\. Then a dropout layer with 50% dropout
    is applied on the output. Finally, a dense layer with 200 nodes (one for each
    class) with softmax activation produces the final output. Lastly, the model is
    compiled using the categorical cross-entropy loss and the adam optimizer.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: This ends our conversation about the Minception model. Do you want to know how
    much this will boost our model’s performance? In the next section, we will train
    the Minception model we defined.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.6 Training Minception
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we’re on to training the model. The training process is very similar to
    what you already did for the Inception Net v1 model, with one difference. We are
    going to use a learning rate reduction schedule to further reduce overfitting
    and improve generalizability. In this example, the learning rate scheduler will
    reduce the learning rate if the model’s performance doesn’t improve within a predefined
    duration (see the next listing).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.11 Training the Minception model
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Sets up an early stopping callback
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Sets up a CSV logger to record metrics
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Sets up a learning rate control callback
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Trains the model
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'When training deep networks, using a learning rate schedule instead of a fixed
    learning rate is quite common. Typically, we get better performance by using a
    higher learning rate at the beginning of the model training and then using a smaller
    learning rate as the model progresses. This is because, as the model converges
    during the optimization process, you should make the step size smaller (i.e.,
    the learning rate). Otherwise, large step sizes can make the model behave erratically.
    We can be smart about this process and reduce the learning rate whenever we do
    not see an increase in an observed metric instead of reducing the learning rate
    in fixed intervals. In Keras you can easily incorporate this into model training
    via the callback ReduceLROnPlateau ([http://mng.bz/M5Oo](http://mng.bz/M5Oo)):'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When using the callback, you need to set the following keyword arguments:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: monitor—Defines the observed metric. In our example, we will decide when to
    reduce the learning rate based on validation loss.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: factor—The multiplicative factor to reduce the learning rate by. If the learning
    rate is 0.01, a factor of 0.1, this means, on reduction, that the learning rate
    will be 0.001.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: patience—Similar to early stopping, how many epochs to wait before reducing
    the learning rate with no improvement in the metric.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mode—Similar to early stopping, whether the metric minimization/maximization
    should be considered as an improvement.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When you train your model, you should get an output like the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Amazing! We get a tremendous accuracy boost just by tweaking the model architecture.
    We now have a model that has around 50% accuracy on the validation set (which
    is equivalent to identifying 100/200 classes of objects accurately, or 50% of
    images classified that are accurate for each class). You can see the interventions
    made by the ReduceLROnPlateau callback in the output.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we save the model using
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we can measure the model’s performance on the test set:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This should give around 51% accuracy on the test set. That’s very exciting news.
    We have almost doubled the performance of the previous model by paying more attention
    to the structure of the model.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: This is a good lesson that teaches us the vital role played by the model architecture
    in deep learning. There’s a misconception that deep learning is the silver bullet
    that solves anything. It is not. For example, you shouldn’t expect any random
    architecture that’s put together to work as well as some of the state-of-the-art
    results published. Getting a well-performing deep network can be a result of days
    or even weeks of hyperparameter optimization and empirically driven choices.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will leverage transfer learning to reach a higher degree
    of accuracy faster. We will download a pretrained model and finetune it on the
    specific data set.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 1 hour and 54 minutes to run 50 epochs.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'You have the following convolution block that you are using to implement an
    image classifier:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You would like to make the following two changes:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Introduce batch normalization after applying the activation
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a residual connection from the output of the convolution layer to the
    output of the batch normalization layers output.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '7.3 If you can''t beat them, join ‘em: Using pretrained networks for enhancing
    performance'
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, you have developed a good image classification model, which uses various
    methods to prevent overfitting. The company was happy until your boss let out
    the news that there’s a new competitor in town that is performing better than
    the model you developed. Rumor is that they have a model that’s around 70% accurate.
    So, it’s back to the drawing board for you and your colleagues. You believe that
    a special technique known as transfer learning can help. Specifically, you intend
    to use a pretrained version of Inception-ResNet v2 that is already trained on
    the original ImageNet image classification data set; fine-tuning this model on
    the tiny-imagenet-200 data set will provide better accuracy than all the models
    implemented thus far.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: If you want to come close to state of the art, you must use every bit of help
    you can get. A great way to begin this quest is to start with a pretrained model
    and then fine-tune it for your task. A pretrained model is a model that has already
    being trained on a similar task. This process falls under the concept of *transfer
    learning*. For example, you can easily find models that have been pretrained on
    the ILSVRC task.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '7.3.1 Transfer learning: Reusing existing knowledge in deep neural networks'
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transfer learning is a massive topic and is something for a separate chapter
    (or even a book). There are many variants of transfer learning. To understand
    different facets of transfer learning, refer to [https://ruder.io/transfer-learning/](https://ruder.io/transfer-learning/).
    One method is to use a pretrained model and fine-tune it for the task to be solved.
    The process looks like figure 7.14.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![07-14](../../OEBPS/Images/07-14.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 How transfer learning works. First, we start with a model that is
    pretrained on a larger data set that is solving a similar/relevant task to the
    one we’re interested in. Then we transfer the model weights (except the last layer)
    and fit a new prediction layer on top of the existing weights. Finally, we fine-tune
    the model on a new task.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: First, you train the model on a task for which you already have a large labeled
    data set (known as the pretrained task). For example, in image classification,
    you have several large labeled data sets, including the ImageNet data set. Once
    you train a model on the large data set, you get the weights of the network (except
    for the final prediction layer) and fit a new prediction layer that matches the
    new task. This gives a very good starting point for the network to solve the new
    task. You are then able to solve the new task with a smaller data set, as you
    have already trained your model on similar, larger data sets.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we use transfer learning to solve our problem? It is not that difficult.
    Keras provides a huge model repository for image classification tasks ([http://mng.bz/aJdo](http://mng.bz/aJdo)).
    These models have been trained predominantly on the ImageNet image classification
    task. Let’s tame the beast produced in the lineage of Inception networks: Inception-ResNet
    v2\. Note that the code for this section can be found at Ch07-Improving-CNNs-and-Explaining/7.2.Transfer_Learning.ipynb.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Inception-ResNet v2
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'We briefly touched on the Inception-ResNet v2 model. It was the last Inception
    model produced. Inception-ResNet v2 has the following characteristics that set
    it apart from other inception models:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Redesigned stem that removes any representational bottlenecks
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception blocks that use residual connections
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction modules that reduce the height/width dimensions of the inputs
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not use auxiliary outputs as in the early Inception nets
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the redesigned stem, Inception-ResNet blocks, and reduction
    modules are being used in the Minception model. And if you compare the diagrams
    of Minception that are provided to the diagrams in the original paper, you will
    see how many similarities they share. Therefore, we will not repeat our discussion
    of these components. If you still want to see the specific details and illustrations
    of the different components, refer to the original paper ([https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf)).
    However, the high-level architecture of Inception-ResNet v2 looks like the following
    figure.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![07-14-unnumb-3](../../OEBPS/Images/07-14-unnumb-3.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: The overall architecture of Inception-ResNet v2 architecture
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the Inception-ResNet v2 model with a single line:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, include_top=False means that the final prediction layer will be discarded.
    It is necessary because the original inception net is designed for 1,000 classes.
    However, we only have 200 classes. pooling='avg' ensures that the last pooling
    layer in the model is an average pooling layer. Next, we will create a new model
    that encapsulates the pretrained Inception-ResNet v2 model as the essence but
    is modified to solve the tiny-ImageNet classification task, as shown in the next
    listing.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.12 Implementing a model based on the pretrained Inception-ResNet v2
    model
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Some important imports
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Defining an input layer for a 224 × 224 image
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The pretrained weights of the Inception-ResNet v2 model
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Apply 40% dropout
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Final prediction layer with 200 classes
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Using a smaller learning rate since the network is already trained on ImageNet
    data (chosen empirically)
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see that we are defining a sequential model that
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: First defines an input layer of size 224 × 224 × 3 (i.e., height = 224, width
    = 224, channels = 3)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defines the Inception-ResNet v2 model as a layer
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses a dropout of 40% on the last average pooling layer
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defines a dense layer that uses softmax activation and has 200 nodes
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One crucial challenge we need to deal with is that the original input Inception-ResNet
    v2 is designed to consume is size 224 × 224 × 3\. Therefore, we will need to find
    a way to present our inputs (i.e., 64 × 64 × 3) in a way that complies with Inception-ResNet
    v2’s requirements. In order to do that, we will make some changes to the ImageDataGenerator,
    as the following listing shows.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.13 The modified ImageDataGenerator that produces 224 × 224 images
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Defines a data-augmenting image data generator and a standard image data generator
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Defines a partial function to avoid repeating arguments
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Uses a target size of 224 × 224
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Uses bilinear interpolation to make images bigger
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Defines the data generators for training and validation sets
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Defines the test data generator
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Uses a target size of 224 × 224 and bilinear interpolation
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Defines the batch size and target size
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Gets the train/valid/test modified data generators using the data_gen_augmented
    function
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it’s time for the grand unveil! We will train the best model we’ve
    come up with:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The training will be identical to the earlier training configuration we used
    when training the Minception model. We will not repeat the details. We are using
    the following:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Metric logging
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate scheduling
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 9 hours and 20 minutes to run 23 epochs.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'You should get a result similar to the following:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Isn’t this great news? We have reached around 74% validation accuracy by combining
    all we have learned. Let’s quickly look at the test accuracy of the model:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This should show you around ~79% accuracy. It hasn’t been an easy journey, but
    you obviously have surpassed your competitor’s model of ~70% accuracy.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the importance of model explainability.
    We will learn about a technique that we can use to explain the knowledge embedded
    in our model.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Inception-ResNet v2 versus Minception
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'The stem of the Minception and Inception-Resnet-v2 are identical in terms of
    the innovations they introduce (e.g., Inception-ResNet blocks, reduction blocks,
    etc.). However, there are the following low-level differences:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Inception-ResNet v2 has three different Inception block types; Minception has
    only two.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception-ResNet v2 has two different types of reduction blocks; Minception
    has only one.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception-ResNet v2 has 25 Inception layers, but Minception (the version we
    implemented) has only three.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also other minor differences, such as the fact that Inception-ResNet
    v2 uses valid padding in a few layers of the model. Feel free to consult the Inception-ResNet
    v2 paper if you want to know the details. Another notable observation is that
    neither the Minception nor the Inception-ResNet v2 uses local response normalization
    (LRN), as they use something far more powerful: batch normalization.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: You want to implement a network using a different pretrained network known as
    VGGNet (16 layers). You can obtain the pretrained network from tf.keras.applications.VGG16.
    Next, you discard the top layer and introduce a max pooling layer on top. Then
    you want to add two dense layers on top of the pretrained network with 100 (ReLU
    activation) and 50 (Softmax activation) nodes. Implement this network.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '7.4 Grad-CAM: Making CNNs confess'
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The company can’t be happier about what you have done for them. You have managed
    to build a model that not only beat the performance of the competitor, but also
    is one of the best in production. However, your boss wants to be certain that
    the model is trustworthy before releasing any news on this. Accuracy alone is
    not enough! You decide to demonstrate how the model makes predictions using a
    recent model interpretation technique known as *Grad-CAM*. Grad-CAM uses the magnitude
    of the gradients generated for a given input with respect to the model’s predictions
    to provide visualizations of where the model focused. A large magnitude of gradients
    in a certain area of an image means that the image focuses more in that area.
    And by superimposing the gradient magnitudes depicted as a heatmap, you are able
    to produce an attractive visualization of what the model is paying attention to
    in a given input.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'Grad-CAM (which stands for gradient class activation map) is a model interpretation
    technique introduced for deep neural networks by Ramprasaath R. Selvaraju et al.
    in “Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization”
    ([https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)).
    Deep networks are notorious for their inexplicable nature and are thus termed
    *black boxes*. Therefore, we must do some analysis and ensure that the model is
    working as intended.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: The following code delineates how Grad-CAM works its magic, and the implementation
    is available in the notebook Ch07-Improving-CNNs-and-Explaining/7.3 .Interpreting_CNNs_GradCAM.ipynb.
    In the interest of conserving the length of this chapter, we will discuss only
    the pseudocode of this approach and will leave the technical details to appendix
    B (see the next listing).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.14 Pseudocode of Grad-CAM computations
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The key computation that is performed by Grad-CAM is, given an input image,
    taking the gradient of the node that corresponds to the true class of the image
    with respect to the last convolution output of the model.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: The magnitude of the gradient at each pixel of the images represents the contribution
    that pixel made to the final outcome. Therefore, by representing Grad-CAM output
    as a heatmap, resizing to match the original image, and superimposing that on
    the original image, you can get a very attractive and informative plot of where
    the model focused to find different objects. The plots are self-explanatory and
    show whether the model is focusing on the correct object to produce a desired
    prediction. In figure 7.15, we show which areas the model focuses on strongly
    (red/dark = highest focus, blue/light = less focus).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![07-15](../../OEBPS/Images/07-15.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 Visualization of the Grad-CAM output for several probe images. The
    redder/darker an area in the image, the more the model focuses on that part of
    the image. You can see that our model has learned to understand some complex scenes
    and separate the model that it needs to focus on.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.15 (i.e., visualization of Grad-CAM) shows that our model is truly
    an intelligent model. It knows where to focus to find a given object, even in
    cluttered environments (e.g., classifying the dining table). As mentioned earlier,
    the redder/ darker the area, the more the model focuses on that area to make a
    prediction. Now it’s time for you to demonstrate the results to your boss and
    build the needed confidence to go public with the new model!
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: We will end our discussion about image classification here. We have learned
    about many different models and techniques that can be used to solve the problem
    effectively. In the next chapter, we will discuss a different facet of computer
    vision known as image segmentation.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image augmentation, dropout, and early stopping are some of the common techniques
    used to prevent overfitting in vision deep networks.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the common image augmentation steps can be achieved through the Keras
    ImageDataGenerator.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to pay attention to the architecture of the model chosen for
    a given problem. One should not randomly choose an architecture but research and
    identify an architecture that has worked for a similar problem. Otherwise, choose
    the architecture through hyperparameter optimization. The Minception model’s architecture
    has been proven to work well on the same data we used in this chapter.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning enables us to use already trained models to solve new tasks
    with better accuracy.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Keras you can get a given model with a single line of code and adapt it to
    the new task.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various pretrained networks are available at [http://mng.bz/M5Oo](http://mng.bz/M5Oo).
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grad-CAM (gradient class activation map) is an effective way to interpret your
    CNN.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grad-CAM computes where the model focused the most based on the magnitude of
    gradients produced with respect to the prediction made by the model.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1**'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'You should reduce the dropout rate to keep more nodes switched during training
    if underfitting is occurring:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Early stopping is introduced using the EarlyStopping callback:'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '**Exercise 2**'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Exercise 3**'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Exercise 4**'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
