- en: '7 Teaching machines to see better: Improving CNNs and making them confess'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教会机器更好地看：改善 CNNs 并让它们承认
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Reducing overfitting of image classifiers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少图像分类器的过拟合
- en: Boosting model performance via better model architectures
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过更好的模型架构提升模型性能
- en: Image classification using pretrained models and transfer learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型和迁移学习进行图像分类
- en: Modern ML explainability techniques to dissect image classifiers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代 ML 解释技术来解剖图像分类器
- en: We have developed and trained a state-of-the-art image classifier known as the
    Inception net v1 on an object classification data set. Inception net v1 is a well-recognized
    image classification model in computer vision. You learned how Inception blocks
    are created by aggregating convolution windows at multiple scales, which encourages
    sparsity in the model. You further saw how 1 × 1 convolutions are employed to
    keep the dimensionality of layers to a minimum. Finally, we observed how Inception
    net v1 uses auxiliary classification layers in the middle of the network to stabilize
    and maintain the gradient flow throughout the network. However, the results didn’t
    really live up to the reputation of the model, which was heavily overfit with
    ~30% validation and test accuracies and a whopping ~94% training accuracy. In
    this chapter, we will discuss improving the model by reducing overfitting and
    improving validation and test accuracies, which will ultimately leave us with
    a model that reaches ~80% accuracy (equivalent to being able to accurately identify
    160/200 classes of objects) on validation and test sets. Furthermore, we will
    look at techniques that allow us to probe the model’s brain to gain insights.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开发并训练了一款名为 Inception net v1 的最先进的图像分类器，它在一个物体分类数据集上进行了训练。Inception net v1
    是计算机视觉中一个广为人知的图像分类模型。你学习了 Inception 块是如何通过在多个尺度上聚合卷积窗口来创建的，这鼓励了模型中的稀疏性。你还看到了如何使用
    1 × 1 卷积来保持层的维度最小。最后，我们观察到 Inception net v1 如何在网络中部使用辅助分类层来稳定和维持整个网络中的梯度流。然而，结果并没有真正达到模型的声誉，它在验证和测试准确性上过度拟合，验证和测试准确率约为
    30%，而训练准确率则高达约 94%。在本章中，我们将讨论通过减少过拟合和提高验证和测试准确率来改善模型，最终将使我们得到一个在验证和测试集上达到约 80%
    准确率（相当于能够准确识别 160/200 类物体）的模型。此外，我们还将研究允许我们探索模型思维的技术，以获得洞察。
- en: 'This chapter takes you through an exciting journey where we turn a suboptimal
    machine learning model to a significantly superior model. This process will be
    reminiscent of what we did in the previous chapter. We will add an additional
    step to explain/interpret the decisions the model made. We will use special techniques
    to see which part of the image the model paid most attention to in order to make
    a prediction. This helps us to build trust in the model. During this process,
    we identify lurking issues in the model and systematically fix them to increase
    performance. We will discuss several important techniques, including the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带你走过一个激动人心的旅程，在这个旅程中，我们将把一个次优的机器学习模型变成一个显著优秀的模型。这个过程将使人联想起我们在上一章所做的事情。我们将增加一个额外的步骤来解释/解读模型所做的决策。我们将使用特殊的技术来看看模型在做出预测时对图像的哪个部分支付了最多的注意力。这有助于我们对模型建立信任。在这个过程中，我们识别出模型中潜藏的问题，并系统地修复它们以提高性能。我们将讨论几种重要的技术，包括以下内容：
- en: Augmenting the data by using various image transformation techniques such as
    brightness/contrast adjustment, rotation, and translation to create more labeled
    data for the model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用各种图像变换技术如亮度/对比度调整、旋转和平移来增广数据，为模型创建更多标记数据
- en: Implementing a variant of Inception net that is more suited for the size and
    type of data used
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个更适合所使用的数据大小和类型的 Inception net 变体
- en: Using transfer learning to leverage a model already trained on a larger data
    set and fine-tuning it to perform well on the data set we have
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习来利用已经在更大数据集上训练过的模型，并对其进行微调，以在我们拥有的数据集上表现良好
- en: This chapter might deeply resonate with you if you have ever had to implement
    a deep learning solution to an unfamiliar problem. Typically, implementing “some”
    deep network will not place you on the apex of success. The novelty of the problem
    or the bespoke nature of the problem at hand can impede your progress if you are
    not careful. Such problems send you into uncharted territory where you need to
    tread carefully to find a solution without exhausting yourself. This chapter will
    provide guidance for anyone who might come face-to-face with such situations in
    computer vision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经需要为不熟悉的问题实施深度学习解决方案，那么这一章可能会与你产生共鸣。通常，仅仅实施“某些”深度网络并不会让你登上成功的顶峰。如果问题的新颖性或手头问题的定制性质没有处理好，它会阻碍你的进展。这样的难题会把你带入未知领域，你需要小心行事，找到解决方案而不至于筋疲力尽。本章将为任何可能在计算机视觉领域面对类似情况的人提供指导。
- en: 7.1 Techniques for reducing overfitting
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 减少过拟合的技术
- en: We are pursuing the ambitious goal of developing an intelligent shopping assistant
    app that will use an image/object classifier as a vital component. For this, we
    will use the data set tiny-imagenet-200, which is a smaller version of the large
    ImageNet image classification data set, and consists of images and a class that
    represents the object present in that image. The data set has a training subset
    and a testing subset. You split the training subset further into a training set
    (90% of the original) and a validation set (10% of the original).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在追求一个雄心勃勃的目标，即开发一个智能购物助手应用程序，其中会使用图像/物体分类器作为重要组件。为此，我们将使用数据集 tiny-imagenet-200，它是大型
    ImageNet 图像分类数据集的一个较小版本，由图像和表示该图像中存在的对象类别组成。该数据集有一个训练子集和一个测试子集。你进一步将训练子集分成训练集（原始的90%）和验证集（原始的10%）。
- en: You have developed a model based on the famous Inception net model, but it is
    overfitting heavily. Overfitting needs to be alleviated, as it leads to models
    that perform exceptionally well on training data but poorly on test/real-world
    data. You know several techniques to reduce overfitting, namely data augmentation
    (creating more data out of existing data; for images this includes creating variants
    of the same image by introducing random brightness/contrast adjustments, translations,
    rotations, etc.), dropout (i.e., turning nodes randomly in the network during
    training), and early stopping (i.e., terminating model training before overfitting
    takes place). You wish to leverage these methods with the Keras API to reduce
    overfitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经基于著名的 Inception 网络模型开发了一个模型，但它严重过拟合。需要缓解过拟合，因为它会导致模型在训练数据上表现非常好，但在测试/真实世界数据上表现不佳。你知道几种减少过拟合的技术，即数据增强（从现有数据中创建更多数据；对于图像，这包括通过引入随机亮度/对比度调整、平移、旋转等方式创建相同图像的变体）、随机失活（即在训练期间随机关闭网络中的节点）以及提前停止（即在过拟合发生前终止模型训练）。你希望利用
    Keras API 来减少过拟合。
- en: Usually, reducing overfitting requires close scrutiny of the machine learning
    pipeline end to end. This involves looking at the data fed in, the model structure,
    and the model training. In this section, we will look at all these aspects and
    see how we can place guards against overfitting. The code for this is available
    at Ch07-Improving-CNNs-and-Explaining/7.1.Image_Classification_Advance.ipynb.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，减少过拟合需要仔细检查整个机器学习流程。这涉及到输入的数据、模型结构和模型训练。在这一节中，我们将看看所有这些方面，并了解如何防止过拟合。此部分的代码可在
    Ch07-Improving-CNNs-and-Explaining/7.1.Image_Classification_Advance.ipynb 中找到。
- en: 7.1.1 Image data augmentation with Keras
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 使用 Keras 进行图像数据增强
- en: 'First in line is augmenting data in the training set. Data augmentation is
    a prevalent method for increasing the amount of data available to deep learning
    networks without labeling new data. For example, in an image classification problem,
    you can create multiple data points from a single image by creating various transformed
    versions of the same image (e.g., shift the image, change brightness) and having
    the same label as for the original image (figure 7.1). As previously stated, more
    data conduces the strength of deep learning models by increasing generalizability
    (and reducing overfitting), leading to reliable performance in the real world.
    For image data, there are many different augmentation techniques you can use:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是在训练集中增加数据。 数据增强是一种常见的方法，可以增加深度学习网络可用的数据量，而无需对新数据进行标记。 例如，在图像分类问题中，您可以通过创建同一图像的多个变换版本（例如，移动图像，更改亮度）并具有与原始图像相同的标签（图7.1）来从单个图像创建多个数据点。
    如前所述，更多数据通过增加泛化能力（减少过拟合）来增强深度学习模型的强度，从而在实际应用中实现可靠的性能。 对于图像数据，您可以使用许多不同的增强技术：
- en: Randomly adjusting brightness, contrast, and so on
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机调整亮度、对比度等
- en: Randomly zooming in/out, rotations, translations, and so on
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机缩放、旋转、平移等
- en: '![07-01](../../OEBPS/Images/07-01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![07-01](../../OEBPS/Images/07-01.png)'
- en: Figure 7.1 Difference between training data and validation data after the augmentation
    step. The figure clearly shows various transformations applied on the training
    data and not on the validation data, as we expected.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 在增强步骤之后的训练数据和验证数据之间的差异。 图清楚地显示了对训练数据应用的各种转换，而对验证数据未应用，正如我们所预期的那样。
- en: Such augmentation can be easily applied by providing several additional parameters
    to the ImageDataGenerator that we used earlier. Let’s define a new Keras ImageDataGenerator
    with data augmentation capability. In Keras you can perform most of these augmentations,
    and there’s hardly a need to look elsewhere. Let’s look at various options an
    ImageDataGenerator provides (only the most important parameters are shown). Figure
    7.2 illustrates the effects of the different parameters listed here.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向我们之前使用的ImageDataGenerator提供几个额外参数，可以轻松地实现这种增强。 让我们定义一个新的Keras ImageDataGenerator，具有数据增强功能。
    在Keras中，您可以执行大多数这些增强，几乎不需要去其他地方寻找。 让我们看看ImageDataGenerator提供的各种选项（仅显示了最重要的参数）。
    图7.2说明了此处列出的不同参数的效果。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: featurewise_center specifies whether the images are centered by subtracting
    the mean value of the whole data set (e.g., True/False).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: featurewise_center指定是否通过减去整个数据集的平均值来使图像居中（例如，True/False）。
- en: samplewise_center specifies whether the images are centered by subtracting individual
    mean values of each image (e.g., True/False).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: samplewise_center指定是否通过减去每个图像的单个平均值来使图像居中（例如，True/False）。
- en: featurewise_std_normalization is the same as featurewise_center, but instead
    of subtracting, the mean images are divided by the standard deviation (True/False).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: featurewise_std_normalization与featurewise_center相同，但是将图像除以标准偏差而不是减去平均值（True/False）。
- en: samplewise_std_normalization is the same as samplewise_center, but instead of
    subtracting, the mean images are divided by the standard deviation (True/ False).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: samplewise_std_normalization与samplewise_center相同，但是将图像除以标准偏差而不是减去平均值（True/False）。
- en: zca_whitening is a special type of image normalization that is geared toward
    reducing correlations present in the image pixels (see [http://mng.bz/DgP0](http://mng.bz/DgP0))
    (True/False).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: zca_whitening是一种特殊类型的图像归一化，旨在减少图像像素中存在的相关性（请参阅[http://mng.bz/DgP0](http://mng.bz/DgP0)）（True/False）。
- en: rotation_range specifies the bounds of the random image rotations (in degrees)
    done during data augmentation. There is a float with values between (0, 360);
    for example, 30 means a range of -30 to 30; 0 is disabled.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rotation_range指定在数据增强期间进行的随机图像旋转的边界（以度为单位）。 具有值在(0, 360)之间的浮点数； 例如，30表示-30到30的范围；
    0禁用。
- en: width_shift_range specifies the bounds for random shifts (as proportions or
    pixels) done on the width axis during data augmentation.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: width_shift_range指定在数据增强期间在宽度轴上进行的随机移位的边界（作为比例或像素）。
- en: A tuple with values between (-1, 1) is considered as a proportion of the width
    (e.g., (-0.4, 0.3)).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值在(-1, 1)之间的元组被视为宽度的比例（例如，(-0.4, 0.3)）。
- en: A tuple with values between (-inf, inf) is considered as pixels (e.g., (-150,
    250)).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素的值在(-inf, inf)之间的元组被视为像素（例如，(-150, 250)）。
- en: height_shift_range is the same as width_shift_range except for the height dimension.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: height_shift_range与width_shift_range相同，只是针对高度维度。
- en: brightness_range specifies the bounds of the random brightness adjustments made
    to data during data augmentation.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`brightness_range`指定在数据增强期间对数据进行的随机亮度调整的范围。'
- en: A tuple with values between (-inf, inf) is, for example, (-0.2, 0.5) or (-5,
    10); 0 is disabled.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元组中的值介于（-inf，inf）之间，例如，（-0.2，0.5）或（-5，10）；0表示禁用。
- en: shear_range is the same as brightness_range but for shearing (i.e., skewing)
    images during data augmentation
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shear_range`与`brightness_range`相同，但用于在数据增强期间剪切（即倾斜）图像。'
- en: A float in degrees is, for example, 30.0.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以度为单位的浮点数，例如，30.0。
- en: zoom_range is the same as brightness_range except for scaling the images during
    data augmentation.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoom_range`与`brightness_range`相同，除了在数据增强期间对图像进行缩放。'
- en: horizontal_flip specifies whether to randomly flip images horizontally during
    data augmentation (True/False).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizontal_flip`指定在数据增强期间是否随机水平翻转图像（是/否）。'
- en: vertical_flip is the same as horizontal_flip but flips vertically (True/False)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vertical_flip`与`horizontal_flip`相同，但垂直翻转（是/否）。'
- en: fill_mode defines how the empty spaces created by various image transformations
    (e.g., translating the image to the left creates an empty space on the right)
    are handled. Possible options are “reflect,” “nearest,” and “constant.” The last
    row of figure 7.2 depicts the differences.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fill_mode`定义了通过各种图像变换（例如，将图像向左移动会在右侧创建空白空间）创建的空白空间如何处理。可能的选项是“reflect”，“nearest”和“constant”。图
    7.2 的最后一行显示了差异。'
- en: rescale rescales the inputs by a constant value.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale`通过常量值重新缩放输入。'
- en: preprocessing_function takes a Python function that can be used to introduce
    additional data augmentation/preprocessing steps that are not readily available.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocessing_function`接受一个Python函数，该函数可用于引入额外的数据增强/预处理步骤，这些步骤不容易获得。'
- en: validation_split addresses how much data should be used as validation data.
    We don’t use this parameter, as we create a data generator for the validation
    set separately because we do not want an augmentations app. A float is, for example,
    0.2.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_split`解决了应该将多少数据用作验证数据的问题。我们不使用此参数，因为我们单独为验证集创建数据生成器，因为我们不希望有增强应用。一个浮点数，例如，0.2。'
- en: '![07-02](../../OEBPS/Images/07-02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![07-02](../../OEBPS/Images/07-02.png)'
- en: Figure 7.2 Effects of different augmentation parameters and their values of
    the ImageDataGenerator
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 不同增强参数及其`ImageDataGenerator`的值的效果。
- en: 'With a good understanding of different parameters, we will define two image
    data generators: one with data augmentation (training data) and the other without
    (testing data). For our project, we will augment the data in the following ways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对不同参数有良好的理解，我们将定义两个图像数据生成器：一个用于数据增强（训练数据），另一个不用于数据增强（测试数据）。对于我们的项目，我们将以以下方式增强数据：
- en: Randomly rotate images
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机旋转图像。
- en: Randomly translate on width dimension
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在宽度维度上随机平移。
- en: Randomly translate on height dimension
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高度维度上随机平移。
- en: Randomly adjust brightness
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机调整亮度。
- en: Randomly shear
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机剪切。
- en: Randomly zoom
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机缩放。
- en: Randomly flip images horizontally
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机水平翻转图像。
- en: Random gamma correct (custom implementation)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机伽马校正（自定义实现）。
- en: Random occlude (custom implementation)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机遮挡（自定义实现）。
- en: The following listing shows how theImageDataGenerator is defined with a validation
    split.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了如何使用验证分割定义`ImageDataGenerator`。
- en: Listing 7.1 Defining the ImageDataGenerator with validation split
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 定义了具有验证分割的`ImageDataGenerator`。
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Defines the ImageDataGenerator for training/validation data
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义用于训练/验证数据的`ImageDataGenerator`。
- en: ❷ We will switch off samplewise_center temporarily and reintroduce it later.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们将暂时关闭`samplewise_center`并稍后重新引入它。
- en: ❸ Various augmentation arguments previously discussed (set empirically)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 先前讨论的各种增强参数（经验设置）。
- en: ❹ Uses a 10% portion of training data as validation data
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将训练数据的 10% 部分用作验证数据。
- en: ❺ Defines a separate ImageDataGenerator for testing data
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义了用于测试数据的单独`ImageDataGenerator`。
- en: We chose the parameters for these arguments empirically. Feel free to experiment
    with other arguments and see the effect they have on the model’s performance.
    One important thing to note is that, unlike previous examples, we set samplewise_center=False.
    This is because we are planning to do few custom preprocessing steps before the
    normalization. Therefore, we will turn off the normalization in the ImageDataGenerator
    and reintroduce it later (through a custom function). Next, we will define the
    training and testing data generators (using a flow function). Following a similar
    pattern as the previous chapter, we will get the training and validation data
    generators through the same data generator (using the validation_split and subset
    arguments; see the following listing).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经验地选择了这些参数的参数。随意尝试其他参数，并查看它们对模型性能的影响。一个重要的事情要注意的是，与以前的例子不同，我们设置了samplewise_center=False。这是因为我们计划在标准化之前进行少量自定义预处理步骤。因此，我们将关闭ImageDataGenerator中的标准化，并稍后重新引入它（通过自定义函数）。接下来，我们将定义训练和测试数据生成器（使用流函数）。与上一章类似的模式，我们将通过同一数据生成器（使用validation_split和subset参数）获取训练和验证数据生成器（参见下一个列表）。
- en: Listing 7.2 Defining the data generators for training, validation, and testing
    sets
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2：定义训练、验证和测试集的数据生成器
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Define a partial function that has all the arguments fixed except for the
    subset argument.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个偏函数，除了子集参数之外所有参数都已固定。
- en: ❷ Get the training data subset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取训练数据子集。
- en: ❸ Get the validation data subset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取验证数据子集。
- en: ❹ Read in the test labels stored in a txt file.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 读取存储在txt文件中的测试标签。
- en: ❺ Define the test data generator.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义测试数据生成器。
- en: 'To refresh our memory, the flow_from_directory(...) has the following function
    signature:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了恢复我们的记忆，flow_from_directory(...)具有以下函数签名：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The train_gen and valid_gen uses image_gen_aug (with data augmentation) to retrieve
    data. train_gen and valid_gen are defined as partial functions of the original
    image_gen.flow_from_directory(), where they share all the arguments except for
    the subset argument. However, it is important to keep in mind that augmentation
    is only applied to training data and must not be applied on the validation subset.
    This is the desired behavior we need, as we want the validation data set to remain
    fixed across epochs. Next, test_gen uses image_gen (without data augmentation).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: train_gen和valid_gen使用image_gen_aug（进行数据增强）来获取数据。train_gen和valid_gen被定义为原始image_gen.flow_from_directory()的偏函数，它们共享除子集参数之外的所有参数。但是，重要的是要记住，增强仅应用于训练数据，不得应用于验证子集。这是我们需要的期望行为，因为我们希望验证数据集跨多个周期保持固定。接下来，test_gen使用image_gen（无数据增强）。
- en: Why should we not augment validation/test data?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不应该增强验证/测试数据？
- en: When augmenting data, you should only augment the training data set and not
    the validation and test sets. Augmentation on validation and test sets will lead
    to inconsistent results between trials/runs (due to the random modifications introduced
    by data augmentation). We want to keep our validation and testing data sets consistent
    from the start to the end of training. Therefore, data augmentation is only done
    to the training data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数据增强时，应该只对训练数据集进行增强，不要对验证和测试集进行增强。在验证和测试集上进行增强会导致不同测试/运行之间结果不一致（因为数据增强引入了随机修改）。我们希望保持验证和测试数据集在训练期间始终保持一致。因此，数据增强只针对训练数据进行。
- en: Remember that Inception Net v1 has three output layers; therefore, the output
    of the generator needs to be a single input and three outputs. We do this by defining
    a new Python generator off the Keras generator that modifies the content accordingly
    (see the next listing).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Inception Net v1有三个输出层；因此，生成器的输出需要是一个输入和三个输出。我们通过定义一个新的Python生成器，修改内容以实现这一点（见下一个列表）。
- en: Listing 7.3 Defining the data generator with several modifications
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3：定义带有几个修饰的数据生成器
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Define a new function that introduces two new augmentation techniques and
    modifies the format of the final output.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个新的函数，引入两种新的增强技术，并修改最终输出的格式。
- en: ❷ Check if the Gamma correction augmentation is needed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查是否需要伽马校正增强。
- en: ❸ Perform Gamma correction-related augmentation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行伽马校正相关的数据增强。
- en: ❹ Check if random occlusion augmentation is needed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 检查是否需要随机遮挡数据增强。
- en: ❺ Defines the starting x/y pixels randomly for occlusion
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 随机定义遮挡的起始x/y像素。
- en: ❻ Apply a white/gray/black color randomly to the occlusion.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 随机为遮挡覆盖添加白色/灰色/黑色。
- en: ❼ Perform the sample-wise centering that was switched off earlier.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对之前关闭的样本居中进行样本级居中。
- en: ❽ Makes sure we replicate the target (y) three times
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 确保我们复制目标（y）三次
- en: ❾ Training data is augmented with random gamma correction and occlusions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 训练数据使用随机gamma校正和遮挡进行增强。
- en: ❿ Validation/testing sets are not augmented.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 验证/测试集不进行增强。
- en: 'You can see how the data_gen_augmented_inceptionnet_v1 returns a single input
    (x) and three replicas of the same output (y). In addition to modifying the format
    of the output, this data_gen_augmented_inceptionnet_v1 will include two extra
    data augmentation steps using a custom implementation (which are not available
    as built-ins):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到`data_gen_augmented_inceptionnet_v1`返回单个输入（x）和相同输出的三个副本（y）。除了修改输出的格式外，`data_gen_augmented_inceptionnet_v1`还将使用自定义实现包括两个额外的数据增强步骤（这些步骤不是内置的）：
- en: '*Gamma correction*—A standard computer vision transformation performed by raising
    the pixel values to the power of some value ([http://mng.bz/lxdz](http://mng.bz/lxdz)).
    In our case, we chose this value randomly between 0.9 and 1.08.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gamma校正*—标准的计算机视觉转换，通过将像素值提高到某个值的幂次方来执行（[http://mng.bz/lxdz](http://mng.bz/lxdz)）。在我们的情况下，我们在0.9和1.08之间随机选择这个值。'
- en: '*Random occlusions*—We will occlude a random patch on the image (10 × 10) with
    white, gray, or black pixels (chosen randomly).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机遮挡*—我们将在图像上随机遮挡一个随机的补丁（10 × 10），用白色、灰色或黑色像素（随机选择）。'
- en: You also need to center the images, as we set the samplewise_center argument
    to False when we defined the ImageDataGenerator. This is done by subtracting the
    mean pixel value of each image from its pixels. With the data_gen_augmented_inceptionnet_v1
    function defined, we can create the modified data generators train_gen_aux, valid_
    gen_aux, and test_gen_aux for training/validation/testing data, respectively.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义ImageDataGenerator时，也需要对图像进行居中处理，因为我们将samplewise_center参数设置为False。这通过从每个图像的像素中减去其平均像素值来完成。定义了data_gen_augmented_inceptionnet_v1函数后，我们可以为训练/验证/测试数据分别创建修改后的数据生成器train_gen_aux、valid_gen_aux和test_gen_aux。
- en: Check, check, check to avoid model performance defects
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 检查，检查，检查以避免模型性能缺陷
- en: If you don’t check to see if just the training data is augmented, you can be
    in trouble. If it doesn’t work as intended, it can easily fly under the radar.
    Technically, your code is working and free of functional bugs. But this will leave
    you scratching your head for days trying to figure out why the model is not performing
    as intended in the real world.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不检查只有训练数据是否被增强，那你可能会陷入麻烦。如果它不能按预期工作，它很容易被忽视。从技术上讲，你的代码是正常工作的，并且没有功能性的bug。但这会让你在实际情况下不断琢磨为什么模型没有按预期执行。
- en: Finally, the most important step in this process is verifying that the data
    augmentation is working as we expect and not corrupting the images in unexpected
    ways, which would impede the learning of the model. For that, we can plot some
    of the samples generated by the train data generator as well as the validation
    data generator. Not only do we need to make sure that the data augmentation is
    working properly, but we also need to make sure that data augmentation is not
    present in the validation set. Figure 7.3 ensures that this is the case.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个过程中最重要的步骤是验证数据增强是否按照我们的期望进行，而不会以意想不到的方式破坏图像，这会妨碍模型的学习。为此，我们可以绘制由训练数据生成器生成的一些样本以及验证数据生成器生成的样本。我们不仅需要确保数据增强正常工作，还需要确保验证集中不存在数据增强。图7.3确保了这一点。
- en: '![07-03](../../OEBPS/Images/07-03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![07-03](../../OEBPS/Images/07-03.png)'
- en: Figure 7.3 Difference between training data and validation data after the augmentation
    step. The figure clearly shows various transformations applied on the training
    data and not on the validation data, as we expected.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 在增强步骤之后训练数据和验证数据之间的差异。该图清楚地显示了应用于训练数据但未应用于验证数据的各种变换，正如我们所预期的那样。
- en: Next, we discuss another regularization technique called dropout.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论另一种正则化技术称为dropout。
- en: '7.1.2 Dropout: Randomly switching off parts of your network to improve generalizability'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 Dropout：随机关闭网络的部分以提高泛化能力
- en: We will now learn a technique called *dropout* to reduce further overfitting.
    Dropout was part of Inception net v1, but we avoided dropout in the previous chapter
    to improve clarity.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习一种称为*dropout*的技术，以进一步减少过拟合。 Dropout是Inception net v1的一部分，但在前一章中，我们避免使用dropout以提高清晰度。
- en: Dropout is a regularization technique for deep networks. A regularization technique’s
    job is to control the deep network in such a way that the network is rid of numerical
    errors during training or troublesome phenomena like overfitting. Essentially,
    regularization keeps the deep network well behaved.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一种用于深度网络的正则化技术。正则化技术的作用是控制深度网络，使其在训练过程中摆脱数值错误或者像过拟合这样的麻烦现象。本质上，正则化使深度网络行为良好。
- en: Dropout switches off output neurons randomly during each training iteration.
    This helps the model learn redundant features during training as it will not always
    have the previously learned features at its disposal. In other words, the network
    only has a subset of parameters of the full network to learn at a given time,
    and it forces the network to learn multiple (i.e., redundant) features to classify
    objects. For example, if the network is trying to identify cats, in the first
    iteration it might learn about whiskers. Then, if the nodes that correspond to
    the knowledge on whiskers are switched off, it might learn about cats’ pointy
    ears (figure 7.4). This leads to a network that learns redundant/different features
    like whiskers, two pointed ears, and so on, leading to better performance at test
    time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 辍学在每次训练迭代期间随机关闭输出神经元。这有助于模型在训练期间学习冗余特征，因为它不总是能够使用先前学到的特征。换句话说，网络在任何给定时间只有部分参数的全网络可学习，并迫使网络学习多个（即冗余的）特征来分类对象。例如，如果网络试图识别猫，那么在第一次迭代中它可能学习关于胡须的知识。然后，如果与胡须知识相关的节点被关闭，它可能学习关于猫耳朵的知识（见图
    7.4）。这导致网络学习了冗余/不同的特征，如胡须、两只尖耳朵等，从而在测试时间表现更好。
- en: '![07-04](../../OEBPS/Images/07-04.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![07-04](../../OEBPS/Images/07-04.png)'
- en: Figure 7.4 How dropout might change the network when learning to classify cat
    images. In the first iteration, it might learn about whiskers. In the second iteration,
    as the part containing information about whiskers is turned off, the network might
    learn about pointy ears. This leads the network to having knowledge about both
    whiskers and ears when it’s time for testing. That’s good in this case, because
    in the test image, you cannot see the cat’s whiskers!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 当学习分类猫图像时，辍学如何改变网络。在第一次迭代中，它可能学习有关胡须的知识。在第二次迭代中，由于包含有关胡须信息的部分被关闭，网络可能学习有关尖耳朵的知识。这使网络在测试时具有关于胡须和耳朵的知识。在这种情况下是好的，因为在测试图像中，你看不到猫的胡须！
- en: The nodes are switched off by applying a random mask of 1s and 0s on each layer
    you want to apply dropout on (figure 7.5). There is also a vital normalization
    step you perform on the active nodes during training. Let’s assume we are training
    a network with 50% dropout (i.e., dropping half of the nodes on every iteration).
    When 50% of your network is switched off, conceptually your network’s total output
    is reduced by half, compared to having the full network on. Therefore, you need
    to multiply the output by a factor of 2 to make sure the total output remains
    constant. Such computational details of dropout are highlighted in figure 7.5\.
    The good news is that you don’t have to implement any of the computational details,
    as dropout is provided as a layer in TensorFlow.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个要应用辍学的层上应用随机的 1 和 0 掩码关闭节点（见图 7.5）。在训练过程中，您还需要对活动节点进行重要的规范化步骤。假设我们正在训练一个辍学率为
    50% 的网络（即在每次迭代中关闭一半的节点）。当你的网络关闭了 50% 时，从概念上讲，你的网络总输出会减少一半，与完整网络相比。因此，您需要将输出乘以一个因子
    2，以确保总输出保持不变。辍学的这些计算细节在图 7.5 中突出显示。好消息是，您不必实现任何计算细节，因为 TensorFlow 中提供了辍学作为一个层。
- en: '![07-05](../../OEBPS/Images/07-05.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![07-05](../../OEBPS/Images/07-05.png)'
- en: Figure 7.5 A computational perspective on how dropout works. If dropout is set
    to 50%, then half the nodes in every layer (except for the last layer) will be
    turned off. But at testing time, all the nodes are switched on.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 辍学如何运作的计算视角。如果辍学设置为 50%，则每个层中的一半节点（除了最后一层）将被关闭。但在测试时，所有节点都被打开。
- en: 'Inception net v1 (figure 7.6) only has dropout for fully connected layers and
    the last average pooling layer. Remember not to use dropout on the last layer
    (i.e., the layer that provides final predictions). There are two changes to perform:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网 v1（见图 7.6）只对全连接层和最后一个平均池化层应用辍学。记住不要在最后一层（即提供最终预测的层）上使用辍学。要执行两个更改：
- en: Apply 70% dropout to the intermediate fully connected layer in the auxiliary
    outputs.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在辅助输出中的中间全连接层应用 70% 的辍学。
- en: Apply 40% dropout to the output of the last average pooling layer.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对最后一个平均池化层的输出应用 40% 的 dropout。
- en: '![07-06](../../OEBPS/Images/07-06.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![07-06](../../OEBPS/Images/07-06.png)'
- en: Figure 7.6 Abstract architecture of Inception net v1\. Inception net starts
    with a stem, which is an ordinary sequence of convolution/pooling layers that
    you will find in a typical CNN. Then Inception net introduces a new component
    known as Inception block. Finally, Inception net also makes use of auxiliary output
    layers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 Inception 网络 v1 的抽象架构。Inception 网络以一个称为干线的组件开始，这是一个典型 CNN 中会找到的普通的卷积/池化层序列。然后
    Inception 网络引入了一个称为 Inception 块的新组件。最后，Inception 网络还利用了辅助输出层。
- en: In TensorFlow, applying dropout is as easy as writing a single line. Once you
    get the output of the fully connected layer, dense1, you can apply dropout with
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，应用 dropout 就像写一行代码一样简单。一旦你得到了全连接层 dense1 的输出，你就可以使用以下方法应用 dropout：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we’re using a 70% dropout rate (as suggested in the original Inception
    net v1 paper) for the auxiliary output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 70% 的 dropout 率（正如原始 Inception 网络 v1 论文中建议的）用于辅助输出。
- en: Dropout on convolution layers
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层上的 dropout
- en: Dropout is mostly applied to dense layers, so one cannot help but wonder, “Why
    are we not applying dropout on convolution layers?” It is still an open debate.
    For example, the original dropout paper by Nitish Srivastava et al. ([http://mng.bz/o2Nv](http://mng.bz/o2Nv))
    argues that using dropout on lower convolution layers provides a performance boost.
    In contrast, the paper “Bayesian CNNs with Bernoulli Approximate Variational Inference”
    by Yarin Gal et al. ([https://arxiv.org/pdf/1506.02158v6.pdf](https://arxiv.org/pdf/1506.02158v6.pdf))
    argues that dropout on convolution layers doesn’t help much as, due to their low
    number of parameters (compared to a dense layer), they are already regularized
    well. Consequentially, dropout can hinder the learning in convolution layers.
    One thing you need to take into account is the time of publication. The dropout
    paper was written two years before the Bayesian CNN paper. Regularization and
    other improvements introduced in that duration could have had a major impact on
    improving deep networks, so the benefit of having dropout in convolution layers
    could become negligible. You can find a more casual discussion on [http://mng.bz/nNQ4](http://mng.bz/nNQ4).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 主要应用在密集层上，所以人们不禁会想，“为什么我们不在卷积层上应用 dropout 呢？”这仍然是一个争论未决的问题。例如，Nitish
    Srivastava 等人的原始 dropout 论文（[http://mng.bz/o2Nv](http://mng.bz/o2Nv)）认为，在低卷积层上使用
    dropout 可以提高性能。相反，Yarin Gal 等人的论文“具有伯努利近似变分推断的贝叶斯 CNN”（[https://arxiv.org/pdf/1506.02158v6.pdf](https://arxiv.org/pdf/1506.02158v6.pdf)）认为，在卷积层上应用
    dropout 并不会有太大帮助，因为它们的参数数量较低（与密集层相比），已经很好地被正则化了。因此，dropout 可以阻碍卷积层的学习。你需要考虑的一件事是出版时间。dropout
    论文是在贝叶斯 CNN 论文之前两年写的。在那段时间内引入的正则化和其他改进可能对改进深度网络产生了重大影响，因此，在卷积层中使用 dropout 的好处可能变得微不足道。你可以在
    [http://mng.bz/nNQ4](http://mng.bz/nNQ4) 找到更多非正式的讨论。
- en: The final code for the auxiliary output is shown in the following listing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助输出的最终代码如下列表所示。
- en: Listing 7.4 Modifying the auxiliary output of Inception net
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 修改了 Inception 网络的辅助输出
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Applying a dropout layer with 70% dropout
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用了 70% 的 dropout 层
- en: Next, we will apply dropout to the output of the last average pooling layer
    before the final prediction layer. We must flatten the output of the average pooling
    layer (flat_out) before feeding into a fully connected (i.e., dense) layer. Then
    dropout is applied on flat_out using
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在最后一个平均池化层的输出上应用 dropout，然后是最后的预测层。在将平均池化层的输出（flat_out）馈送到全连接（即密集）层之前，我们必须将其展平。然后，使用以下方法在
    flat_out 上应用 dropout：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are using a dropout rate of 40% for this layer, as prescribed by the paper.
    The final code (starting from the average pooling layer) looks like this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一层，我们使用了 40% 的 dropout 率，正如论文中所建议的一样。最终的代码（从平均池化层开始）如下所示：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This concludes the discussion on dropout. One final note to keep in mind is
    that you should not naively set the dropout rate. It should be chosen via a hyperparameter
    optimization technique. A very high dropout rate can leave your network severely
    crippled, whereas a very low dropout rate will not contribute to reducing overfitting.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对 dropout 的讨论。要牢记的最后一点是，你不应该简单地设置 dropout 率。应该通过超参数优化技术来选择。非常高的 dropout
    率会严重削弱你的网络，而非常低的 dropout 率则不会有助于减少过拟合。
- en: '7.1.3 Early stopping: Halting the training process if the network starts to
    underperform'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 早停：如果网络开始表现不佳，则停止训练过程
- en: The final technique we will be looking at is called early stopping. As the name
    suggests, early stopping stops training the model when the validation accuracy
    stops increasing. You may be thinking, “What? I thought the more training we do
    the better.” Until you reach a certain point, more training is better, but then
    training starts to reduce the model’s generalizability. Figure 7.7 depicts the
    typical training accuracy and validation accuracy curves you will obtain over
    the course of training a model. As you can see, after a point, the validation
    accuracy stops increasing and starts dropping. This is the start of overfitting.
    You can see that the training accuracy keeps going up, regardless of the validation
    accuracy. This is because modern deep learning models have more than enough parameters
    to “remember” data instead of learning features and patterns present in the data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的最后一种技术叫做早停（early stopping）。顾名思义，早停会在验证准确度不再提高时停止模型训练。你可能会想：“什么？我以为训练越多越好。”在达到某一点之前，训练得越多越好，但是之后，训练开始降低模型的泛化能力。图7.7展示了在训练模型过程中你会获得的典型训练准确度和验证准确度曲线。你可以看到，在某一点之后，验证准确度停止提高并开始下降。这标志着过拟合的开始。你可以看到，无论验证准确度如何，训练准确度都在持续上升。这是因为现代深度学习模型具有足够多的参数来“记住”数据，而不是学习数据中存在的特征和模式。
- en: '![07-07](../../OEBPS/Images/07-07.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![07-07](../../OEBPS/Images/07-07.png)'
- en: Figure 7.7 An illustration of overfitting. At the start, as the number of training
    iterations increases, both training and validation accuracies increase. But after
    a certain point, the validation accuracy plateaus and starts to go down, while
    the training accuracy keeps going up. This behavior is known as overfitting and
    should be avoided.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：过拟合的示意图。在开始时，随着训练迭代次数的增加，训练和验证准确度都会提高。但是在某个时刻之后，验证准确度会趋于平稳并开始下降，而训练准确度则持续上升。这种行为称为过拟合，应该避免。
- en: The early stopping procedure is quite simple to understand. First, you define
    a maximum number of epochs to train for. Then the model is trained for one epoch.
    After the training, the model is evaluated on the validation set using an evaluation
    metric (e.g., accuracy). If the validation accuracy has gone up and hasn’t reached
    the maximum epoch, the training is continued. Otherwise, training is stopped,
    and the model is finalized. Figure 7.8 depicts the early stopping workflow.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 早停过程非常简单易懂。首先，你定义一个最大的训练轮数。然后模型训练一轮。训练之后，使用评估指标（例如准确度）在验证集上评估模型。如果验证准确度提高了并且还没有达到最大epoch，则继续训练。否则，停止训练，并完成模型。图7.8描述了早停的工作流程。
- en: '![07-08](../../OEBPS/Images/07-08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![07-08](../../OEBPS/Images/07-08.png)'
- en: Figure 7.8 The workflow followed during early stopping. First the model is trained
    for one epoch. Then, the validation accuracy is measured. If the validation accuracy
    has increased and the training hasn’t reached maximum epoch, training is continued.
    Otherwise, training is halted.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：早停期间的工作流程。首先，模型训练一轮。然后，测量验证准确度。如果验证准确度提高了并且训练还没有达到最大epoch，则继续训练。否则，停止训练。
- en: 'Implementing early stopping requires minimal changes to your code. First, as
    before, we will set up a function that computes the number of steps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 实施早停需要对你的代码进行最小的更改。首先，和之前一样，我们将建立一个计算步数的函数：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will use the EarlyStopping callback provided by Keras ([http://mng.bz/v6lr](http://mng.bz/v6lr))
    to enable early stopping during the training. A Keras callback is an easy way
    to make something happen at the end of each epoch during training. For example,
    for early stopping, all we need to do is analyze the validation accuracy at the
    end of each epoch and, if it hasn’t shown any improvement, terminate the training.
    Callbacks are ideal for achieving this. We have already used the CSVLogger callback
    to log the metric quantities over the epochs. The EarlyStopping callback has several
    arguments:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Keras提供的EarlyStopping回调（[http://mng.bz/v6lr](http://mng.bz/v6lr)）来在训练过程中启用早停。Keras回调是在每个epoch结束时让某些事情发生的简单方法。例如，对于早停，我们只需在每个epoch结束时分析验证准确度，如果没有显示任何改善，就终止训练。回调是实现这一目标的理想选择。我们已经使用了CSVLogger回调来记录每个epoch的指标数量。EarlyStopping回调有几个参数：
- en: monitor—Which metric needs to be monitored in order to terminate the training.
    You can get the list of defined metric names using the model.metric_names attribute
    of a Keras model. In our example, this will be set to val_loss (i.e., the loss
    value computed on the validation data).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: monitor—需要监测的指标以终止训练。可以使用Keras模型的model.metric_names属性获取定义的指标名称列表。在我们的示例中，这将设置为val_loss（即在验证数据上计算的损失值）。
- en: min_delta—The minimum change required in the monitored metric to be considered
    an improvement (i.e., any improvement < min_delta will be considered a “no improvement”
    [defaults to zero]).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: min_delta—被监测指标所需的最小改变，以被视为改进（即任何改进<min_delta将被视为“没有改进” [默认为零]）。
- en: patience—If there’s no improvement after this many epochs, training will stop
    (defaults to zero).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: patience—如果在这么多个epochs之后没有改进，则训练将停止（默认为零）。
- en: mode—Can be auto/min/max. In min, training will stop if the metric has stopped
    decreasing (e.g., loss). In max, training will stop if the metric has stopped
    increasing (e.g., accuracy). The mode will be automatically inferred from the
    metric name (defaults to auto).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mode—可以是auto/min/max。在min中，如果指标停止减少（如损失），则训练将停止。在max中，如果指标停止增加（如准确度），则训练将停止。该模式将自动从指标名称中推断（默认为auto）。
- en: baseline—Baseline value for the metric. If the metric doesn’t improve beyond
    the baseline, training will stop (defaults to none).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: baseline—指标的基准值。如果指标未超出基准值，则训练将停止（默认为无）。
- en: restore_best_weights—Restores the best weight result in between the start of
    the training and the termination that showed the best value for the chosen metric
    (defaults to false).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: restore_best_weights—在训练开始和终止之间恢复显示选择指标的最佳权重结果（默认为false）。
- en: 'First, we will create a directory called eval if it doesn’t exist. This will
    be used to store the CSV, returned by the CSVLogger:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果不存在，我们将创建一个名为eval的目录。这将用于存储由CSVLogger返回的CSV文件：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we define the EarlyStopping callback. We chose val_loss as the metric
    to monitor and a patience of five epochs. This means the training will tolerate
    a “no improvement” for five epochs. We will leave the other parameters in their
    defaults:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义EarlyStopping回调函数。我们选择val_loss作为监测的指标，以及五个epochs的耐心。这意味着在五个epochs内训练将容忍“没有改进”。我们将保留其他参数为默认值：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally call model.fit() with the data and the appropriate callbacks. Here,
    we use the previously defined train_gen_aux and valid_gen_aux as the training
    and validation data (respectively). We also set epochs to 50 and the training
    steps and the validations steps using the get_steps_per_epoch function. Finally,
    we provide the EarlyStopping and CSVLogger callbacks, so the training stops when
    there’s no improvement under the specified conditions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后使用数据和适当的回调函数调用model.fit()。在这里，我们使用先前定义的train_gen_aux和valid_gen_aux作为训练和验证数据（分别）。我们还将epochs设置为50，并使用get_steps_per_epoch函数设置训练步数和验证步数。最后，我们提供EarlyStopping和CSVLogger回调函数，所以在指定条件下没有改进时训练停止：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The next listing shows a summary of the training logs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个清单展示了训练日志的摘要。
- en: Listing 7.5 Training logs provided during training the model
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 在训练模型期间提供的训练日志
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Because we used a high dropout rate of 70% for some layers, TensorFlow warns
    us about it, as unintended high dropout rates can hinder model performance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 因为我们对一些层使用了高达70%的丢失率，TensorFlow会对此进行警告，因为意外的高丢失率可能会影响模型的性能。
- en: It seems the model doesn’t see a benefit in training the model for 50 epochs.
    After epoch 38, it has decided to terminate the training. This is evident by the
    fact that training stopped before reaching epoch 50 (as shown in the line Epoch
    38/50). The other important observation is that you can see that the training
    accuracy doesn’t explode to large values, as we saw in the last chapter. The training
    accuracy has remained quite close to the validation accuracy (~30%). Though we
    don’t see much of a performance increase, we have managed to reduce overfitting
    significantly. With that, we can focus on getting the accuracy higher.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来模型没有在50个epochs中训练到利益。在第38个epoch之后，它决定终止训练。这在于训练在达到第50个epoch之前停止（如第38/50行所示）。另一个重要的观察结果是，你可以看到训练准确度没有像我们在上一章中看到的那样激增到很高的值。训练准确度一直与验证准确度（~30%）相当接近。尽管我们没有看到很大的性能提升，但我们成功地显著减少了过拟合。因此，我们可以着重提高准确度。
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 1 hour and 30 minutes to run 38 epochs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在一台配备Intel Core i5处理器和NVIDIA GeForce RTX 2070 8GB显卡的机器上，训练大约需要1小时30分钟才能完成38个周期。
- en: Next, we will revisit our model. We will dig into some research and implement
    a model that has proven to work well for this specific classification problem.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重新审视我们的模型。我们将深入研究一些研究，并实现一个已经被证明在这个特定分类问题上运行良好的模型。
- en: Exercise 1
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1
- en: 'You have the following model presented to you, and you see that it is heavily
    underfitting. Underfitting occurs when your model is not approximating the distribution
    of the data closely enough. Suggest how you can change the dropout layer to reduce
    underfitting. You can choose between 20%, 50%, and 80% as dropout rates:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你手头有一个模型呈现给你，你发现它严重欠拟合。欠拟合发生在你的模型没有足够近似数据分布时。建议你如何改变dropout层以减少欠拟合。你可以选择20%、50%和80%作为dropout率：
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Exercise 2
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2
- en: Define an early stopping callback to terminate the training if the validation
    loss value (i.e., val_loss) has not increased by 0.01 after five epochs. Use tf.keras.callbacks.EarlyStopping
    callback for this purpose.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个早停回调函数，如果验证损失值（即val_loss）在五个周期后没有增加0.01，则终止训练。为此目的使用tf.keras.callbacks.EarlyStopping回调函数。
- en: '7.2 Toward minimalism: Minception instead of Inception'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 朝向极简主义：Minception而不是Inception
- en: We now have a model where overfitting is almost nonexistent. However, test performance
    of the model is still not where we want it to be. You feel like you need a fresh
    perspective on this problem and consult a senior data scientist on your team.
    You explain how you have trained an Inception net v1 model on the tiny-imagenet-200
    image classification data set, as well as the poor performance of the model. He
    mentions that he recently read a paper ([cs231n.stanford.edu/reports/2017/pdfs/930.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/930.pdf))
    that uses a modified version of the Inception net that’s motivated by Inception-ResNet
    v2 and has achieved better performance on the data set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个几乎不存在过拟合的模型。然而，模型的测试性能仍然没有达到我们想要的水平。你觉得你需要对这个问题有一个新的视角，并咨询团队中的一位高级数据科学家。你解释了你如何在tiny-imagenet-200图像分类数据集上训练了一个Inception
    net v1模型，以及模型的性能不佳。他提到他最近读过一篇论文（[cs231n.stanford.edu/reports/2017/pdfs/930.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)），该论文使用了一个受Inception-ResNet
    v2启发的修改版本的Inception网络，在数据集上取得了更好的性能。
- en: He further explains two new techniques, batch normalization and residual connections
    (that are used in the modified inception net as well as Inception-ResNet v2),
    and the significant impact they have in helping the model training, especially
    in deep models. Now you will implement this new modified model and see if it will
    improve performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 他进一步解释了两种新技术，批量标准化和残差连接（它们在修改后的Inception网络以及Inception-ResNet v2中使用），以及它们在帮助模型训练方面产生的重大影响，特别是在深度模型中。现在，你将实现这个新的修改后的模型，看看它是否会提高性能。
- en: We have seen a slight increase in the validation and test accuracies. But we
    still have barely scratched the surface when it comes to performance. For example,
    there are reports of ~85% test accuracy for this data set ([http://mng.bz/44ev](http://mng.bz/44ev)).
    Therefore, we need to look for other ways to improve the model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到验证和测试准确率略有提高。但是在性能方面，我们仍然只是触及到了表面。例如，有关这个数据集的测试准确率约为85%（[http://mng.bz/44ev](http://mng.bz/44ev)）。因此，我们需要寻找其他提高模型性能的方法。
- en: That session you had with the senior data scientist on your team couldn’t have
    been more fruitful. We are going to try the new network he read about.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你与团队中的高级数据科学家进行的那次会议简直再好不过了。我们将尝试他所读过的新网络。
- en: 'This network is predominantly inspired by the Inception-Resnet-v2 network that
    was briefly touched on in the previous chapter. This new network (which we will
    call Minception) leverages all the state-of-the-art components used in the Inception-ResNet
    v2 model and modifies them to suit the problem at hand. In this section, you will
    learn this new model in depth. Particularly, Minception net has the following
    elements:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络主要受到了前一章节简要提及的Inception-Resnet-v2网络的启发。这个新网络（我们将其称为Minception）利用了Inception-ResNet
    v2模型中使用的所有最先进的组件，并对它们进行了修改以适应手头的问题。在本节中，你将深入了解这个新模型。特别是，Minception网络具有以下元素：
- en: A stem
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个干扰项
- en: Inception-ResNet block A
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception-ResNet块A
- en: Inception-ResNet block B
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception-ResNet块B
- en: Reduction blocks (a new type of block to reduce output size)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少块（一种新的用于减少输出大小的块）
- en: Average pooling layer
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均池化层
- en: Final prediction layer
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终预测层
- en: Just like other Inception models, this has a stem and Inception blocks. However,
    Minception differs from Inception Net v1 because it does not have auxiliary outputs,
    as they have other techniques to stabilize the training. Another notable difference
    is that Minception has two types of Inception blocks, whereas Inception Net v1
    reuses the same format throughout the network. While discussing the different
    aspects of Minception, we will compare it to Inception Net v1 (which we implemented)
    in more detail. In a later section, we will discuss the architecture of the Inception-ResNet
    v2 model in more detail and compare that to Minception. The code for this is available
    at Ch07-Improving-CNNs-and-Explaining/7.1.Image_Classification_Advance.ipynb.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他 Inception 模型一样，这个模型也有一个干部和 Inception 块。但是，Minception 与 Inception Net v1
    不同，因为它没有辅助输出，因为它们有其他稳定训练的技术。另一个值得注意的区别是，Minception 有两种类型的 Inception 块，而 Inception
    Net v1 在整个网络中重用相同的格式。在讨论 Minception 的不同方面时，我们将更详细地与我们实现的 Inception Net v1 进行比较。在后面的章节中，我们将更详细地讨论
    Inception-ResNet v2 模型的架构，并将其与 Minception 进行比较。此代码可在 Ch07-Improving-CNNs-and-Explaining/7.1.Image_Classification_Advance.ipynb
    中找到。
- en: 7.2.1 Implementing the stem
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 实施干部
- en: First and foremost, we should focus on the stem of the model. To refresh our
    knowledge, a stem is a sequence of convolution and pooling layers and resembles
    a typical CNN. Minception, however, has a more complex layout, as shown in figure
    7.9.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该关注模型的干部。为了更新我们的知识，干部是一系列卷积和池化层，类似于典型的 CNN。然而，Minception 的布局更加复杂，如图 7.9
    所示。
- en: '![07-09](../../OEBPS/Images/07-09.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![07-09](../../OEBPS/Images/07-09.png)'
- en: Figure 7.9 Comparing the stems of Minception and Inception-v1\. Note how Minception
    separates the nonlinear activation of convolution layers. This is because batch
    normalization must be inserted in between the convolution output and the nonlinear
    activation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 比较 Minception 和 Inception-v1 的干部。请注意 Minception 如何分离卷积层的非线性激活。这是因为批量归一化必须插入到卷积输出和非线性激活之间。
- en: You can see that it has parallel streams of convolution layers spread across
    the stem. The stem of the Minception is quite different from Inception Net v1\.
    Another key difference is that Minception does not use local response normalization
    (LRN) but something far more powerful known as *batch normalization*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到它在干部上有并行的卷积层流。Minception 的干部与 Inception Net v1 相比非常不同。另一个关键区别是 Minception
    不使用局部响应归一化（LRN），而是使用更强大的批量归一化。
- en: 'Batch normalization: A versatile normalization technique to stabilize and accelerate
    the training of deep networks'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Batch normalization：一种多功能的归一化技术，用于稳定和加速深度网络的训练。
- en: 'Batch normalization (BN) was introduced in the paper “Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift” by Sergey
    Ioffe et al. ([http://proceedings.mlr.press/v37/ioffe15.pdf](http://proceedings.mlr.press/v37/ioffe15.pdf)).
    As the name suggests, it is a normalization technique that normalizes the intermediate
    outputs of deep networks.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '批量归一化（BN）是由 Sergey Ioffe 等人在论文“Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift”中引入的。正如其名称所示，它是一种归一化技术，用于归一化深度网络的中间输出。([http://proceedings.mlr.press/v37/ioffe15.pdf](http://proceedings.mlr.press/v37/ioffe15.pdf))'
- en: '“Why is this important?” you might ask. It turns out deep networks can cause
    massive headaches if not properly cared for. For example, a batch of improperly
    scaled/ anomalous inputs during training or incorrect weight initialization can
    lead to a poor model. Furthermore, such problems can amplify along the depth of
    the network or over time, leading to changes to the distribution of the inputs
    received by each layer over time. The phenomenon where the distribution of the
    inputs is changed over time is known as a *covariate shift*. This is very common,
    especially in streaming data problems. Batch normalization was invented to solve
    this problem. Let’s understand how BN solves this problem. The batch normalization
    layer does the following things:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: “你可能会问，这很重要吗？” 结果深度网络如果没有得到正确的关注，可能会导致严重的头痛。例如，在训练期间，一批未正确缩放/异常输入或不正确的权重初始化可能导致模型不佳。此外，此类问题可能会随着网络深度或时间的推移而放大，导致每个层接收到的输入分布随时间而改变。输入分布随时间发生变化的现象称为*协变量转移*。这在流数据问题中非常常见。批量归一化是为了解决这个问题而发明的。让我们了解一下
    BN 如何解决这个问题。批量归一化层执行以下操作：
- en: Normalize *x*^((k)), the outputs of the *k*^(th) layer of the network using
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 *x*^((k))，网络的第 *k*^(th) 层的输出进行归一化
- en: '![07_09a](../../OEBPS/Images/07_09a.png)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![07_09a](../../OEBPS/Images/07_09a.png)'
- en: Here, *E*[*x*^((k))] represents the mean of the output, and *Var*[*x*^((k))]
    is the variance of the output. Both *E*[*x*^((k))] and *Var*[*x*^((k))] are vectors.
    For a fully connected layer with n nodes, both *E*[*x*^((k))] and *Var*[*x*^((k))]
    are n-long vectors (computed by taking mean-over-batch dimension). For a convolutional
    layer with f filters/kernels, *E*[*x*^((k))] and *Var*[*x*^((k))] will be f-long
    vectors (computed by taking mean over batch, height, and width dimensions).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，*E*[*x*^((k))] 表示输出的平均值，*Var*[*x*^((k))] 表示输出的方差。*E*[*x*^((k))] 和 *Var*[*x*^((k))]
    都是向量。对于具有 n 个节点的全连接层，*E*[*x*^((k))] 和 *Var*[*x*^((k))] 都是长度为 n 的向量（通过对批次维度求平均计算）。对于具有
    f 个滤波器/卷积核的卷积层，*E*[*x*^((k))] 和 *Var*[*x*^((k))] 将是长度为 f 的向量（通过对批次、高度和宽度维度求平均计算）。
- en: Scale and offset the normalized output using two trainable hyperparameters,
    γ and β (defined separately for each layer), as
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个可训练的超参数 *γ* 和 *β*（分别针对每一层）来缩放和偏移归一化后的输出，如下所示：
- en: '*y*^((k)) = *γ*^((k))*x̂*^((k)) + *β^((k))*'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*^((k)) = *γ*^((k))*x̂*^((k)) + *β^((k))*'
- en: In this process, computing *E*(*x*) and *Var*(*x*) gets a bit tricky, as these
    need to be treated differently in the training and testing phases.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个过程中，计算 *E*(*x*) 和 *Var*(*x*) 会有些棘手，因为在训练和测试阶段需要对它们进行不同处理。
- en: During training, following the stochastic (i.e., looking at a random batch of
    data instead of the full data set at a given time) nature of the training for
    each batch, *E*(*x*) and *Var*(*x*) are computed using only that batch of data.
    Therefore, for each batch, you can compute *E*(*x*) (mean) and *Var*(*x*) (variance)
    without worrying about anything except the current batch.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，根据训练的随机性（即一次只查看一个随机数据批次而不是整个数据集），对于每个批次，只使用该批次的数据计算 *E*(*x*)（平均值）和 *Var*(*x*)（方差）。因此，对于每个批次，你可以计算出
    *E*(*x*) 和 *Var*(*x*)（不必担心除了当前批次以外的任何事情）。
- en: Then, using each *E*(*x*) and *Var*(*x*) computed for each batch of data, we
    estimate *E*(*x*) and *Var*(*x*) for the population. This is achieved by computing
    the running mean of *E*(*x*) and *Var*(*x*). We will not discuss how the running
    mean works. But you can imagine the running mean as an efficiently computed approximate
    representation of the true mean for a large data set.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，利用每个数据批次计算出的 *E*(*x*) 和 *Var*(*x*)，我们估算出了总体的 *E*(*x*) 和 *Var*(*x*)。这是通过计算
    *E*(*x*) 和 *Var*(*x*) 的运行均值来实现的。我们不会讨论运行均值的工作原理。但你可以想象运行均值是对大数据集的真实均值的高效计算的近似表示。
- en: During the testing phase, we use the population-based *E*(*x*) and *Var*(*x*)
    that we computed earlier and perform the earlier defined computations to get *y*^((k)).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试阶段，我们使用之前计算出的基于总体的 *E*(*x*) 和 *Var*(*x*)，并执行之前定义的计算以获得 *y*^((k))。
- en: Due to the complex steps involved in the batch normalization, it will take quite
    some effort to implement this from the scratch. Luckily, you don’t have to. There
    is a batch normalization layer provided in TensorFlow ([http://mng.bz/Qv0Q](http://mng.bz/Qv0Q)).
    If you have the output of some dense layer (let’s call it dense1) to inject batch
    normalization, all you need to do is
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于批归一化涉及的步骤复杂，从头开始实现会需要相当多的工作。幸运的是，你不必这样做。TensorFlow 提供了一个批归一化层（[http://mng.bz/Qv0Q](http://mng.bz/Qv0Q)）。如果你有某些密集层的输出（我们称之为
    dense1）要应用批归一化，你只需要
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Then TensorFlow will automatically take care of all the complex computations
    that need to happen under the hood for batch normalization to work properly. Now
    it’s time to use this powerful technique in our Minception model. In the next
    listing, you can see the implementation of the stem of Minception net. We will
    write a function called stem, which allows us to turn on/off batch normalization
    at will.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 TensorFlow 将自动处理批归一化需要在内部发生的所有复杂计算。现在是时候在我们的 Minception 模型中使用这一强大的技术了。在下一个列表中，你可以看到
    Minception 网络的基干的实现。我们将编写一个名为 stem 的函数，它允许我们随意开启/关闭批归一化。
- en: Listing 7.6 Defining the stem of Minception
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.6 定义 Minception 的基干
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Defines the function. Note that we can switch batch normalization on and off.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义函数。请注意我们可以随时开启/关闭批归一化。
- en: ❷ The first part of the stem until the first split
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 到第一个分支的基干的第一部分
- en: ❸ Note that first batch normalization is applied before applying the nonlinear
    activation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 请注意，在应用非线性激活之前，先应用第一个批归一化。
- en: ❹ Nonlinear activation is applied to the layer after the batch normalization
    step.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 非线性激活应用于批归一化步骤后的层。
- en: ❺ The two parallel streams of the first split
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 第一个分支的两个平行流
- en: ❻ Concatenates the outputs of the two parallel streams in the first split
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 连接第一个分割的两个并行流的输出
- en: ❼ First stream of the second split
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 第二个分割的第一个流
- en: ❽ Second stream of the second split
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 第二个分割的第二个流
- en: ❾ Concatenates the outputs of the two streams in the second split
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 连接第二个分割的两个流的输出
- en: ❿ The third (final split) and the concatenation of the outputs
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 第三个（最终分割）和输出的连接
- en: A key change you should note is that the nonlinear activation of each layer
    is separated from the layers. This is so that batch normalization can be inserted
    in between the output of the layer and the nonlinear activation. This is the original
    way to apply batch normalization, as discussed in the original paper. But whether
    BN should come before or after the nonlinearity is an ongoing discussion. You
    can find a casual discussion on this topic at [http://mng.bz/XZpp](http://mng.bz/XZpp).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键变化需要注意，即每一层的非线性激活与层本身分开。这样做是为了能够在层的输出和非线性激活之间插入批量归一化。这是应用批量归一化的原始方式，正如原始论文中所讨论的那样。但是批量归一化应该在非线性激活之前还是之后是一个持续讨论的问题。您可以在[http://mng.bz/XZpp](http://mng.bz/XZpp)上找到关于这个主题的非正式讨论。
- en: 7.2.2 Implementing Inception-ResNet type A block
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 实现Inception-ResNet类型A块
- en: With the stem of the network behind us, let’s move forward to see what the Inception
    blocks look like in Minception net. Let’s quickly revisit what the Inception block
    is and why it was developed. The Inception block was developed to maximize the
    representational power of convolution layers while encouraging sparsity in model
    parameters and without shooting the memory requirements through the roof. It does
    this by having several parallel convolution layers with varying receptive field
    sizes (i.e., kernel sizes). The Inception block in Minception net uses mostly
    the same framework. However, it introduces one novel concept, known as *residual
    connections*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们已经讨论了网络的干部后，让我们继续看看在Minception网络中Inception块是什么样子的。让我们快速回顾一下Inception块是什么以及为什么会开发它。Inception块的开发旨在最大化卷积层的表示能力，同时鼓励模型参数的稀疏性，而不会使内存需求激增。它通过具有不同感知域大小（即内核大小）的几个并行卷积层来实现这一点。Minception网络中的Inception块主要使用相同的框架。但是，它引入了一个新概念，称为*残差连接*。
- en: 'Residual/skip connections: Shortcuts to stable gradients'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 残差/跳过连接：稳定梯度的捷径
- en: 'We have already touched lightly on residual connections, which introduce one
    of the simplest operations you can think of in mathematics: element-wise adding
    of an input to an output. In other words, you take a previous output of the network
    (call it x) and add it to the current output (call it y), so you get the final
    output z as z = x + y.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经简要讨论了残差连接，它引入了数学中可以想象的最简单的操作之一：将输入逐元素添加到输出中。换句话说，您取网络的前一个输出（称为x）并将其添加到当前输出（称为y）中，因此您得到最终输出z为z
    = x + y。
- en: '![07-09-unnumb-1](../../OEBPS/Images/07-09-unnumb-1.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![07-09-unnumb-1](../../OEBPS/Images/07-09-unnumb-1.png)'
- en: How skip/residual connections are added between convolution layers
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在卷积层之间添加跳过/残差连接
- en: One thing to be aware of when implementing residual connections is to make sure
    their sizes match, as this is an element-wise addition.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现残差连接时需要注意的一点是确保它们的尺寸匹配，因为这是逐元素的加法。
- en: What is residual about residual connections? Mathematical view
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接的数学观点是什么？
- en: It might not be obvious at first, but it’s not clear what is residual about
    skip connections. Assume the following scenario. You have an input x; next you
    have some layer, F(x) = y, that takes an input x and maps it to y. You implement
    the following network.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 起初可能不太明显，但是跳过连接中的残差之处并不清楚。假设以下情景。您有一个输入x；接下来您有一些层，F(x) = y，它将输入x映射到y。您实现了以下网络。
- en: '![07-09-unnumb-2](../../OEBPS/Images/07-09-unnumb-2.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![07-09-unnumb-2](../../OEBPS/Images/07-09-unnumb-2.png)'
- en: Mathematical view of residual connections
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接的数学观点
- en: y[k] = F(x)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] = F(x)
- en: y[k] [+ 1] = F(y[k])
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 1] = F(y[k])
- en: y[k] [+ 2] = y[k] [+ 1] + x
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 2] = y[k] [+ 1] + x
- en: y[k] [+ 2] = y[k] [+ 1] + G(x); let us consider the residual connections as
    a layer that does identity mapping and call it G.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 2] = y[k] [+ 1] + G(x); 让我们将残差连接视为一个执行恒等映射的层，并将其称为G。
- en: y[k] [+ 2] - y[k] [+ 1] = G(x) or
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: y[k] [+ 2] - y[k] [+ 1] = G(x) 或
- en: G(x) = y[k] [+ 2] - y[k] [+ 1]; G, in fact, represents the residual between
    the final output and the previous output.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: G(x) = y[k] [+ 2] - y[k] [+ 1]; 实际上，G代表了最终输出和前一个输出之间的残差。
- en: 'By considering final output as a layer H that takes x and y[k] [+ 1] as inputs,
    we obtain the following equation:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将最终输出视为一个将 x 和 y[k] [+ 1] 作为输入的层 H，我们得到以下方程：
- en: G(x) = H(x, y[k] [+ 1]) - F(y[k])
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: G(x) = H(x, y[k] [+ 1]) - F(y[k])
- en: You can see how the residual enters the picture. Essentially, G(x) is a residual
    between the final layer output and the previous layer’s output
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到残差是如何发挥作用的。本质上，G(x) 是最终层输出与上一层输出之间的残差
- en: 'It could not be easier to implement residual connections. Assume you have the
    following network:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 实现残差连接再简单不过了。假设你有以下网络：
- en: '[PRE17]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You’d like to create a residual connection from d1 to d3. Then all you need
    to do is
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要从 d1 到 d3 创建一个残差连接。你所需要做的就是
- en: '[PRE18]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: or, if you want to use a Keras layer (equivalent to the previous operation),
    you can do
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你想使用一个 Keras 层（与上一个操作等效），你可以这样做
- en: '[PRE19]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'There you have it: d4 is the output of a residual connection. You might remember
    that I said the output sizes must match in order for the residual connections
    to be added. Let’s try adding two incompatible shapes. For example, let’s change
    the Dense layer to have 30 nodes instead of 20:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了：d4 是一个残差连接的输出。你可能还记得我说过，为了添加残差连接，输出尺寸必须匹配。我们尝试添加两个不兼容的形状。例如，让我们将 Dense
    层的节点数从 20 改为 30：
- en: '[PRE20]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you try to run this code, you’ll get the following error:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试运行这段代码，你将会得到以下错误：
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see, TensorFlow is complaining that it was not able to broadcast
    (in this context, this means performing element-wise addition) two tensors with
    shapes 30 and 20\. This is because TensorFlow doesn’t know how to add a (batch_size,
    20) tensor to (batch_size, 30). If you see a similar error when trying to implement
    residual connections, you should go through the network outputs and make sure
    they match. To get rid of this error, all you need to do is change the code as
    follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，TensorFlow 抱怨它无法广播（在这种情况下，这意味着执行逐元素加法）两个形状分别为 30 和 20 的张量。这是因为 TensorFlow
    不知道如何将一个形状为（batch_size，20）的张量与一个形状为（batch_size，30）的张量相加。如果在尝试实现残差连接时出现类似的错误，你应该检查网络输出，并确保它们匹配。要消除此错误，你所需要做的就是按照以下方式更改代码：
- en: '[PRE22]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Minception has two types of Inception blocks (type A and type B). Now let’s
    write Inception-ResNet block (type A) as a function inception_resnet_a. Compared
    to the Inception block you implemented earlier, this new inception block has the
    following additions:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Minception 有两种类型的 Inception 块（类型 A 和类型 B）。现在让我们将 Inception-ResNet 块（类型 A）写成一个名为
    inception_resnet_a 的函数。与之前实现的 Inception 块相比，这个新的 inception 块有以下增加：
- en: Uses batch normalization
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批量归一化
- en: Uses a residual connection from the input to the final output of the block
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个从输入到块的最终输出的残差连接
- en: Figure 7.10 compares Inception-ResNet block type A of Minception to Inception
    Net v1\. An obvious difference is that Inception Net v1 does not harness the power
    of residual connections.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 比较 Minception 的 Inception-ResNet 块类型 A 与 Inception Net v1。一个明显的区别是 Inception
    Net v1 不利用残差连接的优势。
- en: '![07-10](../../OEBPS/Images/07-10.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![07-10](../../OEBPS/Images/07-10.png)'
- en: Figure 7.10 Comparison between Inception-ResNet block A (Minception) and Inception
    net v1’s Inception block
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 比较 Inception-ResNet 块 A（Minception）和 Inception 网 v1 的 Inception 块
- en: Let’s now implement the Minception-ResNet block A. Figure 7.11 shows the type
    of computations and their connectivity that need to be implemented (listing 7.7).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现 Minception-ResNet 块 A。图 7.11 显示了需要实现的计算类型及其连接性（清单 7.7）。
- en: '![07-11](../../OEBPS/Images/07-11.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![07-11](../../OEBPS/Images/07-11.png)'
- en: Figure 7.11 Illustration of the Minception-ResNet block A with annotations from
    code listing 7.7
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 Minception-ResNet 块 A 的示意图，带有代码清单 7.7 的注释
- en: Listing 7.7 Implementation of Minception-ResNet block A
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.7 Minception-ResNet 块 A 的实现
- en: '[PRE23]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ The first parallel stream in the block
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 块中的第一个并行流
- en: ❷ The second parallel stream in the block
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 块中的第二个并行流
- en: ❸ The third parallel stream in the block
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 块中的第三个并行流
- en: ❹ Concatenate the outputs of the three separate streams.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将三个独立流的输出连接起来。
- en: ❺ Incorporate the residual connection (which is multiplied by a factor to improve
    the gradient flow).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 合并残差连接（乘以一个因子以改善梯度流动）。
- en: Though the function appears long, it is mostly playing Legos with convolution
    layers. Figure 7.11 provides you the mental map between the visual inception layer
    and the code. A key observation is how the batch normalization and the nonlinear
    activation (ReLU) are applied in the top part of the block. The last 1 × 1 convolution
    uses batch normalization, not nonlinear activation. Nonlinear activation is only
    applied after the residual connections.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管函数看起来很长，但主要是使用卷积层进行乐高堆砌。 图7.11为您提供了视觉Inception层与代码之间的思维映射。 一个关键观察是批量标准化和非线性激活（ReLU）如何应用于块的顶部部分。
    最后的1×1卷积使用批量标准化，而不是非线性激活。 非线性激活仅在残余连接之后应用。
- en: We are now going to see how to implement the Inception-ResNet B block.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要看看如何实现Inception-ResNet B块。
- en: 7.2.3 Implementing the Inception-ResNet type B block
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 实现Inception-ResNet类型B块
- en: Next up is the Inception-ResNet type B block in the Minception network. We will
    not talk about this at length as it is very similar to the Inception-ResNet A
    block. Figure 7.12 depicts the Inception-ResNet B block and compares it to Inception-ResNet
    A block. Block B looks relatively simpler than block A, with only two parallel
    streams. The code-related annotations help you map the mental model of the Inception
    block to the code, as shown in the following listing.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是Minception网络中的Inception-ResNet类型B块。 我们不会详细讨论这个，因为它与Inception-ResNet A块非常相似。
    图7.12描述了Inception-ResNet B块并将其与Inception-ResNet A块进行了比较。 块B看起来相对简单，只有两个并行流。 代码相关的注释帮助您将Inception块的思维模型映射到代码中，如下列表所示。
- en: '![07-12](../../OEBPS/Images/07-12.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![07-12](../../OEBPS/Images/07-12.png)'
- en: Figure 7.12 Minception’s Inception-ResNet block B (left) and Minception’s Inception-ResNet
    block A (right) side by side
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 Minception的Inception-ResNet块B（左）和Minception的Inception-ResNet块A（右）并排放在一起
- en: Listing 7.8 The implementation of Minception-ResNet block B
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8 Minception-ResNet块B的实现
- en: '[PRE24]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ The first parallel stream in the block
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 块中的第一个并行流
- en: ❷ The second parallel stream in the block
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 块中的第二个并行流
- en: ❸ Concatenate the results from the two parallel streams.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将来自两个并行流的结果连接起来。
- en: ❹ The final convolution layer on top of the concatenated result
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 连接结果顶部的最终卷积层
- en: ❺ Applies the weighted residual connection
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 应用了加权残差连接
- en: This is quite similar to the function inception_resnet_a(...), with two parallel
    streams and residual connections. The differences to note are that the type A
    block has a larger number of convolution layers than the type B block. In addition,
    the type A block uses a 5 × 5 convolution (factorized to two 3 × 3 convolution
    layers) and type B uses a 7 × 7 convolution (factorized to 1 × 7 and 7 × 1 convolution
    layers). I will leave it up to the reader to explore the function in detail.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这与函数inception_resnet_a（...）非常相似，具有两个并行流和残余连接。 需要注意的区别是类型A块的卷积层数量比类型B块多。 另外，类型A块使用5×5卷积（分解为两个3×3卷积层），而类型B使用7×7卷积（分解为1×7和7×1卷积层）。
    我将让读者自行详细了解该函数。
- en: 7.2.4 Implementing the reduction block
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 实现减少块
- en: Inspired by Inception-ResNet models, Minception also uses reduction blocks.
    Reduction blocks are quite similar to Resnet blocks, with the exception of not
    having residual connections in the blocks (see the next listing).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 受Inception-ResNet模型的启发，Minception也使用减少块。 减少块与ResNet块非常相似，唯一的区别是块中没有残余连接（见下一列表）。
- en: Listing 7.9 Implementation of the reduction block of Minception
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9 Minception的减少块的实现
- en: '[PRE25]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ First parallel stream of convolutions
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 卷积的第一个并行流
- en: ❷ Second parallel stream of convolutions
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 卷积的第二个并行流
- en: ❸ Third parallel stream of pooling
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 池化的第三个并行流
- en: ❹ Concatenates all the outputs
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将所有输出连接起来
- en: I will let figure 7.13 speak for itself in terms of explaining listing 7.9\.
    But as you can see, at an abstract level it uses the same types of connections
    and layers as the Inception blocks we discussed.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我将让图7.13自己说明列表7.9。但正如你所看到的，在抽象层面上，它使用了我们讨论过的Inception块相同类型的连接和层。
- en: '![07-13](../../OEBPS/Images/07-13.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![07-13](../../OEBPS/Images/07-13.png)'
- en: Figure 7.13 Illustration of the reduction block
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 减少块的示意图
- en: Now we’re going to see how we can complete the puzzle of Minception by collating
    all the different elements we have implemented thus far.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要看看如何通过汇总到目前为止实现的所有不同元素来完成Minception的拼图。
- en: 7.2.5 Putting everything together
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 将所有内容组合在一起
- en: 'Great work so far. With all the basic blocks ready, our Minception model is
    taking shape. Next, it’s a matter of putting things where they belong. The final
    model uses the following components:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，工作进行得很顺利。随着所有基本块准备就绪，我们的 Minception 模型正在成形。接下来，将事物放在它们应该放置的地方就是问题。最终模型使用以下组件：
- en: A single stem
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个干部
- en: 1x Inception-ResNet block A
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1x Inception-ResNet 块 A
- en: 2x Inception-ResNet block B
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2x Inception-ResNet 块 B
- en: Average pooling
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均池化
- en: Dropout
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Final prediction layer with 200 nodes and softmax activation
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 200 个节点和 softmax 激活的最终预测层
- en: 'In addition, we will make a few more changes to the inputs of the model. According
    to the original paper, the model takes in a 56 × 56 × 3-sized input instead of
    a 64 × 64 × 3-sized input. This is done by the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将对模型的输入进行一些更改。根据原始论文，该模型接收的是 56 × 56 × 3 大小的输入，而不是 64 × 64 × 3 大小的输入。通过以下方式实现：
- en: '*Training phase*—Randomly cropping a 56 × 56 × 3-sized image from the original
    64 × 64 × 3-sized image'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练阶段* — 从原始 64 × 64 × 3 大小的图像中随机裁剪一个 56 × 56 × 3 大小的图像'
- en: '*Validation/testing phase*—Center cropping a 56 × 56 × 3-sized image from the
    original image'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证/测试阶段* — 从原始图像中心裁剪一个 56 × 56 × 3 大小的图像'
- en: 'Furthermore, we will introduce another augmentation step to randomly contrast
    images during the training (as used in the paper). Unfortunately, you cannot achieve
    either of these steps with the ImageDataGenerator. The good news is that since
    TensorFlow 2.2, there have been several new image preprocessing layers introduced
    ([http://mng.bz/yvzy](http://mng.bz/yvzy)). We can incorporate these layers just
    like any other layer in the model. For example, we start with the input just like
    before:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将在训练期间引入另一个增强步骤，随机对比图像（与论文中使用的相同）。不幸的是，您无法使用 ImageDataGenerator 实现这两个步骤中的任何一个。好消息是，自
    TensorFlow 2.2 以来，引入了几个新的图像预处理层（[http://mng.bz/yvzy](http://mng.bz/yvzy)）。我们可以像模型中的任何其他层一样使用这些层。例如，我们像以前一样从输入开始：
- en: '[PRE26]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then you import the RandomCrop and RandomContrast layers and use them as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 然后导入 RandomCrop 和 RandomContrast 层，并按如下方式使用它们：
- en: '[PRE27]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The final model looks like the following listing.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型如下所示。
- en: Listing 7.10 The final Minception model
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.10 最终 Minception 模型
- en: '[PRE28]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Define the 64 × 64 Input layer.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义 64 × 64 输入层。
- en: ❷ Perform random cropping on the input (randomness is only activated during
    training).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对输入进行随机裁剪（仅在训练期间激活随机性）。
- en: ❸ Perform random contrast on the input (randomness is only activated during
    training).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在输入上执行随机对比度调整（仅在训练期间激活随机性）。
- en: ❹ Define the output of the stem.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义干部的输出。
- en: ❺ Define the Inception-ResNet block (type A).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义 Inception-ResNet 块（类型 A）。
- en: ❻ Define a reduction layer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义减少层。
- en: ❼ Define 2 Inception-ResNet block (type B).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义 2 个 Inception-ResNet 块（类型 B）。
- en: ❽ Define the final prediction layer.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义最终预测层。
- en: ❾ Define the model.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义模型。
- en: ❿ Compile the model with categorical crossentropy loss and the adam optimizer.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 使用分类交叉熵损失和 adam 优化器编译模型。
- en: Finally, our Minception model is ready for battle. It takes in a 64 × 64 × 3-sized
    input (like the other models we implemented). It then randomly (during training)
    or center (during validation/testing) crops the image and applies random contrast
    adjustments (during training). This is taken care of automatically. Next, the
    processed input goes into the stem of the network, which produces the output stem_out,
    which goes into an Inception-ResNet block of type A and flows into a reduction
    block. Next, we have two Inception-ResNet type B blocks, one after the other.
    This is followed by an average pooling layer, a Flatten layer that squashes all
    dimensions except the batch dimension to 1\. Then a dropout layer with 50% dropout
    is applied on the output. Finally, a dense layer with 200 nodes (one for each
    class) with softmax activation produces the final output. Lastly, the model is
    compiled using the categorical cross-entropy loss and the adam optimizer.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的 Minception 模型已经准备就绪。它接收一个 64 × 64 × 3 大小的输入（与我们实现的其他模型相同）。然后在训练期间随机（在验证/测试期间居中）裁剪图像并应用随机对比度调整（在训练期间）。这些都会自动处理。接下来，处理后的输入进入网络的干部部分，产生输出干部输出，然后进入类型
    A 的 Inception-ResNet 块并流入减少块。接下来，我们有两个连续的 Inception-ResNet 类型 B 块。然后是一个平均池化层，一个扁平化层，将除批次维度之外的所有维度压缩为
    1。然后在输出上应用 50% 丢失率的 dropout 层。最后，具有 softmax 激活的 200 个节点的密集层生成最终输出。最后，使用分类交叉熵损失和
    adam 优化器编译模型。
- en: This ends our conversation about the Minception model. Do you want to know how
    much this will boost our model’s performance? In the next section, we will train
    the Minception model we defined.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对 Minception 模型的讨论。您想知道这将如何提升我们模型的性能吗？在下一节中，我们将训练我们定义的 Minception 模型。
- en: 7.2.6 Training Minception
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 训练 Minception
- en: Now we’re on to training the model. The training process is very similar to
    what you already did for the Inception Net v1 model, with one difference. We are
    going to use a learning rate reduction schedule to further reduce overfitting
    and improve generalizability. In this example, the learning rate scheduler will
    reduce the learning rate if the model’s performance doesn’t improve within a predefined
    duration (see the next listing).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始训练模型了。训练过程与您已经为 Inception Net v1 模型所做的非常相似，只有一个区别。我们将使用学习率缩减计划进一步减少过拟合并改善泛化能力。在此示例中，如果模型的性能在预定义的持续时间内没有改善，学习率调度器将减少学习率（请参见下一个清单）。
- en: Listing 7.11 Training the Minception model
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.11 训练 Minception 模型
- en: '[PRE29]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Sets up an early stopping callback
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置了早停回调
- en: ❷ Sets up a CSV logger to record metrics
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置了 CSV 记录器以记录指标
- en: ❸ Sets up a learning rate control callback
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置学习率控制回调
- en: ❹ Trains the model
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练模型
- en: 'When training deep networks, using a learning rate schedule instead of a fixed
    learning rate is quite common. Typically, we get better performance by using a
    higher learning rate at the beginning of the model training and then using a smaller
    learning rate as the model progresses. This is because, as the model converges
    during the optimization process, you should make the step size smaller (i.e.,
    the learning rate). Otherwise, large step sizes can make the model behave erratically.
    We can be smart about this process and reduce the learning rate whenever we do
    not see an increase in an observed metric instead of reducing the learning rate
    in fixed intervals. In Keras you can easily incorporate this into model training
    via the callback ReduceLROnPlateau ([http://mng.bz/M5Oo](http://mng.bz/M5Oo)):'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度网络时，使用学习率计划而不是固定学习率非常常见。通常，我们通过在模型训练开始时使用较高的学习率，然后随着模型的进展逐渐减小学习率来获得更好的性能。这是因为，在优化过程中，当模型收敛时，您应该使步长变小（即学习率）。否则，较大的步长会使模型表现不稳定。我们可以在观察到指标没有增加时智能地执行此过程，并在固定间隔内减小学习率。在
    Keras 中，您可以通过回调 ReduceLROnPlateau ([http://mng.bz/M5Oo](http://mng.bz/M5Oo)) 轻松将此纳入模型训练中：
- en: '[PRE30]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When using the callback, you need to set the following keyword arguments:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 使用回调时，您需要设置以下关键字参数：
- en: monitor—Defines the observed metric. In our example, we will decide when to
    reduce the learning rate based on validation loss.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: monitor——定义观察的指标。在我们的示例中，我们将根据验证损失决定何时降低学习率。
- en: factor—The multiplicative factor to reduce the learning rate by. If the learning
    rate is 0.01, a factor of 0.1, this means, on reduction, that the learning rate
    will be 0.001.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: factor——减少学习率的乘法因子。如果学习率为 0.01，0.1 的因子，这意味着在减少时学习率将为 0.001。
- en: patience—Similar to early stopping, how many epochs to wait before reducing
    the learning rate with no improvement in the metric.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沉着——与早停类似，等待多少个时期在指标没有改善的情况下降低学习率。
- en: mode—Similar to early stopping, whether the metric minimization/maximization
    should be considered as an improvement.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mode——与早停类似，指标的最小化/最大化是否应被视为改进。
- en: 'When you train your model, you should get an output like the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当您训练模型时，您应该得到以下输出：
- en: '[PRE31]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Amazing! We get a tremendous accuracy boost just by tweaking the model architecture.
    We now have a model that has around 50% accuracy on the validation set (which
    is equivalent to identifying 100/200 classes of objects accurately, or 50% of
    images classified that are accurate for each class). You can see the interventions
    made by the ReduceLROnPlateau callback in the output.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！通过调整模型架构，我们获得了巨大的准确性提升。我们现在有一个模型，在验证集上的准确率约为 50%（相当于准确识别了 100/200 个类别的对象，或者每个类别的图像有
    50% 被准确分类）。您可以在输出中看到 ReduceLROnPlateau 回调所进行的干预。
- en: Finally, we save the model using
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用以下方式保存模型
- en: '[PRE32]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we can measure the model’s performance on the test set:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以在测试集上衡量模型的性能：
- en: '[PRE33]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This should give around 51% accuracy on the test set. That’s very exciting news.
    We have almost doubled the performance of the previous model by paying more attention
    to the structure of the model.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该在测试集上达到51%的准确率。这是非常令人兴奋的消息。通过更多关注模型结构，我们几乎将之前模型的性能提升了一倍。
- en: This is a good lesson that teaches us the vital role played by the model architecture
    in deep learning. There’s a misconception that deep learning is the silver bullet
    that solves anything. It is not. For example, you shouldn’t expect any random
    architecture that’s put together to work as well as some of the state-of-the-art
    results published. Getting a well-performing deep network can be a result of days
    or even weeks of hyperparameter optimization and empirically driven choices.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的教训，教会了我们模型架构在深度学习中的至关重要的作用。有一个误解认为深度学习是解决一切问题的灵丹妙药。不是的。例如，你不应该期望任何随意组合在一起的架构能够像一些公开发表的最新技术结果那样好。获得一个表现良好的深度网络可能是对超参数进行了数天甚至数周的优化和凭经验的选择的结果。
- en: In the next section, we will leverage transfer learning to reach a higher degree
    of accuracy faster. We will download a pretrained model and finetune it on the
    specific data set.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将利用迁移学习更快地达到更高程度的准确性。我们将下载一个预训练模型，并在特定数据集上进行微调。
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 1 hour and 54 minutes to run 50 epochs.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在一台配备Intel Core i5和NVIDIA GeForce RTX 2070 8GB的机器上，训练大约需要1小时54分钟来运行50个epoch。
- en: Exercise 3
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3
- en: 'You have the following convolution block that you are using to implement an
    image classifier:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 你有以下卷积块，用于实现图像分类器：
- en: '[PRE34]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You would like to make the following two changes:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 你想做以下两个改变：
- en: Introduce batch normalization after applying the activation
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用激活后引入批标准化
- en: Create a residual connection from the output of the convolution layer to the
    output of the batch normalization layers output.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从卷积层的输出到批标准化层的输出创建一个残差连接。
- en: '7.3 If you can''t beat them, join ‘em: Using pretrained networks for enhancing
    performance'
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 如果你无法击败它们，就加入它们：使用预训练网络增强性能
- en: So far, you have developed a good image classification model, which uses various
    methods to prevent overfitting. The company was happy until your boss let out
    the news that there’s a new competitor in town that is performing better than
    the model you developed. Rumor is that they have a model that’s around 70% accurate.
    So, it’s back to the drawing board for you and your colleagues. You believe that
    a special technique known as transfer learning can help. Specifically, you intend
    to use a pretrained version of Inception-ResNet v2 that is already trained on
    the original ImageNet image classification data set; fine-tuning this model on
    the tiny-imagenet-200 data set will provide better accuracy than all the models
    implemented thus far.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经开发了一个很好的图像分类模型，它使用各种方法防止过拟合。公司一直很满意，直到你的老板宣布镇上出现了一个表现比你开发的模型更好的新竞争对手的消息。传言是他们有一个大约70%准确率的模型。所以，你和你的同事又回到了起点。你相信一种特殊的技术，称为迁移学习，可以帮助。具体来说，你打算使用一个在原始ImageNet图像分类数据集上已经训练过的Inception-ResNet
    v2的预训练版本；在tiny-imagenet-200数据集上对这个模型进行微调将比到目前为止实现的所有模型都提供更高的准确性。
- en: If you want to come close to state of the art, you must use every bit of help
    you can get. A great way to begin this quest is to start with a pretrained model
    and then fine-tune it for your task. A pretrained model is a model that has already
    being trained on a similar task. This process falls under the concept of *transfer
    learning*. For example, you can easily find models that have been pretrained on
    the ILSVRC task.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想接近最新技术水平，你必须尽一切可能获得帮助。开始这个探索的一个好方法是使用预训练模型，然后针对你的任务进行微调。预训练模型是已经在类似任务上训练过的模型。这个过程属于*迁移学习*的概念。例如，你可以很容易找到在ILSVRC任务上预训练过的模型。
- en: '7.3.1 Transfer learning: Reusing existing knowledge in deep neural networks'
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 迁移学习：在深度神经网络中重用现有知识
- en: Transfer learning is a massive topic and is something for a separate chapter
    (or even a book). There are many variants of transfer learning. To understand
    different facets of transfer learning, refer to [https://ruder.io/transfer-learning/](https://ruder.io/transfer-learning/).
    One method is to use a pretrained model and fine-tune it for the task to be solved.
    The process looks like figure 7.14.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一个庞大的话题，需要单独的章节（甚至一本书）来讨论。迁移学习有许多变体。要理解迁移学习的不同方面，请参考[https://ruder.io/transfer-learning/](https://ruder.io/transfer-learning/)。一种方法是使用预训练模型并针对要解决的任务进行微调。该过程如图7.14所示。
- en: '![07-14](../../OEBPS/Images/07-14.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![07-14](../../OEBPS/Images/07-14.png)'
- en: Figure 7.14 How transfer learning works. First, we start with a model that is
    pretrained on a larger data set that is solving a similar/relevant task to the
    one we’re interested in. Then we transfer the model weights (except the last layer)
    and fit a new prediction layer on top of the existing weights. Finally, we fine-tune
    the model on a new task.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 迁移学习的工作原理。首先，我们从一个在解决与我们感兴趣的任务类似/相关的较大数据集上预训练的模型开始。然后，我们传输模型权重（除了最后一层），并在现有权重之上拟合一个新的预测层。最后，我们在新任务上进行微调。
- en: First, you train the model on a task for which you already have a large labeled
    data set (known as the pretrained task). For example, in image classification,
    you have several large labeled data sets, including the ImageNet data set. Once
    you train a model on the large data set, you get the weights of the network (except
    for the final prediction layer) and fit a new prediction layer that matches the
    new task. This gives a very good starting point for the network to solve the new
    task. You are then able to solve the new task with a smaller data set, as you
    have already trained your model on similar, larger data sets.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你在一个你已经拥有大型标记数据集的任务上训练模型（称为预训练任务）。例如，在图像分类中，你有几个大型标记数据集，包括ImageNet数据集。一旦你在大型数据集上训练了一个模型，你就会得到网络的权重（除了最后的预测层），并拟合一个匹配新任务的新预测层。这给了网络解决新任务的一个非常好的起点。然后，你可以用较小的数据集解决新任务，因为你已经在类似的较大数据集上训练了模型。
- en: 'How can we use transfer learning to solve our problem? It is not that difficult.
    Keras provides a huge model repository for image classification tasks ([http://mng.bz/aJdo](http://mng.bz/aJdo)).
    These models have been trained predominantly on the ImageNet image classification
    task. Let’s tame the beast produced in the lineage of Inception networks: Inception-ResNet
    v2\. Note that the code for this section can be found at Ch07-Improving-CNNs-and-Explaining/7.2.Transfer_Learning.ipynb.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用迁移学习来解决我们的问题？这并不难。Keras为图像分类任务提供了一个巨大的模型库（[http://mng.bz/aJdo](http://mng.bz/aJdo)）。这些模型主要是在ImageNet图像分类任务上进行训练的。让我们驯服Inception网络系列中产生的野兽：Inception-ResNet
    v2。请注意，本节的代码可以在Ch07-Improving-CNNs-and-Explaining/7.2.Transfer_Learning.ipynb中找到。
- en: Inception-ResNet v2
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Inception-ResNet v2
- en: 'We briefly touched on the Inception-ResNet v2 model. It was the last Inception
    model produced. Inception-ResNet v2 has the following characteristics that set
    it apart from other inception models:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要讨论了Inception-ResNet v2模型。这是最后一个生产的Inception模型。Inception-ResNet v2具有以下特点，使其与其他Inception模型区别开来：
- en: Redesigned stem that removes any representational bottlenecks
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新设计的起始模块，消除了任何表征瓶颈
- en: Inception blocks that use residual connections
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用残差连接的Inception块
- en: Reduction modules that reduce the height/width dimensions of the inputs
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少输入高度/宽度维度的减少模块
- en: Does not use auxiliary outputs as in the early Inception nets
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不像早期的Inception网络那样使用辅助输出
- en: As you can see, the redesigned stem, Inception-ResNet blocks, and reduction
    modules are being used in the Minception model. And if you compare the diagrams
    of Minception that are provided to the diagrams in the original paper, you will
    see how many similarities they share. Therefore, we will not repeat our discussion
    of these components. If you still want to see the specific details and illustrations
    of the different components, refer to the original paper ([https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf)).
    However, the high-level architecture of Inception-ResNet v2 looks like the following
    figure.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，Minception模型中使用了重新设计的起始模块、Inception-ResNet块和减少模块。如果你比较一下Minception的图表与原始论文中提供的图表，你会看到它们有多么相似。因此，我们不会重复讨论这些组件。如果你仍然想看到不同组件的具体细节和插图，请参考原始论文（[https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf)）。然而，Inception-ResNet
    v2的高层架构如下图所示。
- en: '![07-14-unnumb-3](../../OEBPS/Images/07-14-unnumb-3.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![07-14-unnumb-3](../../OEBPS/Images/07-14-unnumb-3.png)'
- en: The overall architecture of Inception-ResNet v2 architecture
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: Inception-ResNet v2 的整体架构
- en: 'You can download the Inception-ResNet v2 model with a single line:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用一行代码下载 Inception-ResNet v2 模型：
- en: '[PRE35]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, include_top=False means that the final prediction layer will be discarded.
    It is necessary because the original inception net is designed for 1,000 classes.
    However, we only have 200 classes. pooling='avg' ensures that the last pooling
    layer in the model is an average pooling layer. Next, we will create a new model
    that encapsulates the pretrained Inception-ResNet v2 model as the essence but
    is modified to solve the tiny-ImageNet classification task, as shown in the next
    listing.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 include_top=False 意味着最终的预测层将被丢弃。这是必要的，因为原始的 inception 网络是为 1000 类设计的。但是，我们只有
    200 类。pooling=‘avg’ 确保模型中的最后一个汇合层是平均汇合层。接下来，我们将创建一个新模型，将预训练的 Inception-ResNet
    v2 模型作为核心，但修改为解决 Tiny ImageNet 分类任务，如下图所示。
- en: Listing 7.12 Implementing a model based on the pretrained Inception-ResNet v2
    model
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 列出了基于预训练 Inception-ResNet v2 模型的模型实现
- en: '[PRE36]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Some important imports
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一些重要的导入
- en: ❷ Defining an input layer for a 224 × 224 image
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为 224 × 224 的图像定义输入层
- en: ❸ The pretrained weights of the Inception-ResNet v2 model
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Inception-ResNet v2 模型的预训练权重
- en: ❹ Apply 40% dropout
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 应用了 40% 的 dropout
- en: ❺ Final prediction layer with 200 classes
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 最终的预测层有 200 个类
- en: ❻ Using a smaller learning rate since the network is already trained on ImageNet
    data (chosen empirically)
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 由于网络已经在 ImageNet 数据上进行了训练，所以使用较小的学习率（经验选择）
- en: Here, you can see that we are defining a sequential model that
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个顺序模型，
- en: First defines an input layer of size 224 × 224 × 3 (i.e., height = 224, width
    = 224, channels = 3)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先定义了一个大小为 224 × 224 × 3 的输入层（即，高度 = 224，宽度 = 224，通道 = 3）
- en: Defines the Inception-ResNet v2 model as a layer
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Inception-ResNet v2 模型定义为一个层
- en: Uses a dropout of 40% on the last average pooling layer
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一个平均汇合层上使用 40% 的 dropout
- en: Defines a dense layer that uses softmax activation and has 200 nodes
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了一个使用 Softmax 激活的密集层，具有 200 个节点
- en: One crucial challenge we need to deal with is that the original input Inception-ResNet
    v2 is designed to consume is size 224 × 224 × 3\. Therefore, we will need to find
    a way to present our inputs (i.e., 64 × 64 × 3) in a way that complies with Inception-ResNet
    v2’s requirements. In order to do that, we will make some changes to the ImageDataGenerator,
    as the following listing shows.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要应对的一个关键挑战是，原始的 Inception-ResNet v2 输入大小为 224 × 224 × 3。因此，我们需要找到一种方法来呈现我们的输入（即，64
    × 64 × 3）以符合 Inception-ResNet v2 的要求。为了做到这一点，我们将对 ImageDataGenerator 进行一些更改，如下面的列表所示。
- en: Listing 7.13 The modified ImageDataGenerator that produces 224 × 224 images
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 列出了生成 224 × 224 图像的修改版 ImageDataGenerator。
- en: '[PRE37]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Defines a data-augmenting image data generator and a standard image data generator
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了一个数据增广的图片数据生成器和一个标准的图片数据生成器
- en: ❷ Defines a partial function to avoid repeating arguments
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义了部分函数以避免重复参数
- en: ❸ Uses a target size of 224 × 224
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 224 × 224 的目标大小
- en: ❹ Uses bilinear interpolation to make images bigger
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用双线性插值使图像变大
- en: ❺ Defines the data generators for training and validation sets
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义了训练和验证集的数据生成器
- en: ❻ Defines the test data generator
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义了测试数据生成器
- en: ❼ Uses a target size of 224 × 224 and bilinear interpolation
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用双线性插值的 224 × 224 目标大小
- en: ❽ Defines the batch size and target size
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义了批量大小和目标大小
- en: ❾ Gets the train/valid/test modified data generators using the data_gen_augmented
    function
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 使用 data_gen_augmented 函数获得训练集/验证集/测试集的修改过的数据生成器
- en: 'Finally, it’s time for the grand unveil! We will train the best model we’ve
    come up with:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，是时候展示我们最好的模型了：
- en: '[PRE38]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The training will be identical to the earlier training configuration we used
    when training the Minception model. We will not repeat the details. We are using
    the following:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 训练将与先前训练 Miniception 模型所使用的训练配置相同。我们不会重复细节。我们使用以下内容：
- en: Metric logging
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录指标
- en: Early stopping
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早期停止
- en: Learning rate scheduling
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率调整
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8GB, the training
    took approximately 9 hours and 20 minutes to run 23 epochs.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在一台配有 NVIDIA GeForce RTX 2070 8GB 的 Intel Core i5 机器上，训练 23 epoch 大约需要 9
    小时 20 分钟。
- en: 'You should get a result similar to the following:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到类似以下的结果：
- en: '[PRE39]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Isn’t this great news? We have reached around 74% validation accuracy by combining
    all we have learned. Let’s quickly look at the test accuracy of the model:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是好消息吗？通过结合我们所学的所有知识，我们已经达到了约 74% 的验证准确率。让我们快速看一下模型的测试准确率：
- en: '[PRE40]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This should show you around ~79% accuracy. It hasn’t been an easy journey, but
    you obviously have surpassed your competitor’s model of ~70% accuracy.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示大约 79% 的准确率。这不是一次轻松的旅程，但显然你已经超过了竞争对手的约 70% 准确率的模型。
- en: In the next section, we will look at the importance of model explainability.
    We will learn about a technique that we can use to explain the knowledge embedded
    in our model.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看看模型可解释性的重要性。我们将学习一种技术，可以用来解释嵌入模型的知识。
- en: Inception-ResNet v2 versus Minception
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: Inception-ResNet v2 对比 Minception
- en: 'The stem of the Minception and Inception-Resnet-v2 are identical in terms of
    the innovations they introduce (e.g., Inception-ResNet blocks, reduction blocks,
    etc.). However, there are the following low-level differences:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: Minception 和 Inception-Resnet-v2 的茎在引入的创新方面是相同的（例如，Inception-ResNet 块，缩减块等）。然而，存在以下低级差异：
- en: Inception-ResNet v2 has three different Inception block types; Minception has
    only two.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception-ResNet v2 有三种不同的 Inception 块类型；Minception 只有两种。
- en: Inception-ResNet v2 has two different types of reduction blocks; Minception
    has only one.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception-ResNet v2 有两种不同类型的缩减块；Minception 只有一个。
- en: Inception-ResNet v2 has 25 Inception layers, but Minception (the version we
    implemented) has only three.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception-ResNet v2 有 25 个 Inception 层，但我们实现的 Minception（版本）只有三个。
- en: 'There are also other minor differences, such as the fact that Inception-ResNet
    v2 uses valid padding in a few layers of the model. Feel free to consult the Inception-ResNet
    v2 paper if you want to know the details. Another notable observation is that
    neither the Minception nor the Inception-ResNet v2 uses local response normalization
    (LRN), as they use something far more powerful: batch normalization.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些小的差异，例如 Inception-ResNet v2 在模型的几个层中使用有效填充。如果你想了解详情，请参阅 Inception-ResNet
    v2 论文。另一个值得注意的观察是，Minception 和 Inception-ResNet v2 都没有使用局部响应归一化（LRN），因为它们使用了更强大的东西：批归一化。
- en: Exercise 4
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 4
- en: You want to implement a network using a different pretrained network known as
    VGGNet (16 layers). You can obtain the pretrained network from tf.keras.applications.VGG16.
    Next, you discard the top layer and introduce a max pooling layer on top. Then
    you want to add two dense layers on top of the pretrained network with 100 (ReLU
    activation) and 50 (Softmax activation) nodes. Implement this network.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用另一个名为 VGGNet（16 层）的不同预训练网络实现一个网络。你可以从 tf.keras.applications.VGG16 获取预训练网络。接下来，你丢弃顶层并在顶部引入一个最大池化层。然后你想在预训练网络的顶部添加两个具有
    100（ReLU 激活）和 50（Softmax 激活）个节点的稠密层。实现这个网络。
- en: '7.4 Grad-CAM: Making CNNs confess'
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '7.4 Grad-CAM: 让 CNN 供认'
- en: The company can’t be happier about what you have done for them. You have managed
    to build a model that not only beat the performance of the competitor, but also
    is one of the best in production. However, your boss wants to be certain that
    the model is trustworthy before releasing any news on this. Accuracy alone is
    not enough! You decide to demonstrate how the model makes predictions using a
    recent model interpretation technique known as *Grad-CAM*. Grad-CAM uses the magnitude
    of the gradients generated for a given input with respect to the model’s predictions
    to provide visualizations of where the model focused. A large magnitude of gradients
    in a certain area of an image means that the image focuses more in that area.
    And by superimposing the gradient magnitudes depicted as a heatmap, you are able
    to produce an attractive visualization of what the model is paying attention to
    in a given input.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 公司对你为他们所做的一切感到非常高兴。你成功地建立了一个不仅击败了竞争对手的性能，而且是生产中最好的模型。然而，你的老板希望在发布任何消息之前确认模型是可信的。仅准确性是不够的！你决定演示模型如何进行预测，使用一种名为*Grad-CAM*的最新模型解释技术。Grad-CAM
    使用相对于模型预测而生成的给定输入的梯度的大小来提供模型关注的可视化。图像中某一区域的梯度大小较大意味着图像更关注该区域。通过将梯度大小叠加成热图，你能够产生一个有吸引力的可视化，显示模型在给定输入中关注的内容。
- en: 'Grad-CAM (which stands for gradient class activation map) is a model interpretation
    technique introduced for deep neural networks by Ramprasaath R. Selvaraju et al.
    in “Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization”
    ([https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)).
    Deep networks are notorious for their inexplicable nature and are thus termed
    *black boxes*. Therefore, we must do some analysis and ensure that the model is
    working as intended.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 'Grad-CAM（梯度类激活映射）是由 Ramprasaath R. Selvaraju 等人在“Grad-CAM: Visual Explanations
    from Deep Networks via Gradient-based Localization”中为深度神经网络引入的一种模型解释技术（[https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)）。深度网络因其不可解释性而臭名昭著，因此被称为*黑匣子*。因此，我们必须进行一些分析，确保模型正常运行。'
- en: The following code delineates how Grad-CAM works its magic, and the implementation
    is available in the notebook Ch07-Improving-CNNs-and-Explaining/7.3 .Interpreting_CNNs_GradCAM.ipynb.
    In the interest of conserving the length of this chapter, we will discuss only
    the pseudocode of this approach and will leave the technical details to appendix
    B (see the next listing).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码详细说明了 Grad-CAM 如何发挥其作用，并且实现代码在笔记本 Ch07-Improving-CNNs-and-Explaining/7.3
    .Interpreting_CNNs_GradCAM.ipynb 中可用。为了节省本章的篇幅，我们将仅讨论该方法的伪代码，技术细节留给附录 B（参见下一个清单）。
- en: Listing 7.14 Pseudocode of Grad-CAM computations
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 清单7.14 Grad-CAM 计算的伪代码
- en: '[PRE41]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The key computation that is performed by Grad-CAM is, given an input image,
    taking the gradient of the node that corresponds to the true class of the image
    with respect to the last convolution output of the model.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM 执行的关键计算是，给定输入图像，计算与图像真实类别相对应的节点对模型的最后卷积输出的梯度。
- en: The magnitude of the gradient at each pixel of the images represents the contribution
    that pixel made to the final outcome. Therefore, by representing Grad-CAM output
    as a heatmap, resizing to match the original image, and superimposing that on
    the original image, you can get a very attractive and informative plot of where
    the model focused to find different objects. The plots are self-explanatory and
    show whether the model is focusing on the correct object to produce a desired
    prediction. In figure 7.15, we show which areas the model focuses on strongly
    (red/dark = highest focus, blue/light = less focus).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中每个像素的梯度大小代表了该像素对最终结果的贡献。因此，通过将 Grad-CAM 输出表示为热图，调整大小以匹配原始图像，并将其叠加在原始图像上，你可以获得一个非常引人注目和信息丰富的图，显示了模型关注的不同对象的位置。这些图解释性强，显示了模型是否专注于正确的对象以产生期望的预测。在图7.15中，我们展示了模型强烈关注的区域（红色/黑色
    = 最高关注，蓝色/浅色 = 较少关注）。
- en: '![07-15](../../OEBPS/Images/07-15.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![07-15](../../OEBPS/Images/07-15.png)'
- en: Figure 7.15 Visualization of the Grad-CAM output for several probe images. The
    redder/darker an area in the image, the more the model focuses on that part of
    the image. You can see that our model has learned to understand some complex scenes
    and separate the model that it needs to focus on.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 展示了几个探测图像的 Grad-CAM 输出的可视化。图像中越红/越暗的区域，模型对该部分的关注越多。你可以看到我们的模型已经学会了理解一些复杂的场景，并将其需要关注的模型分开。
- en: Figure 7.15 (i.e., visualization of Grad-CAM) shows that our model is truly
    an intelligent model. It knows where to focus to find a given object, even in
    cluttered environments (e.g., classifying the dining table). As mentioned earlier,
    the redder/ darker the area, the more the model focuses on that area to make a
    prediction. Now it’s time for you to demonstrate the results to your boss and
    build the needed confidence to go public with the new model!
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15（即 Grad-CAM 的可视化）显示了我们的模型确实是一个智能模型。它知道在混乱的环境中（例如，对餐桌进行分类）要关注哪些地方以找到给定的对象。如前所述，区域越红/越暗，模型就越专注于该区域进行预测。现在是时候向你的老板展示结果，建立必要的信心，公开新模型了！
- en: We will end our discussion about image classification here. We have learned
    about many different models and techniques that can be used to solve the problem
    effectively. In the next chapter, we will discuss a different facet of computer
    vision known as image segmentation.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此结束对图像分类的讨论。我们已经学习了许多可以有效解决问题的不同模型和技术。在下一章中，我们将讨论计算机视觉的另一个方面，即图像分割。
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Image augmentation, dropout, and early stopping are some of the common techniques
    used to prevent overfitting in vision deep networks.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像增强、dropout 和提前停止是在视觉深度网络中防止过拟合的一些常见技术。
- en: Most of the common image augmentation steps can be achieved through the Keras
    ImageDataGenerator.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数常见的图像增强步骤可以通过 Keras ImageDataGenerator 实现。
- en: It is important to pay attention to the architecture of the model chosen for
    a given problem. One should not randomly choose an architecture but research and
    identify an architecture that has worked for a similar problem. Otherwise, choose
    the architecture through hyperparameter optimization. The Minception model’s architecture
    has been proven to work well on the same data we used in this chapter.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所选问题，重要的是要注意所选择模型的架构。我们不应随意选择一个架构，而是要研究并确定一个在类似问题上已经奏效的架构。否则，可以通过超参数优化来选择架构。Minception
    模型的架构已经被证明在我们本章使用的相同数据上表现良好。
- en: Transfer learning enables us to use already trained models to solve new tasks
    with better accuracy.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习使我们能够利用已经训练好的模型来解决新任务，从而获得更好的准确性。
- en: In Keras you can get a given model with a single line of code and adapt it to
    the new task.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中，你可以用一行代码获得给定模型，并将其调整到新的任务中。
- en: Various pretrained networks are available at [http://mng.bz/M5Oo](http://mng.bz/M5Oo).
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [http://mng.bz/M5Oo](http://mng.bz/M5Oo) 上有各种预训练网络可供选择。
- en: Grad-CAM (gradient class activation map) is an effective way to interpret your
    CNN.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grad-CAM（梯度类激活映射）是解释你的 CNN 的有效方法。
- en: Grad-CAM computes where the model focused the most based on the magnitude of
    gradients produced with respect to the prediction made by the model.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据模型对预测产生的梯度大小，Grad-CAM 计算出模型关注最多的地方。
- en: Answers to exercises
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1**'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: 'You should reduce the dropout rate to keep more nodes switched during training
    if underfitting is occurring:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果出现欠拟合，你应该降低 dropout 率，以保持更多节点在训练过程中保持开启状态：
- en: '[PRE42]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Early stopping is introduced using the EarlyStopping callback:'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提前停止是通过使用 EarlyStopping 回调引入的：
- en: '[PRE43]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '**Exercise 2**'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: '[PRE44]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Exercise 3**'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: '[PRE45]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Exercise 4**'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4**'
- en: '[PRE46]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
