- en: 11 Next-generation AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommendations for preparing data for an AI model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendations for which techniques to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Properties the next-generation AI systems should have
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoughts about what future AI systems should support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the AI solutions of the future requires us to address the current limitations
    in today’s systems. A key objective of this book is to provide a clear and honest
    assessment of the current state of AI because it’s only by understanding where
    we are today that we can chart a realistic path to the future. While media portrayals
    of AI often lean toward the sensational, my aim is to provide a balanced perspective.
    Much of the technology we find exciting and innovative today has actually been
    in development for over half a century. Although challenges remain, such as efficiency,
    cost-effectiveness, and adaptability, they present opportunities for growth and
    improvement as we continue on this exciting AI journey.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I draw on over 30 years of experience developing and deploying
    mission-critical AI systems where reliability, precision, and effect are not merely
    ambitions but imperatives. I will outline a set of features that, in my view,
    will characterize the next generation of AI platforms. My examples will mostly
    address clinical reasoning and financial transactions since these are domains
    I’ve worked extensively in, but the aspirations apply generally to most types
    of AI platforms. We’re working toward a day when businesses can easily deploy
    scalable, resilient, adaptive AI systems with more capability and fewer flaws
    than those available today.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Data flexibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Business applications frequently require access to data from diverse sources.
    Real-world data can come in structured or unstructured formats and may be stored
    in various ways. Consider an AI application in the healthcare sector; it may need
    to tap into sources like physicians’ notes, radiology images, electronic health
    records, established best practices, anatomical data, biosensors, and laboratory
    results. An effective AI system must be able to efficiently access and make use
    of all the relevant resources.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the business world, datasets often reach staggering sizes. Exploring a single
    year’s worth of trading data on the New York Stock Exchange, a year’s worth of
    social media data, or a year’s worth of transactions at a retailer like Walmart
    can entail dealing with hundreds of billions of data points. Training an AI model
    on such extensive data can be an arduous process, potentially taking an immense
    amount of time while never converging. To expedite this process and facilitate
    the development and testing of numerous models, an AI system should offer various
    data sampling methods.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward approach is a random sample, where each data point
    has an equal chance of being selected. However, in certain applications, data
    can be categorized differently, and it may be desirable for each category to be
    equally represented. Each business application necessitates its own customized
    sampling solution. For instance, in fraud prevention, sampling may need to be
    performed in a way that adequately captures specific behaviors or ensures that
    certain attributes of the samples follow a particular distribution. The choice
    of sampling method holds significant importance. For instance, understanding cardholder
    behavior wouldn’t be effectively achieved through a random sample of their transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Elimination of irrelevant attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many datasets contain redundant or irrelevant attributes. Therefore, a well-designed
    AI system should incorporate functionality to detect and eliminate such unnecessary
    information. This process streamlines model design and testing, making them more
    efficient and less susceptible to overfitting. For example, if a significant portion
    of values for a particular attribute is missing, it may not contribute meaningfully
    to the model, regardless of its potential relevance. In some cases, certain data,
    such as zip codes or phone numbers, might introduce bias into the model and should
    be omitted.
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating redundant and irrelevant attributes from datasets is imperative
    for optimizing AI system performance. Redundant attributes introduce noise and
    complexity, making it harder for models to discern meaningful patterns, while
    irrelevant ones can lead to overfitting. Removing these irrelevant features makes
    models more efficient and ultimately improves predictive accuracy. Additionally,
    it enhances model generalization and reduces computational complexity, leading
    to faster training times and more efficient resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Data coherence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data can often be complex and unstructured, originating from diverse sources
    with varying formats and potentially containing missing or erroneous values. Instances
    of data may partially overlap or even contradict each other. Inconsistencies,
    contradictions, or discrepancies can exist within a dataset or between different
    datasets, resulting from data points or records that do not align or agree with
    each other. This can occur due to various reasons, including errors in data collection,
    data entry mistakes, data merging issues, or changes in data sources over time.
    Data incoherence undermines data quality and reliability, making it challenging
    to use effectively for analysis, machine learning, or decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and addressing data incoherence is crucial for ensuring accurate and
    trustworthy data-driven processes and models. Resolving data quality issues in
    an AI system goes beyond simple remedies like applying formulas, finding and replacing
    values, or sorting and organizing data. It is also imperative to avoid labeling
    errors when training AI systems. For instance, a medical diagnostic system trained
    on inaccurately labeled data could result in untreated illnesses for some while
    subjecting healthy individuals to unnecessary medical procedures. An effective
    AI system should have mechanisms to flag potential labeling issues. Furthermore,
    AI models must accurately reflect the real-world data that the system will encounter.
    If the system is trained on meticulously cleaned data but encounters incomplete
    or inconsistent samples in the production environment, errors are likely to occur.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 Lack of bias in data and algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Society comprises individuals from diverse backgrounds, living in various socioeconomic
    conditions, each possessing unique strengths and weaknesses. It is imperative
    for AI models to acknowledge and account for this diversity, steering clear of
    assessments or recommendations that inadvertently favor or discriminate against
    particular individuals or groups. Algorithms, built upon logic created by human
    programmers, can carry conscious or unconscious biases that lead to unforeseen
    consequences. Moreover, if the data used to train the model fails to offer a representative
    sample, bias can become ingrained within the system.
  prefs: []
  type: TYPE_NORMAL
- en: A pertinent example of how bias can inadvertently enter a computer program is
    the 2020 A-level grading controversy in the United Kingdom. These exams, similar
    to the SATs and ACTs in the United States, play a crucial role in university admissions.
    In response to COVID-19 distancing measures, the 2020 exams were canceled, and
    an algorithm was employed to generate estimated scores. The algorithm utilized
    seemingly reasonable parameters, including a student’s school grades in relevant
    courses and the historical A-level exam performance of students from their school
    [1]. However, nearly 40% of students received lower scores than they and their
    teachers anticipated. Critics demonstrated that the algorithm exhibited a built-in
    bias in favor of private school students. While it is true that, on average, private
    school students tend to achieve higher test scores, the algorithm could assign
    a lower score to a high-achieving student from a state school compared to an objectively
    weaker student from a private school. This outcome clearly contradicts the intended
    objective. Eventually, the algorithm-generated scores were discarded, and scores
    assessed by the students’ teachers were utilized instead.
  prefs: []
  type: TYPE_NORMAL
- en: In applications like facial-recognition systems in law enforcement or loan-approval
    systems in mortgage lending, eliminating bias is of paramount importance. Formal
    rules and regulations have been established to safeguard individuals’ rights,
    with infringements carrying severe repercussions. AI systems should have a mechanism
    to audit biases in predictions and decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 Feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature engineering is one of the most critical steps in a machine learning
    project. It involves the process of selecting, transforming, and creating relevant
    features or variables from raw data to enhance the performance of machine learning
    models. It is a critical step in designing powerful AI models because the quality
    and relevance of the features used directly affect the model’s ability to learn
    and make accurate predictions. Since every business has its own unique characteristics,
    domain experts are typically needed for their ability to define specifications,
    describe the context, and flag exceptions. For example, an investment analyst
    would know that stock markets are subject to seasonal variation as well as expected
    but unpredictable fluctuations at the end of each week and quarter. There is less
    trading activity during the summer, and trades pick up toward the end of the year
    for tax reasons. An expert in the credit card industry would identify seasonal
    patterns as well, but they might also know the data-enriching potential of rolling
    daily, weekly, and monthly spending rates.
  prefs: []
  type: TYPE_NORMAL
- en: Even though some patterns are easy to understand, there are others that might
    not be as clear. These can also be harder to figure out, even if we know they’re
    there. For example, card transactions might be grouped by an algorithm that considers
    geographic attributes, cash-back requests, refund requests, and the need for manual
    data entry. Entirely different considerations apply to biomedical data, which
    can often be skewed in a way that makes trends hard to see.
  prefs: []
  type: TYPE_NORMAL
- en: The next-generation AI systems should automate the vital feature engineering
    step in model building. This speeds up development significantly and makes these
    systems extremely efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 Technique combination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI comprises a range of techniques and methods, each carrying its own set of
    strengths and weaknesses. Next-generation solutions should combine various AI
    technologies like the ones described in chapters 2 and 3\. Just as soliciting
    diverse expert opinions can improve the efficacy and efficiency of a project,
    AI systems that combine multiple AI technologies can benefit from the strengths
    of various AI techniques. For instance, the integration of data mining, case-based
    reasoning, fuzzy logic, deep learning, genetic algorithms, rule-based systems,
    and smart agents in the development of an AI model for fraud prevention represents
    a formidable leap in the fight against fraudulent activities.
  prefs: []
  type: TYPE_NORMAL
- en: This combination of technologies equips the system with an unparalleled analytical
    prowess. Data mining enables the extraction of meaningful insights from vast and
    complex datasets, allowing for the identification of subtle patterns indicative
    of fraudulent behavior. Case-based reasoning supplements this by drawing on historical
    cases to make informed decisions about new and emerging threats. Fuzzy logic addresses
    the inherent uncertainty and imprecision associated with fraud detection, enhancing
    the system’s adaptability. Deep learning excels in capturing complex relationships
    within the data, while genetic algorithms optimize the model’s parameters. Rule-based
    systems provide a transparent and explainable framework for decision-making, facilitating
    the interpretation of the model’s outputs. Finally, smart agents enable real-time
    monitoring and adaptive capabilities, allowing swift action against new threats.
    The combination of these AI technologies results in a fraud prevention solution
    that is not only highly accurate and adaptive but also capable of staying one
    step ahead of the ever-evolving landscape of fraudulent activities.
  prefs: []
  type: TYPE_NORMAL
- en: 11.8 Unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2019, a tweet by Elon Musk underscored the significance of unsupervised learning
    for the future of AI. Musk, who initially asserted in 2015 that self-driving vehicles
    would conquer any road within a few years, later reversed course when he realized
    the limitations of supervised learning approaches. He articulated [2],
  prefs: []
  type: TYPE_NORMAL
- en: '*A substantial portion of real-world AI challenges must be addressed to enable
    unsupervised, generalized, fully autonomous driving, given that our entire road
    infrastructure is designed for biological neural networks equipped with optical
    sensors.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Indeed, while supervised learning has been instrumental in training AI models
    through labeled datasets, this approach faces practical challenges in the real
    world, primarily due to the scarcity of large, high-quality datasets and the often
    difficult and expensive task of labeling data. Unsupervised learning, in contrast,
    empowers AI systems to learn patterns, relationships, and structures within data
    without explicit guidance. This capability is crucial for dealing with the complexity
    and diversity of unannotated information encountered in various domains.
  prefs: []
  type: TYPE_NORMAL
- en: 11.9 AI factory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The utilization of artificial intelligence is currently somewhat restricted,
    primarily due to the substantial resources and advanced technical expertise required
    for its effective implementation. These prerequisites are typically within the
    grasp of only large corporations and academic institutions with significant financial
    and human capital. Next-generation AI platform transformation is on the horizon
    as we navigate toward turnkey solutions that democratize AI, ultimately making
    its powerful capabilities accessible to companies of all sizes.
  prefs: []
  type: TYPE_NORMAL
- en: When we draw parallels with the innovative thinking of Henry Ford, who revolutionized
    automobile manufacturing by introducing mass-produced parts on a moving assembly
    line and simplifying car assembly into repetitive tasks, it becomes evident that
    we should adopt an “AI factory” approach. For example, one module autonomously
    evaluates data sources, extracting and cleansing pertinent data. A second module
    takes on the role of enriching this data, while a third module focuses on creating
    and optimizing model parameters. Another module manages the training of a multitude
    of models, and yet another is dedicated to testing and evaluating these models.
    A separate module could excel in combining the most effective models to build
    a production-ready AI solution. Furthermore, additional modules could play pivotal
    roles in ensuring model governance and handling specialized functionalities such
    as cybersecurity.
  prefs: []
  type: TYPE_NORMAL
- en: The democratization of AI will lead to a new era in which AI becomes an accessible
    tool for a broader spectrum of professionals, transforming how businesses and
    organizations use its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 11.10 Quality Assurance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quality assurance involves ensuring that a system functions as intended. When
    dealing with AI systems, this task becomes elaborate due to their inherent unpredictability.
    In conventional software, we can employ diagrams and code analysis to understand
    and validate its behavior. However, AI systems present a challenge because their
    functioning relies on complex data patterns not easily discernible by humans.
    Fine-tuning parameters through trial and error doesn’t instill trust in a supervised
    learning program, and unsupervised learning is even more uncertain. Typically,
    an unsupervised algorithm can only be validated through its performance because
    there are no formal metrics or labeled samples available.
  prefs: []
  type: TYPE_NORMAL
- en: The next-generation AI should rely on a dedicated quality assurance protocol
    designed specifically for AI. Testing should occur at every stage of model development,
    covering aspects such as data integrity, performance, adaptability, and resilience
    to unexpected situations. Diverse datasets should be used to assess the system’s
    versatility, and we should intentionally introduce novel data and errors for broad
    testing. Mission-critical systems should be tested for security against attack
    and for resilience against scenarios such as power failure, even when they are
    mirrored in multiple geographic locations.
  prefs: []
  type: TYPE_NORMAL
- en: 11.11 Prediction reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The foundation of any effective AI system lies in its capacity to provide precise
    predictions. To highlight the repercussions of unreliable AI performance, let’s
    use an illustration from the transaction processing domain. CMSPI, a global payments
    consultancy, found that if AI systems aren’t performing well, it can seriously
    affect businesses and economies [3]:'
  prefs: []
  type: TYPE_NORMAL
- en: In the year 2020, U.S. online spending witnessed a staggering increase of $193.7
    billion compared to the preceding year, as reported by the Census Bureau. While
    this surge in online commerce should have been a boon for retailers, it paradoxically
    resulted in approximately $30 billion in foregone sales opportunities due to lower
    approval rates in the online space. To put this figure in perspective, consider
    a small business with annual sales of $1 million that transitions into the online
    market. Instead of merely losing out on $30,000 in in-store sales, this business
    could potentially see a staggering $150,000 worth of legitimate transactions declined
    online.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Online transaction rejections play a vital role in preventing fraudulent activities.
    However, as CMSPI’s data analysis reveals, an alarming one in every five rejected
    transactions is a false positive, meaning that genuine customers are unjustly
    turned away. What’s even more disconcerting is that over half of these wrongly
    rejected customers subsequently take their business to a competitor. Indeed, a
    system that mistakenly identifies legitimate transactions as fraudulent poses
    a formidable challenge. It results in a lose–lose scenario where customers are
    left dissatisfied and businesses suffer a double blow of losing both customers
    and potential profits.
  prefs: []
  type: TYPE_NORMAL
- en: 'This highlights the critical need for a fraud detection system to possess two
    essential attributes: a high detection rate for actual fraudulent activities and
    a minimal rate of false positives for legitimate transactions. This requirement
    isn’t confined to the world of payment processing; it extends across various domains.
    Consequently, the ability to provide reliable predictions stands as an indispensable
    necessity for any AI solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.12 Effective data storage and processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Legacy databases face significant challenges when applied to real-time AI applications.
    Their tabular data structures are ill-equipped to handle complexity, limiting
    their suitability for AI tasks. Moreover, these databases struggle with the complex
    computations and queries demanded by AI algorithms, often resulting in slow response
    times. As AI datasets grow in size, databases encounter difficulties in scaling
    horizontally to meet the increased demand for processing speed and capacity, a
    critical concern in applications requiring low response times, such as real-time
    fraud prevention in credit card authorization.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we had a requirement for an AI risk-scoring system that could
    process over 100 billion transactions per year at a rate of over 50,000 per second
    with a 5-millisecond response time. The system needs to analyze hundreds of variables
    and constraints and aggregate data over numerous timeframes for each of these
    transactions. Databases are simply impractical given such constraints, even with
    the most advanced hardware and programming techniques. To overcome these obstacles,
    alternative technologies like distributed file systems and specialized data processing
    frameworks are required.
  prefs: []
  type: TYPE_NORMAL
- en: 11.13 Deployability and interoperability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An AI platform should possess the fundamental capability to seamlessly interface
    with various software and systems, thereby ensuring that organizations can harness
    AI’s potential to its fullest extent. This extends beyond mere integration; it
    includes the capacity to streamline workflows and enable data exchange while minimizing
    operational complexities and costs. Moreover, a robust AI platform should provide
    the agility for dynamic model deployment, allowing organizations to incorporate
    new or updated AI models into their processes without disrupting system operations
    or workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 11.14 Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Numerous computer systems face a significant challenge in their journey to
    practicality: the inability to handle demanding workloads effectively. This issue
    becomes particularly pronounced in the business world, where scalability is a
    paramount requirement for any computer program. The ability to scale means that
    an AI system can flexibly adapt and accommodate increasing workloads and data
    volumes without sacrificing performance or responsiveness. This is essential because
    businesses often operate in dynamic environments where data generation and processing
    demands can grow rapidly. Scalability empowers organizations to use AI effectively
    as their operations expand, ensuring that the system can continue to provide timely
    insights, support decision-making, and deliver valuable outcomes. It also enables
    cost optimization by allowing businesses to allocate resources efficiently and
    avoid costly system upgrades.'
  prefs: []
  type: TYPE_NORMAL
- en: Next-generation AI systems should prioritize scalable AI algorithms since scalability
    represents a critical test for the worthiness of a solution. Even the smartest
    and most precise predictive models may fall short if they lack the scalability
    necessary to meet the demands of business applications.
  prefs: []
  type: TYPE_NORMAL
- en: 11.15 Resilience and robustness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A system is considered resilient when it can uphold its performance even in
    challenging circumstances. These adverse conditions could encompass hardware failures,
    cyber-attacks, power outages, network disruptions, or a combination of such problems
    occurring simultaneously. Resilience is the vital capacity to maintain performance
    even in the face of challenges. For instance, in the aerospace industry, where
    aircraft navigation relies on advanced avionics systems, resilience ensures that
    a single hardware malfunction or a cyber-attack won’t jeopardize passenger safety.
    Similarly, in the energy sector, power grid control systems must exhibit resilience
    to mitigate the effect of natural disasters, guaranteeing continuous electricity
    supply to homes and businesses. In healthcare, electronic health record systems
    must remain operational, even in the event of network disruptions or cyber threats.
  prefs: []
  type: TYPE_NORMAL
- en: For an AI tool to demonstrate true effectiveness, it must exhibit resilience
    by ensuring its functionality remains intact across a spectrum of situations,
    even in suboptimal conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 11.16 Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the world of AI, especially for critical applications, security is paramount.
    Unfortunately, the landscape of emerging technologies has also become a playground
    for malicious actors, including terrorist groups. These entities have proven their
    adaptability by using the latest advancements to orchestrate attacks. As AI systems
    continue to advance and become more potent, the risks they pose loom larger. Bad
    actors are ready to exploit AI’s capabilities to launch devastating cyberattacks
    and create highly destructive malware. One striking manifestation of this threat
    landscape is the alarming rise in the prevalence and sophistication of ransomware
    attacks. These attacks manifest in various forms, with one common iteration involving
    the remote encryption of a victim’s file system. The perpetrators then demand
    a ransom in exchange for the decryption key, effectively holding critical data
    hostage.
  prefs: []
  type: TYPE_NORMAL
- en: In a grim statistic from 2020, nearly 2,400 government offices, healthcare facilities,
    and educational institutions in the United States alone fell victim to such ransomware
    attacks [4]. A particularly memorable incident occurred in 2015 when Colonial
    Pipeline Company was forced to shut down its operations for an entire week due
    to a cyberattack. This disruption caused significant turmoil by disrupting the
    supply of essential fuels such as diesel, gasoline, and jet fuel across the United
    States. Highlighting the gravity of the situation, Colonial’s CEO, Joseph Blount,
    made the decision to pay a $4.4 million ransom. The *Wall Street Journal* reported,
    “It was an option he felt he had to exercise, given the stakes involved in a shutdown
    of such critical energy infrastructure. The Colonial Pipeline provides roughly
    45% of the fuel for the East Coast, according to the company” [5].
  prefs: []
  type: TYPE_NORMAL
- en: According to CNBC [6], in September 2023, casino operator Caesars paid out a
    ransom worth $15 million to a cybercrime group that managed to infiltrate and
    disrupt its systems.
  prefs: []
  type: TYPE_NORMAL
- en: As the world becomes increasingly reliant on advanced computing technologies,
    the list of potential harm scenarios grows ever longer. This highlights the importance
    of designing AI platforms with end-to-end encryption to safeguard data from interception
    to unauthorized access and protect the integrity of decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: 11.17 Explicability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring the widespread acceptance of AI systems, especially in scenarios where
    decisions hold significant societal consequences, relies on the ability to explain
    how these systems arrive at their conclusions. The use of black-box algorithms,
    notably those rooted in deep learning neural networks, presents a formidable challenge
    in terms of complying with regulations that demand explicability and transparency.
    This renders such algorithms unsuitable for making pivotal decisions in areas
    such as lending and criminal justice. Moreover, the issue extends beyond just
    regulatory compliance. Trustworthiness becomes a paramount concern in mission-critical
    applications. In these instances, accountability necessitates a comprehensive
    understanding of how a decision was reached. A mere declaration of “The computer
    told me to do it” often falls short of what is expected.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, the next-generation AI systems must be engineered with the dual
    objectives of operational comprehensibility and the provision of coherent explanations
    for their outputs. This transparency should encompass the inner workings of the
    AI, allowing stakeholders and end-users to discern the rationale behind each decision.
  prefs: []
  type: TYPE_NORMAL
- en: 11.18 Traceability and monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ability to track changes in an AI system and to step back through its history
    is important not only to prevent its misuse but also to help clarify responsibility.
    Consider, for instance, a scenario where a doctor, reliant on an AI system, provides
    an inaccurate diagnosis; it prompts the complex query of culpability. Should the
    blame primarily fall on the doctor who utilized the AI tool, or should the company
    responsible for its creation share the responsibility? Resolving this complex
    question necessitates a comprehensive inquiry that analyzes not only all the events
    and the way the AI system was employed but also its status and performance at
    the moment of use.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, an AI system should provide a robust framework for tracking its
    model performance and the evolving trends within the data it processes, ensuring
    its continued effectiveness and relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 11.19 Privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In today’s rapidly evolving technological landscape, our personal information
    is being amassed at an unprecedented rate, driven by an ever-expanding array of
    advanced technologies. From fitness trackers meticulously recording our biorhythms
    to sophisticated login systems cataloging our fingerprints and facial expressions,
    a substantial portion of our vital data is increasingly susceptible to exploitation.
    This vulnerability not only creates opportunities for organized criminals but
    also extends a tempting invitation to nation-states eager to engage in activities
    like identity theft, fraud, and other illicit activities. The repercussions of
    inadequate privacy protection are often disastrous. Consider, for instance, the
    alarming frequency of data breaches exposing the personal data of millions, leaving
    individuals vulnerable to financial fraud and identity theft. The fallout from
    such breaches can be long-lasting and financially ruinous for affected individuals.
  prefs: []
  type: TYPE_NORMAL
- en: In response to these threats, it is imperative for next-generation AI systems
    to incorporate robust privacy protection modules. For example, they should employ
    advanced anonymization techniques to strip personal identifiers from raw data,
    making it impossible to trace back to individuals. Furthermore, AI systems should
    allow individuals to retain control over their own data, deciding who has access
    and for what purposes, thus reinforcing their privacy rights.
  prefs: []
  type: TYPE_NORMAL
- en: 11.20 Temporal reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of time is intrinsic to our understanding of the world. The relationship
    between cause and effect requires the understanding of events occurring before
    and after one another. Events vary in their temporal characteristics; some are
    instantaneous, while others have a duration, and they can happen in different
    time frames or exhibit various forms of overlap. Certain events may serve as prerequisites
    for others. While these observations are intuitive to humans and often taken for
    granted, machines will require specific programming logic.
  prefs: []
  type: TYPE_NORMAL
- en: One possible approach is relying on the Allen Intervals, a comprehensive framework
    that defines 13 possible temporal relationships between events. For instance,
    it can identify when one event meets another (Meets), one event occurs before
    another (Before), or when an event starts (Starts) or ends (Ends) in relation
    to another. Overlapping intervals (Overlaps) signify concurrent but not fully
    coinciding events, while During implies containment. The system can also discern
    the inverse relations, such as Met By, After, Started By, and Ended By, to capture
    the opposite direction of these temporal connections. Additionally, it can recognize
    when one event fully overlaps with or contains another, as well as when two events
    are equal in duration (Equality). This comprehensive set of temporal relations
    empowers AI systems to make sense of the temporal aspects of data, facilitating
    applications across various domains, from scheduling to natural language understanding
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: 11.21 Contextual reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Contextual reasoning is an indispensable element of any intelligent decision-making
    system, as it empowers AI to go beyond the surface level of data and understand
    the complexity of a given situation. This enables the system to factor in various
    elements and circumstances, adapting its logic and responses based on the specific
    context. Such adaptability is crucial for making well-informed choices, preventing
    errors, and achieving optimal outcomes. Whether in healthcare, finance, content
    moderation, or any other field, an AI system capable of grasping and integrating
    context can provide more precise, relevant, and reliable assistance, thus becoming
    an essential component in enhancing the utility and reliability of AI-powered
    solutions in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: To highlight the significance of context understanding and reasoning, let’s
    revisit some examples discussed in earlier chapters of this book. Clinical reasoning,
    for instance, heavily relies on context. Consider a pregnant woman, where the
    context drastically alters the approach to medication prescription. Many medications
    could potentially harm the developing fetus, while others might be safe or even
    necessary for the health of both the mother and the baby. Contextual reasoning
    in this scenario involves considering the stage of pregnancy, the specific medical
    condition, any pre-existing conditions, the risk–benefit ratio, and alternative
    treatment options.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in fraud prevention, an AI system must adapt to shifts in spending
    behavior during special events like Black Friday or vacations when a large number
    of transactions are expected to be normal. Recognizing the context and automatically
    adjusting parameters can significantly reduce false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Another context-sensitive domain is stock trading, where considerations such
    as high interest rates, geopolitical issues, unemployment, inflation, and more
    are essential. Each of these factors can profoundly impact the stock market, necessitating
    an adaptable and context-sensitive AI approach.
  prefs: []
  type: TYPE_NORMAL
- en: In text mining, understanding a piece of writing necessitates inferring the
    correct context, as the same word can have different meanings in distinct contexts.
    For example, *apple* can refer to a fruit, New York City, computers, or a company.
  prefs: []
  type: TYPE_NORMAL
- en: In content moderation, context plays a pivotal role. Imagine a video about the
    Third Reich, which could serve either as an educational tool to inform people
    about the atrocities of Nazism or as propaganda by a neofascist group. In one
    instance, promotion would be encouraged, while in the other, blocking would be
    imperative.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, next-generation AI systems must incorporate contextual reasoning
    as an integral component of their functionality due to the dynamic and complex
    nature of the real world. Context provides, in every domain and situation, vital
    information necessary for making intelligent decisions and actions. Without context,
    AI systems may produce inaccurate or inappropriate results.
  prefs: []
  type: TYPE_NORMAL
- en: 11.22 Causality inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Causality is fundamental to our understanding of the world, much like our perception
    of time. It serves as a crucial framework for comprehending the relationships
    between events, actions, and outcomes. However, it becomes a challenge when we
    consider how to encode the concept of cause and effect into a computer program.
    Causality extends far beyond the mere observation of events occurring in chronological
    order. It analyzes the multifaceted web of relationships that define not just
    when events happen but why they happen. This involves the idea that events, conditions,
    objects, and processes can all play roles in influencing one another.
  prefs: []
  type: TYPE_NORMAL
- en: Next-generation AI systems must be capable of not only recognizing patterns
    and correlations but also comprehending the underlying causative factors. This
    empowers AI to create business solutions that can reason, plan, and act with greater
    efficiency, ultimately benefiting society in numerous ways.
  prefs: []
  type: TYPE_NORMAL
- en: 11.23 Analogical reasoning and transferability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Humans possess an exceptional talent for analogical reasoning, a cognitive skill
    deeply rooted in our ability to recognize patterns, establish connections, and
    draw upon past experiences across a wide range of situations. This mental faculty
    is closely tied to our capacity for abstraction and generalization, allowing us
    to identify similarities between seemingly unrelated scenarios. This unique capability
    enables us to smoothly transfer knowledge and effective problem-solving strategies
    from one area to another. Throughout history, analogical reasoning has been the
    driving force behind numerous groundbreaking inventions. For example, George de
    Mestral found inspiration in the way cockleburs clung to his clothes and his dog’s
    fur, ultimately leading to the invention of Velcro. Similarly, James Dyson revolutionized
    the world of vacuum cleaners by drawing an analogy between the efficient, clog-free
    action of a sawmill cyclone and his own vacuum cleaner prototype.
  prefs: []
  type: TYPE_NORMAL
- en: Next-generation AI should have the ability to perform analogical reasoning and
    apply problem-solving techniques learned from one domain to another. This enhancement
    will significantly boost their efficiency and adaptability.
  prefs: []
  type: TYPE_NORMAL
- en: 11.24 Personalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Personalization is a pivotal factor across various domains. Current AI systems,
    which learn from extensive datasets, tend to recognize patterns that are relevant
    only at a broad population level. Take, for example, the typical AI system used
    in financial transactions today; it often applies the same logic to every merchant.
    However, each merchant possesses distinct characteristics and activity patterns.
    Similarly, cardholders exhibit varying spending patterns and purchasing habits.
    Overlooking these individual differences can lead to lower rates of fraud detection
    and a higher rate of false positives. Personalization can enhance the efficiency
    of AI systems. For instance, in the education sector, AI-powered personalized
    learning platforms can adapt to the pace and learning style of each student. In
    healthcare, personalization ensures that each patient receives the most suitable
    treatment plan.
  prefs: []
  type: TYPE_NORMAL
- en: Personalization acknowledges the uniqueness of individuals and situations. Therefore,
    its integration into AI systems will enable them to cater to the unique characteristics
    and requirements of individuals and situations, leading to improved outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 11.25 Sustainable AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Current AI systems have made remarkable strides in various domains, but they
    come at a significant environmental cost. One of the primary concerns is the massive
    energy consumption associated with AI, driven by the computational demands of
    training and running complex models. Data centers that house these AI infrastructures
    are substantial contributors to carbon emissions, often relying on non-renewable
    energy sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'This environmental impact is further exemplified by the astounding increase
    in computing power required for AI milestones, as reported by *WIRED* magazine,
    with a 300,000-fold surge from 2012 to 2018 [7]. Additionally, a recent report
    [8] from the *MIT Technology Review* reveals a startling fact: the complete process
    of building and training an AI system from scratch generates an astonishing 78,468
    pounds of CO2 emissions. This amount exceeds what an individual exhales in their
    lifetime and surpasses the emissions attributed to their entire lifetime of automobile
    use. Given these alarming figures, there is a concern that AI systems could increasingly
    be perceived as a threat to the climate.'
  prefs: []
  type: TYPE_NORMAL
- en: To address this environmental challenge, next-generation AI must prioritize
    efficiency as a core design principle. This involves the development and utilization
    of more streamlined algorithms that can achieve comparable results with significantly
    fewer computational resources. Additionally, efforts should focus on reducing
    data requirements for AI training, minimizing the environmental impact associated
    with data storage and transmission.
  prefs: []
  type: TYPE_NORMAL
- en: By embracing these strategies, next-generation AI can not only maintain its
    technological prowess but also fulfill its responsibility to be environmentally
    conscious and contribute to a more sustainable future.
  prefs: []
  type: TYPE_NORMAL
- en: 11.26 Adaptability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the ever-changing real world, characterized by its unpredictability and constant
    evolution, accurately forecasting future events and trends can be an incredibly
    daunting task. This is precisely where the concept of adaptability emerges as
    a critical factor in the success of various business applications.
  prefs: []
  type: TYPE_NORMAL
- en: To examine this notion further, let’s revisit a previous discussion from chapter
    4 where our focus was squarely on the payment industry. In that context, we highlighted
    the importance of employing flexible AI solutions. These solutions are essential
    for effectively navigating the perpetually shifting landscape of fraudulent activities
    and ever-evolving money laundering tactics that pose significant challenges to
    this industry. One of the challenges we discussed is that legacy AI systems often
    prove inflexible and cumbersome when confronted with even minor modifications
    or adjustments to the parameters of a given problem. These systems typically necessitate
    a complete overhaul and retraining, a process that not only consumes substantial
    resources but is also far from scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the next generation of AI platforms must rely on frameworks that
    possess the capacity to continuously learn and adapt.
  prefs: []
  type: TYPE_NORMAL
- en: 11.27 Human–machine collaboration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI systems often operate in isolation, functioning independently of human interaction.
    However, the evolving landscape of technology and our growing reliance on AI necessitates
    a shift toward AI platforms that are not only capable of independent tasks but
    also excel in forming collaborative partnerships with humans. This shift is driven
    by the realization that true innovation and productivity lie at the intersection
    of human creativity and artificial intelligence. Therefore, the next-generation
    AI systems should possess the capacity to facilitate smooth communication and
    cooperation between humans and machines, ultimately enhancing the capabilities
    of both.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good AI model should handle diverse and large sources of data that are enhanced
    by feature engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A project team should consider combining multiple techniques, including (good)
    unsupervised learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI solutions should be simple and reusable modules that nonexperts could easily
    integrate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Companies should apply the proper quality assurance when using AI models to
    ensure they work as expected, are resilient, and scale effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI models should be easy to deploy and secure and should address privacy concerns.
    Humans should be able to comprehend what they do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve a semblance of intelligence, future AI systems should understand
    analogies, inference, context, and many other concepts they are currently missing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
