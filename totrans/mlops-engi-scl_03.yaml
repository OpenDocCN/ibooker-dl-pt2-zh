- en: 2 Getting started with the data set
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 开始使用数据集
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Introducing a use case for machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入一个机器学习用例
- en: Starting with object storage for serverless machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从对象存储开始使用无服务器机器学习
- en: Using crawlers to automatically discover structured data schemas
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用爬虫自动发现结构化数据模式
- en: Migrating to column-oriented data storage for more efficient analytics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移到基于列的数据存储以实现更高效的分析。
- en: Experimenting with PySpark extract-transform-load (ETL) jobs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试 PySpark 提取-转换-加载（ETL）作业
- en: In the previous chapter, you learned about serverless machine learning platforms
    and some of the reasons they can help you build a successful machine learning
    system. In this chapter, you will get started with a pragmatic, real-world use
    case for a serverless machine learning platform. Next, you are asked to download
    a data set of a few years’ worth of taxi rides from Washington, DC, to build a
    machine learning model for the use case. As you get familiar with the data set
    and learn about the steps for using it to build a machine learning model, you
    are introduced to the key technologies that are a part of a serverless machine
    learning platform, including object storage, data crawlers, metadata catalogs,
    and distributed data processing (extract-transform-load) services. By the conclusion
    of the chapter, you will also see examples with code and shell commands that illustrate
    how these technologies can be used with Amazon Web Services (AWS) so that you
    can apply what you learned in your own AWS account.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了关于无服务器机器学习平台的知识以及它们为何能帮助你构建成功的机器学习系统的一些原因。在本章中，你将开始使用一个实用的、真实世界的无服务器机器学习平台用例。接下来，你被要求下载华盛顿特区几年的出租车乘车记录数据集，以构建一个适用于该用例的机器学习模型。当你熟悉数据集并了解如何使用它来构建机器学习模型的步骤时，你将了解到无服务器机器学习平台的关键技术，包括对象存储、数据爬虫、元数据目录和分布式数据处理（提取-转换-加载）服务。通过本章的结论，你还将看到使用代码和
    shell 命令示例，演示这些技术如何与亚马逊网络服务（AWS）一起使用，以便你可以在自己的 AWS 账户中应用所学的知识。
- en: 2.1 Introducing the Washington, DC taxi rides data set
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 引入华盛顿特区出租车乘车数据集
- en: This section dives into the details of the business domain and the business
    rules for the taxicab industry in Washington, DC. You might be tempted to skip
    these details; after all, they are probably irrelevant to the data sets you are
    planning to use in your machine learning projects. However, I encourage you to
    treat this section as a case study illustrating the kinds of questions you should
    ask about any business domain where you are planning to apply machine learning.
    As you explore the business use case in this section, you can learn more about
    the factors behind the DC taxi trips data and better prepare for building a machine
    learning model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入探讨了华盛顿特区出租车行业的业务领域和业务规则的细节。你可能会想跳过这些细节；毕竟，它们可能与你计划在机器学习项目中使用的数据集无关。然而，我鼓励你将本节视为一个案例研究，说明你在计划应用机器学习的任何业务领域中应该提出的问题种类。当你在本节中探索业务用例时，你可以更多地了解到
    DC 出租车行程数据背后的因素，并更好地为构建机器学习模型做准备。
- en: 2.1.1 What is the business use case?
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 业务用例是什么？
- en: Imagine that you are a machine learning engineer working for a plucky startup
    planning to launch an autonomous, driverless car to take over the ride-sharing
    industry and outmaneuver companies like Waymo, Uber, and Lyft. Your business leadership
    decided that the first market your service will launch in is Washington, DC. Since
    your startup wants to offer prices that are competitive with regular taxis, you
    have been asked to write some code to estimate how much it costs a passenger to
    take a regular taxi from one location to another within the boundaries of Washington,
    DC, and the nearby areas of Virginia and Maryland.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一名机器学习工程师，为一家年轻有为的初创公司工作，计划推出一款自动驾驶的无人驾驶汽车，以接管乘车共享行业，并超越 Waymo、Uber 和
    Lyft 等公司。你的业务领导决定，你的服务首先将在华盛顿特区市场推出。由于你的初创公司希望提供与普通出租车竞争的价格，所以你被要求编写一些代码来估算乘客从一个位置到另一个位置乘坐普通出租车的费用，范围包括华盛顿特区及其附近的弗吉尼亚州和马里兰州。
- en: 2.1.2 What are the business rules?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 业务规则是什么？
- en: 'The business rules for calculating the Washington, DC, taxi fares are available
    on the web from dc.gov.[¹](#pgfId-1011886) The rules are as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿特区出租车费用的计算业务规则可从 dc.gov 网站获取。[¹](#pgfId-1011886) 规则如下：
- en: The charge for the first 1/8 of a mile is $3.50.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前 1/8 英里的费用为 $3.50。
- en: Each additional 1/8 of a mile is charged at $0.27, which adds up to $2.16 per
    mile.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每增加1/8英里的费用为0.27美元，每英里累计费用为2.16美元。
- en: A special duration-based charge is $25 per hour and is accrued in 1-minute increments.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊的按时计费为每小时25美元，并按1分钟递增。
- en: The duration-based charge applies to situations when the cab is in a traffic
    jam so that the fare amount continues to increase over time. The dc.gov website
    also lists additional special charges (e.g., for snow emergency days), but let’s
    ignore them for now.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于持续时间的收费适用于出租车在交通拥堵中的情况，因此车费金额会随着时间的推移而持续增加。dc.gov网站还列出了其他特殊费用（例如，下雪紧急情况的费用），但让我们暂时忽略它们。
- en: 2.1.3 What is the schema for the business service?
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 业务服务的模式是什么？
- en: For a more concrete specification of the taxi fare estimation service interface,
    a software engineer could define the data types for the input and output values
    as shown in table 2.1\. The interface expects an input consisting of pickup and
    drop-off locations (each in terms of a pair of latitude and longitude coordinates),
    as well as a timestamp with an expected start time of the trip. The output of
    the service is just the dollar amount of the estimated taxi fare. The values provided
    in table 2.1 as examples apply to a short, half-mile taxi trip, which costs about
    $6.12\. Due to the fixed charge of $3.50 for the first 1/8 of the mile, and $0.81
    for the remaining 3/8 distance ($0.27 * 3), the remaining $1.81 is likely due
    to the taxi spending time in midday heavy traffic on a Monday in a busy area of
    downtown DC.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于出租车车费估算服务接口的更具体规范，软件工程师可以定义输入和输出值的数据类型，如表2.1所示。接口期望输入由接送位置（每个位置都是一对纬度和经度坐标）以及预计行程开始时间的时间戳组成。服务的输出仅是估计出租车费用的金额。表2.1中提供的示例值适用于短途半英里出租车行程，大约花费6.12美元。由于第1/8英里的固定费用为3.50美元，剩余的3/8距离的费用为0.81美元（0.27美元*3），剩余的1.81美元可能是由于出租车在星期一的午间高峰期在华盛顿特区市中心繁忙地区花费的时间。
- en: Table 2.1 Schema and example values for a taxi fare estimation service interface
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1出租车车费估算服务接口的架构和示例值
- en: '| Input |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 输入 |'
- en: '| Name | Data Type | Example Value |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数据类型 | 示例值 |'
- en: '| Pickup location latitude | FLOAT [²](#pgfId-1044105) | 38.907243 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 接送位置纬度 | 浮点数 [²](#pgfId-1044105) | 38.907243 |'
- en: '| Pickup location longitude | FLOAT | –77.042754 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 接送位置经度 | 浮点数 | –77.042754 |'
- en: '| Drop-off location latitude | FLOAT | 38.90451 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 接送位置经度 | 浮点数 | 38.90451 |'
- en: '| Drop-off location longitude | FLOAT | –77.048813 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 送达位置经度 | 浮点数 | –77.048813 |'
- en: '| Expected start time of the trip | TIMESTAMP [³](#pgfId-1044108) | 01/12/2015
    12:42 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 预计行程开始时间 | 时间戳 [³](#pgfId-1044108) | 2015年01月12日12:42 |'
- en: '| Output |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 输出 |'
- en: '| Name | Data Type | Example Value |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数据类型 | 示例值 |'
- en: '| Estimated fare (dollars) | FLOAT | 6.12 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 估计车费（美元） | 浮点数 | 6.12 |'
- en: The latitude and longitude coordinates of the trip from table 2.1 correspond
    to a pickup address of 1814 N St. NW and a drop-off address of 1100 New Hampshire
    Ave. NW in Washington, DC. Note that the service does not perform any geocoding;
    in other words, the service expects pickup and drop-off locations as latitude
    and longitude coordinates instead of human-readable addresses like 1100 New Hampshire
    Ave. NW. Of course, a user of your service is not expected to type in the latitude
    and longitude values of the coordinates. Instead, the user can be prompted to
    visually drop pins for pickup and drop-off locations on a map in your mobile application.
    The latitude and longitude of the dropped pins can then be used directly with
    the service. Alternatively, there are geocoding features available from Google
    Maps and similar services, but they are outside the scope of this book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1中行程的纬度和经度坐标对应于华盛顿特区1814 N St. NW的接送地址和1100 New Hampshire Ave. NW的送达地址。请注意，该服务不执行任何地理编码；换句话说，该服务期望接送位置和送达位置是纬度和经度坐标，而不是类似于1100
    New Hampshire Ave. NW的人类可读地址。当然，您的服务的用户不需要键入坐标的纬度和经度值。相反，用户可以在您的移动应用程序中的地图上直观地放置接送位置的图钉。然后，可以直接使用图钉的纬度和经度与该服务一起使用。或者，还可以使用来自Google地图和类似服务的地理编码功能，但它们不在本书的范围之内。
- en: 2.1.4 What are the options for implementing the business service?
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 实施业务服务的选项有哪些？
- en: The trip from the example in table 2.1 is based on just one of many possible
    taxi routes in the DC area. For the purposes of the taxi fare estimation service,
    a taxi trip can take place across any pickup and drop-off location, as long as
    both are within the diamond-shaped boundary, which includes the entirety of Washington,
    DC, as well as the nearby areas of Maryland and Virginia. The area on the interactive
    map ([https://osm.org/go/ZZcaT9](https://osm.org/go/ZZcaT9)) includes all possible
    pickup and drop-off locations for the DC taxi trips in this book. A user of your
    startup’s mobile application could place pickup and drop-off pins within the area
    boundary to get back an estimated price for the trip.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1中示例的行程仅是华盛顿特区可能的许多出租车路线中的一种。对于出租车费用估算服务的目的，出租车行程可以跨越任何上车和下车位置，只要两者都在菱形边界内，该边界包括华盛顿特区的整个范围以及马里兰州和弗吉尼亚州的附近地区。交互式地图（[https://osm.org/go/ZZcaT9](https://osm.org/go/ZZcaT9)）上的区域包括本书中华盛顿特区出租车行程的所有可能的上车和下车位置。你创业公司的移动应用的用户可以在区域边界内放置上车和下车标记，以获取行程的估价。
- en: Before diving into the implementation of a machine learning project for estimating
    the taxi fare, consider the traditional software engineering approach for building
    the fare estimation service. A software engineer (assume they are unfamiliar with
    machine learning) may start by developing code to use business rules for calculating
    the fare and by integrating the code with a route planning application programming
    interface (API) from a service such as Google Maps or Bing Maps. Both APIs can
    calculate the shortest driving route from one location to another and estimate
    the distance as well as the duration for the route. The actual route the taxi
    driver takes and the duration of the trip can vary based on traffic, road closures,
    weather, and other factors, but this approach provides a reasonable estimate of
    the distance. Next, the distance returned by the API can be combined with the
    business rules to calculate the estimated taxi fare.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在着手实现用于估算出租车费用的机器学习项目之前，请考虑一下传统的软件工程方法来构建费用估算服务。一个软件工程师（假设他们对机器学习不熟悉）可能会首先开发代码来使用业务规则来计算费用，并将代码与来自服务（如Google
    Maps或Bing Maps）的路线规划应用程序接口（API）集成。这两个API都可以计算从一个位置到另一个位置的最短驾驶路线，并估算路线的距离和持续时间。出租车司机实际走的路线和行程持续时间可能会因交通、道路关闭、天气和其他因素而有所不同，但这种方法可以合理估计距离。接下来，API返回的距离可以与业务规则结合起来计算出预估的出租车费用。
- en: The traditional software engineering approach to building the taxi fare estimation
    service has several advantages. The service is straightforward to implement, even
    for a junior software engineer. There is a large pool of engineers worldwide with
    the skills to deliver the implementation. Once implemented, the service should
    produce accurate estimates, except in the extreme cases where the taxi rides are
    impacted by unusual traffic, weather, or emergency events.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 构建出租车费用估算服务的传统软件工程方法有几个优点。该服务很容易实现，即使对于初级软件工程师也是如此。全球范围内有大量具备交付实施技能的工程师。一旦实施，该服务应该会产生准确的估算结果，除非出租车乘车受到异常交通、天气或紧急事件的影响。
- en: However, for a startup, relying on a route-planning service can be expensive.
    Services like Google Maps charge per API request to perform route planning and
    calculate distances, and to choose a route based on traffic. The costs of these
    services can quickly add up. Also, keep in mind the additional costs of the users
    of your service, who will estimate the price of a trip without actually taking
    the ride. While a larger company could explore the option of developing an on-premises,
    internal deployment of a route-planning service by building on open source data
    software or by purchasing a license from a vendor, the cost of doing so is prohibitive
    for a startup.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于创业公司来说，依赖路线规划服务可能会很昂贵。像Google Maps这样的服务每次API请求都要收费，以执行路线规划和计算距离，并根据交通选择路线。这些服务的成本可能会迅速累积起来。此外，还要考虑到您服务的用户的额外成本，他们将估算行程的价格而实际上并未乘坐车辆。虽然一家更大的公司可以探索通过在开源数据软件上进行构建或从供应商购买许可证来开发本地、内部部署的路线规划服务的选项，但对于创业公司来说，这样做的成本是禁止的。
- en: Instead of relying on traditional software engineering to build the taxi fare
    estimation service, in this book you are introduced to a machine learning approach
    implemented using the serverless capabilities of AWS.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不是依赖于传统软件工程来构建出租车费用估算服务，而是介绍了使用 AWS 的无服务器能力实现的机器学习方法。
- en: 2.1.5 What data assets are available for the business service?
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.5 什么数据资产可用于业务服务？
- en: The Office of the Chief Technology Officer for Washington, DC, maintains a website
    that hosts data from taxi rides that took place within the DC-area boundaries.[⁴](#pgfId-1012413)
    In this book, you will use this historical data set of the taxi rides from 2015
    through 2019 to build machine learning models to estimate how much it costs to
    travel by taxi around DC. The key advantage of the machine learning approach is
    that it will not depend on an expensive external service for route planning and
    distance calculations. The model will learn from the taxi rides data to estimate
    the fares based on taxi trips taken across different locations in DC.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿特区首席技术官办公室维护着一个网站，该网站托管了在华盛顿特区范围内发生的出租车行程的数据。[⁴](#pgfId-1012413) 在本书中，您将使用这个从2015年到2019年的出租车行程的历史数据集来构建机器学习模型，以估计在华盛顿特区乘坐出租车的成本。机器学习方法的关键优势在于，它不依赖于昂贵的外部服务进行路线规划和距离计算。该模型将从出租车行程数据中学习，以估计基于在华盛顿特区不同位置进行的出租车行程的费用。
- en: Later in the book, you will also deploy the models to AWS as a web service with
    an internet-accessible API for taxi fare predictions. The service will process
    HTTP (hyperText transfer protocol) requests containing geographic coordinates
    of the pickup and drop-off locations and will return the estimated taxi fare.
    The service’s API will also take into account the start time of the trip so that
    the model can correctly adjust the predicted fare. For example, fares for multiple
    trips across the same pickup and drop-off locations will vary depending on the
    time of the day (rush hour versus middle of the night), day of the week (weekday
    versus weekend), and even day of the year (holiday versus workday).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的后续部分，您还将部署模型到 AWS 作为一个具有互联网可访问 API 的 Web 服务，用于出租车费用预测。该服务将处理包含接送地点的地理坐标的
    HTTP（超文本传输协议）请求，并返回估计的出租车费用。该服务的 API 还将考虑行程的开始时间，以便模型能够正确调整预测的费用。例如，相同接送地点的多次行程的费用将根据一天中的时间（高峰时段与深夜）、一周中的日期（工作日与周末）甚至一年中的日期（假期与工作日）而变化。
- en: You are also going to observe that the machine learning approach can be adapted
    with minimal changes as your service expands to support other geographical areas.
    Instead of hardcoding city-specific business rules for every city where your startup
    wants to launch, you can simply extend the data set with data about taxi trips
    in other cities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会发现，机器学习方法可以通过最小的更改来适应服务扩展到支持其他地理区域。您不需要为您的创业公司想要推出的每个城市硬编码城市特定的业务规则，而只需将其他城市的出租车行程数据扩展到数据集中即可。
- en: 2.1.6 Downloading and unzipping the data set
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.6 下载和解压数据集
- en: Start with the data set by downloading and unzipping the files from opendata.dc.gov.[⁵](#pgfId-1012467)
    Once the files are downloaded, you should be able to confirm that you have the
    data for years 2015 through 2019, with a separate zip file for each year. Note
    that the data set for 2019 is limited to the data from January through June. After
    you unzip the files, the entire data set should take up to 12 GiB of disk space.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从 opendata.dc.gov 下载并解压文件开始处理数据集。[⁵](#pgfId-1012467) 下载文件后，您应该能够确认您拥有2015年到2019年的数据，每年都有一个单独的
    zip 文件。请注意，2019年的数据集仅限于一月至六月的数据。解压文件后，整个数据集应该占据不到12 GiB 的磁盘空间。
- en: Note After unzipping the files, the contents of the data set get placed into
    separate subdirectories. Before proceeding, move all the files from the data set
    to a single directory. Don’t worry about overwriting the README_DC_ Taxicab_trip.txt
    file; there is an identical copy of this file for every year of the data set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：解压文件后，数据集的内容会被放置到单独的子目录中。在继续之前，请将数据集中的所有文件移动到一个单独的目录中。不用担心覆盖 README_DC_Taxicab_trip.txt
    文件；对于数据集的每一年，该文件都有一个相同的副本。
- en: 'The instructions in this book assume that you are using the bash (or similar)
    shell in Linux or MacOS as your shell environment. Once you have downloaded and
    unzipped the files, you should be able to confirm that the data set occupies roughly
    12 GiB on your disk using the du command from your shell:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的指令假设你在 Linux 或 MacOS 中使用 bash（或类似的）shell 作为你的shell 环境。当你下载并解压缩文件后，你可以使用
    shell 的 du 命令确认数据集大约占据你磁盘的 12 GiB 空间。
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: resulting in the output that starts with the following.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出以以下内容开头。
- en: Listing 2.1 Unzipped files of the DC taxi trips data set from 2015 to 2019
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 显示从 2015 年到 2019 年的 DC 出租车行程数据集的解压缩文件
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For brevity, the output of the du command in listing 2.1 omits most of the files
    in the data set, replacing them with the ellipsis. The entire listing is available
    as a Github Gist ([http://mng.bz/nrov](http://mng.bz/nrov)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为简洁起见，列表 2.1 中 du 命令的输出省略了数据集中的大部分文件，并用省略号代替。完整的列表可在 Github Gist（[http://mng.bz/nrov](http://mng.bz/nrov)）上找到。
- en: Inside the zip files, the data is packaged as a collection of text files (with
    a “.txt” extension) that use the | (pipe) character to separate columns within
    each row. It is common for machine learning practitioners to refer to such files
    as *pipe-delimited comma-separated values* (CSVs). Although this industry terminology
    is confusing, I will keep with the practice of using the acronym CSV for this
    data format throughout the book.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 zip 文件中，数据被打包为一组文本文件（具有“.txt”扩展名），每行使用竖线（|）字符将列分隔开。机器学习从业者通常将这种文件称为*管道分隔逗号分隔值*（CSV）文件。尽管这个行业术语令人困惑，我将继续使用
    CSV 这个缩写词来表达这个数据格式，直到本书结束。
- en: The CSV files for the DC taxi data set contain a header row, meaning that the
    first row of every file has string labels for every column, for example MILEAGE,
    FAREAMOUNT, and others. The remaining rows in the files are the records of the
    taxi trips, one trip per row. Every zip file also contains an identical copy of
    the README_DC_Taxicab_trip.txt file, which provides some additional documentation
    for the data asset. The key parts of the documentation are covered later in this
    chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DC 出租车数据集的 CSV 文件包含标题行，即每个文件的第一行为每一列的字符串标签，例如 MILEAGE、FAREAMOUNT 等。文件中的其余行是出租车行程记录，每行一次行程。每个
    zip 文件还包含一个相同的 README_DC_Taxicab_trip.txt 文件副本，该文件提供了有关数据资产的一些附加文档。文档的关键部分将在本章后面介绍。
- en: 2.2 Starting with object storage for the data set
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 从数据集开始使用对象存储
- en: This section introduces you to the first serverless capability from the machine
    learning project in this book. You are going to build on what you know about traditional
    filesystems to start learning about serverless object storage. Next, you are going
    to use a command line interface for AWS to create a serverless object storage
    location for the DC taxi data set and start to copy your CSV files to the location.
    You are going to become acquainted with using public cloud object storage for
    your machine learning data sets and complete the transfer of the DC taxi data
    set to object storage for further processing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍本书中机器学习项目的第一个无服务器能力。你将在对传统文件系统的了解基础上，开始学习关于无服务器对象存储的知识。接下来，你将使用 AWS 的命令行界面为
    DC 出租车数据集创建一个无服务器对象存储位置，并开始将 CSV 文件复制到该位置。你将熟悉如何使用公共云对象存储来处理你的机器学习数据集，并将 DC 出租车数据集转移到对象存储中进行进一步处理。
- en: This section and the rest of the book will use examples based on Simple Storage
    Service (S3) from AWS to explain how serverless object storage can help you with
    machine learning projects. However, you should know that other public cloud vendors,
    such as Google Cloud and Microsoft Azure, offer similar capabilities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节和本书的其余部分将使用来自 AWS 的简单存储服务（S3）的示例来解释无服务器对象存储如何帮助你进行机器学习项目。但是，你应该知道其他公共云供应商，如
    Google Cloud 和 Microsoft Azure，也提供类似的功能。
- en: 2.2.1 Understanding object storage vs. filesystems
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 理解对象存储和文件系统的区别
- en: There are many similarities between filesystems and object storage, so you will
    find it easier to understand object storage if you start by focusing on the differences
    (table 2.2). Recall that filesystems are designed to store mutable, or changeable,
    data in a named location. This means that with a filesystem you can open a file,
    navigate to any line or a byte location in the file, change as many or as few
    bytes as you would like, and then save the changes back to the filesystem. Since
    files in a filesystem are mutable, after you make the change the original data
    is gone and is replaced on the storage medium (for example, on a solid-state drive)
    with your changes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统和对象存储之间有许多相似之处，因此，如果你首先专注于它们的区别（见表2.2），你会发现更容易理解对象存储。请记住，文件系统设计用于在命名位置存储可变或可更改的数据。这意味着使用文件系统，你可以打开文件，导航到文件中的任何行或字节位置，更改所需数量的字节，然后将更改保存回文件系统。由于文件系统中的文件是可变的，所以在进行更改后，原始数据就不存在了，并且被你的更改替换在存储介质上（例如固态驱动器）。
- en: Table 2.2 While both filesystems and object storage services have similar features
    such as hierarchies of folders and support common operations such as copy, delete,
    and move, there are some important differences, as highlighted in this table.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.2虽然文件系统和对象存储服务都具有类似的功能，如文件夹层次结构和支持复制、删除和移动等常见操作，但在这个表中突出显示了一些重要的区别。
- en: '| File system/Files | Serverless object storage/Objects |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 文件系统/文件 | 无服务器对象存储/对象 |'
- en: '| Mutable | Immutable |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 可变的 | 不可变的 |'
- en: '| Lack globally unique names | Can be globally identified using a URL |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 缺乏全局唯一名称 | 可以使用URL在全球范围内标识 |'
- en: '| Data redundancy across multiple storage devices | Data redundancy across
    multiple availability zones (data centers) and multiple storage devices |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 跨多个存储设备的数据冗余 | 跨多个可用性区域（数据中心）和多个存储设备的数据冗余 |'
- en: In contrast, objects in object storage are immutable. Once you have created
    an object in object storage, it stores exactly the data that was placed in the
    object when it was created. You can create a new version of an object with your
    changes, but as far as the object storage service is concerned, the new version
    is an additional object occupying additional storage space. Of course, you can
    also delete an entire object, freeing up the storage space.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，对象存储中的对象是不可变的。一旦在对象存储中创建了对象，它就会存储在创建对象时放入对象中的确切数据。你可以使用你的更改创建对象的新版本，但是就对象存储服务而言，新版本是一个额外的对象，占用额外的存储空间。当然，你也可以删除整个对象，释放存储空间。
- en: Unlike files, objects in serverless object storage services like AWS S3 are
    designed to be accessible on the internet using the HTTP protocol. By default,
    public internet access to objects is disabled. However, any object in serverless
    object storage can be made available via a public URL. To support this capability,
    object storage services organize objects into *buckets* (known as *containers*
    in Azure), which are named locations with globally unique identifiers. Every object
    in object storage must exist within a bucket, either directly or under some hierarchical,
    folder-like name structure. For example, if <guid> is a globally unique identifier
    name for an S3 bucket, an object named dataset could be accessible via a URL directly
    from the S3 bucket using https://<guid>.us-east-2.amazonaws.com/dataset or under
    a folder named “2015” within the bucket using https://<guid>.us-east-2.amazonaws.com/2015/dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与文件不同，像AWS S3这样的无服务器对象存储服务中的对象设计为可以使用HTTP协议在互联网上访问。默认情况下，对象的公共互联网访问是禁用的。但是，无服务器对象存储中的任何对象都可以通过公共URL提供。为了支持此功能，对象存储服务将对象组织到*桶*（在Azure中称为*容器*）中，这些桶是具有全局唯一标识符的命名位置。对象存储中的每个对象都必须存在于桶中，直接或在一些层次结构的类似文件夹的名称结构下。例如，如果<guid>是S3桶的全局唯一标识符名称，那么名为dataset的对象可以直接通过S3桶的URL访问，使用https://<guid>.us-east-2.amazonaws.com/dataset，或者在桶内名为“2015”的文件夹下使用https://<guid>.us-east-2.amazonaws.com/2015/dataset。
- en: The “us-east-2” portion of the object URL from the example is due to another
    difference between traditional filesystems and object storage. Unlike filesystems
    that rely on multiple storage devices within the same physical server for data
    redundancy,[⁶](#pgfId-1012868) object storage providers like AWS replicate data
    across both multiple storage devices and multiple physical data centers called
    *availability zones*. A redundant cluster of availability zones within a metropolitan
    area, interconnected by a high bandwidth and a low latency network, is called
    a *region*. The “us-east-2” part of the object URL specifies an AWS-specific code
    name for a region storing the object.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的对象 URL 中的“us-east-2”部分是传统文件系统和对象存储之间另一个差异的原因。与依赖于同一物理服务器内的多个存储设备进行数据冗余的文件系统不同，AWS
    等对象存储提供商会在多个存储设备和被称为*可用区*的多个物理数据中心之间复制数据。在一个大都市区域内相互连接的高带宽和低延迟网络上的冗余可用区集群称为*区域*。对象
    URL 的“us-east-2”部分指定了存储对象的区域的 AWS 特定代码名称。
- en: Why should you use serverless object storage for the DC taxi rides data set
    and for the taxi fare estimation service? For the purposes of your machine learning
    project, with serverless object storage you will not have to worry about running
    out of storage space. Services like S3 can help you scale from gigabyte- to petabyte-sized
    data sets. As you recall from the definition of serverless in chapter 1, using
    serverless object storage ensures that you won’t have any storage infrastructure
    to manage, and you will be charged based on the amount of data you keep in your
    storage bucket. Also, since object storage can provide an HTTP-based interface
    to the stored objects, it takes less effort to access and to integrate with the
    data that you have stored.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么您应该为 DC 出租车乘车数据集和出租车费用估算服务使用无服务器对象存储？对于您的机器学习项目，使用无服务器对象存储，您不必担心存储空间不足的问题。像
    S3 这样的服务可以帮助您从 GB 到 PB 的数据集进行扩展。正如您从第 1 章对无服务器的定义中所记得的那样，使用无服务器对象存储可以确保您不需要管理任何存储基础设施，并且您将根据存储桶中保存的数据量收费。此外，由于对象存储可以为存储的对象提供基于
    HTTP 的接口，因此访问和集成您存储的数据所需的工作量更少。
- en: 2.2.2 Authenticating with Amazon Web Services
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Amazon Web Services 进行身份验证
- en: The remaining examples in this chapter depend on AWS services. If you are planning
    to run the code from the examples, you should have the AWS Software Development
    Kit (SDK) installed, and you should know your AWS account’s access and secret
    keys. The details of the SDK installation are available from the AWS documentation
    ([https://docs.aws.amazon.com/cli](https://docs.aws.amazon.com/cli)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中剩余的示例依赖于 AWS 服务。如果您计划从示例中运行代码，则应安装 AWS 软件开发工具包 (SDK)，并了解您的 AWS 帐户的访问和密钥。SDK
    安装的详细信息可在 AWS 文档 ([https://docs.aws.amazon.com/cli](https://docs.aws.amazon.com/cli))
    中找到。
- en: If you don’t have your AWS access and secret keys available, you can generate
    a new pair by navigating to the AWS management console ([https://console.aws.amazon.com/](https://console.aws.amazon.com/)),
    clicking on your user name in the upper right-hand corner drop-down menu, and
    choosing “My Security Credentials.” To create a new pair of keys, click on the
    “Create access key” button.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有可用的 AWS 访问和密钥，可以通过转到 AWS 管理控制台 ([https://console.aws.amazon.com/](https://console.aws.amazon.com/))，点击右上角下拉菜单中的用户名，然后选择“我的安全凭证”来生成新的一对。要创建新的密钥对，请点击“创建访问密钥”按钮。
- en: The instructions in this book assume that you have your shell configured with
    AWS environment variables using
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的说明假定您已经配置了带有 AWS 环境变量的 shell 使用
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: before running any of the commands that depend on the AWS SDK.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行任何依赖 AWS SDK 的命令之前。
- en: Note In this book, all of the listings replace sensitive account-specific information
    with a sequence of █ characters. Take care to use your account-specific values
    for AWS access and secret keys.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在本书中，所有的清单都使用一系列 █ 字符替换了敏感的账户特定信息。请务必使用您的账户特定值来替换 AWS 访问和密钥。
- en: To verify that you have specified valid values for the environment variables
    AWS_ ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY you can run
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证您已指定有效值的环境变量 AWS_ ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY，您可以运行
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'which, in the case of a successful authentication with AWS, should return your
    UserId, Account, and Arn[⁷](#pgfId-1013184) values:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在与 AWS 成功验证的情况下，应返回您的用户 ID、账户和 Arn 值：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 2.2.3 Creating a serverless object storage bucket
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建无服务器对象存储存储桶
- en: This section walks you through the steps for creating an S3 bucket and uploading
    your DC taxi data files as objects to the bucket (figure 2.1). The steps in this
    section are completed using the command line interface (CLI) for AWS, but if you
    prefer, you can complete the same sequence of steps using the graphical user interface
    of the AWS management console ([https://console.aws.amazon.com](https://console.aws.amazon.com)).
    This book focuses on the CLI-based approach because it allows for the steps to
    be easily explained, tested, and re-used as part of a script-based automation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将指导您完成创建 S3 存储桶并将 DC 出租车数据文件作为对象上传到存储桶的步骤（图 2.1）。本节中的步骤使用 AWS 的命令行界面（CLI）完成，但如果您愿意，您也可以使用
    AWS 管理控制台的图形用户界面完成相同的步骤（[https://console.aws.amazon.com](https://console.aws.amazon.com)）。本书专注于基于
    CLI 的方法，因为它可以轻松解释、测试和重用作为脚本自动化的一部分的步骤。
- en: '![02-01](Images/02-01.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![02-01](Images/02-01.png)'
- en: Figure 2.1 To transfer the DC taxi data set to object storage in AWS, you are
    going to create an S3 bucket using the aws s3api create-bucket command specifying
    the region and a globally unique identifier for the bucket. Next, you are going
    to use aws s3 sync to upload the data set files to a folder named “csv” in the
    bucket.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 要将 DC 出租车数据集传输到 AWS 的对象存储中，您将使用 aws s3api create-bucket 命令创建一个 S3 存储桶，指定区域和存储桶的全局唯一标识符。接下来，您将使用
    aws s3 sync 将数据集文件上传到存储桶中名为“csv”的文件夹中。
- en: The selection of the region (and as a consequence of the location) for the bucket
    is important for low latency access to the data stored in the bucket. Going forward,
    you should remember to run any of the code that processes your data from the same
    region you placed your S3 bucket. This section assumes that you will be using
    the us-east-2 region to store the data set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 选择存储桶的区域（以及作为结果的位置）对于访问存储在存储桶中的数据的低延迟非常重要。未来，您应该记住从与您放置 S3 存储桶相同的区域运行任何处理数据的代码。本节假设您将使用
    us-east-2 区域存储数据集。
- en: To export the setting for the AWS_DEFAULT_REGION variable, which is going to
    be used to specify the default region of the bucket, run
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要导出 AWS_DEFAULT_REGION 变量的设置，该变量将用于指定存储桶的默认区域，请运行
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: which should print back your chosen value for the bucket region.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回您选择的存储桶区域的值。
- en: Since bucket names are supposed to be globally unique, it makes no sense for
    this book to publish a fixed and identical bucket name in the code listing. Instead,
    listing 2.2 uses the $RANDOM environment variable, which always returns a pseudorandom
    value. The value is then hashed using the MD5 hash function to a unique identifier
    consisting of a sequence of numbers and characters. The BUCKET_ID variable is
    then set to the value of the first 32 characters of the hash string, as returned
    by the output of the cut command.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存储桶名称应该是全局唯一的，因此在代码清单中发布固定且相同的存储桶名称毫无意义。相反，清单 2.2 使用 $RANDOM 环境变量，该变量始终返回一个伪随机值。然后，使用
    MD5 散列函数对值进行哈希处理，以得到由一系列数字和字符组成的唯一标识符。然后将 BUCKET_ID 变量设置为哈希字符串的前 32 个字符的值，如 cut
    命令的输出所示。
- en: Listing 2.2 Using a pseudorandom generator for a likely unique value of the
    bucket ID
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.2 使用伪随机生成器生成存储桶 ID 的可能唯一值
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Use the first 32 characters of an MD5 hash of the Linux pseudorandom number
    generator.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 Linux 伪随机数生成器的 MD5 哈希的前 32 个字符。
- en: Note If you are running the commands in listing 2.2 using Mac OSX or BSD, you
    may need to use md5 instead of md5sum.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您在 Mac OSX 或 BSD 上运行清单 2.2 中的命令，则可能需要使用 md5 而不是 md5sum。
- en: At this point, you should have exported the environment variables specifying
    the globally unique identifier (in BUCKET_ID) and the region (in AWS_DEFAULT_REGION)
    for the bucket.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该已经导出了环境变量，指定了存储桶的全局唯一标识符（在 BUCKET_ID 中）和区域（在 AWS_DEFAULT_REGION 中）。
- en: Before creating the bucket, run
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建存储桶之前，运行
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: to ensure that your shell is still configured with valid values for the environment
    variables, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, required to authenticate
    with AWS.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以确保您的 shell 配置为有效的环境变量值，这些值是用于与 AWS 进行身份验证所需的 AWS_ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY。
- en: Notice that the following command, which creates the bucket, uses the aws s3api
    instead of the aws s3 you may have expected. This is for compatibility with the
    legacy, fine-grained AWS CLI features that were made available prior to the introduction
    of aws s3 commands.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以下命令创建存储桶时使用的是 aws s3api 而不是您可能期望的 aws s3。这是为了与传统的、细粒度的 AWS CLI 功能兼容，这些功能在引入
    aws s3 命令之前就已经提供。
- en: Note If you would like to use us-east-1 (the Northern Virginia region) instead
    of us-east-2, you need to drop the LocationConstraint argument to the aws s3api
    create-bucket command.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果想要使用 us-east-1（北弗吉尼亚地区）而不是 us-east-2，您需要在 aws s3api create-bucket 命令中删除
    LocationConstraint 参数。
- en: Create the bucket
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 创建存储桶。
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'and confirm that the command returns a result similar to the following JavaScript
    object notation (JSON) response, using your values for the BUCKET_ID and AWS_
    DEFAULT_REGION environment variables in place of the █ characters:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 并确认使用您的 BUCKET_ID 和 AWS_DEFAULT_REGION 环境变量替换 █ 字符后，命令返回类似于以下 JavaScript 对象符号
    (JSON) 响应：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Although the response to the aws s3api create-bucket command returns an HTTP
    URL for the bucket, you will usually refer to the bucket by an AWS specific naming
    scheme that starts with the s3:// prefix. If you lose track of the name you can
    recreate it using
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 aws s3api create-bucket 命令的响应返回存储桶的 HTTP URL，但通常您将使用以 s3:// 前缀开头的 AWS 特定命名方案来引用存储桶。如果您迷失了名称，可以使用以下代码重新创建它：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also use the AWS CLI list-buckets command to print out all the buckets
    that exist in your AWS account; however, the printed names will not use the s3://
    prefix:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 AWS CLI 的 list-buckets 命令打印出 AWS 帐户中存在的所有存储桶。然而，打印的名称不会使用 s3:// 前缀：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The list-buckets command can provide you with a second confirmation that the
    bucket was created successfully. Once you know that the bucket was created, change
    the present working directory of your shell to the directory containing the data
    set files from listing 2.1.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: list-buckets 命令可以为您提供第二次确认，以确保存储桶已经成功创建。一旦您知道存储桶已经创建成功，将当前工作目录更改为包含列表2.1中数据集文件的目录。
- en: Next, use the aws s3 sync command to replicate the data set files to the bucket.
    The command recursively transfers new and modified files to or from a location
    in an S3 bucket. While running, the command relies on multiple threads to speed
    up the transfer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用 aws s3 sync 命令将数据集文件复制到存储桶中。该命令递归地传输新文件和修改后的文件到或从S3存储桶中的位置。在运行时，命令依赖于多个线程来加快传输速度。
- en: Transfer the CSV files from your local working directory to a csv folder in
    your bucket using
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用下面的代码将 CSV 文件从本地工作目录传输到存储桶中的 csv 文件夹：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The time that it takes to transfer the data depends on the bandwidth you have
    available. In most cases, you should expect that it will take over 10 minutes,
    so this is a good point to take a break and resume once the transfer is over.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 数据传输所需的时间取决于您可用的带宽。在大多数情况下，您应该预计需要超过10分钟，因此这是休息并在传输完成后继续的好时机。
- en: 'After the sync command completes, you can confirm that the data set files are
    stored under the csv folder in the bucket using the aws s3 ls command. Just as
    with Unix-like operating systems, the ls command in S3 lists the contents of a
    folder. Try running the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: sync 命令完成后，您可以使用 aws s3 ls 命令确认数据集文件存储在存储桶的 csv 文件夹下。与类 Unix 的操作系统一样，S3 中的 ls
    命令列出文件夹的内容。试着运行以下命令：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice that you have transferred 11.2 GiB of CSV files to your object storage
    bucket. This quantity of data transferred to the bucket should match the size
    of the data set contents from listing 2.1.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意你已经将11.2 GiB 的 CSV 文件传输到了对象存储存储桶中。这个传输到存储桶的数据量应该与列表2.1的数据集内容大小匹配。
- en: After the files have been uploaded to the object storage, they are available
    for download and processing; however, the data is not yet distinguishable from
    unstructured binary large objects (BLOBs). To catalog the data structure in the
    CSV files, you are going to have to crawl the files and discover the data schema.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 文件上传到对象存储后，可以下载和处理它们。然而，这些数据还不能与非结构化的二进制大对象 (BLOB) 区分开来。为了在 CSV 文件中编目数据结构，您将需要遍历文件并发现数据模式。
- en: 2.3 Discovering the schema for the data set
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 发现数据集的模式
- en: At this point, you have created a csv folder in your object storage bucket and
    have transferred the DC taxi data set consisting of 11.2 GiB worth of CSV files
    to the folder. Before starting with the analysis of the files, it is important
    to identify and understand the data set’s schema. While it is possible to discover
    the data set schema manually, for example, by searching the opendata.dc.gov website
    for the schema specification or by exploring the contents of the CSV files directly,
    an automated approach can simplify and accelerate the process of schema discovery.
    In this section, you will learn about a data crawler service that can help you
    automate schema discovery for your data sets so you can better keep up with schema
    changes in your data. You are also going to crawl the CSV files of the DC taxi
    data set and persist the schema of the data set in a data catalog.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您已在对象存储桶中创建了一个 csv 文件夹，并已将由 11.2 GiB 的 CSV 文件组成的 DC 出租车数据集传输到该文件夹中。在开始对文件进行分析之前，重要的是要识别和了解数据集的架构。虽然可以手动发现数据集的架构，例如，通过搜索
    opendata.dc.gov 网站获取架构规范或直接探索 CSV 文件的内容，但自动化方法可以简化和加速架构发现的过程。在本节中，您将了解一个数据爬虫服务，该服务可以帮助您自动发现数据集的架构，以便您更好地跟上数据的架构变化。您还将爬取
    DC 出租车数据集的 CSV 文件，并将数据集的架构持久化存储在数据目录中。
- en: 2.3.1 Introducing AWS Glue
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 介绍 AWS Glue
- en: Glue is as an umbrella name for a toolkit of different AWS services that you
    can use to prepare your data set for analysis. In this book, you will learn about
    the Glue data catalog, Glue extract-transform-load (data processing) jobs, and
    the Glue library for distributed data processing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Glue 是一个包含不同 AWS 服务工具包的总称，您可以使用这些工具包为数据集准备分析。在本书中，您将了解 Glue 数据目录、Glue 提取-转换-加载（数据处理）作业以及用于分布式数据处理的
    Glue 库。
- en: The Glue data catalog is a metadata repository designed to store information
    about data assets, data schemas, and data provenance. The data catalog consists
    of one or more databases, which exist to organize a collection of tables together.
    Since Glue databases and tables are designed to store metadata, your project data
    must exist in storage outside of Glue. For example, Glue tables can store schema
    for data stored in object storage, relational (for example MySQL or PostgreSQL),
    or NoSQL databases.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Glue 数据目录是一个设计用于存储有关数据资产、数据架构和数据来源的元数据存储库。数据目录由一个或多个数据库组成，这些数据库存在于一起组织一组表。由于
    Glue 数据库和表设计用于存储元数据，因此您的项目数据必须存在于 Glue 之外的存储中。例如，Glue 表可以存储存储在对象存储中、关系（例如 MySQL
    或 PostgreSQL）或 NoSQL 数据库中的数据的架构。
- en: In addition to data schemas, Glue tables maintain information about the time
    when the schema was inferred from the data, as well as some basic statistics about
    the data, such as the number of objects from object storage used to store the
    data, the number of rows in the data, and the average amount of space occupied
    by a row in object storage.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据架构之外，Glue 表还保留了有关从数据推断架构的时间以及有关数据的一些基本统计信息，例如用于存储数据的对象存储中使用的对象数量、数据中的行数以及对象存储中一行占用的平均空间量。
- en: While it is possible to manually create a table in the Glue database, in this
    section you will learn about using a Glue crawler to create a table automatically.
    If you are familiar with the term *crawler* in the context of web search engines,
    keep in mind that Glue crawlers are different. They are designed to process and
    analyze structured data formats rather than web pages. A Glue crawler is a process
    that
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以手动在 Glue 数据库中创建表，但在本节中，您将了解如何使用 Glue 爬虫自动创建表。如果您熟悉在网络搜索引擎的上下文中 *爬虫* 一词，请记住
    Glue 爬虫是不同的。它们设计用于处理和分析结构化数据格式，而不是网页。Glue 爬虫是一个过程，它
- en: Establishes a connection to a storage location with structured data (e.g., to
    an object storage bucket)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立与结构化数据存储位置的连接（例如，与对象存储桶的连接）
- en: Identifies the format used by the data (e.g., CSV)
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定数据使用的格式（例如，CSV）
- en: Analyzes the data to infer the data schema, including the various column data
    types, such as integers, floating point numbers, and strings
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析数据以推断数据架构，包括各种列数据类型，例如整数、浮点数和字符串
- en: Crawlers can be scheduled to periodically recrawl the data, so if the schema
    of your data changes over time, a crawler will be able to detect that change the
    next time it runs and update the schema in a table.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫可以被定期调度以定期重新爬取数据，因此，如果您的数据架构随时间变化，下次运行爬虫时，爬虫将能够检测到该变化并更新表中的架构。
- en: To create a crawler, you need to provide a crawler configuration that specifies
    one or more targets, in other words, the unique identifiers specifying storage
    locations with the data that should be processed by the crawler. In addition,
    a crawler in AWS must assume a security role to access the data in the crawler
    configuration target. The cloud providers like AWS require you to create a security
    identity, known as a *role*, whenever an application, service, or a process (e.g.,
    an AWS Glue crawler) is accessing cloud resources on your behalf.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个爬虫，您需要提供一个爬虫配置，该配置指定一个或多个目标，换句话说，指定应该由爬虫处理的存储位置的唯一标识符。此外，AWS中的爬虫必须假定一个安全角色，以访问爬虫配置目标中的数据。像AWS这样的云提供商要求您在应用程序、服务或过程（例如AWS
    Glue爬虫）代表您访问云资源时创建安全身份，称为*角色*。
- en: 2.3.2 Authorizing the crawler to access your objects
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 授权爬虫访问您的对象
- en: Prior to creating a crawler for the DC taxi data, you should complete the steps
    in listing 2.3 to create a role called AWSGlueServiceRole-dc-taxi. The aws iam
    create-role command (listing 2.3 ❶), creates the role with a policy document that
    permits the Glue service ([https://aws.amazon.com/glue](https://aws.amazon.com/glue))
    to assume the AWSGlueServiceRole-dc-taxi security role as specified by the sts:AssumeRole
    permission. In short, the policy document specifies that the Glue crawler should
    use the AWSGlueServiceRole-dc-taxi role.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在为DC出租车数据创建爬虫之前，您应完成列表2.3中的步骤，以创建一个名为AWSGlueServiceRole-dc-taxi的角色。aws iam create-role命令（列表2.3 ❶）创建了一个角色，该角色具有允许Glue服务（[https://aws.amazon.com/glue](https://aws.amazon.com/glue)）假定AWSGlueServiceRole-dc-taxi安全角色的策略文档。简而言之，策略文档指定Glue爬虫应使用AWSGlueServiceRole-dc-taxi角色。
- en: Listing 2.3 Allowing AWS Glue crawler to access the files in your object storage
    bucket
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3 允许AWS Glue爬虫访问对象存储桶中的文件
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Create a security role named AWSGlueServiceRole-dc-taxi.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建名为AWSGlueServiceRole-dc-taxi的安全角色。
- en: ❷ Attach the AWS Glue policy to the AWSGlueServiceRole-dc-taxi role.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将AWS Glue策略附加到AWSGlueServiceRole-dc-taxi角色。
- en: ❸ Assign a policy document to AWSGlueServiceRole-dc-taxi to enable crawling
    of the data set S3 bucket.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为AWSGlueServiceRole-dc-taxi分配一个策略文件，以启用对数据集S3存储桶的爬取。
- en: The aws iam attach-role-policy command (listing 2.3 ❷) attaches an existing
    service role defined by AWS Glue (AWSGlueServiceRole) to the AWSGlueServiceRole-dc-taxi
    role. Attaching the role ensures that the AWSGlueServiceRole-dc-taxi role can
    access Glue databases and tables and perform other required operations with AWS
    resources. The details of the AWSGlueServiceRole specification are available from
    the AWS documentation ([http://mng.bz/XrmY](http://mng.bz/XrmY)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: aws iam attach-role-policy命令（列表2.3 ❷）将AWS Glue定义的现有服务角色（AWSGlueServiceRole）附加到AWSGlueServiceRole-dc-taxi角色。附加角色确保AWSGlueServiceRole-dc-taxi角色可以访问Glue数据库和表，并执行与AWS资源的其他必需操作。AWSGlueServiceRole规范的详细信息可从AWS文档（[http://mng.bz/XrmY](http://mng.bz/XrmY)）中获取。
- en: The aws iam put-role-policy command (listing 2.3 ❸) specifies that the AWSGlueServiceRole-dc-taxi
    role is allowed to access the contents of the object storage bucket you created
    and populated with the DC taxi CSV files earlier in this chapter.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: aws iam put-role-policy命令（列表2.3 ❸）指定AWSGlueServiceRole-dc-taxi角色被允许访问您在本章早些时候创建并填充DC出租车CSV文件的对象存储桶的内容。
- en: 2.3.3 Using a crawler to discover the data schema
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 使用爬虫发现数据模式
- en: In this section you will create a database and a crawler in Glue, configure
    the crawler to process the DC taxi data, and run the crawler to populate the database
    with a table containing the data schema. You have the option of using a browser
    interface to AWS[⁸](#pgfId-1014877) to complete these steps. However, listing
    2.4 and upcoming listings in this chapter will explain the CLI-based commands
    to create a Glue database and a crawler, and to start the crawler to discover
    the DC taxi data schema.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，您将在Glue中创建一个数据库和爬虫，配置爬虫以处理DC出租车数据，并运行爬虫以填充数据库，其中包含数据架构的表。您可以选择使用AWS的浏览器界面来完成这些步骤[⁸](#pgfId-1014877)。然而，本章中的列表2.4和即将出现的列表将解释基于CLI的命令，以创建一个Glue数据库和一个爬虫，并启动爬虫来发现DC出租车数据模式。
- en: In listing 2.4 ❶, the aws glue create-database command creates the Glue metadata
    database named dc_taxi_db, which is going to be used to store a schema for the
    DC taxi data set along with a table based on the schema.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.4 ❶中，aws glue create-database命令创建了名为dc_taxi_db的Glue元数据数据库，该数据库将用于存储DC出租车数据集的模式以及基于该模式的表。
- en: Listing 2.4 Creating a database and confirming the database exists
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 创建数据库并确认数据库存在
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Create the database named dc_taxi_db.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建名为`dc_taxi_db`的数据库。
- en: ❷ Confirm the database named dc_taxi_db was created.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确认名为`dc_taxi_db`的数据库已创建。
- en: Since it is a process, a Glue crawler cycles through a sequence of states. Once
    a crawler is created successfully, it begins its existence in a READY state. After
    is it started, the crawler transitions to a RUNNING state. While in a RUNNING
    state, it establishes a connection to a storage location specified in the crawler
    configuration. Based on the crawler configuration, the crawler identifies what
    locations in storage are included or excluded from processing and uses the data
    in the locations to infer the data schema. The RUNNING state often takes the longest
    period of time for the crawler to complete because it is the state in which the
    crawler is doing most of the work. Next, the crawler transitions to a STOPPING
    state to populate a Glue data catalog table with the schema and other metadata
    discovered during the process. Assuming that the process completes successfully,
    the crawler returns to a READY state.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个过程，Glue 爬虫会循环通过一系列状态。成功创建爬虫后，它会从 READY 状态开始存在。启动后，爬虫转移到 RUNNING 状态。在 RUNNING
    状态下，爬虫会建立与爬虫配置中指定的存储位置的连接。根据爬虫配置，爬虫会识别在处理过程中包含或排除的存储位置，并使用这些位置的数据推断数据架构。RUNNING
    状态通常是爬虫完成的时间最长的状态，因为爬虫在此状态下进行大部分工作。接下来，爬虫转移到 STOPPING 状态，以使用在过程中发现的架构和其他元数据填充
    Glue 数据目录表。假设进程成功完成，爬虫将返回到 READY 状态。
- en: Listing 2.5 Creating and starting a Glue crawler
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5 创建和启动 Glue 爬虫
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Use dc-taxi-csv-crawler as the crawler name.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将`dc-taxi-csv-crawler`用作爬虫名称。
- en: ❷ Store crawler output in dc_taxi_db.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将爬虫输出存储在`dc_taxi_db`中。
- en: ❸ Tables created by the crawler should start with dc_taxi_ prefix.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 爬虫创建的表名应以`dc_taxi_`前缀开头。
- en: ❹ Use the AWSGlueServiceRole-dc-taxi role for the crawler.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用`AWSGlueServiceRole-dc-taxi`角色进行爬虫操作。
- en: ❺ Configure the crawler to crawl the csv folder of the data set bucket.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 配置爬虫以爬取数据集存储桶的`csv`文件夹。
- en: ❻ Exclude the README_DC_Taxicab_trip documentation file from crawler.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 排除爬虫中的`README_DC_Taxicab_trip`文档文件。
- en: ❼ Start the crawler.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 启动爬虫。
- en: In listing 2.5, the crawler is created using dc-taxi-csv-crawler ❶ and is configured
    to store the metadata discovered during the crawling process in the dc_taxi_db
    database. The crawler is also configured to use a table prefix of dc_taxi_ for
    any tables created by the crawler in the database.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.5中，使用`dc-taxi-csv-crawler`❶创建爬虫，并配置将爬取过程中发现的元数据存储在`dc_taxi_db`数据库中。还配置了爬虫在数据库中创建的任何表的表前缀为`dc_taxi_`。
- en: Notice that the command specified by listing 2.5 ❹, is more complex than other
    shell commands you have encountered in this chapter. In bash, a command enclosed
    in $( ) characters are evaluated first, and the output of the evaluation is used
    in the original command. So, the aws iam get-role command nested in $( ) is used
    to find out the Amazon resource name (Arn) for the role you created in listing
    2.3.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，列表2.5中指定的命令❹比本章遇到的其他 shell 命令更复杂。在 bash 中，用 $( ) 字符括起来的命令首先被评估，然后评估的输出用于原始命令中。因此，在
    $( ) 中嵌套的`aws iam get-role`命令用于查找您在列表2.3中创建的角色的亚马逊资源名称（Arn）。
- en: In listing 2.5, the crawler is configured to crawl the csv folder in the object
    storage bucket where you uploaded the DC taxi data files, taking care to ignore
    objects with ❺a README prefix ❻.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.5中，配置爬虫以爬取您上传 DC 出租车数据文件的对象存储桶中的`csv`文件夹，并注意忽略以❺`README`前缀❻的对象。
- en: Finally, dc-taxi-csv-crawler is started using the aws glue start-crawler command
    per listing 2.5 ❼.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，根据第❼条指示使用`aws glue start-crawler`命令启动`dc-taxi-csv-crawler`。
- en: 'In case of the DC taxi data set, the crawling process should take just over
    a minute. To monitor the state of the crawler, you can use the AWS management
    console from your browser, or run the following command to print out the state
    of the crawler:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DC 出租车数据集，爬取过程应该在一分钟左右完成。您可以使用浏览器中的 AWS 管理控制台监视爬虫的状态，或者运行以下命令来打印爬虫的状态：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: While the crawler is running, the state should be running. As soon as the crawler
    is done it should change to ready.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当爬虫正在运行时，状态应为运行中。一旦爬虫完成，它应该变为就绪状态。
- en: Note To print a refreshed state of the crawler every two seconds, you can type
    in “watch” before the aws glue get-crawler command.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要每两秒打印一次爬虫的最新状态，您可以在`aws glue get-crawler`命令之前输入“watch”。
- en: Once the crawler returns to the READY state, you can find out whether the crawl
    succeeded using
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦爬虫返回到READY状态，您可以使用以下方法确定爬取是否成功
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The last crawl details requested by the --query 'Crawler.LastCrawl' argument
    include a status message indicating whether the last run of the crawler succeeded
    or failed.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由--query 'Crawler.LastCrawl'参数请求的最后一次爬取详情包括一个状态消息，指示爬虫的最后一次运行是成功还是失败。
- en: Assuming the crawler completed successfully, you can list the column names and
    the column data types of the schema discovered by the crawler using
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 假设爬虫成功完成，您可以使用以下方法列出爬虫发现的模式的列名和列数据类型
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that the table name “dc_taxi_csv” was automatically assigned by the crawler
    based on the combination of the crawler table prefix from listing 2.5 ❷, and the
    csv folder name in the crawled bucket, as specified by ❸ in listing 2.5.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，表名“dc_taxi_csv”是由爬虫根据清单2.5中爬虫表前缀的组合和爬取存储桶中csv文件夹的名称自动分配的，如清单2.5中所示。
- en: Keep in mind that you can also view the schema printed by the aws glue get-table
    command using your browser by navigating to the Glue service in AWS,[⁹](#pgfId-1015821)
    choosing “Data Catalog > Databases > Tables” on the left sidebar, and clicking
    on the dc_taxi_csv table on the right.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您也可以使用浏览器查看aws glue get-table命令打印的模式，方法是导航到AWS中的Glue服务，选择左侧边栏中的“数据目录 > 数据库
    > 表”，然后在右侧单击“dc_taxi_csv”表。
- en: At this point, your project has progressed beyond treating the DC taxi data
    as a collection of BLOBs and created a more detailed specification for the structure
    of the data, enumerating the data columns and their data types. However, the CSV
    data format you have been using so far is poorly suited for efficient and scalable
    analysis. In the upcoming sections of this chapter, you will learn how you can
    modify your data format to reduce latency for your analytical data queries.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您的项目已经超越了将DC出租车数据视为BLOB集合的阶段，并为数据的结构创建了更详细的规范，列举了数据列及其数据类型。然而，到目前为止您一直使用的CSV数据格式并不适合进行高效和可扩展的分析。在本章的即将到来的部分，您将学习如何修改数据格式以减少分析数据查询的延迟。
- en: 2.4 Migrating to columnar storage for more efficient analytics
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移到列式存储以进行更高效的分析
- en: In the next chapter of this book, you will learn about an interactive query
    service that can help you query the DC taxi data set using the table and data
    schema you discovered with the Glue crawler. However, as explained in this section,
    analytical queries against row-oriented data storage formats such as CSV are inefficient
    when working with large data sets. Although you could dive into the analysis of
    the DC data set right away, this section is going to first introduce you to the
    benefits of using column-oriented (columnar) data storage formats like Apache
    Parquet instead of CSV for analytics. After explaining the advantages and disadvantages
    of column-oriented formats, the remainder of the section will cover another serverless
    capability of AWS for distributed data processing using PySpark (Apache Spark).
    By the conclusion of the section, you will learn an example of a typical PySpark
    job that can help you re-encode your CSV files into the Parquet format so that
    in the upcoming chapters you can analyze the data set faster and more efficiently.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章中，您将了解到一种交互式查询服务，该服务可以帮助您使用Glue爬虫发现的表和数据模式查询DC出租车数据集。然而，正如本节所解释的，针对行式数据存储格式（如CSV）的分析查询在处理大型数据集时效率低下。虽然您可以立即深入分析DC数据集，但本节将首先向您介绍使用列式（列式）数据存储格式（如Apache
    Parquet）而不是CSV进行分析的好处。在解释了列式格式的优缺点之后，本节的其余部分将涵盖AWS的另一个用于使用PySpark（Apache Spark）进行分布式数据处理的无服务器功能。通过本节的结论，您将学习到一个典型的PySpark作业示例，该示例可以帮助您将CSV文件重新编码为Parquet格式，以便在即将到来的章节中更快、更高效地分析数据集。
- en: 2.4.1 Introducing column-oriented data formats for analytics
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入列式数据格式用于分析
- en: The CSV data format used by the DC taxi data set is an example of a row-oriented
    format. With CSV files, every line in a file stores a single row of data from
    the structured data set. Row-oriented data formats (illustrated on the left side
    of figure 2.2) are commonly used by traditional relational databases to store
    sequences of data records. The row-oriented format works well for transactional
    workloads that are typical for relational databases. Transactional workloads operate
    on individual rows of data and often just on a single row at a time. For example,
    consider a transactional database that stores a record of a taxi trip. If a passenger
    decides to change the destination of the trip halfway there, a transactional database
    can easily handle identifying the row of data about the trip, updating the latitude
    and the longitude coordinates for the destination, and then saving the changes
    back to the database.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: DC出租车数据集使用的CSV数据格式是行定向格式的一个例子。对于CSV文件，文件中的每一行存储来自结构化数据集的单个数据行。行定向数据格式（如图2.2左侧所示）通常由传统关系型数据库使用，用于存储数据记录序列。行定向格式非常适合关系型数据库的事务工作负载。事务工作负载是对数据的单个行进行操作，并且通常一次仅操作一行。例如，考虑一个存储出租车行程记录的事务性数据库。如果乘客在行程的一半决定更改目的地，事务性数据库可以轻松处理识别关于行程的数据行、更新目的地的纬度和经度坐标，然后将更改保存回数据库。
- en: '![02-02](Images/02-02.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![02-02](Images/02-02.png)'
- en: Figure 2.2 Row-oriented storage (left) used by CSV files and traditional relational
    databases, is designed for transactional processing, enabling changes to a row
    of data at a time. Column-oriented storage (right) used by Apache Parquet and
    many modern data warehouses, works best with analytical queries over immutable
    datasets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2行定向存储（左侧）由CSV文件和传统关系型数据库使用，设计用于事务处理，使得一次可以更改一行数据。列定向存储（右侧）由Apache Parquet和许多现代数据仓库使用，最适合于对不可变数据集进行分析查询。
- en: Analytical workloads are significantly different from transactional ones. While
    performing an analytical query on a data set, it is typical to process all of
    the rows in a data set, for example to identify rows with taxi trips during specific
    hours of a day or to exclude rows with trips where the fare is more than $20\.
    Analytical queries often include aggregation functions that process a set of values
    across matching rows and compute a single value based on the set. Examples of
    aggregation functions include sum, average (arithmetic mean), minimum, and maximum.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 分析工作负载与事务性工作负载有显著不同。在对数据集执行分析查询时，通常处理数据集中的所有行，例如识别一天中特定小时内的出租车行程或排除费用超过$20的行程的行。分析查询通常包括聚合函数，这些函数在匹配行上处理一组值，并基于该组计算单个值。聚合函数的示例包括求和、平均值（算术平均值）、最小值和最大值。
- en: To perform an analytical query on data in a row-oriented storage format, a processing
    node needs to fetch and operate on a block of rows at a time. For example, consider
    a query that computes the average taxi trip duration for the trips started between
    the hours of 11:00 a.m. and 1:00 p.m. To filter the rows with matching trip times,
    it is necessary to transfer blocks of rows from storage to the processing node,
    despite the fact that most of the data in the block will consist of information
    unrelated to the query, such as the pickup and drop-off coordinates, drop-off
    time, and more.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要对行定向存储格式中的数据执行分析查询，处理节点需要一次获取并操作一块数据行。例如，考虑一个计算在上午11:00至下午1:00之间开始的出租车行程的平均持续时间的查询。为了筛选出具有匹配行程时间的行，需要将行块从存储传输到处理节点，尽管块中的大多数数据都与查询无关，例如上车和下车坐标、下车时间等。
- en: In addition to the unnecessarily long transfer times for the data moved between
    the storage and the node, row-oriented formats waste precious, high-speed cache
    memory in the processor. Since most of the data per row is unusable for performing
    the query, the contents of the caches need to be evicted frequently and unnecessarily
    to replace one block of rows with another.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据在存储和节点之间转移的不必要长时间之外，行定向格式还浪费了处理器中宝贵的高速缓存内存。由于大多数每行的数据对于执行查询是无用的，因此需要经常不必要地将缓存内容驱逐出去，以替换为另一个数据块。
- en: In contrast, column-oriented data formats (right side of figure 2.2) store data
    in columns instead of rows. Most modern data warehouse systems use columnar storage,
    and the format was also adopted by open source projects like Apache Parquet[^(10)](#pgfId-1038861)
    to improve efficiency of analytical workloads.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，列式数据格式（图 2.2 的右侧）将数据存储在列中而不是行中。大多数现代数据仓库系统使用列式存储，此格式也被像 Apache Parquet[^10]
    这样的开源项目采用，以提高分析工作负载的效率。
- en: Consider how the analytical query to find the average taxi trip duration for
    a midday trip would work in the column-oriented format. To filter the matching
    trip times, only the data for the trip start time column needs to be transferred
    to the processing node. Once the trips with matching start times are found, only
    the corresponding entries from the trip duration column need to be fetched to
    compute the average.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑如何在列式格式中执行分析查询以找到中午行程的平均出租车行程持续时间。要筛选匹配的行程时间，只需将行程开始时间列的数据传输到处理节点。一旦找到具有匹配开始时间的行程，只需获取行程持续时间列的相应条目以计算平均值。
- en: In both steps there are significant savings with the amount of data that needs
    to be transferred to the processing node and to its cache. In addition, columnar
    formats support various encoding and compression schemes to convert text data
    to binary to further reduce the amount of storage space occupied by data.[^(11)](#pgfId-1019201)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个步骤中，需要传输到处理节点及其缓存的数据量都会有显著的节省。此外，列式格式支持各种编码和压缩方案，将文本数据转换为二进制，以进一步减少数据占用的存储空间[^11]。
- en: Keep in mind that columnar formats are not designed for transactional workloads.
    The compression and encoding schemes used by formats such as Parquet add latency
    to write operations compared to simple file appends or row-specific changes that
    are possible with row-oriented formats in CSV files or traditional databases.
    If you are planning to adopt Parquet or another columnar format, you need to remember
    that these formats are best suited for immutable data sets. For example, the records
    of DC taxi trip data are not expected to change, making Parquet a great format
    to adopt for more efficient storage and for lower latency analytical queries.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 切记，列式格式并非设计用于事务工作负载。例如 Parquet 等格式所使用的压缩和编码方案相比于简单的文件附加或行特定更改（在 CSV 文件或传统数据库中可能出现的情况）会增加写入操作的延迟。如果你计划采用
    Parquet 或其他列式格式，你需要记住这些格式最适用于不可变数据集。例如，DC 出租车行程数据的记录不太可能会更改，这使得 Parquet 成为更高效的存储和更低延迟分析查询的优秀选择。
- en: 2.4.2 Migrating to a column-oriented data format
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 迁移至基于列的数据格式
- en: As you learned earlier in this chapter, AWS Glue includes the capability to
    create and run data-processing jobs, including jobs that extract-transform-load
    (ETL) data into destination storage for analysis. In this section, you will create
    and use ETL jobs in Glue to transform the original, row-oriented, CSV-based DC
    taxi data set to a column-oriented Parquet format and load the resulting Parquet
    objects to a location in your S3 bucket.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本章早些时候学到的，AWS Glue 包括创建和运行数据处理作业的能力，包括将数据提取转换加载（ETL）到目标存储进行分析的作业。在本节中，您将创建和使用
    Glue 中的 ETL 作业将原始的、面向行的、基于 CSV 的 DC 出租车数据集转换为列式 Parquet 格式，并将结果 Parquet 对象加载到您
    S3 存储桶中的位置。
- en: Glue data-processing jobs can be implemented using the Python programming language.
    Since Glue is serverless, as a machine learning practitioner you will simply need
    to implement the job and submit the job code to the Glue service. The service
    will be responsible for validating your code, ensuring that it can execute, provisioning
    the distributed infrastructure, completing the job using the infrastructure, and
    tearing down the infrastructure once the job is finished.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 Python 编程语言实现 Glue 数据处理作业。由于 Glue 是无服务器的，作为一个机器学习从业者，你只需要实现作业并将作业代码提交到
    Glue 服务。该服务将负责验证您的代码，确保其可执行，为您的作业提供分布式基础设施，使用基础设施完成作业，并在作业完成后拆除基础设施。
- en: An example of a Python-based job to convert the CSV data set to the Parquet
    format and store the converted data as objects in S3 is shown in listing 2.6.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将 CSV 数据集转换为 Parquet 格式并将转换后的数据存储为 S3 对象的基于 Python 的作业示例在清单 2.6 中展示。
- en: The code in the listing, starting from the beginning of the file until ❶, consists
    of standard library imports for the objects and functions needed by the code.
    Code between ❶ and ❷ is the boilerplate heading for an instantiation of a Glue
    job and amounts to initialization of the job based on the runtime arguments passed
    to the job instance.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单中，从文件开头到❶的代码是所需对象和函数的标准库导入。从❶到❷之间的代码是用于实例化 Glue 作业的样板头部，相当于根据作业实例传递的运行时参数对作业进行初始化。
- en: The key steps in the code are annotated with ❸ and ❹. The createOrReplaceTempView
    method used at ❸ modifies the state of the Spark session to declare a temporary
    (nonmaterialized) view named dc_taxi_csv that can be queried using a SQL statement.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的关键步骤带有❸和❹的注释。❸处使用的 createOrReplaceTempView 方法修改了 Spark 会话的状态，声明了一个临时（非物化）视图，名称为
    dc_taxi_csv，可以使用 SQL 语句进行查询。
- en: The method at ❹ executes a SQL query against the dc_taxi_csv view so that the
    job can process the contents of the CSV files from the data set and output a selection
    of columns, while casting the content of the columns into both DOUBLE and STRING
    data types.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 位于❹的方法执行针对 dc_taxi_csv 视图的 SQL 查询，以便作业可以处理数据集中 CSV 文件的内容并输出一些列，同时将列的内容转换为 DOUBLE
    和 STRING 数据类型。
- en: The job commit operation at ❺ simply instructs the job to persist the output
    of the transformation to storage.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 位于❺的作业提交操作仅指示作业将转换的输出持久化到存储中。
- en: Listing 2.6 Saving the code in the listing to a file named “dctaxi_csv_to_parquet.py”
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.6 将清单中的代码保存到名为“dctaxi_csv_to_parquet.py”的文件中
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Import AWS Glue Job to later manage the job life cycle.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 AWS Glue 作业以便稍后管理作业的生命周期。
- en: ❷ Retrieve the JOB_NAME parameter passed to the job.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检索传递给作业的 JOB_NAME 参数。
- en: ❸ Read the CSV files located at BUCKET_SRC_PATH into a Spark DataFrame.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将位于 BUCKET_SRC_PATH 的 CSV 文件读取到 Spark DataFrame 中。
- en: ❹ Eliminate the new lines in the Python multiline string of the SQL query for
    Spark SQL compatibility.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 消除 Spark SQL 兼容性的 Python 多行字符串中的新行。
- en: ❺ Save using Parquet format to the object storage location specified by BUCKET_DST_PATH.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 Parquet 格式保存到由 BUCKET_DST_PATH 指定的对象存储位置。
- en: Note that you need to save the contents of listing 2.6 to a file named dctaxi_csv_
    to_parquet.py. As shown in listing 2.7, you need to upload the job source code
    file to a location in your S3 bucket to ensure that the Glue service can access
    it to start a job.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要将清单 2.6 的内容保存到名为 dctaxi_csv_ to_parquet.py 的文件中。如清单 2.7 所示，您需要将作业源代码文件上传到
    S3 存储桶中的位置，以确保 Glue 服务可以访问它以启动作业。
- en: Listing 2.7 Uploading to glue/dctaxi_csv_to_parquet.py in your project’s bucket
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.7 将代码上传到项目的存储桶中的 glue/dctaxi_csv_to_parquet.py
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Copy the PySpark job file to the Glue folder of the S3 bucket.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 PySpark 作业文件复制到 S3 存储桶的 Glue 文件夹中。
- en: ❷ Confirm that the file uploaded as expected.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确认文件已按预期上传。
- en: 'You should expect an output similar to the following, with a different timestamp
    in the first column:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该期望类似于以下内容的输出，第一列中的时间戳可能不同：
- en: '[PRE22]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After the job file is uploaded, you should create and start the job as shown
    in listing 2.8.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上传作业文件后，应按清单 2.8 中所示创建并启动作业。
- en: Listing 2.8 Creating and starting the dc-taxi-csv-to-parquet-job Glue job
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.8 创建并启动 dc-taxi-csv-to-parquet-job Glue 作业
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To monitor the execution of the job you can use the following command directly,
    or prefix it with a watch command:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要监视作业的执行，可以直接使用以下命令，或者在其前面加上 watch 命令：
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After the job succeeds, you can list the contents of the parquet folder in the
    bucket using
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作业成功后，您可以使用以下命令列出存储桶中 parquet 文件夹的内容
- en: '[PRE25]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: and confirm that the compression caused by the conversion to the Parquet format
    reduced the data size to 940.7 MiB from 11.2 GiB of CSV data stored in the row-oriented
    format.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 并确认由于转换为 Parquet 格式而引起的压缩将数据大小从以行为导向格式存储的 11.2 GiB CSV 数据减少到 940.7 MiB。
- en: 'Then, you can create a new table in the Glue data catalog and have the table
    describe the newly created data stored in the Apache Parquet format. Use the approach
    from listing 2.5, with a few changes, including the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以在 Glue 数据目录中创建一个新表，并让该表描述以 Apache Parquet 格式存储的新创建数据。使用清单 2.5 中的方法，做一些更改，包括以下内容：
- en: Renaming the crawler to dc-taxi-parquet-crawler ❶, ❸, ❹
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将爬虫重命名为 dc-taxi-parquet-crawler❶,❸,❹
- en: Changing the bucket location to use the parquet folder ❷
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将存储桶位置更改为使用 parquet 文件夹❷
- en: Dropping the Exclusions option, since the Parquet-formatted data does not include
    a README file ❷
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除排除选项，因为 Parquet 格式的数据不包括 README 文件❷
- en: '[PRE26]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Create an instance of the dc-taxi-parquet-crawler crawler.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建 `dc-taxi-parquet-crawler` 爬虫实例。
- en: ❷ Crawl the parquet subfolder of the S3 bucket containing the converted data
    set.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 爬取包含转换后数据集的 S3 存储桶的 `parquet` 子文件夹。
- en: ❸ Start the crawler.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 启动爬虫。
- en: ❹ Get the current crawler state.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取当前爬虫状态。
- en: You can confirm that the transformation of the data from CSV to Parquet resulted
    in a new Glue table. If you execute
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以确认从 CSV 到 Parquet 的数据转换是否生成了新的 Glue 表。如果你执行
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: then the output should be similar to the one from when you ran the aws glue
    get-table command against the dc_taxi_csv table, with the exception of the change
    in the value for the Parameters.classification key. The value should change from
    csv to parquet.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，输出应该类似于当你针对 `dc_taxi_csv` 表运行 `aws glue get-table` 命令时的结果，唯一的区别是 Parameters.classification
    键的值发生了变化。该值应该从 csv 变为 parquet。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: A machine learning approach to building a taxi fare estimation service can help
    you reduce operational costs and avoid hardcoding city-specific business rules.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习方法建立出租车费估算服务可以帮助你降低运营成本，并避免硬编码特定城市的商业规则。
- en: You will use a publicly available data set of taxi trips in DC to learn how
    to build a taxi fare estimation API using serverless machine learning.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将使用一个公开的华盛顿特区出租车行程数据集来学习如何使用无服务器机器学习构建出租车费估算 API。
- en: Serverless object storage services (such as S3) help you take what you already
    know about managing data as files on filesystems and apply that to storing and
    managing large data sets (gigabytes to petabytes of data) as objects in object
    storage.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无服务器对象存储服务（如 S3）可以帮助你将管理文件系统上的文件数据的知识应用于将大型数据集（从千兆字节到拍字节的数据）作为对象存储在对象存储中。
- en: AWS Glue data crawlers help you discover the schema of your data regardless
    of whether your data is in filesystems, object storage, or relational databases.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Glue 数据爬虫可以帮助你发现数据的模式，无论你的数据在文件系统、对象存储还是关系数据库中。
- en: AWS Glue extract-transform-load job services help you move data across various
    storage locations, transforming the data in the process to prepare it for analysis.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Glue 提取-转换-加载（ETL）作业服务可以帮助你在不同存储位置之间移动数据，并在过程中对数据进行转换，为分析做好准备。
- en: The column-oriented data format improves data-processing efficiency for analytical
    queries.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列式数据格式可以提高分析查询的数据处理效率。
- en: '^(1.)January 2018 archive.org snapshot of the taxi fares: [http://mng.bz/6m0G](http://mng.bz/6m0G).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)2018年1月 archive.org 的出租车费快照：[http://mng.bz/6m0G](http://mng.bz/6m0G)。
- en: ^(2.) The schema data types are illustrated using the ANSI SQL format.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.) 模式数据类型使用 ANSI SQL 格式来说明。
- en: ^(3.) The timestamp is stored as a string using the month/day/year hour:minute
    format.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.) 时间戳以月/日/年 小时:分钟的格式存储为字符串。
- en: '^(4.)Catalog of DC taxi rides from 2015 through 2019: [http://mng.bz/o8nN](http://mng.bz/o8nN).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.) 2015年至2019年期间华盛顿特区出租车行程目录：[http://mng.bz/o8nN](http://mng.bz/o8nN)。
- en: '^(5.)Catalog of DC taxi rides from 2015 through 2019: [http://mng.bz/o8nN](http://mng.bz/o8nN).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)2015年至2019年期间华盛顿特区出租车行程目录：[http://mng.bz/o8nN](http://mng.bz/o8nN)。
- en: '^(6.)A redundant array of independent disks is used to ensure data redundancy
    in server hardware: [http:// mng.bz/v4ax](http://mng.bz/v4ax).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ^(6.) 采用独立磁盘冗余阵列来确保服务器硬件中的数据冗余：[http:// mng.bz/v4ax](http://mng.bz/v4ax)。
- en: ^(7.)Amazon Resource Name (ARN) is an AWS-specific, globally unique identifier
    for resources, including user IDs and accounts in AWS. You can learn more about
    ARNs from [http://mng.bz/4KGB](http://mng.bz/4KGB).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^(7.)Amazon 资源名称（ARN）是 AWS 专用的、全球唯一的资源标识符，包括 AWS 中的用户 ID 和账户。你可以从 [http://mng.bz/4KGB](http://mng.bz/4KGB)
    了解更多关于 ARN 的信息。
- en: ^(8.)AWS Glue user interface is available from [https://console.aws.amazon.com/glue](https://console.aws.amazon.com/glue).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^(8.)AWS Glue 用户界面可从 [https://console.aws.amazon.com/glue](https://console.aws.amazon.com/glue)
    访问。
- en: ^(9.)AWS Glue user interface is available from [https://console.aws.amazon.com/glue](https://console.aws.amazon.com/glue).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ^(9.)AWS Glue 用户界面可从 [https://console.aws.amazon.com/glue](https://console.aws.amazon.com/glue)
    访问。
- en: ^(10.)Apache Parquet is an open source columnar data storage format developed
    as a collaboration between Twitter and Cloudera and maintained by the Apache Software
    Foundation project. You can learn more about the format from [https://github.com/apache/parquet-mr](https://github.com/apache/parquet-mr).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ^(10.)Apache Parquet 是一种开放源代码的列式数据存储格式，由 Twitter 和 Cloudera 合作开发，由 Apache 软件基金会项目维护。你可以从
    [https://github.com/apache/parquet-mr](https://github.com/apache/parquet-mr) 了解更多关于该格式的信息。
- en: ^(11.)Examples of encoding and compression schemes used by Apache Parquet columnar
    storage format are available from [http://mng.bz/yJpJ](http://mng.bz/yJpJ).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet 列式存储格式使用的编码和压缩方案示例可从[http://mng.bz/yJpJ](http://mng.bz/yJpJ)获取。
