- en: B.1\. Tensor creation and tensor axis conventions
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1\. 张量的创建和张量轴约定
- en: 'Remember that a *tensor* is simply a data container. Every tensor has two fundamental
    properties: data type (dtype) and shape. *dtype* controls what kinds of values
    are stored within the tensor. A given tensor can store only one kind of value.
    At the time of this writing (version 0.13.5), the supported dtypes are float32,
    int32, and bool.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，*张量*只是一个数据容器。每个张量都有两个基本属性：数据类型（dtype）和形状。*dtype* 控制张量中存储的值的类型。给定张量只能存储一种类型的值。截至本文撰写时（版本
    0.13.5），支持的 dtype 为 float32、int32 和 bool。
- en: The *shape* is an array of integers indicating how many elements are in the
    tensor and how they are organized. It can be thought of as the “shape and size”
    of the container that is the tensor (see [figure B.1](#app02fig01)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*形状*是一个整数数组，指示张量中有多少个元素以及它们是如何组织的。可以将其视为张量的“形状和大小”，即张量作为容器的形状（参见 [图 B.1](#app02fig01)）。'
- en: Figure B.1\. Examples of tensors of rank 0, 1, 2, 3, and 4
  id: totrans-3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 B.1\. 秩为 0、1、2、3 和 4 的张量示例
- en: '![](btab01_alt.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](btab01_alt.jpg)'
- en: The length of the shape is known as the tensor’s *rank*. For example, a 1D tensor,
    also known as a *vector*, has rank 1\. The shape of a 1D tensor is an array containing
    one number, and that number tells us how long the 1D tensor is. Increasing rank
    by one, we get a 2D tensor, which can be visualized as a grid of numbers in a
    2D plane (like a grayscale image). The shape of a 2D tensor has two numbers, which
    tell us how tall and how wide the grid is. Further increasing the rank by one,
    we get a 3D tensor. As shown in the example in [figure B.1](#app02fig01), you
    can visualize a 3D tensor as a 3D grid of numbers. The shape of a 3D tensor consists
    of three integers; they tell us the size of the 3D grid along the three dimensions.
    So, you see the pattern. Tensors of rank 4 (4D tensors) are harder to visualize
    directly because the world we live in has only three spatial dimensions. 4D tensors
    are frequently used in many models, such as deep convnets. TensorFlow.js supports
    tensors up to rank 6\. In practice, rank-5 tensors are used only in some niche
    cases (for example, those involving video data), while rank-6 tensors are encountered
    even more rarely.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 形状的长度被称为张量的*秩*。例如，1D 张量，也称为*向量*，秩为 1。1D 张量的形状是一个包含一个数字的数组，这个数字告诉我们 1D 张量的长度。将秩增加一，我们得到一个
    2D 张量，可以将其可视化为 2D 平面上的数字网格（如灰度图像）。2D 张量的形状有两个数字，告诉我们网格的高度和宽度。再增加一秩，我们得到一个 3D 张量。如
    [图 B.1](#app02fig01) 中的示例所示，你可以将 3D 张量可视化为 3D 数字网格。3D 张量的形状由三个整数组成；它们告诉我们沿着三个维度的
    3D 网格的大小。所以，你看到了规律。秩为 4 的张量（4D 张量）更难以直接可视化，因为我们生活的世界只有三个空间维度。4D 张量经常在许多模型中使用，例如深度卷积网络。TensorFlow.js
    支持秩高达 6 的张量。在实践中，秩为 5 的张量仅在某些小众情况下使用（例如涉及视频数据的情况），而秩为 6 的张量甚至更少见。
- en: B.1.1\. Scalar (rank-0 tensor)
  id: totrans-6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.1\. 标量（秩为 0 的张量）
- en: 'A scalar is a tensor whose shape is an empty array (`[]`). It has no axes and
    always contains exactly one value. You can create a new scalar using the `tf.scalar()`
    function. At the JavaScript console (again, assuming TensorFlow.js is loaded and
    available at the `tf` symbol), do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 标量是形状为空数组(`[]`)的张量。它没有轴，始终只包含一个值。可以使用 `tf.scalar()` 函数创建一个新的标量。在 JavaScript
    控制台（假设已加载 TensorFlow.js 并在 `tf` 符号处可用）中执行以下操作：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ¹
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that for space and clarity, we will skip the JavaScript console output
    lines that result from assignments, as they are not illustrative to the issue
    at hand.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请注意，出于空间和清晰起见，我们将跳过由于赋值而产生的 JavaScript 控制台输出行，因为它们对所讨论的问题没有说明性。
- en: 'We have created a scalar tensor holding just the value 2018\. Its shape is
    the empty list, as expected. It has the default dtype (`"float32"`). To force
    the dtype to be an integer, provide `''int32''` as an additional argument when
    calling `tf.scalar()`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已创建一个标量张量，其中仅包含值 2018。其形状是空列表，正如预期的那样。它具有默认的 dtype (`"float32"`)。要将 dtype
    强制为整数，请在调用 `tf.scalar()` 时提供 `'int32'` 作为额外参数：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To get the data back out of the tensor, we can use the async method `data()`.
    The method is async because, in general, the tensor may be hosted out of the main
    memory, such as on GPUs, as a WebGL texture. Retrieving the value of such tensors
    involves operations that are not guaranteed to resolve immediately, and we don’t
    want those operations to block the main JavaScript thread. This is why the `data()`
    method is async. There is also a synchronous function that retrieves the values
    of tensors through polling: `dataSync()`. This method is convenient but blocks
    the main JavaScript thread, so it should be used sparingly (for example, during
    debugging). Prefer the async `data()` method whenever possible:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要从张量中获取数据，我们可以使用异步方法`data()`。该方法是异步的，因为一般来说，张量可能被托管在主内存之外，例如在GPU上，作为WebGL纹理。检索这些张量的值涉及到不一定能立即解决的操作，我们不希望这些操作阻塞主JavaScript线程。这就是为什么`data()`方法是异步的。还有一个同步函数通过轮询检索张量的值：`dataSync()`。这种方法很方便，但会阻塞主JavaScript线程，所以应该尽量少用（例如，在调试期间）。尽量使用异步的`data()`方法：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To use `dataSync()`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`dataSync()`：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We see that for float32-type tensors, the `data()` and `dataSync()` methods
    return the values as a JavaScript `Float32Array` primitive. This may be a little
    surprising if you expected a plain old number, but it makes more sense when considering
    that tensors of other shapes may need to return a container of multiple numbers.
    For int32-type and bool-type tensors, `data()` and `dataSync()` return `Int32Array`
    and `Uint8Array`, respectively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，对于float32类型的张量，`data()`和`dataSync()`方法将值作为JavaScript的`Float32Array`原始值返回。如果你期望的是一个普通的数字，这可能有点令人惊讶，但是当考虑到其他形状的张量可能需要返回包含多个数字的容器时，这就更合理了。对于int32类型和bool类型的张量，`data()`和`dataSync()`分别返回`Int32Array`和`Uint8Array`。
- en: Note that even though a scalar always contains exactly one element, the converse
    is not true. A tensor whose rank is greater than 0 may have exactly one element
    as well, as long as the product of the numbers in its shape is 1\. For example,
    a 2D tensor of shape `[1, 1]` has only one element, but it has two axes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使标量始终包含一个元素，反之则不成立。张量的秩大于0的张量也可以只有一个元素，只要其形状中的数字乘积为1即可。例如，形状为`[1, 1]`的2D张量只有一个元素，但是它有两个轴。
- en: B.1.2\. tensor1d (rank-1 tensor)
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.2\. tensor1d（秩-1张量）
- en: 'A 1D tensor is sometimes called a rank-1 tensor or a vector. A 1D tensor has
    exactly one axis, and its shape is a length-1 array. The following code will create
    a vector at the console:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 1D张量有时被称为秩-1张量或向量。1D张量恰好有一个轴，其形状是长度为1的数组。下面的代码将在控制台创建一个向量：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This 1D tensor has four elements and can be called a 4-dimensional vector. Don’t
    confuse a 4D *vector* with a 4D *tensor*! A 4D vector is a 1D tensor that has
    one axis and contains exactly four values, whereas a 4D tensor has four axes (and
    may have any number of dimensions along each axis). Dimensionality can denote
    either the number of elements along a specific axis (as in our 4D vector) or the
    number of axes in a tensor (for example, a 4D tensor), which can be confusing
    at times. It’s technically more correct and less ambiguous to refer to a rank-4
    tensor, but the ambiguous notation 4D tensor is common regardless. In most cases,
    this shouldn’t be a problem, as it can be disambiguated based on the context.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个1D张量有四个元素，可以称为4维向量。不要混淆4D*向量*和4D*张量*！4D向量是一个只有一个轴并且包含确切四个值的1D张量，而4D张量有四个轴（并且每个轴上可以有任意数量的维度）。维度可以表示沿着特定轴的元素数量（如我们的4D向量）或张量中的轴数（例如，4D张量），这有时可能会令人困惑。在技术上，更正确和不含糊的是指一个秩-4张量，但是无论如何都常见到模糊的表示法4D张量。在大多数情况下，这不应该是个问题，因为它可以根据上下文来消除歧义。
- en: As in the case of scalar tensors, you can use the `data()` and `dataSync()`
    methods to access the values of the 1D tensor’s elements; for example,
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与标量张量一样，您可以使用`data()`和`dataSync()`方法来访问1D张量元素的值；例如，
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Alternatively, you can use the synchronous version of `data()`—namely, `dataSync()`—but
    be aware that `dataSync()` may block the UI thread and should be avoided if possible:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`data()`的同步版本，即`dataSync()`，但要注意，如果可能，应该避免使用`dataSync()`会阻塞UI线程：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to access the value of a specific element of the 1D tensor, you can
    simply index into the TypedArray returned by `data()` or `dataSync()`; for example,
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问1D张量的特定元素的值，您可以简单地索引到`data()`或`dataSync()`返回的TypedArray；例如，
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: B.1.3\. tensor2d (rank-2 tensor)
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.3\. tensor2d（秩-2张量）
- en: A 2D tensor has two axes. In some cases, a 2D tensor is referred to as a *matrix*,
    and its two axes can be interpreted as the row and column indices of the matrix,
    respectively. You can visually interpret a matrix as a rectangular grid of elements
    (see the third panel of [figure B.1](#app02fig01)). In TensorFlow.js,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二维张量有两个轴。在某些情况下，一个二维张量被称为*矩阵*，它的两个轴可以被解释为矩阵的行和列索引，分别。您可以将矩阵视为元素的矩形网格（参见[图
    B.1 的第三面板](#app02fig01)）。在 TensorFlow.js 中，
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The entries from the first axis are called the *rows*, and the entries from
    the second axis are the *columns*. In the previous example, `[1, 2, 3]` is the
    first row, and `[1, 40]` is the first column. It is important to know that when
    returning the data, using `data()` or `dataSync()`, the data will come as a flat
    array in *row-major* order. In other words, the elements of the first row will
    appear in the `Float32Array` first, followed by elements of the second row, and
    so forth:^([[2](#app02fn2)])
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个轴的条目称为*行*，第二个轴的条目称为*列*。在前面的例子中，`[1, 2, 3]` 是第一行，`[1, 40]` 是第一列。重要的是要知道，当使用
    `data()` 或 `dataSync()` 返回数据时，数据将以*行优先*的方式作为扁平数组返回。换句话说，第一行的元素将首先出现在 `Float32Array`
    中，然后是第二行的元素，依此类推：^([[2](#app02fn2)])
- en: ²
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is different from the column-major ordering seen in some other numerical
    frameworks such as MATLAB and R.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这与一些其他数值框架（如 MATLAB 和 R）中看到的列优先排序不同。
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Previously, we mentioned that the `data()` and `dataSync()` methods, when followed
    by indexing, can be used to access the value of any element of a 1D tensor. When
    used on 2D tensors, the indexing operation becomes tedious because the TypedArray
    returned by `data()` and `dataSync()` *flattens* the elements of the 2D tensor.
    For instance, in order to determine the element of the TypedArray that corresponds
    to the element in the second row and second column of the 2D tensor, you’d have
    to perform arithmetic like the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们提到 `data()` 和 `dataSync()` 方法，当跟随索引时，可以用于访问一维张量的任何元素的值。当用于二维张量时，索引操作变得繁琐，因为
    `data()` 和 `dataSync()` 返回的 TypedArray 会*扁平化*二维张量的元素。例如，为了确定与二维张量中第二行第二列的元素对应的
    TypedArray 元素，您必须执行如下算术：
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Fortunately, TensorFlow.js provides another set of methods to download values
    from tensors into plain JavaScript data structures: `array()` and `arraySync()`.
    Unlike `data()` and `dataSync()`, these methods return nested JavaScript arrays
    that properly preserve the rank and shape of the original tensors. For example,'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，TensorFlow.js提供了另一组方法，用于将张量的值下载到普通的JavaScript数据结构中：`array()` 和 `arraySync()`。与
    `data()` 和 `dataSync()` 不同，这些方法返回正确保留原始张量秩和形状的嵌套JavaScript数组。例如，
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To access an element at the second row and second column, we can simply perform
    indexing into the nested array twice:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问第二行第二列的元素，我们只需对嵌套数组进行两次索引：
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This gets rid of the need to perform index arithmetic and will be especially
    convenient for higher-dimensional tensors. `arraySync()` is the synchronous version
    of `array()`. Like `dataSync()`, `arraySync()` may block the UI thread and should
    be used with caution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这消除了执行索引算术的需要，并且对于更高维度的张量特别方便。`arraySync()` 是 `array()` 的同步版本。与 `dataSync()`
    类似，`arraySync()` 可能会阻塞 UI 线程，应谨慎使用。
- en: 'In the `tf.tensor2d()` call, we provided a nested JavaScript array as the argument.
    The argument consists of rows of arrays nested within another array. This nesting
    structure is used by `tf.tensor2d()` to infer the shape of the 2D tensor—that
    is, how many rows and how many columns there are, respectively. An alternative
    way to create the same 2D tensor with `tf.tensor2d()` is to provide the elements
    as a flat (non-nested) JavaScript array and accompany it by a second argument
    that specifies the shape of the 2D tensor:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `tf.tensor2d()` 调用中，我们提供了一个嵌套的JavaScript数组作为参数。参数由嵌套在另一个数组中的数组行组成。这种嵌套结构由
    `tf.tensor2d()` 用于推断二维张量的形状——即有多少行和多少列，分别。使用 `tf.tensor2d()` 创建相同的二维张量的另一种方法是提供元素作为平面（非嵌套）JavaScript数组，并伴随一个第二个参数，指定二维张量的形状：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this approach, the product of all the numbers in the `shape` argument must
    match the number of elements in the float array, or else an error will be thrown
    during the `tf.tensor2d()` call. For tensors of ranks higher than 2, there are
    also two analogous approaches to tensor creation: using either a single nested
    array as the argument or a flat array accompanied by a shape argument. You will
    see both approaches used in different examples throughout this book.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，`shape` 参数中所有数字的乘积必须与浮点数组中的元素数相匹配，否则在 `tf.tensor2d()` 调用期间将抛出错误。对于秩高于
    2 的张量，创建张量还有两种类似的方法：使用一个嵌套数组作为参数，或者使用一个带有形状参数的平坦数组。在本书的不同示例中，您会看到这两种方法都被使用。
- en: B.1.4\. Rank-3 and higher-dimensional tensors
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.4\. 秩为 3 及更高维度的张量
- en: 'If you pack several 2D tensors into a new array, you will get a 3D tensor,
    which you can imagine as a cube of elements (the fourth panel in [figure B.1](#app02fig01)).
    Rank-3 tensors can be created in TensorFlow.js following the same pattern as previously:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将几个 2D 张量打包到一个新数组中，您将获得一个 3D 张量，您可以将其想象为元素的立方体（[图 B.1](#app02fig01) 中的第四个面板）。在
    TensorFlow.js 中，可以按照以前的模式创建秩为 3 的张量：
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Another way to do the same thing is to provide a flat (non-nested) array of
    values, together with an explicit shape:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种执行相同操作的方法是提供一个扁平（非嵌套）值数组，以及一个显式形状：
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `tf.tensor3d()` function in this example can be replaced with the more
    generic `tf.tensor()` function. This allows you to generate tensors of any rank
    up to 6\. In the following, we create a rank-3 and a rank-6 tensor:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`tf.tensor3d()` 函数可以被更通用的 `tf.tensor()` 函数替代。这允许你生成任何秩（rank）的张量，最高可达到
    6。在下面的示例中，我们创建了一个秩为 3 和一个秩为 6 的张量：
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: B.1.5\. The notion of data batches
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.5\. 数据批次的概念
- en: In practice, the first axis (axis 0, because indexing starts at 0) in all tensors
    you’ll come across in deep learning will almost always be the *batch axis* (sometimes
    called the *samples axis* or *batch dimension*). Therefore, an actual tensor taken
    by a model as input has a rank that exceeds the rank of an individual input feature
    by 1\. This is true throughout the TensorFlow.js models in this book. The size
    of the first dimension equals the number of examples in the batch, known as *batch
    size*. For instance, in the iris-flower-classification example in [chapter 3](kindle_split_014.html#ch03)
    ([listing 3.9](kindle_split_014.html#ch03ex09)), the input feature of every example
    consists of four numbers represented as a length-4 vector (a 1D tensor of shape
    `[4]`). Hence the input to the iris-classification model is 2D and has a shape
    `[null, 4]`, where the first null value indicates a batch size that will be determined
    at the model’s runtime (see [figure B.2](#app02fig02)). This batching convention
    also applies to the output of models. For example, the iris-classification model
    outputs a one-hot encoding for the three possible types of iris for every individual
    input example, which is a 1D tensor of shape `[3]`. However, the model’s actual
    output shape is 2D and has a shape of `[null, 3]`, where the null-valued first
    dimension is the to-be-determined batch size.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，在深度学习中你将遇到的所有张量中，第一个轴（轴 0，因为索引从 0 开始）几乎总是*批处理轴*（有时称为*样本轴*或*批处理维度*）。因此，一个模型作为输入获取的实际张量的秩比单个输入特征的秩高
    1。这一点贯穿于本书中所有 TensorFlow.js 模型。第一个维度的大小等于批次中的示例数，称为*批处理大小*。例如，在[第 3 章](kindle_split_014.html#ch03)（[清单
    3.9](kindle_split_014.html#ch03ex09)）中的鸢尾花分类示例中，每个示例的输入特征由表示为长度为 4 的向量的四个数字组成（形状为
    `[4]` 的 1D 张量）。因此，鸢尾花分类模型的输入是 2D 的，形状为 `[null, 4]`，其中第一个 null 值表示模型运行时将确定的批处理大小（见[图
    B.2](#app02fig02)）。这种批处理约定也适用于模型的输出。例如，鸢尾花分类模型为每个个别输入示例输出一个三种可能的鸢尾花类型的独热编码，这是一个形状为
    `[3]` 的 1D 张量。但是，模型的实际输出形状是 2D 的，形状为 `[null, 3]`，其中第一个 null 值是待确定的批处理大小。
- en: Figure B.2\. Tensor shapes for individual examples (left) and batched examples
    (right). The tensor for batched examples has a rank one greater than the tensor
    for an individual example and is the format accepted by the `predict()`, `fit()`,
    and `evaluate()` methods of `tf.Model` objects. The `null` in the shape of the
    tensor for batch examples indicates that the first dimension of the tensor has
    an undetermined size, which can be every positive integer during actual calls
    to the aforementioned methods.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 B.2\. 单个示例（左）和批示例（右）的张量形状。批示例的张量比单个示例的张量的秩高一级，并且是`tf.Model`对象的`predict()`、`fit()`和`evaluate()`方法所接受的格式。批示例张量的形状中的`null`表示张量的第一维具有一个未确定的大小，在对上述方法进行实际调用时可以是任何正整数。
- en: '![](btab02_alt.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](btab02_alt.jpg)'
- en: B.1.6\. Real-world examples of tensors
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.6\. 张量的真实例子
- en: 'Let’s make tensors more concrete with a few examples similar to what you’ll
    encounter in the book. The data you’ll manipulate will almost always fall into
    one of the following categories. In the previous discussion, we follow the batching
    convention and always included the number of examples in the batch (`numExamples`)
    as the first axis:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几个与本书中遇到的类似的例子使张量更加具体化。你将操作的数据几乎总是属于以下类别之一。在前面的讨论中，我们遵循批处理约定，并始终将批次中的示例数（`numExamples`）作为第一个轴包括进去：
- en: '*Vector data*—2D tensors with shape `[numExamples, features]`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量数据*—形状为`[numExamples, features]`的2D张量'
- en: '*Time-series (sequence) data*—3D tensors with shape `[numExamples, timesteps,
    features]`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间序列（序列）数据*—形状为`[numExamples, timesteps, features]`的3D张量'
- en: '*Images*—4D tensors with shape `[numExamples, height, width, channels]`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像*—形状为`[numExamples, height, width, channels]`的4D张量'
- en: '*Video*—5D tensors with shape `[numExamples, frame, height, width, channels]`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视频*—形状为`[numExamples, frame, height, width, channels]`的5D张量'
- en: Vector data
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向量数据
- en: This is the most common case. In such a dataset, each single data sample can
    be encoded as a vector, and thus a batch of data will be encoded as a rank-2 tensor,
    where the first axis is the samples axis, and the second axis is the features
    axis.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常见的情况。在这样的数据集中，每个单独的数据样本可以被编码为一个向量，因此数据的批将被编码为一个秩为2的张量，其中第一个轴是样本轴，第二个轴是特征轴。
- en: 'Let’s take a look at two examples:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看两个例子：
- en: An actuarial dataset of people, in which we consider each person’s age, ZIP
    code, and income. Each person can be characterized as a vector of 3 values, and
    thus an entire dataset of 100,000 people can be stored in a 2D tensor with shape
    `[100000, 3]`.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人口数据集，其中我们考虑每个人的年龄、邮政编码和收入。每个人可以被描述为一个包含3个值的向量，因此一个包含10万人的完整数据集可以存储在形状为`[100000,
    3]`的2D张量中。
- en: A dataset of text documents, where we represent each document by the counts
    of how many times each word appears in it (for example, out of an English dictionary
    of the 20,000 most common words). Each document can be encoded as a vector of
    20,000 values (one count per word in the dictionary), and thus a batch of 500
    documents can be stored in a tensor of shape `[500, 20000]`.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文本文档数据集，其中我们通过每个单词出现的次数来表示每个文档（例如，从包含20000个最常见单词的英语词典中）。每个文档可以被编码为一个包含20000个值的向量（词典中每个单词的计数），因此500个文档的批可以存储在形状为`[500,
    20000]`的张量中。
- en: Time-series or sequence data
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 时间序列或序列数据
- en: Whenever time matters in your data (or the notion of sequence order), it makes
    sense to store it in a 3D tensor with an explicit time axis. Each sample is encoded
    as a sequence of vectors (a 2D tensor), and thus a batch of samples will be encoded
    as a 3D tensor (see [figure B.3](#app02fig03)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每当数据中涉及时间（或者序列顺序的概念）时，将其存储在具有显式时间轴的3D张量中是有意义的。每个样本被编码为一系列向量（一个2D张量），因此样本批将被编码为3D张量（参见[图
    B.3](#app02fig03)）。
- en: Figure B.3\. A 3D time-series data tensor
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 B.3\. 一个3D时间序列数据张量
- en: '![](btab03.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](btab03.jpg)'
- en: 'The time axis is almost always the second axis (axis of index 1) by convention,
    as in the following examples:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎总是按照惯例，时间轴是第二个轴（索引为1的轴），如下例所示：
- en: A dataset of stock prices. Every minute we store the current price of the stock,
    the highest price in the past minute, and the lowest price in the past minute.
    Thus, each minute is encoded as a vector of three values. Since there are 60 minutes
    in an hour, an hour of trading is encoded as a 2D tensor of shape `[60, 3]`. If
    we have a dataset of 250 independent hours of sequences, the shape of the dataset
    will be `[250, 60, 3].`
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个股票价格数据集。每分钟我们存储股票的当前价格，过去一分钟内的最高价格和最低价格。因此，每分钟被编码为一个三个值的向量。由于一个小时有 60 分钟，一小时的交易被编码为一个形状为
    `[60, 3]` 的二维张量。如果我们有一个包含 250 个独立小时序列的数据集，数据集的形状将是 `[250, 60, 3]`。
- en: A dataset of tweets in which we encode each tweet as a sequence of 280 characters
    out of an alphabet of 128 unique characters. In this setting, each character can
    be encoded as a binary vector of size 128 (all zeros except a 1 entry at the index
    corresponding to the character). Then each can be considered as a rank-2 tensor
    of shape `[280, 128]`. A dataset of 1 million tweets can be stored in a tensor
    of shape `[1000000, 280, 128]`.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个推文数据集，其中我们将每个推文编码为由 128 个唯一字符组成的 280 个字符序列。在这种设置中，每个字符都可以编码为大小为 128 的二进制向量（除了在对应字符的索引处有一个
    1 的条目外，全部为零）。然后，每个字符可以被视为一个形状为 `[280, 128]` 的二阶张量。一个包含 100 万条推文的数据集可以存储在一个形状为
    `[1000000, 280, 128]` 的张量中。
- en: Image data
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像数据
- en: 'The data of an image typically has three dimensions: height, width, and color
    depth. Although grayscale images have only a single color channel, by convention,
    image tensors are always rank 3, with a 1-dimensional color channel for grayscale
    images. A batch of 128 grayscale images of size 256 × 256 would thus be stored
    in a tensor of shape `[128, 256, 256, 1]`, and a batch of 128 color images would
    be stored in a tensor of shape `[128, 256, 256, 3]` (see [figure B.4](#app02fig04)).
    This is called the NHWC convention (see [chapter 4](kindle_split_015.html#ch04)
    for more details).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的数据通常具有三个维度：高度、宽度和颜色深度。尽管灰度图像只有一个颜色通道，但按照惯例，图像张量始终是三阶的，对于灰度图像有一个一维的颜色通道。因此，一个大小为
    256 × 256 的 128 个灰度图像批次将存储在一个形状为 `[128, 256, 256, 1]` 的张量中，而一个包含 128 个彩色图像的批次将存储在一个形状为
    `[128, 256, 256, 3]` 的张量中（参见[图 B.4](#app02fig04)）。这称为 NHWC 约定（有关更多详情，请参见[第四章](kindle_split_015.html#ch04)）。
- en: Figure B.4\. A 4D image data tensor
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 B.4\. 一个 4D 图像数据张量
- en: '![](btab04.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](btab04.jpg)'
- en: Some frameworks put the channels dimension before the height and width, using
    the NCHW convention. We don’t use this convention in this book, but don’t be surprised
    to see an image tensor of a shape such as `[128, 3, 256, 256]` elsewhere.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一些框架在高度和宽度之前放置通道维度，使用 NCHW 约定。在本书中，我们不使用这个约定，但在其他地方看到形状如 `[128, 3, 256, 256]`
    的图像张量也不要感到惊讶。
- en: Video data
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 视频数据
- en: Raw video data is one of the few types of common real-world data for which you’ll
    need rank-5 tensors. A video can be understood as a sequence of frames, each frame
    being a color image. Since each frame can be stored in a rank-3 tensor `[height,
    width, colorChannel]`, a sequence of frames can be stored in a 4D tensor `[frames,
    height, width, colorChannel]`, and thus a batch of different videos would be stored
    in a 5D tensor of shape `[samples, frames, height, width, colorChannel]`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 原始视频数据是少数几种常见的真实世界数据之一，你需要使用到五维张量。视频可以理解为一系列帧，每一帧都是一幅彩色图像。由于每一帧可以存储在一个三维张量 `[height,
    width, colorChannel]` 中，一系列帧可以存储在一个四维张量 `[frames, height, width, colorChannel]`
    中，因此一批不同的视频将存储在一个五维张量中，形状为 `[samples, frames, height, width, colorChannel]`。
- en: For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames
    per second would have 240 frames. A batch of four such video clips would be stored
    in a tensor of shape `[4, 240, 144, 256, 3]`. That’s a total of 106,168,320 values!
    If the dtype of the tensor were `'float32'`, then each value would be stored in
    32 bits, so the tensor would represent 405 MB. This is a heavy amount of data!
    Videos you encounter in real life are much lighter because they aren’t stored
    in float32, and they’re typically compressed by a large factor (such as in the
    MPEG format).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个 60 秒长，144 × 256 分辨率的 YouTube 视频剪辑，每秒采样 4 帧，将有 240 帧。四个这样的视频剪辑批次将存储在一个形状为
    `[4, 240, 144, 256, 3]` 的张量中。总共有 106,168,320 个值！如果张量的数据类型为 `'float32'`，那么每个值将以
    32 位存储，因此张量将表示 405 MB。这是大量的数据！你在现实生活中遇到的视频要轻得多，因为它们不是以 float32 存储，并且通常被大幅压缩（例如
    MPEG 格式）。
- en: B.1.7\. Creating tensors from tensor buffers
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.7\. 从张量缓冲区创建张量
- en: 'We’ve shown how to create tensors from JavaScript arrays using functions such
    as `tf.tensor2d()` and `tf.tensor()`. To do so, you must have determined the values
    of all the elements and set them in the JavaScript arrays beforehand. In some
    cases, however, it is somewhat tedious to create such a JavaScript array from
    scratch. For instance, suppose you want to create a 5 × 5 matrix in which all
    the off-diagonal elements are zero, and the diagonal elements form an increasing
    series that equals the row or column index plus 1:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何使用诸如 `tf.tensor2d()` 和 `tf.tensor()` 等函数从 JavaScript 数组创建张量。为此，您必须先确定所有元素的值，并在之前的
    JavaScript 数组中设置它们。但是，在某些情况下，从头开始创建这样一个 JavaScript 数组有点繁琐。例如，假设您要创建一个 5 × 5 矩阵，其中所有的非对角线元素都为零，而对角线元素形成一个递增序列，等于行或列索引加
    1：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you were to create a nested JavaScript array to meet this requirement, the
    code would look something like the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要创建一个满足此要求的嵌套 JavaScript 数组，代码将如下所示：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, you can convert the nested JavaScript array `matrixArray` into a 2D
    tensor:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以把嵌套的 JavaScript 数组 `matrixArray` 转换成一个二维张量：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This code looks a little tedious. It involves two nested `for` loops. Is there
    a way to simplify it? The answer is yes: we can use the `tf.tensorBuffer()` method
    to create a `TensorBuffer`. A `TensorBuffer` object allows you to specify its
    elements by indices and change their values by using the `set()` method. This
    is different from a tensor object in TensorFlow.js, whose element values are *immutable*.
    When you have finished setting the values of all the elements of a `TensorBuffer`
    you wish to set, the `TensorBuffer` can be conveniently converted to an actual
    tensor object through its `toTensor()` method. Hence, if we use `tf.tensorBuffer()`
    to achieve the same tensor-creation task as the previous code, the new code will
    look like'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码看起来有点繁琐。它涉及两个嵌套的 `for` 循环。有没有简化它的方法？答案是有：我们可以使用 `tf.tensorBuffer()` 方法创建一个
    `TensorBuffer`。`TensorBuffer` 对象允许您通过索引指定其元素，并使用 `set()` 方法更改其值。这与 TensorFlow.js
    中的张量对象不同，后者的元素值是*不可变的*。当您完成设置 `TensorBuffer` 的所有元素的值后，可以通过其 `toTensor()` 方法方便地将
    `TensorBuffer` 转换为实际的张量对象。因此，如果我们使用 `tf.tensorBuffer()` 来实现与上述代码相同的张量创建任务，新代码将如下所示
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1*** Specifies the tensor shape when creating a TensorBuffer. A TensorBuffer
    has all-zero values after creation.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 创建 TensorBuffer 时指定张量形状。创建后，TensorBuffer 的所有值都为零。'
- en: '***2*** The first arg is the desired values, while the remaining args are the
    indices of the element to be set.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 第一个参数是所需的值，而其余的参数是要设置的元素的索引。'
- en: '***3*** Gets an actual tensor object from the TensorBuffer'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 从 TensorBuffer 获取实际的张量对象'
- en: Therefore, by using `tf.tensorBuffer()`, we reduced the lines of code from 10
    to 5.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用 `tf.tensorBuffer()`，我们将代码行数从 10 减少到 5。
- en: B.1.8\. Creating all-zero and all-one tensors
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.8\. 创建全零和全一张量
- en: It is often desirable to create a tensor of a given shape with all elements
    equal to zero. You can use the `tf.zeros()` function to achieve this. To call
    the function, provide the desired shape as the input argument; for example,
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通常希望创建一个给定形状的全零张量。你可以使用 `tf.zeros()` 函数来实现这一点。调用该函数时，将期望的形状作为输入参数提供；例如，
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The tensor created has the default dtype (float32). To create all-zero tensors
    of other dtypes, specify the dtype as the second argument to `tf.zeros()`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的张量具有默认的 dtype（float32）。要创建其他 dtype 的全零张量，请将 dtype 指定为 `tf.zeros()` 的第二个参数。
- en: A related function is `tf.zerosLike()`, which lets you create an all-zero tensor
    of the same shape and dtype as an existing tensor. For example,
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的函数是 `tf.zerosLike()`，它让您可以创建一个与现有张量具有相同形状和 dtype 的全零张量。例如，
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: is equivalent to
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 等同于
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: but is more succinct.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 但更简洁。
- en: 'Analogous methods allow you to create tensors of which all elements are equal
    to one: `tf.ones()` and `tf.onesLike()`.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的方法允许你创建所有元素都相等的张量：`tf.ones()` 和 `tf.onesLike()`。
- en: B.1.9\. Creating randomly valued tensors
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.9\. 创建随机值张量
- en: 'Creating randomly valued tensors is useful in many cases, such as the initialization
    of weights. The most frequently used functions for creating randomly valued tensors
    are `tf.randomNormal()` and `tf.randomUniform()`. The two functions have similar
    syntax but lead to different distributions in element values. As its name suggests,
    `tf.randomNormal()` returns tensors in which the element values follow a normal
    (Gaussian) distribution.^([[3](#app02fn3)]) If you invoke the function with only
    a shape argument, you will get a tensor whose elements follow the *unit* normal
    distribution: a normal distribution with mean = 0 and standard deviation (SD)
    = 1\. For example,'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 创建具有随机值的张量在许多情况下都很有用，比如权重的初始化。创建具有随机值张量最常用的函数是 `tf.randomNormal()` 和 `tf.randomUniform()`。这两个函数具有类似的语法，但导致元素值的分布不同。顾名思义，`tf.randomNormal()`
    返回的张量中的元素值遵循正态（高斯）分布。如果你只用一个形状参数调用该函数，你会得到一个元素遵循 *单位* 正态分布的张量：均值 = 0，标准差（SD）=
    1。例如，
- en: ³
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For readers familiar with statistics, the element values are independent from
    each other.
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于熟悉统计学的读者，元素值彼此独立。
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you want the normal distribution to have a no-default mean or SD, you may
    provide them as the second and third input arguments, respectively. For instance,
    the following call creates a tensor in which the elements follow a normal distribution
    of mean = -20 and SD = 0.6:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望正态分布有一个非默认的均值或标准差，你可以将它们作为第二和第三个输入参数提供给 `tf.randomUniform()`。例如，以下调用创建了一个元素值遵循均值
    = -20，标准差 = 0.6 的正态分布的张量：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`tf.randomUniform()` lets you create random tensors with uniformly distributed
    element values. By default, the uniform distribution is a unit one—that is, with
    lower bound 0 and upper bound 1:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.randomUniform()` 允许你创建具有均匀分布元素值的随机张量。默认情况下，均匀分布是单位分布，即下界为 0，上界为 1：'
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: If you want to let the element value follow a non-unit uniform distribution,
    you can specify the lower and upper bounds as the second and third arguments to
    `tf.randomUniform()`, respectively. For example,
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让元素值遵循非单位均匀分布，你可以将下界和上界指定为 `tf.randomUniform()` 的第二和第三个参数，例如，
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'creates a tensor with values randomly distributed in the `[-10, 10)` interval:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个值在 `[-10, 10)` 区间内随机分布的张量：
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`tf.randomUniform()` can be used to create randomly valued int32-type tensors.
    This is useful for cases in which you want to generate random labels. For example,
    the following code creates a length-10 vector in which the values are randomly
    drawn from the integers 0 through 100 (the interval `[0, 100)`):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.randomUniform()` 可用于创建具有随机值的 int32 类型张量。这在你想要生成随机标签的情况下非常有用。例如，以下代码创建了一个长度为
    10 的向量，其中的值随机抽取自整数 0 到 100（区间 `[0, 100)`）：'
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that the `'int32'` argument is the key in this example. Without it, the
    tensor you get will contain float32 values instead of int32 ones.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个例子中 `'int32'` 参数是关键。没有它，你得到的张量将包含 float32 值而不是 int32 值。
