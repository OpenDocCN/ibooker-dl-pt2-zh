- en: '6 Prompt Engineering: Optimizing Your Generative AI Experience'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 提示工程：优化您的生成式 AI 体验
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: What is prompt engineering
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是提示工程
- en: Prompt engineering best practices
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程最佳实践
- en: Zero-shot and few shot prompting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零样本和少样本提示
- en: Prompting LLMs for historical time series data sets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为历史时间序列数据集提示 LLMs
- en: I’ll bet that, before actually opening this book, many - perhaps most - of you
    expected *prompt engineering* to be a primary focus. And yet, here we are in chapter
    6 (*half way through the book!*) and we’re only just hitting the topic? What’s
    the story here?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我打赌，在真正打开这本书之前，许多 - 也许大多数 - 人都期望*提示工程*成为主要焦点。然而，我们现在已经到了第六章（*书已经过半！*）了，我们才刚刚涉及到这个话题？这是怎么回事？
- en: In my defence, I’d say that it’s partly about what we mean when we use the term.
    For some, "prompt engineering" covers a lot of what you’ll figure out on your
    own by just having fun experimenting with ChatGPT or MidJourney. It matters, but
    it doesn’t require a whole book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我的辩护，我会说这在一定程度上取决于我们使用该术语的含义。对于一些人来说，“提示工程”包含了你通过与 ChatGPT 或 MidJourney 玩得开心来自己摸索出来的许多内容。这很重要，但不需要一整本书。
- en: But I’d also argue that what I’ve given you so far - and what’s yet to come
    the remaining chapters - goes far beyond prompts. Sure, the phrasing you use is
    important, but the API and programmatic tools we’re discovering will take your
    prompts a lot further.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但我也会认为，到目前为止我给你的东西 - 以及剩下的章节 - 远远超出了提示。当然，你使用的措辞很重要，但我们正在发现的 API 和编程工具将大大拓展你的提示。
- en: There’s one more thing going on. In my experience, as GPT and other generative
    AI models improve, they’re getting better at figuring out what you want even when
    you provide a weak prompt. I can’t count the number of times that GPT has successfully
    seen right through my spelling and grammar errors, poor wording, or sometimes
    even outright technical mistakes. So, many of the problems that popular "prompt
    engineering" advice seeks to prevent, are already easily handled by the AI itself.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一件事。根据我的经验，随着 GPT 和其他生成式 AI 模型的改进，它们越来越擅长在你提供一个弱提示时找出你想要的东西。我数不清有多少次 GPT
    成功地看穿了我的拼写和语法错误，措辞不当，甚至有时甚至是明显的技术错误。因此，许多流行的“提示工程”建议试图防止的问题，AI 本身已经可以轻松处理。
- en: Still, no one wants to look like an idiot - even if the only one who can see
    is a robot. And you can never know when even an AI won’t be able to figure out
    what you’re really after. So I’ll devote this entire chapter to the fine art of
    engineering your prompts. We’ll begin with a helpful definition.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，没有人想看起来像个白痴 - 即使唯一能看到的只是一个机器人。而且你永远不知道，甚至 AI 都无法弄清楚你真正想要什么。所以我会把整个章节都献给精心设计你的提示的艺术。我们将从一个有用的定义开始。
- en: 6.1 What is prompt engineering?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 什么是提示工程？
- en: Prompt engineering is a technique used in the context of language models like
    GPT to effectively guide the model’s responses and improve its performance. It
    involves crafting specific instructions or queries, known as prompts, to encourage
    (or even force) a desired output from the model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 GPT 这样的语言模型的背景下使用的提示工程是一种有效引导模型响应并提高其性能的技术。它涉及制定特定的指令或查询，称为提示，以鼓励（甚至强制）模型产生所需的输出。
- en: Prompt engineering can be used to shape the behavior of the model by providing
    it with explicit instructions, context, or constraints. By carefully constructing
    prompts, researchers and developers can influence the model’s output and make
    it more consistent, accurate, or better aligned with specific criteria.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程可以通过向模型提供明确的指令、上下文或约束来塑造模型的行为。通过精心构建提示，研究人员和开发人员可以影响模型的输出，并使其更一致、准确或与特定标准更好地对齐。
- en: 'There are various strategies for prompt engineering, depending on what you’re
    trying to accomplish. These strategies may involve:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你想要实现的目标，有各种各样的提示工程策略。这些策略可能涉及：
- en: Asking the model to assume a particular role ("You are an expert investment
    counsellor")
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求模型扮演特定角色（“你是一位专业的投资顾问”）
- en: Specifying the format you’d like for the model’s response ("Give me the response
    in .CSV format")
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定模型响应的格式（“给我一个 .CSV 格式的响应”）
- en: Asking the model to think step-by-step ("Can you walk me through the process
    of installing software package X, step by step?")
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求模型逐步思考（“你能逐步向我介绍安装软件包 X 的过程吗？”）
- en: Providing additional context or background information ("This assumes that the
    value for the variable `my_number` is 10")
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供额外的上下文或背景信息（“这假定变量`my_number`的值是10”）。
- en: Using system messages (like error messages) to guide the model’s behavior
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用系统消息（如错误消息）来指导模型的行为。
- en: One common technique is to use "prompt engineering by demonstration," where
    developers manually generate desired model outputs for a set of example inputs.
    The model is then fine-tuned based on this data, allowing it to generalize and
    produce similar responses for future inputs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的技巧是使用“通过示范进行提示工程”，即开发者手动为一组示例输入生成期望的模型输出。然后根据这些数据对模型进行微调，使其能够泛化，并为将来的输入产生类似的响应。
- en: It’s important to keep in mind that prompt engineering is an *iterative process*.
    That means that you don’t always expect to get the completion you’re after on
    the first try. Instead, you’ll experiment, analyse the model’s initial behavior,
    and refine subsequent prompts based on feedback and evaluation. The process of
    gradual iteration lets you leverage the power of language models while maintaining
    control over the output and ensuring it aligns with your intentions and requirements.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，提示工程是一个*迭代的过程*。这意味着你不能指望在第一次尝试时就得到想要的结果。相反，你需要进行实验，分析模型的初始行为，并根据反馈和评估来完善后续的提示。渐进式迭代的过程让你可以在保持输出控制的同时，利用语言模型的强大功能，确保其与您的意图和要求一致。
- en: For one example, you might want to use iterative prompting when your LLM gives
    you programming code that doesn’t work. Rather than starting over by asking the
    same question a second time, you could copy and paste any error message you received
    and ask how that could be avoided.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，如果你的LLM返回的编程代码不能正常工作，你可能需要使用迭代提示，而不是重复提出同样的问题。你可以复制并粘贴收到的任何错误消息，并询问如何避免这些错误。
- en: 6.2 Prompt engineering best practices
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 提示工程最佳实践
- en: These suggestions are based on guidance found in [official OpenAI documentation](articles.html).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些建议基于[官方OpenAI文档](articles.html)中找到的指导原则。
- en: 6.2.1 Be specific
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 要具体
- en: Be generous with the details and descriptions you include in your prompt.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中要慷慨地提供详细信息和描述。
- en: Tell me about quantum mechanics
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我关于量子力学的事情。
- en: 'Won’t be as effective as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做可能不如这样：
- en: Explain quantum mechanics in 200 words or less and in terms that can be understood
    by a 12 year old
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在200字以内并且用12岁儿童可以理解的术语解释量子力学。
- en: Similarly,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，
- en: Compose an email to my boss asking for a raise
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 给我的老板写一封电子邮件，要求加薪。
- en: 'Will be a lot less likely to end happily than something like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不如使用类似这样的提示更可能产生良好的结果：
- en: Compose a polite but forceful email to my boss explaining how my hard work and
    successfully executed, on-time projects have earned the company new clients
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 给我的老板写一封礼貌但坚定的电子邮件，解释我辛勤工作和按时完成的项目如何为公司赢得了新客户。
- en: 6.2.2 Be clear
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 要清晰
- en: 'AIs like clarity just as much as people do. The more obvious and unambiguous
    you can make your prompt, the less chance there is that you’ll get something unexpected.
    This example isn’t necessarily bad:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能和人类一样喜欢清晰明了的提示。提示越明确，得到意外结果的可能性就越小。这个例子不一定是坏的：
- en: Considering the data provided, list all the key topics, arguments, and people
    that are referenced
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供的数据，列举所有提到的关键主题、论点和人名。
- en: 'But you’re far more likely to achieve success the first time around using something
    like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但你更有可能在第一次就获得成功，使用类似下面这样的提示：
- en: Considering the data provided, list all the key topics that are referenced,
    then the arguments that are presented, and finally each of the people who are
    mentioned.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供的数据，列举所有提到的关键主题，然后是呈现的论点，最后是提到的每个人。
- en: 'Desired format:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的格式：
- en: 'Topics: <list_divided _by_commas>'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 主题：<用逗号分隔的列表>
- en: 'Arguments:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 论点：
- en: 'People:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 人物：
- en: 6.2.3 Avoid unnecessary words
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 避免不必要的词语
- en: 'There’s a higher risk of misunderstanding and poor completion results when
    you use this kind of overly verbose prompt:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种过于冗长的提示会增加误解和产生不佳结果的风险：
- en: I’m looking for a fairly long and completely detailed description of each of
    the ten most popular passenger cars of the early 1970s (by domestic US sales).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我在寻找关于1970年代早期最受欢迎的十款乘用车（按美国国内销售量）的相当长且详细的描述。
- en: 'Instead, try something like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 反而试试这样：
- en: List and describe each of the ten highest selling cars in the US during the
    early 1970s.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列举并描述1970年代早期美国销售量最高的十款汽车。
- en: When I ran both of those examples - the "extra verbose" and "sleek and punchy"
    versions - past ChatGPT, the results I got were both equally impressive. So I’d
    say this is a good illustration of GPT improvements I mentioned earlier. It’s
    also an indication of the growing irrelevance of the topic of prompt engineering
    as a whole.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我将这两个示例 - “额外冗长”和“简洁有力”的版本 - 都传递给ChatGPT时，我得到的结果都同样令人印象深刻。所以我认为这是我之前提到的GPT改进的很好说明。这也表明了提示工程这个话题作为一个整体的日益无关紧要。
- en: 6.2.4 Separate reference text from your instructions
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 将参考文本与您的说明分开。
- en: 'You should make it clear where your instructions end and any reference text
    you’re including begins. This example might not work:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该明确说明您的说明的结束位置以及任何您包含的参考文本的开始位置。这个例子可能行不通：
- en: Who was the author of:q We can only see a short distance ahead, but we can see
    plenty there that needs to be done.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文本的作者是谁：我们只能看到前方的一小段距离，但我们可以看到有许多事情需要做。
- en: 'But this probably will (note the use of triple quotations - although I’m not
    sure that they’re still as important as they once were):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但这可能会（注意使用了三重引号 - 虽然我不确定它们是否像以前那样重要）。
- en: 'Who was the author of the following text:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文本的作者是谁：
- en: 'Text: """We can only see a short distance ahead, but we can see plenty there
    that needs to be done."""'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 文本：“““我们只能看到前方的一小段距离，但我们可以看到有许多事情需要做。”””
- en: By the way, as I’m sure you’re curious, the author was Alan Turing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，我相信你一定很好奇，这段文字的作者是阿兰·图灵。
- en: 6.2.5 Be positive, not negative
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.5 要积极，而不是消极。
- en: 'This works in personal relationships, too. But right now we’re more concerned
    with the way you get along with your favorite generative AI. It seems that GPT
    and its cousins can sometimes get confused by negatives like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这在个人关系中也适用。但现在我们更关心您与您喜爱的生成式AI的相处方式。看起来GPT及其兄弟姐妹有时会因为这样的否定句而感到困惑：
- en: When responding to the following request for help from a customer who is locked
    out of their account, DO NOT suggest they update their password.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当回应来自被锁定帐户的客户的以下请求寻求帮助时，请勿建议他们更新密码。
- en: 'Rephrasing the prompt as a positive might make a difference:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重新将提示转述为积极的可能会有所不同：
- en: When responding to the following request for help from a customer who is locked
    out of their account, instead of suggesting they update their password, refer
    them to available online documentation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当回应来自被锁定帐户的客户的以下请求寻求帮助时，请不要建议他们更新密码，而是引导他们查阅可用的在线文档。
- en: There are a few other LLM training methods that can be applied in the specific
    context of prompts to improve the quality of your completions. We’ll look at those
    next.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的LLM训练方法，可以应用在提升完成质量的具体提示上。我们接下来会看看这些。
- en: Takeaway
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 总结
- en: When composing your prompts, remember to be specific, clear, concise, and positive,
    and to clearly demarcate you reference text.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写提示时，请记住要具体、清晰、简明，并且是积极的，并清晰地划分您的参考文本。
- en: 6.2.6 Control for temperature (randomness)
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.6 控制温度（随机性）
- en: 'You can directly incorporate temperature within a prompt:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接在提示中嵌入温度：
- en: Generate a creative and unique story beginning with the sentence "It was a dark
    and stormy night." Use a temperature setting of 0.8 to add some randomness and
    creativity to the story.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个以句子“这是一个黑暗而多雨的夜晚。”开始的创意独特的故事。使用温度设置为0.8以增加故事的一些随机性和创造力。
- en: In this example, the temperature setting of 0.8 indicates that the generated
    response will have a moderate level of randomness and creativity. The higher the
    temperature, the more varied and unpredictable the output will be. You can adjust
    the temperature value to control the amount of randomness in the generated text.
    A higher value like 0.8 will result in more diverse and imaginative responses,
    while a lower value like 0.2 will produce more focused and deterministic responses.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，温度设置为0.8表示生成的响应将具有适度的随机性和创造力。温度值越高，输出的变化和不可预测性就越多。您可以调整温度值来控制生成文本中随机性的数量。像0.8这样较高的值将产生更多样化和富有想象力的响应，而像0.2这样较低的值将产生更集中和确定性的响应。
- en: 6.3 Zero-shot and few shot prompting
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 零射和少射提示
- en: Zero-shot and few-shot prompting are techniques used in natural language processing
    (NLP) to generate responses or perform tasks without explicit training on specific
    examples or with only a limited amount of training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 零射和少射提示是自然语言处理（NLP）中用于生成响应或执行任务的技术，而无需对特定示例进行显式训练或仅使用有限数量的训练数据。
- en: Zero-shot prompting refers to the ability of a model to provide meaningful responses
    or perform tasks for which it has not been explicitly trained. The model is capable
    of generalizing from its training data to understand and respond to new inputs
    or tasks. This is achieved by using prompts or instructions that guide the model’s
    behavior. For example, if a language model has been trained on a variety of topics,
    it can still generate coherent responses on a new topic by providing a prompt
    that specifies the desired topic.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本提示是指模型能够为其尚未明确训练的任务提供有意义的响应或执行任务的能力。该模型能够从其训练数据中泛化以理解和响应新的输入或任务。通过使用提示或指令来引导模型的行为来实现这一点。例如，如果语言模型已经在各种主题上进行了训练，那么它仍然可以通过提供指定所需主题的提示来对新主题生成连贯的响应。
- en: Few-shot prompting, on the other hand, involves training a model with only a
    small amount of labeled data or examples. By leveraging this limited training
    data, the model is expected to learn how to generalize and perform tasks on unseen
    or novel examples. This approach is useful when the availability of labeled data
    is scarce or when adapting a model to new tasks quickly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，少样本提示涉及使用仅有的少量标记数据或示例对模型进行训练。通过利用这些有限的训练数据，期望模型学会如何泛化并在未见过或新颖的示例上执行任务。当标记数据的可用性稀缺或需要快速调整模型以适应新任务时，这种方法就很有用。
- en: Both zero-shot and few-shot prompting leverage the pre-training and fine-tuning
    methodology. In pre-training, a model is trained on a large corpus of text data
    to learn general language patterns and representations. Fine-tuning follows, where
    the pre-trained model is further trained on specific tasks or domains using limited
    labeled data or prompts. This combination enables the model to exhibit adaptability
    and generate meaningful responses or perform tasks in a zero-shot or few-shot
    manner.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本和少样本提示都利用了预训练和微调的方法论。在预训练阶段，模型在大型文本语料库上进行训练，以学习通用语言模式和表示。接着是微调，将预训练的模型进一步训练到特定任务或领域，使用有限的标记数据或提示。这种组合使模型能够表现出适应性，并以零样本或少样本的方式生成有意义的响应或执行任务。
- en: 'These techniques have proven to be powerful in various NLP tasks, such as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术已被证明在各种自然语言处理任务中非常强大，比如：
- en: Text classification
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Question answering
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: Summarization
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Language translation
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Text generation
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: They allow models to demonstrate a degree of understanding and perform adequately
    on new or unseen inputs, even without extensive training on specific examples.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 它们使模型能够展示一定程度的理解，并且即使没有在特定示例上进行广泛训练，也能够在新的或未见过的输入上表现良好。
- en: Here’s an example of zero-shot prompting.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个零样本提示的示例。
- en: 'Let’s say you have a language model that has been trained on a variety of topics
    but hasn’t been explicitly trained on the topic of space exploration. Using zero-shot
    prompting, you can still generate coherent responses on space-related questions.
    For example, you could provide the following prompt:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个语言模型，已经在各种主题上进行了训练，但尚未明确在太空探索主题上进行过训练。使用零样本提示，您仍然可以生成关于太空的问题的连贯响应。例如，您可以提供以下提示：
- en: What are the key challenges and achievements in space exploration?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 太空探索中的关键挑战和成就是什么？
- en: The model, even without specific training on space exploration, can generate
    a response by drawing upon its general knowledge and understanding of the topic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有针对太空探索进行特定训练，该模型也可以通过利用其对该主题的一般知识和理解生成响应。
- en: And here’s an example of few-shot prompting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个少样本提示的示例。
- en: Suppose you have a model that has been pre-trained on a large corpus of text
    but hasn’t been fine-tuned for sentiment analysis. However, with few-shot prompting,
    you can train the model on a small labeled dataset containing a few examples of
    positive and negative sentiments. The model can then generalize from this limited
    training data and perform sentiment analysis on new, unseen text.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个在大型文本语料库上进行了预训练但尚未进行情感分析微调的模型。但是，通过少样本提示，您可以在包含一些正面和负面情感示例的小型标记数据集上对模型进行训练。然后，该模型可以从这些有限的训练数据中泛化，并对新的、未见过的文本进行情感分析。
- en: Here’s a more generic example of how a few-shot prompt might look. We would
    first train the model using these prompt/completion examples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更通用的示例，展示了少样本提示可能的形式。我们首先会使用这些提示/完成示例来训练模型。
- en: 'English sentence: "I love to travel."'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 英文句子："I love to travel."
- en: 'French translation: "J’adore voyager."'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 法语翻译："J’adore voyager."
- en: 'English sentence: "The cat is sleeping."'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 英文句子："The cat is sleeping."
- en: 'French translation: "Le chat dort."'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 法语翻译："Le chat dort."
- en: 'English sentence: "Where is the nearest train station?"'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 英文句子："Where is the nearest train station?"
- en: 'French translation: "Où se trouve la gare la plus proche?"'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 法语翻译："Où se trouve la gare la plus proche?"
- en: 'At this point, you’ve "trained" the model to anticipate the kind of result
    you want. You’re now ready to submit an actual prompt:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你已经“训练”了模型以预测你想要的结果类型。现在你已经准备好提交一个实际的提示了：
- en: English sentence "Can you recommend a good restaurant?"
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 英文句子："Can you recommend a good restaurant?"
- en: 'French translation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 法语翻译：
- en: Both zero-shot and few-shot prompting leverage the model’s ability to generalize
    from its pre-training and make predictions or perform tasks on new inputs or tasks,
    either with minimal or no specific training. They’re fundamental tools used by
    AI engineers when they design their LLMs, but the same basic principles can also
    be used for our own day-to-day AI interactions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本和少样本提示都利用了模型从其预训练中概括的能力，并对新的输入或任务进行预测或执行任务，无论是最少还是没有具体训练。当AI工程师设计他们的LLMs时，它们是基本的工具，但相同的基本原则也可以用于我们自己的日常AI交互。
- en: '6.4 Prompt for time series data: a practical example'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 时间序列数据的提示：一个实际示例
- en: When you know someone who happens to have perfect command of the entire internet,
    creating new value from your relationship is often just a matter of being creative
    enough to ask the right questions. As one does, I recently had an irrepressible
    urge to visualize historical improvements in server hardware components. Has capacity
    growth been consistent over the years? Has capacity grown at similar rates for
    all component categories?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当你认识一个碰巧完全掌握整个互联网的人时，从你们关系中创造新价值通常只是要足够有创意地提出正确的问题。正如一个人所做的，我最近有一种难以抑制的冲动，想要可视化服务器硬件组件的历史改进。容量增长是否在多年来保持一致？所有组件类别的容量增长速度是否相似？
- en: But where would I find the data? My curiosity wasn’t irrepressible enough to
    justify scouring archived versions of vendor websites for hours on end. Could
    my smart "friend" (by which I mean GPT or one of its LLM cousins) help me out
    here?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我应该在哪里找到数据呢？我的好奇心不足以证明为了这个目的连续几个小时搜索供应商网站的存档版本是合理的。我的聪明“朋友”（我的意思是GPT或它的LLM堂兄弟之一）能帮我吗？
- en: 'No reason not to try. Here’s my prompt:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由不尝试。这是我的提示：
- en: 'Give me the basic specifications for out-of-the-box, top-of-the-line rack-mount
    servers from each year between 1994 and 2021\. Show me the output in CSV format
    using the following columns: Year, clock speed (GHz), Maximum RAM (GB), Total
    Drive Capacity (GB)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 给我每年1994年至2021年的开箱即用、顶级机架式服务器的基本规格。使用以下列显示输出：年份，时钟速度（GHz），最大内存（GB），总存储容量（GB）
- en: Since I expected to load the output into a Python program, I figured I’d save
    myself some work and ask for the data in comma-separated values (CSV) format using
    exactly the column headers I preferred. I tried this out using both ChatGPT and
    [Perplexity Labs' LLM server](labs.perplexity.ai.html). To my astonishment, GPT
    gave me nicely formatted CSV data that at least looked realistic. For some reason,
    Perplexity interpreted "CSV" as "Markdown", but fixing that wasn’t a big deal.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我期望将输出加载到Python程序中，我想省点事，直接要求以逗号分隔的值（CSV）格式提供数据，使用我喜欢的确切列标题。我尝试过使用ChatGPT和[Perplexity
    Labs的LLM服务器](labs.perplexity.ai.html)。令我惊讶的是，GPT给了我看起来至少是合理的格式化CSV数据。出于某种原因，Perplexity将"CSV"解释为"Markdown"，但修复这个问题并不是什么大问题。
- en: The data itself (along with the code used in the following examples) are available
    [as part of the book’s GitHub repo](main.html).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据本身（以及以下示例中使用的代码）可在[该书的GitHub存储库的一部分](main.html)中找到。
- en: 6.4.1 Visualizing the Data
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 数据可视化
- en: Of course, properly visualizing my data will be essential for both assessing
    whether the output makes sense and, if it does, what insights it might give me.
    But, as I’ll show you, the *way* you visualize this data will determine how well
    you’ll understand it. Let me explain that by showing you how to generate charts
    using both normalized and non-normalized data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，正确地可视化我的数据对于评估输出是否合理以及如果合理的话可能给我带来什么见解至关重要。但是，正如我将向您展示的，您可视化这些数据的方式将决定您对其的理解程度。让我通过向您展示如何使用归一化和非归一化数据生成图表来解释一下。
- en: Normalization refers to the process of adjusting data values to a common scale
    or standard, typically to facilitate meaningful comparisons between different
    data points. It’s a common technique used in data visualization and analysis to
    remove the influence of varying scales or units in the data, making it easier
    to identify patterns and trends. In our case, that’s important because the *scale*
    of the units used to measure CPU clock speeds (GHz) is very different from the
    units used to measure memory (GB) and storage (also GB).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化指的是将数据值调整到一个共同的比例或标准，通常是为了方便不同数据点之间的有意义比较。这是数据可视化和分析中常用的技术，可以消除数据中不同比例或单位的影响，从而更容易识别出模式和趋势。在我们的案例中，这很重要，因为用于测量
    CPU 时钟频率（GHz）的单位的*规模*与用于测量内存（GB）和存储（也是 GB）的单位非常不同。
- en: Normalization helps ensure that the relative relationships and variations within
    the data are preserved while removing the influence of different scales. This
    is especially useful when comparing data from different sources or when visualizing
    data on the same graph with different units or scales.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化有助于确保保留数据中的相对关系和变化程度，同时消除不同比例的影响。这在比较来自不同来源的数据或在同一图表上可视化具有不同单位或比例的数据时特别有用。
- en: '**Min-Max Scaling**, also known as **Min-Max Normalization**, is a data normalization
    method used to transform data into a specific range, typically between 0 and 1\.
    The purpose of Min-Max Scaling is to standardize data values, making them more
    comparable and suitable for various data analysis and machine learning techniques.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小最大缩放**，也被称为**最小最大标准化**，是一种数据标准化方法，用于将数据转化为特定范围，通常在0到1之间。最小最大缩放的目的是标准化数据的值，使其更具可比性，并适用于各种数据分析和机器学习技术。'
- en: 'Here’s how Min-Max Scaling works:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最小最大缩放的工作原理如下：
- en: Find the minimum (min) and maximum (max) values within the dataset for the feature
    you want to normalize.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到要标准化的特征数据集中的最小值（min）和最大值（max）。
- en: 'For each data point in that feature, apply the following formula to scale it
    to the range [0, 1]:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于该特征中的每个数据点，应用以下公式将其缩放到范围[0,1]：
- en: Scaled Value = (Original Value - Min) / (Max - Min)
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放值 = （原始值 - 最小值）/（最大值 - 最小值）
- en: The resulting "Scaled Value" for each data point will fall within the range
    of 0 to 1, where 0 represents the minimum value in the dataset, and 1 represents
    the maximum value.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据点的结果“缩放值”将落在0到1的范围内，其中0表示数据集中的最小值，1表示最大值。
- en: Min-Max Scaling is particularly useful when you want to preserve the relationships
    and proportions between data points while ensuring that all values are on a consistent
    scale. It’s widely used in various data analysis and machine learning tasks, especially
    when algorithms like neural networks, k-means clustering, or support vector machines
    are sensitive to the scale of input features.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最小最大缩放在想要保留数据点之间的关系和比例同时确保所有值都在一致比例上时特别有用。它广泛应用于各种数据分析和机器学习任务中，特别是在神经网络、k-means
    聚类或支持向量机等算法对输入特征的比例敏感时。
- en: To illustrate, suppose you have a dataset that represents the employee salaries
    ranging from $40,000 to $100,000\. You also have data representing the number
    of years of experience each employee has - ranging from 2 to 20 years. You want
    to standardize these values using Min-Max Scaling.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，假设您有一个数据集，代表了员工的薪水范围从40,000美元到100,000美元。您还有代表每个员工的工作年限的数据 - 范围从2到20年。您想使用最小最大缩放对这些值进行标准化。
- en: Without Min-Max Scaling, the salary values could be in the range of 40,000 to
    100,000, while the years of experience could be in the range of 2 to 20\. The
    data points for salary and years of experience would be on very different scales.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 没有最小最大缩放，薪水值可能在40,000到100,000的范围内，而工作经验可能在2到20的范围内。薪水和工作经验的数据点将处于非常不同的比例上。
- en: Now, if you apply Min-Max Scaling to both features, you might scale the values
    to a range of 0 to 1\. So, a salary of $50,000 could be scaled to 0.25, and 10
    years of experience might be scaled to 0.5.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您将最小最大缩放应用于这两个特征，您可能会将值缩放到0到1的范围内。因此，50,000美元的薪水可能缩放为0.25，而10年的工作经验可能缩放为0.5。
- en: Min-Max Scaling is the tool we’ll use here. But first, I’ll show you what we
    get using non-normalized data for a graph so you’ll see with your own eyes why
    normalization can be helpful.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将在此处使用的工具。但首先，我将向您展示使用非标准化数据进行绘图后得到的结果，这样您就可以亲眼看到为什么标准化是有帮助的。
- en: 6.4.2 Graphing the time series data without normalization
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 在不进行标准化的情况下绘制时间序列数据
- en: Listing [6.1](#listing-XYZ-6-1). shows the complete code for producing a non-normalized
    graph of our data. Look through it yourself and try to understand what’s happening,
    and then we’ll work through it one section at a time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [6.1](#listing-XYZ-6-1). 展示了生成我们数据的非标准化图形的完整代码。自己仔细看一下，试着理解正在发生的事情，然后我们将逐个部分地进行解释。
- en: Listing 6.1 Visualizing a time series without normalization
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.1 可视化时间序列而不进行归一化
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s walk through that code. As always, we begin by importing the libraries
    we’ll need. Pandas will handle the data itself, and matplotlib will help us with
    the graphs. We’ll then import the CSV data file into a data frame.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解该代码。像往常一样，我们首先导入所需的库。Pandas 将处理数据本身，matplotlib 将帮助我们制作图形。然后我们将 CSV 数据文件导入到数据框中。
- en: '[PRE1]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I’ll define the dimensions of the graph (or *figure* as it’s more commonly described)
    that I’ll eventually generate. By all means, experiment with alternate values
    to see what changes. But these should be perfectly workable.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我将定义图形的尺寸（或者*图*，因为通常是这样描述的），最终我会生成。尽管可以尝试使用替代值来进行实验，看看会有什么变化。但这些应该是完全可行的。
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We’ll use the NumPy tool to take each of the columns and convert it to a NumPy
    array - which is a data format that’s usable for plotting our graph/figure. We’ll
    give the array created from the data in each column a name. The column `df_all['Year']`
    for instance, will be called `years`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 NumPy 工具来获取每列并将其转换为 NumPy 数组 - 这是一种可用于绘制图形/图的数据格式。我们将从每列数据创建的数组命名。例如，列
    `df_all['Year']` 将被称为 `years`。
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since `years` will be used as our x-axis, I’ll now plot the other three NumPy
    arrays. Each of those arrays will be associated with the `years` values and given
    a display label like: `label=''Clock speed (GHz)''`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `years` 将被用作我们的 x 轴，现在我将绘制其他三个 NumPy 数组。每个数组都将与 `years` 值关联，并被赋予一个显示标签，如：`label='时钟速度
    (GHz)'`。
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To make the graph more readable, we’ll add labels to both the x and y axes,
    and give the figure itself a title. We’ll also add a color-coded legend so we’ll
    be able to quickly understand which plot line represents which column of data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使图形更易读，我们将在 x 轴和 y 轴上添加标签，并给图形本身加上一个标题。我们还将添加一个彩色编码的图例，这样我们就能迅速理解哪条图线代表哪列数据。
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we’ll pull the trigger and generate the figure itself:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将触发并生成图形本身：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And here’s how it’ll all come out:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的全部内容：
- en: Figure 6.1 Our hardware component data visualized using non-normalized data
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.1 使用非标准化数据可视化的硬件组件数据
- en: '![gai 6 1](images/gai-6-1.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![gai 6 1](images/gai-6-1.png)'
- en: Besides the fact that it won’t be easy for those of you reading this in a physical
    book to distinguish between the color-coded plot lines, there’s another, more
    serious, problem here. A casual glance would lead us to conclude that processor
    clock speeds haven’t improved at all in the years since 1994\. But that’s ridiculous.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在纸质书籍中阅读本书的人很难区分彩色编码的图线外，这里还有另一个更严重的问题。粗略一看，我们会得出结论，自 1994 年以来处理器时钟速度根本没有改进。但这是荒谬的。
- en: The data itself shows average clock speeds going from 1 GHz to 11 GHz over that
    time. That last number (11) is weird. I’m aware of no processor on earth that
    can run at 11 GHz. I suspect that the Perplexity LLM is accounting for the spread
    of multi-core systems and simply adding the maximum speeds of each parallel core
    that might be installed on a system. But in any case, why does that growth not
    show up in our plot line?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数据本身显示的是在那段时间内平均时钟速度从 1 GHz 增加到 11 GHz。最后那个数字 (11) 很奇怪。我不知道地球上有哪款处理器能以 11 GHz
    运行。我怀疑 Perplexity LLM 正在考虑多核系统的扩展，并简单地将每个可能安装在系统上的并行核心的最大速度相加。但无论如何，为什么这种增长没有显示在我们的图线上呢？
- en: The answer is that the difference between 1 GHz and 11 GHz isn’t anything like
    the difference between 32 GB and 6,144,960 GB. It’s all about *scale*. To fix
    *that* we’ll need to normalize our data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是 1 GHz 和 11 GHz 之间的差异与 32 GB 和 6,144,960 GB 之间的差异完全不同。这一切都与*规模*有关。要解决*这个*问题，我们需要对数据进行归一化处理。
- en: 6.4.3 Graphing the time series data with normalization
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 对时间序列数据进行归一化处理的图形化
- en: You’ll immediately notice that the code in listing [6.2](#listing-XYZ-6-2) is
    significantly different from the non-normalization example just before. The first
    difference is that we’re importing the `MinMaxScaler` module from the Sclikit-learn
    library. Look through the whole thing then and then we’ll work through the rest
    of the code section-by-section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您会立即注意到列表[6.2](#listing-XYZ-6-2)中的代码与之前的非标准化示例有很大的不同。第一个区别是我们从Sclikit-learn库中导入了`MinMaxScaler`模块。然后请浏览整个代码，然后我们将逐个部分地处理其余的代码。
- en: Listing 6.2 Visualizing a time series with normalization
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2 使用标准化可视化时间序列
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We extract the data from the `Year` column the same way we did earlier. Those
    values will work just fine as they are. But then we’ll normalize the other columns
    of data by applying the `MinMaxScaler` module (identified as `scaler`). We’ll
    assign the magically-transformed data to the variable `normalized_data`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与之前相同的方式从`Year`列中提取数据。这些值作为它们本身就可以使用。但是我们将通过应用`MinMaxScaler`模块（标识为`scaler`）将其他列的数据进行标准化。我们将将经过神奇变换的数据赋给变量`normalized_data`。
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We’ll then cycle through each column of data (`for i, column_name`) and plot
    their data against the `years` data. This data is then generated (`plt.plot`).
    The `label=column_name` argument applies the existing names for each of the three
    regular data columns to the normalized data we’re generating.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将遍历每列数据（`for i, column_name`）并将它们的数据与`years`数据绘制在一起。然后生成这些数据（`plt.plot`）。`label=column_name`参数将每个三个常规数据列的现有名称应用于我们正在生成的标准化数据。
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, as before, we set the labels and title, add a legend, and then generate
    the figure itself. Here’s how that looks:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，和以前一样，我们设定标签和标题，添加图例，然后生成图形。下面是它的样子：
- en: Figure 6.2 Our hardware component data visualized using normalized data
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.2 使用标准化数据可视化的硬件组件数据
- en: '![gai 6 2](images/gai-6-2.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![gai 6 2](images/gai-6-2.png)'
- en: That’s a huge improvement. We’re shown relatively steady growth trends for all
    three component classes. The thing to remember is that the plot lines normalization
    gives us are also imperfect. That’s because the start and finish values for all
    three plot lines are set to 0 and 1 respectively. So, by looking at this graph,
    it’ll be impossible to tell whether any one class grew *faster* than the others.
    But, given that limitation, we should be able to visually identify any significant
    trends - like the apparent lack of growth in storage capacity between 2009 and
    2013 or so.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个巨大的改进。我们展示了三个组件类别的相对稳定的增长趋势。需要记住的是，绘图线的标准化也是不完美的。这是因为所有三条绘图线的起点和终点分别设定为0和1。因此，通过观察这个图表，将无法判断任何一个类别是否比其他类别增长得更快。但是，鉴于这个限制，我们应该能够在视觉上发现任何显著的趋势-比如2009年至2013年左右存储能力看似未增长。
- en: Of course, this is AI-generated data, so you shouldn’t assume it’s all correct.
    Don’t go investing your life savings based on these numbers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是人工智能生成的数据，所以不要认为它们全部正确。不要根据这些数字来进行投资。
- en: 6.5 Summary
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 总结
- en: Prompt engineering techniques can be used to precisely guide LLMs to respond
    to our particular needs.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程技术可用于准确引导LLM以满足我们特定的需求。
- en: Basic prompt engineering best practices include being specific, clear, concise,
    and positive.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的提示工程最佳实践包括具体、清晰、简洁和积极。
- en: Few shot prompts can incorporate examples to teach a model the kinds of results
    you’re looking for.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少数示例提示可以使用示例来教导模型你想要的结果种类。
- en: LLMs can be prompted for estimates of real-world time series data and then explored
    two ways to visualize that data (normalized and non-normalized).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可以要求估计真实世界的时间序列数据，然后可以探索两种方式对数据进行可视化（标准化和非标准化）。
- en: 6.6 Try this for yourself
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 请你自己试试
- en: Prompt your favorite LLM with a simple "one shot" question ("Write me a script
    for a dialog between an IT support professional and an inexperienced client",
    perhaps). Then ask for the same dialog, but this time start the request off with
    a few shot prefix.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 用一个简单的“一次性”问题（例如“为IT支持专业人员和无经验的客户之间的对话编写一个剧本”）来提示您最喜欢的LLM。然后，再次要求相同的对话，但这次使用一个少数示例前缀开始请求。
