- en: '6 Prompt Engineering: Optimizing Your Generative AI Experience'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-shot and few shot prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting LLMs for historical time series data sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ll bet that, before actually opening this book, many - perhaps most - of you
    expected *prompt engineering* to be a primary focus. And yet, here we are in chapter
    6 (*half way through the book!*) and we’re only just hitting the topic? What’s
    the story here?
  prefs: []
  type: TYPE_NORMAL
- en: In my defence, I’d say that it’s partly about what we mean when we use the term.
    For some, "prompt engineering" covers a lot of what you’ll figure out on your
    own by just having fun experimenting with ChatGPT or MidJourney. It matters, but
    it doesn’t require a whole book.
  prefs: []
  type: TYPE_NORMAL
- en: But I’d also argue that what I’ve given you so far - and what’s yet to come
    the remaining chapters - goes far beyond prompts. Sure, the phrasing you use is
    important, but the API and programmatic tools we’re discovering will take your
    prompts a lot further.
  prefs: []
  type: TYPE_NORMAL
- en: There’s one more thing going on. In my experience, as GPT and other generative
    AI models improve, they’re getting better at figuring out what you want even when
    you provide a weak prompt. I can’t count the number of times that GPT has successfully
    seen right through my spelling and grammar errors, poor wording, or sometimes
    even outright technical mistakes. So, many of the problems that popular "prompt
    engineering" advice seeks to prevent, are already easily handled by the AI itself.
  prefs: []
  type: TYPE_NORMAL
- en: Still, no one wants to look like an idiot - even if the only one who can see
    is a robot. And you can never know when even an AI won’t be able to figure out
    what you’re really after. So I’ll devote this entire chapter to the fine art of
    engineering your prompts. We’ll begin with a helpful definition.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 What is prompt engineering?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt engineering is a technique used in the context of language models like
    GPT to effectively guide the model’s responses and improve its performance. It
    involves crafting specific instructions or queries, known as prompts, to encourage
    (or even force) a desired output from the model.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering can be used to shape the behavior of the model by providing
    it with explicit instructions, context, or constraints. By carefully constructing
    prompts, researchers and developers can influence the model’s output and make
    it more consistent, accurate, or better aligned with specific criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various strategies for prompt engineering, depending on what you’re
    trying to accomplish. These strategies may involve:'
  prefs: []
  type: TYPE_NORMAL
- en: Asking the model to assume a particular role ("You are an expert investment
    counsellor")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying the format you’d like for the model’s response ("Give me the response
    in .CSV format")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking the model to think step-by-step ("Can you walk me through the process
    of installing software package X, step by step?")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing additional context or background information ("This assumes that the
    value for the variable `my_number` is 10")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using system messages (like error messages) to guide the model’s behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One common technique is to use "prompt engineering by demonstration," where
    developers manually generate desired model outputs for a set of example inputs.
    The model is then fine-tuned based on this data, allowing it to generalize and
    produce similar responses for future inputs.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to keep in mind that prompt engineering is an *iterative process*.
    That means that you don’t always expect to get the completion you’re after on
    the first try. Instead, you’ll experiment, analyse the model’s initial behavior,
    and refine subsequent prompts based on feedback and evaluation. The process of
    gradual iteration lets you leverage the power of language models while maintaining
    control over the output and ensuring it aligns with your intentions and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: For one example, you might want to use iterative prompting when your LLM gives
    you programming code that doesn’t work. Rather than starting over by asking the
    same question a second time, you could copy and paste any error message you received
    and ask how that could be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Prompt engineering best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These suggestions are based on guidance found in [official OpenAI documentation](articles.html).
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Be specific
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be generous with the details and descriptions you include in your prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Tell me about quantum mechanics
  prefs: []
  type: TYPE_NORMAL
- en: 'Won’t be as effective as:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain quantum mechanics in 200 words or less and in terms that can be understood
    by a 12 year old
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: Compose an email to my boss asking for a raise
  prefs: []
  type: TYPE_NORMAL
- en: 'Will be a lot less likely to end happily than something like:'
  prefs: []
  type: TYPE_NORMAL
- en: Compose a polite but forceful email to my boss explaining how my hard work and
    successfully executed, on-time projects have earned the company new clients
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Be clear
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AIs like clarity just as much as people do. The more obvious and unambiguous
    you can make your prompt, the less chance there is that you’ll get something unexpected.
    This example isn’t necessarily bad:'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the data provided, list all the key topics, arguments, and people
    that are referenced
  prefs: []
  type: TYPE_NORMAL
- en: 'But you’re far more likely to achieve success the first time around using something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the data provided, list all the key topics that are referenced,
    then the arguments that are presented, and finally each of the people who are
    mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Desired format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics: <list_divided _by_commas>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: 'People:'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Avoid unnecessary words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There’s a higher risk of misunderstanding and poor completion results when
    you use this kind of overly verbose prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: I’m looking for a fairly long and completely detailed description of each of
    the ten most popular passenger cars of the early 1970s (by domestic US sales).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, try something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: List and describe each of the ten highest selling cars in the US during the
    early 1970s.
  prefs: []
  type: TYPE_NORMAL
- en: When I ran both of those examples - the "extra verbose" and "sleek and punchy"
    versions - past ChatGPT, the results I got were both equally impressive. So I’d
    say this is a good illustration of GPT improvements I mentioned earlier. It’s
    also an indication of the growing irrelevance of the topic of prompt engineering
    as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4 Separate reference text from your instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You should make it clear where your instructions end and any reference text
    you’re including begins. This example might not work:'
  prefs: []
  type: TYPE_NORMAL
- en: Who was the author of:q We can only see a short distance ahead, but we can see
    plenty there that needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: 'But this probably will (note the use of triple quotations - although I’m not
    sure that they’re still as important as they once were):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Who was the author of the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text: """We can only see a short distance ahead, but we can see plenty there
    that needs to be done."""'
  prefs: []
  type: TYPE_NORMAL
- en: By the way, as I’m sure you’re curious, the author was Alan Turing.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.5 Be positive, not negative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This works in personal relationships, too. But right now we’re more concerned
    with the way you get along with your favorite generative AI. It seems that GPT
    and its cousins can sometimes get confused by negatives like this:'
  prefs: []
  type: TYPE_NORMAL
- en: When responding to the following request for help from a customer who is locked
    out of their account, DO NOT suggest they update their password.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rephrasing the prompt as a positive might make a difference:'
  prefs: []
  type: TYPE_NORMAL
- en: When responding to the following request for help from a customer who is locked
    out of their account, instead of suggesting they update their password, refer
    them to available online documentation.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few other LLM training methods that can be applied in the specific
    context of prompts to improve the quality of your completions. We’ll look at those
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaway
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When composing your prompts, remember to be specific, clear, concise, and positive,
    and to clearly demarcate you reference text.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.6 Control for temperature (randomness)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can directly incorporate temperature within a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a creative and unique story beginning with the sentence "It was a dark
    and stormy night." Use a temperature setting of 0.8 to add some randomness and
    creativity to the story.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the temperature setting of 0.8 indicates that the generated
    response will have a moderate level of randomness and creativity. The higher the
    temperature, the more varied and unpredictable the output will be. You can adjust
    the temperature value to control the amount of randomness in the generated text.
    A higher value like 0.8 will result in more diverse and imaginative responses,
    while a lower value like 0.2 will produce more focused and deterministic responses.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Zero-shot and few shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero-shot and few-shot prompting are techniques used in natural language processing
    (NLP) to generate responses or perform tasks without explicit training on specific
    examples or with only a limited amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot prompting refers to the ability of a model to provide meaningful responses
    or perform tasks for which it has not been explicitly trained. The model is capable
    of generalizing from its training data to understand and respond to new inputs
    or tasks. This is achieved by using prompts or instructions that guide the model’s
    behavior. For example, if a language model has been trained on a variety of topics,
    it can still generate coherent responses on a new topic by providing a prompt
    that specifies the desired topic.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting, on the other hand, involves training a model with only a
    small amount of labeled data or examples. By leveraging this limited training
    data, the model is expected to learn how to generalize and perform tasks on unseen
    or novel examples. This approach is useful when the availability of labeled data
    is scarce or when adapting a model to new tasks quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Both zero-shot and few-shot prompting leverage the pre-training and fine-tuning
    methodology. In pre-training, a model is trained on a large corpus of text data
    to learn general language patterns and representations. Fine-tuning follows, where
    the pre-trained model is further trained on specific tasks or domains using limited
    labeled data or prompts. This combination enables the model to exhibit adaptability
    and generate meaningful responses or perform tasks in a zero-shot or few-shot
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'These techniques have proven to be powerful in various NLP tasks, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They allow models to demonstrate a degree of understanding and perform adequately
    on new or unseen inputs, even without extensive training on specific examples.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of zero-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you have a language model that has been trained on a variety of topics
    but hasn’t been explicitly trained on the topic of space exploration. Using zero-shot
    prompting, you can still generate coherent responses on space-related questions.
    For example, you could provide the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the key challenges and achievements in space exploration?
  prefs: []
  type: TYPE_NORMAL
- en: The model, even without specific training on space exploration, can generate
    a response by drawing upon its general knowledge and understanding of the topic.
  prefs: []
  type: TYPE_NORMAL
- en: And here’s an example of few-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a model that has been pre-trained on a large corpus of text
    but hasn’t been fine-tuned for sentiment analysis. However, with few-shot prompting,
    you can train the model on a small labeled dataset containing a few examples of
    positive and negative sentiments. The model can then generalize from this limited
    training data and perform sentiment analysis on new, unseen text.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a more generic example of how a few-shot prompt might look. We would
    first train the model using these prompt/completion examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'English sentence: "I love to travel."'
  prefs: []
  type: TYPE_NORMAL
- en: 'French translation: "J’adore voyager."'
  prefs: []
  type: TYPE_NORMAL
- en: 'English sentence: "The cat is sleeping."'
  prefs: []
  type: TYPE_NORMAL
- en: 'French translation: "Le chat dort."'
  prefs: []
  type: TYPE_NORMAL
- en: 'English sentence: "Where is the nearest train station?"'
  prefs: []
  type: TYPE_NORMAL
- en: 'French translation: "Où se trouve la gare la plus proche?"'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you’ve "trained" the model to anticipate the kind of result
    you want. You’re now ready to submit an actual prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: English sentence "Can you recommend a good restaurant?"
  prefs: []
  type: TYPE_NORMAL
- en: 'French translation:'
  prefs: []
  type: TYPE_NORMAL
- en: Both zero-shot and few-shot prompting leverage the model’s ability to generalize
    from its pre-training and make predictions or perform tasks on new inputs or tasks,
    either with minimal or no specific training. They’re fundamental tools used by
    AI engineers when they design their LLMs, but the same basic principles can also
    be used for our own day-to-day AI interactions.
  prefs: []
  type: TYPE_NORMAL
- en: '6.4 Prompt for time series data: a practical example'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you know someone who happens to have perfect command of the entire internet,
    creating new value from your relationship is often just a matter of being creative
    enough to ask the right questions. As one does, I recently had an irrepressible
    urge to visualize historical improvements in server hardware components. Has capacity
    growth been consistent over the years? Has capacity grown at similar rates for
    all component categories?
  prefs: []
  type: TYPE_NORMAL
- en: But where would I find the data? My curiosity wasn’t irrepressible enough to
    justify scouring archived versions of vendor websites for hours on end. Could
    my smart "friend" (by which I mean GPT or one of its LLM cousins) help me out
    here?
  prefs: []
  type: TYPE_NORMAL
- en: 'No reason not to try. Here’s my prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Give me the basic specifications for out-of-the-box, top-of-the-line rack-mount
    servers from each year between 1994 and 2021\. Show me the output in CSV format
    using the following columns: Year, clock speed (GHz), Maximum RAM (GB), Total
    Drive Capacity (GB)'
  prefs: []
  type: TYPE_NORMAL
- en: Since I expected to load the output into a Python program, I figured I’d save
    myself some work and ask for the data in comma-separated values (CSV) format using
    exactly the column headers I preferred. I tried this out using both ChatGPT and
    [Perplexity Labs' LLM server](labs.perplexity.ai.html). To my astonishment, GPT
    gave me nicely formatted CSV data that at least looked realistic. For some reason,
    Perplexity interpreted "CSV" as "Markdown", but fixing that wasn’t a big deal.
  prefs: []
  type: TYPE_NORMAL
- en: The data itself (along with the code used in the following examples) are available
    [as part of the book’s GitHub repo](main.html).
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Visualizing the Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Of course, properly visualizing my data will be essential for both assessing
    whether the output makes sense and, if it does, what insights it might give me.
    But, as I’ll show you, the *way* you visualize this data will determine how well
    you’ll understand it. Let me explain that by showing you how to generate charts
    using both normalized and non-normalized data.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization refers to the process of adjusting data values to a common scale
    or standard, typically to facilitate meaningful comparisons between different
    data points. It’s a common technique used in data visualization and analysis to
    remove the influence of varying scales or units in the data, making it easier
    to identify patterns and trends. In our case, that’s important because the *scale*
    of the units used to measure CPU clock speeds (GHz) is very different from the
    units used to measure memory (GB) and storage (also GB).
  prefs: []
  type: TYPE_NORMAL
- en: Normalization helps ensure that the relative relationships and variations within
    the data are preserved while removing the influence of different scales. This
    is especially useful when comparing data from different sources or when visualizing
    data on the same graph with different units or scales.
  prefs: []
  type: TYPE_NORMAL
- en: '**Min-Max Scaling**, also known as **Min-Max Normalization**, is a data normalization
    method used to transform data into a specific range, typically between 0 and 1\.
    The purpose of Min-Max Scaling is to standardize data values, making them more
    comparable and suitable for various data analysis and machine learning techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how Min-Max Scaling works:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the minimum (min) and maximum (max) values within the dataset for the feature
    you want to normalize.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each data point in that feature, apply the following formula to scale it
    to the range [0, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scaled Value = (Original Value - Min) / (Max - Min)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting "Scaled Value" for each data point will fall within the range
    of 0 to 1, where 0 represents the minimum value in the dataset, and 1 represents
    the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: Min-Max Scaling is particularly useful when you want to preserve the relationships
    and proportions between data points while ensuring that all values are on a consistent
    scale. It’s widely used in various data analysis and machine learning tasks, especially
    when algorithms like neural networks, k-means clustering, or support vector machines
    are sensitive to the scale of input features.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, suppose you have a dataset that represents the employee salaries
    ranging from $40,000 to $100,000\. You also have data representing the number
    of years of experience each employee has - ranging from 2 to 20 years. You want
    to standardize these values using Min-Max Scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Without Min-Max Scaling, the salary values could be in the range of 40,000 to
    100,000, while the years of experience could be in the range of 2 to 20\. The
    data points for salary and years of experience would be on very different scales.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you apply Min-Max Scaling to both features, you might scale the values
    to a range of 0 to 1\. So, a salary of $50,000 could be scaled to 0.25, and 10
    years of experience might be scaled to 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Min-Max Scaling is the tool we’ll use here. But first, I’ll show you what we
    get using non-normalized data for a graph so you’ll see with your own eyes why
    normalization can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Graphing the time series data without normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Listing [6.1](#listing-XYZ-6-1). shows the complete code for producing a non-normalized
    graph of our data. Look through it yourself and try to understand what’s happening,
    and then we’ll work through it one section at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Visualizing a time series without normalization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s walk through that code. As always, we begin by importing the libraries
    we’ll need. Pandas will handle the data itself, and matplotlib will help us with
    the graphs. We’ll then import the CSV data file into a data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I’ll define the dimensions of the graph (or *figure* as it’s more commonly described)
    that I’ll eventually generate. By all means, experiment with alternate values
    to see what changes. But these should be perfectly workable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use the NumPy tool to take each of the columns and convert it to a NumPy
    array - which is a data format that’s usable for plotting our graph/figure. We’ll
    give the array created from the data in each column a name. The column `df_all['Year']`
    for instance, will be called `years`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `years` will be used as our x-axis, I’ll now plot the other three NumPy
    arrays. Each of those arrays will be associated with the `years` values and given
    a display label like: `label=''Clock speed (GHz)''`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To make the graph more readable, we’ll add labels to both the x and y axes,
    and give the figure itself a title. We’ll also add a color-coded legend so we’ll
    be able to quickly understand which plot line represents which column of data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll pull the trigger and generate the figure itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s how it’ll all come out:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 Our hardware component data visualized using non-normalized data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 6 1](images/gai-6-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Besides the fact that it won’t be easy for those of you reading this in a physical
    book to distinguish between the color-coded plot lines, there’s another, more
    serious, problem here. A casual glance would lead us to conclude that processor
    clock speeds haven’t improved at all in the years since 1994\. But that’s ridiculous.
  prefs: []
  type: TYPE_NORMAL
- en: The data itself shows average clock speeds going from 1 GHz to 11 GHz over that
    time. That last number (11) is weird. I’m aware of no processor on earth that
    can run at 11 GHz. I suspect that the Perplexity LLM is accounting for the spread
    of multi-core systems and simply adding the maximum speeds of each parallel core
    that might be installed on a system. But in any case, why does that growth not
    show up in our plot line?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that the difference between 1 GHz and 11 GHz isn’t anything like
    the difference between 32 GB and 6,144,960 GB. It’s all about *scale*. To fix
    *that* we’ll need to normalize our data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 Graphing the time series data with normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ll immediately notice that the code in listing [6.2](#listing-XYZ-6-2) is
    significantly different from the non-normalization example just before. The first
    difference is that we’re importing the `MinMaxScaler` module from the Sclikit-learn
    library. Look through the whole thing then and then we’ll work through the rest
    of the code section-by-section.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Visualizing a time series with normalization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We extract the data from the `Year` column the same way we did earlier. Those
    values will work just fine as they are. But then we’ll normalize the other columns
    of data by applying the `MinMaxScaler` module (identified as `scaler`). We’ll
    assign the magically-transformed data to the variable `normalized_data`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We’ll then cycle through each column of data (`for i, column_name`) and plot
    their data against the `years` data. This data is then generated (`plt.plot`).
    The `label=column_name` argument applies the existing names for each of the three
    regular data columns to the normalized data we’re generating.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, as before, we set the labels and title, add a legend, and then generate
    the figure itself. Here’s how that looks:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 Our hardware component data visualized using normalized data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 6 2](images/gai-6-2.png)'
  prefs: []
  type: TYPE_IMG
- en: That’s a huge improvement. We’re shown relatively steady growth trends for all
    three component classes. The thing to remember is that the plot lines normalization
    gives us are also imperfect. That’s because the start and finish values for all
    three plot lines are set to 0 and 1 respectively. So, by looking at this graph,
    it’ll be impossible to tell whether any one class grew *faster* than the others.
    But, given that limitation, we should be able to visually identify any significant
    trends - like the apparent lack of growth in storage capacity between 2009 and
    2013 or so.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is AI-generated data, so you shouldn’t assume it’s all correct.
    Don’t go investing your life savings based on these numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt engineering techniques can be used to precisely guide LLMs to respond
    to our particular needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic prompt engineering best practices include being specific, clear, concise,
    and positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few shot prompts can incorporate examples to teach a model the kinds of results
    you’re looking for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be prompted for estimates of real-world time series data and then explored
    two ways to visualize that data (normalized and non-normalized).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.6 Try this for yourself
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt your favorite LLM with a simple "one shot" question ("Write me a script
    for a dialog between an IT support professional and an inexperienced client",
    perhaps). Then ask for the same dialog, but this time start the request off with
    a few shot prefix.
  prefs: []
  type: TYPE_NORMAL
