- en: '7 Working with Keras: A deep dive'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Keras models with keras_model_ sequential(), the Functional API, and
    model subclassing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using built-in Keras training and evaluation loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Keras callbacks to customize training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TensorBoard to monitor training and evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing training and evaluation loops from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ve now got some experience with Keras—you’re familiar with the Sequential
    model, dense layers, and built-in APIs for training, evaluation, and inference—
    compile(), fit(), evaluate(), and predict(). You even learned in chapter 3 how
    to use new_layer_class() to create custom layers, and how to use the TensorFlow
    GradientTape() to implement a step-by-step training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the coming chapters, we’ll dig into computer vision, time-series forecasting,
    natural language processing, and generative deep learning. These complex applications
    will require much more than a keras_model_sequential() architecture and the default
    fit() loop. So, let’s first turn you into a Keras expert! In this chapter, you’ll
    get a complete overview of the key ways to work with Keras APIs: everything you’re
    going to need to handle the advanced deep learning use cases you’ll encounter
    next.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 A spectrum of workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The design of the Keras API is guided by the principle of *progressive disclosure
    of complexity*: make it easy to get started, yet make it possible to handle high-complexity
    use cases, requiring only incremental learning at each step. Simple use cases
    should be easy and approachable, and arbitrarily advanced workflows should be
    *possible*: no matter how niche and complex the thing you want to do, there should
    be a clear path to it—a path that builds upon the various things you’ve learned
    from simpler workflows. This means that you can grow from beginner to expert and
    still use the same tools, only in different ways.'
  prefs: []
  type: TYPE_NORMAL
- en: As such, there’s not a single “true” way of using Keras. Rather, Keras offers
    a *spectrum of workflows*, from the very simple to the very flexible. There are
    different ways to build Keras models, and different ways to train them, answering
    different needs. Because all these workflows are based on shared APIs, such as
    Layer and Model, components from any workflow can be used in any other workflow—they
    can all talk to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Different ways to build Keras models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Three APIs exist for building models in Keras (see [figure 7.1](#fig7-1)):'
  prefs: []
  type: TYPE_NORMAL
- en: The *Sequential model*, the most approachable API—it’s basically a list. As
    such, it’s limited to simple stacks of layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Functional API*, which focuses on graph-like model architectures. It represents
    a nice midpoint between usability and flexibility, and as such, it’s the most
    commonly used model-building API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model subclassing*, a low-level option where you write everything yourself
    from scratch. This is ideal if you want full control over every little thing.
    However, you won’t get access to many built-in Keras features, and you will be
    more at risk of making mistakes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0186-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.1 Progressive disclosure of complexity for model building**'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 The Sequential model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest way to build a Keras model is to use keras_model_sequential(),
    which you already know about.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.1 keras_model_sequential()
  prefs: []
  type: TYPE_NORMAL
- en: library(keras)
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: Note that it’s possible to build the same model incrementally with %>%.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.2 Incrementally building a Sequential model
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential()
  prefs: []
  type: TYPE_NORMAL
- en: model %>% layer_dense(64, activation = "relu")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% layer_dense(10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: 'You saw in chapter 4 that layers get built (which is to say, create their weights)
    only when they are called for the first time. That’s because the shape of the
    layers’ weights depends on the shape of their input: until the input shape is
    known, they can’t be created.'
  prefs: []
  type: TYPE_NORMAL
- en: As such, the preceding Sequential model does not have any weights (listing 7.3)
    until you actually call it on some data, or call its build() method with an input
    shape (listing 7.4).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.3 Models that aren’t yet built have no weights
  prefs: []
  type: TYPE_NORMAL
- en: model$weights➊
  prefs: []
  type: TYPE_NORMAL
- en: 'Error in py_get_attr_impl(x, name, silent):'
  prefs: []
  type: TYPE_NORMAL
- en: 'ValueError: Weights for model sequential_1 have not yet been created. Weights
    are created when the Model is first called on inputs or `build()` is called with
    an `input_shape`.'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **At that point, the model isn't built yet.**
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.4 Calling a model for the first time to build it
  prefs: []
  type: TYPE_NORMAL
- en: model$build(input_shape = shape(NA, 3))➊
  prefs: []
  type: TYPE_NORMAL
- en: str(model$weights)➋
  prefs: []
  type: TYPE_NORMAL
- en: List of 4
  prefs: []
  type: TYPE_NORMAL
- en: $ :<tf.Variable 'dense_2/kernel:0' shape=(3, 64) dtype=float32, numpy=…>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<tf.Variable 'dense_2/bias:0' shape=(64) dtype=float32, numpy=…>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<tf.Variable 'dense_3/kernel:0' shape=(64, 10) dtype=float32, numpy=…>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<tf.Variable 'dense_3/bias:0' shape=(10) dtype=float32, numpy=…>
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Build the model—now the model will expect samples of shape (3). The NA in
    the input shape signals that the batch size could be anything.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Now you can retrieve the model's weights.**
  prefs: []
  type: TYPE_NORMAL
- en: After the model is built, you can display its contents via the print() method,
    which comes in handy for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.5 The print() method
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0188-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this model happens to be named “sequential_1.” You can give
    names to everything in Keras—every model, every layer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.6 Naming models and layers with the name argument
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model_sequential(name = "my_example_model")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% layer_dense(64, activation = "relu", name = "my_first_layer")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% layer_dense(10, activation = "softmax", name = "my_last_layer")
  prefs: []
  type: TYPE_NORMAL
- en: model$build(shape(NA, 3))
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0188-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When building a Sequential model incrementally, it’s useful to be able to print
    a summary of what the current model looks like after you add each layer. But you
    can’t print a summary until the model is built! There’s actually a way to have
    your Sequential model built on the fly: just declare the shape of the model’s
    inputs in advance. You can do this by passing input_shape to keras_model_sequential().'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.7 Specifying the input shape of your model in advance
  prefs: []
  type: TYPE_NORMAL
- en: model <
  prefs: []
  type: TYPE_NORMAL
- en: keras_model_sequential(input_shape = c(3)) %>%➊
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu")
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Supply input_shape to declare the shape of the inputs. Note that the shape
    argument must be the shape of each sample, not the shape of one batch.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can use print() to follow how the output shape of your model changes
    as you add more layers:'
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0189-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: model %>% layer_dense(10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0189-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a pretty common debugging workflow when dealing with layers that transform
    their inputs in complex ways, such as the convolutional layers you’ll learn about
    in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 The Functional API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Sequential model is easy to use, but its applicability is extremely limited:
    it can only express models with a single input and a single output, applying one
    layer after the other in a sequential fashion. In practice, it’s pretty common
    to encounter models with multiple inputs (say, an image and its metadata), multiple
    outputs (different things you want to predict about the data), or a nonlinear
    topology.'
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, you’d build your model using the Functional API. This is what
    most Keras models you’ll encounter in the wild use. It’s fun and powerful—it feels
    like playing with LEGO bricks.
  prefs: []
  type: TYPE_NORMAL
- en: A SIMPLE EXAMPLE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with something simple: the stack of two layers we used in the previous
    section. Its Functional API version looks like the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.8 A simple Functional model with two Dense layers
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(3), name = "my_input")
  prefs: []
  type: TYPE_NORMAL
- en: features <- inputs %>% layer_dense(64, activation = "relu")
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- features %>% layer_dense(10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs = inputs, outputs = outputs)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go over this step by step. We started by declaring a layer_input() (note
    that you can also give names to these input objects, like everything else):'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(3), name = "my_input")
  prefs: []
  type: TYPE_NORMAL
- en: 'This inputs object holds information about the shape and dtype of the data
    that the model will process:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs$shape➊
  prefs: []
  type: TYPE_NORMAL
- en: TensorShape([None, 3])
  prefs: []
  type: TYPE_NORMAL
- en: inputs$dtype➋
  prefs: []
  type: TYPE_NORMAL
- en: tf.float32
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The model will process batches where each sample has shape (3). The number
    of samples per batch is variable (indicated by the None batch size).**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **These batches will have dtype float32.**
  prefs: []
  type: TYPE_NORMAL
- en: We call such an object a *symbolic tensor*. It doesn’t contain any actual data,
    but it encodes the specifications of the actual tensors of data that the model
    will see when you use it. It *stands for* future tensors of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a layer and compose with the input:'
  prefs: []
  type: TYPE_NORMAL
- en: features <- inputs %>% layer_dense(64, activation = "relu")
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Functional API, piping a symbolic tensor into a layer constructor invokes
    the layer’s call() method. In essence, this is what is happening:'
  prefs: []
  type: TYPE_NORMAL
- en: layer_instance <- layer_dense(units = 64, activation = "relu") features <- layer_instance(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is different from the Sequential API, where composing a layer with a model
    (model %>% layer_dense()) means this:'
  prefs: []
  type: TYPE_NORMAL
- en: layer_instance <- layer_dense(units = 64, activation = "relu") model$add(layer_instance)
  prefs: []
  type: TYPE_NORMAL
- en: 'All Keras layers can be called both on real tensors of data and on these symbolic
    tensors. In the latter case, they return a new symbolic tensor, with updated shape
    and dtype information:'
  prefs: []
  type: TYPE_NORMAL
- en: features$shape
  prefs: []
  type: TYPE_NORMAL
- en: TensorShape([None, 64])
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that symbolic tensors work with almost all the same R generic methods
    as eager tensors. This means that you can also do something like this to get the
    shape as an R integer vector:'
  prefs: []
  type: TYPE_NORMAL
- en: dim(features)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] NA 64'
  prefs: []
  type: TYPE_NORMAL
- en: 'After obtaining the final outputs, we instantiated the model by specifying
    its inputs and outputs in the keras_model() constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- layer_dense(features, 10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs = inputs, outputs = outputs)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the summary of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: model
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0191-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MULTI-INPUT, MULTI-OUTPUT MODELS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike this toy model, most deep learning models don’t look like lists—they
    look like graphs. They may, for instance, have multiple inputs or multiple outputs.
    It’s for this kind of model that the Functional API really shines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you’re building a system to rank customer support tickets by priority
    and route them to the appropriate department. Your model has three inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The title of the ticket (text input)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text body of the ticket (text input)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any tags added by the user (categorical input, assumed here to be one-hot encoded)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can encode the text inputs as arrays of ones and zeros of size vocabulary_size
    (see chapter 11 for detailed information about text encoding techniques). Your
    model also has two outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The department that should handle the ticket (a softmax over the set of departments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build this model in a few lines with the Functional API.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.9 A multi-input, multi-output Functional model
  prefs: []
  type: TYPE_NORMAL
- en: vocabulary_size <- 10000
  prefs: []
  type: TYPE_NORMAL
- en: num_tags <- 100
  prefs: []
  type: TYPE_NORMAL
- en: num_departments <- 4
  prefs: []
  type: TYPE_NORMAL
- en: title <- layer_input(shape = c(vocabulary_size), name = "title")➊
  prefs: []
  type: TYPE_NORMAL
- en: text_body <- layer_input(shape = c(vocabulary_size), name = "text_body")➊
  prefs: []
  type: TYPE_NORMAL
- en: tags <- layer_input(shape = c(num_tags), name = "tags")➊
  prefs: []
  type: TYPE_NORMAL
- en: features <-
  prefs: []
  type: TYPE_NORMAL
- en: layer_concatenate(list(title, text_body, tags)) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu")➌
  prefs: []
  type: TYPE_NORMAL
- en: priority <- features %>%➍
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid", name = "priority")
  prefs: []
  type: TYPE_NORMAL
- en: department <- features %>%➍
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(num_departments, activation = "softmax", name = "department")
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(➎
  prefs: []
  type: TYPE_NORMAL
- en: inputs = list(title, text_body, tags),
  prefs: []
  type: TYPE_NORMAL
- en: outputs = list(priority, department)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Define the model inputs.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Combine the input features into a single tensor by concatenating them.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Apply an intermediate layer to recombine input features into richer representations.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Define the model outputs.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Create the model by specifying its inputs and outputs.**
  prefs: []
  type: TYPE_NORMAL
- en: The Functional API is a simple, LEGO-like, yet very flexible way to define arbitrary
    graphs of layers like these.
  prefs: []
  type: TYPE_NORMAL
- en: TRAINING A MULTI-INPUT, MULTI-OUTPUT MODEL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can train your model in much the same way as you would train a Sequential
    model, by calling fit() with lists of input and output data. These lists of data
    should be in the same order as the inputs you passed to the keras_model() constructor.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.10 Training a model by providing lists of input and target arrays
  prefs: []
  type: TYPE_NORMAL
- en: num_samples <- 1280
  prefs: []
  type: TYPE_NORMAL
- en: random_uniform_array <- function(dim)
  prefs: []
  type: TYPE_NORMAL
- en: array(runif(prod(dim)), dim)
  prefs: []
  type: TYPE_NORMAL
- en: random_vectorized_array <- function(dim)
  prefs: []
  type: TYPE_NORMAL
- en: array(sample(0:1, prod(dim), replace = TRUE), dim)
  prefs: []
  type: TYPE_NORMAL
- en: title_data <- random_vectorized_array(c(num_samples, vocabulary_size))➊
  prefs: []
  type: TYPE_NORMAL
- en: text_body_data <- random_vectorized_array(c(num_samples, vocabulary_size))➊
  prefs: []
  type: TYPE_NORMAL
- en: tags_data <- random_vectorized_array(c(num_samples, num_tags))➊
  prefs: []
  type: TYPE_NORMAL
- en: priority_data <- random_vectorized_array(c(num_samples, 1))➋
  prefs: []
  type: TYPE_NORMAL
- en: department_data <- random_vectorized_array(c(num_samples, num_departments))➋
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = c("mean_squared_error", "categorical_crossentropy"),
  prefs: []
  type: TYPE_NORMAL
- en: metrics = c("mean_absolute_error", "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: x = list(title_data, text_body_data, tags_data),
  prefs: []
  type: TYPE_NORMAL
- en: y = list(priority_data, department_data),
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 1
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model %>% evaluate(x = list(title_data, text_body_data, tags_data),
  prefs: []
  type: TYPE_NORMAL
- en: y = list(priority_data, department_data))
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0193-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: c(priority_preds, department_preds) %<-% {➌
  prefs: []
  type: TYPE_NORMAL
- en: model %>% predict(list(title_data, text_body_data, tags_data))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Dummy input data**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Dummy target data**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **To use %<-% and %>% in the same expression, you need to wrap the pipe sequence
    with {} or () to override the default operator precedence.**
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t want to rely on input order (for instance, because you have many
    inputs or outputs), you can also leverage the names you gave to the input_shape
    and the output layers, and pass data via a named list.
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT When using named lists, the order of the list is not guaranteed to
    be preserved. Be sure to keep track of items *either* by position *or* by name,
    but not a combination of both.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Listing 7.11 Training a model by providing named lists of input and target arrays
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = c(priority = "mean_squared_error",
  prefs: []
  type: TYPE_NORMAL
- en: department = "categorical_crossentropy"),
  prefs: []
  type: TYPE_NORMAL
- en: metrics = c(priority = "mean_absolute_error",
  prefs: []
  type: TYPE_NORMAL
- en: department = "accuracy"))
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: fit(list(title = title_data,
  prefs: []
  type: TYPE_NORMAL
- en: text_body = text_body_data,
  prefs: []
  type: TYPE_NORMAL
- en: tags = tags_data),
  prefs: []
  type: TYPE_NORMAL
- en: list(priority = priority_data,
  prefs: []
  type: TYPE_NORMAL
- en: department = department_data), epochs = 1)
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: evaluate(list(title = title_data,
  prefs: []
  type: TYPE_NORMAL
- en: text_body = text_body_data,
  prefs: []
  type: TYPE_NORMAL
- en: tags = tags_data),
  prefs: []
  type: TYPE_NORMAL
- en: list(priority = priority_data,
  prefs: []
  type: TYPE_NORMAL
- en: department = department_data))
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0194-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: c(priority_preds, department_preds) %<-%
  prefs: []
  type: TYPE_NORMAL
- en: predict(model, list(title = title_data,
  prefs: []
  type: TYPE_NORMAL
- en: text_body = text_body_data,
  prefs: []
  type: TYPE_NORMAL
- en: tags = tags_data))
  prefs: []
  type: TYPE_NORMAL
- en: 'THE POWER OF THE FUNCTIONAL API: ACCESS TO LAYER CONNECTIVITY'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Functional model is an explicit graph data structure. This makes it possible
    to inspect how layers are connected and reuse previous graph nodes (which are
    layer outputs) as part of new models. It also nicely fits the “mental model” that
    most researchers use when thinking about a deep neural network: a graph of layers.
    This enables two important use cases: model visualization and feature extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize the connectivity of the model we just defined (the *topology*
    of the model). You can plot a Functional model as a graph with the plot() method
    (see [figure 7.2](#fig7-2)):'
  prefs: []
  type: TYPE_NORMAL
- en: plot(model)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0194-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.2 Plot generated by plot(model) on our ticket classifier model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add to this plot the input and output shapes of each layer in the model,
    which can be helpful during debugging (see [figure 7.3](#fig7-3)):'
  prefs: []
  type: TYPE_NORMAL
- en: plot(model, show_shapes = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0195-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.3 Model plot with shape information added**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “None” in the tensor shapes represents the batch size: this model allows
    batches of any size.'
  prefs: []
  type: TYPE_NORMAL
- en: Access to layer connectivity also means that you can inspect and reuse individual
    nodes (layer calls) in the graph. The model$layers model property provides the
    list of layers that make up the model, and for each layer you can query layer$input
    and layer$output.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.12 Retrieving the inputs or outputs of a layer in a Functional model
  prefs: []
  type: TYPE_NORMAL
- en: str(model$layers)
  prefs: []
  type: TYPE_NORMAL
- en: List of 7
  prefs: []
  type: TYPE_NORMAL
- en: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da63a0>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da6430>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da68e0>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<keras.layers.merge.Concatenate object at 0x7fc962d2e130>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<keras.layers.core.dense.Dense object at 0x7fc962da6c40>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<keras.layers.core.dense.Dense object at 0x7fc962da6340>
  prefs: []
  type: TYPE_NORMAL
- en: $ :<keras.layers.core.dense.Dense object at 0x7fc962d331f0>
  prefs: []
  type: TYPE_NORMAL
- en: str(model$layers[[4]]$input)
  prefs: []
  type: TYPE_NORMAL
- en: List of 3
  prefs: []
  type: TYPE_NORMAL
- en: '$ :<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01.jpg) ''title'')>'
  prefs: []
  type: TYPE_NORMAL
- en: '$ :<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01.jpg) ''text_body'')>'
  prefs: []
  type: TYPE_NORMAL
- en: '$ :<KerasTensor: shape=(None, 100) dtype=float32 (created by layer ''tags'')>'
  prefs: []
  type: TYPE_NORMAL
- en: str(model$layers[[4]]$output)
  prefs: []
  type: TYPE_NORMAL
- en: '<KerasTensor: shape=(None, 20100) dtype=float32 (created by layer'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common01.jpg) ''concatenate'')>'
  prefs: []
  type: TYPE_NORMAL
- en: This enables you to do *feature extraction*, creating models that reuse intermediate
    features from another model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you want to add another output to the previous model—you want to
    estimate how long a given issue ticket will take to resolve, a kind of difficulty
    rating. You could do this via a classification layer over three categories: “quick,”
    “medium,” and “difficult.” You don’t need to recreate and retrain a model from
    scratch. You can start from the intermediate features of your previous model,
    because you have access to them, like this.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.13 Creating a new model by reusing intermediate layer outputs
  prefs: []
  type: TYPE_NORMAL
- en: features <- model$layers[[5]]$output➊
  prefs: []
  type: TYPE_NORMAL
- en: difficulty <- features %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(3, activation = "softmax", name = "difficulty")
  prefs: []
  type: TYPE_NORMAL
- en: new_model <- keras_model(
  prefs: []
  type: TYPE_NORMAL
- en: inputs = list(title, text_body, tags),
  prefs: []
  type: TYPE_NORMAL
- en: outputs = list(priority, department, difficulty)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **layer[[5]] is our intermediate dense layer. You can also retrieve a layer
    by name with get_layer().**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot our new model (see [figure 7.4](#fig7-4)):'
  prefs: []
  type: TYPE_NORMAL
- en: plot(new_model, show_shapes = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0196-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.4 Plot of our new model: Updated ticket classifier**'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3 Subclassing the Model class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last model-building pattern you should know about is the most advanced
    one: Model subclassing. You learned in chapter 3 how to use new_layer_class()
    to subclass the Layer class and create custom layers. Using new_model_class()
    to subclass Model class is pretty similar:'
  prefs: []
  type: TYPE_NORMAL
- en: In the initialize() method, define the layers the model will use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the call() method, define the forward pass of the model, reusing the layers
    previously created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate your subclass, and call it on data to create its weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: REWRITING OUR PREVIOUS EXAMPLE AS A SUBCLASSED MODEL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s take a look at a simple example: we will reimplement the customer support
    ticket management model using new_model_class() to define a Model subclass.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.14 A simple subclassed model
  prefs: []
  type: TYPE_NORMAL
- en: CustomerTicketModel <- new_model_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "CustomerTicketModel",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(num_departments) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize()➊
  prefs: []
  type: TYPE_NORMAL
- en: self$concat_layer <- layer_concatenate()
  prefs: []
  type: TYPE_NORMAL
- en: self$mixing_layer <-➋
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(units = 64, activation = "relu")
  prefs: []
  type: TYPE_NORMAL
- en: self$priority_scorer <-
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(units = 1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: self$department_classifier <-
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(units = num_departments, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: call = function(inputs) {➌
  prefs: []
  type: TYPE_NORMAL
- en: title <- inputs$title➍
  prefs: []
  type: TYPE_NORMAL
- en: text_body <- inputs$text_body
  prefs: []
  type: TYPE_NORMAL
- en: tags <- inputs$tags
  prefs: []
  type: TYPE_NORMAL
- en: features <- list(title, text_body, tags) %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$concat_layer() %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$mixing_layer()
  prefs: []
  type: TYPE_NORMAL
- en: priority <- self$priority_scorer(features)
  prefs: []
  type: TYPE_NORMAL
- en: department <- self$department_classifier(features)
  prefs: []
  type: TYPE_NORMAL
- en: list(priority, department)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Don't forget to call the super$initialize()!**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Define the sublayers in the constructor. Note that we're specifying the
    units argument name here, so that we get back a layer instance.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Define the forward pass in the call() method.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **For inputs, we'll provide the model with a named list.**
  prefs: []
  type: TYPE_NORMAL
- en: We implemented a bare-bones version of Model in chapter 3\. The key thing to
    be aware of is that we’re defining a custom class, our model, that subclasses
    Model. Model provides many methods and capabilities that you can opt-in to, as
    you’ll see in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve defined the model, you can instantiate it. Note that it will create
    its weights only the first time you call it on some data, much like Layer subclasses:'
  prefs: []
  type: TYPE_NORMAL
- en: model <- CustomerTicketModel(num_departments = 4)
  prefs: []
  type: TYPE_NORMAL
- en: c(priority, department) %<-% model(list(title = title_data,
  prefs: []
  type: TYPE_NORMAL
- en: text_body = text_body_data,
  prefs: []
  type: TYPE_NORMAL
- en: tags = tags_data))
  prefs: []
  type: TYPE_NORMAL
- en: 'Models are, fundamentally, a type of Layer. That means you can easily add the
    ability for a model to compose nicely with %>% by using create_layer_wrapper(),
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- list(title = title_data,
  prefs: []
  type: TYPE_NORMAL
- en: text_body = text_body_data,
  prefs: []
  type: TYPE_NORMAL
- en: tags = tags_data)
  prefs: []
  type: TYPE_NORMAL
- en: layer_customer_ticket_model <- create_layer_wrapper(CustomerTicketModel)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_customer_ticket_model(num_departments = 4)
  prefs: []
  type: TYPE_NORMAL
- en: c(priority, department) %<-% outputs
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, everything looks very similar to Layer subclassing, a workflow you
    encountered in chapter 3\. What, then, is the difference between a Layer subclass
    and a Model sub-class? It’s simple: a “layer” is a building block you use to create
    models, and a “model” is the top-level object that you will actually train, export
    for inference, and so on. In short, a Model has fit(), evaluate(), and predict()
    methods. Layers don’t. Other than that, the two classes are virtually identical.
    (Another difference is that you can *save* a model to a file on disk, which we
    will cover in a few sections.) You can compile and train a Model subclass just
    like a Sequential or Functional model:'
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = c("mean_squared_error",
  prefs: []
  type: TYPE_NORMAL
- en: '"categorical_crossentropy"),➊'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = c("mean_absolute_error", "accuracy"))
  prefs: []
  type: TYPE_NORMAL
- en: x <- list(title = title_data,➋
  prefs: []
  type: TYPE_NORMAL
- en: text_body = text_body_data,
  prefs: []
  type: TYPE_NORMAL
- en: tags = tags_data)
  prefs: []
  type: TYPE_NORMAL
- en: y <- list(priority_data, department_data)➌
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(x, y, epochs = 1)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% evaluate(x, y)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0198-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: c(priority_preds, department_preds) %<-% {
  prefs: []
  type: TYPE_NORMAL
- en: model %>% predict(x)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The structure of what you pass as the loss and metrics arguments must match
    exactly what gets returned by call()—here, a lists of two elements.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The structure of the input data must match exactly what is expected by the
    call() method— here, a named list with entries title, text_body, and tags. (Remember,
    list order is ignored when matching by names!)**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The structure of the target data must match exactly what is returned by
    the call() method—here, a list of two elements.**
  prefs: []
  type: TYPE_NORMAL
- en: The Model subclassing workflow is the most flexible way to build a model. It
    enables you to build models that cannot be expressed as directed acyclic graphs
    of layers—imagine, for instance, a model where the call() method uses layers inside
    a for loop, or even calls them recursively. Anything is possible—you’re in charge.
  prefs: []
  type: TYPE_NORMAL
- en: 'BEWARE: WHAT SUBCLASSED MODELS DON’T SUPPORT'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This freedom comes at a cost: with subclassed models, you are responsible for
    more of the model logic, which means your potential error surface is much larger.
    As a result, you will have more debugging work to do. You are developing a new
    class object, not just snapping together LEGO bricks.'
  prefs: []
  type: TYPE_NORMAL
- en: Functional and subclassed models are also substantially different in nature.
    A Functional model is an explicit data structure—a graph of layers, which you
    can view, inspect, and modify. A subclassed model is a collection of R code—a
    class with a call() method that is an R function. This is the source of the subclassing
    workflow’s flexibility—you can code up whatever functionality you like—but it
    introduces new limitations.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, because the way layers are connected to each other is hidden inside
    the body of the call() method, you cannot access that information. Calling summary()
    will not display layer connectivity, and you cannot plot the model topology via
    plot(). Likewise, if you have a subclassed model, you cannot access the nodes
    of the graph of layers to do feature extraction because there is simply no graph.
    Once the model is instantiated, its forward pass becomes a complete black box.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4 Mixing and matching different components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Crucially, choosing one of these patterns—the Sequential model, the Functional
    API, or Model subclassing—does not lock you out of the others. All models in the
    Keras API can smoothly interoperate with each other, whether they’re Sequential
    models, Functional models, or subclassed models written from scratch. They’re
    all part of the same spectrum of workflows. For instance, you can use a subclassed
    layer or model in a Functional model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.15 Creating a Functional model that includes a subclassed model
  prefs: []
  type: TYPE_NORMAL
- en: ClassifierModel <- new_model_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "Classifier",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(num_classes = 2) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize()
  prefs: []
  type: TYPE_NORMAL
- en: if (num_classes == 2) {
  prefs: []
  type: TYPE_NORMAL
- en: num_units <- 1
  prefs: []
  type: TYPE_NORMAL
- en: activation <- "sigmoid"
  prefs: []
  type: TYPE_NORMAL
- en: '} else {'
  prefs: []
  type: TYPE_NORMAL
- en: num_units <- num_classes
  prefs: []
  type: TYPE_NORMAL
- en: activation <- "softmax"
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: self$dense <- layer_dense(units = num_units, activation = activation)
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: call = function(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: self$dense(inputs)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(3))
  prefs: []
  type: TYPE_NORMAL
- en: classifier <- ClassifierModel(num_classes = 10)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(64, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: classifier()
  prefs: []
  type: TYPE_NORMAL
- en: model <- keras_model(inputs = inputs, outputs = outputs)
  prefs: []
  type: TYPE_NORMAL
- en: Inversely, you can use a Functional model as part of a subclassed layer or model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.16 Creating a subclassed model that includes a Functional model
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(64))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>% layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: binary_classifier <- keras_model(inputs = inputs, outputs = outputs)
  prefs: []
  type: TYPE_NORMAL
- en: MyModel <- new_model_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "MyModel",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(num_classes = 2) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize()
  prefs: []
  type: TYPE_NORMAL
- en: self$dense <- layer_dense(units = 64, activation = "relu")
  prefs: []
  type: TYPE_NORMAL
- en: self$classifier <- binary_classifier
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: call = function(inputs) {
  prefs: []
  type: TYPE_NORMAL
- en: inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$dense() %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$classifier()
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- MyModel()
  prefs: []
  type: TYPE_NORMAL
- en: '7.2.5 Remember: Use the right tool for the job'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ve learned about the spectrum of workflows for building Keras models, from
    the simplest workflow, the Sequential model, to the most advanced one, model subclassing.
    When should you use one over the other? Each one has its pros and cons—pick the
    one most suitable for the job at hand.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the Functional API provides you with a pretty good tradeoff between
    ease of use and flexibility. It also gives you direct access to layer connectivity,
    which is very powerful for use cases such as model plotting or feature extraction.
    If you *can* use the Functional API—that is, if your model can be expressed as
    a directed acyclic graph of layers—I recommend using it over model subclassing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going forward, all examples in this book will use the Functional API, simply
    because all the models we will work with are expressible as graphs of layers.
    We will, however, make frequent use of subclassed layers (using new_layer_class()).
    In general, using Functional models that include subclassed layers provides the
    best of both worlds: high development flexibility while retaining the advantages
    of the Functional API.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Using built-in training and evaluation loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The principle of progressive disclosure of complexity—access to a spectrum of
    work-flows that go from dead easy to arbitrarily flexible, one step at a time—also
    applies to model training. Keras provides you with different workflows for training
    models. They can be as simple as calling fit() on your data or as advanced as
    writing a new training algorithm from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: You are already familiar with the compile(), fit(), evaluate(), predict() work-flow.
    As a reminder, take a look at the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 7.17 The standard workflow: compile(), fit(), evaluate(), predict()'
  prefs: []
  type: TYPE_NORMAL
- en: get_mnist_model <- function() {➊
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(28 * 28))
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(512, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: keras_model(inputs, outputs)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: c(c(images, labels), c(test_images, test_labels)) %<-%➋
  prefs: []
  type: TYPE_NORMAL
- en: dataset_mnist()
  prefs: []
  type: TYPE_NORMAL
- en: images <- array_reshape(images, c(-1, 28 * 28)) / 255
  prefs: []
  type: TYPE_NORMAL
- en: test_images <- array_reshape(test_images, c(-1, 28 * 28)) / 255
  prefs: []
  type: TYPE_NORMAL
- en: val_idx <- seq(10000)
  prefs: []
  type: TYPE_NORMAL
- en: val_images <- images[val_idx, ]
  prefs: []
  type: TYPE_NORMAL
- en: val_labels <- labels[val_idx]
  prefs: []
  type: TYPE_NORMAL
- en: train_images <- images[-val_idx, ]
  prefs: []
  type: TYPE_NORMAL
- en: train_labels <- labels[-val_idx]
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_mnist_model()
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",➌
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(train_images, train_labels,➍
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 3,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(val_images, val_labels))
  prefs: []
  type: TYPE_NORMAL
- en: test_metrics <- model %>% evaluate(test_images, test_labels)➎
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- model %>% predict(test_images)➏
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Create a model (we factor this into a separate function so as to reuse it
    later).**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Load your data, reserving some for validation.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Compile the model by specifying its optimizer, the loss function to minimize,
    and the metrics to monitor.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Use fit() to train the model, optionally providing validation data to monitor
    performance on unseen data.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Use evaluate() to compute the loss and metrics on new data.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Use predict() to compute classification probabilities on new data.**
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of ways you can customize this simple workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide your own custom metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass *callbacks* to the fit() method to schedule actions to be taken at specific
    points during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at these.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Writing your own metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Metrics are key to measuring the performance of your model, in particular, to
    measuring the difference between its performance on the training data and its
    performance on the test data. Commonly used metrics for classification and regression
    are already part of the keras package, all starting with the prefix metric_, and
    most of the time that’s what you will use. But if you’re doing anything out of
    the ordinary, you will need to be able to write your own metrics. It’s simple!
  prefs: []
  type: TYPE_NORMAL
- en: A Keras metric is a subclass of the Keras Metric class. Like layers, a metric
    has an internal state stored in TensorFlow variables. Unlike layers, these variables
    aren’t updated via backpropagation, so you have to write the state-update logic
    yourself, which happens in the update_state() method. For example, here’s a simple
    custom metric that measures the root mean squared error (RMSE).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.18 Implementing a custom metric by subclassing the Metric class
  prefs: []
  type: TYPE_NORMAL
- en: library(tensorflow)➊
  prefs: []
  type: TYPE_NORMAL
- en: metric_root_mean_squared_error <- new_metric_class( classname➋
  prefs: []
  type: TYPE_NORMAL
- en: = "RootMeanSquaredError",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(name = "rmse", …) {➌
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize(name = name, …)
  prefs: []
  type: TYPE_NORMAL
- en: self$mse_sum <- self$add_weight(name = "mse_sum",
  prefs: []
  type: TYPE_NORMAL
- en: initializer = "zeros",
  prefs: []
  type: TYPE_NORMAL
- en: dtype = "float32")
  prefs: []
  type: TYPE_NORMAL
- en: self$total_samples <- self$add_weight(name = "total_samples",
  prefs: []
  type: TYPE_NORMAL
- en: initializer = "zeros",
  prefs: []
  type: TYPE_NORMAL
- en: dtype = "int32")
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: update_state = function(y_true, y_pred, sample_weight = NULL) {➍
  prefs: []
  type: TYPE_NORMAL
- en: num_samples <- tf$shape(y_pred)[1]
  prefs: []
  type: TYPE_NORMAL
- en: num_features <- tf$shape(y_pred)[2]
  prefs: []
  type: TYPE_NORMAL
- en: y_true <- tf$one_hot➎(y_true, depth = num_features)➏
  prefs: []
  type: TYPE_NORMAL
- en: mse <- sum((y_true - y_pred) ^ 2)➐
  prefs: []
  type: TYPE_NORMAL
- en: self$mse_sum$assign_add(mse)
  prefs: []
  type: TYPE_NORMAL
- en: self$total_samples$assign_add(num_samples)
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: result = function() {
  prefs: []
  type: TYPE_NORMAL
- en: sqrt(self$mse_sum /
  prefs: []
  type: TYPE_NORMAL
- en: tf$cast(self$total_samples, "float32"))➑
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: reset_state = function() {
  prefs: []
  type: TYPE_NORMAL
- en: self$mse_sum$assign(0)
  prefs: []
  type: TYPE_NORMAL
- en: self$total_samples$assign(0L)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We will be using tf module functions here.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Define a new class that subclasses the Metric base class.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Define the state variables in the constructor. Like for layers, you have
    access to the add_weight() method.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Implement the state update logic in update_state(). The y_true argument
    is the targets (or labels) for one batch, and y_pred represents the corresponding
    predictions from the model. You can ignore the sample_weight argument—we won't
    use it here.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Remember, tf module functions use 0-based counting conventions. A value
    of 0 in y_true places the 1 in the first position of the one-hot vector.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **To match our MNIST model, we expect categorical predictions and integer
    labels.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **We could also write this as tf$reduce_sum (tf$square(tf$subtract(y_true,
    y_pred))).**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Cast total_samples to match the dtype of mse_sum.**
  prefs: []
  type: TYPE_NORMAL
- en: Note that in update_state() we use tf$shape(y_pred) instead of y_pred$shape.
    tf$shape() returns the shape as a tf.Tensor, instead of a tf.TensorShape like
    y_pred$shape would. tf$shape() allows tf_function() to compile a function that
    can operate on tensors with undefined shapes, like our inputs here which have
    a undefined batch dimension. We learn more about tf_function() soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'You use the result() method to return the current value of the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: result = function()
  prefs: []
  type: TYPE_NORMAL
- en: sqrt(self$mse_sum /
  prefs: []
  type: TYPE_NORMAL
- en: tf$cast(self$total_samples, "float32"))
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, you also need to expose a way to reset the metric state without
    having to reinstantiate it—this enables the same metric objects to be used across
    different epochs of training or across both training and evaluation. You do this
    with the reset_state() method:'
  prefs: []
  type: TYPE_NORMAL
- en: reset_state = function() {
  prefs: []
  type: TYPE_NORMAL
- en: self$mse_sum$assign(0)
  prefs: []
  type: TYPE_NORMAL
- en: self$total_samples$assign(0L)➊
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Note that we pass an integer because total_samples has an integer dtype.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Custom metrics can be used just like built-in ones. Let’s test-drive our own
    metric:'
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_mnist_model()
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = list("accuracy", metric_root_mean_squared_error()))
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: fit(train_images, train_labels,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 3,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(val_images, val_labels))
  prefs: []
  type: TYPE_NORMAL
- en: test_metrics <- model %>% evaluate(test_images, test_labels)
  prefs: []
  type: TYPE_NORMAL
- en: You can now see the fit() progress bar displaying the RMSE of your model.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Using callbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Launching a training run on a large dataset for tens of epochs using the model
    fit() method can be a bit like launching a paper airplane: past the initial impulse,
    you don’t have any control over its trajectory or its landing spot. If you want
    to avoid bad outcomes (and thus wasted paper airplanes), it’s smarter to use not
    a paper plane but a drone that can sense its environment, send data back to its
    operator, and automatically make steering decisions based on its current state.
    The Keras *callbacks* API will help you transform your call to fit(model) from
    a paper airplane into a smart, autonomous drone that can self-introspect and dynamically
    take action.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A callback is an object (a class instance implementing specific methods) that
    is passed to the model in the call to fit() and that is called by the model at
    various points during training. It has access to all the available data about
    the state of the model and its performance, and it can take action: interrupt
    training, save a model, load a different weight set, or otherwise alter the state
    of the model. Here are some examples of ways you can use callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model checkpointing*—Saving the current state of the model at different points
    during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Early stopping*—Interrupting training when the validation loss is no longer
    improving (and, of course, saving the best model obtained during training).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dynamically adjusting the value of certain parameters during training*—Such
    as the learning rate of the optimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logging training and validation metrics during training, or visualizing the
    representations learned by the model as they’re updated*—The fit() progress bar
    that you’re familiar with is in fact a callback!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The keras package includes a number of built-in callbacks (this is not an exhaustive
    list):'
  prefs: []
  type: TYPE_NORMAL
- en: callback_model_checkpoint()
  prefs: []
  type: TYPE_NORMAL
- en: callback_early_stopping()
  prefs: []
  type: TYPE_NORMAL
- en: callback_learning_rate_scheduler()
  prefs: []
  type: TYPE_NORMAL
- en: callback_reduce_lr_on_plateau()
  prefs: []
  type: TYPE_NORMAL
- en: callback_csv_logger()
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review two of them to give you an idea of how to use them: callback_early_stopping()
    and callback_model_checkpoint().'
  prefs: []
  type: TYPE_NORMAL
- en: THE EARLY STOPPING AND MODEL CHECKPOINT CALLBACKS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you’re training a model, there are many things you can’t predict from the
    start. In particular, you can’t tell how many epochs will be needed to get to
    an optimal validation loss. Our examples so far have adopted the strategy of training
    for enough epochs that you begin overfitting, using the first run to figure out
    the proper number of epochs to train for, and then finally launching a new training
    run from scratch using this optimal number. Of course, this approach is wasteful.
    A much better way to handle this is to stop training when you measure that the
    validation loss is no longer improving. This can be achieved using callback_early_stopping().
  prefs: []
  type: TYPE_NORMAL
- en: 'The early stopping callback interrupts training once a target metric being
    monitored has stopped improving for a fixed number of epochs. For instance, this
    callback allows you to interrupt training as soon as you start overfitting, thus
    avoiding having to retrain your model for a smaller number of epochs. This callback
    is typically used in combination with callback_model_checkpoint(), which lets
    you continually save the model during training (and, optionally, save only the
    current best model so far: the version of the model that achieved the best performance
    at the end of an epoch).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 7.19 Using the callbacks argument in the fit() method
  prefs: []
  type: TYPE_NORMAL
- en: callbacks_list <- list(
  prefs: []
  type: TYPE_NORMAL
- en: callback_early_stopping(
  prefs: []
  type: TYPE_NORMAL
- en: monitor = "val_accuracy", patience = 2),➊
  prefs: []
  type: TYPE_NORMAL
- en: callback_model_checkpoint(➋
  prefs: []
  type: TYPE_NORMAL
- en: filepath = "checkpoint_path.keras",➌
  prefs: []
  type: TYPE_NORMAL
- en: monitor = "val_loss", save_best_only = TRUE)➍
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_mnist_model()
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")➎
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: train_images, train_labels,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callbacks_list,➏
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(val_images, val_labels))➐
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Interrupt training when validation accuracy has stopped improving for two
    epochs.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Save the current weights after every epoch.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Path to the destination model file**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **These two arguments mean you won't overwrite the model file unless val_loss
    has improved, which allows you to keep the best model seen during training.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **You monitor accuracy, so it should be part of the model's metrics.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Callbacks are passed to the model via the callbacks argument in fit(), which
    takes a list of callbacks. You can pass any number of callbacks.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Note that because the callback will monitor validation loss and validation
    accuracy, you need to pass validation_data to the call to fit().**
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you can always save models manually after training as well—just call
    save_ model_tf(model, ‘my_checkpoint_path’). To reload the model you’ve saved,
    just use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: model <- load_model_tf("checkpoint_path.keras")
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.3 Writing your own callbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you need to take a specific action during training that isn’t covered by
    one of the built-in callbacks, you can write your own callback. Callbacks are
    implemented by subclassing the Keras Callback class with new_callback_class().
    You can then implement any number of the following transparently named methods,
    which are called at various points during training:'
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_begin(epoch, logs)➊
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_end(epoch, logs)➋
  prefs: []
  type: TYPE_NORMAL
- en: on_batch_begin(batch, logs)➌
  prefs: []
  type: TYPE_NORMAL
- en: on_batch_end(batch, logs)➍
  prefs: []
  type: TYPE_NORMAL
- en: on_train_begin(logs)➎
  prefs: []
  type: TYPE_NORMAL
- en: on_train_end(logs)➏
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Called at the start of every epoch**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Called at the end of every epoch**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Called right before processing each batch**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Called right after processing each batch**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Called at the start of training**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Called at the end of training**
  prefs: []
  type: TYPE_NORMAL
- en: These methods are all called with a logs argument, which is a named list containing
    information about the previous batch, epoch, or training run—training and validation
    metrics, and so on. The on_epoch_* and on_batch_* methods also take the epoch
    or batch index as their first argument (an integer).
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a simple example that saves a list of per-batch loss values during training
    and saves a graph of these values at the end of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.20 Creating a custom callback by subclassing the Callback class**'
  prefs: []
  type: TYPE_NORMAL
- en: callback_plot_per_batch_loss_history <- new_callback_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "PlotPerBatchLossHistory",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(file = "training_loss.pdf") {
  prefs: []
  type: TYPE_NORMAL
- en: private$outfile <- file
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: on_train_begin = function(logs = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: private$plots_dir <- tempfile()
  prefs: []
  type: TYPE_NORMAL
- en: dir.create(private$plots_dir)
  prefs: []
  type: TYPE_NORMAL
- en: private$per_batch_losses <-
  prefs: []
  type: TYPE_NORMAL
- en: fastmap::faststack(init = self$params$steps)
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_begin = function(epoch, logs = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: private$per_batch_losses$reset()
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: on_batch_end = function(batch, logs = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: private$per_batch_losses$push(logs$loss)
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_end = function(epoch, logs = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: losses <- as.numeric(private$per_batch_losses$as_list())
  prefs: []
  type: TYPE_NORMAL
- en: filename <- sprintf("epoch_%04i.pdf", epoch)
  prefs: []
  type: TYPE_NORMAL
- en: filepath <- file.path(private$plots_dir, filename)
  prefs: []
  type: TYPE_NORMAL
- en: pdf(filepath, width = 7, height = 5)
  prefs: []
  type: TYPE_NORMAL
- en: on.exit(dev.off())
  prefs: []
  type: TYPE_NORMAL
- en: plot(losses, type = "o",
  prefs: []
  type: TYPE_NORMAL
- en: ylim = c(0, max(losses)),
  prefs: []
  type: TYPE_NORMAL
- en: panel.first = grid(),
  prefs: []
  type: TYPE_NORMAL
- en: main = sprintf("Training Loss for Each Batch\n(Epoch %i)", epoch),
  prefs: []
  type: TYPE_NORMAL
- en: xlab = "Batch", ylab = "Loss")
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: on_train_end = function(logs) {
  prefs: []
  type: TYPE_NORMAL
- en: private$per_batch_losses <- NULL
  prefs: []
  type: TYPE_NORMAL
- en: plots <- sort(list.files(private$plots_dir, full.names = TRUE))
  prefs: []
  type: TYPE_NORMAL
- en: qpdf::pdf_combine(plots, private$outfile)
  prefs: []
  type: TYPE_NORMAL
- en: unlink(private$plots_dir, recursive = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Growing R objects with fastmap::faststack()
  prefs: []
  type: TYPE_NORMAL
- en: Growing R vectors with c() or [[<- is typically slow and best avoided. In this
    example, we’re using fastmap::faststack() instead to collect the per-batch losses
    more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: private and self in custom class methods
  prefs: []
  type: TYPE_NORMAL
- en: In all the previous examples, we’ve used self to keep track of instance properties,
    but in this callback example we used private. What’s the difference? Any property
    like self$foo is also accessible directly from the class instance at instance$foo.
    Properties of private, however, are accessible only from inside class methods.
  prefs: []
  type: TYPE_NORMAL
- en: Another important difference is that Keras automatically converts everything
    assigned to self to a Keras native format. This helps Keras automatically find,
    for example, all the tf.Variables associated with a custom Layer. This automatic
    conversion, however, can sometimes have a performance impact, or even fail for
    certain types of R objects (like faststack()). private, on the other hand, is
    a plain R environment that is left untouched by Keras. Only the class methods
    you write will directly interact with private properties.
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_mnist_model()
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(train_images, train_labels,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = list(callback_plot_per_batch_loss_history()),
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(val_images, val_labels))
  prefs: []
  type: TYPE_NORMAL
- en: We get plots that look like [figure 7.5](#fig7-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0208-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.5 The output of our custom history plotting callback**'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.4 Monitoring and visualization with TensorBoard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To do good research or develop good models, you need rich, frequent feedback
    about what’s going on inside your models during your experiments. That’s the point
    of running experiments: to get information about how well a model performs—as
    much information as possible. Making progress is an iterative process, a loop:
    you start with an idea and express it as an experiment, attempting to validate
    or invalidate your idea. You run this experiment and process the information it
    generates. This inspires your next idea. The more iterations of this loop you’re
    able to run, the more refined and powerful your ideas become. Keras helps you
    go from idea to experiment in the least possible time, and fast GPUs can help
    you get from experiment to result as quickly as possible. But what about processing
    the experiment’s results? That’s where TensorBoard comes in (see [figure 7.6](#fig7-6)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0208-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.6 The loop of progress**'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard ([http://www.tensorflow.org/tensorboard](http://www.tensorflow.org/tensorboard))
    is a browser-based application that you can run locally. It’s the best way to
    monitor everything that goes on inside your model during training. With TensorBoard,
    you can
  prefs: []
  type: TYPE_NORMAL
- en: Visually monitor metrics during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize your model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize histograms of activations and gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore embeddings in 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you’re monitoring more information than just the model’s final loss, you
    can develop a clearer vision of what the model does and doesn’t do, and you can
    make progress more quickly. The easiest way to use TensorBoard with a Keras model
    and the fit() method is to use callback_tensorboard(). In the simplest case, just
    specify where you want the callback to write logs, and you’re good to go:'
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_mnist_model()
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = "rmsprop",
  prefs: []
  type: TYPE_NORMAL
- en: loss = "sparse_categorical_crossentropy", metrics = "accuracy")
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(train_images, train_labels,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 10,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data = list(val_images, val_labels),
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callback_tensorboard(log_dir = "logs/"))➊
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Path to your log dir**
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model starts running, it will write logs at the target location. You
    can then view the logs by calling tensorboard(); this will launch a browser with
    tensorboard running:'
  prefs: []
  type: TYPE_NORMAL
- en: tensorboard(log_dir = "logs/")➊
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Launches a browser with TensorBoard**
  prefs: []
  type: TYPE_NORMAL
- en: In the TensorBoard interface, you will be able to monitor live graphs of your
    training and evaluation metrics (see [figure 7.7](#fig7-7)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0209-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7.7 TensorBoard can be used for easy monitoring of training and evaluation
    metrics.**'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Writing your own training and evaluation loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fit() workflow strikes a nice balance between ease of use and flexibility.
    It’s what you will use most of the time. However, it isn’t meant to support everything
    a deep learning researcher may want to do, even with custom metrics, custom losses,
    and custom callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all, the built-in fit() workflow is solely focused on *supervised learning*:
    a setup where there are known *targets* (also called *labels* or *annotations*)
    associated with your input data, and where you compute your loss as a function
    of these targets and the model’s predictions. However, not every form of machine
    learning falls into this category. There are other setups where no explicit targets
    are present, such as *generative learning* (which we will discuss in chapter 12),
    *self-supervised learning* (where targets are obtained from the inputs), and *reinforcement
    learning* (where learning is driven by occasional “rewards,” much like training
    a dog). Even if you’re doing regular supervised learning, as a researcher, you
    may want to add some novel bells and whistles that require low-level flexibility.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever you find yourself in a situation where the built-in fit() is not enough,
    you will need to write your own custom training logic. You already saw simple
    examples of low-level training loops in chapters 2 and 3\. As a reminder, the
    contents of a typical training loop look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Run the forward pass (compute the model’s output) inside a gradient tape
    to obtain a loss value for the current batch of data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Retrieve the gradients of the loss with regard to the model’s weights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** Update the model’s weights so as to lower the loss value on the current
    batch of data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps are repeated for as many batches as necessary. This is essentially
    what fit() does under the hood. In this section, you will learn to reimplement
    fit() from scratch, which will give you all the knowledge you need to write any
    training algorithm you may come up with. Let’s go over the details.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.1 Training vs. inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the low-level training loop examples you’ve seen so far, step 1 (the forward
    pass) was done via predictions <- model(inputs), and step 2 (retrieving the gradients
    computed by the gradient tape) was done via gradients <- tape$gradient(loss, model$weights).
    In the general case, there are actually two subtleties you need to take into account.
  prefs: []
  type: TYPE_NORMAL
- en: Some Keras layers, such as layer_dropout(), have different behaviors during
    *training* and during *inference* (when you use them to generate predictions).
    Such layers expose a training Boolean argument in their call() method. Calling
    dropout (inputs, training = TRUE) will drop some activation entries, whereas calling
    dropout (inputs, training = FALSE) does nothing. By extension, Functional and
    Sequential models also expose this training argument in their call() methods.
    Remember to pass training = TRUE when you call a Keras model during the forward
    pass! Our forward pass thus becomes predictions <- model(inputs, training = TRUE).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, note that when you retrieve the gradients of the weights of your
    model, you should not use tape$gradients(loss, model$weights), but rather tape$gradients(loss,
    model$trainable_weights). Indeed, layers and models own two kinds of weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Trainable weights*—These are meant to be updated via backpropagation to minimize
    the loss of the model, such as the kernel and bias of a Dense layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nontrainable weights*—These are meant to be updated during the forward pass
    by the layers that own them. For instance, if you wanted a custom layer to keep
    a counter of how many batches it has processed so far, that information would
    be stored in a nontrainable weight, and at each batch, your layer would increment
    the counter by one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Among Keras built-in layers, the only layer that features nontrainable weights
    is layer_ batch_normalization(), which we will discuss in chapter 9\. The batch
    normalization layer needs nontrainable weights to track information about the
    mean and standard deviation of the data that passes through it, so as to perform
    an online approximation of *feature normalization* (a concept you learned about
    in chapter 6). Taking into account these two details, a supervised-learning training
    step ends up looking like this:'
  prefs: []
  type: TYPE_NORMAL
- en: library(tensorflow)
  prefs: []
  type: TYPE_NORMAL
- en: train_step <- function(inputs, targets) {
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- model(inputs, training = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: loss <- loss_fn(targets, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: gradients <- tape$gradients(loss, model$trainable_weights)
  prefs: []
  type: TYPE_NORMAL
- en: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We introduced zip_lists() in chapter 2.**
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.2 Low-level usage of metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a low-level training loop, you will probably want to leverage Keras metrics
    (whether custom ones or the built-in ones). You’ve already learned about the metrics
    API: simply call update_state(y_true, y_pred) for each batch of targets and predictions,
    and then use result() to query the current metric value:'
  prefs: []
  type: TYPE_NORMAL
- en: metric <- metric_sparse_categorical_accuracy()
  prefs: []
  type: TYPE_NORMAL
- en: targets <- c(0, 1, 2)
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- rbind(c(1, 0, 0),
  prefs: []
  type: TYPE_NORMAL
- en: c(0, 1, 0),
  prefs: []
  type: TYPE_NORMAL
- en: c(0, 0, 1))
  prefs: []
  type: TYPE_NORMAL
- en: metric$update_state(targets, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: current_result <- metric$result()
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("result: %.2f", as.array(current_result))➊'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "result: 1.00"'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **as.array() to convert Tensor to R value**
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also need to track the average of a scalar value, such as the model’s
    loss. You can do this via metric_mean():'
  prefs: []
  type: TYPE_NORMAL
- en: values <- c(0, 1, 2, 3, 4)
  prefs: []
  type: TYPE_NORMAL
- en: mean_tracker <- metric_mean()
  prefs: []
  type: TYPE_NORMAL
- en: for (value in values)
  prefs: []
  type: TYPE_NORMAL
- en: mean_tracker$update_state(value)
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("Mean of values: %.2f", as.array(mean_tracker$result()))'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] "Mean of values: 2.00"'
  prefs: []
  type: TYPE_NORMAL
- en: Remember to use metric$reset_state() when you want to reset the current results
    (at the start of a training epoch or at the start of evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.3 A complete training and evaluation loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s combine the forward pass, backward pass, and metrics tracking into a fit()-like
    training step function that takes a batch of data and targets and returns the
    logs that would get displayed by the fit() progress bar.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.21 Writing a step-by-step training loop: The training step function**'
  prefs: []
  type: TYPE_NORMAL
- en: model <- get_mnist_model()
  prefs: []
  type: TYPE_NORMAL
- en: loss_fn <- loss_sparse_categorical_crossentropy()➊
  prefs: []
  type: TYPE_NORMAL
- en: optimizer <- optimizer_rmsprop()➋
  prefs: []
  type: TYPE_NORMAL
- en: metrics <- list(metric_sparse_categorical_accuracy())➌
  prefs: []
  type: TYPE_NORMAL
- en: loss_tracking_metric <- metric_mean()➍
  prefs: []
  type: TYPE_NORMAL
- en: train_step <- function(inputs, targets) {
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- model(inputs, training = TRUE)➎
  prefs: []
  type: TYPE_NORMAL
- en: loss <- loss_fn(targets, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: gradients <- tape$gradient(loss,➏
  prefs: []
  type: TYPE_NORMAL
- en: model$trainable_weights)➏
  prefs: []
  type: TYPE_NORMAL
- en: optimizer$apply_gradients(zip_lists(gradients,
  prefs: []
  type: TYPE_NORMAL
- en: model$trainable_weights))
  prefs: []
  type: TYPE_NORMAL
- en: logs <- list()
  prefs: []
  type: TYPE_NORMAL
- en: for (metric in metrics) {➐
  prefs: []
  type: TYPE_NORMAL
- en: metric$update_state(targets, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: logs[[metric$name]] <- metric$result()
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: loss_tracking_metric$update_state(loss)➑
  prefs: []
  type: TYPE_NORMAL
- en: logs$loss <- loss_tracking_metric$result()
  prefs: []
  type: TYPE_NORMAL
- en: logs➒
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Prepare the loss function.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Prepare the optimizer.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Prepare the list of metrics to monitor.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Prepare a metric_mean() tracker to keep track of the loss average.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Run the forward pass. Note that we pass training = TRUE.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Run the backward pass. Note that we use model$trainable_weights.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Keep track of metrics.**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Keep track of the loss average.**
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Return the current values of the metrics and the loss.**
  prefs: []
  type: TYPE_NORMAL
- en: We will need to reset the state of our metrics at the start of each epoch and
    before running evaluation. Here’s a utility function to do it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.22 Writing a step-by-step training loop: Resetting the metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: reset_metrics <- function() {
  prefs: []
  type: TYPE_NORMAL
- en: for (metric in metrics)
  prefs: []
  type: TYPE_NORMAL
- en: metric$reset_state()
  prefs: []
  type: TYPE_NORMAL
- en: loss_tracking_metric$reset_state()
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: We can now lay out our complete training loop. Note that we use a TensorFlow
    Data-set object from the tfdatasets package to turn our R array data into an iterator
    that iterates over the data in batches of size 32\. The mechanics are identical
    to the dataset iterator we implemented in chapter 2, just the names are different
    now. We build the TensorFlow Dataset instance from our R arrays with tensor_slices_dataset(),
    convert it to an iterator with as_iterator(), and then repeatedly call iter_next()
    on the iterator to get the next batch. One difference between what we saw in chapter
    2 and now is that iter_next() returns Tensor objects, not R arrays. We cover much
    more about tfdatasets in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.23 Writing a step-by-step training loop: The loop itself**'
  prefs: []
  type: TYPE_NORMAL
- en: library(tfdatasets)
  prefs: []
  type: TYPE_NORMAL
- en: training_dataset <-
  prefs: []
  type: TYPE_NORMAL
- en: list(inputs = train_images, targets = train_labels) %>%
  prefs: []
  type: TYPE_NORMAL
- en: tensor_slices_dataset() %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_batch(32)
  prefs: []
  type: TYPE_NORMAL
- en: epochs <- 3
  prefs: []
  type: TYPE_NORMAL
- en: for (epoch in seq(epochs)) {
  prefs: []
  type: TYPE_NORMAL
- en: reset_metrics()
  prefs: []
  type: TYPE_NORMAL
- en: training_dataset_iterator <- as_iterator(training_dataset)
  prefs: []
  type: TYPE_NORMAL
- en: repeat {
  prefs: []
  type: TYPE_NORMAL
- en: batch <- iter_next(training_dataset_iterator)
  prefs: []
  type: TYPE_NORMAL
- en: if (is.null(batch))➊
  prefs: []
  type: TYPE_NORMAL
- en: break
  prefs: []
  type: TYPE_NORMAL
- en: logs <- train_step(batch$inputs, batch$targets)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: writeLines(c(
  prefs: []
  type: TYPE_NORMAL
- en: sprintf("Results at the end of epoch %s", epoch),
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
  prefs: []
  type: TYPE_NORMAL
- en: ))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Results at the end of epoch 1
  prefs: []
  type: TYPE_NORMAL
- en: '…sparse_categorical_accuracy: 0.9156'
  prefs: []
  type: TYPE_NORMAL
- en: '…loss: 0.2687'
  prefs: []
  type: TYPE_NORMAL
- en: Results at the end of epoch 2
  prefs: []
  type: TYPE_NORMAL
- en: '…sparse_categorical_accuracy: 0.9539'
  prefs: []
  type: TYPE_NORMAL
- en: '…loss: 0.1659'
  prefs: []
  type: TYPE_NORMAL
- en: Results at the end of epoch 3
  prefs: []
  type: TYPE_NORMAL
- en: '…sparse_categorical_accuracy: 0.9630'
  prefs: []
  type: TYPE_NORMAL
- en: '…loss: 0.1371'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **iterator exhausted**
  prefs: []
  type: TYPE_NORMAL
- en: 'And here’s the evaluation loop: a simple for loop that repeatedly calls a test_step()
    function, which processes a single batch of data. The test_step() function is
    just a subset of the logic of train_step(). It omits the code that deals with
    updating the weights of the model—that is to say, everything involving the GradientTape()
    and the optimizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.24 Writing a step-by-step evaluation loop**'
  prefs: []
  type: TYPE_NORMAL
- en: test_step <- function(inputs, targets) {
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- model(inputs, training = FALSE)➊
  prefs: []
  type: TYPE_NORMAL
- en: loss <- loss_fn(targets, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: logs <- list()
  prefs: []
  type: TYPE_NORMAL
- en: for (metric in metrics) {
  prefs: []
  type: TYPE_NORMAL
- en: metric$update_state(targets, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: logs[[paste0("val_", metric$name)]] <- metric$result()
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: loss_tracking_metric$update_state(loss)
  prefs: []
  type: TYPE_NORMAL
- en: logs[["val_loss"]] <- loss_tracking_metric$result()
  prefs: []
  type: TYPE_NORMAL
- en: logs
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: val_dataset <- list(val_images, val_labels) %>%
  prefs: []
  type: TYPE_NORMAL
- en: tensor_slices_dataset() %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_batch(32)
  prefs: []
  type: TYPE_NORMAL
- en: reset_metrics()
  prefs: []
  type: TYPE_NORMAL
- en: val_dataset_iterator <- as_iterator(val_dataset)
  prefs: []
  type: TYPE_NORMAL
- en: repeat {
  prefs: []
  type: TYPE_NORMAL
- en: batch <- iter_next(val_dataset_iterator)
  prefs: []
  type: TYPE_NORMAL
- en: if(is.null(batch)) break➋
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs_batch, targets_batch) %<-% batch
  prefs: []
  type: TYPE_NORMAL
- en: logs <- test_step(inputs_batch, targets_batch)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: writeLines(c(
  prefs: []
  type: TYPE_NORMAL
- en: '"Evaluation results:",'
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
  prefs: []
  type: TYPE_NORMAL
- en: ))
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '…val_sparse_categorical_accuracy: 0.9461'
  prefs: []
  type: TYPE_NORMAL
- en: '…val_loss: 0.1871'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Note that we pass training = FALSE.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **iter_next() returns NULL once the dataset batch iterator is exhausted.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Congrats—you’ve just reimplemented fit() and evaluate()! Or almost: fit() and
    evaluate() support many more features, including large-scale distributed computation,
    which requires a bit more work. It also includes several key performance optimizations.
    Let’s take a look at one of these optimizations: TensorFlow function compilation.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.4 Make it fast with tf_function()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed that your custom loops are running significantly slower
    than the built-in fit() and evaluate(), despite implementing essentially the same
    logic. That’s because, by default, TensorFlow code is executed line by line, *eagerly*,
    much like regular R code using R arrays. Eager execution makes it easier to debug
    your code, but it is far from optimal from a performance standpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s more performant to *compile* your TensorFlow code into a *computation
    graph* that can be globally optimized in a way that code interpreted line by line
    cannot. The syntax to do this is very simple: just call tf_function() on any function
    you want to compile before executing, as shown in the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 7.25 Using tf_function() with our evaluation-step function**'
  prefs: []
  type: TYPE_NORMAL
- en: tf_test_step <- tf_function(test_step)➊
  prefs: []
  type: TYPE_NORMAL
- en: val_dataset_iterator <- as_iterator(val_dataset)➋
  prefs: []
  type: TYPE_NORMAL
- en: reset_metrics()
  prefs: []
  type: TYPE_NORMAL
- en: while(!is.null(iter_next(val_dataset_iterator) -> batch)) {
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs_batch, targets_batch) %<-% batch
  prefs: []
  type: TYPE_NORMAL
- en: logs <- tf_test_step(inputs_batch, targets_batch)➌
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: writeLines(c(
  prefs: []
  type: TYPE_NORMAL
- en: '"Evaluation results:",'
  prefs: []
  type: TYPE_NORMAL
- en: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
  prefs: []
  type: TYPE_NORMAL
- en: ))
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '…val_sparse_categorical_accuracy: 0.5190'
  prefs: []
  type: TYPE_NORMAL
- en: '…val_loss: 1.6764'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Pass the test_step we defined previously to tf_function().**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Reuse the same TF Dataset defined in the previous example, but make a new
    iterator.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Use the compiled test step function this time.**
  prefs: []
  type: TYPE_NORMAL
- en: On my machine we go from taking 2.4 seconds to run the evaluation loop to only
    0.6 seconds. Much faster!
  prefs: []
  type: TYPE_NORMAL
- en: 'The speedup is even greater when the TF Dataset iteration loop is also compiled
    as a graph operation. You can use tf_function() to compile the full evaluation
    loop like this:'
  prefs: []
  type: TYPE_NORMAL
- en: my_evaluate <- tf_function(function(model, dataset) {
  prefs: []
  type: TYPE_NORMAL
- en: reset_metrics()
  prefs: []
  type: TYPE_NORMAL
- en: for (batch in dataset) {
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs_batch, targets_batch) %<-% batch
  prefs: []
  type: TYPE_NORMAL
- en: logs <- test_step(inputs_batch, targets_batch)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: logs
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: system.time(my_evaluate(model, val_dataset))["elapsed"]
  prefs: []
  type: TYPE_NORMAL
- en: elapsed
  prefs: []
  type: TYPE_NORMAL
- en: '0.283'
  prefs: []
  type: TYPE_NORMAL
- en: This cuts the evaluation time by more than half again!
  prefs: []
  type: TYPE_NORMAL
- en: Remember, while you are debugging your code, prefer running it eagerly, without
    any calls to tf_function(). It’s easier to track bugs this way. Once your code
    is working and you want to make it fast, add a tf_function() decorator to your
    training step and your evaluation step or any other performance-critical function.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4.5 Leveraging fit() with a custom training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we were writing our own training loop entirely from
    scratch. Doing so provides you with the most flexibility, but you end up writing
    a lot of code while simultaneously missing out on many convenient features of
    fit(), such as call-backs or built-in support for distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you need a custom training algorithm, but you still want to leverage
    the power of the built-in Keras training logic? There’s actually a middle ground
    between fit() and a training loop written from scratch: you can provide a custom
    training step function and let the framework do the rest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do this by overriding the train_step() method of the Model class. This
    is the function that is called by fit() for every batch of data. You will then
    be able to call fit() as usual, and it will be running your own learning algorithm
    under the hood. Here’s a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a new class that subclasses Model by calling new_model_class().
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We override the method train_step( data). Its contents are nearly identical
    to what we used in the previous section. It returns a named list mapping metric
    names (including the loss) to their current values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implement a metrics active property that tracks the model’s Metric instances.
    This enables the model to automatically call reset_state() on the model’s metrics
    at the start of each epoch and at the start of a call to evaluate(), so you don’t
    have to do it by hand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loss_fn <- loss_sparse_categorical_crossentropy()
  prefs: []
  type: TYPE_NORMAL
- en: loss_tracker <- metric_mean(name = "loss")➊
  prefs: []
  type: TYPE_NORMAL
- en: CustomModel <- new_model_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "CustomModel",
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(data) {➋
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs, targets) %<-% data
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- self(inputs, training = TRUE)➌
  prefs: []
  type: TYPE_NORMAL
- en: loss <- loss_fn(targets, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: gradients <- tape$gradient(loss, model$trainable_weights)
  prefs: []
  type: TYPE_NORMAL
- en: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
  prefs: []
  type: TYPE_NORMAL
- en: loss_tracker$update_state(loss)➍
  prefs: []
  type: TYPE_NORMAL
- en: list(loss = loss_tracker$result())➎
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = mark_active(function() list(loss_tracker))➏
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **This metric object will be used to track the average of per-batch losses
    during training and evaluation.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We override the train_step method.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **We use self(inputs, training = TRUE) instead of model(inputs, training =
    TRUE), because our model is the class itself.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We update the loss tracker metric that tracks the average of the loss.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **We return the average loss so far by querying the loss tracker metric.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Any metric you would like to reset across epochs should be listed here.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now instantiate our custom model, compile it (we pass only the optimizer,
    because the loss is already defined outside of the model), and train it using
    fit() as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(28 * 28))
  prefs: []
  type: TYPE_NORMAL
- en: features <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(512, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- features %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model <- CustomModel(inputs = inputs, outputs = outputs)➊
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = optimizer_rmsprop())
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(train_images, train_labels, epochs = 3)
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Because we didn''t provide an initialize() method, the same signature as
    keras_model() is used: inputs, outputs, and optionally name.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of points to note:'
  prefs: []
  type: TYPE_NORMAL
- en: This pattern does not prevent you from building models with the Functiona API.
    You can do this whether you’re building Sequential models, Functional API models,
    or subclassed models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t need to call tf_function() when you override train_step—the framework
    does it for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, what about metrics, and what about configuring the loss via compile()?
    After you’ve called compile(), you get access to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: self$compiled_loss—The loss function you passed to compile().
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: self$compiled_metrics—A wrapper for the list of metrics you passed, which allows
    you to call self$compiled_metrics$update_state() to update all of your metrics
    at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: self$metrics—The actual list of metrics you passed to compile(). Note that it
    also includes a metric that tracks the loss, similar to what we did manually with
    our loss_tracking_metric earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can thus write:'
  prefs: []
  type: TYPE_NORMAL
- en: CustomModel <- new_model_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "CustomModel",
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(data) {
  prefs: []
  type: TYPE_NORMAL
- en: c(inputs, targets) %<-% data
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: predictions <- self(inputs, training = TRUE)
  prefs: []
  type: TYPE_NORMAL
- en: loss <- self$compiled_loss(targets, predictions)➊
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: gradients <- tape$gradient(loss, model$trainable_weights)
  prefs: []
  type: TYPE_NORMAL
- en: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
  prefs: []
  type: TYPE_NORMAL
- en: self$compiled_metrics$update_state(
  prefs: []
  type: TYPE_NORMAL
- en: targets, predictions)➋
  prefs: []
  type: TYPE_NORMAL
- en: results <- list()
  prefs: []
  type: TYPE_NORMAL
- en: for(metric in self$metrics)
  prefs: []
  type: TYPE_NORMAL
- en: results[[metric$name]] <- metric$result()
  prefs: []
  type: TYPE_NORMAL
- en: results➌
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Compute the loss via self$compiled_loss.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Update the model's metrics via self$compiled_metrics.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Return a named list mapping metric names to their current value.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape = c(28 * 28))
  prefs: []
  type: TYPE_NORMAL
- en: features <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(512, activation = "relu") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.5)
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- features %>% layer_dense(10, activation = "softmax")
  prefs: []
  type: TYPE_NORMAL
- en: model <- CustomModel(inputs = inputs, outputs = outputs)
  prefs: []
  type: TYPE_NORMAL
- en: model %>% compile(optimizer = optimizer_rmsprop(),
  prefs: []
  type: TYPE_NORMAL
- en: loss = loss_sparse_categorical_crossentropy(),
  prefs: []
  type: TYPE_NORMAL
- en: metrics = metric_sparse_categorical_accuracy())
  prefs: []
  type: TYPE_NORMAL
- en: model %>% fit(train_images, train_labels, epochs = 3)
  prefs: []
  type: TYPE_NORMAL
- en: That was a lot of information, but you now know enough to use Keras to do almost
    anything.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras offers a spectrum of different workflows, based on the principle of *progressive
    disclosure of complexity*. They all smoothly operate together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build models via the Sequential API with keras_model_sequential(), via
    the Functional API with keras_model(), or by subclassing the Model class with
    new_model_class(). Most of the time, you’ll be using the Functional API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simplest way to train and evaluate a model is via the default fit() and
    evaluate() methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras callbacks provide a simple way to monitor models during your call to fit()
    and automatically take action based on the state of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also fully take control of what fit() does by overriding the train_
    step() method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond fit(), you can also write your own training loops entirely from scratch.
    This is useful for researchers implementing brand-new training algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
