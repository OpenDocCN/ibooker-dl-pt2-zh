- en: 2 Clustering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this second chapter, we are going to cover the following topics:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clustering techniques and salient use cases in the industry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various clustering algorithms available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means, hierarchical clustering, and DBSCAN clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of algorithms in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study on cluster analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*“Simplicity is the ultimate sophistication”* – Leonardo da Vinci'
  prefs: []
  type: TYPE_NORMAL
- en: Nature loves simplicity, and teaches us to follow the same path. Most of the
    time, our decisions are simple choices. Simple solutions are easier to comprehend,
    less time consuming, and painless to maintain and ponder over. The machine learning
    world is no different. An elegant machine learning solution is not one which is
    the most complicated algorithm available, but one which solves the business problem.
    A robust machine learning solution is easy enough to readily decipher and pragmatic
    enough to implement. Clustering solutions are generally easier to be understood.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we defined unsupervised learning and discussed the
    various unsupervised algorithms available. We will cover each of those algorithms
    as we work through this book; in this second chapter we are going to focus in
    on the first of these: Clustering algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: We will define clustering first and then study the different types of clustering
    techniques. We will examine the mathematical foundation, accuracy measurements,
    and pros and cons of each algorithm. We will implement three of these algorithms
    using Python code on a dataset to complement the theoretical knowledge. The chapter
    ends with the various use cases of clustering techniques in the pragmatic business
    scenario to prepare for the actual business world. This technique is being followed
    throughout the book where we study the concepts first, implement the actual code
    to enhance the Python skills, and dive into real-world business problems.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to study basic clustering algorithms in this chapter which are
    K-means clustering, hierarchical clustering, and DBSCAN clustering. These clustering
    algorithms are generally the starting points whenever we want to study clustering.
    In the later chapters of the book, we are going to explore more complex algorithms
    like spectrum clustering, Gaussian Mixture Models, time series clustering, fuzzy
    clustering etc. If you have a good understanding of K-means clustering, hierarchical
    clustering, and DBSCAN – you can skip to the next chapter. Still, it is advisable
    to read the chapter once – you might find something useful to refresh your concepts!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first understand what we mean by “*clustering*”. All the very best on
    your journey to master unsupervised learning based clustering techniques!
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are using the 3.6+ version of Python in this chapter. A basic understanding
    of Python and code execution is expected. You are advised to refresh concepts
    of object-oriented programming and Python concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we are using Jupyter notebook to execute the code. Jupyter
    offers flexibility in execution and debugging, hence it is being used. It is quite
    user-friendly and is platform or operating system agnostic. So if you are using
    Windows or Mac OS or Linux, Jupyter should work just fine.
  prefs: []
  type: TYPE_NORMAL
- en: All the datasets and code files are checked-in to the Github repository at ([https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter2](main.html)).
    You need to install the following Python libraries to execute the code – `numpy`,
    `pandas`, `matplotlib`, `scipy`, `sklearn`. CPU is good enough for execution,
    but if you face some computing lags, and would like to speed up the execution,
    switch to GPU or Google Collaboratory (colab). Google colab offers free-of-cost
    computation for machine learning solutions. You are advised to study more about
    Google Colab and how to use it for training the machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are starting with clustering in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider this scenario. A group of children are asked to group the items in
    a room into different segments. Each child can use their own logic. Someone might
    club the objects based on the weight, other children might use material while
    someone might use all three - weight, material, and colour. The permutations are
    many and depend on the *parameters* used for grouping. Here, a child is segmenting
    or *clustering* the objects based on the chosen logic.
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, *clustering* is used to group objects with similar attributes
    in the same segments, and the objects with different attributes in different segments.
    The resultant clusters share similarities within themselves while they are more
    heterogeneous between each other. We can understand it better by means of the
    following diagram (Figure 2.1).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 Clustering is grouping of objects with similar attributes into logical
    segments. The grouping is based on a similarity trait shared by different observations
    and hence they are grouped into a group. We are using shape as a variable for
    clustering here.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_01](images/02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster analysis is not one individual algorithm or solution, rather it is used
    as a problem solving mechanism in practical business scenarios. They are a class
    of algorithms under unsupervised learning. It is an iterative process following
    a logical approach and qualitative business inputs. It results in generating a
    thorough understanding of the data, logical patterns in it, pattern discovery,
    and information retrieval. Being an unsupervised approach, clustering does not
    need a target variable. It performs segmenting by analysing underlying patterns
    in the dataset which are generally multi-dimensional and hence difficult to analyse
    with traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally we would want the clustering algorithms to have the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: The output clusters should be easy to explain and comprehend, usable and should
    make business sense. The number of clusters should not be too little or too much.
    For example, if we have only 2 clusters the division is not clear and decisive.
    Or if we have 20 clusters, the handling will become a challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm should not be too sensitive to outliers or missing values or the
    noise in the dataset. Generally put, a good solution will be able to handle multiple
    data types.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A good solution will require less domain understanding for the input parameters
    used for the clustering purpose. It allows analysts with less domain understanding
    to train the clustering algorithm.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm should be independent of the order of the input parameters. If
    the order matters, the clustering is biased on the order and hence it will add
    more confusion to the process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As we generate new datasets continuously, the clusters have to be scalable to
    newer training examples and should not be a time-consuming process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As one could imagine, the clustering output will depend on the attributes used
    for grouping. In the (Figure 2.2) shown below, there can be two logical groupings
    for the same dataset and both are equally valid. Hence, it is prudent that the
    attributes or *variables* for clustering are chosen wisely and often it depends
    on the business problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 Using different attributes for clustering results in different clusters
    for the same dataset. Hence, choosing the correct set of attributes define the
    final set of results we will achieve
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_02](images/02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Along with the attributes used in clustering, the actual technique used also
    makes a lot of difference. There are quite a few (in fact more than 80) clustering
    techniques researchers have worked upon. For the interested audience, we are providing
    a list of all the clustering algorithms in the Appendix. We are starting with
    understanding different clustering techniques in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Clustering techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clustering can be achieved using a variety of algorithms. These algorithms use
    different methodologies to define similarity between objects. For example, density
    based clustering, centroid based clustering, distribution based methods etc. Even
    to measure the distance between objects, there are multiple techniques like Euclidean
    distance, Manhattan distance etc. The choice of distance measurement leads to
    different similarity scores. We are going to study these similarity measurement
    parameters in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level we can identify two broad clustering methods: *hard clustering*
    and *soft clustering* (see Figure 2.3). When the decision is quite clear that
    an object belongs to a certain class or cluster it is referred as Hard clustering.
    In hard clustering an algorithm is quite sure of an object’s class. On the other
    hand, soft clustering assigns a likelihood score for an object to belong to a
    particular cluster. So a soft clustering method will not put an object into a
    cluster, rather an object can belong to multiple clusters. Soft clustering sometimes
    is also called *fuzzy* clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 Hard clustering has distinct clusters whereas in the case of soft
    clustering, a data point can belong to multiple clusters and we get likelihood
    score for a data point to belong to a cluster. The first figure on the left is
    hard clustering and the one on the right is soft clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_03](images/02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can broadly classify the clustering techniques as shown in the (Table 2.1)
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 Classification of clustering methodologies, brief descriptions and
    examples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| S. No. | Clustering methodology | A brief description of the method | Example
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Centroid based clustering | Distance from a defined centroid | k-means
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Density based models | Data points are connected in dense regions in
    a vector space | DBSCAN, OPTICS |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Connectivity based clustering | Distance connectivity is the modus operandi
    | Hierarchical clustering, BIRCH |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Distribution models | Modelling is based on statistical distributions
    | Gaussian Mixture models |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Deep learning models | Unsupervised neural network based | Self-organizing
    maps |'
  prefs: []
  type: TYPE_TB
- en: The methods described in (Table 2.1) are not the only ones which are available
    to be used. We can have graph-based models, overlapping clustering, subspace models
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the popular six algorithms used in clustering in the industry are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering (with variants like k-medians, k-medoids)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Agglomerative clustering or hierarchical clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spectral Clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gaussian mixture models or GMM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BIRCH (Balanced Iterative Reducing & Clustering using Hierarchies)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are multiple other algorithms available like Chinese whisper, canopy clustering,
    SUBCLU, FLAME etc. We are studying the first three algorithms in this chapter
    and some of the advanced ones in subsequent chapters in the book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   DBSCAN clustering is centroid-based clustering technique. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   Clustering is a supervised learning technique having a fixed target variable.
    TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   What is the difference between hard clustering and soft clustering?
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are starting with the centroid based clustering methods
    where we will study k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Centroid based clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Centroid (*see Appendix if you are not sure what is centroid*) based algorithms
    measure similarity of the objects based on their distance to the centroid of the
    clusters. The distance is measured between a specific data point to the centroid
    for the cluster. The smaller the distance is, higher is the similarity. We can
    understand the concept by looking at Figure 2.4 that follows. The figure on the
    right side represents the respective centroids for each of the group of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To get more clarity on the concept of centroid and other mathematical concepts,
    refer to the appendix at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 Centroid based clustering methods create a centroid for the respective
    clusters and the similarity is measured based on the distance from the centroid.
    In this case, we have 5 centroids. And hence, we have five distinct clusters here
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_03a](images/02_03a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In clustering, distance plays a central part as many algorithms use it as a
    metric to measure the similarity. In centroid-based clustering, distance is measured
    between points and between centroids. There are multiple ways to measure the distance.
    The most widely used are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean distance**: It is the most common distance metric used. It represents
    the straight line distance between the two points in space and is the shortest
    path between the two points. If we want to calculate the distance between points
    P[1] and P[2] where coordinates of P[1] are (x[1], y[1]) and P[2] are (x[2], y[2]),
    then Euclidean distance is given by (Equation 2.1) below. The geometric representation
    is shown in Figure 2.5'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Equation 2.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Distance = √(y[2] – y[1])² + (x[2] – x[1])²
  prefs: []
  type: TYPE_NORMAL
- en: If you want to refresh the concepts of geometry (coordinate geometry) refer
    to the Appendix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '**Chebyshev distance**: Named after Russian mathematician *Pafnuty Chebyshev*,
    it is defined as the distance between two points such that their differences are
    maximum value along any co-ordinate dimension. Mathematically, we can represent
    Chebyshev distance in (Equation 2.2) below and shown in (Figure 2.5):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Equation 2.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Distance [Chebyshev] = max (|y[2] – y[1]|, |x[2] – x[1]|)
  prefs: []
  type: TYPE_NORMAL
- en: '**Manhattan distance**: Manhattan distance is a very easy concept. It simply
    calculates the distance between two points in a grid-like path and the distance
    is hence measured along the axes at right angles. Hence, sometimes it is also
    referred to as city block distance or taxi cab metric. Mathematically, we can
    represent Manhattan distance in (Equation 2.3) and as shown in Figure 2.5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Equation 2.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Distance [Manhattan] = (|y[2] – y[1]| + |x[2] – x[1]|)
  prefs: []
  type: TYPE_NORMAL
- en: Manhattan distance in L1 norm form while Euclidean distance is L2 norm form.
    You can refer to the Appendix to study the L1 norm and L2 norm in detail. If we
    have high number of dimensions or variables in the dataset, Manhattan distance
    is a better choice than Euclidean distance. This is due to *Curse of Dimensionality*
    which we will be studying in Chapter 3 of the book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cosine distance**: Cosine distance is used to measure the similarity between
    two points in a vector-space diagram. In trigonometry, cosine of 0 is 1 and cosine
    of 90^o is 0\. Hence, if two points are similar to each other, the angle between
    them will be zero hence cosine will be 1 which means the two points are very similar
    to each other, and vice versa. Mathematically, cosine similarity can be shown
    as (Equation 2.4). If we want to measure the cosine between two vectors A and
    B, then cosine is'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Equation 2.4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Distance [cosine] = (A . B) / (||A|| ||B||)
  prefs: []
  type: TYPE_NORMAL
- en: If you want to refresh the concepts of vector factorization refer to the Appendix.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 2.5 Euclidean distance, Manhattan distance, Chebyshev distance and cosine
    similarity are the primary distance metrics used. Note, how the distance is different
    for two points using these metrics. In Euclidean distance, the direct distance
    is measured between two points as shown by the first figure on the left.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_03b](images/02_03b.png)'
  prefs: []
  type: TYPE_IMG
- en: There are other distance measuring metrics like Hamming distance, Jaccard distance
    etc. Mostly, we use Euclidean distance in our pragmatic business problems but
    other distance metrics are also used sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The above distance metrics are true for other clustering algorithms too. You
    are advised to test the Python codes in the book with different distance metrics
    and compare the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have understood the various distance metrics, we will proceed to study
    k-means clustering which is the most widely used algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 K-means clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: k-means clustering is an easy and straightforward approach. It is arguably the
    most widely used clustering method to segment the data points and create non-overlapping
    clusters. We have to specify the number of clusters “k” we wish to create as an
    input and the algorithm will associate each observation to exactly one of the
    k clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: k-means clustering is sometimes confused with k-nearest neighbor classifier
    (knn). Though there is some relationship between the two, knn is used for classification
    and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite an elegant approach and starts with some initial cluster centers
    and then iterates to assign each observation to the closest centre. In the process
    the centers are re-calculated as the mean of points in the cluster. Let’s study
    the approach used in step-by-step fashion by using the diagram in (Figure 2.6)
    below. For the sake of simplicity, we are assuming that there are three clusters
    in the dataset below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Let us assume that we have all the data points as shown below in
    Step 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 Step 1 represents the raw data set. In step 2, the algorithm initiates
    random three centroids as we have given the input of a number of clusters as three.
    In step 3, all the neighboring points of the centroids, are assigned the same
    cluster
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_04](images/02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 2**: The *three* centers are initialized randomly as shown by three
    squares – blue, red and green. This input of three is the final number of clusters
    we wish to have.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: The distance of all the data points is calculated to the centers
    and the points are assigned to the nearest centre. Note that the points have attained
    blue, red and green colors as they are nearest to those respective centers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4**: The three centers are re-adjusted in this step. The centers are
    re-calculated as the mean of the points in that cluster as shown in Figure 2.7\.
    We can see that in Step 4, the three squares have changed their respective positions
    as compared to Step 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 The centroids are re-calculated in step 4\. In step 5, the data points
    are again re-assigned new centers. In step 6, the centroids are again re-adjusted
    as per the new calculations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_05](images/02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 5**: The distance of all the data points is re-calculated to the new
    centres and the points are reassigned to the nearest centres again. Note that
    two blue data points have become red while a red point has become green in this
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6**: The centers are again re-adjusted as they were done in Step 4.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 The centroids are re-calculated and this process continues till we
    can no longer improve the clustering. And then the process stops as shown in step
    8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_06](images/02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 7**: The data points are again assigned a new cluster as shown in the
    preceding figure (Figure 2.8).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8**: And the process will continue till the convergence is achieved.
    In other words, the process continues till there is no more reassignment of the
    data points. And hence, we cannot improve the clustering further and the final
    clustering is achieved.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of k-means clustering is to ensure that the within-cluster variation
    is as small as possible while the difference between clusters is as big as possible.
    In other words, the members of the same cluster are most similar to each other
    while members in different clusters are dissimilar. Once the results no longer
    change, we can conclude that a local optimum has been reached and clustering can
    stop. Hence, the final clusters are homogeneous within themselves while heterogeneous
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is imperative to note two points here:'
  prefs: []
  type: TYPE_NORMAL
- en: Since k-means clustering initializes the centers randomly, hence it finds a
    local optimum solution rather than a global optimum solution. Hence, it is advisable
    to iterate the solution multiple times and choose the best output from all the
    results. By iteration, we mean to repeat the process multiple times as in each
    of the iteration, the centroid chosen randomly will be different.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have to input the number of final clusters “k” we wish to have and it changes
    the output drastically. A very small value of k relative to the data size, will
    result in redundant clusters as there will not be any use. Or in other words,
    if we have a very small value of k relative to a big sized data, data points with
    different characteristics will be cobbled together in a few groups. Having a very
    high value of k, will create clusters which are different from each other minutely.
    Moreover, having a very high number of clusters will be difficult to manage and
    refresh in the long run. Let’s study by an example. If a telecom operator has
    1 million subscribers, then if we take number of clusters as 2 or 3, the resultant
    cluster size will be very large. It can also lead to different customers classified
    in the same segment. On the other hand, if we take the number of clusters as 50
    or 60, due to the sheer number of clusters – the output becomes unmanageable to
    manage, analyze and maintain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With different values of “k” we get different results, hence it is necessary
    that we understand how we can choose the optimum number of clusters for a dataset.
    Now, lets examine the process to measure the accuracy of clustering solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Measure the accuracy of clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One objective of clustering is to find the cleanest clusters. Theoretically
    (though not ideal), if we have the same number of clusters as the number of observations
    the results will be completely accurate. In other words, if we have 1 million
    customers, the purest clustering will have 1 million clusters – wherein each customer
    is in a separate cluster. But it is not the best approach and is not a pragmatic
    solution. Clustering intends to create group of similar observations in one cluster
    and we use the same principle to measure the accuracy of our solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Within the cluster sum of squares (WCSS) or Cohesion**: This index measures
    the variability of the data points with respect to the distance they are from
    the centroid of the cluster. This metric is average distance of each data point
    from the cluster’s centroid, which is repeated for each data point. If the value
    is too large, it shows there is a large data spread whereas the smaller value
    indicates that the data points are quite similar and homogeneous and hence the
    cluster is compact.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes, this intracluster distance is also referred to as *inertia* for that
    cluster. It is simply the summation of all the distances. Lower the value of inertia,
    better the cluster is.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 Intra cluster vs inter cluster distance – both are used to measure
    the purity of the final clusters and the performance of the clustering solution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_07](images/02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Inter cluster sum of squares**: This metric is used to measure the distance
    between centroids of all the clusters. To get it, we measure the distance between
    centroids of all the clusters and divide it by the number of clusters to get the
    average value. The bigger it is, better is the clustering indicating that clusters
    are heterogeneous and distinguishable from each other as we have represented in
    (Figure 2.9).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Silhouette Value** is one of the metrics used to measure the success of clustering.
    It ranges from -1 to +1 and a higher value is better. It measures how an data
    point is similar to other data points in its own cluster as compared to other
    clusters. As a first step, for each observation - we calculate the average distance
    from all the data points in the same cluster, let’s call is x[i]. Then we calculate
    the average distance from all the data points in the nearest cluster, let’s call
    it y[i]. We will then calculate the coefficient by the equation (Equation 2.5)
    below'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Equation 2.5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Silhouette Coefficient = (y[i] – x[i])/ max (y[i], x[i])
  prefs: []
  type: TYPE_NORMAL
- en: If the value of coefficient is -1, it means that the observation is in the wrong
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If it is 0, the observation is very close to the neighboring clusters.
  prefs: []
  type: TYPE_NORMAL
- en: If the values of coefficient +1, it means that the observation is at a distance
    from the neighboring clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we would expect to get the highest value for the coefficient to have
    a good clustering solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dunn Index** can also be used to measure the efficacy of the clustering.
    It uses the inter and intra distance measurements defined in point 2 and point
    3 above and is given by the (Equation 2.6) below'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Equation 2.6)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dunn Index = min (Inter cluster distance)/max (Intra cluster distance)
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, we would strive to maximize the value of Dunn index. To achieve it,
    the numerator should be as big as possible implying that clusters are at a distance
    from each other, while the denominator should be as low as possible signifying
    that the clusters are quite robust and close-packed.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have examined the methods to measure the performance of our algorithm.
    We will now move to find out the best value of “k” for k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Finding the optimum value of “k”
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Choosing the most optimum number of clusters is not easy. As we have said earlier,
    the finest clustering is when the number of clusters equals the number of observations
    – but as we studied in the last section, it is not practically possible. But we
    have to provide the number of clusters “k” as an input to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 Elbow method to find the optimal number of clusters. The red circle
    shows the kink. But the final number of clusters is dependent on business logic
    and often we merge/split clusters as per business knowledge. Ease to maintain
    the clusters also plays a crucial role in the same
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_08](images/02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Perhaps the most widely used method for finding the optimum value of “k” is
    the *Elbow Method.* In this method, we calculate within the cluster sum of squares
    or WCSS for different values of “k”. The process is the same as discussed in the
    last section. Then, WCSS is plotted on a graph against different values of “k”.
    Wherever we observe a kink or elbow, as shown in (Figure 2.10), it is the most
    optimum number of clusters for the dataset. Notice the sharp edge depicted in
    (Figure 2.10).
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   K-means clustering does not require number of clusters as an input- TRUE
    or FALSE
  prefs: []
  type: TYPE_NORMAL
- en: 2.   Knn and k-means clustering are one and the same thing – TRUE or FALSE
  prefs: []
  type: TYPE_NORMAL
- en: 3.   Describe one possible process to find the most optimal value of “k”
  prefs: []
  type: TYPE_NORMAL
- en: But it does not mean that it is the final number of clusters we suggest for
    the business problem. Based on the number of observations falling in each of the
    clusters, a few clusters might be combined or broken into sub-clusters. We also
    consider the computation cost required to create the clusters. Higher the number
    of clusters, greater is the computation cost and the time required.
  prefs: []
  type: TYPE_NORMAL
- en: We can also find the optimum number of clusters using the Silhouette Coefficient
    we discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is imperative that business logic of merging a few clusters or breaking a
    few clusters is explored. Ultimately, the solution has to be implemented in real-world
    business scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have examined nuts and bolts of k-means clustering – the mathematical
    concepts and the process, the various distance metrics and determining the best
    value of k. We will now study the advantages k-means algorithm offers to us.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Pros and cons of k-means clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'k-means algorithm is quite a popular and widely implemented clustering solution.
    The solution offers the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It is simple to comprehend and relatively easier to implement as compared to
    other algorithms. The distance measurement calculation makes it quite intuitive
    to understand even by users from non-statistics backgrounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the number of dimensions is large, k-means algorithm is faster than other
    clustering algorithms and creates tighter clusters. It is hence preferred if the
    number of dimensions are quite big.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It quickly adapts to new observations and can generalize very well to clusters
    of various shapes and sizes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution produces results through a series of iterations of re-calculations.
    Most of the time the Euclidean distance metric is used which makes it less computationally
    expensive. It also ensures that the algorithm surely converges and produces results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K-means is widely used for real life business problems. Though there are clear
    advantages of k-means clustering, we do face certain challenges with the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the most optimum number of clusters is not easy. We have to provide
    it as an input. With different values of “k”, the results will be completely different.
    The process to choose the best value of “k” is explored in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution is dependent on the initial values of centroids. Since the centroids
    are initialized randomly, the output will be different with each iteration. Hence,
    it is advisable to run multiple versions of the solution and choose the best one.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is quite sensitive to outliers. They can mess up the final results
    and hence it is imperative that we treat outliers before starting with clustering.
    We can also implement other variants of k-means algorithm like *k-modes* clustering
    to deal with the issue of outliers. We are discussing dealing with outliers in
    subsequent chapters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the basic principle of k-means clustering is to calculate the distance,
    hence the solution is not directly applicable for categorical variables. Or in
    other words, we cannot use categorical variables directly, since we can calculate
    the distance between numeric values but cannot perform mathematical calculations
    on categorical variables. To resolve it, we can convert categorical variables
    to numeric ones using one-hot encoding which we are discussing towards the end
    of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these problems, k-means clustering is one of the most used clustering
    solutions owing to its simplicity and ease to implement. There are different implementations
    of k-means algorithm like k-medoids, k-median etc. which are sometimes used to
    resolve the problems faced.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, **k-median clustering** is based on medians of the dataset
    as compared to centroid in k-means. This increases the amount of computation time
    as median can be found only after the data has been sorted. But at the same time,
    k-means is sensitive to outliers whereas k-medians is less affected by them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we have **k-medoids clustering** as one of the variants of the k-means
    algorithm. Medoids are similar to means except they are always from the same dataset
    and are implemented when it is difficult to get means like images. A medoid can
    be thought as the most central point in a cluster which is least dissimilar to
    all the other members in the cluster. K-medoids choose the actual observations
    as the centers as compared to k-means where the centroids may not even be part
    of the data. It is less sensitive to outliers as compared to k-means clustering
    algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are other versions too like kmeans++, mini-batch k-means etc. Generally,
    in the industry kmeans is used for most of the clustering solutions. You can explore
    other options like kmeans++, mini-batch kmeans etc. if the results are not desirable
    or if the computation is taking a lot of time. Moreover, having different distance
    measurement metrics may produce different results for k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This section concludes our discussion on k-means clustering algorithm. It is
    time to hit the lab and develop actual Python code!
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.5 k-means clustering implementation using Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now create a Python solution for k-means clustering. In this case, we
    are using the dataset from the link at github at
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter2](main.html)'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset has information about features of four models of cars. Based on
    the features of the car, we are going to group them into different clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Import the libraries and the dataset into a dataframe. Here, vehicles.csv
    is the input data file. If the data file is not in the same folder as the Jupyter
    notebook, you would have to provide the complete path to the file. Dropna is used
    to remove the missing values, if any.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2:** Perform some initial checks on the data, like shape, info, top
    five rows, distribution of classes etc. This is to ensure that we have loaded
    the complete dataset and there is no corruption while loading the dataset. `Shape`
    command will give the number of rows and columns in the data, `info` will describe
    all the variables and their respective types and `head` will display the first
    5 rows. The `value_counts` displays the distribution for the `class` variable.
    Or in other words, `value_counts` returns the count of the unique values.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3:** Let’s generate two plots for the variable `“class”`. The dataset
    has more examples from car while for bus and van it is a balanced data. We have
    used matplotlib library to plot these graphs. The output of the plots are shown
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![02_08a](images/02_08a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 4:** We will now check if there are any missing data points in our dataset.
    There are no missing data points in our dataset as we have already dealt with
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will be discussing the methods to deal with missing values in later chapters
    as dropping the missing values is generally not the best approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5:** We will standardize our dataset now. It is a good practice to standardize
    the dataset for clustering. It is important as the different dimensions might
    be on a different scale, and one dimension may dominate the computation of distance
    if its values are naturally much larger than other dimensions. This is done using
    `zscore` and `StandardScaler()` function below. Refer to the appendix of the book
    to examine the difference between `zscore` and `StandardScaler()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6:** We will now have a quick look at the dataset by generating a scatter
    plot. The plot displays the distribution of all the data points we have created
    as `X_standard` in the last step.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![02_08b](images/02_08b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 7:** We will now perform k-means clustering. First, we have to select
    the optimum number of clusters using the elbow method. From the sklearn library,
    we are importing KMeans. In a for loop, we iterate for the values of clusters
    from 1 to 10\. In other words, the algorithm will create 1, 2,3, 4 up to 10 clusters
    and will then generate the results for us to choose the most optimal value of
    k.'
  prefs: []
  type: TYPE_NORMAL
- en: In the code snippet below, the model object contains the output of the KMeans
    algorithm which is then fit on the X_standard generated in the last step. Here,
    Euclidean distance has been used as a distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![02_08c](images/02_08c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 8:** As we can observe, the optimal number of clusters is 3\. It is
    the point, where we can observe a sharp kink in the graph. We will continue with
    k-means clustering with a number of clusters as 3\. While there is nothing special
    about the number 3 here, it is best suited for this dataset, we might also use
    4 or 5 for the number of clusters. `random_state` is a parameter that is used
    to determine random numbers for centroid initialization. We are setting it to
    a value to make randomness deterministic.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9:** Get the centroids for the clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10:** Now we are using the centroids so that they can be profiled by
    the columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 11:** We will now create a `dataframe` only for the purpose of creating
    the labels and then we are converting it into categorical variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 12:** In this step, we are joining the two `dataframes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 13:** A group by is done to create a data frame requires for the analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 14:** Now, will create a visualization for the clusters we have defined.
    This is done using the `mpl_toolkits` library. The logic is simple to understand.
    The data points are coloured as per the respective labels. The rest of the steps
    are related to the display of plot by adjusting the label, title, ticks etc. Since
    it is not possible to plot all the 18 variables in the plot, we have chosen 3
    variables to show in the plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![02_08d](images/02_08d.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also test the above code with multiple other values of k. We have created
    the code with different values of k. In the interest of space, we have put the
    code of testing with different values of k at the github location.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, we first did a small exploratory analysis of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA) holds the key to a robust machine learning solution
    and a successful project. In the subsequent chapters, we will create detailed
    EDA for datasets.
  prefs: []
  type: TYPE_NORMAL
- en: It was followed by identifying the optimum number of clusters which in this
    case comes out to be three. Then we implemented k-means clustering. You are expected
    to iterate the k-means solution with different initializations and compare the
    results, iterate with different values of k, and visualize to analyze the movements
    of data points. We will be using the same dataset later in the chapter where we
    will create hierarchical clustering using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid-based clustering is one of the most recommended solutions owing to
    its less complicated logic, ease to implement, flexibility and trouble-free maintenance.
    Whenever we require clustering as a solution, mostly we start with creating a
    k-means clustering solution that acts as a benchmark. The algorithm is highly
    popular and generally one of the first solutions utilized for clustering. Then
    we test and iterate with other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of the discussion on centroid-based clustering algorithms.
    We will now move forward to connectivity-based solutions and discuss hierarchical
    clustering in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Connectivity based clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Birds of the same feather flock together” is the principle followed in connectivity-based
    clusters. The core concept is - objects which are connected with each other are
    similar to each other. Hence, based on the connectivity between these objects
    they are clubbed into clusters. An example of such a representation is shown in
    Figure 2.11, where we can iteratively group observations. As an example, we are
    initiating with all things, dividing into living and non-living and so on. Such
    representation is better shown using the diagram on the right, called the *Dendrogram.*
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 Hierarchical clustering utilizes grouping similar objects iteratively.
    On the right, we have the visual representation of the clustering called dendrogram
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_09](images/02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Since there is a tree-like structure, connectivity-based clustering is sometimes
    referred as *Hierarchical* clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering fits nicely into human intuition, and hence is easy
    to comprehend by us. Unlike k-means clustering, in hierarchical clustering, we
    do not have to input the number of final clusters but the method does require
    a termination condition i.e. when the clustering should stop. At the same time,
    hierarchical clustering does not suggest the optimum number of clusters. From
    the hierarchy/ dendrogram generated, we have to choose the best number of clusters
    ourselves. We will explore more on it when we create the Python code for it in
    subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering can be understood by means of Figure 2.12, which follows.
    Here the first node is the root, which is then iteratively split into nodes and
    subnodes. Whenever a node cannot be split further, it is called a terminal node
    or *leaf*.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 Hierarchical clustering has a root that splits into nodes and subnodes.
    A node that cannot be split further is called the leaf. In the bottom-up approach,
    merging of the leaves will take place
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_10](images/02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since there is more than one process or logic to merge the observations into
    clusters, we can generate a large number of dendrograms which is given by (Equation
    2.7) below:'
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 2.7)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Number of dendrograms = (2n-3)!/[2^((n-2)) (n-2)!]
  prefs: []
  type: TYPE_NORMAL
- en: where n is the number of observations or the leaves. So if we have only 2 observations,
    we can have only 1 dendrogram. If we have 5 observations, we can have 105 dendrograms.
    Hence, based on the number of observations we can generate a lot of dendograms.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering can be further classified based on the process used
    to create grouping of observations, which we are exploring next.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Types of hierarchical clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the strategy to group, hierarchical clustering can be subdivided into
    two types: *agglomerative* clustering and *divisive* clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '| S.No. | Agglomerative methodology | Divisive methodology |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Bottom-up approach | Top-down approach |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Each observation creates its own cluster and then merging takes place
    as the algorithm goes up | We start with one cluster and then observations are
    iteratively split to create a tree-like structure |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Greedy approach is followed to merge (greedy approach is described below)
    | Greedy approach is followed to split |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | An observation will find the best pair to merge and the process completes
    when all the observations have merged with each other | All the observations are
    taken at the start and then based on division conditions, splitting takes place
    until all the observations are exhausted or the termination condition is met |'
  prefs: []
  type: TYPE_TB
- en: Figure 2.13 Step followed in hierarchical clustering. Left-to-right we have
    agglomerative clustering (splitting of the nodes) while right-to-left we have
    divisive clustering (merging of the nodes)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_11](images/02_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s explore the meaning of *greedy approach* first. Greedy approach or greedy
    algorithm is any algorithm which makes a best choice at each step without considering
    the impact on the future states. In other words, we live in-the-moment and choose
    the best option from the available choices at that moment. The current choice
    is independent of the future choices and the algorithm will solve the subproblems
    later. Greedy approach may *not* provide the most optimal solution but generally
    provides a locally optimal solution which is close to the optimal solution in
    a reasonable time. Hierarchical clustering follows this greedy approach while
    merging or splitting at a node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now examine the steps followed in hierarchical clustering approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** As shown in (Figure 2.13) above, let us say we have five observations
    in our data set – 1, 2, 3, 4 and 5.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2:** In this step, observation 1 and 2 are grouped into one, 4 and 5
    are clubbed in one. 3 is not clubbed in any one.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3:** Now in this step, we group the output of 4,5 in the last step and
    observation 3 into one cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4:** The output from step 3 is clubbed with the output of 1,2 as a single
    cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, from left-to-right, we have an agglomerative approach and
    from right-to-left a divisive approach is represented. In an agglomerative approach,
    we are merging the observations while in a divisive approach we are splitting
    the observations. We can use both agglomerative or divisive approaches for hierarchical
    clustering. Divisive clustering is an exhaustive approach and sometimes might
    take more time than the other.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to k-means clustering, the distance metric used to measure plays a significant
    role here. We are aware and understand how to measure the distance between data
    points but there are multiple methods to define that distance which we are studying
    now.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Linkage criterion for distance measurement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are aware that we can use Euclidean distance or Manhattan distance or Chebyshev
    distance etc. to measure the distance between two observations. At the same time,
    we can employ various methods to define that distance. And based on this input
    criterion, the resultant clusters will be different. The various methods to define
    the distance metric are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nearest neighbors or single linkages** use the distance between the two nearest
    points in different clusters. The distance between the closest neighbors in distinct
    clusters is calculated and it is used to determine the next split/merging. It
    is done by an exhaustive search among all the pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Farthest neighbor or complete linkage** is opposite of the nearest neighbor
    approach. Here, instead of taking the nearest neighbors we concentrate on most-distant
    neighbors in different clusters. In other words, we determine the distance between
    the clusters is calculated by the greatest distance between two objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group average linkage** calculates the average of distances between all the
    possible pairs of objects in two different clusters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward linkage** method aims to minimize the variance of the clusters which
    are getting merged into one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use these options of distance metrics while we are developing the actual
    code for hierarchical clustering, and compare the accuracies to determine the
    best distance metrics for the dataset. During the algorithm training, the algorithm
    merges the observations which will minimize the linkage criteria chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Such inputs to the algorithm are referred to as hyper-parameters. These are
    the parameters we feed to the algorithm to generate the results as per our requirement.
    An example of hyperparameter is “k” in k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualise the various linkages in Figure 2.14 below.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 (i) Single linkage is for closest neighbors (ii) Complete linkage
    is for farthest neighbors and (iii) Group average is for average of the distance
    between clusters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_12](images/02_12.png)'
  prefs: []
  type: TYPE_IMG
- en: With this, we have understood the working mechanisms in hierarchical clustering.
    But we have still not addressed the mechanism to determine the optimum number
    of clusters using hierarchical clustering, which we are examining next.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3 Optimal number of clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall in k-means clustering we have to give the number of clusters as an input
    to the algorithm. We use elbow method to determine the optimum number of clusters.
    In the case of hierarchical clustering, we do not have to specify the number of
    clusters to the algorithm, but still we have to identify the number of final clusters
    we wish to have. We use a dendrogram to answer that problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let us assume that we have 10 data points in total at the bottom of the chart
    as shown in Figure 2.15\. The clusters are merged iteratively till we get the
    one final cluster at the top. The height of the dendrogram at which two clusters
    get merged with each other represents the respective distance between the said
    clusters in the vector-space diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 Dendrogram to identify the optimum number of clusters. The distance
    between X and Y is more than A & B and P & Q, hence we choose that as the cut
    to create clusters and number of clusters chosen are 5\. The x-axis represents
    the clusters while the y-axis represents the distance (dissimilarity) between
    two clusters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_13](images/02_13.png)'
  prefs: []
  type: TYPE_IMG
- en: From a dendrogram, the number of clusters is given by the number of vertical
    lines being cut by a horizontal line. The *optimum* number of clusters is given
    by the number of the vertical lines in the dendrogram cut by a horizontal line
    such that it intersects the tallest of the vertical lines. Or if the cut is shifted
    from one end of the vertical line to another, the length hence covered is the
    maximum. A dendrogram utilizes branches of clusters to show how closely various
    data points are related to each other. In a dendrogram, clusters that are located
    at the same height level are more closely related than clusters that are located
    at different height levels.
  prefs: []
  type: TYPE_NORMAL
- en: In the example shown in Figure 2.15, we have shown three potential cuts – AB,
    PQ and XY. If we take a cut above AB it will result in two very broad clusters
    while below PQ will result in nine clusters which will become difficult to analyze
    further.
  prefs: []
  type: TYPE_NORMAL
- en: Here the distance between X and Y is more than A & B and P & Q. So we can conclude
    that the distance between X and Y is the maximum and hence we can finalize that
    as the best cut. This cut, intersects at five distinct points hence we should
    have five clusters. Hence, the height of the cut in the dendrogram is similar
    to the value of k in k-means clustering. In k-means clustering, “k” determines
    the number of clusters. In hierarchical clustering, the best cut determines the
    number of clusters we wish to have.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to k-means clustering, the final number of clusters is not dependent
    on the choice from the algorithm only. The business acumen and the pragmatic logic
    play a vital role in determining the final number of clusters. Recall that one
    of the important attributes of clusters is their usability which we discussed
    in section 2.2 earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   What is the greedy approach used in hierarchical clustering?
  prefs: []
  type: TYPE_NORMAL
- en: 2.   Complete linkage is used for finding distances for closest neighbors –
    TRUE or FALSE
  prefs: []
  type: TYPE_NORMAL
- en: 3.   What is the difference between group linkage and ward linkage?
  prefs: []
  type: TYPE_NORMAL
- en: 4.   Describe the process to find the most optimal value of “k”
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered the background of hierarchical clustering and how we determine
    the clusters. We will now discuss the advantages and challenges we face with hierarchical
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.4 Pros and cons of hierarchical clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hierarchical clustering is a strong clustering technique and quite popular
    too. Similar to k-means, it is also using distance as a metric to measure the
    similarity. At the same time, there are a few challenges with the algorithm. We
    are discussing pros and cons of hierarchical clustering now. The advantages of
    hierarchical clustering are:'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the biggest advantage we have with hierarchical clustering is reproducibility
    of results. Recall in k-means clustering, the process starts with random initialization
    of centroids giving different results. In hierarchical clustering we can reproduce
    the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In hierarchical clustering, we do not have to input the number of clusters to
    segment the data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation is easy to implement and comprehend. Since it follows a tree-like
    structure, it is explainable to users from non-technical backgrounds.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The dendrogram generated can be interpreted to generate a very good understanding
    of the data with a visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the same time, we do face some challenges with hierarchical clustering algorithm
    which are:'
  prefs: []
  type: TYPE_NORMAL
- en: The biggest challenge we face with hierarchical clustering is the time taken
    to converge. The time complexity for k-means is linear while for hierarchical
    clustering is quadratic. For example, if we have “n” data points, then for k-means
    clustering the time complexity will be O(n) while for hierarchical clustering
    is O(n³).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can refer to the appendix of the book if you want to study O(n)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since the time complexity is O(n³), it is a time-consuming task. Moreover, the
    memory required to compute is at least O(n²) making hierarchical clustering quite
    a time consuming and memory intensive process. And this is the issue even if the
    dataset is medium. The computation required might not be a challenge if we are
    using really high-end processors but surely can be a concern for regular computers
    we use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interpretation of dendrograms at times can be subjective hence due diligence
    is required while interpretation of dendrograms. The key to interpret a dendrogram
    is to focus on the height at which any two data points are connected with each
    other. It can be subjective as different analysts can decipher different cuts
    and try to prove their methodology. Hence, it is advisable to interpret the results
    in the light of mathematics and marry the results with real-world business problem.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The hierarchical clustering cannot undo the previous steps it has done. In case,
    we feel that a connection made is not proper and should be rolled back, still
    there is no mechanism to remove the connection.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is very sensitive to outliers and messy dataset. Presence of outliers,
    NULL, missing values, duplicates etc. make a dataset messy. And hence the resultant
    output might not be proper and not what we expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But despite all the challenges, hierarchical clustering is one of the most widely
    used clustering algorithms. Generally, we create both k-means clustering and hierarchical
    clustering for the same dataset to compare the results of the two. If the number
    of clusters suggested and the distribution of respective clusters look similar,
    we get more confident on the clustering methodology used.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the theoretical understanding of hierarchical clustering. It
    is time for action and jump into Python for coding.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.5 Hierarchical clustering case study using Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now create a Python solution for hierarchical clustering, using the
    same dataset we used for k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps 1-6:** Load the required libraries and dataset. For this, follow the
    steps 1 to 6 we have followed in k-means algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7:** Next, we are going to create hierarchical clustering using three
    linkages methods – average, ward and complete. Then the clusters are getting plotted.
    The input to the method is the X_Standard variable, linkage method used and the
    distance metric. Then, using matplotlib library, we are plotting the dendrogram.
    In the code snippet, simply change the method from ‘average’ to ‘ward’ and ‘complete’
    and get the respective results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![02_13a](images/02_13a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 8:** We now want to choose the number of clusters we wish to have. For
    this purpose, let’s recreate the dendrogram by sub-setting the last 10 merged
    clusters. We have chosen 10 as it is generally an optimal choice, you are advised
    to test with other values too.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![02_13b](images/02_13b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 9:** We can observe that the most optimal distance is 10.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 10:** Cluster the data into different groups. By using the logic described
    in the last section, the number of optimal clusters is coming to be four.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 11:** Plot the distinct clusters using matplotlib library.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![02_13c](images/02_13c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 12:** For different values of distance, the number of clusters will
    change and hence the plot will look different. We are showing different results
    for distances of 5, 15 20, and different numbers of clusters generated for each
    iteration. Here, we can observe that we get completely different results for different
    values of distances while we move from left to right. We have to be cautious while
    we choose the value of the distance and sometimes, we might have to iterate a
    few times to get the best value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![02_13d](images/02_13d.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, we can observe that using hierarchical clustering, we have segmented
    the data on the left side to the one on the right side of Figure 2\. below. The
    left side is the representation of the raw data while on the right we have a representation
    of the clustered dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![02_13e](images/02_13e.png)'
  prefs: []
  type: TYPE_IMG
- en: Hierarchical clustering is a robust method and is a highly recommended one.
    Along with k-means, it creates a great foundation for clustering-based solutions.
    Most of the time, at least these two techniques are worked upon when we create
    clustering solutions. And then we move to iterate with other methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of the discussion on connectivity-based clustering algorithms.
    We will now move forward to density-based solutions and discuss DBSCAN clustering
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Density based clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have studied k-means in the earlier sections. Recall how it uses a centroid-based
    method to assign a cluster to each of the data points. If an observation is an
    outlier, the outlier point pulls the centroid towards itself and is also assigned
    a cluster like a normal observation. These outliers do not necessarily bring information
    to the cluster and can impact other data points disproportionally but are still
    made a part of the cluster. Moreover, getting clusters of arbitrary shape as shown
    in Figure 2.16 is a challenge with the k-means algorithm. Density based clustering
    methods solve the problem for us.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 DBSCAN is highly-recommended for irregular shaped clusters. With
    k-means we generally get spherical clusters, DBSCAN can resolve it for us
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_14](images/02_14.png)'
  prefs: []
  type: TYPE_IMG
- en: In a density-based clustering algorithm, we resolve all of these problems. In
    the density-based method, the clusters are identified as the areas which have
    a higher density as compared to the rest of the dataset. In other words, given
    a vector-space diagram where the data points are represented – a cluster is defined
    by adjacent regions or neighboring regions of high-density points. This cluster
    will be separated from other clusters by regions of low-density points. The observations
    in the sparse areas or separating regions are considered as noise or outliers
    in the dataset. A few examples of density-based clustering are shown in (Figure
    2.16).
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned two terms - “neighborhood” and “density”. To understand density-based
    clustering, we will study these terms in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Neighborhood and density
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine we represent data observations in a vector-space. And we have a point
    P. We now define the neighborhood for this point P. The representation is shown
    in Figure 2.17 below.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17 Representation of data points in a vector-space diagram. On the
    right-side we have a point P and the circle drawn is of radius ε. So, for ε >
    0, the neighborhood of P is defined by the set of points which are at less than
    equal to ε distance from the point P
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_15](images/02_15.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can make out from Figure 2.17 above, for a point P we have defined a ε
    - neighborhoods for it which are the points equidistant from P. In a 2-D space,
    it is represented by a circle, in a 3-D space it is a sphere, and for a n-dimensional
    space it is n-sphere with center P and radius ε. This defines the concept of *neighborhood*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore the term, *density*. Recall density is mass divided by volume
    (mass/volume). Higher the mass, the higher the density, and the lower the mass,
    the lower the density. Conversely, the lower the volume, the higher the density,
    and the higher the volume, the lower the density.
  prefs: []
  type: TYPE_NORMAL
- en: In the context above, mass is the number of points in the neighborhood. In (Figure
    2.18) below, we can observe the impact of ε on the number of data points or the
    mass.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.18 The impact of radius ε, on the left side the number of points is
    more than on the right-side. So, the mass of right side is less since it contains
    a smaller number of data points
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_16](images/02_16.png)'
  prefs: []
  type: TYPE_IMG
- en: When it comes to volume, in the case of 2-d space, volume is πr², while for
    a sphere that is three-dimensional, it is 4/3 πr³. For spheres of n-dimensions,
    we can calculate the respective volume as per the number of dimensions which will
    be π times a numerical constant raised to the number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the two cases shown in (Figure 2.18), for a point “P” we can get the
    number of points (mass) and volumes and then we can calculate the respective densities.
    But the absolute values of these densities mean nothing to us, but how they are
    similar (or different) from nearby areas. It is used to cluster the points having
    similar densities. Or in other words, the points which are in the same neighborhood
    and have similar densities can be clubbed in one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal case scenario, we wish to have highly dense clusters having maximum
    number of points. In the two cases shown in (Figure 2.19) below, we have a less
    dense cluster depicted on the left and high-dense one on the right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.19 Denser clusters are preferred over less dense ones. Ideally a dense
    cluster, with maximum number of data points is what we aim to achieve from clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_17](images/02_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the discussion above, we can conclude that:'
  prefs: []
  type: TYPE_NORMAL
- en: If we *increase* the value of ε, we will get a *higher* volume but not necessarily
    a *higher* number of points (mass). It depends on the distribution of the data
    points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, if we *decrease* the value of ε, we will get a *lower* volume but
    not necessarily a *lower* number of points (mass).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are the fundamental points we adhere to. Hence, it is imperative that
    while choosing the clusters we choose clusters that have high density and cover
    the maximum number of neighboring points.
  prefs: []
  type: TYPE_NORMAL
- en: We have hence concluded the concepts for density-based clustering. These concepts
    are the building blocks for DBSCAN clustering which we are discussing next!
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 DBSCAN Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Density-Based Spatial Clustering of Applications with Noise or DBSCAN clustering
    is a one of the highly recommended density-based algorithms. It clusters the data
    observations which are closely packed in a densely populated area but not considering
    the outliers in low-density regions. Unlike k-means, we do not specify the number
    of clusters and the algorithm is able to identify irregular-shaped clusters whereas
    k-means generally proposes spherical-shaped clusters. Similar to hierarchical
    clustering, it works by connecting the data points but with the observations which
    satisfy the density-criteria or the threshold value. The more can be understood
    in the steps we are describing below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DBSCAN was proposed in 1996 by Martin Ester, Hans-Peter Kriegal, Jörg Sander
    and Xiaowei Xu. The algorithm was awarded the test of time award in 2014 at ACM
    SIGKDD. The paper can be assessed at [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.1980](viewdoc.html).
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN works on the concepts of neighborhood we discussed in the last section.
    We will now dive deep into the working methodology and building blocks of DBSCAN.
  prefs: []
  type: TYPE_NORMAL
- en: nuts and bolts of DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will now examine the core building blocks of DBSCAN clustering. We know it
    is a density-based clustering algorithm and hence neighborhood concept is applicable
    over here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider we have a few data observations which we need to cluster. We also
    locate a data point “P”. Then, we can easily define two hyperparameter terms:'
  prefs: []
  type: TYPE_NORMAL
- en: The radius of the neighborhood around P, known as *ε*, which we have discussed
    in the last section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The minimum number of points we wish to have in the neighborhood of P or in
    other words, minimum number of points that are required to create a dense region.
    This is referred to as *minPts*. And is one of the parameters we can input by
    applying a threshold on minPts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Based on the concepts above, we can classify the observations into three broad
    categories - core points, border or reachable points, and outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core points**: any data point “x” can be termed as a core point if at least
    minPts are within ε distance of it (including x itself), shown as squares in (Figure
    2.20) below. They are the building blocks of our clusters and hence are called
    core. We use the same value of radius (ε) for each point and hence the *volume*
    of each neighborhood remains constant. But the number of points will vary and
    hence the *mass* varies. Consequently therefore, the density varies as well. Since
    we put a threshold using minPoints, we are putting a limit on density. So, we
    can conclude that core points fulfil the minimum density threshold requirement.
    It is imperative to note that we can choose different values of ε and minPts to
    iterate and fine-tune the clusters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.20 Core points are shown in square, border points are shown in filled
    circle while noise is unfilled circles. Together these three are the building
    blocks for DBSCAN clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_18](images/02_18.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Border points or reachable points**: a point that is not a core point in
    the clusters is called a border point, shown as filled circles in Figure 2.20.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A point “y” is directly reachable from x if y is within ε distance of core point
    x. A point can only be approached from a core point and it is the primary condition
    or rule to be followed. Only a core-point can reach a non-core point and the opposite
    is not true. In other words, a non-core point can only be reached by other core-points,
    it cannot reach anyone else. In Figure 2.20, border points are represented as
    dark circles.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the process better, we have to understand the term *density-reachable*
    or *connectedness*. As shown in (Figure 2.21) below, we have two core points X
    and Y. We can directly go from X to Y. Point Z is not in the neighborhood of X
    but is the neighborhood of Y. So, we cannot directly reach Z from X. But we can
    surely reach Z from X through Y or in other words using neighborhood of Y, we
    can travel to Z from X. We cannot go from Z to Z since, Z is the border point
    and as described earlier, we cannot travel from a border point.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.21 X and Y are the core points and we can travel from X to Y. Though
    Z is not in the immediate neighborhood of X, we can still reach Z from X through
    Y. It is the core concept of density-connected points
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![02_19](images/02_19.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Outliers**: all the other points are outliers. In other words, if it is not
    a core point or is not a reachable point, it is an outlier, shown as unfilled
    circles in (Figure 2.20) above. They are not assigned any cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have defined the building block for DBSCAN. We will now proceed to the
    process followed in DBSCAN in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: steps in DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now we have defined the building block for DBSCAN. We will now examine the
    steps followed in DBSCAN:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with assigning values for ε and minimum points (minPts) required to
    create a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We start with picking a random point let’s say “P” which is not yet given any
    label i.e. which has not been analyzed and assigned any cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then analyze the neighborhood for P. If it contains a sufficient number of
    points i.e. higher than minPts, then the condition is met to start a cluster.
    If so, we tag the point P as *core-point*. If a point cannot be recognized as
    a core-point, we will assign it the tag of *outlier* or *noise*. We should note
    this point can be made a part of a different cluster later. We go back to step
    2 then.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once this core point “P” is found, we start creating this cluster by adding
    all directly reachable points from P and then increase this cluster size by adding
    more directly points reachable from P. Then we add all the points to the cluster,
    which can be included using the neighborhood by iterating through all of these
    points. If we add an outlier point to the cluster, the tag of the outlier point
    is changed to a border point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process continues till density-cluster is complete. We then find a new
    unassigned point and repeat the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all the points have been assigned to a cluster or called as an outlier,
    we stop our clustering process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are iterations done in the process. And once the clustering concludes,
    we utilize business logic to either merge or split a few clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   Compare and contrast the importance of DBSCAN clustering with respect to
    kmeans clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   A non-core point can reach a core point and vice versa is also true – TRUE
    or FALSE
  prefs: []
  type: TYPE_NORMAL
- en: 3.   Explain the significance of neighborhood and minPts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.   Describe the process to find the most optimal value of “k”
  prefs: []
  type: TYPE_NORMAL
- en: Now we are clear with the process of DBSCAN clustering. Before creating the
    Python solution, we will examine the advantages and disadvantages of the DBSCAN
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: pros and cons of DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DBSCAN has following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike k-means, we need not specify the number of clusters to DBSCAN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is quite a robust solution for unclean datasets. Unlike other
    algorithms, it can deal with outliers effectively.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can determine irregular-shaped clusters too. Arguably, it is the biggest
    advantage of DBSCAN clustering.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Only the input of radius and minPts is required by the algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DBSCAN suffers from the following challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: The differentiation in clusters is sometimes not clear using DBSCAN. Depending
    on the order of processing the observations, a point can change its cluster. In
    other words, if a border point P is accessible by more than one cluster, P can
    belong to either cluster, which is dependent on the order of processing the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the difference in densities among different areas of the datasets is very
    big, then the optimum combination of ε and minPts will be difficult to determine
    and hence, DBSCAN will not be generating effective results.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance metric used plays a highly significant role in clustering algorithms
    including DBSCAN. Arguably, the most common metric used in Euclidean distance,
    but if the number of dimensions is quite large then it becomes a challenge to
    compute.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is very sensitive to different values of ε and minPts. Sometimes,
    finding the most optimum value becomes a challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now create a Python solution for DBSCAN clustering.
  prefs: []
  type: TYPE_NORMAL
- en: python solution for DBSCAN clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will use the same dataset we have used for k-means and hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps 1-6:** Load the libraries and dataset up to step 6 in k-means algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7:** Import additional libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8:** We are fitting the model with a value for minDist and radius.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9:** The number of distinct clusters are one.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10:** We are not getting any results for clustering here. In other words,
    there will not be any logical results of clustering since we have not provided
    the optimal values for minPts and ε.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 11:** Now, we will find out the optimum values for the ε. For this,
    we will calculate the distance to nearest points for each point and then sort
    and plot the results. Wherever the curvature is maximum, it is the best value
    for ε. For minPts, generally minPts ≥ d+1 where d is the number of dimensions
    in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![02_19a](images/02_19a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You are advised to go through the paper at the link to further study on how
    to choose the values of radius for DBSCAN [https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf](012012.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 12:** The best value is coming as 1.5 as observed the point of defection
    above. We will use it and set the minPts as 5 which is generally taken as a standard.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 13:** Now we can observe that we are getting more than one cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 14:** Let’s plot the clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![02_19b](images/02_19b.png)'
  prefs: []
  type: TYPE_IMG
- en: We have thus created a solution using DBSCAN. You are advised to compare the
    results from all three algorithms. In real-world scenarios, we test the solution
    with multiple algorithms, iterate with hyperparameters, and then choose the best
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Density-based clustering is quite an efficient solution and to a certain extent
    very effective one too. It is heavily recommended if the shape of the clusters
    is suspected to be irregular.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude our discussion on DBSCAN clustering. In the next section,
    we are solving a business use case on clustering. In the case study, the focus
    is less on technical concepts but more on the business understanding and the solution
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Case study using clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now define a case study which employs clustering as one of the solutions.
    The objective of the case study is to give you a flavour of the practical and
    real-life business world. This case study-based approach is also followed in job
    related interviews wherein a case is discussed during the interview stage. And
    hence, it is highly recommended for you to understand how we implement machine
    learning solutions in pragmatic business scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: A case study typically has a business problem, the data set available, the various
    solutions which can be used, challenges faced and the final chosen solution. We
    also discuss the issues faced while implementing the solution in real-world business.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start our case study on clustering using unsupervised learning. In
    the case study, we are focusing on the steps we take to solve the case study and
    not on the technical algorithms, as there can be multiple technical solutions
    to a particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Business context**: The industry we are considering can be retail, telecom,
    BFSI, aviation, health-care. Basically, any business which deals with customers
    (almost all businesses have customers). For any business, the objective is to
    generate more revenue for the business and ultimately to increase the overall
    profit of the business. And to increase the revenue, the business would wish to
    have increasingly newer customers. The business would also wish the existing consumers
    to buy more and buy more often. So, the business always strives to keep the consumers
    engaged, keep them happy and increase their transactional value with themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this to happen, the business should have a thorough understanding of a
    consumer base, know their preferences, tastes, price points, liking of categories
    etc. And once the business has examined and understood the consumer base minutely,
    then:'
  prefs: []
  type: TYPE_NORMAL
- en: The product team can improve the product features as per the consumer’s need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pricing team can improve the price of the products by aligning them to customer’s
    preferred prices. The prices can be customized for a customer or loyalty discounts
    can be offered.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The marketing team and customer relationship team (CRM), can target the consumers
    by a customized offer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The teams can win-back the consumers which are going to churn or stop buying
    from the business, can enhance their spend, increase the stickiness and increase
    the customer lifetime value.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, different teams can align their offerings as per the understanding
    of the consumers generated. And the end consumer will be happier, more engaged,
    more loyal to the business leading to more fruitful consumer engagement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The business hence has to dive deep into the consumers' data and generate an
    understanding of the base. The customer data can look like as shown in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset for the analysis**: We are taking an example for an apparel retailer
    (H&M, Uniqlo etc). A retailer having a loyalty program saves the customer’s transaction
    details. The various (not exhaustive) data sources can be as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![02_19c](images/02_19c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can have store details which have all the details of a store like store ID,
    store name, city, area, number of employees, etc. We can have the item hierarchies
    table which has all the details of the items like price, category, etc. Then we
    can have customer demographic details like age, gender, city, and customer transactional
    history which has details of the consumer’s past sales with us. Clearly, by joining
    such tables, we will be able to create a master table that will have all the details
    in one place.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You are advised to develop a good skill set for SQL. It is required in almost
    each of the domains related to data – be it data science, data engineering or
    data visualization, SQL is ubiquitous.
  prefs: []
  type: TYPE_NORMAL
- en: We are depicting an example of a master table below. It is not an exhaustive
    list of variables and the number of variables can be much larger than the ones
    below. The master table has some raw variables like Revenue, Invoices, etc. and
    derived variables like Average Transaction value and Average basket size etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![02_19d](images/02_19d.png)'
  prefs: []
  type: TYPE_IMG
- en: We could also take an example of a telecom operator. In that subscriber usage,
    call rate, revenue, days spent on the network, data usage, etc. will be the attributes
    we will be analyzing. Hence, based on the business domain at hand, the data sets
    will change.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have got the data set, we generally create derived attributes from them.
    For example, the average transaction value attributes is total revenue divided
    by the number of invoices. We create such attributes in addition to the raw variables
    we already have.
  prefs: []
  type: TYPE_NORMAL
- en: '**Suggested solutions**: There can be multiple solutions to the problem, some
    of which we are describing below:'
  prefs: []
  type: TYPE_NORMAL
- en: We can create a dashboard to depict the major KPI (key performance indicators).
    It will allow us to analyze the history and take necessary actions based on it.
    But the solution will be more reporting in nature with trends, KPI which we already
    are aware of.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can perform data analysis using some of techniques we used in the solutions
    in the earlier sections. It will solve a part of the problem and moreover, it
    is difficult to consider multiple dimensions simultaneously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can create predictive models to predict if the customers are going to shop
    in the coming months or going to churn in the next X days, but it will not solve
    the problem completely. To be clear, churn here refers that customer no longer
    shops with the retailer in the next X days. Here, duration X is defined as per
    the business domain. For example, for telecom domain X will be lesser than insurance
    domain. It is due to the fact that people use mobile phone everyday whereas for
    insurance domain, most customers might be paying premium yearly. So customer interaction
    is less for insurance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can create customer segmentation solutions wherein we are grouping customers
    based on their historical trends and attributes. This is the solution we will
    use to solve this business problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solution for the problem**: Recall in Chapter 1 in Figure 1.9, where we discussed
    the steps, we follow in the machine learning algorithm. Everything starts with
    defining the business problem and then data discovery, pre-processing etc. For
    the case study above, we will utilize a similar strategy. We have already defined
    the business problem; data discovery is done and we have completed the EDA and
    pre-processing of the data. We wish to create a segmentation solution using clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** We start with finalizing the dataset we wish to feed to the clustering
    algorithms. We might have created some derived variables, treated some missing
    values or outliers etc. In the case study, we would want to know the minimum/maximum/average
    values of transactions, invoices, items bought, etc. We would be interested to
    know the gender and age distribution. We also would like to know the mutual relationships
    between these variables like if women customers use online mode more than male
    customers. All these questions are answered as part of this step.'
  prefs: []
  type: TYPE_NORMAL
- en: A Python Jupyter notebook is checked-in at the Github repository, which provides
    detailed steps and code for the exploratory data analysis(EDA) and data pre-processing.
    Check it out!
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2:** We create the first solution using k-means clustering followed
    by hierarchical clustering. For each of the algorithms, iterations are done by
    changing hyperparameters. In the case study, we will choose parameters like the
    number of visits, total revenue, distinct categories purchased, online/offline
    transactions ratio, gender, age, etc. as parameters for clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3:** A final version of the algorithm and respective hyperparameters
    are chosen. The clusters are analyzed further in the light of business understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4:** More often, the clusters are merged or broken, depending on the
    size of the observations and the nature of the attributes present in them. For
    example, if the total customer base is 1 million, it will be really hard to action
    on a cluster of size 100\. At the same time, it will be equally difficult to manage
    a cluster of size 700,000.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5:** We then analyze the clusters we have finally got. The clusters
    distribution is checked for the variables, their distinguishing factors are understood
    and we give logical names to the clusters. We can expect to see such a clustering
    output as shown in (Figure 3-) below.'
  prefs: []
  type: TYPE_NORMAL
- en: In the example clusters shown below, we have depicted spending patterns, responsiveness
    to previous campaigns, life stage, and overall engagement as a few dimensions.
    And respective sub-divisions of each of these dimensions have also been shown.
    The clusters will be a logical combination of these dimensions. The actual dimensions
    can be much higher.
  prefs: []
  type: TYPE_NORMAL
- en: '![02_19e](images/02_19e.png)'
  prefs: []
  type: TYPE_IMG
- en: The segmentation shown above can be used for multiple domains and businesses.
    The parameters and attributes might change, the business context is different,
    the extent of data available might vary – but the overall approach remains similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the few applications, we saw in the last section, we are examining
    some of the use cases now:'
  prefs: []
  type: TYPE_NORMAL
- en: Market research utilizes clustering to segment the groups of consumers into
    market segments. And then the groups can be analyzed better in terms of their
    preferences. The product placement can be improved, pricing can be made tighter
    and geography selection will be more scientific.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the bioinformatics and medical industry, clustering can be used to group
    the genes into distinct categories. Groups of genes can be segmented and comparisons
    can be assessed by analyzing the attributes of the groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is used as an effective data pre-processing step before we create algorithms
    using supervised learning solutions. It can also be used to reduce the data size
    by focusing on the data points belonging to a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is utilized for pattern detection across both structured and unstructured
    datasets. We have already studied the case for a structured dataset. For text
    data, it can be used to group similar types of documents, journals, news, etc.
    We can also employ clustering to work and develop solutions for images. We are
    going to study unsupervised learning solutions for text and images in later chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the algorithms work on similarity measurements, it can be used to segment
    the incoming data set as fraud or genuine, which can be used to reduce the amount
    of criminal activities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The use cases of clustering are quite a lot. We have discussed only the prominent
    ones. It is one of the algorithms which change the working methodologies and generate
    a lot of insights around the data. It is widely used across telecom, retail, BFSI,
    aviation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, there are a few issues with the algorithm. We will now examine
    the common problems we face with clustering in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Common challenges faced in clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering is not a completely straight-forward solution without any challenges.
    Similar to any other solution in the world, clustering too has its share of issues
    faced. We are discussing the most common challenges we face in clustering which
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes the magnitude of the data is quite big and there are a lot of dimensions
    available. In such a case, it becomes really difficult to manage the data set.
    The computation power might be limited and like any project, there is finite time
    available. To overcome the issue, we can:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to reduce the number of dimensions by finding the most significant variables
    by using a supervised learning-based regression approach or decision tree algorithm
    etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the number of dimensions by employing Principal Component Analysis (PCA)
    or Singular Value Decomposition (SVD) etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Noisy data set: “Garbage in garbage out” – this cliché is true for clustering
    too. If the data set is messy it creates a lot of problems. The issues can be:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Missing values i.e. NULL, NA, ? , blanks etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Outliers are present in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Junk values are present like #€¶§^ etc. are present in the dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrong entries are made in the data. For example, if name are entered in the
    revenue field it is an incorrect entry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to discuss the steps and process to resolve these issues in each
    of the chapters. In this chapter, we are examining – how to work with categorical
    variables
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical variables: Recall that while discussing we discussed the issue
    with k-means not able to use categorical variables. We are solving that issue
    now.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To convert categorical variables into numeric one, we can use *one-hot encoding*.
    This technique adds additional columns equal to the number of distinct classes
    as shown in (Figure 2.) below. The variable city has unique values as London and
    NewDelhi. We can observe that two additional columns have been created with 0
    or 1 filled for the values.
  prefs: []
  type: TYPE_NORMAL
- en: '![02_19f](images/02_19f.png)'
  prefs: []
  type: TYPE_IMG
- en: But using one-hot encoding does not always ensure an effective and efficient
    solution. Imagine if the number of cities in the example above is 100, then we
    will have 100 additional columns in the dataset and most of the values will be
    filled with zero. Hence, in such a situation it is advisable to group a few values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance metrics: with different distance metrics we might get different results.
    Though there is no “one size fits all”, mostly you would find Euclidean distance
    is used for measuring distance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpretations for the clusters are quite subjective. By using different attributes,
    completely different clustering can be done for the same datasets. As discussed
    earlier, the focus should be on solving the business problem at hand. This holds
    the key to choosing the hyperparameters and the final algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Time-consuming: since a lot of dimensions have to be dealt with simultaneously,
    sometimes converging the algorithm takes a lot of time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But despite all these challenges, clustering is a widely recognized and utilized
    technique. We have discussed the use cases of clustering in the real-world in
    the last section.
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of the discussion on challenges with clustering. Let’s conclude
    the chapter with some closing thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Concluding Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning is not an easy task. But it is certainly a very engaging
    one. It does not require any target variable and the solution identifies the patterns
    itself, which is one of the biggest advantages of unsupervised learning algorithms.
    And the implementations are already creating a great impact on the business world.
    We studied one of such solution classes called clustering in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is an unsupervised learning solution that is useful for pattern identifications,
    exploratory analysis and of course segmenting the data points. Organizations heavily
    use clustering algorithms and proceed to the next level of understanding consumer
    data. Better prices can be offered, more relevant offers can be suggested, consumer
    engagement can be improved and overall customer experience becomes better. After
    all, a happy consumer is the goal of any business. Not only structured data, we
    can use clustering for text data, images, videos, and audio too. Owing to its
    capability in finding patterns across multiple data sets using a large number
    of dimensions – clustering is the go-to solution whenever we want to analyze multiple
    dimensions together.
  prefs: []
  type: TYPE_NORMAL
- en: In this second chapter of this book, we introduced concepts of unsupervised-based
    clustering methods. We examined different types of clustering algorithms – k-means
    clustering, hierarchical clustering, and DBSCAN clustering along with their mathematical
    concepts, respective use cases, and pros and cons with an emphasis on creating
    actual Python code for the same datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will study dimensionality reduction techniques
    like PCA and SVD. The building blocks for techniques, their mathematical foundation,
    advantages and disadvantages, use cases and actual Python implementation will
    be done.
  prefs: []
  type: TYPE_NORMAL
- en: You can proceed to the question section now!
  prefs: []
  type: TYPE_NORMAL
- en: Practical next steps and suggested readings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Get the online retail data from the link ([https://www.kaggle.com/hellbuoy/online-retail-customer-clustering](hellbuoy.html)).
    This dataset contains all the online transactions occurring between 1/12/2010
    and 09/12/2011 for a UK based retailer. Apply the three algorithms described in
    the chapter to identify which customers the company should target and why.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the IRIS dataset from the link ([https://www.kaggle.com/uciml/iris](uciml.html)).
    It includes three iris species with 50 samples each having some properties of
    the flowers. Use kmeans and DBSCAN and compare the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the dataset at UCI for clustering ([http://archive.ics.uci.edu/ml/index.php](.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the following papers on kmeans clustering, hierarchical clustering and
    DBSCAN clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) Kmeans algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: i.   [https://www.ee.columbia.edu/~dpwe/papers/PhamDN05-kmeans.pdf](papers.html)
  prefs: []
  type: TYPE_NORMAL
- en: ii.   [https://www.researchgate.net/publication/271616608_A_Clustering_Method_Based_on_K-Means_Algorithm](publication.html)
  prefs: []
  type: TYPE_NORMAL
- en: iii.   [https://ieeexplore.ieee.org/document/1017616](document.html)
  prefs: []
  type: TYPE_NORMAL
- en: b) Hierarchical clustering
  prefs: []
  type: TYPE_NORMAL
- en: i.   [https://ieeexplore.ieee.org/document/7100308](document.html)
  prefs: []
  type: TYPE_NORMAL
- en: ii.   [https://papers.nips.cc/paper/7200-hierarchical-clustering-beyond-the-worst-case.pdf](paper.html)
  prefs: []
  type: TYPE_NORMAL
- en: iii.   [https://papers.nips.cc/paper/8964-foundations-of-comparison-based-hierarchical-clustering.pdf](paper.html)
  prefs: []
  type: TYPE_NORMAL
- en: c) DBSCAN clustering
  prefs: []
  type: TYPE_NORMAL
- en: i.   [https://arxiv.org/pdf/1810.13105.pdf](pdf.html)
  prefs: []
  type: TYPE_NORMAL
- en: ii.   [https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.9220](viewdoc.html)
  prefs: []
  type: TYPE_NORMAL
- en: 2.9 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed an unsupervised learning technique known as clustering. Using clustering,
    we find out the underlying patterns in a data set and find out the natural groupings
    in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We understood that clustering is used for a variety of purposes across all industries,
    be it retail, telecom, finance, pharma, etc. Clustering solutions are implemented
    for customer segmentation, and marketing segmentation to understand the customer
    base better which further improves the targeting of the customers.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We studied and understood that there can be multiple clustering techniques based
    on the methodology. A few examples are K-means clustering, hierarchical clustering,
    DBSCAN, fuzzy clustering, etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We covered K-means clustering, hierarchical clustering, and DBSCAN clustering
    algorithms in detail.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We studied that kmeans is based on the centroid of the cluster while hierarchical
    clustering is an agglomerative clustering technique. DBSCAN is a density-based
    clustering algorithm.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We also went through the pros and cons of these clustering algorithms in detail.
    For example, for kmeans we have to specify the number of clusters, hierarchical
    clustering is quite time-consuming while DBSCAN’s output depends on the order
    of processing of observations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We covered that to measure the accuracy of the clustering technique we can take
    the help of WCSS, inter-cluster sum of squares, Silhouette value and Dunn Index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implemented Python-based solutions for each of the techniques. The main library
    used is sklearn.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we had the practical case study to complement
    the study.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
