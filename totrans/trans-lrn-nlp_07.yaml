- en: 5 Preprocessing data for recurrent neural network deep transfer learning experiments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 处理数据以用于循环神经网络深度迁移学习实验
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An overview of modeling architectures for transfer learning in NLP that rely
    on recurrent neural networks (RNNs)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）在NLP迁移学习中的建模架构概述
- en: Preprocessing and modeling tabular text data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理和建模表格文本数据
- en: Analyzing a new pair of representative NLP problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析一对新的代表性NLP问题
- en: In the previous chapter, we looked in some detail at some important shallow
    neural network architectures important in transfer learning for NLP, including
    word2vec and sent2vec. Recall also that the vectors produced by these methods
    are static and noncontextual, in the sense that they produce the same vector for
    the word or sentence in question, regardless of the surrounding context. This
    means these methods are unable to disambiguate, or distinguish, between different
    possible meanings of a word or sentence.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们详细研究了一些在NLP迁移学习中重要的浅层神经网络架构，包括word2vec和sent2vec。 还要记住，这些方法产生的向量是静态和非上下文的，也就是说，它们对所讨论的词或句子产生相同的向量，无论周围上下文如何。这意味着这些方法无法消歧或区分词或句子的不同可能含义。
- en: In this and the next chapter, we will cover some representative deep transfer
    learning modeling architectures for NLP that rely on recurrent neural networks
    (RNNs) for key functions. Specifically, we will be looking at the modeling frameworks
    SIMOn,[¹](#pgfId-1096604) ELMo,[²](#pgfId-1096607) and ULMFiT.[³](#pgfId-1096610)
    The deeper nature of the neural networks employed by these methods will allow
    the resulting embedding to be contextual, that is, to produce word embeddings
    that are functions of context and allow disambiguation. Recall that we first encountered
    ELMo in chapter 3\. In the next chapter, we will take a closer look at its architecture.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章和下一章中，我们将介绍一些代表性的自然语言处理（NLP）深度迁移学习建模架构，这些架构依赖于循环神经网络（RNNs）的关键功能。具体来说，我们将研究建模框架SIMOn，[¹](#pgfId-1096604)
    ELMo，[²](#pgfId-1096607) 和ULMFiT.[³](#pgfId-1096610) 这些方法所使用的更深层次的神经网络的性质将使得所得到的嵌入是具有上下文的，即产生依赖上下文的词嵌入并实现消歧。回想一下，我们在第3章首次遇到了ELMo。在下一章中，我们将更加深入地研究它的架构。
- en: Semantic Inference for the Modeling of Ontologies (SIMOn) was developed during
    DARPA’s Data-Driven Discovery of Models (D3M) program, which was an attempt to
    automate some typical tasks faced by data scientists,[⁴](#pgfId-1096619) including
    the automatic construction of processing pipelines for data cleaning, feature
    extraction, feature importance ranking, and model selection for any given data
    science problem. These tasks fall under what is often referred to as *automatic
    machine learning,* or *AutoML*. Specifically, the model looks to classify every
    column in a tabular dataset into a fundamental type, such as integer, string,
    float, or address. The idea is that an AutoML system can decide what to do with
    input tabular data—an important kind of data encountered in practice—based on
    this information. One can download prepackaged Docker images of various tools
    developed during the program, including SIMOn, from the aforementioned D3M program
    webpage.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对本体进行建模的语义推理（SIMOn）是在DARPA的数据驱动模型发现（D3M）计划期间开发的，该计划旨在自动化数据科学家面临的一些典型任务，[⁴](#pgfId-1096619)
    包括自动构建用于数据清洗、特征提取、特征重要性排名和为任何给定数据科学问题选择模型的处理管道。这些任务通常被称为*自动机器学习*或*AutoML*。具体来说，该模型试图将表格数据集中的每一列分类为基本类型，比如整数、字符串、浮点数或地址。其想法是AutoML系统可以根据这些信息决定如何处理输入的表格数据——这是实践中遇到的一种重要数据。人们可以从前述D3M计划网页下载程序中开发的各种工具的预打包的Docker镜像，包括SIMOn。
- en: SIMOn’s development was motivated by analogies to transfer learning in computer
    vision, which were discussed at the conclusion of chapter 1\. Its training procedure
    demonstrates how transfer learning can be used to augment a small set of manually
    labeled data with simulated data. The process of expanding the set of handled
    classes beyond those that were initially trained for is another task where transfer
    learning is illustratively employed within this framework. This model has been
    heavily used within D3M, and we use it in this chapter as a comparatively simple
    practical example of leveraging transfer learning to solve real, practical challenges.
    SIMOn has also been applied to detecting potentially harmful communication on
    social media.[⁵](#pgfId-1096624) Column-type classification is used as an illustrative
    example for this modeling framework.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: SIMOn的开发受到了计算机视觉中的迁移学习的类比的启发，这些内容在第一章的结尾进行了讨论。它的训练过程展示了如何使用迁移学习来使用模拟数据来补充少量手动标记的数据。将处理的类别集扩展到最初进行训练的类别之外是另一个在这个框架中生动地使用迁移学习的任务。这个模型在D3M中被大量使用，在本章中作为一个相对简单的实际例子，说明了如何利用迁移学习来解决真正的、实际的挑战。SIMOn还被用于在社交媒体上检测潜在有害的沟通。[#pgfId-1096624](#pgfId-1096624)列类型分类被用作这个建模框架的一个生动例子。
- en: We begin the chapter with a section that introduces the column data type classification
    example. The related simulated data generation and preprocessing procedures are
    also briefly touched on in that section. We follow this up with a section describing
    equivalent steps for a “fake news” detection example, to be used as a running
    example for ELMo later in the next chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从一个介绍列数据类型分类示例的章节开始本章。在那一节中，相关的模拟数据生成和预处理过程也得到了简要的涉及。我们接着描述等效步骤用于“假新闻”检测示例，在下一章中将用于ELMo的一个实例。 '
- en: A visualization of the SIMOn architecture is shown, in the context of the tabular
    column-type classification example, in figure 5.1\. Roughly speaking, it employs
    convolutional neural networks (CNNs) to build preliminary embeddings for sentences
    and a pair of RNNs to first build internal context for characters in a sentence
    and then to build external context for sentences in a document.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.1中展示了SIMOn架构的可视化，在表格列类型分类示例的背景下。粗略地说，它使用卷积神经网络（CNNs）来为句子构建初步的嵌入，使用一对RNNs来首先为句子中的字符构建内部上下文，然后为文档中的句子构建外部上下文。
- en: '![05_01](../Images/05_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![05_01](../Images/05_01.png)'
- en: Figure 5.1 Visualizing SIMOn architecture in the context of the tabular column-type
    classification example
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 在表格列类型分类示例中展示了SIMOn架构的可视化
- en: We see from the figure that the architecture is composed of elements of character-level
    convolutional neural networks (CNNs) and bidirectional long short-term memory
    (bi-LSTM) networks, a type of recurrent neural network (RNN). It is also worth
    highlighting at this stage that the input text is tokenized into sentences, not
    words, in this framework. Additionally, viewing each sentence as a cell of a column
    that corresponds to a given document allows for the conversion of unstructured
    text into the tabular dataset context considered by the framework.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，这种架构由字符级卷积神经网络（CNNs）和双向长短期记忆（bi-LSTM）网络元素组成，这是一种递归神经网络（RNN）的类型。在这个框架中，值得强调的是输入文本被分词成句子，而不是单词。另外，将每个句子视为对应给定文档的列的单元，能够将非结构化文本转换为框架考虑的表格数据集上下文。
- en: Embeddings from Language Models (ELMo) is arguably the most popular early pretrained
    language model associated with the ongoing NLP transfer learning revolution. It
    shares a lot of architectural similarities with SIMOn, also being composed of
    character-level CNNs followed by bi-LSTMs. This similarity makes a deeper dive
    into the architecture of ELMo a natural next step after the introduction of SIMOn.
    We will apply ELMo to an illustrative example problem, namely, “fake news” detection,
    to provide a practical context.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型嵌入（ELMo）可以说是与正在进行的NLP迁移学习革命相关联的最受欢迎的早期预训练语言模型。它与SIMOn有许多架构上的相似之处，也由字符级CNNs与bi-LSTMs组成。这种相似性使得在介绍SIMOn之后深入挖掘ELMo的架构成为一个自然的下一步。我们将把ELMo应用到一个用于说明的问题上，即“假新闻”检测，以提供一个实际的背景。
- en: Figure 5.2 shows a visualization of the ELMo architecture in the context of
    tabular column-type classification. Some similarities and differences between
    the two frameworks are immediately evident. We can see that both frameworks employ
    character-level CNNs and bi-LSTMs. However, whereas SIMOn has two context-building
    stages with RNNs—one for characters in a sentence and another for sentences in
    a document—ELMo has a single stage, focusing on building context for words in
    the input document.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2显示了在表格列类型分类的背景下可视化的ELMo架构。 两个框架之间的一些相似之处和差异立即显而易见。 我们可以看到，两个框架都使用字符级CNN和双向LSTM。
    但是，虽然SIMOn有两个用于RNN的上下文构建阶段——一个用于句子中的字符，另一个用于文档中的句子——而ELMo只有一个阶段，重点是为输入文档中的单词建立上下文。
- en: '![05_02](../Images/05_02.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![05_02](../Images/05_02.png)'
- en: Figure 5.2 Visualizing ELMo architecture in the context of the tabular column-type
    classification example
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2在表格列类型分类示例的背景下可视化了ELMo架构。
- en: Finally, we will take a look at the Universal Language Model Fine-Tuning (ULMFiT)
    framework, which introduces and demonstrates some key techniques and concepts
    enabling more effective adaptation of a pretrained language model for new settings,
    such as discriminative fine-tuning and gradual unfreezing. Discriminative fine-tuning
    stipulates that because the different layers of a language model contain different
    type of information, they should be tuned at different rates. Gradual unfreezing
    describes a procedure for fine-tuning progressively more parameters in a gradual
    manner that aims to reduce the risks of overfitting. The ULMFiT framework also
    includes innovations in varying the learning rate in a unique way during the adaptation
    process. We will introduce the model after ELMo in the next chapter, along with
    several of these concepts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将介绍通用语言模型微调（ULMFiT）框架，该框架引入并演示了一些关键技术和概念，使预训练语言模型能够更有效地适应新环境，如区分微调和逐步解冻。
    区分性微调规定，由于语言模型的不同层包含不同类型的信息，因此应以不同的速率进行调整。 逐步解冻描述了一种逐渐微调更多参数的过程，旨在减少过拟合的风险。 ULMFiT框架还包括在适应过程中以独特方式改变学习率的创新。
    我们将在下一章介绍ELMo之后介绍该模型，以及其中的几个概念。
- en: 5.1 Preprocessing tabular column-type classification data
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 预处理表格列类型分类数据
- en: In this section, we introduce the first example dataset that will be explored
    in this and the subsequent chapter. Here, we are interested in developing an algorithm
    that can ingest a tabular dataset and determine for the user which fundamental
    type is in each column, that is, which columns are integer, string, float, address,
    and so on. The key motivation for doing this is that an automatic machine learning
    system can decide what to do with the input tabular data—an important kind of
    data encountered in practice—based on this information. Detected latitude and
    longitude coordinate values could be plotted on a map and displayed to the user,
    for instance. Detected float columns could be potential candidate inputs or outputs
    of a regression problem, and categorical columns are candidates for classification
    problem dependent variables. We visualize the essence of this problem with a simple
    example in figure 5.3.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了在本章和随后的章节中将探讨的第一个示例数据集。 在这里，我们有兴趣开发一种算法，该算法可以接收表格数据集，并为用户确定每列中的基本类型，即确定哪些列是整数、字符串、浮点数、地址等。
    这样做的关键动机是，自动机器学习系统可以根据这些信息决定如何处理输入的表格数据——这是实践中遇到的一种重要数据类型。 例如，检测到的纬度和经度坐标值可以绘制在地图上并显示给用户。
    检测到的浮点列可能是回归问题的潜在候选输入或输出，而分类列是分类问题的依赖变量的候选项。 我们用图5.3中的一个简单示例可视化了这个问题的本质。
- en: '![05_03](../Images/05_03.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![05_03](../Images/05_03.png)'
- en: Figure 5.3 Visualizing the tabular column data-type classification problem with
    a simple example
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 使用简单示例可视化表格列数据类型分类问题
- en: We stress that this is a multilabel, multiclass problem, because there are multiple
    possible classes for each input example, and each input sample can be assigned
    multiple such classes. For instance, in figure 5.3, the first column, Customer
    ID, has multiple output labels, namely `categorical` and `int`. This also helps
    to handle the cases where the input columns are not “clean,” in the sense that
    they contain multiple types. Such columns can be labeled with all types present
    and passed to a relevant parser for further cleaning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调这是一个多标签、多类问题，因为每个输入示例都有多种可能的类别，并且每个输入样本可以分配多个这样的类别。例如，在图5.3中，第一列客户ID具有多个输出标签，即`categorical`和`int`。这还有助于处理输入列不是“干净”的情况，即它们包含多种类型。这些列可以带有所有存在的类型标签，并传递给相关解析器进行进一步清洁。
- en: Now that we have gained a better sense of the problem, let’s go ahead and obtain
    some tabular data for the experiments in this section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对于这个问题有了更好的理解，让我们开始获取一些表格数据，用于本节的实验。
- en: 5.1.1 Obtaining and visualizing tabular data
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 获取和可视化表格数据
- en: We will use two simple datasets to illustrate the tabular column-type classification
    example in the next chapter. The first of these datasets is the baseball player
    statistics dataset made available by OpenML.[⁶](#pgfId-1096670) This dataset describes
    the baseball statistics for a set of players, as well as whether or not they ended
    up in the Hall of Fame.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两个简单的数据集来说明下一章中的表格列类型分类示例。这两个数据集中的第一个是由OpenML提供的棒球球员统计数据集。[⁶](#pgfId-1096670)该数据集描述了一组球员的棒球统计数据，以及他们是否最终进入名人堂。
- en: 'On a Linux system, we can obtain the dataset as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux系统上，我们可以按如下方式获取数据集：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Recall from the previous chapters that the exclamation sign (!) is required
    only when executing in a Jupyter environment, such as the Kaggle environment we
    recommend for these exercises. When executing via a terminal, it should be dropped.
    Also observe that the .arff format is functionally equivalent to .csv for our
    purposes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从以前的章节中可以回忆到，“!”符号仅在执行Jupyter环境（例如我们建议在这些练习中使用的Kaggle环境）时需要，当在终端中执行时，应该将其删除。同时请注意，对于我们的目的，`.arff`格式与`.csv`格式在功能上是等效的。
- en: 'Having obtained the dataset of interest, let us peek at it as usual, using
    Pandas:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 获取了感兴趣的数据集后，让我们像往常一样使用Pandas进行预览：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The .arff format is functionally equivalent to .csv for our purposes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❶对于我们的目的，`.arff`格式与`.csv`格式在功能上是等效的。
- en: 'This displays the top five lines of the DataFrame, as shown next:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示DataFrame的前五行，如下所示：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see that this is a dataset of baseball statistics for some players, as advertised.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这是一组广告中所述的突击手棒球统计数据集。
- en: Let’s proceed to obtaining another tabular dataset. Without getting into too
    many details, this dataset will be used to expand our SIMOn classifier beyond
    the set of classes the pretrained model we will use was designed to detect. This
    exercise will provide an interesting use case for transfer learning that can stimulate
    ideas for your own applications.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们获取另一个表格数据集。不多赘述，这个数据集将用于扩展我们的SIMOn分类器，超越预训练模型所设计的类别集合。这个练习将为转移学习提供一个有趣的使用案例，可以激发你自己应用的创意。
- en: 'The second dataset we will look at is the multiyear British Columbia public
    library statistics dataset, which we obtained from the BC Data Catalogue[⁷](#pgfId-1096716)
    but have also attached to our companion Kaggle notebook for your convenience.
    To load the dataset using Pandas, we execute the following command, where the
    location of the file in our Kaggle environment should be replaced by your local
    path if you chose to work locally:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的第二个数据集是多年的不列颠哥伦比亚省公共图书馆统计数据集，我们从BC数据目录[⁷](#pgfId-1096716)获得，但也将其附加到我们的伴随Kaggle笔记本上，以方便你使用。要使用Pandas加载数据集，我们执行以下命令，其中我们Kaggle环境中该文件的位置应该替换为您本地的路径，如果选择在本地工作：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can peek at the dataset using the following command
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令查看数据集：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: which yields
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are only interested in a pair of columns, of type percent and integer, which
    we extract and display as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对百分比和整数这一对列感兴趣，我们可以按以下方式提取并显示：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ There are lots of columns in this dataset; let's just focus on these two.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶这个数据集有很多列，我们只关注这两列。
- en: 'This yields the following output, showcasing the two remaining columns we will
    use:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出，展示我们将使用的另外两列：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 5.1.2 Preprocessing tabular data
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 预处理表格数据
- en: 'Now let’s preprocess the obtained tabular data into a form that the SIMOn framework
    can accept. Because we will be using a pretrained model, which comes prepackaged
    with an encoder that we will apply for this purpose, we will need to install SIMOn
    as a first step using the following command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将获取的表格数据预处理成 SIMOn 框架可以接受的形式。由于我们将使用一个预训练模型，该模型预先包含一个编码器，我们将应用于此目的，因此我们需要首先安装
    SIMOn，使用以下命令：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Having done this, we need to also make a few required imports, shown here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些之后，我们还需要导入一些必需的模块，如下所示：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Imports the SIMOn model class
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 SIMOn 模型类
- en: ❷ Imports the SIMOn data encoder class for converting input text into numbers
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入 SIMOn 数据编码器类，用于将输入文本转换为数字
- en: These imports respectively represent the SIMOn model class, the data encoder
    class, a utility for standardizing all input data to a fixed length, and a class
    for generating simulated data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导入分别代表了 SIMOn 模型类、数据编码器类、将所有输入数据标准化为固定长度的实用程序，以及生成模拟数据的类。
- en: 'Next we obtain a pretrained SIMOn model, which comes with its own encoder for
    transforming text to numbers. The model is composed of two files: one containing
    the encoder and other configurations, and the second one containing the model
    weights. We obtain these files using the following commands:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取一个预训练的 SIMOn 模型，它带有自己的编码器，用于将文本转换为数字。该模型由两个文件组成：一个包含编码器和其他配置，另一个包含模型权重。我们使用以下命令获取这些文件：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ A pretrained SIMOn model configuration, encoder, and so on
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预训练的 SIMOn 模型配置、编码器等
- en: ❷ Corresponding model weights
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对应的模型权重
- en: 'Before we can load the model weights themselves, we first need to load their
    configurations, which include the encoder, via the following sequence of commands:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们加载模型权重之前，首先需要加载它们的配置，这些配置包括编码器，通过以下一系列命令：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The model weights are at the current level.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模型权重位于当前级别。
- en: ❷ The name of our pretrained model configuration that was downloaded
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 下载的预训练模型配置的名称
- en: ❸ Creates a text classifier instance for loading the encoder from model configurations
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个文本分类器实例，用于从模型配置中加载编码器。
- en: ❹ Loads the model configuration
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 加载模型配置
- en: ❺ Extracts the encoder
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 提取编码器
- en: ❻ Extract the checkpoint name
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 提取检查点名称
- en: To make sure that we downloaded the right set of weights, double-check the weights
    file the model expects via
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们下载了正确的权重集，通过以下方式双重检查模型所需的权重文件：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'which should confirm that we obtained the right file by printing the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印以下内容，应确认我们获取了正确的文件：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, we need to specify two key parameters for modeling the tabular data.
    The parameter `max_cells` specifies the maximum numbers of cells in every column
    of the table. The parameter `max_len` specifies the maximum length of each cell.
    These are visualized in figure 5.4.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要为建模表格数据指定两个关键参数。参数`max_cells`指定表格每列的最大单元格数。参数`max_len`指定每个单元格的最大长度。这在图5.4中有所体现。
- en: '![05_04](../Images/05_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![05_04](../Images/05_04.png)'
- en: Figure 5.4 Visualizing the tabular data-modeling parameters. The parameter `max_cells`
    specifies the maximum number of cells or rows per column in the table. The parameter
    `max_len` specifies the maximum length of each cell or row.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 可视化表格数据建模参数。参数`max_cells`指定表格中每列的最大单元格或行数。参数`max_len`指定每个单元格或行的最大长度。
- en: 'The maximum number of cells per column must match the value of 500 used in
    training and can be extracted from the encoder as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每列的最大单元格数必须与训练中使用的 500 的值匹配，并且可以从编码器中提取，如下所示：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Additionally, we set `max_len` to 20 for consistency with the pretrained model
    settings and extract the categories supported by the pretrained model as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们将`max_len`设置为 20，以与预训练模型设置保持一致，并提取预训练模型支持的类别，如下所示：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ The number of categories the pretrained model supports
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预训练模型支持的类别数量
- en: 'We find that the handled categories are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现处理的类别如下：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 5.1.3 Encoding preprocessed data as numbers
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 将预处理数据编码为数字
- en: We will now use the encoder to convert the tabular data into a set of numbers
    that the SIMOn model can use to make predictions. This involves converting each
    character in every input string into a unique integer representing that character
    in the model’s encoding scheme.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用编码器将表格数据转换为 SIMOn 模型可以用来进行预测的一组数字。这涉及将每个输入字符串中的每个字符转换为该字符在模型编码方案中表示的唯一整数。
- en: Because convolutional neural networks (CNNs) require all inputs to be of a fixed,
    prespecified length, the encoder will also standardize the length of each input
    column. This step replicates random cells in columns shorter than `max_cells`
    and discards some random cells in columns that are longer. This ensures that all
    columns are of length `max_cells` exactly. Additionally, all cells are standardized
    to length `max_len`, with added padding if needed. We will not worry about these
    details too much, because the SIMOn API will handle it for us behind the scenes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因为卷积神经网络（CNNs）需要所有输入都是固定的、预先指定的长度，所以编码器还将标准化每个输入列的长度。这一步骤会复制短于`max_cells`的列中的随机单元，并丢弃一些长列中的随机单元。这确保了所有列的长度恰好为`max_cells`。此外，如果需要，所有单元都标准化为长度`max_len`，并添加填充。我们不会过于担心这些细节，因为SIMOn
    API会在幕后为我们处理它。
- en: 'We encode the baseball dataset and display its shape using the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对棒球数据集进行编码，并使用以下代码显示其形状：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Encodes the data (standardization, transposition, conversion to NumPy array)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 编码数据（标准化、转置、转换为NumPy数组）
- en: ❷ Displays the shape of the encoded data
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 显示了编码数据的形状
- en: ❸ Displays the encoded first column
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 显示了编码的第一列
- en: 'Executing this yields the following output, where the output shape tuple is
    displayed first, followed by the encoded first column:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作会产生以下输出，其中首先显示输出形状元组，然后显示编码的第一列：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We see that each encoded column is a `max_cells=500` by `max_len=20` array as
    expected. We also note that the -1 entries of the encoded column represent the
    padding of cells shorter than `max_len` by the empty character.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到每个编码列都是一个`max_cells=500`乘以`max_len=20`的数组，正如预期的那样。我们还注意到编码列的-1条目代表了短于`max_len`的单元的填充。
- en: 'We also encode the library data for later use as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对图书馆数据进行编码，以便以后使用：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: At this stage, we have converted the example input datasets into NumPy arrays
    of an appropriate shape. This encodes the text into numbers compatible for ingestion
    and analysis by the first stage of the SIMOn neural net—the CNNs producing the
    preliminary input sentence embeddings.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经将示例输入数据集转换成了适当形状的NumPy数组。这将文本编码为适合SIMOn神经网络第一阶段——生成初步输入句子嵌入的CNN的摄入和分析的数字。
- en: 5.2 Preprocessing fact-checking example data
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 预处理事实检验示例数据
- en: In this section, we introduce the second example dataset that will be examined
    in this and the subsequent chapter. Here, we are interested in developing an algorithm
    to distinguish between factual news and potentially false information or misinformation.
    This application area is increasingly important and is frequently referred to
    as “automated fake news detection.”
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们介绍了将在本章和后续章节中研究的第二个示例数据集。在这里，我们感兴趣的是开发一种算法，用于区分事实新闻和潜在的错误信息或虚假信息。这个应用领域变得越来越重要，并经常被称为“自动假新闻检测”。
- en: 'Conveniently, for us, an applicable dataset is available on Kaggle[⁸](#pgfId-1096892)
    for this purpose. This dataset contains more than 40,000 articles divided into
    two categories: “fake” and “true.” The true articles were collected from reuters.com,
    a reputable news website. The fake articles, on the other hand, were collected
    from a variety of sources flagged by PolitiFact—a fact-checking organization—as
    unreliable. Most of the articles center on politics and world news.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说很方便的是，Kaggle[⁸](#pgfId-1096892)上有一个适用的数据集。该数据集包含超过40,000篇文章，分为两类：“假”和“真”。真实的文章来自一家名声显赫的新闻网站reuters.com。而假新闻则来自PolitiFact标记为不可靠的各种来源。这些文章大多涉及政治和世界新闻。
- en: 5.2.1 Special problem considerations
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 特殊问题考虑
- en: The subject of what can be called fake is undoubtedly a hot-button topic that
    deserves a brief discussion. It is certainly true that the biases ingrained in
    whoever prepared the labels for the training data will likely transfer to the
    classification system. In this sensitive context, the validity of the labels deserves
    particular attention and consideration in how they are created.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 可以称为假的主题无疑是一个值得讨论的敏感话题。可以肯定的是，准备训练数据标签的人的偏见可能会转移到分类系统中。在这样敏感的语境下，标签的有效性需要特别注意和考虑如何创建。
- en: Moreover, although our purpose in this section is to develop a content-based
    classification system for true versus potentially fake articles, it is important
    to stress that the realistic scenario is significantly more complex. In other
    words, detection of potentially false information spread is only one aspect of
    the problem of detecting *influence operations*. To understand the difference
    between the two, consider that even true information can be used to influence
    opinion to harm a brand if it is taken out of context or unnaturally amplified.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管我们在本节的目的是开发一个基于内容的分类系统，用于区分真实文章与潜在虚假文章，但重要的是要强调，现实场景要复杂得多。换句话说，检测潜在错误信息传播只是检测*影响行动*问题的一个方面。要理解两者之间的区别，请考虑即使真实信息也可以用来影响意见，从而损害品牌，如果将其放在错误的上下文或不自然地放大。
- en: Detecting influence operations can be naturally framed as an anomaly-detection
    problem,[⁹](#pgfId-1096901) but such a system can be effective only as part of
    a bigger strategy for mitigation. It must be cross-platform to be effective, with
    as many potential information channels as possible monitored and analyzed for
    anomalies. Moreover, most practical systems today have humans embedded, in the
    sense that the detection systems only flag aggregated suspicious activity and
    leave the final call of action to a human analyst.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 检测影响行动可以自然地被构造为一个异常检测问题，[⁹](#pgfId-1096901) 但这样的系统只有作为缓解策略的一部分时才能有效。它必须是跨平台的，尽可能监控和分析尽可能多的潜在信息渠道中的异常情况。此外，今天的大多数实用系统都嵌入了人类，即检测系统只标记聚合的可疑活动，并将最终行动呼叫留给人类分析员。
- en: 5.2.2 Loading and visualizing fact-checking data
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 加载和可视化事实检查数据
- en: Now we jump directly into loading the fact-checking data and preparing it for
    classification using the ELMo modeling framework. Recall from section 3.2.1, where
    we applied ELMo to spam detection and movie review sentiment analysis, that the
    model expects each input document as a single string. This makes things easier—no
    tokenization is required. Also note that the dataset has already been attached
    to the companion Jupyter notebook for the book on Kaggle.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们直接跳转到加载事实检查数据并使用 ELMo 建模框架对其进行分类的步骤。回想一下第 3.2.1 节，我们在那里将 ELMo 应用于垃圾邮件检测和电影评论情感分析，该模型期望将每个输入文档作为单个字符串。这使得事情变得更容易——不需要分词。还要注意，数据集已经附加到了
    Kaggle 上的伴随 Jupyter 笔记本上。
- en: We load the true and fake data from the dataset using the code in listing 5.1\.
    Note that we are choosing to load 1,000 samples of each here, to be consistent
    with section 3.2.1.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用列表 5.1 中的代码从数据集中加载真假数据。请注意，我们选择在此加载每种 1,000 个样本，以保持与第 3.2.1 节的一致性。
- en: Listing 5.1 Loading 1,000 samples each of true and fake articles
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 加载每种 1,000 个真假文章样本
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Reads the true news data into a Pandas DataFrame
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将真实新闻数据读入 Pandas DataFrame
- en: ❷ Reads the fake news data into a Pandas DataFrame
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将假新闻数据读入 Pandas DataFrame
- en: ❸ Number of samples to generate in each class—true, fake
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个类别生成的样本数——真实，虚假
- en: ❹ Concatenated true and fake samples
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 连接的真假样本
- en: ❺ Combines the title, body text, and topics into one string per document
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将标题、正文和主题组合成每个文档的一个字符串
- en: ❻ Corresponding labels
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对应的标签
- en: 'Secondly, we shuffle the data and split it into 70% training/30% validation
    using the following code, replicated here from section 3.2.1 for your convenience:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们使用以下代码将数据洗牌并将其分为 70% 的训练/30% 的验证，以方便起见，这些代码在此处从第 3.2.1 节复制：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ A function for shuffling data in unison with the label header, to remove any
    potential order bias
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个用于与标签头一起洗牌数据的函数，以消除任何潜在的顺序偏差
- en: ❷ Shuffles data by calling a previously defined function
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过调用先前定义的函数来洗牌数据
- en: ❸ Splits into independent 70% training and 30% testing sets
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 分成独立的 70% 训练和 30% 测试集
- en: ❹ 70% of data for training
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 70% 的数据用于训练
- en: ❺ The remaining 30% for validation
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 剩余 30% 用于验证
- en: Having introduced and preprocessed the example problem data, we proceed to apply
    the three RNN-based neural network models—which were overviewed at the beginning
    of the chapter—to the example problem data in the next chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍和预处理示例问题数据之后，我们将在下一章中将章节开头概述的三个基于 RNN 的神经网络模型应用于示例问题数据。
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Character-level models, as opposed to word-level models, can handle misspellings
    and other social media features, such as emoticons and niche vernacular.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与单词级模型相比，字符级模型可以处理拼写错误和其他社交媒体特征，如表情符号和小众俚语。
- en: Bidirectional language modeling is key for building word embeddings that are
    aware of their local context.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向语言建模是构建意识到其局部上下文的词嵌入的关键。
- en: SIMOn and ELMo both employ character-level CNNs and bi-LSTMs, with the latter
    helping to achieve bidirectional context-building.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SIMOn和ELMo都采用字符级CNN和双向LSTM，后者有助于实现双向上下文建模。
- en: 1. P. Azunre et al., “Semantic Classification of Tabular Datasets via Character-Level
    Convolutional Neural Networks,” *arXiv* (2019).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 1. P. Azunre等人，“基于字符级卷积神经网络的表格数据集的语义分类”，*arXiv*（2019年）。
- en: 2. M. E. Peters et al., “Deep Contextualized Word Representations,” Proc. of
    NAACL-HLT (2018).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 2. M. E. Peters等人，“Deep Contextualized Word Representations”，NAACL-HLT会议论文集（2018年）。
- en: 3. J. Howard et al., “Universal Language Model Fine-Tuning for Text Classification,”
    Proc. of the 56th Annual Meeting of the Association for Computational Linguistics
    (2018).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 3. J. Howard等人，“文本分类的通用语言模型微调”，第56届计算语言学年会论文集（2018年）。
- en: 4. [https://docs.datadrivendiscovery.org/](https://docs.datadrivendiscovery.org/)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 4. [https://docs.datadrivendiscovery.org/](https://docs.datadrivendiscovery.org/)
- en: 5. N. Dhamani et al., “Using Deep Networks and Transfer Learning to Address
    Disinformation,” AI for Social Good ICML Workshop (2019).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 5. N. Dhamani等人，“利用深度网络和迁移学习解决虚假信息问题”，AI for Social Good ICML研讨会（2019年）。
- en: 6. [https://www.openml.org/d/185](https://www.openml.org/d/185)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 6. [https://www.openml.org/d/185](https://www.openml.org/d/185)
- en: 7. [https://catalogue.data.gov.bc.ca/dataset/bc-public-libraries-statistics-2002-present](https://catalogue.data.gov.bc.ca/dataset/bc-public-libraries-statistics-2002-present)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 7. [https://catalogue.data.gov.bc.ca/dataset/bc-public-libraries-statistics-2002-present](https://catalogue.data.gov.bc.ca/dataset/bc-public-libraries-statistics-2002-present)
- en: 8. [https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 8. [https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)
- en: '9. P. Azunre et al., “Disinformation: Detect to Disrupt,” Conference for Truth
    and Trust Online 1, no. 1 (2019).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 9. P. Azunre等人，“虚假信息：检测到阻断”，真相和信任在线会议1卷1期（2019年）。
