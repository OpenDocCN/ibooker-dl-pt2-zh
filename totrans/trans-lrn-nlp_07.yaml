- en: 5 Preprocessing data for recurrent neural network deep transfer learning experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: An overview of modeling architectures for transfer learning in NLP that rely
    on recurrent neural networks (RNNs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing and modeling tabular text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing a new pair of representative NLP problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we looked in some detail at some important shallow
    neural network architectures important in transfer learning for NLP, including
    word2vec and sent2vec. Recall also that the vectors produced by these methods
    are static and noncontextual, in the sense that they produce the same vector for
    the word or sentence in question, regardless of the surrounding context. This
    means these methods are unable to disambiguate, or distinguish, between different
    possible meanings of a word or sentence.
  prefs: []
  type: TYPE_NORMAL
- en: In this and the next chapter, we will cover some representative deep transfer
    learning modeling architectures for NLP that rely on recurrent neural networks
    (RNNs) for key functions. Specifically, we will be looking at the modeling frameworks
    SIMOn,[¹](#pgfId-1096604) ELMo,[²](#pgfId-1096607) and ULMFiT.[³](#pgfId-1096610)
    The deeper nature of the neural networks employed by these methods will allow
    the resulting embedding to be contextual, that is, to produce word embeddings
    that are functions of context and allow disambiguation. Recall that we first encountered
    ELMo in chapter 3\. In the next chapter, we will take a closer look at its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Inference for the Modeling of Ontologies (SIMOn) was developed during
    DARPA’s Data-Driven Discovery of Models (D3M) program, which was an attempt to
    automate some typical tasks faced by data scientists,[⁴](#pgfId-1096619) including
    the automatic construction of processing pipelines for data cleaning, feature
    extraction, feature importance ranking, and model selection for any given data
    science problem. These tasks fall under what is often referred to as *automatic
    machine learning,* or *AutoML*. Specifically, the model looks to classify every
    column in a tabular dataset into a fundamental type, such as integer, string,
    float, or address. The idea is that an AutoML system can decide what to do with
    input tabular data—an important kind of data encountered in practice—based on
    this information. One can download prepackaged Docker images of various tools
    developed during the program, including SIMOn, from the aforementioned D3M program
    webpage.
  prefs: []
  type: TYPE_NORMAL
- en: SIMOn’s development was motivated by analogies to transfer learning in computer
    vision, which were discussed at the conclusion of chapter 1\. Its training procedure
    demonstrates how transfer learning can be used to augment a small set of manually
    labeled data with simulated data. The process of expanding the set of handled
    classes beyond those that were initially trained for is another task where transfer
    learning is illustratively employed within this framework. This model has been
    heavily used within D3M, and we use it in this chapter as a comparatively simple
    practical example of leveraging transfer learning to solve real, practical challenges.
    SIMOn has also been applied to detecting potentially harmful communication on
    social media.[⁵](#pgfId-1096624) Column-type classification is used as an illustrative
    example for this modeling framework.
  prefs: []
  type: TYPE_NORMAL
- en: We begin the chapter with a section that introduces the column data type classification
    example. The related simulated data generation and preprocessing procedures are
    also briefly touched on in that section. We follow this up with a section describing
    equivalent steps for a “fake news” detection example, to be used as a running
    example for ELMo later in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A visualization of the SIMOn architecture is shown, in the context of the tabular
    column-type classification example, in figure 5.1\. Roughly speaking, it employs
    convolutional neural networks (CNNs) to build preliminary embeddings for sentences
    and a pair of RNNs to first build internal context for characters in a sentence
    and then to build external context for sentences in a document.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_01](../Images/05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 Visualizing SIMOn architecture in the context of the tabular column-type
    classification example
  prefs: []
  type: TYPE_NORMAL
- en: We see from the figure that the architecture is composed of elements of character-level
    convolutional neural networks (CNNs) and bidirectional long short-term memory
    (bi-LSTM) networks, a type of recurrent neural network (RNN). It is also worth
    highlighting at this stage that the input text is tokenized into sentences, not
    words, in this framework. Additionally, viewing each sentence as a cell of a column
    that corresponds to a given document allows for the conversion of unstructured
    text into the tabular dataset context considered by the framework.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings from Language Models (ELMo) is arguably the most popular early pretrained
    language model associated with the ongoing NLP transfer learning revolution. It
    shares a lot of architectural similarities with SIMOn, also being composed of
    character-level CNNs followed by bi-LSTMs. This similarity makes a deeper dive
    into the architecture of ELMo a natural next step after the introduction of SIMOn.
    We will apply ELMo to an illustrative example problem, namely, “fake news” detection,
    to provide a practical context.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 shows a visualization of the ELMo architecture in the context of
    tabular column-type classification. Some similarities and differences between
    the two frameworks are immediately evident. We can see that both frameworks employ
    character-level CNNs and bi-LSTMs. However, whereas SIMOn has two context-building
    stages with RNNs—one for characters in a sentence and another for sentences in
    a document—ELMo has a single stage, focusing on building context for words in
    the input document.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_02](../Images/05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 Visualizing ELMo architecture in the context of the tabular column-type
    classification example
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will take a look at the Universal Language Model Fine-Tuning (ULMFiT)
    framework, which introduces and demonstrates some key techniques and concepts
    enabling more effective adaptation of a pretrained language model for new settings,
    such as discriminative fine-tuning and gradual unfreezing. Discriminative fine-tuning
    stipulates that because the different layers of a language model contain different
    type of information, they should be tuned at different rates. Gradual unfreezing
    describes a procedure for fine-tuning progressively more parameters in a gradual
    manner that aims to reduce the risks of overfitting. The ULMFiT framework also
    includes innovations in varying the learning rate in a unique way during the adaptation
    process. We will introduce the model after ELMo in the next chapter, along with
    several of these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Preprocessing tabular column-type classification data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the first example dataset that will be explored
    in this and the subsequent chapter. Here, we are interested in developing an algorithm
    that can ingest a tabular dataset and determine for the user which fundamental
    type is in each column, that is, which columns are integer, string, float, address,
    and so on. The key motivation for doing this is that an automatic machine learning
    system can decide what to do with the input tabular data—an important kind of
    data encountered in practice—based on this information. Detected latitude and
    longitude coordinate values could be plotted on a map and displayed to the user,
    for instance. Detected float columns could be potential candidate inputs or outputs
    of a regression problem, and categorical columns are candidates for classification
    problem dependent variables. We visualize the essence of this problem with a simple
    example in figure 5.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_03](../Images/05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 Visualizing the tabular column data-type classification problem with
    a simple example
  prefs: []
  type: TYPE_NORMAL
- en: We stress that this is a multilabel, multiclass problem, because there are multiple
    possible classes for each input example, and each input sample can be assigned
    multiple such classes. For instance, in figure 5.3, the first column, Customer
    ID, has multiple output labels, namely `categorical` and `int`. This also helps
    to handle the cases where the input columns are not “clean,” in the sense that
    they contain multiple types. Such columns can be labeled with all types present
    and passed to a relevant parser for further cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gained a better sense of the problem, let’s go ahead and obtain
    some tabular data for the experiments in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Obtaining and visualizing tabular data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use two simple datasets to illustrate the tabular column-type classification
    example in the next chapter. The first of these datasets is the baseball player
    statistics dataset made available by OpenML.[⁶](#pgfId-1096670) This dataset describes
    the baseball statistics for a set of players, as well as whether or not they ended
    up in the Hall of Fame.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a Linux system, we can obtain the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Recall from the previous chapters that the exclamation sign (!) is required
    only when executing in a Jupyter environment, such as the Kaggle environment we
    recommend for these exercises. When executing via a terminal, it should be dropped.
    Also observe that the .arff format is functionally equivalent to .csv for our
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having obtained the dataset of interest, let us peek at it as usual, using
    Pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ❶ The .arff format is functionally equivalent to .csv for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This displays the top five lines of the DataFrame, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We see that this is a dataset of baseball statistics for some players, as advertised.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed to obtaining another tabular dataset. Without getting into too
    many details, this dataset will be used to expand our SIMOn classifier beyond
    the set of classes the pretrained model we will use was designed to detect. This
    exercise will provide an interesting use case for transfer learning that can stimulate
    ideas for your own applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second dataset we will look at is the multiyear British Columbia public
    library statistics dataset, which we obtained from the BC Data Catalogue[⁷](#pgfId-1096716)
    but have also attached to our companion Kaggle notebook for your convenience.
    To load the dataset using Pandas, we execute the following command, where the
    location of the file in our Kaggle environment should be replaced by your local
    path if you chose to work locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can peek at the dataset using the following command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: which yields
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are only interested in a pair of columns, of type percent and integer, which
    we extract and display as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ❶ There are lots of columns in this dataset; let's just focus on these two.
  prefs: []
  type: TYPE_NORMAL
- en: 'This yields the following output, showcasing the two remaining columns we will
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 5.1.2 Preprocessing tabular data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s preprocess the obtained tabular data into a form that the SIMOn framework
    can accept. Because we will be using a pretrained model, which comes prepackaged
    with an encoder that we will apply for this purpose, we will need to install SIMOn
    as a first step using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Having done this, we need to also make a few required imports, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Imports the SIMOn model class
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Imports the SIMOn data encoder class for converting input text into numbers
  prefs: []
  type: TYPE_NORMAL
- en: These imports respectively represent the SIMOn model class, the data encoder
    class, a utility for standardizing all input data to a fixed length, and a class
    for generating simulated data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we obtain a pretrained SIMOn model, which comes with its own encoder for
    transforming text to numbers. The model is composed of two files: one containing
    the encoder and other configurations, and the second one containing the model
    weights. We obtain these files using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ❶ A pretrained SIMOn model configuration, encoder, and so on
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Corresponding model weights
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can load the model weights themselves, we first need to load their
    configurations, which include the encoder, via the following sequence of commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ❶ The model weights are at the current level.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The name of our pretrained model configuration that was downloaded
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Creates a text classifier instance for loading the encoder from model configurations
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Loads the model configuration
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Extracts the encoder
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Extract the checkpoint name
  prefs: []
  type: TYPE_NORMAL
- en: To make sure that we downloaded the right set of weights, double-check the weights
    file the model expects via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'which should confirm that we obtained the right file by printing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we need to specify two key parameters for modeling the tabular data.
    The parameter `max_cells` specifies the maximum numbers of cells in every column
    of the table. The parameter `max_len` specifies the maximum length of each cell.
    These are visualized in figure 5.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![05_04](../Images/05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 Visualizing the tabular data-modeling parameters. The parameter `max_cells`
    specifies the maximum number of cells or rows per column in the table. The parameter
    `max_len` specifies the maximum length of each cell or row.
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum number of cells per column must match the value of 500 used in
    training and can be extracted from the encoder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we set `max_len` to 20 for consistency with the pretrained model
    settings and extract the categories supported by the pretrained model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ❶ The number of categories the pretrained model supports
  prefs: []
  type: TYPE_NORMAL
- en: 'We find that the handled categories are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 5.1.3 Encoding preprocessed data as numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now use the encoder to convert the tabular data into a set of numbers
    that the SIMOn model can use to make predictions. This involves converting each
    character in every input string into a unique integer representing that character
    in the model’s encoding scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Because convolutional neural networks (CNNs) require all inputs to be of a fixed,
    prespecified length, the encoder will also standardize the length of each input
    column. This step replicates random cells in columns shorter than `max_cells`
    and discards some random cells in columns that are longer. This ensures that all
    columns are of length `max_cells` exactly. Additionally, all cells are standardized
    to length `max_len`, with added padding if needed. We will not worry about these
    details too much, because the SIMOn API will handle it for us behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We encode the baseball dataset and display its shape using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Encodes the data (standardization, transposition, conversion to NumPy array)
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Displays the shape of the encoded data
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Displays the encoded first column
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing this yields the following output, where the output shape tuple is
    displayed first, followed by the encoded first column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We see that each encoded column is a `max_cells=500` by `max_len=20` array as
    expected. We also note that the -1 entries of the encoded column represent the
    padding of cells shorter than `max_len` by the empty character.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also encode the library data for later use as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: At this stage, we have converted the example input datasets into NumPy arrays
    of an appropriate shape. This encodes the text into numbers compatible for ingestion
    and analysis by the first stage of the SIMOn neural net—the CNNs producing the
    preliminary input sentence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Preprocessing fact-checking example data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the second example dataset that will be examined
    in this and the subsequent chapter. Here, we are interested in developing an algorithm
    to distinguish between factual news and potentially false information or misinformation.
    This application area is increasingly important and is frequently referred to
    as “automated fake news detection.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Conveniently, for us, an applicable dataset is available on Kaggle[⁸](#pgfId-1096892)
    for this purpose. This dataset contains more than 40,000 articles divided into
    two categories: “fake” and “true.” The true articles were collected from reuters.com,
    a reputable news website. The fake articles, on the other hand, were collected
    from a variety of sources flagged by PolitiFact—a fact-checking organization—as
    unreliable. Most of the articles center on politics and world news.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Special problem considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The subject of what can be called fake is undoubtedly a hot-button topic that
    deserves a brief discussion. It is certainly true that the biases ingrained in
    whoever prepared the labels for the training data will likely transfer to the
    classification system. In this sensitive context, the validity of the labels deserves
    particular attention and consideration in how they are created.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, although our purpose in this section is to develop a content-based
    classification system for true versus potentially fake articles, it is important
    to stress that the realistic scenario is significantly more complex. In other
    words, detection of potentially false information spread is only one aspect of
    the problem of detecting *influence operations*. To understand the difference
    between the two, consider that even true information can be used to influence
    opinion to harm a brand if it is taken out of context or unnaturally amplified.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting influence operations can be naturally framed as an anomaly-detection
    problem,[⁹](#pgfId-1096901) but such a system can be effective only as part of
    a bigger strategy for mitigation. It must be cross-platform to be effective, with
    as many potential information channels as possible monitored and analyzed for
    anomalies. Moreover, most practical systems today have humans embedded, in the
    sense that the detection systems only flag aggregated suspicious activity and
    leave the final call of action to a human analyst.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Loading and visualizing fact-checking data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we jump directly into loading the fact-checking data and preparing it for
    classification using the ELMo modeling framework. Recall from section 3.2.1, where
    we applied ELMo to spam detection and movie review sentiment analysis, that the
    model expects each input document as a single string. This makes things easier—no
    tokenization is required. Also note that the dataset has already been attached
    to the companion Jupyter notebook for the book on Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: We load the true and fake data from the dataset using the code in listing 5.1\.
    Note that we are choosing to load 1,000 samples of each here, to be consistent
    with section 3.2.1.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Loading 1,000 samples each of true and fake articles
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Reads the true news data into a Pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Reads the fake news data into a Pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Number of samples to generate in each class—true, fake
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Concatenated true and fake samples
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Combines the title, body text, and topics into one string per document
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Corresponding labels
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, we shuffle the data and split it into 70% training/30% validation
    using the following code, replicated here from section 3.2.1 for your convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: ❶ A function for shuffling data in unison with the label header, to remove any
    potential order bias
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Shuffles data by calling a previously defined function
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Splits into independent 70% training and 30% testing sets
  prefs: []
  type: TYPE_NORMAL
- en: ❹ 70% of data for training
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The remaining 30% for validation
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced and preprocessed the example problem data, we proceed to apply
    the three RNN-based neural network models—which were overviewed at the beginning
    of the chapter—to the example problem data in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Character-level models, as opposed to word-level models, can handle misspellings
    and other social media features, such as emoticons and niche vernacular.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional language modeling is key for building word embeddings that are
    aware of their local context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SIMOn and ELMo both employ character-level CNNs and bi-LSTMs, with the latter
    helping to achieve bidirectional context-building.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1. P. Azunre et al., “Semantic Classification of Tabular Datasets via Character-Level
    Convolutional Neural Networks,” *arXiv* (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 2. M. E. Peters et al., “Deep Contextualized Word Representations,” Proc. of
    NAACL-HLT (2018).
  prefs: []
  type: TYPE_NORMAL
- en: 3. J. Howard et al., “Universal Language Model Fine-Tuning for Text Classification,”
    Proc. of the 56th Annual Meeting of the Association for Computational Linguistics
    (2018).
  prefs: []
  type: TYPE_NORMAL
- en: 4. [https://docs.datadrivendiscovery.org/](https://docs.datadrivendiscovery.org/)
  prefs: []
  type: TYPE_NORMAL
- en: 5. N. Dhamani et al., “Using Deep Networks and Transfer Learning to Address
    Disinformation,” AI for Social Good ICML Workshop (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 6. [https://www.openml.org/d/185](https://www.openml.org/d/185)
  prefs: []
  type: TYPE_NORMAL
- en: 7. [https://catalogue.data.gov.bc.ca/dataset/bc-public-libraries-statistics-2002-present](https://catalogue.data.gov.bc.ca/dataset/bc-public-libraries-statistics-2002-present)
  prefs: []
  type: TYPE_NORMAL
- en: 8. [https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)
  prefs: []
  type: TYPE_NORMAL
- en: '9. P. Azunre et al., “Disinformation: Detect to Disrupt,” Conference for Truth
    and Trust Online 1, no. 1 (2019).'
  prefs: []
  type: TYPE_NORMAL
