- en: 1 Understanding Generative AI Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An introduction to generative AI: what’s really going on under the hood?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinguishing between the many generative AI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A review of the global trends that brought us to the generative AI revolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welcome! As advertised, this book is obsolete. Which means that by the time
    you got around to opening it, most of what’s written here will either not work,
    or will be so outdated as to be useless. Now I bet you’re feeling just a bit silly
    for sinking good money into a product like this. Well I assure you: you don’t
    feel half as weird for buying this book as I felt writing it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will definitely get around to the fun stuff - or, at least stuff that was
    fun back in the Before Times when I was originally writing this - soon enough.
    We’ll learn how generative artificial intelligence can be used for far more than
    just stand-alone ChatGPT prompts. Curious to see whether:'
  prefs: []
  type: TYPE_NORMAL
- en: AI can read statistical data archives and then derive serious insights?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI can access the live internet, aggregate data from multiple sites, and use
    that to pick out real-world trends?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI can accurately summarize large bodies of your own text-based content?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI models can be fine-tuned to provide responses that’re a better match to your
    needs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI models can be used to generate original video and audio content?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Me too. Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: This book is really focused on getting practical stuff done using generative
    AI tools. That means we’re going to minimize some of the under-the-hood theoretical
    and technical background that drive these technologies and, instead, concentrate
    on effective project execution. Expect to learn about new and powerful tools almost
    immediately - and to continue adding skills all the way through the rest of the
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'More importantly: expect to become faster and more effective at whatever it
    is that you do pretty much right away. That’s only partly because the large language
    model (LLM) chat tools like ChatGPT that generate all that "generative AI" stuff
    can give you amazing answers to the questions you throw at them. But as you’ll
    see very quickly while working through this book, interacting with LLMs using
    the *automation and scripting tools* I’m going to show you will take that to a
    whole different level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, I won’t lie: you probably won’t squeeze every possible drop of
    AI goodness from your AI prompts without at having least some appreciation for
    the logic behind moving parts like *models*, *temperature*, and *text injections*.
    Every step of every project we’ll do here *will* work and will even make sense
    in the context I’ll present it. But applying your own *customized* configurations
    might sometimes be challenging without some technical background. So I’ve added
    a full set of definitions as an appendix at the back of the book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By the way, you can go a long way with these technologies without knowing this,
    but "GPT" stands for: Generative Pre-trained Transformer. Is that important? Not
    really.'
  prefs: []
  type: TYPE_NORMAL
- en: But first, just what is generative AI, how does it work, and just what is an
    *AI model*?
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Stepping into the generative AI world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ok, you’re in. What’s next?
  prefs: []
  type: TYPE_NORMAL
- en: Chatting with a modern AI tool can feel deceptively - *Turing-test* close -
    to speaking with a real human being. The Turing test is a standard devised by
    artificial intelligence pioneer Alan Turing three quarters of a century ago. A
    machine was deemed to have achieved the standard if humans could not reliably
    tell whether they’d just been interacting with another human or a machine.
  prefs: []
  type: TYPE_NORMAL
- en: Well I can definitely say that, had I not knowingly initiated the connection,
    many of my recent interactions with tools like GPT would have left me unsure on
    that score. But I did add the word "deceptively" to my description. That’s because,
    in reality, it’s all a fake. At this point, at least, even the best AI models
    aren’t actually intelligent in a human way and most certainly aren’t aware of
    their own existence. It’s really just clever software combined with massive data
    sets that give the *impression* of intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: How does that work? The software uses *natural language processing* to analyse
    the text of your prompt and then, guided by the model’s training and configurations,
    predict the best possible response. We’ll talk more about models in the next chapter.
    But for now, we’ll note that "training" consists of feeding a model with (pretty
    much) the entire public internet. All that content is used to analyse human-generated
    text for patterns so it can use probability calculations to predict the most appropriate
    way to form its own new text.
  prefs: []
  type: TYPE_NORMAL
- en: Initial drafts of a possible response to your specific prompt will be tested
    against preset standards and preferences and iteratively improved before a final
    version is displayed for you. If you respond with a follow-up prompt, the LLM
    will add previous interactions in the session to its context and repeat the process
    as it works to compose its own new response.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ll see over and over through the rest of this book, these same processes
    can be used in a fast-growing range of ways. Beyond text responses, we’re already
    seeing remarkable progress in multimodal learning, where text prompts can be used
    to generate audio, images, videos and who knows what else.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Categorizing AI models by function and objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Models* are the software frameworks that deliver specific features and functionality.
    For our purposes, the term "model" generally refers to a computational framework
    designed to understand, generate, or manipulate human language and usually describes
    *large language models* (LLMs). It learns patterns, semantics, and syntax from
    vast amounts of text data, enabling it to perform tasks like translation, text
    generation, and question answering. The LLM’s effectiveness relies on its ability
    to predict and generate coherent sequences of words, making it a versatile tool
    for natural language understanding and generation across various applications.'
  prefs: []
  type: TYPE_NORMAL
- en: An LLM is the engine used to drive a particular provider’s product. Thus, OpenAI
    currently uses GPT-(x) while Google’s Bard is built on both the Language Model
    for Dialogue Applications (LaMDA) and the Pathways Language Model 2 (PaLM-2).
    We’re told that PaLM-2 is the LLM that’s replacing the LaMDA LLM - which was mostly
    focused on text-based interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'But it’s not quite that simple. The very word "model" can have different meanings
    even *within* the LLM world. Being clear about this now can help avoid trouble
    later. For instance, by their own count, OpenAI has *seven* general-use top-level
    models, including GPT-3, GPT-3.5, and GPT-4\. But, just within the context of
    OpenAI products, here are some specialized tools also often thought of as models,
    even though they’re actually tapping the functionality of one or another top-level
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DALL-E** for generating images from text prompts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whisper**, the multilingual speech recognition model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Moderation** model that’s designed specifically to optimize measuring
    compliance with OpenAI usage policies - to help ensure an LLM isn’t misused
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddings**, a classification tool for measuring "the relatedness between
    two pieces of text" - a key element in the work LLMs do'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Codex**, the engine driving the programming assistant used by Copilot - GitHub’s
    AI tool for generating contextually-aware programming code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But those shouldn’t be confused with the long list of GPT model "flavors" available
    to choose from (like code-davinci-002 or gpt-3.5-turbo). For some reason, OpenAI
    [also refers](models.html) to each of those as "models." While you’re not wrong
    for calling those "models", it might be a bit more accurate to describe them as
    specialized *versions* of a top-level GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever you prefer to call them, it’ll be useful to know how they work. So
    let’s take a look at each of the (currently) active models you can select for
    your operations. Even if the precise names listed here might be different from
    what you’ll probably see on official sites way off in the deep, distant future
    of, I don’t know, next Thursday, being familiar with these will still provide
    useful background.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Understanding usage tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It can be helpful to think of a token as a unit of language characters. Within
    the GPT universe at least, One token is more or less equal to four characters
    of English text. Sometimes we’re interested in *how many* tokens a task will consume,
    and other times on what *kinds* of tokens will do the best job completing a task.
    The most obvious differences between various model flavors are their maximum token
    limits and the cut-off date for their training data. You’re generally billed according
    the number of such units a prompt consumes.
  prefs: []
  type: TYPE_NORMAL
- en: Models based on GPT-3, for example, were trained only on data in existence up
    to September, 2021\. And they won’t allow a single request to consume more than
    2,049 tokens between both the prompt and completion (i.e., response). By contrast,
    the newer GPT-4 models will allow a maximum of either 8,192 or 32,768 tokens per
    prompt/completion (the standard model allows 8,192, but you can get 32,768 using
    the limited-access API). Those limits will impact how much content you can incorporate
    into your prompts, and how much depth you can expect from the responses.
  prefs: []
  type: TYPE_NORMAL
- en: A limit of 2,049 tokens, for example, means that total content of both your
    prompt *and* its response mustn’t use up more than around 1,600 words. So if your
    prompt is, say, already 1,000 words long, there won’t be much space left for a
    response.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ll see later however, there are various tools available for circumventing
    at least some token limits for any model.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 GPT-4 models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are currently four models within the GPT-4 family, although two of them
    appear to be short-term "placeholders" that’ll be deprecated when some internal
    company milestone is reached. The two more permanent models are `gpt-4` and `gpt-4-32k-0314`.
    If you look closely at the naming convention they’re using, that second model’s
    name would seem to indicate that it provides a 32 thousand token limit (`32k`)
    and that it was released on March 14, 2023 (`0314`).
  prefs: []
  type: TYPE_NORMAL
- en: At least as of my getting out of bed this morning, GPT-4 was still not in general
    release and even the beta version wasn’t available across all platforms or in
    all countries. Also, the laundry basket was sticking out and I bumped into it
    on my way to the bathroom.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 GPT-3.5 models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are four long-term models based on GPT-3.5\. All but `code-davinci-002`
    allow 4,097 tokens. A single `code-davinci-002` prompt/completion can consume
    as many as 8,001 tokens. Let’s describe each of those models.
  prefs: []
  type: TYPE_NORMAL
- en: '`gpt-3.5-turbo` is optimized for chat (of the ChatGPT type), although it’s
    still a good general purpose model, and it’s both more capable and significantly
    cheaper than other GPT-3.5 models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-davinci-003` is focused on language-based tasks and has been optimized
    for "consistent instruction-following". This refers to the ability of a language
    model to consistently and accurately follow a set of instructions provided by
    a user or a prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-davinci-002` is comparable to `text-davinci-003` but it was trained using
    supervised fine-tuning, which is a machine learning technique used to improve
    the performance of a pre-trained model to adapt them to perform specific tasks
    or to make them more useful for particular applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`code-davinci-002` is primarily optimized for tasks involving programming code-completion
    to help users solve programming problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.2.4 GPT-3 models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As I’m sure you’ve noticed for yourself, OpenAI uses the names of great innovators
    in science and technology when naming their models. That’s nowhere more obvious
    than in the names they use for GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: '`text-curie-001` is described as capable while being particularly inexpensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-babbage-001` is perhaps not as much of a general-purpose tool but, for
    text classifications it excels. That could include determine the sentiment (positive,
    negative, neutral) of customer reviews or social media posts. This is known as
    sentiment analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-ada-001` is, for most purposes, extremely fast, but it’s most effective
    at simple natural language tasks like conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`davinci` is an excellent general-purpose model capable of handling more complicated
    text processing to better understand the nuances of human language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`curie` is both faster and cheaper than `davinci`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`babbage` is described in identical terms to `text-babbage-001`, although its
    capacity of 125 million parameters is far lower than the 1.2 billion parameters
    of `text-babbage-001`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ada` is described in identical terms to `ada-001` but, similar to `babbage`,
    its capacity (40 million parameters) is far lower than that of `text-ada-001`
    (0.125 billion parameters).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Incorporating more parameters into the training of a Large Language Model (LLM)
    enhances its capacity to capture intricate language patterns and knowledge, resulting
    in improved performance. The larger the model size, the better understanding of
    context, finer-grained text generation you’ll get. So if "bigger is better", why
    don’t all models use 10 billions parameters? That’s because it would require substantial
    computational resources, data, and costs to train effectively.
  prefs: []
  type: TYPE_NORMAL
- en: If the distinctions between all of those model use-cases feels a bit abstract,
    don’t worry. In fact, all existing models are probably going to do a decent job
    on nearly everything you throw at them. The important thing is to know that specializations
    exist, and that you may need to seek out the right one should you ever have a
    particularly cutting edge need.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Model fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Fine-tuning* refers to the process of further training a pre-trained language
    model on specific tasks or domains using labeled data or prompts. The objective
    of fine-tuning is to adapt the pre-trained model to a particular task, making
    it more specialized and capable of generating more accurate and contextually relevant
    responses. Fine-tuning *can* be part of the ChatGPT prompt creation process. However,
    the fine-tuning big picture extends well beyond simple prompts to encompass much
    more sophisticated configurations of AI models. I’ll include steps that can be
    used through the entire process here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-training.** A language model is initially trained on a large corpus of
    text data to learn general language patterns, grammar, and semantic representations.
    This pre-training phase allows the model to develop a broad understanding of language
    and acquire knowledge about various domains and topics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task-specific dataset.** To fine-tune the pre-trained model for a specific
    task, a labeled dataset or prompts related to that task are required. The dataset
    contains examples or prompts paired with the desired outputs or correct responses.
    For example, in sentiment analysis, the dataset would consist of sentences labeled
    as positive or negative sentiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture adaptation.** The pre-trained language model’s architecture
    is usually modified or extended to accommodate the specific task or requirements.
    This may involve adding task-specific layers, modifying the model’s attention
    mechanisms, or adjusting the output layers to match the desired task format.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning process.** The pre-trained model is then further trained on the
    task-specific dataset or prompts. During fine-tuning, the model’s parameters are
    updated using gradient-based optimization algorithms, such as stochastic gradient
    descent (SGD) or Adam, to minimize the difference between the model’s predictions
    and the desired outputs in the labeled dataset. This process allows the model
    to specialize and adapt its representations to the specific task at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative refinement.** Fine-tuning is typically an iterative process. The
    model is trained on the task-specific dataset for multiple epochs, adjusting the
    parameters and optimizing the model’s performance over time. The fine-tuning process
    aims to improve the model’s accuracy, contextual understanding, and generate task-specific
    responses.'
  prefs: []
  type: TYPE_NORMAL
- en: By fine-tuning a pre-trained language model, the model can leverage its general
    language understanding and adapt it to perform more effectively and accurately
    on specific tasks or domains. This approach saves significant computational resources
    and training time compared to training a model from scratch. Fine-tuning allows
    for task specialization and enables the model to generate contextually relevant
    responses based on the specific prompts or tasks it has been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 The technologies that make generative AI work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could spend pages and pages describing the key software frameworks and methodologies
    that fuelled the AI explosion. In fact, you can find those very pages in the appendix
    I just mentioned. But those represent *ideas*, and often decades-old ideas, at
    that. What’s been holding those ideas back all this time? It’s not like there
    weren’t crowds of extremely smart engineers, mathematicians, and theoretical researchers
    working on the problem back in 1970, 1980, and 2000\. And it’s not like there
    weren’t hyper-ambitious tech entrepreneurs aggressively looking for the Next Big
    Thing back in 1970, 1980, and 2000\. What prevented all this from happening 30
    or even 10 years ago?
  prefs: []
  type: TYPE_NORMAL
- en: Most of the bottleneck was hardware limitations. For those of you old enough
    to remember, the costs and physical constraints of processor speeds, disk storage,
    and volatile memory made for a very different computing experience in, say, 1990\.
    That was when I got my first work computer, a hand-me-down from a business that
    had, until just before, been using it for cutting-edge scientific research. That
    monster boasted a whopping 640k of RAM, a 10MB hard drive, and a text-only display.
    Video graphics memory? Don’t make me laugh. Its CPU didn’t even have a math co-processor.
  prefs: []
  type: TYPE_NORMAL
- en: The workstation I’m currently using has more than 20,000 **times** more memory
    and 5,000 **times** more storage space. And it cost me 1/4 of one percent of the
    price (when adjusted for inflation). I’m sure you get the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Without cheap storage, memory, processors, and especially Graphics Processing
    Units (GPUs) and Tensor Processing Unit (TPUs), it would have been simply impossible
    to imagine training and then deploying the original pioneering LLMs like GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, the easy availability of all those resources in a production-ready
    state on cloud platforms - and specifically Microsoft’s Azure - probably cut years
    off development times. From my early career in IT I know how long it takes to
    research, tender bids, seek approval, purchase, wait for delivery, and then actually
    deploy hardware on-premises. And that was for one or two rack servers or network
    switches at a time. I can barely fathom what it would have taken to put together
    the kind of hardware necessary to drive GPT development. But with the cloud, it’s
    really only a matter of entering your credit card information and clicking a few
    buttons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the actual hardware infrastructure, there were three other critical
    trends that made modern AI possible:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Access to large-scale datasets (i.e., the internet): The existence of vast
    amounts of labeled (meaning: data or images that have been tagged with verified
    descriptions) and unlabeled data, often referred to as big data, facilitated the
    training of generative AI models by providing a diverse and representative sample
    of real-world examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Increased computational efficiency: Optimization techniques, such as parallel
    processing, distributed computing, and model compression, played a crucial role
    in improving the efficiency of generative AI models, making them more practical
    and feasible for real-world applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Research collaborations and knowledge sharing: The active collaboration and
    exchange of ideas within the research community accelerated progress in generative
    AI, enabling the cross-pollination of techniques, methodologies, and best practices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And finally, there’s Moore’s Law: an observation and prediction made by Gordon
    Moore, co-founder of Intel, in 1965\. It states that the number of transistors
    on a microchip doubles approximately every two years, leading to a significant
    increase in computing power while reducing costs. In other words, the density
    of transistors on integrated circuits tends to double every 18 to 24 months. This
    exponential growth in transistor count has been a driving force behind the rapid
    advancement of technology, enabling more powerful and efficient computers, as
    well as smaller and more capable electronic devices. Although Moore’s Law is not
    a physical law, it has held true for several decades and has guided the semiconductor
    industry’s progress.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 AI and Data Privacy and Ownership
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Through this book, we’re going to be using all kinds of generative AI tools
    in all kinds of ways. And when I say "using generative AI tools", I really mean
    exposing your prompts and, in many cases, data resources to online services. This
    can raise concerns about the collection and use of personal data, particularly
    if the data is sensitive or contains personally identifiable information (PII).
    It is important to understand how the AI is collecting and using data, and to
    only provide data that is necessary and appropriate for the intended purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Some AI tools may monitor user activity and collect information about users'
    interactions with the technology. This could potentially raise concerns about
    surveillance and the misuse of personal information. Users should be aware of
    what information is being collected and how it will be used before engaging with
    an AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Publicly available generative AIs may also pose security risks if they are
    not properly secured. For example, if an attacker gains access to an AI’s training
    data or model architecture, they could potentially use this information to launch
    targeted attacks against users (meaning: you). There may be risks associated with
    integrating LLMs into critical infrastructure systems, such as power grids or
    financial networks. So if you work in - oh, I don’t know - a nuclear weapons facility,
    you should perhaps think carefully before introducing GPT around the office.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hoping for the best is always an approach. But it’s probably also a good idea
    to at least think about security and privacy concerns. Consider the following
    best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose AI tools from reputable developers who have a track record of prioritizing
    privacy and ethics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review the tools' documentation and terms of service to understand how they
    collect, use, and protect user data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get in the habit of only providing data that’s necessary and appropriate for
    the intended purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protect your own programming code and infrastructure from unauthorized access
    and exploitation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the other side, you should also consider how, through your use of generative
    AI services, *you* might be stepping on *someone else’s* rights. It’s unlikely,
    but an AI might produce text that’s uncomfortably similar to content it was trained
    on. If any of that content was not in the public domain or available rights-free,
    you might end up publishing some else’s protected property as your own. We call
    that *plagiarism*.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, out of curiosity I once asked a friend to submit a very large
    body of text from GPT to a professional plagiarism detecting service to see what
    came back. Not a single one of the tens of thousands of AI-generated words in
    the sample was identified as a problem. So the odds are you’ll never encounter
    this kind of trouble in the real world. Having said *that*, you’ll see a nasty,
    real-world counter example for yourself when you get to chapter three. So it can’t
    hurt to be just a little bit paranoid. Better safe than sorry.
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 AI and Reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should also share a word or two about *hallucinations*. Although before we
    begin, you might want to make sure GPT (and friends) aren’t within earshot. From
    experience I can tell you that they don’t react well to these discussions.
  prefs: []
  type: TYPE_NORMAL
- en: Put bluntly, AIs will sometimes produce output that qualifies more as *creative*
    than *clever*. They’ve been caught inventing legal precedents, academic papers,
    authors, and even entire universities. To put that in context, I had a high school
    student who would sometimes do all that, too. But he was just cheerfully pranking
    the system to see if anyone would notice. And he went on to a successful academic
    and professional career. Your friendly large langauge model, by contrast, has
    no clue that there’s anything wrong at all and will often politely suggest that
    this is all your fault ("I apologize for the confusion…​").
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, AIs are generally no better than the content they’ve been fed. While
    OpenAI and other AI companies have tried to minimize the problem, there is some
    evidence that LLMs will sometimes adopt the subjective political or social opinions
    of their training content and appear to take sides on controversial topics. This,
    too, should be a consideration when consuming AI responses.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are also notoriously bad at simple arithmetic. I recently fed a PDF file
    containing historical sales data for books to an AI. Some individual titles had
    more than one entry - representing multiple editions of the same book - in the
    file. I thought I’d save myself five or ten minutes of work setting a simple spreadsheet
    by having the AI do the calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here was my prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Based on the PDF, can you tell me how many copies of each title were sold in
    total. I’d like you to ignore individual ISBN numbers and just look at the book
    titles.
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter how often and precisely I rephrased that prompt, the AI insisted
    on picking one value seemingly at random, ignoring all the others, and presenting
    that single number as the total. But it was unfailingly polite:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I apologize for the confusion. You are correct that I missed some entries in
    my previous responses. Here are the corrected total net units sold for each book
    title, taking into account all entries in the PDF:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lesson is that we should treat LLMs the way journalists are supposed to
    treat sources: "If you mother says she loves you, demand corroboration." In other
    words, check facts and sources yourself before publishing AI output.'
  prefs: []
  type: TYPE_NORMAL
- en: '1.7 What’s still ahead:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before moving on, I’d like to let you in on the big picture. Here’s what we’re
    planning to cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Customizing text, code, and media content creation based on your organization’s
    data and specific needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training AI models on your local data stores or on the live internet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering business intelligence and analytics applications for AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your own AI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking ahead to the future of generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s now things look from this end. Now get reading. I’ll see you on the other
    side.
  prefs: []
  type: TYPE_NORMAL
- en: 1.8 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI is built on dozens of tools, methodologies, and technologies,
    including natural language processing, reinforcement learning, and neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technological advances in data storage, graphics processing, and network connectivty,
    along with steady reductions in hardware costs, have contributed to the generative
    AI revolution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
