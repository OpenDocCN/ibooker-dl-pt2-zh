- en: 1 Understanding Generative AI Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 了解生成式人工智能基础
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: 'An introduction to generative AI: what’s really going on under the hood?'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式AI的简介：在底层到底发生了什么？
- en: Distinguishing between the many generative AI models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分众多生成式AI模型
- en: A review of the global trends that brought us to the generative AI revolution
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾带领我们走向生成式AI革命的全球趋势
- en: 'Welcome! As advertised, this book is obsolete. Which means that by the time
    you got around to opening it, most of what’s written here will either not work,
    or will be so outdated as to be useless. Now I bet you’re feeling just a bit silly
    for sinking good money into a product like this. Well I assure you: you don’t
    feel half as weird for buying this book as I felt writing it.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎！正如广告所说，这本书已经过时了。这意味着当你打算打开它的时候，这里所写的大部分都行不通，或者已经过时到无用。现在我打赌你一定对把好钱花在这样的产品上感到有点傻。好吧，我向你保证：你买这本书的感觉可能还不如我写这本书的感觉怪异。
- en: 'We will definitely get around to the fun stuff - or, at least stuff that was
    fun back in the Before Times when I was originally writing this - soon enough.
    We’ll learn how generative artificial intelligence can be used for far more than
    just stand-alone ChatGPT prompts. Curious to see whether:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一定会开始玩乐趣的东西 - 或者至少在我刚开始写这篇文章的**以前时代**是有趣的东西 - 很快会到来。我们将学习生成式人工智能如何可以用于远远不止独立的ChatGPT提示。好奇是否能看到：
- en: AI can read statistical data archives and then derive serious insights?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI能够阅读统计数据档案，然后获得重大的见解吗？
- en: AI can access the live internet, aggregate data from multiple sites, and use
    that to pick out real-world trends?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI如何访问互联网，从多个网站汇总数据，并利用该数据选择出真实世界的趋势？
- en: AI can accurately summarize large bodies of your own text-based content?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI能够准确地总结你自己的大量基于文本的内容吗？
- en: AI models can be fine-tuned to provide responses that’re a better match to your
    needs?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI模型是否可以微调以提供与您需求更匹配的回答？
- en: AI models can be used to generate original video and audio content?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI模型是否可以用于生成原创视频和音频内容？
- en: Me too. Let’s find out.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我也很好奇。让我们来找出答案吧。
- en: This book is really focused on getting practical stuff done using generative
    AI tools. That means we’re going to minimize some of the under-the-hood theoretical
    and technical background that drive these technologies and, instead, concentrate
    on effective project execution. Expect to learn about new and powerful tools almost
    immediately - and to continue adding skills all the way through the rest of the
    book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书将专注于使用生成式AI工具进行实际任务。这意味着我们将尽量减少驱动这些技术的底层理论和技术背景，并专注于有效的项目执行。期望您几乎马上就能学习到新的强大工具，并在接下来的内容中不断增加技能。
- en: 'More importantly: expect to become faster and more effective at whatever it
    is that you do pretty much right away. That’s only partly because the large language
    model (LLM) chat tools like ChatGPT that generate all that "generative AI" stuff
    can give you amazing answers to the questions you throw at them. But as you’ll
    see very quickly while working through this book, interacting with LLMs using
    the *automation and scripting tools* I’m going to show you will take that to a
    whole different level.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是：期望您在几乎立即就能变得更快更有效地完成任何事情。这只是部分原因，因为像ChatGPT这样生成所有那些“生成式AI”东西的大型语言模型(LLM)聊天工具可以给出你提出的问题的惊人答案。但是，通过使用我将向您展示的*自动化和脚本工具*与LLM进行交互，您将很快发现这将达到一个完全不同的水平。
- en: 'Nevertheless, I won’t lie: you probably won’t squeeze every possible drop of
    AI goodness from your AI prompts without at having least some appreciation for
    the logic behind moving parts like *models*, *temperature*, and *text injections*.
    Every step of every project we’ll do here *will* work and will even make sense
    in the context I’ll present it. But applying your own *customized* configurations
    might sometimes be challenging without some technical background. So I’ve added
    a full set of definitions as an appendix at the back of the book.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我不会撒谎：如果没有对*模型*、*温度*和*文本注入*等移动部件的逻辑至少有一定的欣赏，您可能无法从您的AI提示中充分发挥出AI的优势。我们在这里做的每个项目的每个步骤都会*起作用*，并且在我所展示的情境中甚至会有意义。但是，如果没有一些技术背景，自定义配置可能有时会有一些挑战。因此，我在书的附录中添加了一个完整的定义集合。
- en: 'By the way, you can go a long way with these technologies without knowing this,
    but "GPT" stands for: Generative Pre-trained Transformer. Is that important? Not
    really.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，即使不了解上述内容，您也可以在这些技术上有很大的发展空间，但是“GPT”的意思是：生成式预训练转换器(Generative Pre-trained
    Transformer)。这重要吗？实际上并不重要。
- en: But first, just what is generative AI, how does it work, and just what is an
    *AI model*?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，生成式人工智能究竟是什么，它是如何工作的，以及*AI 模型*是什么？
- en: 1.1 Stepping into the generative AI world
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 踏入生成式人工智能世界
- en: Ok, you’re in. What’s next?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，你进来了。接下来呢？
- en: Chatting with a modern AI tool can feel deceptively - *Turing-test* close -
    to speaking with a real human being. The Turing test is a standard devised by
    artificial intelligence pioneer Alan Turing three quarters of a century ago. A
    machine was deemed to have achieved the standard if humans could not reliably
    tell whether they’d just been interacting with another human or a machine.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与现代 AI 工具聊天会让人感到欺骗性地 - *图灵测试* 接近 - 就像与真正的人类交谈一样。图灵测试是由人工智能先驱艾伦·图灵三分之一世纪前制定的标准。如果人类无法可靠地区分他们刚刚与另一个人类还是机器进行了交互，那么机器就被认为已经达到了标准。
- en: Well I can definitely say that, had I not knowingly initiated the connection,
    many of my recent interactions with tools like GPT would have left me unsure on
    that score. But I did add the word "deceptively" to my description. That’s because,
    in reality, it’s all a fake. At this point, at least, even the best AI models
    aren’t actually intelligent in a human way and most certainly aren’t aware of
    their own existence. It’s really just clever software combined with massive data
    sets that give the *impression* of intelligence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以明确地说，如果我没有故意发起连接，我最近与 GPT 等工具的许多交互将使我对这一点不确定。但我确实在我的描述中加入了“欺骗性”一词。因为，在现实中，这都是假的。至少在这一点上，即使是最好的
    AI 模型也不是以人类方式真正智能的，而且绝对不会意识到自己的存在。这实际上只是 clever 软件与大量数据集的结合，给人以智能的*印象*。
- en: How does that work? The software uses *natural language processing* to analyse
    the text of your prompt and then, guided by the model’s training and configurations,
    predict the best possible response. We’ll talk more about models in the next chapter.
    But for now, we’ll note that "training" consists of feeding a model with (pretty
    much) the entire public internet. All that content is used to analyse human-generated
    text for patterns so it can use probability calculations to predict the most appropriate
    way to form its own new text.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何运作的呢？该软件使用*自然语言处理*来分析您的提示文本，然后在模型的训练和配置的指导下，预测最佳可能的响应。我们将在下一章更详细地讨论模型。但现在，我们要注意，“训练”包括向模型提供（几乎）整个公共互联网。所有这些内容都用于分析人类生成的文本，以便它可以使用概率计算来预测形成自己新文本的最适当方式。
- en: Initial drafts of a possible response to your specific prompt will be tested
    against preset standards and preferences and iteratively improved before a final
    version is displayed for you. If you respond with a follow-up prompt, the LLM
    will add previous interactions in the session to its context and repeat the process
    as it works to compose its own new response.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您特定提示的初始草稿将根据预设的标准和偏好进行测试，并在最终版本显示给您之前进行迭代改进。如果您以后提出了跟进提示，LLM 将把会话中的先前交互添加到其上下文中，并重复这个过程，努力撰写自己的新响应。
- en: As we’ll see over and over through the rest of this book, these same processes
    can be used in a fast-growing range of ways. Beyond text responses, we’re already
    seeing remarkable progress in multimodal learning, where text prompts can be used
    to generate audio, images, videos and who knows what else.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书的其余部分中将一再看到的那样，这些相同的过程可以以快速增长的方式用于多种方式。除了文本响应外，我们已经在多模式学习方面看到了显著的进展，其中文本提示可以用于生成音频、图像、视频等等，还有谁知道什么其他的东西。
- en: 1.2 Categorizing AI models by function and objective
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 按功能和目标对 AI 模型进行分类
- en: '*Models* are the software frameworks that deliver specific features and functionality.
    For our purposes, the term "model" generally refers to a computational framework
    designed to understand, generate, or manipulate human language and usually describes
    *large language models* (LLMs). It learns patterns, semantics, and syntax from
    vast amounts of text data, enabling it to perform tasks like translation, text
    generation, and question answering. The LLM’s effectiveness relies on its ability
    to predict and generate coherent sequences of words, making it a versatile tool
    for natural language understanding and generation across various applications.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型*是提供特定功能和功能的软件框架。对于我们的目的，术语“模型”通常指的是设计用于理解、生成或操作人类语言的计算框架，并且通常描述为*大型语言模型*（LLM）。它从大量文本数据中学习模式、语义和语法，使其能够执行翻译、文本生成和问答等任务。LLM
    的有效性依赖于其预测和生成连贯词序列的能力，使其成为跨多个应用领域的自然语言理解和生成的多功能工具。'
- en: An LLM is the engine used to drive a particular provider’s product. Thus, OpenAI
    currently uses GPT-(x) while Google’s Bard is built on both the Language Model
    for Dialogue Applications (LaMDA) and the Pathways Language Model 2 (PaLM-2).
    We’re told that PaLM-2 is the LLM that’s replacing the LaMDA LLM - which was mostly
    focused on text-based interactions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是驱动特定提供商产品的引擎。因此，OpenAI目前使用GPT-(x)，而Google的巴德是建立在对话应用语言模型（LaMDA）和路径语言模型2（PaLM-2）上的。我们被告知PaLM-2是取代LaMDA
    LLM的LLM - 后者主要侧重于基于文本的交互。
- en: 'But it’s not quite that simple. The very word "model" can have different meanings
    even *within* the LLM world. Being clear about this now can help avoid trouble
    later. For instance, by their own count, OpenAI has *seven* general-use top-level
    models, including GPT-3, GPT-3.5, and GPT-4\. But, just within the context of
    OpenAI products, here are some specialized tools also often thought of as models,
    even though they’re actually tapping the functionality of one or another top-level
    model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但事情并不是那么简单。甚至在LLM世界*内*，“模型”这个词都可能有不同的含义。现在明确这一点可以帮助避免以后的麻烦。例如，根据他们自己的统计，OpenAI拥有*七个*通用顶级模型，包括GPT-3、GPT-3.5和GPT-4。但是，在OpenAI产品的上下文中，以下一些专门工具通常也被认为是模型，尽管它们实际上是利用了一个或另一个顶级模型的功能：
- en: '**DALL-E** for generating images from text prompts'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DALL-E**用于从文本提示生成图像'
- en: '**Whisper**, the multilingual speech recognition model'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Whisper**，多语言语音识别模型'
- en: The **Moderation** model that’s designed specifically to optimize measuring
    compliance with OpenAI usage policies - to help ensure an LLM isn’t misused
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专门设计用于优化遵守OpenAI使用政策的**审查**模型 - 以帮助确保LLM不被误用
- en: '**Embeddings**, a classification tool for measuring "the relatedness between
    two pieces of text" - a key element in the work LLMs do'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**，一种用于衡量“两段文字之间相关性”的分类工具 - 这是LLM所做工作的关键要素'
- en: '**Codex**, the engine driving the programming assistant used by Copilot - GitHub’s
    AI tool for generating contextually-aware programming code'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Codex**，Copilot使用的编程助手引擎 - GitHub的人工智能工具，用于生成具有上下文意识的编程代码'
- en: But those shouldn’t be confused with the long list of GPT model "flavors" available
    to choose from (like code-davinci-002 or gpt-3.5-turbo). For some reason, OpenAI
    [also refers](models.html) to each of those as "models." While you’re not wrong
    for calling those "models", it might be a bit more accurate to describe them as
    specialized *versions* of a top-level GPT model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些不应与长列表的GPT模型“风味”混淆，可供选择（如code-davinci-002或gpt-3.5-turbo）。出于某种原因，OpenAI[也将](models.html)这些都称为“模型”。虽然称这些为“模型”并不完全错误，但将它们描述为顶级GPT模型的专门化*版本*可能更准确一些。
- en: Whatever you prefer to call them, it’ll be useful to know how they work. So
    let’s take a look at each of the (currently) active models you can select for
    your operations. Even if the precise names listed here might be different from
    what you’ll probably see on official sites way off in the deep, distant future
    of, I don’t know, next Thursday, being familiar with these will still provide
    useful background.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你更喜欢如何称呼它们，了解它们的工作原理将是有用的。因此，让我们看看您可以选择的（当前）活动模型的每一个。即使在此列出的确切名称可能与您在深远未来（我不知道，也许是下周四）的官方网站上看到的名称有所不同，熟悉这些名称仍将提供有用的背景知识。
- en: 1.2.1 Understanding usage tokens
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 理解使用令牌
- en: It can be helpful to think of a token as a unit of language characters. Within
    the GPT universe at least, One token is more or less equal to four characters
    of English text. Sometimes we’re interested in *how many* tokens a task will consume,
    and other times on what *kinds* of tokens will do the best job completing a task.
    The most obvious differences between various model flavors are their maximum token
    limits and the cut-off date for their training data. You’re generally billed according
    the number of such units a prompt consumes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将令牌视为语言字符的单位可能会有所帮助。至少在GPT的宇宙中，一个令牌大致等于四个英文文本字符。有时我们对任务将消耗的*令牌数量*感兴趣，其他时候我们对完成任务效果最好的*令牌种类*感兴趣。各种模型风味之间最明显的区别是它们的最大令牌限制和其训练数据的截止日期。通常按照提示消耗的此类单位数量计费。
- en: Models based on GPT-3, for example, were trained only on data in existence up
    to September, 2021\. And they won’t allow a single request to consume more than
    2,049 tokens between both the prompt and completion (i.e., response). By contrast,
    the newer GPT-4 models will allow a maximum of either 8,192 or 32,768 tokens per
    prompt/completion (the standard model allows 8,192, but you can get 32,768 using
    the limited-access API). Those limits will impact how much content you can incorporate
    into your prompts, and how much depth you can expect from the responses.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，基于 GPT-3 的模型只是在 2021 年 9 月之前的数据上进行了训练。它们不允许单个请求在提示和完成（即，响应）之间消耗超过 2049 个令牌。相比之下，更新的
    GPT-4 模型将允许每个提示/完成最多使用 8192 或 32768 个令牌（标准模型允许 8192，但您可以通过有限访问 API 获得 32768）。这些限制将影响您可以整合到提示中的内容量，以及您可以从响应中期望的深度。
- en: A limit of 2,049 tokens, for example, means that total content of both your
    prompt *and* its response mustn’t use up more than around 1,600 words. So if your
    prompt is, say, already 1,000 words long, there won’t be much space left for a
    response.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，2049 个令牌的限制意味着您的提示和响应的总内容不能超过大约 1600 个字。所以如果您的提示已经有了 1000 个字，那么剩下的空间就不多了。
- en: As we’ll see later however, there are various tools available for circumventing
    at least some token limits for any model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们稍后将看到的，目前有各种工具可用于规避至少一些模型的令牌限制。
- en: 1.2.2 GPT-4 models
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 GPT-4 模型
- en: There are currently four models within the GPT-4 family, although two of them
    appear to be short-term "placeholders" that’ll be deprecated when some internal
    company milestone is reached. The two more permanent models are `gpt-4` and `gpt-4-32k-0314`.
    If you look closely at the naming convention they’re using, that second model’s
    name would seem to indicate that it provides a 32 thousand token limit (`32k`)
    and that it was released on March 14, 2023 (`0314`).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有四个属于 GPT-4 家族的模型，尽管其中两个似乎是短期的“占位符”，将在达到某个内部公司里程碑时被弃用。两个更长久的模型是 `gpt-4` 和
    `gpt-4-32k-0314`。如果您仔细观察他们使用的命名约定，您会发现第二个模型的名称似乎表明它提供了 32,000 个令牌的限制（`32k`），并且它是在
    2023 年 3 月 14 日发布的（`0314`）。
- en: At least as of my getting out of bed this morning, GPT-4 was still not in general
    release and even the beta version wasn’t available across all platforms or in
    all countries. Also, the laundry basket was sticking out and I bumped into it
    on my way to the bathroom.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我今天早上起床时，至少到目前为止，GPT-4 仍然没有普遍发布，甚至测试版也没有在所有平台或所有国家都可用。此外，洗衣篮还没有收好，我在去浴室的路上撞到了它。
- en: 1.2.3 GPT-3.5 models
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 GPT-3.5 模型
- en: There are four long-term models based on GPT-3.5\. All but `code-davinci-002`
    allow 4,097 tokens. A single `code-davinci-002` prompt/completion can consume
    as many as 8,001 tokens. Let’s describe each of those models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GPT-3.5 的模型有四个长期模型。除了 `code-davinci-002` 外，其余模型允许 4097 个令牌。单个 `code-davinci-002`
    的提示/完成可以使用多达 8001 个令牌。让我们描述一下这些模型。
- en: '`gpt-3.5-turbo` is optimized for chat (of the ChatGPT type), although it’s
    still a good general purpose model, and it’s both more capable and significantly
    cheaper than other GPT-3.5 models.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpt-3.5-turbo` 专为聊天（ChatGPT 类型）进行了优化，尽管它仍然是一个很好的通用模型，而且它既更强大又价格显著更低廉，比其他 GPT-3.5
    模型都更便宜。'
- en: '`text-davinci-003` is focused on language-based tasks and has been optimized
    for "consistent instruction-following". This refers to the ability of a language
    model to consistently and accurately follow a set of instructions provided by
    a user or a prompt.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text-davinci-003` 专注于基于语言的任务，并已经针对“一致的指令遵循”进行了优化。这指的是语言模型能够始终准确地遵循用户或提示提供的一系列指令的能力。'
- en: '`text-davinci-002` is comparable to `text-davinci-003` but it was trained using
    supervised fine-tuning, which is a machine learning technique used to improve
    the performance of a pre-trained model to adapt them to perform specific tasks
    or to make them more useful for particular applications.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text-davinci-002` 可与 `text-davinci-003` 相提并论，但它是使用监督式微调训练的，这是一种机器学习技术，用于改进预训练模型的性能，以使它们适应执行特定任务或使它们更适用于特定应用。'
- en: '`code-davinci-002` is primarily optimized for tasks involving programming code-completion
    to help users solve programming problems.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`code-davinci-002` 主要优化用于涉及编程代码补全的任务，以帮助用户解决编程问题。'
- en: 1.2.4 GPT-3 models
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 GPT-3 模型
- en: As I’m sure you’ve noticed for yourself, OpenAI uses the names of great innovators
    in science and technology when naming their models. That’s nowhere more obvious
    than in the names they use for GPT-3.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你自己注意到的那样，OpenAI 在给他们的模型命名时使用了科学技术领域伟大创新者的名字。这一点在他们为 GPT-3 使用的名称中尤为明显。
- en: '`text-curie-001` is described as capable while being particularly inexpensive.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text-curie-001` 被描述为具有能力，同时价格特别低廉。'
- en: '`text-babbage-001` is perhaps not as much of a general-purpose tool but, for
    text classifications it excels. That could include determine the sentiment (positive,
    negative, neutral) of customer reviews or social media posts. This is known as
    sentiment analysis.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text-babbage-001` 也许不是一个通用工具，但是对于文本分类来说表现出色。这可能包括确定客户评论或社交媒体帖子的情感（积极、消极、中性）。这就是所谓的情感分析。'
- en: '`text-ada-001` is, for most purposes, extremely fast, but it’s most effective
    at simple natural language tasks like conversation.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大多数目的而言，`text-ada-001` 是非常快速的，但是它在简单的自然语言任务（如对话）中效果最好。
- en: '`davinci` is an excellent general-purpose model capable of handling more complicated
    text processing to better understand the nuances of human language.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`davinci` 是一个出色的通用模型，能够处理更复杂的文本处理以更好地理解人类语言的细微差别。'
- en: '`curie` is both faster and cheaper than `davinci`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`curie` 比 `davinci` 更快，更便宜。'
- en: '`babbage` is described in identical terms to `text-babbage-001`, although its
    capacity of 125 million parameters is far lower than the 1.2 billion parameters
    of `text-babbage-001`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`babbage` 和 `text-babbage-001` 被描述为相同的术语，尽管它的容量（1.25亿个参数）远远低于 `text-babbage-001`
    的12亿个参数。'
- en: '`ada` is described in identical terms to `ada-001` but, similar to `babbage`,
    its capacity (40 million parameters) is far lower than that of `text-ada-001`
    (0.125 billion parameters).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ada` 和 `ada-001` 被描述为相同的术语，但是与 `text-ada-001` 相比，其容量（4000万个参数）要低得多（12.5亿个参数）。'
- en: Training parameters
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练参数
- en: Incorporating more parameters into the training of a Large Language Model (LLM)
    enhances its capacity to capture intricate language patterns and knowledge, resulting
    in improved performance. The larger the model size, the better understanding of
    context, finer-grained text generation you’ll get. So if "bigger is better", why
    don’t all models use 10 billions parameters? That’s because it would require substantial
    computational resources, data, and costs to train effectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将更多参数纳入大型语言模型（LLM）的训练中增强了其捕捉复杂语言模式和知识的能力，从而提高了性能。模型越大，对上下文的理解越好，生成的文本就越细致。所以，如果“更大更好”，为什么不所有模型都使用100亿个参数？这是因为这需要大量的计算资源、数据和培训成本。
- en: If the distinctions between all of those model use-cases feels a bit abstract,
    don’t worry. In fact, all existing models are probably going to do a decent job
    on nearly everything you throw at them. The important thing is to know that specializations
    exist, and that you may need to seek out the right one should you ever have a
    particularly cutting edge need.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有这些模型用例之间的区别让你感觉有些抽象，不要担心。事实上，所有现有的模型可能在你投入的几乎所有任务上表现出色。重要的是要知道专业化存在，并且如果你有特别前沿的需求，你可能需要寻找合适的模型。
- en: 1.3 Model fine-tuning
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 模型微调
- en: '*Fine-tuning* refers to the process of further training a pre-trained language
    model on specific tasks or domains using labeled data or prompts. The objective
    of fine-tuning is to adapt the pre-trained model to a particular task, making
    it more specialized and capable of generating more accurate and contextually relevant
    responses. Fine-tuning *can* be part of the ChatGPT prompt creation process. However,
    the fine-tuning big picture extends well beyond simple prompts to encompass much
    more sophisticated configurations of AI models. I’ll include steps that can be
    used through the entire process here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*Fine-tuning* 是指在特定任务或领域上进一步训练预训练语言模型的过程，使用带标签的数据或提示。Fine-tuning 的目标是使预训练模型适应特定任务，使其更专业化，能够生成更准确和具有上下文相关性的响应。Fine-tuning
    *可以* 是 ChatGPT 提示创建过程的一部分。然而，fine-tuning 的大局远远超出了简单提示的范围，包括更复杂的 AI 模型配置。我将在这里列出整个过程中可以使用的步骤：'
- en: '**Pre-training.** A language model is initially trained on a large corpus of
    text data to learn general language patterns, grammar, and semantic representations.
    This pre-training phase allows the model to develop a broad understanding of language
    and acquire knowledge about various domains and topics.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练。**语言模型最初在大量的文本数据语料库上进行训练，以学习一般的语言模式、语法和语义表示。这个预训练阶段使模型能够对语言有广泛的理解，并获取关于各种领域和主题的知识。'
- en: '**Task-specific dataset.** To fine-tune the pre-trained model for a specific
    task, a labeled dataset or prompts related to that task are required. The dataset
    contains examples or prompts paired with the desired outputs or correct responses.
    For example, in sentiment analysis, the dataset would consist of sentences labeled
    as positive or negative sentiments.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务特定数据集。**为了对预训练模型进行微调以执行特定任务，需要一个带有标签的数据集或与该任务相关的提示。数据集包含与所需输出或正确响应配对的示例或提示。例如，在情感分析中，数据集将包含被标记为积极或消极情感的句子。'
- en: '**Architecture adaptation.** The pre-trained language model’s architecture
    is usually modified or extended to accommodate the specific task or requirements.
    This may involve adding task-specific layers, modifying the model’s attention
    mechanisms, or adjusting the output layers to match the desired task format.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构适应。**预训练语言模型的架构通常会被修改或扩展，以适应特定的任务或要求。这可能涉及添加任务特定的层、修改模型的注意机制，或调整输出层以匹配所需的任务格式。'
- en: '**Fine-tuning process.** The pre-trained model is then further trained on the
    task-specific dataset or prompts. During fine-tuning, the model’s parameters are
    updated using gradient-based optimization algorithms, such as stochastic gradient
    descent (SGD) or Adam, to minimize the difference between the model’s predictions
    and the desired outputs in the labeled dataset. This process allows the model
    to specialize and adapt its representations to the specific task at hand.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调过程。**然后，预训练模型进一步在任务特定的数据集或提示上进行训练。在微调过程中，使用梯度优化算法（例如随机梯度下降（SGD）或Adam）更新模型的参数，以使模型在标记的数据集中的预测与所需输出之间的差异最小化。这个过程使模型能够专门化，并适应其表示以适应手头的特定任务。'
- en: '**Iterative refinement.** Fine-tuning is typically an iterative process. The
    model is trained on the task-specific dataset for multiple epochs, adjusting the
    parameters and optimizing the model’s performance over time. The fine-tuning process
    aims to improve the model’s accuracy, contextual understanding, and generate task-specific
    responses.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代细化。**微调通常是一个迭代过程。模型在任务特定数据集上进行多次epoch的训练，调整参数，并随着时间的推移优化模型的性能。微调过程旨在提高模型的准确性、上下文理解，并生成任务特定的响应。'
- en: By fine-tuning a pre-trained language model, the model can leverage its general
    language understanding and adapt it to perform more effectively and accurately
    on specific tasks or domains. This approach saves significant computational resources
    and training time compared to training a model from scratch. Fine-tuning allows
    for task specialization and enables the model to generate contextually relevant
    responses based on the specific prompts or tasks it has been trained on.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对预训练的语言模型进行微调，模型可以利用其对通用语言的理解，并将其调整以在特定任务或领域上更有效地执行并更准确地执行。与从头开始训练模型相比，这种方法节省了大量的计算资源和训练时间。微调允许任务专业化，并使模型能够根据其训练的特定提示或任务生成具有上下文相关性的响应。
- en: 1.4 The technologies that make generative AI work
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 使生成式人工智能运作的技术
- en: We could spend pages and pages describing the key software frameworks and methodologies
    that fuelled the AI explosion. In fact, you can find those very pages in the appendix
    I just mentioned. But those represent *ideas*, and often decades-old ideas, at
    that. What’s been holding those ideas back all this time? It’s not like there
    weren’t crowds of extremely smart engineers, mathematicians, and theoretical researchers
    working on the problem back in 1970, 1980, and 2000\. And it’s not like there
    weren’t hyper-ambitious tech entrepreneurs aggressively looking for the Next Big
    Thing back in 1970, 1980, and 2000\. What prevented all this from happening 30
    or even 10 years ago?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以花费大量篇幅描述推动人工智能爆炸的关键软件框架和方法论。事实上，你可以在我刚提到的附录中找到那些页面。但那些只代表了*想法*，而且通常是数十年前的想法。这些想法到底被拖延了多久？不像是
    1970 年、1980 年和 2000 年时没有一大群极其聪明的工程师、数学家和理论研究人员在解决这个问题。也不像是 1970 年、1980 年和 2000
    年时没有一大群雄心勃勃的科技企业家在积极寻找下一个大事件。是什么阻止了所有这些事情 30 年甚至 10 年前就发生？
- en: Most of the bottleneck was hardware limitations. For those of you old enough
    to remember, the costs and physical constraints of processor speeds, disk storage,
    and volatile memory made for a very different computing experience in, say, 1990\.
    That was when I got my first work computer, a hand-me-down from a business that
    had, until just before, been using it for cutting-edge scientific research. That
    monster boasted a whopping 640k of RAM, a 10MB hard drive, and a text-only display.
    Video graphics memory? Don’t make me laugh. Its CPU didn’t even have a math co-processor.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分瓶颈都是硬件限制。如果你够老，还记得处理器速度、磁盘存储和易失性内存的成本和物理限制在 1990 年左右带来的非常不同的计算体验。那是我得到的第一台工作电脑的时候，一台由一家商业公司借给我的，之前一直在用于前沿科学研究。那个庞然大物拥有令人瞠目结舌的
    640k RAM，10MB 硬盘，和只能显示文本的显示器。视频显存？别逗了。它的 CPU 甚至没有数学协处理器。
- en: The workstation I’m currently using has more than 20,000 **times** more memory
    and 5,000 **times** more storage space. And it cost me 1/4 of one percent of the
    price (when adjusted for inflation). I’m sure you get the picture.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我当前使用的工作站的内存比以前多了 20,000 **倍**，存储空间多了 5,000 **倍**。而且，价格只有原价的四分之一（根据通货膨胀调整后）。我相信你明白其中的含义。
- en: Without cheap storage, memory, processors, and especially Graphics Processing
    Units (GPUs) and Tensor Processing Unit (TPUs), it would have been simply impossible
    to imagine training and then deploying the original pioneering LLMs like GPT.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有廉价的存储、内存、处理器，特别是图形处理单元（GPU）和张量处理单元（TPU），简直无法想象训练和部署像 GPT 这样的原始先驱 LLMs。
- en: Beyond that, the easy availability of all those resources in a production-ready
    state on cloud platforms - and specifically Microsoft’s Azure - probably cut years
    off development times. From my early career in IT I know how long it takes to
    research, tender bids, seek approval, purchase, wait for delivery, and then actually
    deploy hardware on-premises. And that was for one or two rack servers or network
    switches at a time. I can barely fathom what it would have taken to put together
    the kind of hardware necessary to drive GPT development. But with the cloud, it’s
    really only a matter of entering your credit card information and clicking a few
    buttons.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，云平台上所有这些资源的轻松获取，尤其是微软的 Azure，可能削减了几年的开发时间。从我在IT行业的早期职业生涯中，我知道研究、招标、寻求批准、购买、等待交付，然后实际在现场部署硬件需要多长时间。而且那时只是每次购买一两台机架服务器或网络交换机。我几乎无法想象组建驱动
    GPT 开发所需的硬件需要多长时间。但是有了云，只需要输入信用卡信息然后点击几下按钮就可以了。
- en: 'Besides the actual hardware infrastructure, there were three other critical
    trends that made modern AI possible:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实际的硬件基础设施外，还有三个关键趋势使现代人工智能成为可能：
- en: 'Access to large-scale datasets (i.e., the internet): The existence of vast
    amounts of labeled (meaning: data or images that have been tagged with verified
    descriptions) and unlabeled data, often referred to as big data, facilitated the
    training of generative AI models by providing a diverse and representative sample
    of real-world examples.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得大规模数据集的访问（即互联网）：存在大量经过标记的（意思是：已经用验证过的描述标记的数据或图像）和未经标记的数据，通常称为大数据，通过提供丰富多样的、真实世界的样本，促进了生成式人工智能模型的训练。
- en: 'Increased computational efficiency: Optimization techniques, such as parallel
    processing, distributed computing, and model compression, played a crucial role
    in improving the efficiency of generative AI models, making them more practical
    and feasible for real-world applications.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高计算效率：优化技术，例如并行处理、分布式计算和模型压缩，在提高生成式AI模型的效率、使其更适用于实际应用方面起着至关重要的作用。
- en: 'Research collaborations and knowledge sharing: The active collaboration and
    exchange of ideas within the research community accelerated progress in generative
    AI, enabling the cross-pollination of techniques, methodologies, and best practices.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究合作和知识共享：研究社区内的积极合作和思想交流加速了生成式AI的进展，实现了技术、方法和最佳实践的相互交叉。
- en: 'And finally, there’s Moore’s Law: an observation and prediction made by Gordon
    Moore, co-founder of Intel, in 1965\. It states that the number of transistors
    on a microchip doubles approximately every two years, leading to a significant
    increase in computing power while reducing costs. In other words, the density
    of transistors on integrated circuits tends to double every 18 to 24 months. This
    exponential growth in transistor count has been a driving force behind the rapid
    advancement of technology, enabling more powerful and efficient computers, as
    well as smaller and more capable electronic devices. Although Moore’s Law is not
    a physical law, it has held true for several decades and has guided the semiconductor
    industry’s progress.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有摩尔定律：由英特尔联合创始人戈登·摩尔在1965年提出的观察和预测。它指出，微芯片上的晶体管数量大约每两年翻一番，从而在降低成本的同时显著提高计算能力。换句话说，集成电路上的晶体管密度往往会每18至24个月翻一倍。晶体管数量的指数增长是推动技术飞速发展的驱动力，使计算机更强大、更高效，以及电子设备更小巧、更强大。虽然摩尔定律不是物理定律，但它已经保持了几十年，并指导了半导体行业的进步。
- en: 1.5 AI and Data Privacy and Ownership
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 AI和数据隐私和所有权
- en: Through this book, we’re going to be using all kinds of generative AI tools
    in all kinds of ways. And when I say "using generative AI tools", I really mean
    exposing your prompts and, in many cases, data resources to online services. This
    can raise concerns about the collection and use of personal data, particularly
    if the data is sensitive or contains personally identifiable information (PII).
    It is important to understand how the AI is collecting and using data, and to
    only provide data that is necessary and appropriate for the intended purpose.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本书，我们将以各种方式使用各种生成式AI工具。当我说“使用生成式AI工具”时，我真正指的是将你的提示和在许多情况下数据资源暴露给在线服务。这可能引起有关个人数据收集和使用的担忧，特别是如果数据敏感或包含可识别个人身份信息（PII）。了解AI如何收集和使用数据，并只提供必要和适用于预期目的的数据是很重要的。
- en: Some AI tools may monitor user activity and collect information about users'
    interactions with the technology. This could potentially raise concerns about
    surveillance and the misuse of personal information. Users should be aware of
    what information is being collected and how it will be used before engaging with
    an AI.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一些AI工具可能会监控用户活动并收集有关用户与技术交互的信息。这可能会引起有关监视和个人信息滥用的担忧。用户在使用AI之前应了解正在收集什么信息以及将如何使用。
- en: 'Publicly available generative AIs may also pose security risks if they are
    not properly secured. For example, if an attacker gains access to an AI’s training
    data or model architecture, they could potentially use this information to launch
    targeted attacks against users (meaning: you). There may be risks associated with
    integrating LLMs into critical infrastructure systems, such as power grids or
    financial networks. So if you work in - oh, I don’t know - a nuclear weapons facility,
    you should perhaps think carefully before introducing GPT around the office.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果公开可用的生成式人工智能没有得到适当的安全保护，它们可能会带来安全风险。例如，如果攻击者获得了AI的训练数据或模型架构，他们可能会利用这些信息对用户（也就是你）发起有针对性的攻击。在将LLMs集成到诸如电网或金融网络等关键基础设施系统中存在风险。因此，如果你在
    - 哦，我不知道 - 某个核武器设施工作，那么在公司引入GPT之前，你应该慎重考虑。
- en: 'Hoping for the best is always an approach. But it’s probably also a good idea
    to at least think about security and privacy concerns. Consider the following
    best practices:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 抱有最好的期望始终是一种方法。但至少考虑安全和隐私问题可能也是一个好主意。请考虑以下最佳实践：
- en: Choose AI tools from reputable developers who have a track record of prioritizing
    privacy and ethics.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择来自声誉良好的开发者的AI工具，这些开发者优先考虑隐私和伦理。
- en: Review the tools' documentation and terms of service to understand how they
    collect, use, and protect user data.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查阅工具的文档和服务条款，了解它们如何收集、使用和保护用户数据。
- en: Get in the habit of only providing data that’s necessary and appropriate for
    the intended purpose.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 养成只提供必要和适用于预期目的的数据的习惯。
- en: Protect your own programming code and infrastructure from unauthorized access
    and exploitation.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护自己的编程代码和基础设施，防止未经授权的访问和利用。
- en: From the other side, you should also consider how, through your use of generative
    AI services, *you* might be stepping on *someone else’s* rights. It’s unlikely,
    but an AI might produce text that’s uncomfortably similar to content it was trained
    on. If any of that content was not in the public domain or available rights-free,
    you might end up publishing some else’s protected property as your own. We call
    that *plagiarism*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通过使用生成AI服务，您也应该考虑，*您*可能会侵犯*他人*的权利。虽然不太可能，但AI可能会生成与其训练内容非常相似而令人不舒服的文本。如果其中任何内容不属于公共领域或是具有免版权的，您可能会发布其他人的受保护财产作为自己的。我们称之为*抄袭*。
- en: Having said that, out of curiosity I once asked a friend to submit a very large
    body of text from GPT to a professional plagiarism detecting service to see what
    came back. Not a single one of the tens of thousands of AI-generated words in
    the sample was identified as a problem. So the odds are you’ll never encounter
    this kind of trouble in the real world. Having said *that*, you’ll see a nasty,
    real-world counter example for yourself when you get to chapter three. So it can’t
    hurt to be just a little bit paranoid. Better safe than sorry.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，出于好奇，我曾让一个朋友向一家专业的学术抄袭检测服务提交了一份大量GPT生成的文字样本，看看会有什么结果。在样本中的数以万计由AI生成的文字中，没有一个被识别为问题。所以在现实世界中，你很少会遇到这种麻烦。话虽如此，当你进入第三章时，你会亲眼见到一个恶劣的现实情况，那时你会明白保持一丁点的多虑没有坏处。谨慎总比后悔好。
- en: 1.6 AI and Reliability
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 AI和可靠性
- en: We should also share a word or two about *hallucinations*. Although before we
    begin, you might want to make sure GPT (and friends) aren’t within earshot. From
    experience I can tell you that they don’t react well to these discussions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也应该谈谈*幻觉*。尽管在我们开始之前，你可能要确保GPT（以及朋友们）不能听到。从经验来看，我可以告诉你，他们对这些讨论反应不太好。
- en: Put bluntly, AIs will sometimes produce output that qualifies more as *creative*
    than *clever*. They’ve been caught inventing legal precedents, academic papers,
    authors, and even entire universities. To put that in context, I had a high school
    student who would sometimes do all that, too. But he was just cheerfully pranking
    the system to see if anyone would notice. And he went on to a successful academic
    and professional career. Your friendly large langauge model, by contrast, has
    no clue that there’s anything wrong at all and will often politely suggest that
    this is all your fault ("I apologize for the confusion…​").
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 直白地说，有时候AI生成的输出更多地属于*创作*而不是*聪明*。它们已被发现创造法律先例、学术论文、作者，甚至整个大学。把这放在背景中，我曾经有一个高中学生也会做这些事情。但他只是开心地戏弄系统，看看有没有人注意到。他取得了成功的学术和职业生涯。相比之下，友善的大语言模型完全不知道有什么问题，并且经常礼貌地暗示这一切都是你的错（"对于混淆我向您道歉...）"。
- en: Similarly, AIs are generally no better than the content they’ve been fed. While
    OpenAI and other AI companies have tried to minimize the problem, there is some
    evidence that LLMs will sometimes adopt the subjective political or social opinions
    of their training content and appear to take sides on controversial topics. This,
    too, should be a consideration when consuming AI responses.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，AI通常不会比它们所接收到的内容更好。尽管OpenAI和其他AI公司已经尽力减小这个问题，但有一些证据表明，有时LLM会采纳其训练内容中的主观政治或社会观点，并在有争议的话题上显露出自己的立场。在使用AI响应时，这也应该成为一个考虑因素。
- en: LLMs are also notoriously bad at simple arithmetic. I recently fed a PDF file
    containing historical sales data for books to an AI. Some individual titles had
    more than one entry - representing multiple editions of the same book - in the
    file. I thought I’d save myself five or ten minutes of work setting a simple spreadsheet
    by having the AI do the calculation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在简单算术方面也是出了名的差。最近我把一份包含历史销售数据的PDF文件输入到一个AI中。文件中有些书名有多个条目 - 表示同一本书的多个版本。我想让AI做计算，以免花费我五到十分钟的时间设置一个简单的电子表格。
- en: 'Here was my prompt:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的提示：
- en: Prompt engineering
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示工程
- en: Based on the PDF, can you tell me how many copies of each title were sold in
    total. I’d like you to ignore individual ISBN numbers and just look at the book
    titles.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 根据PDF，你能告诉我每个标题的总销售数量吗？我希望您忽略个别的ISBN号，只看书名。
- en: 'No matter how often and precisely I rephrased that prompt, the AI insisted
    on picking one value seemingly at random, ignoring all the others, and presenting
    that single number as the total. But it was unfailingly polite:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我多么频繁和精确地重新表述提示，AI都坚持选择一个数值，似乎是随机的，忽略了其他所有的内容，并将这个单一的数字呈现为总数。但它总是非常礼貌：
- en: 'I apologize for the confusion. You are correct that I missed some entries in
    my previous responses. Here are the corrected total net units sold for each book
    title, taking into account all entries in the PDF:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于造成的混乱，我感到抱歉。您是正确的，我错过了一些条目。以下是修正后的每本书总净销售量，考虑到PDF中的所有条目：
- en: 'The lesson is that we should treat LLMs the way journalists are supposed to
    treat sources: "If you mother says she loves you, demand corroboration." In other
    words, check facts and sources yourself before publishing AI output.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 教训是我们应该像记者对待消息源一样对待LLMs：“如果你的母亲说她爱你，要求证实。” 换句话说，在发布AI输出之前，请自行核实事实和来源。
- en: '1.7 What’s still ahead:'
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 未来展望：
- en: 'Before moving on, I’d like to let you in on the big picture. Here’s what we’re
    planning to cover:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我想让你了解大局。这是我们计划要涵盖的内容：
- en: Customizing text, code, and media content creation based on your organization’s
    data and specific needs
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据您组织的数据和具体需求定制文本、代码和媒体内容创作
- en: Training AI models on your local data stores or on the live internet
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地数据存储或实时互联网上对AI模型进行训练
- en: Discovering business intelligence and analytics applications for AI
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索AI的商业智能和分析应用
- en: Building your own AI models
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建您自己的AI模型
- en: Looking ahead to the future of generative AI
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展望生成AI的未来
- en: That’s now things look from this end. Now get reading. I’ll see you on the other
    side.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一端看，情况就是这样了。现在开始阅读吧。我们在另一端见。
- en: 1.8 Summary
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.8 摘要
- en: Generative AI is built on dozens of tools, methodologies, and technologies,
    including natural language processing, reinforcement learning, and neural networks.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成AI建立在数十种工具、方法和技术之上，包括自然语言处理、强化学习和神经网络。
- en: Technological advances in data storage, graphics processing, and network connectivty,
    along with steady reductions in hardware costs, have contributed to the generative
    AI revolution.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储、图形处理和网络连接技术的技术进步，以及硬件成本的稳定降低，推动了生成AI革命。
