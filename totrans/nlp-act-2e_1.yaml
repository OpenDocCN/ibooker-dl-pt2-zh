- en: 1 Machines that read and write (NLP overview)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The power of human language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How natural language processing (NLP) is changing society
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kinds of NLP tasks that machines can now do well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why unleashing the NLP genie is profitable …​ and dangerous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to start building a simple chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How NLP technology is programming itself and making itself smarter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words are powerful. They can change minds. And they can change the world. Natural
    language processing puts the power of words into algorithms. Those algorithms
    are changing your world right before your eyes. You are about to see how the majority
    of the words and ideas that enter your mind are filtered and generated by NLP
    and how you can take back some of that control over your world.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine what you would do with a machine that could understand and act on every
    word it reads on the Internet? Imagine the information and knowledge you’d be
    able to harvest and profit from. NLP promises to create the second information
    revolution by turning vast amounts of unstructured data into actionable knowledge
    and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Early on, Big Tech discovered the power of NLP to glean knowledge from natural
    language text. They use that power to affect our behavior and our minds in order
    to improve their bottom line.^([[1](#_footnotedef_1 "View footnote.")]) Governments
    too are waking up to the impact NLP has on culture, society and humanity. Fortunately,
    a few courageous liberal democracies are attempting to free your mind by steering
    businesses towards sustainable and ethical uses for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: On the other end of the spectrum, authoritarian governments are using NLP to
    coopt our prosocial instincts to make us easier to track and control. The Chinese
    government uses NLP to prevent you from even talking about Tibet or Hong Kong
    in the video games you play.^([[2](#_footnotedef_2 "View footnote.")]) The authors
    of this book needed to dig through the Internet Archive to replace disappearing
    article links with permalinks.^([[3](#_footnotedef_3 "View footnote.")]) Governments
    and businesses that censor public media are corrupting the datasets used by even
    the most careful NLP engineers who only use high quality online encyclopedias
    for training.^([[4](#_footnotedef_4 "View footnote.")]) And surprisingly, even
    in the US, there are corporations, politicians, and government agencies which
    use NLP to influence the public discourse about pandemics, climate change, and
    many other of the *21 Lessons for the 21st Century*.^([[5](#_footnotedef_5 "View
    footnote.")]) NLP is even being used to influence what you think about AI and
    NLP itself. Of course, not all corporations and politicians have your best interests
    at heart.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will begin to build your NLP understanding and skill so
    you can take control of the information and ideas that affect what you believe
    and think. You first need to see all the ways NLP is used in the modern world.
    This chapter will open your eyes to these NLP applications happening behind the
    scenes in your everyday life. Hopefully this will help you write a few lines of
    Python code to help you track, classify, and influence the packets of thought
    bouncing around on the Internet and into your brain. Your understanding of natural
    language processing will give you greater influence and control over the words
    and ideas in your world. And it will give you and your business the ability to
    escape Big Tech’s stranglehold on information, so you can succeed.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Programming language vs. natural language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Programming languages are very similar to natural languages like English. Both
    kinds of languages are used to communicate instructions from one information processing
    system to another. Both languages can communicate thoughts from human to human,
    human to machine, or even machine to machine. Both languages define the concept
    of *tokens*, the smallest packet of meaningful text. No matter whether your text
    is natural language or a programming language, the first thing that a machine
    does is to split the text into tokens. For natural language, tokens are usually
    words or combinations of words that go together (compound words).
  prefs: []
  type: TYPE_NORMAL
- en: And both natural and programming languages use *grammars*. A grammar is a set
    of rules that tell you how to combine words in a sequence to create an expression
    or statement that others will understand. And the words "expression" and "statement"
    mean similar things whether you are in a computer science class or an English
    grammar class. And you may have heard of *regular expressions* in computer science.
    They give you a way to create grammar rules for processing text. In this book,
    you will use regular expressions to match patterns in all kinds of text, including
    natural language and computer programs.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these similarities between programming and natural language, you need
    new skills and new tools to process natural language with a machine. Programming
    languages are artificially designed languages we use to tell a computer what to
    do. Computer programming languages are used to explicitly define a sequence of
    mathematical operations on bits of information, ones and zeros. And programming
    languages only need to be *processed* by machines rather than *understood*. A
    machine needs to do *what* the programmer asks it to do. It does not need to understand
    *why* the program is the way it is. And it doesn’t need abstractions or mental
    models of the computer program to understand anything outside of the world of
    ones and zeroes that it is processing. And almost all computers use the Von Neumann
    architecture developed in 1945.^([[6](#_footnotedef_6 "View footnote.")]) Modern
    CPUs (Central Processing Units) implement the *Von Neumann architecture* as a
    register machine, a version of the *universal Turing machine* idea of 1936.^([[7](#_footnotedef_7
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Natural languages, however, evolved naturally, *organically*. Natural languages
    communicate ideas, understanding, and knowledge between living organisms that
    have brains rather than CPUs. These natural languages must be "runnable" or *understandable*
    on a wide variety of wetware (brains). In some cases, natural language even enables
    communication across animal species. Koko (gorilla), Woshoe (chimpanzee), Alex
    (parrot) and other famous animals have demonstrated command of some English words.^([[8](#_footnotedef_8
    "View footnote.")]). Reportedly, Alex the parrot discovered the meaning of the
    word "none" on its own. Alex’s dying words to its grieving owner were "Be good,
    I love you" ([https://www.wired.com/2007/09/super-smart-par](09.html)). And Alex’s
    words inspired Ted Chiang’s masterful short story "The Great Silence." That is
    profound cross-species communication, no matter whether the words came from intelligence
    and sentience or not.
  prefs: []
  type: TYPE_NORMAL
- en: Given how differently natural languages and programming languages evolved, it
    is no surprise they’re used for different things. We do not use programming languages
    to tell each other about our day or to give directions to the grocery store. Similarly,
    natural languages did not evolve to be readily compiled into thought packets that
    can be manipulated by machines to derive conclusions. But that’s exactly what
    you are going to learn how to do with this book. With NLP you can program machines
    to process natural language text to derive conclusions, infer new facts, create
    meaningful abstractions, and even respond meaningfully in a conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Even though there are no compilers for natural language there are *parsers*
    and *parser generators*, such as PEGN ^([[9](#_footnotedef_9 "View footnote.")])
    and SpaCy’s Matcher class. And SpaCy allows you to define word patterns or grammars
    with a syntax similar to regular expressions. But there is no single algorithm
    or Python package that takes natural language text and turns it into machine instructions
    for automatic computation or execution. Stephen Wolfram has essentially spent
    his life trying to build a general-purpose intelligent "computational" machine
    that can interact with us in plain English. Even he has resorted to assembling
    a system out of many different NLP and AI algorithms that must be constantly expanded
    and evolved to handle new kinds of natural language instructions.^([[10](#_footnotedef_10
    "View footnote.")]) And towards the end of this book you will learn about our
    open source chatbot framework `qary.ai` that allows you to plug in any Python
    algorithm you can find or dream up.^([[11](#_footnotedef_11 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: With this book, you can build on the shoulders of giants. If you understand
    all the concepts in this book, you too will be able to combine these approaches
    to create remarkably intelligent conversational chatbots. You will even be able
    to build bots that understand and generate more meaningful and truthful text than
    ChatGPT or whatever comes next in this world of rent-seeking AI apps.^([[12](#_footnotedef_12
    "View footnote.")]) You have a big advantage over BigTech, you actually care about
    your users.^([[13](#_footnotedef_13 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Natural language processing is an evolving practice in computer science and
    artificial intelligence (AI) concerned with processing natural languages such
    as English or Mandarin. This processing generally involves translating natural
    language into data (numbers) that a computer can use to learn about the world.
    This understanding of the world is sometimes used to generate natural language
    text that reflects that understanding.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter shows you how your software can *process* natural language to produce
    useful output. You might even think of your program as a natural language interpreter,
    similar to how the Python interpreter processes source code. When the computer
    program you develop processes natural language, it will be able to act on those
    statements or even reply to them.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a programming language where each keyword has an unambiguous interpretation,
    natural languages are much more fuzzy. This fuzziness of natural language leaves
    open to you the interpretation of each word. So, you get to choose how the bot
    responds to each situation. Later you will explore advanced techniques in which
    the machine can learn from examples, without you knowing anything about the content
    of those examples.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A natural language processing system is called a "pipeline" because it natural
    language must be processed in several stages. Natural language text flows in one
    end and text or data flows out of the other end, depending on what sections of
    "pipe" (Python code) you include in your pipeline. It’s like a conga line of Python
    snakes passing the data along from one to the next.
  prefs: []
  type: TYPE_NORMAL
- en: You will soon have the power to write software that does interesting, human-like
    things with text. This book will teach you how to teach machines to carry on a
    conversation. It may seem a bit like magic, as new technology often does, at first.
    But you will pull back the curtain and explore the technology behind these magic
    shows. You will soon discover all the props and tools you need to do the magic
    tricks yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.1 Natural Language Understanding (NLU)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A really important part of NLP is the automatic processing of text to extract
    a numerical representation of the *meaning* of that text. This is the *natural
    language understanding* (NLU) part of NLP. The numerical representation of the
    meaning of natural language usually takes the form of a vector called an embedding.
    Machines can use embeddings to do all sorts of useful things. Embeddings are used
    by search engines to understand what your search query means and then find you
    web pages that contain information about that topic. And the embedding vectors
    for emails in your inbox are used by your email service to classify those emails
    as Important or not.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 Natural Language Understanding (NLU)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![text NLU vector graphviz](images/text-NLU-vector-graphviz.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Machines can accomplish many common NLU tasks with high accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: semantic search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: text alignment (for translation or plagiarism detection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: paraphrase recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: intent classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: authorship attribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And recent advances in deep learning have made it possible to solve many NLU
    tasks that were impossible only ten years ago:'
  prefs: []
  type: TYPE_NORMAL
- en: analogy problem solving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reading comprehension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: extractive summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: medical diagnosis based on symptom descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, there remain many NLU tasks where humans significantly outperform
    machines. Some problems require the machine to have common-sense knowledge, learn
    the logical relationships between those common-sense facts, and use all of this
    on the context surrounding a particular piece of text. This makes these problems
    much more difficult for machines:'
  prefs: []
  type: TYPE_NORMAL
- en: euphemism & pun recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: humor & sarcasm recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hate-speech & troll detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: logical entailment and fallacy recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: database schema discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: knowledge extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll learn the current state-of-the-art approaches to NLU and what is possible
    for these difficult problems. And your *behind-the-scenes* understanding of NLU
    will help you increase the effectiveness of your NLU pipelines for your particular
    applications, even on these hard problems.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.2 Natural Language Generation (NLG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may not be aware that machines can also compose text that sounds human-like.
    Machines can create human-readable text based on a numerical representation of
    the meaning and sentiment you would like to convey. This is the *natural language
    generation* (NLG) side of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.2 Natural Language Generation (NLG)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![vector NLG text graphviz](images/vector-NLG-text-graphviz.png)'
  prefs: []
  type: TYPE_IMG
- en: You will soon master many common NLG tasks that build on your NLU skills. The
    following tasks mainly rely on your ability to *encode* natural language into
    meaningful embedding vectors with NLU.
  prefs: []
  type: TYPE_NORMAL
- en: synonym substitution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: frequently-asked question answering (information retrieval)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: extractive generation of question answers (reading comprehension tests)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spelling and grammar correction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: casual conversation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you understand how to accomplish these foundational tasks that help you
    hone your NLU skill, more advanced NLG tasks will be within your reach.
  prefs: []
  type: TYPE_NORMAL
- en: abstractive summarization and simplification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: machine translation with neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sentence paraphrasing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: therapeutic conversational AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: factual question generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discussion facilitation and moderation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: argumentative essay writing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you understand how to summarize, paraphrase and translate text that gives
    you the ability to "translate" a text message into an appropriate response. You
    can even suggest new text for your user to include in their own writing. And you
    will discover approaches that help you summarize and generate longer and longer,
    and more complicated text.
  prefs: []
  type: TYPE_NORMAL
- en: build a bot that can participate in debate on social media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: compose poetry and song lyrics that don’t sound robotic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: compose jokes and sarcastic comments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generate text that fools (hacks) other people’s NLU pipelines into doing what
    you want
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: measure the robustness of NLP pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: automatically summarize long technical documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: compose programming language expressions from natural language descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This last development in NLG is particularly powerful. Machines can now write
    correct code that comes close to matching your intent based only on a natural
    language description. Machines aren’t programming themselves yet, but they may
    soon, according to the latest (September 2023) consensus on Metaculus. The community
    predicts that by September, 2026, we will have "AIs program programs that can
    program AIs."^([[14](#_footnotedef_14 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: The combination of NLU and NLG will give you the tools to create machines that
    interact with humans in surprising ways.^([[15](#_footnotedef_15 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.3 Plumbing it all together for positive-impact AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you understand how NLG and NLU work, you will be able to assemble them
    into your own NLP pipelines, like a plumber. Businesses are already using pipelines
    like these to extract value from their users.
  prefs: []
  type: TYPE_NORMAL
- en: You too can use these pipelines to further *your* own objectives in life, business,
    and social impact. This technology explosion is a rocket that you can ride and
    maybe steer a little bit. You can use it in your life to handle your inbox and
    journals while protecting your privacy and maximizing your mental well-being.
    Or you can advance your career by showing your peers how machines that understand
    and generate words can improve the efficiency and quality of almost any information-age
    task. And as an engineer who thinks about the impact of your work on society,
    you can help nonprofits build NLU and NLG pipelines that lift up the needy. As
    an entrepreneur, you can help create a regenerative prosocial business that spawns
    whole new industries and communities that thrive together.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how NLP works will open your eyes and empower you. You will soon
    see all the ways machines are being used to mine your words for profit, often
    at your expense. And you will see how machines are training you to become more
    easily manipulated. This will help you insulate yourself, and perhaps even fight
    back. You will soon learn how to survive in a world overrun with algorithms that
    manipulate you. You will harness the power of NLP to protect your well-being and
    contribute to the health of society as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Machines that can understand and generate natural language harness the power
    of words. Because machines can now understand and generate text that seems human,
    they can act on your behalf in the world. You’ll be able to create bots that will
    automatically follow your wishes and accomplish the goals you program them to
    achieve. But, beware Aladdin’s Three Wishes trap. Your bots may create a tsunami
    of blowback for your business or your personal life. Be careful about the goals
    you give your bots.^([[16](#_footnotedef_16 "View footnote.")]) This is called
    the "AI control problem" or the challenge of "AI safety."^([[17](#_footnotedef_17
    "View footnote.")]) Like the age-old three-wishes problem, you may find yourself
    trying to undo all the damage caused by your earlier wishes and bots.
  prefs: []
  type: TYPE_NORMAL
- en: The control problem and AI safety are not the only challenges you will face
    on your quest for positive-impact NLP. The danger of superintelligent AI that
    can manipulate us into giving it ever greater power and control may be decades
    away, but the danger of dumb AI that deceives and manipulates us has been around
    for years. The search and recommendation engine NLP that determines which posts
    you are allowed to see is not doing what you want, it is doing what investors
    want, stealing your attention, time and money.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you use the search feature of meetup.com to try to find when
    the next San Diego Python User Group meetup is happening, you will find that they
    give you everything except what you are looking for. It doesn’t matter if you
    have previously signed up for and attended these meetups for years, no matter
    how much information you give them their NLP will always choose money-making links
    for them over useful links for you. Try searching for "DefCon 31 Cory Doctorow"
    on YouTube. Instead of his famous rant against platform rent-seeking, you will
    only see ads and videos that the platform’s owners think will keep you enthralled
    in ads and prevent you from waking up from this trance. Researchers call this
    the "AI ethics" challenge, and the more direct ones call it what it is, the AI
    enshittification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 The magic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is so magical about a machine that can read and write in a natural language?
    Machines have been processing languages since computers were invented. But those
    were computer languages, such as Ada, Bash, and C, designed for computers to be
    able to understand. Programming languages avoid ambiguity so that computers can
    always do exactly what you *tell* them to do, even if that is not always what
    you *want* them to do.
  prefs: []
  type: TYPE_NORMAL
- en: Computer languages can only be interpreted (or compiled) in one correct way.
    With NLP you can talk to machines in your own language rather than having to learn
    computerese. When software can process languages not designed for machines to
    understand, it is magic — something we thought only humans could do.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, machines can access a massive amount of natural language text, such
    as Wikipedia, to learn about the world and human thought. Google’s index of natural
    language documents is well over 100 million gigabytes,^([[18](#_footnotedef_18
    "View footnote.")]) and that is just the index. And that index is incomplete.
    The size of the actual natural language content currently online probably exceeds
    100 billion gigabytes.^([[19](#_footnotedef_19 "View footnote.")]) This massive
    amount of natural language text makes NLP a useful tool.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Today, Wikipedia lists approximately 700 programming languages. Ethnologue_
    ^([[20](#_footnotedef_20 "View footnote.")]) identifies more than 7,000 natural
    languages. And that doesn’t include many other natural language sequences that
    can be processed using the techniques you’ll learn in this book. The sounds, gestures,
    and body language of animals, as well as the DNA and RNA sequences within their
    cells, can all be processed with NLP.^([[21](#_footnotedef_21 "View footnote.")])^([[22](#_footnotedef_22
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Machines with the capability to process something natural is not natural. It
    is kind of like building a building that can do something useful with architectural
    designs. When software can process languages not designed for machines to understand,
    it seems magical — something we thought was a uniquely human capability.
  prefs: []
  type: TYPE_NORMAL
- en: For now, you only need to think about one natural language —  English. You’ll
    ease into more difficult languages like Mandarin Chinese later in the book. But
    you can use the techniques you learn in this book to build software that can process
    any language, even a language you do not understand or has yet to be deciphered
    by archaeologists and linguists. We are going to show you how to write software
    to process and generate that language using only one programming language, Python.
  prefs: []
  type: TYPE_NORMAL
- en: Python was designed from the ground up to be a readable language. It also exposes
    a lot of its own language processing "guts." Both of these characteristics make
    it a natural choice for learning natural language processing. It is a great language
    for building maintainable production pipelines for NLP algorithms in an enterprise
    environment, with many contributors to a single codebase. We even use Python in
    lieu of the "universal language" of mathematics and mathematical symbols, wherever
    possible. After all, Python is an unambiguous way to express mathematical algorithms,
    ^([[23](#_footnotedef_23 "View footnote.")]) and it is designed to be as readable
    as possible by programmers like you.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Language and thought
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linguists and philosophers such as Sapir and Whorf postulated that our vocabulary
    affects the thoughts we think. For example, Australian Aborigines have words to
    describe the position of objects on their body according to the cardinal points
    of the compass. They don’t talk about the boomerang in their right hand, they
    talk about the boomerang on the north side of their body. This makes them adept
    at communicating and orienteering during hunting expeditions. Their brains are
    constantly updating their understanding of their orientation in the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stephen Pinker flips that notion around and sees language as a window into
    our brains and how we think: "Language is a collective human creation, reflecting
    human nature, how we conceptualize reality, how we relate to one another."^([[24](#_footnotedef_24
    "View footnote.")]) Whether you think of words as affecting your thoughts or as
    helping you see and understand your thoughts, either way, they are packets of
    thought. You will soon learn the power of NLP to manipulate those packets of thought
    and amp up your understanding of words, …​ and maybe thought itself. It’s no wonder
    many businesses refer to NLP and chatbots as AI - Artificial Intelligence.'
  prefs: []
  type: TYPE_NORMAL
- en: What about math? We think with precise mathematical symbols and programming
    languages as well as with fuzzier natural language words and symbols. And we can
    use fuzzy words to express logical thoughts like mathematics concepts, theorems,
    and proofs. But words aren’t the only way we think. Jordan Elenberg, a geometer
    at Harvard, writes in his new book *Shape* about how he first "discovered" the
    commutative property of algebra while staring at a stereo speaker with a grid
    of dots, 6x8\. He’d memorized the multiplication table, the symbols for numbers.
    And he knew that you could reverse the order of symbols on either side of a multiplication
    symbol. But he didn’t really *know* it until he realized that he could visualize
    the 48 dots as 6 columns of 8 dots, or 8 rows of 6 dots. And it was the same dots!
    So it had to be the same number. It hit him at a deeper level, even deeper than
    the symbol manipulation rules that he learned in algebra class.
  prefs: []
  type: TYPE_NORMAL
- en: So you use words to communicate thoughts with others and with yourself. When
    ephemeral thoughts can be gathered up into words or symbols, they become compressed
    packets of thought that are easier to remember and to work with in your brain.
    You may not realize it, but as you are composing sentences you are actually rethinking
    and manipulating and repackaging these thoughts. What you want to say, and the
    idea you want to share is crafted while you are speaking or writing. This act
    of manipulating packets of thought in your mind is called "symbol manipulation"
    by AI researchers and neuroscientists. In fact, in the age of GOFAI (Good Old-Fashioned
    AI) researchers assumed that AI would need to learn to manipulate natural language
    symbols and logical statements the same way it compiles programming languages.
    In this book, you’re going to learn how to teach a machine to do symbol manipulation
    on natural language in Chapter 11.
  prefs: []
  type: TYPE_NORMAL
- en: But that’s not the most impressive power of NLP. Think back to a time when you
    had a difficult email to send to someone close. Perhaps you needed to apologize
    to a boss or teacher, or maybe your partner or a close friend. Before you started
    typing, you probably started thinking about the words you would use, the reasons
    or excuses for why you did what you did. And then you imagined how your boss or
    teacher would perceive those words. You probably reviewed in your mind what you
    would say many many times before you finally started typing. You manipulated packets
    of thought as words in your mind. And when you did start typing, you probably
    wrote and rewrote twice as many words as you actually sent. You chose your words
    carefully, discarding some words or ideas and focusing on others.
  prefs: []
  type: TYPE_NORMAL
- en: The act of revision and editing is a thinking process. It helps you gather your
    thoughts and revise them. And in the end, whatever comes out of your mind is not
    at all like the first thoughts that came to you. The act of writing improves how
    you think, and it will improve how machines think as they get better and better
    at reading and writing.
  prefs: []
  type: TYPE_NORMAL
- en: So reading and writing is thinking. And words are packets of thought that you
    can store and manipulate to improve those thoughts. We use words to put thoughts
    into clumps or compartments that we can play with in our minds. We break complicated
    thoughts into several sentences. And we reorder those thoughts so they make more
    sense to our reader or even our future self. Every sentence in this 2nd edition
    of the book has been edited several times - sometimes with the help of generous
    readers of the LiveBook. ^([[25](#_footnotedef_25 "View footnote.")]) I’ve deleted,
    rewritten and reordered these paragraphs several times just now, with the help
    of suggestions and ideas from friends and readers like you.^([[26](#_footnotedef_26
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: But words and writing aren’t the *only* way to think logically and deeply. Drawing,
    diagramming, and even dancing and acting out are all expressions of thought. And
    we visually imagine these drawings in our minds — sketching ideas and concepts
    and thoughts in our head. And sometimes you just physically move things around
    or act things out in the real world. But the act of composing words into sentences
    and sentences into paragraphs is something that we do almost constantly.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and writing is also a special kind of thought. It seems to compress
    our thoughts and make them easier to remember and manage within our heads. Once
    we know the perfect word for a concept, we can file it away in our minds. We don’t
    have to keep refreshing it to understand it. We know that once we think of the
    word again, the concept will come flooding back and we can use it again.
  prefs: []
  type: TYPE_NORMAL
- en: This is all thinking or what is sometimes called *cognition*. So by teaching
    machines to understand and compose text, you are in some small way, teaching them
    to think. This is why people think of NLP as artificial intelligence (AI). And
    conversational AI is one of the most widely recognized and useful forms of AI.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Machines that converse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though you spend a lot of time working with words as packets of thought internally
    within your head, the real fun is when you use those words to interact with others.
    The act of conversation brings two (or more!) people into your thinking. This
    can create a powerful positive feedback loop that reinforces good ideas and weeds
    out weak ones.
  prefs: []
  type: TYPE_NORMAL
- en: Words are critical to this process. They are our shared thought vocabulary.
    When you want to trigger a thought in another person’s brain, all you need to
    do is to say the right words so that they understand some of the thoughts in your
    mind. For example, when you are feeling great pain, frustration or shock, you
    can use a curse word. And you can almost be guaranteed to cause that shock and
    discomfort to be conveyed to your listener or reader. That is the sole purpose
    of curse words — to shock (and awe?) your listener.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There is *another_NLP* that takes this idea to the extreme. Neuro-linguistic
    programming (the *other_NLP*) is a pseudoscientific psychotherapy approach that
    claims to change your behavior through the use of words. Because there is money
    to be made in claiming to help people achieve their life goals, this pseudoscience
    has taken on a cult status for the practitioners who teach it (preach it?).^([[27](#_footnotedef_27
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: As with astrology, fortune telling, hypnotherapy, conspiracy theories, religions
    and cults, there is usually a small hint of truth somewhere within it. Words do
    indeed affect our thoughts. And thoughts do affect our behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Though we cannot "program" another human with our words, we can use them to
    communicate extremely complex ideas. When you engage in conversation you are acting
    as a neuron in the collective consciousness, the hive mind. Unfortunately, when
    profit motives and unfettered competition is the rule of the day, the hornet nest
    of social media is the result.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language cannot be directly translated into a precise set of mathematical
    operations. But natural language does contain information and instructions that
    can be extracted. Those pieces of information and instruction can be stored, indexed,
    searched, or immediately acted upon. One of those actions could be to generate
    a sequence of words in response to a statement. This is the function of the "dialog
    engine" or chatbot that you will build.
  prefs: []
  type: TYPE_NORMAL
- en: This book focuses entirely on English text documents and messages, not spoken
    statements. Chapter 7 does give you a brief foray into processing audio files,
    Morse code. But most of NLPiA is focused on the words that have been put to paper…​
    or at least put to transistors in a computer. There are whole books on speech
    recognition and speech-to-text (STT) systems and text-to-speech (TTS) systems.
    There are ready-made open-source projects for STT and TTS. If your application
    is a mobile application, modern smartphone SDKs provide you with speech recognition
    and speech generation APIs. If you want your virtual assistant to live in the
    cloud, there are Python packages to accomplish SST and TTS on any Linux server
    with access to your audio stream.
  prefs: []
  type: TYPE_NORMAL
- en: In this book you will focus on what happens between the *ears* of the machine.
    This can help you build a smarter voice assistant when you add your *brains* to
    open source projects such as Home Assistant,^([[28](#_footnotedef_28 "View footnote.")])
    Mycroft AI,^([[29](#_footnotedef_29 "View footnote.")]) or OVAL Genie,^([[30](#_footnotedef_30
    "View footnote.")]). And you’ll understand all the helpful NLP that the big boys
    could be giving you within their voice assistants …​ assuming commercial voice
    assistants wanted to help you with more than just lightening your wallet.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition systems
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to build a customized speech recognition or generation system, that
    undertaking is a whole book in itself; we leave that as an "exercise for the reader."
    It requires a lot of high-quality labeled data, voice recordings annotated with
    their phonetic spellings, and natural language transcriptions aligned with the
    audio files. Some of the algorithms you learn in this book might help, but most
    of the algorithms are quite different.^([[31](#_footnotedef_31 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 The math
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Processing natural language to extract useful information can be difficult.
    It requires tedious statistical bookkeeping, but that is what machines are for.
    Like many other technical problems, solving it is a lot easier once you know the
    answer. Machines still cannot perform most practical NLP tasks, such as conversation
    and reading comprehension, as accurately and reliably as humans. So you might
    be able to tweak the algorithms you learn in this book to do some NLP tasks a
    bit better.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques you will learn, however, are powerful enough to create machines
    that can surpass humans in both accuracy and speed for some surprisingly subtle
    tasks. For example, you might not have guessed that recognizing sarcasm in an
    isolated Twitter message can be done more accurately by a machine than by a human.
    Well-trained human judges could not match the performance (68% accuracy) of a
    simple sarcasm detection NLP algorithm.^([[32](#_footnotedef_32 "View footnote.")])
    Simple BOW (bag-of-words) models achieve 63% accuracy and state of the art transformer
    models achieve 81% accuracy. ^([[33](#_footnotedef_33 "View footnote.")]) Do not
    worry, humans are still better at recognizing humor and sarcasm within an ongoing
    dialog because we are able to maintain information about the context of a statement.
    However, machines are getting better and better at maintaining context. This book
    helps you incorporate context (metadata) into your NLP pipeline if you want to
    try your hand at advancing the state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: Once you extract structured numerical data, or vectors, from natural language,
    you can take advantage of all the tools of mathematics and machine learning. We
    use the same linear algebra tricks as the projection of 3D objects onto a 2D computer
    screen, something that computers and drafters were doing long before natural language
    processing came into its own. These breakthrough ideas opened up a world of "semantic"
    analysis, allowing computers to interpret and store the "meaning" of statements
    rather than just word or character counts. Semantic analysis, along with statistics,
    can help resolve the ambiguity of natural language — the fact that words or phrases
    often have multiple meanings or interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: So extracting information is not at all like building a programming language
    compiler (fortunately for you). The most promising techniques bypass the rigid
    rules of regular grammars (patterns) or formal languages. You can rely on statistical
    relationships between words instead of a deep system of logical rules.^([[34](#_footnotedef_34
    "View footnote.")]) Imagine if you had to define English grammar and spelling
    rules in a nested tree of if…​then statements. Could you ever write enough rules
    to deal with every possible way that words, letters, and punctuation can be combined
    to make a statement? Would you even begin to capture the semantics, the meaning
    of English statements? Even if it were useful for some kinds of statements, imagine
    how limited and brittle this software would be. Unanticipated spelling or punctuation
    would break or befuddle your algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Natural languages have an additional "decoding" challenge that is even harder
    to solve. Speakers and writers of natural languages assume that a human is the
    one doing the processing (listening or reading), not a machine. So when I say
    "good morning," I assume that you have some knowledge about what makes up a morning,
    including that the morning comes before noon, afternoon, and evening, but it also
    comes after midnight. You need to know that morning can represent times of day
    as well as a general period of time. The interpreter is assumed to know that "good
    morning" is a common greeting, and that it does not contain much information at
    all about the morning. Rather, it reflects the state of mind of the speaker and
    her readiness to speak with others.
  prefs: []
  type: TYPE_NORMAL
- en: This theory of mind about the human processor of language turns out to be a
    powerful assumption. It allows us to say a lot with few words if we assume that
    the "processor" has access to a lifetime of common sense knowledge about the world.
    This degree of compression is still out of reach for machines. There is no clear
    "theory of mind" you can point to in an NLP pipeline. However, we show you techniques
    in later chapters to help machines build ontologies, or knowledge bases, of common
    sense knowledge to help interpret statements that rely on this knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language processing is everywhere. It is so ubiquitous that you’d have
    a hard time getting through the day without interacting with several NLP algorithms
    every hour. Some of the examples here may surprise you.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.3 Graph of NLP applications
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![nlp applications](images/nlp-applications.png)'
  prefs: []
  type: TYPE_IMG
- en: At the core of this network diagram are the NLU and NLG **sides** of NLP. Branching
    out from the NLU hub node are foundational applications like sentiment analysis
    and search. These eventually connect with foundational NLG tools such as spelling
    correctors and automatic code generators to create conversational AI and even
    pair programming assistants.
  prefs: []
  type: TYPE_NORMAL
- en: A search engine can provide more meaningful results if it indexes web pages
    or document archives in a way that takes into account the meaning of natural language
    text. Autocomplete uses NLP to complete your thought and is common among search
    engines and mobile phone keyboards. Many word processors, browser plugins, and
    text editors have spelling correctors, grammar checkers, concordance composers,
    and most recently, style coaches. Some dialog engines (chatbots) use natural language
    search to find a response to their conversation partner’s message.
  prefs: []
  type: TYPE_NORMAL
- en: NLP pipelines that generate text can be used not only to compose short replies
    in chatbots and virtual assistants but also to assemble much longer passages of
    text. The Associated Press uses NLP "robot journalists" to write entire financial
    news articles and sporting event reports.^([[35](#_footnotedef_35 "View footnote.")])
    Bots can compose weather forecasts that sound a lot like what your hometown weather
    person might say, perhaps because human meteorologists use word processors with
    NLP features to draft scripts.
  prefs: []
  type: TYPE_NORMAL
- en: More and more businesses are using NLP to automate their business processes.
    This can improve team productivity and job satisfaction, as well as the quality
    of the product. For example, chatbots can automate the responses to many customer
    service requests.^([[36](#_footnotedef_36 "View footnote.")]) NLP spam filters
    in early email programs helped email overtake telephone and fax communication
    channels in the '90s. And some teams use NLP to automate and personalize e-mails
    between teammates or communicate with job applicants.
  prefs: []
  type: TYPE_NORMAL
- en: NLP pipelines, like all algorithms, make mistakes and are almost always biased
    in many ways. So if you use NLP to automate communication with humans, be careful.
    At Tangible AI we use NLP for the critical job of helping us find developers to
    join our team, so we were extremely cautious. We used NLP to help us filter out
    job applications only when the candidate was nonresponsive or did not appear to
    understand several questions on the application. We had rigorous quality control
    on the NLP pipeline with periodic random sampling of the model predictions. We
    used simple models and sample-efficient NLP models ^([[37](#_footnotedef_37 "View
    footnote.")]) to focus human attention on those predictions where the machine
    learning was least confident — see the `predict_proba` method on SciKit Learn
    classifiers. As a result NLP for HR (human relations) actually cost us more time
    and attention and did not save us money. But it did help us cast a broader net
    when looking for candidates. We had hundreds of applications from around the globe
    for a junior developer role, including applicants located in Ukraine, Africa,
    Asia and South America. NLP helped us quickly evaluate English and technical skill
    before proceeding with interviews and paid take-home assignments.
  prefs: []
  type: TYPE_NORMAL
- en: The spam filters have retained their edge in the cat-and-mouse game between
    spam filters and spam generators for email but may be losing in other environments
    like social networks. An estimated 20% of the tweets about the 2016 US presidential
    election were composed by chatbots.^([[38](#_footnotedef_38 "View footnote.")])
    These bots amplify their owners' and developers' viewpoints with the resources
    and motivation to influence popular opinion. And these "puppet masters" tend to
    be foreign governments or large corporations.
  prefs: []
  type: TYPE_NORMAL
- en: NLP systems can generate more than just short social network posts. NLP can
    be used to compose lengthy movie and product reviews on online shop websites and
    elsewhere. Many reviews are the creation of autonomous NLP pipelines that have
    never set foot in a movie theater or purchased the product they are reviewing.
    And it’s not just movies, a large portion of all product reviews that bubble to
    the top in search engines and online retailers are fake. You can use NLP to help
    search engines and prosocial social media communities (Mastodon) ^([[39](#_footnotedef_39
    "View footnote.")]) ^([[40](#_footnotedef_40 "View footnote.")]) detect and remove
    misleading or fake posts and reviews.^([[41](#_footnotedef_41 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: There are chatbots on Slack, IRC, and even customer service websites — places
    where chatbots have to deal with ambiguous commands or questions. And chatbots
    paired with voice recognition and generation systems can even handle lengthy conversations
    with an indefinite goal or "objective function" such as making a reservation at
    a local restaurant.^([[42](#_footnotedef_42 "View footnote.")]) NLP systems can
    answer phones for companies that want something better than a phone tree, but
    they do not want to pay humans to help their customers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Consider the ethical implications whenever you, or your boss, decide to deceive
    your users. With its **Duplex** demonstration at Google IO, engineers and managers
    overlooked concerns about the ethics of teaching chatbots to deceive humans. In
    most "entertainment" social networks, bots are not required to reveal themselves.
    We unknowingly interact with these bots on Facebook, Reddit, Twitter and even
    dating apps. Now that bots and deep fakes can so convincingly deceive us, the
    AI control problem ^([[43](#_footnotedef_43 "View footnote.")]). Yuval Harari’s
    cautionary forecast of "Homo Deus"^([[44](#_footnotedef_44 "View footnote.")])
    may come sooner than we think.
  prefs: []
  type: TYPE_NORMAL
- en: NLP systems exist that can act as email "receptionists" for businesses or executive
    assistants for managers. These assistants schedule meetings and record summary
    details in an electronic Rolodex, or CRM (customer relationship management system),
    interacting with others by email on their boss’s behalf. Companies are putting
    their brand and face in the hands of NLP systems, allowing bots to execute marketing
    and messaging campaigns. And some inexperienced daredevil NLP textbook authors
    are letting bots author several sentences in their book. More on that later.
  prefs: []
  type: TYPE_NORMAL
- en: The most surprising and powerful application of NLP is in psychology. If you
    think that a chatbot could never replace your therapist, you may be surprised
    by recent advancements.^([[45](#_footnotedef_45 "View footnote.")]) Commercial
    virtual companions such as Xiaoice in China and Replika.AI in the US helped hundreds
    of millions of lonely people survive the emotional impact of social isolation
    during Covid-19 lockdowns in 2020 and 2021.^([[46](#_footnotedef_46 "View footnote.")])
    Fortunately, you don’t have to rely on engineers at large corporations to look
    out for your best interests. Many psychotherapy and cognitive assistant technology
    is completely free and open source.^([[47](#_footnotedef_47 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Processing programming languages with NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern deep-learning NLP pipelines have proven so powerful and versatile that
    they can now accurately understand and generate programming languages. Rule-based
    compilers and generators for NLP were helpful for simple tasks like autocomplete
    and providing snippet suggestions. And users can often use information retrieval
    systems, or search engines, to find snippets of code to complete their software
    development project.
  prefs: []
  type: TYPE_NORMAL
- en: And these tools just got a whole lot smarter. Previous code generation tools
    were **extractive**. Extractive text generation algorithms find the most relevant
    text in your history and just regurgitate it, verbatim as a suggestion to you.
    So if the term "prosocial artificial intelligence" appears a lot in the text an
    algorithm was trained on, an auto-complete will recommend the word "artificial
    intelligence" to follow prosocial rather than just "intelligence". You can see
    how this might start to influence what you type and how you think.
  prefs: []
  type: TYPE_NORMAL
- en: 'And transformers have advanced NLP even further recently with massive deep
    learning networks that are more **abstractive**, generating new text you haven’t
    seen or typed before. For example, the 175 billion parameter version of GPT-3
    was trained on all of GitHub to create a model called Codex. Codex is part of
    the Copilot plugin for VSCode. It suggests entire function and class definitions
    and all you have to supply is a short comment and the first line of the function
    definition. Here is the example for the typescript prompt shown on the copilot
    home page: ^([[48](#_footnotedef_48 "View footnote.")])'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the demo animation, Copilot then generated the rest of the typescript required
    for a working function that estimated the sentiment of a body of text. Think about
    that for a second. Microsoft’s algorithm is writing code for you to analyze the
    sentiment of natural language text, such as the text you might be writing up in
    your emails or personal essay. And the examples shown on the Copilot home page
    all lean towards Microsoft products and services. This means you will end up with
    an NLP pipeline that has **Microsoft’s** perspective on what is positive and what
    is not. It values what **Microsoft** told it to value. Just as Google Search influenced
    the kind of code you wrote indirectly, now Microsoft algorithms are directly writing
    code for you.
  prefs: []
  type: TYPE_NORMAL
- en: Since you’re reading this book, you are probably planning to build some pretty
    cool NLP pipelines. You may even build a pipeline that helps you write blog posts
    and chatbots and core NLP algorithms. This can create a positive feedback loop
    that shifts the kinds of NLP pipelines and models that are built and deployed
    by engineers like you. So pay attention to the **meta** tools that you use to
    help you code and think. These have huge leverage on the direction of your code,
    and the direction of your life.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Language through a computer’s "eyes"
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you type "Good Morning Rosa", a computer sees only "01000111 01101111
    01101111 …​". How can you program a chatbot to respond to this binary stream intelligently?
    Could a nested tree of conditionals (`if`…​ `else`…​" statements) check each one
    of those bits and act on them individually? This would be equivalent to writing
    a special kind of program called a finite state machine (FSM). An FSM that outputs
    a sequence of new symbols as it runs, like the Python `str.translate` function,
    is called a finite state transducer (FST). You’ve probably already built a FSM
    without even knowing it. Have you ever written a regular expression? That’s the
    kind of FSM we use in the next section to show you one possible approach to NLP:
    the pattern-based approach.'
  prefs: []
  type: TYPE_NORMAL
- en: What if you decided to search a memory bank (database) for the exact same string
    of bits, characters, or words, and use one of the responses that other humans
    and authors have used for that statement in the past? But imagine if there was
    a typo or variation in the statement. Our bot would be sent off the rails. And
    bits aren’t continuous or forgiving — they either match or they do not. There
    is no obvious way to find a similarity between two streams of bits that takes
    into account what they signify. The bits for "good" will be just as similar to
    "bad!" as they are to "okay".
  prefs: []
  type: TYPE_NORMAL
- en: But let’s see how this approach would work before we show you a better way.
    Let’s build a small regular expression to recognize greetings like "Good morning
    Rosa" and respond appropriately — our first tiny chatbot!
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.1 The language of locks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Surprisingly the humble combination lock is actually a simple language processing
    machine. So, if you are mechanically inclined, this section may be illuminating.
    But if you do not need mechanical analogies to help you understand algorithms
    and how regular expressions work, then you can skip this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'After finishing this section, you will never think of your combination bicycle
    lock the same way again. A combination lock certainly can’t read and understand
    the textbooks stored inside a school locker, but it can understand the language
    of locks. It can understand when you try to "tell" it a "password": a combination.
    A padlock combination is any sequence of symbols that matches the "grammar" (pattern)
    of lock language. Even more importantly, the padlock can tell if a lock "statement"
    matches a particularly meaningful statement, the one for which there is only one
    correct "response," to release the catch holding the U-shaped hasp so you can
    get into your locker.'
  prefs: []
  type: TYPE_NORMAL
- en: This lock language (regular expressions) is a particularly simple one. But it’s
    not so simple that we can’t use it in a chatbot. We can use it to recognize a
    key phrase or command to unlock a particular action or behavior.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we’d like our chatbot to recognize greetings such as "Hello Rosa,"
    and respond to them appropriately. This kind of language, like the language of
    locks, is a formal language because it has strict rules about how an acceptable
    statement must be composed and interpreted. If you’ve ever written a math equation
    or coded a programming language expression, you’ve written a formal language statement.
  prefs: []
  type: TYPE_NORMAL
- en: Formal languages are a subset of natural languages. Many natural language statements
    can be matched or generated using a formal language grammar, such as regular expressions
    or regular grammars. That’s the reason for this diversion into the mechanical,
    "click, whirr"^([[49](#_footnotedef_49 "View footnote.")]) language of locks.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.2 Regular expressions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular expressions use a special class of formal language grammars called a
    regular grammar. Regular grammars have predictable, provable behavior, and yet
    are flexible enough to power some of the most sophisticated dialog engines and
    chatbots on the market. Amazon Alexa and Google Now are mostly pattern-based engines
    that rely on regular grammars. Deep, complex regular grammar rules can often be
    expressed in a single line of code called a regular expression. There are successful
    chatbot frameworks in Python, like `Will`, ^([[50](#_footnotedef_50 "View footnote.")])
    and `qary` ^([[51](#_footnotedef_51 "View footnote.")]) that rely exclusively
    on this kind of language processing to produce some effective chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Regular expressions implemented in Python and in Posix (Unix) applications such
    as `grep` are not true regular grammars. They have language and logic features
    such as look-ahead and look-back that make leaps of logic and recursion that aren’t
    allowed in a regular grammar. As a result, regular expressions aren’t provably
    halting; they can sometimes "crash" or run forever. ^([[52](#_footnotedef_52 "View
    footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: You may be saying to yourself, "I’ve heard of regular expressions. I use `grep`.
    But that’s only for search!" And you are right. **R**egular **E**xpressions are
    indeed used mostly for search, for sequence matching. But anything that can find
    matches within text is also great for carrying out a dialog. Some chatbots, use
    "search" to find sequences of characters within a user statement that they know
    how to respond to. These recognized sequences then trigger a scripted response
    appropriate to that particular regular expression match. And that same regular
    expression can also be used to extract a useful piece of information from a statement.
    A chatbot can add that bit of information to its knowledge base about the user
    or about the world the user is describing.
  prefs: []
  type: TYPE_NORMAL
- en: A machine that processes this kind of language can be thought of as a formal
    mathematical object called a finite state machine or deterministic finite automaton
    (DFA). FSMs come up again and again in this book. So, you will eventually get
    a good feel for what they’re used for without digging into FSM theory and math.
    For those who can’t resist trying to understand a bit more about these computer
    science tools, figure 1.1 shows where FSMs fit into the nested world of automata
    (bots). And the side note that follows explains a bit more formal detail about
    formal languages.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.4 Kinds of automata
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![kinds of automata](images/kinds-of-automata.png)'
  prefs: []
  type: TYPE_IMG
- en: Formal mathematical explanation of formal languages
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Kyle Gorman describes programming languages this way:'
  prefs: []
  type: TYPE_NORMAL
- en: Most (if not all) programming languages are drawn from the class of context-free
    languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context free languages are parsed with context-free grammars, which provide
    efficient parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regular languages are also efficiently parsable and used extensively in
    computing for string matching.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: String matching applications rarely require the expressiveness of context-free.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a number of formal language classes, a few of which are shown here
    (in decreasing complexity):^([[53](#_footnotedef_53 "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursively enumerable
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Context-sensitive
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Context-free
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Natural languages are:'
  prefs: []
  type: TYPE_NORMAL
- en: Not regular ^([[54](#_footnotedef_54 "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not context-free ^([[55](#_footnotedef_55 "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can’t be defined by any formal grammar ^([[56](#_footnotedef_56 "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.5 A simple chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us build a quick and dirty chatbot. It will not be very capable, and it
    will require a lot of thinking about the English language. You will also have
    to hardcode regular expressions to match the ways people may try to say something.
    But do not worry if you think you couldn’t have come up with this Python code
    yourself. You will not have to try to think of all the different ways people can
    say something, like we did in this example. You will not even have to write regular
    expressions (regexes) to build an awesome chatbot. We show you how to build a
    chatbot of your own in later chapters without hardcoding anything. A modern chatbot
    can learn from reading (processing) a bunch of English text. And we show you how
    to do that in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern-matching chatbot is an example of a tightly controlled chatbot.
    Pattern-matching chatbots were common before modern machine learning chatbot techniques
    were developed. And a variation of the pattern-matching approach we show you here
    is used in chatbots like Amazon Alexa and other virtual assistants.
  prefs: []
  type: TYPE_NORMAL
- en: For now let’s build an FSM, a regular expression, that can speak lock language
    (regular language). We could program it to understand lock language statements,
    such as "01-02-03." Even better, we’d like it to understand greetings, things
    like "open sesame" or "hello Rosa."
  prefs: []
  type: TYPE_NORMAL
- en: An important feature of a prosocial chatbot is to be able to respond to a greeting.
    In high school, teachers often chastised me for being impolite when I’d ignore
    greetings like this while rushing to class. We surely do not want that for our
    benevolent chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: For communication between two machines, you would define a handshake with something
    like an `ACK` (acknowledgement) signal to confirm receipt of each message. But
    our machines are going to be interacting with humans who say things like "Good
    morning, Rosa". We do not want it sending out a bunch of chirps, beeps, or `ACK`
    messages, like it’s syncing up a modem or HTTP connection at the start of a conversation
    or web browsing session.
  prefs: []
  type: TYPE_NORMAL
- en: Human greetings and handshakes are a little more informal and flexible. So recognizing
    the greeting *intent* won’t be as simple as building a machine handshake. You
    will want a few different approaches in your toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An intent is a category of possible intentions the user has for the NLP system
    or chatbot. Words "hello" and "hi" might be collected together under the *greeting*
    intent, for when the user wants to start a conversation. Another intent might
    be to carry out some task or command, such as a "translate" command or the query
    "How do I say 'Hello' in Ukrainian?". You’ll learn about intent recognition throughout
    the book and put it to use in a chatbot in chapter 12.
  prefs: []
  type: TYPE_NORMAL
- en: 1.6 Keyword-based greeting recognizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your first chatbot will be straight out of the 80’s. Imagine you want a chatbot
    to help you select a game to play, like chess…​ or a Thermonuclear War. But first,
    your chatbot must find out if you are Professor Falken or General Beringer, or
    some other user that knows what they are doing.^([[57](#_footnotedef_57 "View
    footnote.")]) It will only be able to recognize a few greetings. But this approach
    can be extended to help you implement simple keyword-based intent recognizers
    on projects similar to those mentioned earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 1.1 Keyword detection using `str.split`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This simple NLP pipeline (program) has only two intent categories: "greeting"
    and "unknown" (`else`). And it uses a very simple algorithm called keyword detection.
    Chatbots that recognize the user’s intent like this have capabilities similar
    to modern command line applications or phone trees from the 90’s.'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based chatbots can be much more fun and flexible than this simple program.
    Developers have so much fun building and interacting with chatbots that they build
    chatbots to make even deploying and monitoring servers a lot of fun. *Chatops*,
    or DevOps with chatbots, has become popular on most software development teams.
    You can build a chatbot like this to recognize more intents by adding `elif` statements
    before the `else`. Or you can go beyond keyword-based NLP and start thinking about
    ways to improve it using regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 1.6.1 Pattern-based intent recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A keyword-based chatbot would recognize "Hi", "Hello", and "Greetings", but
    it wouldn’t recognize "Hiiii" or "Hiiiiiiiiiiii" - the more excited renditions
    of "Hi". Perhaps you could hardcode the first 200 versions of "Hi", such as `["Hi",
    "Hii", "Hiii", …​]`. Or you could programmatically create such a list of keywords.
    Or you could save yourself a lot of trouble and make your bot deal with literally
    infinite variations of "Hi" using *regular expressions*. Regular expression *patterns*
    can match text much more robustly than any hard-coded rules or lists of keywords.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions recognize patterns for any sequence of characters or symbols.^([[58](#_footnotedef_58
    "View footnote.")]) With keyword-based NLP, you and your users need to spell keywords
    and commands in exactly the same way for the machine to respond correctly. So
    your keyword greeting recognizer would miss greetings like "Hey" or even "hi"
    because those strings aren’t in your list of greeting words. And what if your
    "user" used a greeting that starts or ends with punctuation, such as "'sup" or
    "Hi,". You could do *case folding* with the `str.split()` method on both your
    greetings and the user statement. And you could add more greetings to your list
    of greeting words. You could even add misspellings and typos to ensure they aren’t
    missed. But that is a lot of manual "hard-coding" of data into your NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: You will soon learn how to use machine learning for more data-driven and automated
    NLP pipelines. And when you graduate to the much more complex and accurate *deep
    learning* models of chapter 7 and beyond, you will find that there is still much
    "brittleness" in modern NLP pipelines. Robin Jia’s thesis explains how to measure
    and improve NLP robustness in his thesis ([https://proai.org/robinjia-thesis](proai.org.html))]
    But for now, you need to understand the basics. When your user wants to specify
    actions with precise patterns of characters similar to programming language commands,
    that’s where regular expressions shine.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In regular expressions, you can specify a character class with square brackets.
    And you can use a dash (`-`) to indicate a range of characters without having
    to type them all out individually. So the regular expression `"[a-z]"` will match
    any single lowercase letter, "a" through "z". The star ("\*") after a character
    class means that the regular expression will match any number of consecutive characters
    if they are all within that character class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make our regular expression a lot more detailed to try to match more greetings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The "r" before the quote symbol (`r'`) indicates that the quoted string literal
    is a *raw* string. The "r" does not mean **regular** expression. A Python raw
    string just makes it easier to use the backslashes used to escape special symbols
    within a regular expression. Telling Python that a string is "raw" means that
    Python will skip processing the backslashes and pass them on to the regular expression
    parser (`re` package). Otherwise, you would have to escape each and every backslash
    in your regular expression with a double backslash (`'\\'`). So the whitespace
    matching symbol `'\s'` would become `'\\s'`, and special characters like literal
    curly braces would become `'\\{'` and `'\\}'`.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of logic packed into that first line of code, the regular expression.
    It gets the job done for a surprising range of greetings. But it missed that "Manning"
    typo, which is one of the reasons NLP is hard. In machine learning and medical
    diagnostic testing, that’s called a *false negative* classification error. Unfortunately,
    it will also match some statements that humans would be unlikely to ever say — a
    *false positive*, which is also a bad thing. Having both false positive and false
    negative errors means that our regular expression is both too liberal (inclusive)
    and too strict (exclusive). These mistakes could make our bot sound a bit dull
    and mechanical. We’d have to do a lot more work to refine the phrases it matches
    for the bot to behave in a more intelligent human-like way.
  prefs: []
  type: TYPE_NORMAL
- en: And this tedious work would be highly unlikely to ever succeed at capturing
    all the slang and misspellings people use. Fortunately, composing regular expressions
    by hand isn’t the only way to train a chatbot. Stay tuned for more on that later
    (the entire rest of the book). So we only use them when we need precise control
    over a chatbot’s behavior, such as when issuing commands to a voice assistant
    on your mobile phone.
  prefs: []
  type: TYPE_NORMAL
- en: But let’s go ahead and finish up our one-trick chatbot by adding an output generator.
    It needs to say something. We use Python’s string formatter to create a "template"
    for our chatbot response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So if you run this little script and chat to our bot with a phrase like "Hello
    Rosa", it will respond by asking about your day. If you use a slightly rude name
    to address the chatbot, she will be less responsive, but not inflammatory, to
    encourage politeness.^([[59](#_footnotedef_59 "View footnote.")]) If you name
    someone else who might be monitoring the conversation on a party line or forum,
    the bot will keep quiet and allow you and whomever you are addressing to chat.
    Obviously, there is no one else out there watching our `input()` line, but if
    this were a function within a larger chatbot, you want to deal with these sorts
    of things.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the limitations of computational resources, early NLP researchers
    had to use their human brain’s computational power to design and hand-tune complex
    logical rules to extract information from a natural language string. This is called
    a pattern-based approach to NLP. The patterns do not have to be merely character
    sequence patterns, like our regular expression. NLP also often involves patterns
    of word sequences, or parts of speech, or other "higher level" patterns. The core
    NLP building blocks like stemmers and tokenizers as well as sophisticated end-to-end
    NLP dialog engines (chatbots) like ELIZA were built this way, from regular expressions
    and pattern matching. The art of pattern-matching approaches to NLP is coming
    up with elegant patterns that capture just what you want, without too many lines
    of regular expression code.
  prefs: []
  type: TYPE_NORMAL
- en: Theory of a computational mind
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This classical NLP pattern-matching approach is based on the computational theory
    of mind (CTM). CTM theorizes that thinking is a deterministic computational process
    that acts in a single logical thread or sequence.^([[60](#_footnotedef_60 "View
    footnote.")]) Advancements in neuroscience and NLP led to the development of a
    "connectionist" theory of mind around the turn of the century. This newer theory
    inspired the artificial neural networks of deep learning used that process natural
    language sequences in many different ways simultaneously, in parallel.^([[61](#_footnotedef_61
    "View footnote.")]) ^([[62](#_footnotedef_62 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 2 you will learn more about pattern-based approaches to tokenizing — splitting
    text into tokens or words with algorithms such as the "Treebank tokenizer." You
    will also learn how to use pattern matching to stem (shorten and consolidate)
    tokens with something called a Porter stemmer. But in later chapters we take advantage
    of the exponentially greater computational resources, as well as our larger datasets,
    to shortcut this laborious hand programming and refining.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to regular expressions and want to learn more, you can check
    out Appendix B or the online documentation for Python regular expressions. But
    you do not have to understand them just yet. We’ll continue to provide you with
    sample regular expressions as we use them for the building blocks of our NLP pipeline.
    So, do not worry if they look like gibberish. Human brains are pretty good at
    generalizing from a set of examples, and I’m sure it will become clear by the
    end of this book. And it turns out machines can learn this way as well…​
  prefs: []
  type: TYPE_NORMAL
- en: 1.6.2 Another way
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine a giant database containing sessions of dialog between humans. You might
    have statements paired with responses from thousands or even millions of conversations.
    One way to build a chatbot would be to search such a database for the exact same
    string of characters the user just "said" to your chatbot. And then you could
    use one of the responses to that statement that other humans have said in the
    past. That would result in a statistical or data-driven approach to chatbot design.
    And that could take the place of all that tedious pattern-matching algorithm design.
  prefs: []
  type: TYPE_NORMAL
- en: Think about how a single typo or variation in the statement would trip up a
    pattern-matching bot or even a data-driven bot with millions of statements (utterances)
    in its database. Bit and character sequences are discrete and very precise. They
    either match or they do not. And people are creative. It may not seem like it
    sometimes, but very often people say something with new patterns of characters
    never seen before. So you’d like your bot to be able to measure the difference
    in *meaning* between character sequences. In later chapters, you’ll get better
    and better at extracting meaning from text!
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use character sequence matches to measure distance between natural
    language phrases, we’ll often get it wrong. Phrases with similar meanings, like
    "good" and "okay", can often have different character sequences and large distances
    when we count up character-by-character matches to measure distance. And sometimes
    two words look almost the same but mean completely different things: "bad" and
    "bag." You can count the number of characters that change from one word to another
    with algorithms such as Jaccard and Levenshtein algorithms. But these distance
    or "change" counts fail to capture the essence of the relationship between two
    dissimilar strings of characters such as "good" and "okay.".= And they fail to
    account for how small spelling differences might not really be typos but rather
    completely different words, such as "bad" and "bag".'
  prefs: []
  type: TYPE_NORMAL
- en: Distance metrics designed for numerical sequences and vectors are useful for
    a few NLP applications, like spelling correctors and recognizing proper nouns.
    So we use these distance metrics when they make sense. But for NLP applications
    where we are more interested in the meaning of the natural language than its spelling,
    there are better approaches. We use vector representations of natural language
    words and text and some distance metrics for those vectors for those NLP applications.
    We show you each approach, one by one, as we talk about these different applications
    and the kinds of vectors they are used with.
  prefs: []
  type: TYPE_NORMAL
- en: We do not stay in this confusing binary world of logic for long, but let’s imagine
    we’re famous World War II-era code-breaker Mavis Batey at Bletchley Park and we
    have just been handed that binary, Morse code message intercepted from communication
    between two German military officers. It could hold the key to winning the war.
    Where would we start? Well, the first layer of deciding would be to do something
    statistical with that stream of bits to see if we can find patterns. We can first
    use the Morse code table (or ASCII table, in our case) to assign letters to each
    group of bits. Then, if the characters are gibberish to us, as they are to a computer
    or a cryptographer in WWII, we could start counting them up, looking up the short
    sequences in a dictionary of all the words we have seen before and putting a mark
    next to the entry every time it occurs. We might also make a mark in some other
    log book to indicate which message the word occurred in, creating an encyclopedic
    index to all the documents we have read before. This collection of documents is
    called a *corpus*, and the words or sequences we have listed in our index are
    called a *lexicon*.
  prefs: []
  type: TYPE_NORMAL
- en: If we’re lucky, and we’re not at war, and the messages we’re looking at aren’t
    strongly encrypted, we’ll see patterns in those German word counts that mirror
    counts of English words used to communicate similar kinds of messages. Unlike
    a cryptographer trying to decipher German Morse code intercepts, we know that
    the symbols have consistent meaning and aren’t changed with every key click to
    try to confuse us. This tedious counting of characters and words is just the sort
    of thing a computer can do without thinking. And surprisingly, it’s nearly enough
    to make the machine appear to understand our language. It can even do math on
    these statistical vectors that coincides with our human understanding of those
    phrases and words. When we show you how to teach a machine our language using
    Word2Vec in later chapters, it may seem magical, but it’s not. It’s just math,
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: But let’s think for a moment about what information has been lost in our effort
    to count all the words in the messages we receive. We assign the words to bins
    and store them away as bit vectors like a coin or token sorter (see Figure 1.2)
    directing different kinds of tokens to one side or the other in a cascade of decisions
    that piles them in bins at the bottom. Our sorting machine must take into account
    hundreds of thousands if not millions of possible token "denominations," one for
    each possible word that a speaker or author might use. Each phrase or sentence
    or document we feed into our token sorting machine will come out the bottom, where
    we have a "vector" with a count of the tokens in each slot. Most of our counts
    are zero, even for large documents with verbose vocabulary. But we have not lost
    any words yet. What have we lost? Could you, as a human understand a document
    that we presented you in this way, as a count of each possible word in your language,
    without any sequence or order associated with those words? I doubt it. But if
    it was a short sentence or tweet, you’d probably be able to rearrange them into
    their intended order and meaning most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.5 Canadian coin sorter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![canadian coin sorter](images/canadian-coin-sorter.png)'
  prefs: []
  type: TYPE_IMG
- en: Here’s how our token sorter fits into an NLP pipeline right after a tokenizer
    (see Chapter 2). We have included a stopword filter as well as a "rare" word filter
    in our mechanical token sorter sketch. Strings flow in from the top, and bag-of-word
    vectors are created from the height profile of the token "stacks" at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 Token sorting tray
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![sketch token sorter](images/sketch-token-sorter.png)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that machines can handle this bag of words quite well and glean
    most of the information content of even moderately long documents this way. Each
    document, after token sorting and counting, can be represented as a vector, a
    sequence of integers for each word or token in that document. You see a crude
    example in Figure 1.3, and then Chapter 2 shows some more useful data structures
    for bag-of-word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: This is our first vector space model of a language. Those bins and the numbers
    they contain for each word are represented as long vectors containing a lot of
    zeros and a few ones or twos scattered around wherever the word for that bin occurred.
    All the different ways that words could be combined to create these vectors is
    called a *vector space*. And relationships between vectors in this space are what
    make up our model, which is attempting to predict combinations of these words
    occurring within a collection of various sequences of words (typically sentences
    or documents). In Python, we can represent these sparse (mostly empty) vectors
    (lists of numbers) as dictionaries. And a Python `Counter` is a special kind of
    dictionary that bins objects (including strings) and counts them just like we
    want.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can probably imagine some ways to clean those tokens up. We do just that
    in the next chapter. But you might also think to yourself that these sparse, high-dimensional
    vectors (many bins, one for each possible word) aren’t very useful for language
    processing. They are, however, good enough for some industry-changing tools like
    spam filters, which we discuss in Chapter 3.
  prefs: []
  type: TYPE_NORMAL
- en: And we can imagine feeding into this machine, one at a time, all the documents,
    statements, sentences, and even single words we could find. We’d count up the
    tokens in each slot at the bottom after each of these statements was processed,
    and we’d call that a vector representation of that statement. All the possible
    vectors a machine might create this way is called a *vector space*. And this model
    of documents and statements and words is called a *vector space model*. It allows
    us to use linear algebra to manipulate these vectors and compute things like distances
    and statistics about natural language statements, which helps us solve a much
    wider range of problems with less human programming and less brittleness in the
    NLP pipeline. One statistical question that is asked of bag-of-words vector sequences
    is, "What is the combination of words most likely to follow a particular bag of
    words?" Or, even better, if a user enters a sequence of words, "What is the closest
    bag of words in our database to a bag-of-words vector provided by the user?" This
    is a search query. The input words are the words you might type into a search
    box, and the closest bag-of-words vector corresponds to the document or web page
    you were looking for. The ability to efficiently answer these two questions would
    be sufficient to build a machine learning chatbot that could get better and better
    as we gave it more and more data.
  prefs: []
  type: TYPE_NORMAL
- en: But wait a minute, perhaps these vectors aren’t like any you’ve ever worked
    with before. They’re extremely high-dimensional. It’s possible to have millions
    of dimensions for a 3-gram vocabulary computed from a large corpus. In Chapter
    3, we discuss the curse of dimensionality and some other properties that make
    high-dimensional vectors difficult to work with.
  prefs: []
  type: TYPE_NORMAL
- en: 1.7 A brief overflight of hyperspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Chapter 3 you will learn how to consolidate words into a smaller number of
    vector dimensions to deal with the *curse of dimensionality*. You may even be
    able to turn the curse into a blessing by using all those dimensions to identify
    the subtle things that you want your NLU pipeline to understand. You project vectors
    onto each other to determine the distance between each pair. This gives you a
    reasonable estimate of the similarity in their *meaning* rather than merely their
    statistical word usage. When you compute a vector distance this way it is called
    a *cosine distance metric*. You will first use cosine distance in Chapter 3, and
    then uncover its true power when you are able to reduce the thousands of dimensions
    of topic vectors down to just a few in Chapter 4\. You can even project ("embed"
    is the more precise term) these vectors onto a 2D plane to have a "look" at them
    in plots and diagrams. This is one of the best ways to find patterns and clusters
    in high dimensional data. You can then teach a computer to recognize and act on
    these patterns in ways that reflect the underlying meaning of the words that produced
    those vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine all the possible tweets or messages or sentences that humans might
    write. Even though we do repeat ourselves a lot, that’s still a lot of possibilities.
    And when those tokens are each treated as separate, distinct dimensions, there
    is no concept that "Good morning, Hobs" has some shared meaning with "Guten Morgen,
    Hannes." We need to create some reduced dimension vector space model of messages
    so we can label them with a set of continuous (float) values. We could rate messages
    and words for qualities like subject matter and sentiment. We could ask questions
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: How likely is this message to be a question?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much is it about a person?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much is it about me?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How angry or happy does it sound?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it something I need to respond to?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of all the ratings we could give statements. We could put these ratings
    in order and "compute" them for each statement to compile a "vector" for each
    statement. The list of ratings or dimensions we could give a set of statements
    should be much smaller than the number of possible statements, and statements
    that mean the same thing should have similar values for all our questions.
  prefs: []
  type: TYPE_NORMAL
- en: These rating vectors become something that a machine can be programmed to react
    to. We can simplify and generalize vectors further by clumping (clustering) statements
    together, making them close on some dimensions and not on others.
  prefs: []
  type: TYPE_NORMAL
- en: But how can a computer assign values to each of these vector dimensions? Well,
    if we simplified our vector dimension questions to things like, "Does it contain
    the word 'good'? Does it contain the word 'morning'?" And so on. You can see that
    we might be able to come up with a million or so questions resulting in numerical
    value assignments that a computer could make to a phrase. This is the first practical
    vector space model, called a bit vector language model, or the sum of "one-hot
    encoded" vectors. You can see why computers are just now getting powerful enough
    to make sense of natural language. The millions of million-dimensional vectors
    that humans might generate simply "Does not compute!" on a supercomputer of the
    80s, but is no problem on a commodity laptop in the 21st century. More than just
    raw hardware power and capacity made NLP practical; incremental, constant-RAM,
    linear algebra algorithms were the final piece of the puzzle that allowed machines
    to crack the code of natural language.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an even simpler, but much larger representation that can be used in
    a chatbot. What if our vector dimensions completely described the exact sequence
    of characters? The vector for each character would contain the answer to binary
    (yes/no) questions about every letter and punctuation mark in your alphabet:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the first letter an ''A''?"'
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the first letter a ''B''?"'
  prefs: []
  type: TYPE_NORMAL
- en: …​
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the first letter a ''z''?"'
  prefs: []
  type: TYPE_NORMAL
- en: And the next vector would answer the same boring questions about the next letter
    in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the second letter an A?"'
  prefs: []
  type: TYPE_NORMAL
- en: '"Is the second letter a B?"'
  prefs: []
  type: TYPE_NORMAL
- en: …​
  prefs: []
  type: TYPE_NORMAL
- en: Despite all the "no" answers or zeroes in this vector sequence, it does have
    one advantage over all other possible representations of text - it retains every
    tiny detail, every bit of information contained in the original text, including
    the order of the characters and words. This is like the paper representation of
    a song for a player piano that only plays a single note at a time. The "notes"
    for this natural language mechanical player piano are the 26 uppercase and lowercase
    letters plus any punctuation that the piano must know how to "play." The paper
    roll wouldn’t have to be much wider than for a real player piano and the number
    of notes in some long piano songs doesn’t exceed the number of characters in a
    small document.
  prefs: []
  type: TYPE_NORMAL
- en: But this one-hot character sequence encoding representation is mainly useful
    for recording and then replaying an exact piece rather than composing something
    new or extracting the essence of a piece. We can’t easily compare the piano paper
    roll for one song to that of another. And this representation is longer than the
    original ASCII-encoded representation of the document. The number of possible
    document representations just exploded to retain information about each sequence
    of characters. We retained the order of characters and words but expanded the
    dimensionality of our NLP problem.
  prefs: []
  type: TYPE_NORMAL
- en: These representations of documents do not cluster together well in this character-based
    vector world. The Russian mathematician Vladimir Levenshtein came up with a brilliant
    approach for quickly finding similarities between vectors (strings of characters)
    in this world. Levenshtein’s algorithm made it possible to create some surprisingly
    fun and useful chatbots, with only this simplistic, mechanical view of language.
    But the real magic happened when we figured out how to compress/embed these higher
    dimensional spaces into a lower dimensional space of fuzzy meaning or topic vectors.
    We peek behind the magician’s curtain in Chapter 4, when we talk about latent
    semantic indexing and latent Dirichlet allocation, two techniques for creating
    much more dense and meaningful vector representations of statements and documents.
  prefs: []
  type: TYPE_NORMAL
- en: 1.8 Word order and grammar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The order of words matters. Those rules that govern word order in a sequence
    of words (like a sentence) are called the grammar of a language. That’s something
    that our bag of words or word vector discarded in the earlier examples. Fortunately,
    in most short phrases and even many complete sentences, this word vector approximation
    works OK. If you just want to encode the general sense and sentiment of a short
    sentence, word order is not terribly important. Take a look at all these orderings
    of our "Good morning Rosa" example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now if you tried to interpret each of those strings in isolation (without looking
    at the others), you’d probably conclude that they all probably had similar intent
    or meaning. You might even notice the capitalization of the word "Good" and place
    the word at the front of the phrase in your mind. But you might also think that
    "Good Rosa" was some sort of proper noun, like the name of a restaurant or flower
    shop. Nonetheless, a smart chatbot or clever woman of the 1940s in Bletchley Park
    would likely respond to any of these six permutations with the same innocuous
    greeting, "Good morning my dear General."
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try that (in our heads) on a much longer, more complex phrase, a logical
    statement where the order of the words matters a lot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The number of permutations exploded from `factorial(3) == 6` in our simple greeting
    to `factorial(12) == 479001600` in our longer statement! And it’s clear that the
    logic contained in the order of the words is important to any machine that would
    like to reply with the correct response. Even though common greetings are not
    usually garbled by bag-of-words processing, more complex statements can lose most
    of their meaning when thrown into a bag. A bag of words is not the best way to
    begin processing a database query, like the natural language query in the preceding
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Whether a statement is written in a formal programming language like SQL, or
    in an informal natural language like English, word order and grammar are important
    when a statement intends to convey logical relationships between things. That’s
    why computer languages depend on rigid grammar and syntax rule parsers. Fortunately,
    recent advances in natural language syntax tree parsers have made possible the
    extraction of syntactical and logical relationships from natural language with
    remarkable accuracy (greater than 90%).^([[63](#_footnotedef_63 "View footnote.")])
    In later chapters, we show you how to use packages like `SyntaxNet` (Parsey McParseface)
    and `SpaCy` to identify these relationships.
  prefs: []
  type: TYPE_NORMAL
- en: And just as in the Bletchley Park example greeting, even if a statement doesn’t
    rely on word order for logical interpretation, sometimes paying attention to that
    word order can reveal subtle hints of meaning that might facilitate deeper responses.
    These deeper layers of natural language processing are discussed in the next section.
    And Chapter 2 shows you a trick for incorporating some of the information conveyed
    by word order into our word-vector representation. It also shows you how to refine
    the crude tokenizer used in the previous examples (`str.split()`) to more accurately
    bin words into more appropriate slots within the word vector, so that strings
    like "good" and "Good" are assigned the same bin, and separate bins can be allocated
    for tokens like "rosa" and "Rosa" but not "Rosa!".
  prefs: []
  type: TYPE_NORMAL
- en: 1.9 A chatbot natural language pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The NLP pipeline required to build a dialog engine, or chatbot, is similar to
    the pipeline required to build a question answering system described in *Taming
    Text* (Manning, 2013).^([[64](#_footnotedef_64 "View footnote.")]) However, some
    of the algorithms listed within the five subsystem blocks may be new to you. We
    help you implement these in Python to accomplish various NLP tasks essential for
    most applications, including chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.7 Chatbot recirculating (recurrent) pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![chatbot pipeline](images/chatbot-pipeline.png)'
  prefs: []
  type: TYPE_IMG
- en: A chatbot requires four kinds of processing as well as a database to maintain
    a memory of past statements and responses. Each of the four processing stages
    can contain one or more processing algorithms working in parallel or in series
    (see figure 1.4).
  prefs: []
  type: TYPE_NORMAL
- en: '*Parse* — Extract features, structured numerical data, from natural language
    text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Analyze* — Generate and combine features by scoring text for sentiment, grammaticality,
    semantics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Generate* — Compose possible responses using templates, search, or language
    models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Execute* — Plan statements based on conversation history and objectives, and
    select the next response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these four stages can be implemented using one or more of the algorithms
    listed within the corresponding boxes in the block diagram. We show you how to
    use Python to accomplish near-state-of-the-art performance for each of these processing
    steps. And we show you several alternative approaches to implementing these five
    subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: Most chatbots will contain elements of all five of these subsystems (the four
    processing stages as well as the database). But many applications require only
    simple algorithms for many of these steps. Some chatbots are better at answering
    factual questions, and others are better at generating lengthy, complex, convincingly
    human responses. Each of these capabilities requires different approaches; we
    show you techniques for both.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, deep learning and data-driven programming (machine learning, or
    probabilistic language modeling) have rapidly diversified the possible applications
    for NLP and chatbots. This data-driven approach allows ever greater sophistication
    for an NLP pipeline by providing it with greater and greater amounts of data in
    the domain you want to apply it to. And when a new machine learning approach is
    discovered that makes even better use of this data, with more efficient model
    generalization or regularization, then large jumps in capability are possible.
  prefs: []
  type: TYPE_NORMAL
- en: The NLP pipeline for a chatbot shown in Figure 1.4 contains all the building
    blocks for most of the NLP applications that we described at the start of this
    chapter. As in *Taming Text*, we break out our pipeline into four main subsystems
    or stages. In addition, we have explicitly called out a database to record data
    required for each of these stages and persist their configuration and training
    sets over time. This can enable batch or online retraining of each of the stages
    as the chatbot interacts with the world. We have also shown a "feedback loop"
    on our generated text responses so that our responses can be processed using the
    same algorithms used to process the user statements. The response "scores" or
    features can then be combined in an objective function to evaluate and select
    the best possible response, depending on the chatbot’s plan or goals for the dialog.
    This book is focused on configuring this NLP pipeline for a chatbot, but you may
    also be able to see the analogy to the NLP problem of text retrieval or "search,"
    perhaps the most common NLP application. And our chatbot pipeline is certainly
    appropriate for the question-answering application that was the focus of *Taming
    Text*.
  prefs: []
  type: TYPE_NORMAL
- en: The application of this pipeline to financial forecasting or business analytics
    may not be so obvious. But imagine the features generated by the analysis portion
    of your pipeline. These features of your analysis or feature generation can be
    optimized for your particular finance or business prediction. That way they can
    help you incorporate natural language data into a machine learning pipeline for
    forecasting. Despite focusing on building a chatbot, this book gives you the tools
    you need for a broad range of NLP applications, from search to financial forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: One processing element in Figure 1.4 that is not typically employed in search,
    forecasting, or question-answering systems is natural language *generation*. For
    chatbots, this is their central feature. Nonetheless, the text generation step
    is often incorporated into a search engine NLP application and can give such an
    engine a large competitive advantage. The ability to consolidate or summarize
    search results is a winning feature for many popular search engines (DuckDuckGo,
    Bing, and Google). And you can imagine how valuable it is for a financial forecasting
    engine to be able to generate statements, tweets, or entire articles based on
    the business-actionable events it detects in natural language streams from social
    media networks and news feeds.
  prefs: []
  type: TYPE_NORMAL
- en: The next section shows how the layers of such a system can be combined to create
    greater sophistication and capability at each stage of the NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 1.10 Processing in depth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stages of a natural language processing pipeline can be thought of as layers,
    like the layers in a feed-forward neural network. Deep learning is all about creating
    more complex models and behavior by adding additional processing layers to the
    conventional two-layer machine learning model architecture of feature extraction
    followed by modeling. In Chapter 5 we explain how neural networks help spread
    the learning across layers by backpropagating model errors from the output layers
    back to the input layers. But here we talk about the top layers and what can be
    done by training each layer independently of the other layers.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.8 Example layers for an NLP pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![nlp layers](images/nlp-layers.png)'
  prefs: []
  type: TYPE_IMG
- en: The top four layers in Figure 1.8 correspond to the first two stages in the
    chatbot pipeline (feature extraction and feature analysis) in the previous section.
    For example, part-of-speech tagging (POS tagging), is one way to generate features
    within the Analyze stage of our chatbot pipeline. POS tags are generated automatically
    by the default `SpaCY` pipeline, which includes all the top four layers in this
    diagram. POS tagging is typically accomplished with a finite state transducer
    like the methods in the `nltk.tag` package.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom two layers (Entity Relationships and a Knowledge Base) are used to
    populate a database containing information (knowledge) about a particular domain.
    And the information extracted from a particular statement or document using all
    six of these layers can then be used in combination with that database to make
    inferences. Inferences are logical extrapolations from a set of conditions detected
    in the environment, like the logic contained in the statement of a chatbot user.
    This kind of "inference engine" in the deeper layers of this diagram is considered
    the domain of artificial intelligence, where machines can make inferences about
    their world and use those inferences to make logical decisions. However, chatbots
    can make reasonable decisions without this knowledge database, using only the
    algorithms of the upper few layers. And these decisions can combine to produce
    surprisingly human-like behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Over the next few chapters, we dive down through the top few layers of NLP.
    The top three layers are all that is required to perform meaningful sentiment
    analysis and semantic search and to build human-mimicking chatbots. In fact, it’s
    possible to build a useful and interesting chatbot using only a single layer of
    processing, using the text (character sequences) directly as the features for
    a language model. A chatbot that only does string matching and search is capable
    of participating in a reasonably convincing conversation if given enough example
    statements and responses.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the open source project `ChatterBot` simplifies this pipeline by
    merely computing the string "edit distance" (Levenshtein distance) between an
    input statement and the statements recorded in its database. If its database of
    statement-response pairs contains a matching statement, the corresponding reply
    (from a previously "learned" human or machine dialog) can be reused as the reply
    to the latest user statement. For this pipeline, all that is required is step
    3 (Generate) of our chatbot pipeline. And within this stage, only a brute-force
    search algorithm is required to find the best response. With this simple technique
    (no tokenization or feature generation required), `ChatterBot` can maintain a
    convincing conversion as the dialog engine for Salvius, a mechanical robot built
    from salvaged parts by Gunther Cox.^([[65](#_footnotedef_65 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: '`Will` is an open source Python chatbot framework by Steven Skoczen with a
    completely different approach.^([[66](#_footnotedef_66 "View footnote.")]) `Will`
    can only be trained to respond to statements by programming it with regular expressions.
    This is the labor-intensive and data-light approach to NLP. This grammar-based
    approach is especially effective for question-answering systems and task-execution
    assistant bots, like Lex, Siri, and Google Now. These kinds of systems overcome
    the "brittleness" of regular expressions by employing "fuzzy regular expressions."footnote:[The
    Python `regex` package is backward compatible with `re` and adds fuzziness among
    other features. The `regex` will replace the `re` package in future Python versions
    ([https://pypi.python.org/pypi/regex](pypi.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly `TRE agrep`, or "approximate grep," ([https://github.com/laurikari/tre](laurikari.html))
    is an alternative to the UNIX command-line application `grep.`] and other techniques
    for finding approximate grammar matches. Fuzzy regular expressions find the closest
    grammar matches among a list of possible grammar rules (regular expressions) instead
    of exact matches by ignoring some maximum number of insertion, deletion, and substitution
    errors. However, expanding the breadth and complexity of behaviors for pattern-matching
    chatbots requires a lot of difficult human development work. Even the most advanced
    grammar-based chatbots, built and maintained by some of the largest corporations
    on the planet (Google, Amazon, Apple, Microsoft), remain in the middle of the
    pack for depth and breadth of chatbot IQ.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of powerful things can be done with shallow NLP. And little, if any, human
    supervision (labeling or curating of text) is required. Often a machine can be
    left to learn perpetually from its environment (the stream of words it can pull
    from Twitter or some other source).^([[67](#_footnotedef_67 "View footnote.")])
    We show you how to do this in Chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: 1.11 Natural language IQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like human brainpower, the power of an NLP pipeline cannot be easily gauged
    with a single IQ score without considering multiple "smarts" dimensions. A common
    way to measure the capability of a robotic system is along the dimensions of behavior
    complexity and the degree of human supervision required. But for a natural language
    processing pipeline, the goal is to build systems that fully automate the processing
    of natural language, eliminating all human supervision (once the model is trained
    and deployed). So a better pair of IQ dimensions should capture the breadth and
    depth of the complexity of the natural language pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: A consumer product chatbot or virtual assistant like Alexa or Allo is usually
    designed to have extremely broad knowledge and capabilities. However, the logic
    used to respond to requests tends to be shallow, often consisting of a set of
    trigger phrases that all produce the same response with a single if-then decision
    branch. Alexa (and the underlying Lex engine) behave like a single layer, flat
    tree of (if, elif, elif, …​) statements.^([[68](#_footnotedef_68 "View footnote.")])
    Google Dialogflow (which was developed independently of Google’s Allo and Google
    Assistant) has similar capabilities to Amazon Lex, Contact Flow, and Lambda, but
    without the drag-and-drop user interface for designing your dialog tree.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the Google Translate pipeline (or any similar machine translation
    system) relies on a deep tree of feature extractors, decision trees, and knowledge
    graphs connecting bits of knowledge about the world. Sometimes these feature extractors,
    decision trees, and knowledge graphs are explicitly programmed into the system,
    as in Figure 1.5\. Another approach rapidly overtaking this "hand-coded" pipeline
    is the deep learning data-driven approach. Feature extractors for deep neural
    networks are learned rather than hard-coded, but they often require much more
    training data to achieve the same performance as intentionally designed algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: You will use both approaches (neural networks and hand-coded algorithms) as
    you incrementally build an NLP pipeline for a chatbot capable of conversing within
    a focused knowledge domain. This will give you the skills you need to accomplish
    the natural language processing tasks within your industry or business domain.
    Along the way you will probably get ideas about how to expand the breadth of things
    this NLP pipeline can do. Figure 1.6 puts the chatbot in its place among the natural
    language processing systems that are already out there. Imagine the chatbots you
    have interacted with. Where do you think they might fit in a plot like this? Have
    you attempted to gauge their intelligence by probing them with difficult questions
    or something like an IQ test? Try asking a chatbot something ambiguous that requires
    common sense logic and the ability to ask clarifying questions, such as "What’s
    larger, the sun or a nickel?"^([[69](#_footnotedef_69 "View footnote.")]) you
    will get a chance to do exactly that in later chapters, to help you decide how
    your chatbot stacks up against some of the others in this diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. 9\. 2D IQ of some natural language processing systems
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![nlp iq](images/nlp-iq.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you progress through this book, you will be building the elements of a chatbot.
    Chatbots require all the tools of NLP to work well:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction (usually to produce a vector space model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information extraction to be able to answer factual questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic search to learn from previously recorded natural language text or dialog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language generation to compose new, meaningful statements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning gives us a way to trick machines into behaving as if we had
    spent a lifetime programming them with hundreds of complex regular expressions
    or algorithms. We can teach a machine to respond to patterns similar to the patterns
    defined in regular expressions by merely providing it examples of user statements
    and the responses we want the chatbot to mimic. And the "models" of language,
    the FSMs, produced by machine learning, are much better. They are less picky about
    mispelings and typoz.
  prefs: []
  type: TYPE_NORMAL
- en: And machine learning NLP pipelines are easier to "program." We do not have to
    anticipate every possible use of symbols in our language. We just have to feed
    the training pipeline with examples of the phrases that match and with example
    phrases that do not match. As long as we label the example phrases during training
    so that the chatbot knows which is which, it will learn to discriminate between
    them. And there are even machine learning approaches that require little if any
    "labeled" data.
  prefs: []
  type: TYPE_NORMAL
- en: We have given you some exciting reasons to learn about natural language processing.
    You want to help save the world, do you not? And we have attempted to pique your
    interest with some practical NLP applications that are revolutionizing the way
    we communicate, learn, do business, and even think. It will not be long before
    you are able to build a system that approaches human-like conversational behavior.
    And you should be able to see in upcoming chapters how to train a chatbot or NLP
    pipeline with any domain knowledge that interests you — from finance and sports
    to psychology and literature. If you can find a corpus of writing about it, then
    you can train a machine to understand it.
  prefs: []
  type: TYPE_NORMAL
- en: This book is about using machine learning to build smart text-reading machines
    without you having to anticipate all the ways people can say things. Each chapter
    incrementally improves on the basic NLP pipeline for the chatbot introduced in
    this chapter. As you learn the tools of natural language processing, you will
    be building an NLP pipeline that can not only carry on a conversation but help
    you accomplish your goals in business and in life.
  prefs: []
  type: TYPE_NORMAL
- en: 1.12 Test yourself
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Chapter 1 review questions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some review questions for you to test your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is NLP considered to be a core enabling feature for AGI (human-like AI)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do advanced NLP models tend to show significant discriminatory biases?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is it possible to create a prosocial chatbot using training data from sources
    that include antisocial examples?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are 4 different approaches or architectures for building a chatbot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is NLP used within a search engine?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a regular expression to recognize your name and all the variations on
    its spelling (including nicknames) that you’ve seen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a regular expression to try to recognize a sentence boundary (usually
    a period ("."), question mark "?", or exclamation mark "!")
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Active learning, quizzing yourself with questions such as these, is a fast way
    to gain deep understanding of any new topic. It turns out, this same approach
    is effective for machine learning and model evaluation as well.footnote:[Suggested
    answers are provided within the Python packages `nlpia` ([https://gitlab.com/tangibleai/nlpia](tangibleai.html))
    and `qary` ([https://gitlab.com/tangibleai/qary](tangibleai.html)) where they
    are used to evaluate advanced NLP models for reading comprehension and question
    answering. Pooja Sethi will share active learning NLP insights on Substack ([https://activelearning.substack.com](.html))
    and github ([https://poojasethi.github.io](.html)) by the time this book goes
    to print. ProAI.org, the team of contributing authors for this book is doing the
    same on substack ([https://proai.substack.com](.html)) and their home page ([https://proai.org](.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 1.13 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Good NLP may help save the world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The meaning and intent of words can be deciphered by machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A smart NLP pipeline will be able to deal with ambiguity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can teach machines common sense knowledge without spending a lifetime training
    them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots can be thought of as semantic search engines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular expressions are useful for more than just search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) In 2013 The Guardian and other news organizations revealed
    Facebook’s experiments to maniuplate users'' emotions using NLP ( [https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds](29.html)).
    Search engine giants and their algorithms perform these same kinds of experiments
    each time you enter text into the search box ( [https://www.computerservicesolutions.in/all-google-search-algorithm-updates/](all-google-search-algorithm-updates.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) "Genshin Impact won’t let players write ''Tibet'', ''Hong
    Kong'', ''Taiwan'' because of Chinese censorship" by Paolo Sirio ( [https://archive.is/MxNZI](archive.is.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) "How to circumvent Facebook’s content censorship using
    archive" by Emil O. W. Kirkegaard ( [https://archive.is/CA71O](archive.is.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) "Censorship of Online Encyclopedias Implications for
    NLP Models" ( [https://www.researchgate.net/publication/348757384_Censorship_of_Online_Encyclopedias_Implications_for_NLP_Models](publication.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) Yuval Noah Harari’s seminal book *21 Lessons for the
    21st Century* - Wikipedia article ( [https://en.wikipedia.org/wiki/21_Lessons_for_the_21st_Century](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) Von Neumann Architecture on Wikipedia ( [https://en.wikipedia.org/wiki/Von_Neumann_architecture](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) "The secrets of computer power revealed" by Daniel Dennett
    ( [https://sites.tufts.edu/rodrego/](rodrego.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) Animal Language" on Wikipedia ( [https://en.wikipedia.org/wiki/Animal_language](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) Parsing Expression Grammar Notation home page ( [https://pegn.dev/](pegn.dev.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) ( [https://writings.stephenwolfram.com/2023/02/a-50-year-quest-my-personal-journey-with-the-second-law-of-thermodynamics/](a-50-year-quest-my-personal-journey-with-the-second-law-of-thermodynamics.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) The ConvoHub project at ( [https://qary.ai](.html))
    and on GitLab ( [https://gitlab.com/tangibleai/community/convohub](community.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) Wikipedia article on Enshittification and rent-seeking
    ( [https://en.wikipedia.org/wiki/Enshittification](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) At DefCon 31, Cory Doctorow explained how interoperable
    APIs will win out over walled gardens and rent-seeking on YouTube( [https://www.youtube.com/watch?v=rimtaSgGz_4](www.youtube.com.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) "When will AIs program programs that can program AIs"
    on Metaculus ( [https://www.metaculus.com/questions/406/when-will-ais-program-programs-that-can-program-ais/](when-will-ais-program-programs-that-can-program-ais.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) You may have heard of Microsoft’s and OpenAI’s Copilot
    project. GPT-J can do almost as well, and it’s completely open source and open
    data. ( [https://huggingface.co/models?sort=likes&search=gpt-j](huggingface.co.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) *Human Compatible: Artificial Intelligence and the
    Problem of Control* by Stuart Russell'
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) AI safety article on Wikipedia ( [https://en.wikipedia.org/wiki/AI_safety](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) See the web page titled, "How Google’s Site Crawlers
    Index Your Site - Google Search" ( [https://proai.org/google-search](proai.org.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) You can estimate the amount of actual natural language
    text out there to be at least a thousand times the size of Google’s index.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) [http://ethnologue.com](.html) maintains statistics
    about natural languages. ISO 639-3 lists 7,486 three-letter language codes ( [http://proai.org/language-codes](proai.org.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) *The Great Silence* by Ted Chiang ( [https://proai.org/great-silence](proai.org.html))
    describes an imagined dialog with an endangered species of parrot that concludes
    with the parrot saying to humanity, "Be Good. I love you."'
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) Dolphin Communication Project ( [https://proai.org/dolphin-communication](proai.org.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) Mathematical notation is ambiguous. See the "Mathematical
    notation" section of the Wikipedia article "Ambguity" ( [https://en.wikipedia.org/wiki/Ambiguity#Mathematical_notation](wiki.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) Thank you to "Tudor" on MEAP for setting me straight
    about this. ( [https://www.ted.com/talks/steven_pinker_what_our_language_habits_reveal/transcript](steven_pinker_what_our_language_habits_reveal.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) Thank you "Tudor" for improving this section and my
    thinking about linguistic relativism'
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) Thank you Leo Hepis!'
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) From the Wikipedia article on Neuro-linguistic-programming
    ( [https://en.wikipedia.org/wiki/Neuro-linguistic_programming](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) Open source Home Assistant pipeline on GitHub ( [https://github.com/home-assistant/](home-assistant.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) You can install MyCroft AI on any RaspberryPi with
    a speaker and a microphone ( [https://mycroft.ai/](mycroft.ai.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) Stanford’s Open Virtual Assistant Lab within their
    Human-centered AI Institute ( [https://hai.stanford.edu/news/open-source-challenger-popular-virtual-assistants](news.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) Some open source voice assistants you could contribute
    to ( [https://gitlab.com/tangibleai/team/-/tree/main/exercises/1-voice/](1-voice.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) "Identifying Sarcasm in Twitter: A Closer Look" by
    Roberto González-Ibáñez ( [https://aclanthology.org/P11-2102.pdf](aclanthology.org.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) Interpretable Multi-Head Self-Attention Architecture
    for Sarcasm Detection in Social Media by Ramya Akula et al., 2021 ( [https://www.mdpi.com/1099-4300/23/4/394/pdf](394.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) Some grammar rules can be implemented in a computer
    science abstraction called a finite state machine. Regular grammars can be implemented
    in regular expressions. There are two Python packages for running regular expression
    finite state machines, `re` which is built in, and `regex` which must be installed,
    but may soon replace `re`. Finite state machines are just trees of if…​then…​else
    statements for each token (character/word/n-gram) or action that a machine needs
    to react to or generate.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) "AP’s ''robot journalists'' are writing their own
    stories now", The Verge, Jan 29, 2015, [http://www.theverge.com/2015/1/29/7939067/ap-journalism-automation-robots-financial-reporting](7939067.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) Many chatbot frameworks, such as qary ( [http://gitlab.com/tangibleai.com/qary](tangibleai.com.html))
    allow importing of legacy FAQ lists to automatically compose a rule-based dialog
    engine for your chatbot.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[37]](#_footnoteref_37) "Are Sample-Efficient NLP Models More Robust?" by
    Nelson F. Liu, Ananya Kumar, Percy Liang, Robin Jia ( [https://arxiv.org/abs/2210.06456](abs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[38]](#_footnoteref_38) New York Times, Oct 18, 2016, [https://www.nytimes.com/2016/11/18/technology/automated-pro-trump-bots-overwhelmed-pro-clinton-messages-researchers-say.html](technology.html)
    and MIT Technology Review, Nov 2016, [https://www.technologyreview.com/s/602817/how-the-bot-y-politic-influenced-this-election/](how-the-bot-y-politic-influenced-this-election.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[39]](#_footnoteref_39) "A beginners guide to Mastodon" by Amanda Silberling
    at Tech Crunch ( [https://techcrunch.com/2022/11/08/what-is-mastodon/](what-is-mastodon.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[40]](#_footnoteref_40) Amanda Silberling, Senior Tech editor on Mastodon
    ( [https://journa.host/@amanda](journa.host.html) )'
  prefs: []
  type: TYPE_NORMAL
- en: '[[41]](#_footnoteref_41) 2021, E.Madhorubagan et al "Intelligent Interface
    for Fake Product Review Monitoring and Removal" ( [https://ijirt.org/master/publishedpaper/IJIRT151055_PAPER.pdf](publishedpaper.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[42]](#_footnoteref_42) Google Blog May 2018 about their *Duplex* system [https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html](05.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[43]](#_footnoteref_43) Wikipedia is probably your most objective reference
    on the "AI control problem" ( [https://en.wikipedia.org/wiki/AI_control_problem](wiki.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[44]](#_footnoteref_44) WSJ Blog, March 10, 2017 [https://blogs.wsj.com/cio/2017/03/10/homo-deus-author-yuval-noah-harari-says-authority-shifting-from-people-to-ai/](homo-deus-author-yuval-noah-harari-says-authority-shifting-from-people-to-ai.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[45]](#_footnoteref_45) John Michael Innes and Ben W. Morrison at the University
    of South Australia "Machines can do most of a psychologist’s job", 2021, ( [https://theconversation.com/machines-can-do-most-of-a-psychologists-job-the-industry-must-prepare-for-disruption-154064](theconversation.com.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[46]](#_footnoteref_46) "Humans Bonding with Virtual Companions" by C.S. Voll
    ( [https://archive.ph/6nSx2](archive.ph.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[47]](#_footnoteref_47) Tangible AI builds open source cognitive assistants
    that help users take control of their emotions such as Syndee and `qary` ( [https://gitlab.com/tangibleai/qary](tangibleai.html))
    Some of Replika.AI’s core technologies are open source ( [https://github.com/lukalabs](github.com.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[48]](#_footnoteref_48) Taken from animation on copilot.github.com that was
    unchanged from 2022 to March 2023 ( [https://copilot.github.com/](copilot.github.com.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[49]](#_footnoteref_49) One of Cialdini’s six psychology principles in his
    popular book *Influence* [http://changingminds.org/techniques/general/cialdini/click-whirr.htm](cialdini.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[50]](#_footnoteref_50) Steven Skoczen’s Will chatbot framework ( [https://github.com/skoczen/will](skoczen.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[51]](#_footnoteref_51) Tangible AI’s chatbot framework called `qary` ( [https://docs.qary.ai](.html))
    with examples deployed for WeSpeakNYC ( [https://wespeaknyc.cityofnewyork.us/](wespeaknyc.cityofnewyork.us.html))
    and others'
  prefs: []
  type: TYPE_NORMAL
- en: '[[52]](#_footnoteref_52) Stack Exchange Went Down for 30 minutes on July 20,
    2016 when a regex "crashed" ( [http://stackstatus.net/post/147710624694/outage-postmortem-july-20-2016](147710624694.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[53]](#_footnoteref_53) See the web page titled "Chomsky hierarchy - Wikipedia"
    ( [https://en.wikipedia.org/wiki/Chomsky_hierarchy](wiki.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[54]](#_footnoteref_54) "English is not a regular language" ( [http://cs.haifa.ac.il/~shuly/teaching/08/nlp/complexity.pdf#page=20](nlp.html))
    by Shuly Wintner'
  prefs: []
  type: TYPE_NORMAL
- en: '[[55]](#_footnoteref_55) "Is English context-free?" ( [http://cs.haifa.ac.il/~shuly/teaching/08/nlp/complexity.pdf#page=24](nlp.html))
    by Shuly Wintner'
  prefs: []
  type: TYPE_NORMAL
- en: '[[56]](#_footnoteref_56) See the web page titled "1.11\. Formal and Natural
    Languages — How to Think like a Computer Scientist: Interactive Edition" ( [https://runestone.academy/ns/books/published/fopp/GeneralIntro/FormalandNaturalLanguages.html](GeneralIntro.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[57]](#_footnoteref_57) The code here simplifies the behavior of the chatbot
    called "Joshua" in the "War Games" movie. See Wikiquote ( [https://en.wikiquote.org/wiki/WarGames](wiki.html))
    for more chatbot script ideas.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[58]](#_footnoteref_58) SpaCy ''Matcher'' ( [https://spacy.io/api/matcher](api.html))
    is a regular expression interpreter for patterns of words, parts of speech, and
    other symbol sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[59]](#_footnoteref_59) The idea for this defusing response originated with
    Viktor Frankl’s *Man’s Search for Meaning*, his Logotherapy ( [https://en.wikipedia.org/wiki/Logotherapy](wiki.html))
    approach to psychology and the many popular novels where a child protagonist like
    Owen Meany has the wisdom to respond to an insult with a response like this.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[60]](#_footnoteref_60) Stanford Encyclopedia of Philosophy, Computational
    Theory of Mind, [https://plato.stanford.edu/entries/computational-mind/](computational-mind.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[61]](#_footnoteref_61) Stanford Encyclopedia of Philosophy, Connectionism,
    [https://plato.stanford.edu/entries/connectionism/](connectionism.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[62]](#_footnoteref_62) "Toward a Connectionist Model of Recursion in Human
    Linguistic Performance" Christiansen and Chater, 1999, Southern Illinois University
    ( [https://archive.is/0Lx3o](archive.is.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[63]](#_footnoteref_63) A comparison of the syntax parsing accuracy of SpaCy
    (93%), SyntaxNet (94%), Stanford’s CoreNLP (90%), and others is available at [https://spacy.io/docs/api/](api.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[64]](#_footnoteref_64) Ingersol, Morton, and Farris, [http://www.manning.com/books/taming-text/?a_aid=totalgood](taming-text.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[65]](#_footnoteref_65) ChatterBot by Gunther Cox and others at [https://github.com/gunthercox/ChatterBot](gunthercox.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[66]](#_footnoteref_66) See the GitHub page for "Will," a chatbot for HipChat,
    by Steven Skoczen and the HipChat community ( [https://github.com/skoczen/will](skoczen.html)).
    In 2018 it was updated to integrate with Slack'
  prefs: []
  type: TYPE_NORMAL
- en: '[[67]](#_footnoteref_67) Simple neural networks are often used for unsupervised
    feature extraction from character and word sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[68]](#_footnoteref_68) More complicated logic and behaviors are now possible
    when you incorporate Lambdas into an AWS Contact Flow dialog tree. See "Creating
    Call Center Bot with AWS Connect" ( [https://greenice.net/creating-call-center-bot-aws-connect-amazon-lex-can-speak-understand](greenice.net.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[69]](#_footnoteref_69) "CommonSense QA: A Question Answering Challenge Targeting
    Commonsense Knowledge" by Alon Talmor et al ( [https://aclanthology.org/N19-1421.pdf](aclanthology.org.html)).
    You can help collect more questions and datasets like this in the nlpia2 GitLab
    repository ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/data/iq_test.csv](data.html)
    ).'
  prefs: []
  type: TYPE_NORMAL
