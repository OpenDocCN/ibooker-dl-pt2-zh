["```py\ndef build_model(max_seq_length):                                          ❶\n    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n    bert_inputs = [in_id, in_mask, in_segment]\n\n    bert_output = BertLayer(n_fine_tune_layers=0)(bert_inputs)            ❷\n    dense = tf.keras.layers.Dense(256, activation=\"relu\")(bert_output)\n    pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n\n    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.summary()\n\n    return model\n\ndef initialize_vars(sess):                                                ❸\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)\n\nbert_path = \"https:/ /tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\ntokenizer = create_tokenizer_from_hub_module(bert_path)                   ❹\n\ntrain_examples = convert_text_to_examples(train_x, train_y)               ❺\ntest_examples = convert_text_to_examples(test_x, test_y)\n# Convert to features\n(train_input_ids,train_input_masks,train_segment_ids,train_labels) =       ❻\n     convert_examples_to_features(tokenizer, train_examples,               ❻\n     max_seq_length=maxtokens)                                             ❻\n(test_input_ids,test_input_masks,test_segment_ids,test_labels) = \n     convert_examples_to_features(tokenizer, test_examples, \n     max_seq_length=maxtokens)\n\nmodel = build_model(maxtokens)                                             ❼\n\ninitialize_vars(sess)                                                      ❽\n\nhistory = model.fit([train_input_ids, train_input_masks, train_segment_ids], ❾\ntrain_labels,validation_data=([test_input_ids, test_input_masks, \ntest_segment_ids],test_labels), epochs=5, batch_size=32)\n```", "```py\nfrom transformers import pipeline\n\nqNa= pipeline('question-answering', model= 'bert-large-cased-whole-word-masking-finetuned-squad', tokenizer='bert-large-cased-whole-word-masking-finetuned-squad')          ❶\n\nparagraph = 'A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer deaths by the start of June. Containment policies had a large impact on the number of COVID-19 cases and deaths, directly by reducing transmission rates and indirectly by constraining people’s behaviour. They account for roughly half the observed change in the growth rates of cases and deaths.'\n```", "```py\nans = qNa({'question': 'What is this article about?','context': f'{paragraph}'})\nprint(ans)\n```", "```py\n{'score': 0.47023460869354494, 'start': 148, 'end': 168, 'answer': 'Containment policies'}\n```", "```py\nans = qNa({'question': 'Which country is this article about?',\n           'context': f'{paragraph}'})\nprint(ans)\n```", "```py\n{'score': 0.795254447990601, 'start': 34, 'end': 36, 'answer': 'US'}\n```", "```py\nans = qNa({'question': 'Which disease is discussed in this article?',\n           'context': f'{paragraph}'})\nprint(ans)\n```", "```py\n{'score': 0.9761025334558902, 'start': 205, 'end': 213, 'answer': 'COVID-19'}\n```", "```py\nans = qNa({'question': 'What time period is discussed in the article?',\n           'context': f'{paragraph}'})\nprint(ans)\n```", "```py\n{'score': 0.21781831588181433, 'start': 71, 'end': 79, 'answer': '1 April,'}\n```", "```py\nfrom transformers import pipeline\n\nfill_mask = pipeline(\"fill-mask\",model=\"bert-base-cased\",tokenizer=\"bert-base-cased\")\n```", "```py\nfill_mask(\"A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer [MASK] by the start of June\")\n```", "```py\n[{'sequence': '[CLS] A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer deaths by the start of June [SEP]',\n  'score': 0.19625532627105713,\n  'token': 6209},\n {'sequence': '[CLS] A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer executions by the start of June [SEP]',\n  'score': 0.11479416489601135,\n  'token': 26107},\n {'sequence': '[CLS] A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer victims by the start of June [SEP]',\n  'score': 0.0846652239561081,\n  'token': 5256},\n {'sequence': '[CLS] A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer masks by the start of June [SEP]',\n  'score': 0.0419488325715065,\n  'token': 17944},\n {'sequence': '[CLS] A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer arrests by the start of June [SEP]',\n  'score': 0.02742016687989235,\n  'token': 19189}] \n```", "```py\n!pip install transformers==3.0.1 # upgrade transformers for NSP\n```", "```py\nfrom transformers import BertTokenizer, BertForNextSentencePrediction   ❶\nimport torch\nfrom torch.nn.functional import softmax                                 ❷\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = BertForNextSentencePrediction.from_pretrained('bert-base-cased')\nmodel.eval()                                                            ❸\n```", "```py\nprompt = \"A new study estimates that if the US had universally mandated masks on 1 April, there could have been nearly 40% fewer deaths by the start of June.\"\nnext_sentence = \"Containment policies had a large impact on the number of COVID-19 cases and deaths, directly by reducing transmission rates and indirectly by constraining people’s behavior.\"\nencoding = tokenizer.encode(prompt, next_sentence, return_tensors='pt')\nlogits = model(encoding)[0]                                              ❶\nprobs = softmax(logits)                                                  ❷\nprint(\"Probabilities: [not plausible, plausible]\")\nprint(probs)\n```", "```py\nProbabilities: [not plausible, plausible]\ntensor([[0.1725, 0.8275]], grad_fn=<SoftmaxBackward>)\n```", "```py\nProbabilities: [not plausible, plausible]\ntensor([0.7666, 0.2334], grad_fn=<SoftmaxBackward>)\n```", "```py\nfrom transformers import BertTokenizerFast                                   ❶\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")❷\n```", "```py\nfrom transformers import BertForMaskedLM                                  ❶\n\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")   ❷\n\nprint(\"Number of parameters in mBERT model:\")\nprint(model.num_parameters())\n```", "```py\nfrom transformers import LineByLineTextDataset\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"../input/jw300entw/jw300.en-tw.tw\",\n    block_size=128)                                  ❶\n```", "```py\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True, mlm_probability=0.15)                  ❶\n```", "```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"twimbert\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_gpu_train_batch_size=16,\n    save_total_limit=1,\n)\n```", "```py\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n    prediction_loss_only=True)\n```", "```py\nimport time\nstart = time.time()\ntrainer.train()\nend = time.time()\nprint(\"Number of seconds for training:\")\nprint((end-start))\n```", "```py\ntrainer.save_model(\"twimbert\")\n```", "```py\nfrom transformers import pipeline\n\nfill_mask = pipeline(                                 ❶\n    \"fill-mask\",\n    model=\"twimbert\",\n    tokenizer=tokenizer)\n\nprint(fill_mask(\"Eyi de ɔhaw kɛse baa [MASK] hɔ.\"))   ❷\n```", "```py\n[{'sequence': '[CLS] Eyi de ɔhaw kɛse baa me hɔ. [SEP]', 'score': 0.13256989419460297, 'token': 10911}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa Israel hɔ. [SEP]', 'score': 0.06816119700670242, 'token': 12991}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa ne hɔ. [SEP]', 'score': 0.06106790155172348, 'token': 10554}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa Europa hɔ. [SEP]', 'score': 0.05116277188062668, 'token': 11313}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa Eden hɔ. [SEP]', 'score': 0.033920999616384506, 'token': 35409}]\n```", "```py\nfrom tokenizers import BertWordPieceTokenizer \n\npaths = ['../input/jw300entw/jw300.en-tw.tw']\n\ntokenizer = BertWordPieceTokenizer()                                 ❶\n\ntokenizer.train(                                                     ❷\n    paths,\n    vocab_size=10000,\n    min_frequency=2,\n    show_progress=True,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],   ❸\n    limit_alphabet=1000,\n    wordpieces_prefix=\"##\")\n\n!mkdir twibert                                                       ❹\n\ntokenizer.save(\"twibert\") \n```", "```py\nfrom transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"twibert\", max_len=512)    ❶\n```", "```py\nfrom transformers import BertForMaskedLM, BertConfig\nmodel = BertForMaskedLM(BertConfig())                  ❶\n```"]