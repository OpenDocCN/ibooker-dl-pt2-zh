- en: 11 Deploying and serving NLP applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 部署和提供 NLP 应用程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Choosing the right architecture for your NLP application
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合您的 NLP 应用程序的正确架构
- en: Version-controlling your code, data, and model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制您的代码、数据和模型
- en: Deploying and serving your NLP model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署和提供您的 NLP 模型
- en: Interpreting and analyzing model predictions with LIT (Language Interpretability
    Tool)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LIT（Language Interpretability Tool）解释和分析模型预测
- en: Where chapters 1 through 10 of this book are about building NLP models, this
    chapter covers everything that happens *outside* NLP models. Why is this important?
    Isn’t NLP all about building high-quality ML models? It may come as a surprise
    if you don’t have much experience with production NLP systems, but a large portion
    of an NLP system has very little to do with NLP at all. As shown in figure 11.1,
    only a tiny fraction of a typical real-world ML system is the ML code, but the
    “ML code” part is supported by numerous components that provide various functionalities,
    including data collection, feature extraction, and serving. Let’s use a nuclear
    power plant as an analogy. In operating a nuclear power plant, only a tiny fraction
    concerns nuclear reaction. Everything else is a vast and complex infrastructure
    that supports safe and efficient generation and transportation of materials and
    electricity—how to use the generated heat to turn the turbine to make electricity,
    how to cool and circulate water safely, how to transmit the electricity efficiently,
    and so on. All those supporting infrastructures have little to do with nuclear
    physics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第 1 至 10 章是关于构建 NLP 模型的，而本章涵盖的是*不在* NLP 模型之外发生的一切。为什么这很重要？难道 NLP 不都是关于构建高质量的
    ML 模型吗？如果您没有太多生产 NLP 系统的经验，这可能会让您感到惊讶，但典型现实世界的 ML 系统的很大一部分与 NLP 几乎没有关系。如图11.1所示，典型实际
    ML 系统的只有一小部分是 ML 代码，但“ML 代码”部分由提供各种功能的许多组件支持，包括数据收集、特征提取和服务。让我们用核电站作为类比。在操作核电站时，只有一小部分涉及核反应。其他一切都是支持安全有效地生成和传输材料和电力的庞大而复杂的基础设施——如何利用生成的热量转动涡轮发电，如何安全冷却和循环水，如何高效传输电力等等。所有这些支持基础设施与核物理几乎无关。
- en: '![CH11_F01_Hagiwara](../Images/CH11_F01_Hagiwara.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F01_Hagiwara](../Images/CH11_F01_Hagiwara.png)'
- en: Figure 11.1 A typical ML system consists of many different components, and the
    ML code is only a tiny fraction of it. We cover the highlighted components in
    this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 一个典型的 ML 系统由许多不同的组件组成，而 ML 代码只是其中的一小部分。我们在本章中介绍了突出显示的组件。
- en: Partly due to the “AI hype” in popular media, I personally think people pay
    too much attention to the ML modeling part and too little attention to how to
    serve the model in a useful way. After all, the goal of your product is to deliver
    values to the users, not provide them with the raw predictions of the model. Even
    if your model is 99% accurate, it’s not useful if you cannot make the most of
    the prediction so that users can benefit from them. Using the previous analogy,
    users want to power their appliances and light their houses with electricity and
    do not care much how exactly the electricity is generated in the first place.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 部分原因是由于大众媒体上的“人工智能炒作”，我个人认为人们过分关注 ML 建模部分，而对如何以有用的方式为模型提供服务关注不足。毕竟，您的产品的目标是向用户提供价值，而不是仅仅为他们提供模型的原始预测。即使您的模型准确率达到
    99%，如果您无法充分利用预测，使用户受益，那么它就没有用。用之前的类比来说，用户想要用电来驱动家用电器并照亮房屋，而不太在意电是如何生成的。
- en: In the rest of this chapter, we’ll discuss how to architect your NLP applications—we
    focus on some of the best practices when it comes to designing and developing
    NLP applications in a reliable and effective manner. Then we talk about deploying
    your NLP models—this is how we bring the NLP models to production and serve their
    predictions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将讨论如何构建您的 NLP 应用程序——我们侧重于在可靠和有效的方式设计和开发 NLP 应用程序时的一些最佳实践。然后我们谈论部署您的
    NLP 模型——这是我们如何将 NLP 模型投入生产并提供其预测的方法。
- en: 11.1 Architecting your NLP application
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 构建您的 NLP 应用程序架构
- en: '*Machine learning engineering* is still software engineering. All the best
    practices (decoupled software architectures, well-designed abstractions, clean
    and readable code, version control, continuous integration, etc.) apply to ML
    engineering as well. In this section, we’ll discuss some best practices specific
    to designing and building NLP/ML applications.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习工程*仍然是软件工程。所有最佳实践（解耦的软件架构、设计良好的抽象、清晰易读的代码、版本控制、持续集成等）同样适用于 ML 工程。在本节中，我们将讨论一些特定于设计和构建
    NLP/ML 应用程序的最佳实践。'
- en: 11.1.1 Before machine learning
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 机器学习之前
- en: I understand this is a book about NLP and ML, but you should seriously think
    about whether you need ML at all for your product before you start working on
    your NLP application. Building an ML system is no easy feat—it requires a lot
    of money and time to collect data, train models, and serve predictions. If you
    can solve your problem by writing some rules, by all means do so. As a rule of
    thumb, if a deep learning model can achieve an accuracy of 80%, a simpler, rule-based
    model can take you at least halfway there.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我明白这是一本关于 NLP 和 ML 的书，但在您开始着手处理您的 NLP 应用程序之前，您应该认真考虑您是否真的需要 ML 来解决您的产品问题。构建一个
    ML 系统并不容易——需要花费大量的时间和金钱来收集数据、训练模型和提供预测。如果您可以通过编写一些规则来解决问题，那就这样做吧。作为一个经验法则，如果深度学习模型可以达到
    80% 的准确率，那么一个更简单的基于规则的模型至少可以将您带到一半的路上。
- en: Also, you should consider using existing solutions, if any. Many open source
    NLP libraries (including AllenNLP and Transformers, the two libraries that we’ve
    been using extensively throughout the book) exist that come with a wide range
    of pretrained models. Cloud service providers (such as AWS AI services ([https://aws.amazon.com/machine-learning/ai-services/](https://aws.amazon.com/machine-learning/ai-services/)),
    Google Cloud AutoML ([https://cloud.google.com/automl](https://cloud.google.com/automl)),
    and Microsoft Azure Cognitive Services ([https://azure.microsoft.com/en-us/services/cognitive-services/](https://azure.microsoft.com/en-us/services/cognitive-services/)))
    offer a wide range of ML-related APIs for many domains, including NLP. If your
    task is something that can be solved using their offerings with no or little modification,
    that’d usually be a cost-efficient way to build your NLP application. After all,
    the most expensive component of any NLP application is usually highly skilled
    talent (i.e., your salary), and you should think twice before you go all-in and
    build in-house NLP solutions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果有现成的解决方案，您应该考虑使用。许多开源的 NLP 库（包括我们在整本书中广泛使用的 AllenNLP 和 Transformers 两个库）都提供了各种预训练模型。云服务提供商（如
    AWS AI 服务 ([https://aws.amazon.com/machine-learning/ai-services/](https://aws.amazon.com/machine-learning/ai-services/))、Google
    Cloud AutoML ([https://cloud.google.com/automl](https://cloud.google.com/automl))
    和 Microsoft Azure Cognitive Services ([https://azure.microsoft.com/en-us/services/cognitive-services/](https://azure.microsoft.com/en-us/services/cognitive-services/))）为许多领域提供了广泛的与
    ML 相关的 API，包括 NLP。如果您的任务可以通过它们提供的解决方案进行零或少量修改来解决，那通常是构建 NLP 应用的一种成本效益较高的方式。毕竟，任何
    NLP 应用程序中最昂贵的组件通常是高技能人才（即您的工资），在您全力投入并构建内部 NLP 解决方案之前，您应该三思而后行。
- en: In addition, you shouldn’t rule out “traditional” machine learning approaches.
    We’ve paid little attention to traditional ML models in this book, but you can
    find rich literature of statistical NLP models that were mainstream before the
    advent of deep NLP methods. Quickly building a prototype with statistical features
    (such as n-grams) and ML models (such as SVM) is often a great start. Non-deep
    algorithms such as *gradient-boosted decision trees* (GBDTs) often work almost
    as well as, if not better than, deep learning methods at a fraction of the cost.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您不应排除“传统”的机器学习方法。在本书中，我们很少关注传统的 ML 模型，但在深度 NLP 方法出现之前，您可以找到丰富的统计 NLP 模型的文献。使用统计特征（例如
    n-gram）和 ML 模型（例如 SVM）快速构建原型通常是一个很好的开始。非深度学习算法，例如 *梯度提升决策树*（GBDT），通常以比深度学习方法更低的成本几乎同样有效，如果不是更好。
- en: Finally, I always recommend that practitioners start by developing the validation
    set and choosing the right evaluation metric first, even before starting to choose
    the right ML approach. A validation set doesn’t need to be big, and most people
    can afford to sit down for a couple of hours and manually annotate a couple of
    hundred instances. Doing this offers many benefits—first, by solving the task
    manually, you get a feel of what’s important when it comes to solving the problem
    and whether it’s something that a machine can really solve automatically. Second,
    by putting yourself in the machine’s shoes, you gain a lot of insights into the
    task (what the data looks like, how the input and output data are distributed,
    and how they are related), which become valuable when it comes to actually designing
    an ML system to solve it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我始终建议从开发验证集和选择正确的评估指标开始，甚至在开始选择正确的ML方法之前。验证集不需要很大，大多数人都可以抽出几个小时手动注释几百个实例。这样做有很多好处——首先，通过手动解决任务，你可以感受到在解决问题时什么是重要的，以及是否真的可以自动解决。其次，通过把自己置于机器的角度，你可以获得许多关于任务的见解（数据是什么样子，输入和输出数据是如何分布的，它们是如何相关的），这在实际设计ML系统来解决它时变得有价值。
- en: 11.1.2 Choosing the right architecture
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 选择正确的架构
- en: Except for rare occasions where the output of an ML system is the end product
    itself (such as machine translation), NLP modules usually interact with a larger
    system that collectively provide some values to the end users. For example, a
    spam filter is usually implemented as a module or a microservice embedded in a
    larger application (email service). Voice assistant systems are usually large,
    complex combinations of many ML/NLP subcomponents, including voice recognition,
    sentence-intent classification, question answering, and speech generation, that
    interact with each other. Even machine translation models can be one tiny component
    in a larger complex system if you include data pipelines, the backend, and the
    translation interface that the end users interact with.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了极少数情况下，ML系统的输出本身就是最终产品（比如机器翻译）之外，NLP模块通常与一个更大的系统交互，共同为最终用户提供一些价值。例如，垃圾邮件过滤器通常被实现为嵌入在更大的应用程序（邮件服务）中的模块或微服务。语音助手系统通常是许多ML/NLP子组件的大型、复杂组合，包括语音识别、句子意图分类、问答和语音生成，它们相互交互。即使是机器翻译模型，如果包括数据管道、后端和最终用户交互的翻译界面，也可以是更大复杂系统中的一个小组件。
- en: An NLP application can take many forms. Surprisingly, many NLP components can
    be structured as a one-off task that takes some static data as its input and produces
    transformed data as its output. For example, if you have a static database of
    some documents and you’d like to classify them by their topics, your NLP classifier
    can be a simple one-off Python script that runs this classification task. If you’d
    like to extract common entities (e.g., company names) from the same database,
    you can write a Python script that runs a named entity recognition (NER) model
    to do it. Even a text-based recommender engine that finds objects based on textual
    similarity can be a daily task that reads from and writes data to the database.
    You don’t need to architect a complex software system with many services talking
    to each other.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: NLP应用可以采取多种形式。令人惊讶的是，许多NLP组件可以被构造为一次性任务，它以一些静态数据作为输入，产生转换后的数据作为输出。例如，如果你有一组文档的静态数据库，并且想要按其主题对它们进行分类，你的NLP分类器可以是一个简单的一次性Python脚本，运行这个分类任务。如果你想要从同一数据库中提取通用实体（例如公司名称），你可以编写一个Python脚本来运行一个命名实体识别（NER）模型来实现。甚至一个基于文本相似度找到对象的文本推荐引擎也可以是一个每日任务，它从数据库读取数据并写入数据。你不需要设计一个复杂的软件系统，其中有许多服务相互交流。
- en: Many other NLP components can be structured as a (micro)service that runs prediction
    in batches, which is the architecture that I recommend for many scenarios. For
    example, a spam filter doesn’t need to classify every single email as soon as
    they arrive—the system can queue a certain number of emails that arrive at the
    system and pass the batched emails to the classifier service. The NLP application
    usually communicates with the rest of the system via some intermediary (e.g.,
    a RESTful API or a queuing system). This configuration is great for applications
    that require some freshness for their prediction (after all, users do not want
    to wait for hours until their emails arrive to their inbox), but the requirement
    is not that strict.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他 NLP 组件可以被构造成批量运行预测的（微）服务，这是我推荐的许多场景的架构。例如，垃圾邮件过滤器并不需要在每封邮件到达时立即对其进行分类 -
    系统可以将到达系统的一定数量的邮件排队，并将批处理的邮件传递给分类器服务。NLP 应用程序通常通过某种中介（例如 RESTful API 或排队系统）与系统的其余部分进行通信。这种配置非常适合需要对其预测保持一定新鲜度的应用程序（毕竟，用户不希望等待几个小时直到他们的电子邮件到达收件箱），但要求并不那么严格。
- en: Finally, NLP components can also be designed so that they serve real-time prediction.
    This is necessary when, for example, an audience needs real-time subtitles for
    a speech. Another example is when the system wants to show ads based on the user’s
    real-time behavior. For these cases, the NLP service needs to receive a stream
    of input data (such as audio or user events) and produce another stream of data
    (such as transcribed text or ad-click probabilities). *Real-time streaming frameworks*
    such as Apache Flink ([https://flink.apache.org/](https://flink.apache.org/))
    are often used for processing such stream data. Also, if your application is based
    on a server-client architecture, as with typical mobile and web apps, and you
    want to show some real-time prediction to the users, you can choose to run ML/NLP
    models on the client side, such as the web browser or the smartphones. Client-side
    ML frameworks such as TensorFlow.js ([https://www.tensorflow.org/js](https://www.tensorflow.org/js)),
    Core ML ([https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml)),
    and ML Kit ([https://developers.google.com/ml-kit](https://developers.google.com/ml-kit))
    can be used for such purposes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，NLP 组件也可以设计成为提供实时预测的方式。例如，当观众需要演讲的实时字幕时，这是必要的。另一个例子是当系统想要根据用户的实时行为显示广告时。对于这些情况，NLP
    服务需要接收一系列输入数据（如音频或用户事件），并生成另一系列数据（如转录文本或广告点击概率）。诸如 Apache Flink ([https://flink.apache.org/](https://flink.apache.org/))
    这样的*实时流处理框架*经常用于处理此类流数据。另外，如果您的应用程序基于服务器-客户端架构，例如典型的移动和 Web 应用程序，并且您想向用户显示一些实时预测，您可以选择在客户端上运行
    ML/NLP 模型，例如 Web 浏览器或智能手机。诸如 TensorFlow.js ([https://www.tensorflow.org/js](https://www.tensorflow.org/js))、Core
    ML ([https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml))
    和 ML Kit ([https://developers.google.com/ml-kit](https://developers.google.com/ml-kit))
    这样的客户端 ML 框架可用于此类目的。
- en: 11.1.3 Project structure
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.3 项目结构
- en: Many NLP applications follow somewhat similar project structures. A typical
    NLP project may need to manage datasets to train a model from, intermediate files
    generated by preprocessing data, model files produced as a result of training,
    source code for training and inference, and log files that store additional information
    about the training and inference.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 NLP 应用程序遵循着类似的项目结构。一个典型的 NLP 项目可能需要管理数据集以从中训练模型，预处理数据生成的中间文件，由训练产生的模型文件，用于训练和推断的源代码，以及存储有关训练和推断的其他信息的日志文件。
- en: 'Because typical NLP applications have many components and directories in common,
    it’d be useful if you simply follow best practices as your default choice when
    starting a new project. Here are my recommendations for structuring your NLP projects:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为典型的 NLP 应用程序有许多共同的组件和目录，所以如果您在启动新项目时只是遵循最佳实践作为默认选择，那将是有用的。以下是我为组织您的 NLP 项目提出的建议：
- en: '*Data management*—Make a directory called data and put all the data in it.
    It may also be helpful to subdivide this into raw, interim, and result directories.
    The raw directory contains the unprocessed dataset files you obtained externally
    (such as the Stanford Sentiment Treebank we’ve been using throughout this book)
    or built internally. It is very critical that *you do not modify any files in
    this raw directory by hand*. If you need to make changes, write a script that
    runs some processing against the raw files and then writes the result to the interim
    directory, which serves as a place for intermediate results. Or make a patch file
    that manages the “diff” you made to the raw file, and version-control the patch
    files instead. The final results such as predictions and metrics should be stored
    in the result directory.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据管理*—创建一个名为 data 的目录，并将所有数据放入其中。将其进一步细分为原始、中间和结果目录可能也会有所帮助。原始目录包含您外部获取的未经处理的数据集文件（例如我们在本书中一直在使用的斯坦福情感树库）或内部构建的文件。非常重要的一点是*不要手动修改此原始目录中的任何文件*。如果需要进行更改，请编写一个运行一些处理以针对原始文件运行的脚本，然后将结果写入中间目录的脚本，该目录用作中间结果的存储位置。或者创建一个管理您对原始文件进行的“差异”的补丁文件，并将补丁文件进行版本控制。最终的结果，例如预测和指标，应存储在结果目录中。'
- en: '*Virtual environment*—It is strongly recommended that you work in a virtual
    environment so that your dependencies are separated and reproducible. You can
    use tools like Conda ([https://docs.conda.io/en/latest/](https://docs.conda.io/en/latest/))
    (my recommendation) and venv ([https://docs.python.org/3/library/venv.html](https://docs.python.org/3/library/venv.html))
    to set up a separate environment for your project and use pip to install individual
    packages. Conda can export the environment configuration into an environment.yml
    file, which you can use to recover the exact Conda environment. You can also keep
    track of pip packages for the project in a requirements.txt file. Even better,
    you can use Docker containers to manage and package the entire ML environment.
    This greatly reduces dependency-related issues and simplifies deployment and serving.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*虚拟环境*—强烈建议您在虚拟环境中工作，以便您的依赖项分开且可重现。您可以使用诸如 Conda ([https://docs.conda.io/en/latest/](https://docs.conda.io/en/latest/))（我推荐的）和
    venv ([https://docs.python.org/3/library/venv.html](https://docs.python.org/3/library/venv.html))
    等工具为您的项目设置一个单独的环境，并使用 pip 安装单个软件包。Conda 可以将环境配置导出到一个 environment.yml 文件中，您可以使用该文件来恢复确切的
    Conda 环境。您还可以将项目的 pip 包跟踪在一个 requirements.txt 文件中。更好的是，您可以使用 Docker 容器来管理和打包整个
    ML 环境。这极大地减少了与依赖项相关的问题，并简化了部署和服务化。'
- en: '*Experiment management*—Training and inference pipelines for an NLP application
    usually consist of several steps, such as preprocessing and joining the data,
    converting them into features, training and running the model, and converting
    the results back to a human-readable format. These steps can easily get out of
    hand if you try to remember to manage them manually. A good practice is to keep
    track of the steps for the pipeline in a shell script file so that the experiments
    are reproducible with a single command, or use dependency management software
    such as GNU Make, Luigi ([https://github.com/spotify/luigi](https://github.com/spotify/luigi)),
    and Apache Airflow ([https://airflow.apache.org/](https://airflow.apache.org/)).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实验管理*—NLP 应用程序的训练和推理管道通常包括多个步骤，例如预处理和连接数据，将其转换为特征，训练和运行模型，以及将结果转换回人类可读格式。如果试图手动记住管理这些步骤，很容易失控。一个好的做法是在一个
    shell 脚本文件中跟踪管道的步骤，以便只需一个命令即可重现实验，或者使用依赖管理软件，如 GNU Make、Luigi ([https://github.com/spotify/luigi](https://github.com/spotify/luigi))
    和 Apache Airflow ([https://airflow.apache.org/](https://airflow.apache.org/))。'
- en: '*Source code*—Python source code is usually put in a directory of the same
    name as the project, which is further subdivided into directories such as data
    (for data-processing code), model (for model code), and scripts (for putting scripts
    for training and other one-off tasks).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*源代码*—Python 源代码通常放在与项目同名的目录中，该目录进一步细分为诸如 data（用于数据处理代码）、model（用于模型代码）和 scripts（用于放置用于训练和其他一次性任务的脚本）等目录。'
- en: 11.1.4 Version control
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.4 版本控制
- en: You probably don’t need to be convinced that version-controlling your source
    code is important. Tools like Git help you keep track of the changes and manage
    different versions of the source code. Development of NLP/ML applications is usually
    an iterative process where you (often with other people) make many changes to
    the source code and experiment with many different models. You can easily end
    up with a number of slightly different versions of the same code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能不需要说服您版本控制您的源代码很重要。像Git这样的工具帮助您跟踪变更并管理源代码的不同版本。NLP/ML应用程序的开发通常是一个迭代过程，在此过程中，您（通常与其他人）对源代码进行许多更改，并尝试许多不同的模型。您很容易最终拥有一些略有不同版本的相同代码。
- en: In addition to version-controlling your source code, it is also important to
    *version-control your data and models*. This means that you should version-control
    your training data, source code, and models separately, as shown in the dotted-line
    boxes in figure 11.2\. This is one of the major differences between regular software
    projects and ML applications. Machine learning is about improving computer algorithms
    through data. By definition, the behavior of any ML system depends on data it
    is fed. This could lead to a situation where the behavior of the system is different
    even if you use the same code.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对源代码进行版本控制外，对*数据和模型进行版本控制*也很重要。这意味着您应该分别对训练数据、源代码和模型进行版本控制，如图11.2中虚线框所示。这是常规软件项目和机器学习应用之间的主要区别之一。机器学习是通过数据改进计算机算法的过程。根据定义，任何机器学习系统的行为都取决于其所接收的数据。这可能会导致即使您使用相同的代码，系统的行为也会有所不同的情况。
- en: Tools like Git Large File Storage ([https://git-lfs.github.com/](https://git-lfs.github.com/))
    and DVC ([https://dvc.org](https://dvc.org)) can version-control your data and
    models in a seamless way. Even if you are not using these tools, you should at
    least manage different versions as separate files that are named clearly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如Git Large File Storage ([https://git-lfs.github.com/](https://git-lfs.github.com/))和DVC
    ([https://dvc.org](https://dvc.org))可以以无缝的方式对数据和模型进行版本控制。即使您不使用这些工具，您也应该至少将不同版本作为清晰命名的单独文件进行管理。
- en: '![CH11_F02_Hagiwara](../Images/CH11_F02_Hagiwara.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F02_Hagiwara](../Images/CH11_F02_Hagiwara.png)'
- en: 'Figure 11.2 Machine learning components to version-control: training data,
    source code, and models'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 机器学习组件的版本控制：训练数据、源代码和模型
- en: In a larger and more complex ML project, you may want to version-control your
    model and your feature pipeline separately, because the behavior of an ML model
    can be different depending on how you preprocess the input, even with the same
    model and input data. This will also mitigate the train-serve skew problem we’ll
    discuss later in section 11.3.2.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个更大更复杂的机器学习项目中，您可能希望将模型和特征管道的版本控制分开，因为机器学习模型的行为可能会因为您对输入进行预处理的方式不同而不同，即使是相同的模型和输入数据。这也将减轻我们稍后将在11.3.2节讨论的训练服务偏差问题。
- en: 'Finally, when working on ML applications, you will experiment with a lot of
    different settings—different combinations of training datasets, feature pipelines,
    models, and hyperparameters—which can easily get out of control. I recommend keeping
    track of the training settings using some experiment management system, such as
    Weights & Biases ([https://wandb.ai/](https://wandb.ai/)), but you can also use
    something as simple as a spreadsheet in which you enter experiment information
    manually. When keeping track of experiments, be sure to record the following information
    for each experiment:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当您在机器学习应用上工作时，您将尝试许多不同的设置——不同的训练数据集、特征管道、模型和超参数的组合——这可能会很难控制。我建议您使用一些实验管理系统来跟踪训练设置，例如Weights
    & Biases ([https://wandb.ai/](https://wandb.ai/))，但您也可以使用像手动输入实验信息的电子表格这样简单的东西。在跟踪实验时，请务必记录每个实验的以下信息：
- en: Versions of the model code, feature pipeline, and the training data used
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的模型代码版本、特征管道和训练数据的版本
- en: Hyperparameters used to train the model
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练模型的超参数
- en: Evaluation metrics for the training and the validation data
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的评估指标
- en: Platforms like AllenNLP support experiment configuration by default, which makes
    the first two items easy. Tools like TensorBoard, which is supported by AllenNLP
    and Hugging Face out of the box, make it trivial to keep track of various metrics.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 像AllenNLP这样的平台默认支持实验配置，这使得前两项变得容易。工具如TensorBoard，它们默认由AllenNLP和Hugging Face支持，使得跟踪各种指标变得轻而易举。
- en: 11.2 Deploying your NLP model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 部署您的NLP模型
- en: In this section, we’ll move on to the deployment stage, where your NLP application
    is put on a server and becomes available for use. We’ll discuss practical considerations
    when deploying NLP/ML applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进入部署阶段，将您的 NLP 应用程序放在服务器上，并可供使用。我们将讨论部署 NLP/ML 应用程序时的实际考虑因素。
- en: 11.2.1 Testing
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 测试
- en: As with software engineering, testing is an important part of building reliable
    NLP/ML applications. The most fundamental and important tests are unit tests,
    which automatically check whether small units of software (such as methods and
    classes) are working as expected. In NLP/ML applications, it is important to unit-test
    your feature pipeline. For example, if you write a method that converts raw text
    into a tensor representation, make sure that it works for typical and corner cases
    with unit tests. In my experience, this is where bugs often sneak in. Reading
    a dataset, building a vocabulary from a corpus, tokenizing, converting tokens
    into integer IDs—these are all essential yet error-prone steps in preprocessing.
    Fortunately, frameworks such as AllenNLP offer standardized, well-tested components
    for these steps, which makes building NLP applications easier and bug-free.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与软件工程一样，测试是构建可靠的 NLP/ML 应用程序的重要组成部分。最基本和重要的测试是单元测试，它们自动检查软件的小单元（如方法和类）是否按预期工作。在
    NLP/ML 应用程序中，对功能管道进行单元测试非常重要。例如，如果你编写了一个将原始文本转换为张量表示的方法，请确保它在典型和边界情况下都能正常工作。根据我的经验，这往往是错误
    sneak in 的地方。从数据集读取、从语料库构建词汇表、标记化、将标记转换为整数 ID —— 这些都是预处理中必不可少但容易出错的步骤。幸运的是，诸如
    AllenNLP 等框架为这些步骤提供了标准化、经过充分测试的组件，这使得构建 NLP 应用程序更加容易和无 bug。
- en: In addition to unit tests, you need to make sure that your model learns what
    it’s told to learn. This corresponds to testing logic errors in regular software
    engineering—types of errors where the software runs without crashing yet produces
    incorrect results. This type of error is more difficult to catch and fix in NLP/ML,
    because you need more insight into how the learning algorithm works mathematically.
    Moreover, many ML algorithms involve some randomness, such as random initialization
    and sampling, which makes testing even more difficult.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了单元测试之外，你还需要确保你的模型学到了它应该学到的东西。这对应于测试常规软件工程中的逻辑错误 —— 即软件运行时没有崩溃但产生了不正确的结果的错误类型。这种类型的错误在
    NLP/ML 中更难捕捉和修复，因为你需要更多的了解学习算法在数学上是如何工作的。此外，许多 ML 算法涉及一些随机性，如随机初始化和抽样，这使得测试变得更加困难。
- en: 'One recommended technique for testing NLP/ML models is sanity checks against
    the model output. You can start with a small and simple model and just a few toy
    instances with obvious labels. If you are testing a sentiment analysis model,
    for example, this goes as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个推荐的测试 NLP/ML 模型的技术是对模型输出进行 sanity checks。你可以从一个小而简单的模型开始，只使用几个带有明显标签的玩具实例。例如，如果你正在测试情感分析模型，可以按照以下步骤进行：
- en: Create a small and simple model for debugging, such as a toy encoder that simply
    averages the input word embeddings with a softmax layer on top.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为调试创建一个小而简单的模型，比如一个简单的玩具编码器，它只是将输入的单词嵌入平均化，并在顶部使用一个 softmax 层。
- en: Prepare a few toy instances, such as “The best movie ever!” (positive) and “This
    is an awful movie!” (negative).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备一些玩具实例，比如“最棒的电影！”（积极）和“这是一部糟糕的电影！”（消极）。
- en: Feed these instances to the model, and train it until convergence. Because we
    are using a very small dataset without a validation set, the model will heavily
    overfit to the instances, and that’s totally fine. Check whether the training
    loss goes down as expected.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些实例提供给模型，并训练直到收敛。由于我们使用的是一个非常小的数据集，没有验证集，所以模型会严重过拟合到这些实例上，这完全可以接受。检查训练损失是否如预期下降。
- en: Feed the same instances to the trained model, and check whether the predicted
    labels match the expected ones.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相同的实例提供给训练好的模型，检查预测的标签是否与预期的标签匹配。
- en: Try the steps above with more toy instances and a larger model.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多玩具实例和更大的模型尝试上述步骤。
- en: As a related technique, I always recommend you start with a smaller dataset,
    especially if the original dataset is large. Because training NLP/ML models takes
    a long time (hours or even days), you often find that your code has some errors
    only after your training is finished. You can subsample your training data, for
    example, by simply taking one out of every 10 instances, so that your entire training
    finishes quickly. Once you are sure that your model works as expected, you can
    gradually ramp up the amount of data you use for training. This technique is also
    great for quickly iterating and experimenting with many different architectures
    and hyperparameter settings. When you have just started building your model, you
    don’t usually have clear understanding of the best models for your task. With
    a smaller dataset, you can quickly validate a large number of different options
    (RNN versus Transformers, different tokenizers, etc.) and narrow down the set
    of candidate models that work best. One caveat to this approach is that the best
    model architectures and hyperparameters may depend on the size of the training
    data. Because of this, don’t forget to run the validation against the full dataset,
    too.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种相关技术，我总是建议您从较小的数据集开始，特别是如果原始数据集很大。因为训练自然语言处理/机器学习模型需要很长时间（几小时甚至几天），您经常会发现只有在训练完成后才能发现代码中的一些错误。您可以对训练数据进行子采样，例如，只需取出每10个实例中的一个，以便整个训练过程能够迅速完成。一旦您确信您的模型按预期工作，您可以逐渐增加用于训练的数据量。这种技术也非常适合快速迭代和尝试许多不同的架构和超参数设置。当您刚开始构建模型时，您通常不清楚最适合您任务的最佳模型。有了较小的数据集，您可以快速验证许多不同的选项（RNN与Transformers，不同的分词器等），并缩小最适合的候选模型集。这种方法的一个警告是，最佳模型架构和超参数可能取决于训练数据的大小。因此，请不要忘记针对完整数据集运行验证。
- en: Finally, you can use integration tests to verify whether the individual components
    of your application work in combination. For NLP, this usually means running the
    whole pipeline to see if the prediction is correct. Similar to the unit tests,
    you can prepare a small number of instances where the expected prediction is clear
    and run them against the trained model. Note that these instances are not for
    measuring how good the model is, but rather to serve as a sanity check whether
    your model can produce correct predictions for “obvious” cases. It is a good practice
    to run integration tests every time a new model or code is deployed. This is usually
    part of *continuous integration* (CI) used for regular software engineering.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用集成测试来验证应用程序的各个组件是否结合正常工作。对于自然语言处理（NLP），这通常意味着运行整个流程，以查看预测是否正确。与单元测试类似，您可以准备一小部分实例，其中期望的预测是明确的，并将它们运行到经过训练的模型上。请注意，这些实例不是用于衡量模型的好坏，而是作为一个合理性检查，以确定您的模型是否能够为“显而易见”的情况产生正确的预测。每次部署新模型或代码时运行集成测试是一个好习惯。这通常是用于常规软件工程的*持续集成*（CI）的一部分。
- en: 11.2.2 Train-serve skew
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 训练-服务偏差
- en: One common source of errors in ML applications is called *train-serve skew*,
    a situation where there’s a discrepancy between how instances are processed at
    the training and the inference times. This could occur in various situations,
    but let’s discuss a concrete example. Say you are building a sentiment-analyzer
    system with AllenNLP and would like to convert texts into instances. You usually
    start writing a data loader first, which reads the dataset and produces instances.
    Then you write a Python script or a config file that tells AllenNLP how the model
    should be trained. You train and validate your model. So far, so good. However,
    when it comes to using the model for prediction, things look slightly different.
    You need to write a predictor, which, given an input text, converts it into an
    instance and passes it to the model’s forward method. Notice that now you have
    two independent pipelines that preprocess the input—one for the training in the
    dataset reader, and another for the inference in the predictor.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习应用中常见的错误来源之一被称为*训练-服务偏差*，即在训练和推理时实例处理方式存在差异的情况。这可能发生在各种情况下，但让我们讨论一个具体的例子。假设您正在使用AllenNLP构建一个情感分析系统，并希望将文本转换为实例。您通常首先编写一个数据加载器，它读取数据集并生成实例。然后您编写一个Python脚本或配置文件，告诉AllenNLP模型应该如何训练。您对模型进行训练和验证。到目前为止，一切顺利。然而，当使用模型进行预测时，情况略有不同。您需要编写一个预测器，它会将输入文本转换为实例，并将其传递给模型的前向方法。请注意，现在您有两个独立的流程来预处理输入——一个用于数据集读取器中的训练，另一个用于预测器中的推理。
- en: What happens if you want to modify the way the input text is processed? For
    example, let’s say you find something you want to improve in your tokenization
    process, and you make changes to how input text is tokenized in your data loader.
    You update your data loader code, retrain the model, and deploy the model. However,
    you forgot to update the corresponding tokenization code in your predictor, effectively
    creating a discrepancy in how input is tokenized between training and serving.
    This is illustrated in figure 11.3.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![CH11_F03_Hagiwara](../Images/CH11_F03_Hagiwara.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 Train-serve skew is caused by discrepancies in how input is processed
    between training and serving.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The best way to fix this—or even better, to prevent this from happening in the
    first place—is to share as much of the feature pipeline as possible between the
    training and the serving infrastructure. A common practice in AllenNLP is to implement
    a method called _text_to_instance() in the dataset reader, which takes an input
    and returns an instance. By making sure that both the dataset reader and the predictor
    refer to the same method, you can minimize the discrepancy between the pipelines.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: In NLP, the fact that input text is tokenized and converted to numerical values
    makes debugging your model even more difficult. For example, an obvious bug in
    tokenization that you can spot easily with your naked eyes can be quite difficult
    to identify if everything is numerical values. A good practice is to log some
    intermediate results into a log file that you can inspect later.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that some behaviors of neural networks do differ between training
    and serving. One notable example is dropout, a regularization method we briefly
    covered in section 10.3.1\. To recap, dropout regularizes the model by randomly
    masking activation values in a neural network. This makes sense in training, because
    by removing activations, the model learns to make robust predictions based on
    available values. However, remember to turn it off at the serving time, because
    you don’t want your model to randomly drop neurons. PyTorch models implement methods—train()
    and eval()—that switch between the training and prediction modes, affecting how
    layers like dropout behave. If you are implementing a training loop manually,
    remember to call model.eval()to disable dropout. The good news is that frameworks
    such as AllenNLP can handle this automatically as long as you are using their
    default trainer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 Monitoring
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with other software services, deployed ML systems should be monitored continuously.
    In addition to the usual server metrics (e.g., CPU and memory usage), you should
    also monitor metrics related to the input and the output of the model. Specifically,
    you can monitor some higher-level statistics such as the distribution of input
    values and output labels. As mentioned earlier, logic errors, which are a type
    of error that causes the model to produce wrong results without crashing it, are
    the most common and hardest to find in ML systems. Monitoring those high-level
    statistics makes it easier to find them. Libraries and platforms like PyTorch
    Serve and Amazon SageMaker (discussed in section 11.3) support monitoring by default.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.4 Using GPUs
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training large modern ML models almost always requires hardware accelerators
    such as GPUs. Recall that back in chapter 2, we used overseas factories as an
    analogy for GPUs, which are designed to execute a huge number of arithmetic operations
    such as vector and matrix addition and multiplications in parallel. In this subsection,
    we’ll cover how to use GPUs to accelerate the training and prediction of ML models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t own GPUs or have never used cloud-based GPU solutions before, the
    easiest way to “try” GPUs for free is to use Google Colab. Go to its URL ([https://colab.research.google.com/](https://colab.research.google.com/)),
    create a new notebook, go to the Runtime menu, and choose “Change runtime type.”
    This will bring up the dialog box shown in figure 11.4.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![CH11_F04_Hagiwara](../Images/CH11_F04_Hagiwara.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Google Colab allows you to choose the type of hardware accelerator.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose GPU as the type of the hardware accelerator, and type !nvidia-smi in
    a code block and execute it. Some detailed information about your GPU is displayed,
    as shown next:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The nvidia-smi command (short for Nvidia System Management Interface) is a handy
    tool for checking information about Nvidia GPUs on the machine. From the previous
    snippet, you can see the version of the driver and CUDA (an API and a library
    for interacting with GPUs), type of GPUs (Tesla T4), available and used memory
    (15109 MiB and 3 MiB), and the list of processes that currently use GPUs (there
    aren’t any). The most typical use of this command is to check how much memory
    your current process(es) use, because in GPU programming, you can easily get an
    out-of-memory error if your program uses more memory than is available.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: If you use cloud infrastructures such as AWS (Amazon Web Services) and GCP (Google
    Cloud Platform), you’ll find a wide array of virtual machine templates that you
    can use to quickly create cloud instances that support GPUs. For example, GCP
    has Nvidia’s official GPU-optimized images for PyTorch and TensorFlow, which you
    can use as templates to launch your GPU instances. AWS offers Deep Learning AMIs
    (Amazon Machine Images), which preinstall basic GPU libraries such as CUDA, as
    well as deep learning libraries such as PyTorch. With these templates, you don’t
    need to install necessary drivers and libraries manually—you can start building
    your ML applications right away. Note that although these templates are free,
    you do need to pay for the infrastructure. The price for GPU-enabled virtual machines
    is usually significantly higher than CPU machines. Make sure to check their price
    before you keep them running for an extended period of time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用云基础设施，比如 AWS（Amazon Web Services）和 GCP（Google Cloud Platform），你会发现有很多虚拟机模板，可以用来快速创建支持
    GPU 的云实例。例如，GCP 提供了 Nvidia 官方的 GPU 优化图像，可以用作模板来启动 GPU 实例。AWS 提供了深度学习 AMIs（Amazon
    Machine Images），预先安装了基本的 GPU 库，如 CUDA，以及深度学习库，如 PyTorch。使用这些模板时，你不需要手动安装必要的驱动程序和库——你可以直接开始构建你的
    ML 应用程序。请注意，尽管这些模板是免费的，但你需要为基础设施付费。支持 GPU 的虚拟机的价格通常比 CPU 机器高得多。在长时间运行之前，请确保检查它们的价格。
- en: If you are setting up GPU instances from scratch, you can find detailed instructions[¹](#pgfId-1107018)
    for how to set up necessary drivers and libraries. To build NLP applications with
    the libraries that we covered in this book (namely, AllenNLP and Transformers),
    you need to install CUDA drivers and toolkits, as well as a PyTorch version that
    supports GPU.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要从头开始设置 GPU 实例，你可以找到详细的说明 [¹](#pgfId-1107018) 来设置必要的驱动程序和库。要使用本书中介绍的库（即，AllenNLP
    和 Transformers）构建 NLP 应用程序，你需要安装 CUDA 驱动程序和工具包，以及支持 GPU 的 PyTorch 版本。
- en: 'If your machine has GPU(s), you can enable GPU acceleration by specifying cuda_device
    in an AllenNLP config file as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器有 GPU，你可以通过在 AllenNLP 配置文件中指定 cuda_device 来启用 GPU 加速，如下所示：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This tells the trainer to use the first GPU for training and validating the
    AllenNLP model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉训练器使用第一个 GPU 训练和验证 AllenNLP 模型。
- en: 'If you are writing PyTorch code from scratch, you need to manually transfer
    your model and tensors to the GPU. Using an analogy, this is when your materials
    get shipped to an overseas factory in container ships. First, you can specify
    the device (GPU ID) to use, and invoke the to() method of tensors and models to
    move them between devices. For example, you can use the following code snippet
    to run text generation on a GPU with Hugging Face Transformers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要从头开始编写 PyTorch 代码，你需要手动将模型和张量转移到 GPU 上。用比喻来说，这就像是你的材料被运往海外工厂的集装箱船上。首先，你可以指定要使用的设备（GPU
    ID），并调用张量和模型的 to（）方法在设备之间移动它们。例如，你可以使用以下代码片段在使用 Hugging Face Transformers 的 GPU
    上运行文本生成：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The rest is identical to the code we used in section 8.4.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的与我们在第8.4节中使用的代码相同。
- en: '11.3 Case study: Serving and deploying NLP applications'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 案例研究：提供和部署 NLP 应用程序
- en: In this section, we will go over a case study where we serve and deploy an NLP
    model built with Hugging Face. Specifically, we’ll take a pretrained language
    generation model (DistilGPT2), serve it with TorchServe, and deploy it to a cloud
    server using Amazon SageMaker.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对一个案例研究进行概述，在其中，我们使用 Hugging Face 构建了一个 NLP 模型。具体地说，我们将使用预训练的语言生成模型（DistilGPT2），使用
    TorchServe 进行服务，并使用 Amazon SageMaker 部署到云服务器。
- en: 11.3.1 Serving models with TorchServe
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 用 TorchServe 提供模型
- en: As you have seen, deploying an NLP application is more than just writing an
    API for your ML model. You need to take care of a number of production-related
    considerations, including how to deal with high traffic by parallelizing model
    inference with multiple workers, how to store and manage different versions of
    multiple ML models, how to consistently handle pre- and postprocessing of the
    data, and how to monitor the health of the server as well as various metrics about
    the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，部署 NLP 应用程序不仅仅是编写 ML 模型的 API。你需要考虑许多与生产相关的问题，包括如何使用多个 worker 并行化模型推理来处理高流量，如何存储和管理多个
    ML 模型的不同版本，如何一致地处理数据的预处理和后处理，并且如何监视服务器的健康状况以及数据的各种指标。
- en: Because these problems are so common, ML practitioners have been working on
    general-purpose platforms for serving and deploying ML models. In this section,
    we’ll use TorchServe ([https://github.com/pytorch/serve](https://github.com/pytorch/serve)),
    an easy-to-use framework for serving PyTorch models jointly developed by Facebook
    and Amazon. TorchServe is shipped with many functionalities that can address the
    issues mentioned earlier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'TorchServe can be installed by running the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this case study, we’ll use a pretrained language model called *DistilGPT2*.
    DistilGPT2 is a smaller version of GPT-2 built using a technique called *knowledge
    distillation*. Knowledge distillation (or simply *distillation*) is a machine
    learning technique where a smaller model (called a *student*) is trained in such
    a way that it mimics the predictions produced by a larger model (called a *teacher*).
    It is a great way to train a smaller model that produces high quality output,
    and it often produces a better model than training a smaller model from scratch.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s download the pretrained DistilGPT2 model from the Hugging Face
    repository by running the following commands. Note that you need to install Git
    Large File Storage ([https://git-lfs.github.com/](https://git-lfs.github.com/)),
    a Git extension for handling large files under Git:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This creates a subdirectory called distilgpt2, which contains files such as
    config.json and pytorch_model.bin.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: As the next step, you need write a handler for TorchServe, a lightweight wrapper
    class that specifies how to initialize your model, preprocess and postprocess
    the input, and run the inference on the input. Listing 11.1 shows the handler
    code for serving the DistilGPT2 model. In fact, nothing in the handler is specific
    to the particular model we use (DistilGPT2). You can use the same code for other
    GPT-2-like models, including the original GPT-2 models, as long as you use the
    Transformers library.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 Handler for TorchServe
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Initializes the model
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Preprocesses and tokenizes the incoming data
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Runs inference on the data
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Postprocesses the prediction
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: ❺ The handler method called by TorchServe
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Your handler needs to inherit from BaseHandler and override a few methods including
    initialize() and inference(). Your handler script also includes handle(), a top-level
    method where the handler is initialized and called.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to run torch-model-archiver, which is a command-line tool
    that packages your model and your handler, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first two options specify the name and the version of the model. The next
    option, serialized-file, specifies the main weight file of the PyTorch model you
    want to package (which usually ends with .bin or .pt). You can also add any extra
    files (specified by extra-files) that are needed for the model to run. Finally,
    you need to pass the handler file you just wrote to the handler option.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'When finished, this creates a file named distilgpt2.mar (.mar stands for “model
    archive”) in the same directory. Let’s create a new directory named model_ store
    and move the .mar file there as follows. This directory serves as a model store,
    a place where all the model files are stored and served from:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now you are ready to spin up TorchServe and start serving your model! All you
    need is to run the following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When the server is fully up, you can start making the HTTP requests to the
    server. It exposes a couple of endpoints, but if you just want to run inference,
    you need to invoke http://127.0.0.1:8080/predictions/ with the model name as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we are using a prompt from OpenAI’s original post about GPT-2 ([https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)).
    This returns the generated sentences, shown next. The generated text is of decent
    quality, considering that the model is a distilled, smaller version:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '*In a shocking finding, scientist discovered a herd of unicorns living in a
    remote, previously unexplored valley, in the Andes Mountains. Even more surprising
    to the researchers was the fact that the unicorns spoke perfect English. They
    used to speak the Catalan language while working there, and so the unicorns were
    not just part of the local herd, they were also part of a population that wasn''t
    much less diverse than their former national-ethnic neighbors, who agreed with
    them.*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '*“In a sense they learned even better than they otherwise might have been,”
    says Andrea Rodriguez, associate professor of language at the University of California,
    Irvine. “They told me that everyone else was even worse off than they thought.”*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '*The findings, like most of the research, will only support the new species
    that their native language came from. But it underscores the incredible social
    connections between unicorns and foreigners, especially as they were presented
    with a hard new platform for studying and creating their own language.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '*“Finding these people means finding out the nuances of each other, and dealing
    with their disabilities better,” Rodriguez says.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '*...*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are finished, you can run the following command to stop serving:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 11.3.2 Deploying models with SageMaker
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Amazon SageMaker is a managed platform for training and deploying machine learning
    models. It enables you to spin up a GPU server, run a Jupyter Notebook inside
    it, build and train ML models there, and directly deploy them in a hosted environment.
    Our next step is to deploy the machine learning model as a cloud SageMaker endpoint
    so that production systems can make requests to it. The concrete steps for deploying
    an ML model with SageMaker consist of the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Upload your model to S3.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register and upload your inference code to Amazon Elastic Container Registry
    (ECR).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a SageMaker model and an endpoint.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make requests to the endpoint.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are going to follow the official tutorial ([http://mng.bz/p9qK](http://mng.bz/p9qK))
    with a slight modification. First, let’s go to the SageMaker console ([https://console.aws.amazon.com/sagemaker/home](https://console.aws.amazon.com/sagemaker/home))
    and start a notebook instance. When you open the notebook, run the following code
    to install the necessary packages and start a SageMaker session:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The variable bucket_name contains a string like sagemaker-xxx-yyy where xxx
    is the region name (like us-east-1). Take note of this name—you need it to upload
    your model to S3 in the next step.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need to upload your model to an S3 bucket by running the following
    commands from the machine where you just created the .mar file (not from the SageMaker
    notebook instance). Before uploading, you first need to compress your .mar file
    into a tar.gz file, a format supported by SageMaker. Remember to replace sagemaker-xxx-yyy
    with the actual bucket name specified by bucket_name:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The next step is to register and push the TorchServe inference code to ECR.
    Before you start, in your SageMaker notebook instance, open torchserve-examples/Dockerfile
    and modify the following line (add —no-cache-dir transformers):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now you can build a Docker container and push it to ECR as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now you are ready to create a SageMaker model and create an endpoint for it,
    as shown next:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The predictor object is something you can call directly to run the inference
    as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The content of the response should look something like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Congratulations! We just completed our journey—we started building an ML model
    in chapter 2 and came all the way to deploying it to a cloud platform in this
    chapter.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Interpreting and visualizing model predictions
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People often talk about the metrics and leaderboard performance on standardized
    datasets, but analyzing and visualizing model predictions and internal states
    is important for NLP applications in the real world. Although deep learning models
    can be really good at what they do, often reaching human-level performance on
    some NLP tasks, those deep models are black boxes, and it is difficult to know
    *why* they make certain predictions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Because of this (somewhat troubling) property of deep learning models, a growing
    field in AI called *explainable AI* (XAI) is working to develop methods to explain
    the predictions and behavior of ML models. Interpreting ML models is useful for
    debugging—it gives you a lot of clues if you know why it made certain predictions.
    In some domains such as medical applications and self-driving cars, making ML
    models explainable is critical for legal and practical reasons. In this final
    section of the chapter, we’ll go over a case study where we use the *Language
    Interpretability Tool* (LIT) ([https://pair-code.github.io/lit/](https://pair-code.github.io/lit/))
    for visualizing and interpreting the predictions and behavior of NLP models.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'LIT is an open source toolkit developed by Google and offers a browser-based
    interface for interpreting and visualizing ML predictions. Note that it is framework
    agnostic, meaning that it works with any Python-based ML frameworks of choice,
    including AllenNLP and Hugging Face Transformers.[²](#pgfId-1107296) LIT offers
    a wide range of features, including the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '*Saliency map*—Visualizing in color which part of the input played an important
    role to reach the current prediction'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Aggregate statistics*—Showing aggregate statistics such as dataset metrics
    and confusion matrices'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Counterfactuals*—Observing how model predictions change for generated new
    examples'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the remainder of this section, let’s take one of the AllenNLP models we trained
    (the BERT-based sentiment analysis model in chapter 9) and analyze it via LIT.
    LIT offers a set of extensible abstractions such as datasets and models to make
    it easier to work with any Python-based ML models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s install LIT. It can be installed with a single call of pip as
    follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, you need to wrap your dataset and model with the abstract classes defined
    by LIT. Let’s create a new script called run_lit.py, and import the necessary
    modules and classes, as shown here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The next code shows how to define a dataset for LIT. Here, we are creating
    a toy dataset that consists of just four hardcoded examples, but in practice,
    you may want to read a real dataset that you want to explore. Remember to define
    the spec() method that returns the type specification of the dataset:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, we are ready to define the main model, as shown next.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Defining the main model for LIT
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Loads the AllenNLP archive
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Extracts and sets the predictor
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Runs the predict method of the predictor
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: In the constructor (__init__), we are loading an AllenNLP model from an archive
    file and creating a predictor from it. We are assuming that your model is put
    under model/model.tar.gz and hard-coding its path, but feel free to modify this,
    depending on where your model is located.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The model prediction is computed in predict_minibatch(). Given the input (which
    is simply an array of dataset instances), it runs the model via the predictor
    and returns the result. Note that the predictions are made instance-by-instance,
    although in practice, you should consider making predictions in batches because
    it will improve throughput for larger input data. The method also returns the
    embeddings for predicted classes (as cls_emb), which will be used for visualizing
    embeddings (figure 11.5).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![CH11_F05_Hagiwara](../Images/CH11_F05_Hagiwara.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 LIT can show saliency maps, aggregate statistics, and embeddings
    for analyzing your model and predictions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, here’s the code for running the LIT server:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After running the script above, go to http:/./localhost:5432/ on your browser.
    You should see a screen similar to the one shown in figure 11.5\. You can see
    an array of panels corresponding to various information about the data and predictions,
    including embeddings, the dataset table and editor, classification results, and
    saliency maps (which shows contributions of tokens computed via an automated method
    called *LIME*[³](#pgfId-1107391)).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上面的脚本后，转到 http:/./localhost:5432/ 在你的浏览器上。你应该会看到一个类似于图 11.5 的屏幕。你可以看到一系列面板，对应于有关数据和预测的各种信息，包括嵌入、数据集表和编辑器、分类结果以及显著性图（显示通过一种名为
    *LIME* 的自动方法计算的标记贡献）[³](#pgfId-1107391)。
- en: Visualizing and interacting with model predictions are a great way to get insights
    into how the model works and how you should improve it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化和与模型预测进行交互是了解模型工作原理以及如何改进的好方法。
- en: 11.5 Where to go from here
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 从这里开始去哪里
- en: In this book, we’ve only scratched the surface of this vast, long-historied
    field of natural language processing. If you are interested in learning the practical
    aspects of NLP further, *Natural Language Processing in Action* by Hobson Lane
    and others (Manning Publications, 2019) and *Practical Natural Language Processing*
    by Sowmya Vajjala and others (O’Reilly, 2020) can be a good next step. *Machine
    Learning Engineering* by Andriy Burkov (True Positive Inc., 2020) is also a good
    book to learn engineering topics for machine learning in general.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们只是浅尝了这个广阔而悠久的自然语言处理领域的表面。如果你对进一步学习 NLP 的实践方面感兴趣，*Natural Language Processing
    in Action*，作者是 Hobson Lane 和其他人（Manning Publications，2019），以及 *Practical Natural
    Language Processing*，作者是 Sowmya Vajjala 和其他人（O’Reilly，2020），可以成为下一个很好的步骤。*Machine
    Learning Engineering*，作者是 Andriy Burkov（True Positive Inc.，2020），也是学习机器学习工程主题的好书。
- en: If you are interested in learning more mathematical and theoretical aspects
    of NLP, I’d recommend giving some popular textbooks a try, such as *Speech and
    Language Processing* by Dan Jurafsky and James H. Martin (Prentice Hall, 2008)[⁴](#pgfId-1107407)
    and *Introduction to Natural Language Processing* by Jacob Eisenstein (MIT Press,
    2019). *Foundations of Statistical Natural Language Processing* by Christopher
    D. Manning and Hinrich Schütze (Cambridge, 1999), though a bit outdated, is also
    a classic textbook that can give you a solid foundation for a wide variety of
    NLP methods and models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对学习 NLP 的数学和理论方面更感兴趣，我建议你尝试一些流行的教材，比如 *Speech and Language Processing*，作者是
    Dan Jurafsky 和 James H. Martin（Prentice Hall，2008）[⁴](#pgfId-1107407)，以及 *Introduction
    to Natural Language Processing*，作者是 Jacob Eisenstein（MIT Press，2019）。虽然 *Foundations
    of Statistical Natural Language Processing*，作者是 Christopher D. Manning 和 Hinrich
    Schütze（Cambridge，1999），有点过时，但它也是一本经典教材，可以为你提供广泛的 NLP 方法和模型打下坚实的基础。
- en: Also remember that you can often find great resources online for free. A free
    AllenNLP course, “A Guide to Natural Language Processing with AllenNLP” ([https://guide
    .allennlp.org/](https://guide.allennlp.org/)), and the documentation for Hugging
    Face Transformers ([https://huggingface.co/transformers/index.html](https://huggingface.co/transformers/index.html))
    are great places to go to if you want to learn those libraries in depth.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 也要记住，你通常可以免费在网上找到很棒的资源。一个免费的 AllenNLP 课程，“A Guide to Natural Language Processing
    with AllenNLP”（[https://guide .allennlp.org/](https://guide.allennlp.org/)），以及
    Hugging Face Transformers 的文档（[https://huggingface.co/transformers/index.html](https://huggingface.co/transformers/index.html)）是学习这些库的深入了解的好地方。
- en: Finally, the most effective way to learn NLP is actually doing it yourself.
    If you have problems for your hobby, work, or anything that involves dealing with
    natural language text, think whether any of the techniques you learned in this
    book are applicable. Is it a classification, tagging, or sequence-to-sequence
    problem? Which models do you use? How do you get the training data? How do you
    evaluate your model? If you don’t have NLP problems laying around, don’t worry—head
    over to Kaggle, where you can find a number of NLP-related competitions in which
    you can “get your hands dirty” and gain NLP experience while working on real-world
    problems. NLP conferences and workshops often host shared tasks, where participants
    can compete on a common task, datasets, and evaluation metrics, which are also
    a great way to learn further if you want to deep dive into a particular field
    of NLP.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning code is usually a small portion in real-world NLP/ML systems,
    supported by a complex infrastructure for data collection, feature extraction,
    and model serving and monitoring.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP modules can be developed as a one-off script, a batch prediction service,
    or a real-time prediction service.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to version-control your model and data, in addition to the source
    code. Beware of train-serve skew that causes discrepancies between the training
    and the testing times.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can easily serve PyTorch models with TorchServe and deploy them to Amazon
    SageMaker.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainable AI is a new field for explaining and interpreting ML models and
    their predictions. You can use LIT (Language Interpretability Tool) to visualize
    and interpret model predictions.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^(1.)GCP: [https://cloud.google.com/compute/docs/gpus/install-drivers-gpu;](https://cloud.google.com/compute/docs/gpus/install-drivers-gpu)
    AWS: [https://docs.aws.amazon .com/AWSEC2/latest/UserGuide/install-nvidia-driver.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: ^(2.)There is another toolkit called AllenNLP Interpret ([https://allennlp.org/interpret](https://allennlp.org/interpret))
    that offers a similar set of features for understanding NLP models, although it
    is specifically designed to interact with AllenNLP models.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '^(3.)Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions
    of Any Classifier,” (2016). [https://arxiv.org/abs/1602.04938](https://arxiv.org/abs/1602.04938).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: ^(4.)You can read the draft of the third edition (2021) for free at [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
