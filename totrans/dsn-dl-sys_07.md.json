["```py\n# step 1: start backend predictor service\ndocker build -t orca3/intent-classification-predictor:latest \\   ❶\n  -f predictor/Dockerfile predictor\n\ndocker run --name intent-classification-predictor \\              ❷\n   --network orca3 --rm -d -p \"${ICP_PORT}\":51001 \\              ❷\n   -v \"${MODEL_CACHE_DIR}\":/models \\                             ❷\n   orca3/intent-classification-predictor:latest\n\n# step 2: start the prediction service (the web api)\ndocker build -t orca3/services:latest -f \\                       ❸\n  services.dockerfile .\n\ndocker run --name prediction-service --network orca3 \\           ❹\n  --rm -d -p \"${PS_PORT}\":51001 -v \"${MODEL_CACHE_DIR}\":/tmp/modelCache \\\n  orca3/services:latest prediction-service.jar\n```", "```py\n#./scripts/lab-004-model-serving.sh 1 \"merry christmas\"\ngrpcurl -plaintext \n  -d \"{\n    \"runId\": \"1\",                        ❶\n    \"document\": \"merry christmas\"        ❷\n }\" \nlocalhost:\"${PS_PORT}\" \nprediction.PredictionService/Predict\n\nmodel_id is 1                            ❸\ndocument is hello world                  ❸\n{\n  \"response\": \"{\\\"result\\\": \\\"joy\\\"}\"    ❸\n}\n```", "```py\npublic void predict(PredictRequest request, .. .. ..) {\n  .. .. ..\n  String runId = request.getRunId();                        ❶\n\n  if (predictorManager.containsArtifact(runId)) {           ❷\n    artifactInfo = predictorManager.getArtifact(runId);\n  } else {\n    try {\n       artifactInfo = msClient.getArtifact(                 ❷\n                GetArtifactRequest.newBuilder()\n               .setRunId(runId).build());\n     } catch (Exception ex) {\n       .. .. .. \n    }\n  } \n\n # Step 4, pick predictor backend client by model algorithm type\n  PredictorBackend predictor;\n  if (predictorManager.containsPredictor(\n        artifactInfo.getAlgorithm())) {\n\n    predictor = predictorManager.getPredictor(              ❸\n        artifactInfo.getAlgorithm());\n  } else {\n   .. .. ..\n  }\n\n  # Step 5, use the selected predictor client to download the model files\n  predictor.downloadModel(runId, artifactInfo);             ❹\n\n  # Step 6, use the selected predictor client to call\n # its backend predictor for model serving\n  String r = predictor.predict(                             ❺\n     artifactInfo, request.getDocument());                  ❺\n  .. .. ..\n}\n```", "```py\nservice PredictionService {\n rpc Predict(PredictRequest) returns (PredictResponse);\n}\n\nmessage PredictRequest {\n string runId = 3;\n string document = 4;\n}\n\nmessage PredictResponse {\n string response = 1;\n}\n```", "```py\n# the algorithm and predictor mapping can be defined in \n# either app config or docker properties\n\n# enable algorithm types\nps.enabledPredictors=intent-classification\n\n# define algorithm and predictors mapping\n# predictor.<algorithm_type>.XXX = predictor[host, port, type]\npredictors.intent-classification.host= \\      ❶\n  Intent-classification-predictor             ❶\npredictors.intent-classification.port=51001\npredictors.intent-classification.techStack=customGrpc\n```", "```py\npublic class PredictorConnectionManager {\n  private final Map<String, List<ManagedChannel>> \n    channels = new HashMap<>();\n\n  private final Map<String, PredictorBackend>                ❶\n    clients = new HashMap<>();\n\n  private final Map<String, GetArtifactResponse>             ❷\n    artifactCache;\n\n  // create predictor backend objects for \n // the registered algorithm and predictor\n  public void registerPredictor(String algorithm, \n       Properties properties) {\n\n    String host = properties.getProperty(                    ❸\n       String.format(“predictors.%s.host”, algorithm));\n\n    int port = Integer.parseInt(properties.getProperty(      ❸\n       String.format(“predictors.%s.port”, algorithm)));\n\n    String predictorType = properties.getProperty(           ❸\n       String.format(“predictors.%s.techStack”, algorithm));\n\n    ManagedChannel channel = ManagedChannelBuilder\n       .forAddress(host, port)\n       .usePlaintext().build();\n\n    switch (predictorType) {\n      .. ..\n      case “customGrpc”:                                     ❹\n      default:\n        channels.put(algorithm, List.of(channel));\n        clients.put(algorithm, new CustomGrpcPredictorBackend(\n          channel, modelCachePath, minioClient));\n      break;\n     }\n  }\n\n  .. .. ..\n}\n```", "```py\npublic interface PredictorBackend {\n   void downloadModel(String runId, \n           GetArtifactResponse artifactResponse);\n\n   String predict(GetArtifactResponse artifact, String document);\n\n   void registerModel(GetArtifactResponse artifact);\n}\n```", "```py\n// calling backend predictor for model serving\npublic String predict(GetArtifactResponse artifact, String document) {\n   return stub.predictorPredict(PredictorPredictRequest\n       .newBuilder().setDocument(document)    ❶\n       .setRunId(artifact.getRunId())         ❷\n       .build()).getResponse();\n}\n```", "```py\nservice Predictor {\n rpc PredictorPredict(PredictorPredictRequest) returns (PredictorPredictResponse);\n}\n\nmessage PredictorPredictRequest {\n string runId = 1;\n string document = 2;\n}\n\nmessage PredictorPredictResponse {\n string response = 1;\n}\n```", "```py\n├── manifest.json                          ❶\n├── model.pth                              ❷\n└── vocab.pth                              ❸\n\n// A sample manifest.json file \n{\n  \"Algorithm\": \"intent-classification\",    ❹\n  \"Framework\": \"Pytorch\",\n  \"FrameworkVersion\": \"1.9.0\",\n  \"ModelName\": \"intent\",\n  \"CodeVersion\": \"80bf0da\",\n  \"ModelVersion\": \"1.0\",\n  \"classes\": {                             ❺\n    \"0\": \"cancel\",\n    \"1\": \"ingredients_list\",\n    \"2\": \"nutrition_info\",\n    \"3\": \"greeting\",\n     .. .. ..\n}\n```", "```py\nclass TextClassificationModel(nn.Module):\n\n  def __init__(self, vocab_size, embed_dim,   ❶\n      fc_size, num_class):                    ❶\n\n    super(TextClassificationModel, self).__init__()\n    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n    self.fc1 = nn.Linear(embed_dim, fc_size)\n    self.fc2 = nn.Linear(fc_size, num_class)\n    self.init_weights()\n\n  def forward(self, text, offsets):\n    embedded = self.embedding(text, offsets)\n    return self.fc2(self.fc1(embedded))\n```", "```py\nclass ModelManager:\n  def __init__(self, config, tokenizer, device):\n    self.model_dir = config.MODEL_DIR\n    self.models = {}                                      ❶\n\n  # load model file and initialize model\n  def load_model(self, model_id):\n    if model_id in self.models:\n      return\n\n    # load model files, including vocabulary, prediction class mapping.\n    vacab_path = os.path.join(self.model_dir, model_id, \"vocab.pth\")\n    manifest_path = os.path.join(self.model_dir, model_id, \"manifest.json\")\n    model_path = os.path.join(self.model_dir, model_id, \"model.pth\")\n\n    vocab = torch.load(vacab_path)\n    with open(manifest_path, 'r') as f:\n    manifest = json.loads(f.read())\n    classes = manifest['classes']\n\n    # initialize model graph and load model weights\n    num_class, vocab_size, emsize = len(classes), len(vocab), 64\n    model = TextClassificationModel(vocab_size, emsize, \n       self.config.FC_SIZE, num_class).to(self.device)\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n\n    self.models[self.model_key(model_id)] = model        ❷\nself.models[self.model_vocab_key(model_id)]              ❷\n  ➥ = vocab                                             ❷\nself.models[self.model_classes(model_id)]                ❷\n  ➥ = classes                                           ❷\n\n  # run model to make prediction\n  def predict(self, model_id, document):\n    # fetch model graph, dependency and \n    # classes from cache by model id \n    model = self.models[self.model_key(model_id)]\n    vocab = self.models[self.model_vocab_key(model_id)]\n    classes = self.models[self.model_classes(model_id)]\n\n    def text_pipeline(x):\n      return vocab(self.tokenizer(x))\n\n    # transform user input data (text string) \n # to model graph’s input\n    processed_text = torch.tensor(text_pipeline(document), dtype=torch.int64)\n    offsets = [0, processed_text.size(0)]\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n\n    val = model(processed_text, offsets)                ❸\n\n    # convert prediction result from an integer to \n # a text string (class)\n    res_index = val.argmax(1).item()\n    res = classes[str(res_index)]\n    print(\"label is {}, {}\".format(res_index, res))\n    return res\n```", "```py\ndef serve():\n  .. .. ..\n  model_manager = ModelManager(config, \n    tokenizer=get_tokenizer('basic_english'), device=\"cpu\")\n  server = grpc.server(futures.\n    ThreadPoolExecutor(max_workers=10))                         ❶\n\n  prediction_service_pb2_grpc.add_PredictorServicer_to_server(\n    PredictorServicer(model_manager), server)                   ❷\n  .. .. ..\n\nclass PredictorServicer(prediction_service_pb2_grpc.PredictorServicer):\n  def __init__(self, model_manager):\n    self.model_manager = model_manager\n\n # Serving logic \n  def PredictorPredict(self, request, context: grpc.ServicerContext):\n\n # load model \n    self.model_manager.load_model(model_id=request.runId)\n    class_name = self.model_manager.                            ❸\n      predict(request.runId, request.document)\n    return PredictorPredictResponse(response=json.dumps({'res': class_name}))\n```", "```py\n# step 1: start torchserve backend\ndocker run --name intent-classification-torch-predictor\\\n --network orca3 --rm -d -p \"${ICP_TORCH_PORT}\":7070 \\\n -p \"${ICP_TORCH_MGMT_PORT}\":7071 \\\n -v \"${MODEL_CACHE_DIR}\":/models \\                     ❶\n -v \"$(pwd)/config/torch_server_config.properties\": \\\n     /home/model-server/config.properties \\\n pytorch/torchserve:0.5.2-cpu torchserve \\             ❷\n --start --model-store /models                         ❸\n\n# step 2: start the prediction service (the web frontend)\ndocker build -t orca3/services:latest -f services.dockerfile .\ndocker run --name prediction-service --network orca3 \\\n  --rm -d -p \"${PS_PORT}\":51001 \\\n  -v \"${MODEL_CACHE_DIR}\":/tmp/modelCache \\            ❹\n orca3/services:latest  \\\n prediction-service.jar\n\n# step 3: make a prediction request, ask intent for “merry christmas”\ngrpcurl -plaintext \n  -d \"{\n    \"runId\": \"${MODEL_ID}\",\n    \"document\": \"merry christmas\"\n }\" \n localhost:\"${PS_PORT}\" prediction.PredictionService/Predict\n```", "```py\npublic void registerModel(GetArtifactResponse artifact) {\n  String modelUrl = String.format(MODEL_FILE_NAME_TEMPLATE,\n        artifact.getRunId());\n\n  String torchModelName = String.format(TORCH_MODEL_NAME_TEMPLATE,\n            artifact.getName(), artifact.getVersion());\n  ManagementResponse r = managementStub.registerModel(     ❶\n           RegisterModelRequest.newBuilder()\n             .setUrl(modelUrl)\n             .setModelName(torchModelName)\n             .build());\n\n  # Assign resource (TorchServe worker) for this model\n  managementStub.scaleWorker(ScaleWorkerRequest.newBuilder()\n             .setModelName(torchModelName)\n             .setMinWorker(1)\n             .build());\n}\n```", "```py\n# prediction with single input on model resnet-18\ncurl http:/ /localhost:8080/predictions/resnet-18 \\\n -F \"data=@kitten_small.jpg\"\n# prediction with multiple inputs on model squeezenet1_1\ncurl http:/ /localhost:8080/predictions/squeezenet1_1 \\\n -F 'data=@docs/images/dogs-before.jpg' \\\n -F 'data=@docs/images/kitten_small.jpg'\n```", "```py\n// call TorchServe gRPC prediction api\npublic String predict(GetArtifactResponse artifact, String document) {\n  return stub.predictions(PredictionsRequest.newBuilder()\n           .setModelName(String.format(TORCH_MODEL_NAME_TEMPLATE,\n              artifact.getName(), artifact.getVersion()))\n           .putAllInput(ImmutableMap.of(\"data\",                         ❶\n              ByteString.copyFrom(document, StandardCharsets.UTF_8)))\n                .build()).getPrediction()\n           .toString(StandardCharsets.UTF_8);\n}\n```", "```py\n# intent.mar content\n├── MAR-INF\n│   └── MANIFEST.json          ❶\n├── manifest.json              ❷\n├── model.pth                  ❸\n├── torchserve_handler.py      ❹\n└── vocab.pth                  ❺\n\n# MANIFEST.json, TorchServe .mar metadata \n{\n \"createdOn\": \"09/11/2021 10:26:59\",\n \"runtime\": \"python\",\n \"model\": {\n   \"modelName\": \"intent_80bf0da\",\n   \"serializedFile\": \"model.pth\",\n   \"handler\": \"torchserve_handler.py\",\n   \"modelVersion\": \"1.0\"\n },\n \"archiverVersion\": \"0.4.2\"\n}\n```", "```py\nclass ModelHandler(BaseHandler):\n   \"\"\"\n   A custom model handler implementation for serving \n   intent classification prediction in torch serving server.\n   \"\"\"\n\n   # Model architecture \n   class TextClassificationModel(nn.Module):\n       def __init__(self, vocab_size, embed_dim, fc_size, num_class):\n           super(ModelHandler.TextClassificationModel, self)\n➥ .__init__()\n           self.embedding = nn.EmbeddingBag(vocab_size, \n➥ embed_dim, sparse=True)\n           self.fc1 = nn.Linear(embed_dim, fc_size)\n           self.fc2 = nn.Linear(fc_size, num_class)\n           self.init_weights()\n\n       def init_weights(self):\n           .. .. ..\n\n       def forward(self, text, offsets):\n           embedded = self.embedding(text, offsets)\n           return self.fc2(self.fc1(embedded))\n\n   # Load dependent files and initialize model\n   def initialize(self, ctx):\n\n       model_dir = properties.get(\"model_dir\")\n       model_path = os.path.join(model_dir, \"model.pth\")\n       vacab_path = os.path.join(model_dir, \"vocab.pth\")\n       manifest_path = os.path.join(model_dir, \"manifest.json\")\n\n       # load vocabulary\n       self.vocab = torch.load(vacab_path)\n\n       # load model manifest, including label index map.\n       with open(manifest_path, 'r') as f:\n           self.manifest = json.loads(f.read())\n       classes = self.manifest['classes']\n\n       # intialize model\n       self.model = self.TextClassificationModel(\n           vocab_size, emsize, self.fcsize, num_class).to(\"cpu\")\n       self.model.load_state_dict(torch.load(model_path))\n       self.model.eval()\n       self.initialized = True\n\n # Transform raw input into model input data.\n   def preprocess(self, data):\n\n       preprocessed_data = data[0].get(\"data\")\n       if preprocessed_data is None:\n           preprocessed_data = data[0].get(\"body\")\n\n       text_pipeline = lambda x: self.vocab(self.tokenizer(x))\n\n       user_input = \" \".join(str(preprocessed_data))\n       processed_text = torch.tensor(text_pipeline(user_input), \n            dtype=torch.int64)\n       offsets = [0, processed_text.size(0)]\n       offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n       return (processed_text, offsets)\n\n   # Run model inference by executing the model with model input\n   def inference(self, model_input):\n       model_output = self.model.forward(model_input[0], model_input[1])\n       return model_output\n\n # Take output from network and post-process to desired format\n   def postprocess(self, inference_output):\n       res_index = inference_output.argmax(1).item()\n       classes = self.manifest['classes']\n       postprocess_output = classes[str(res_index)]\n       return [{\"predict_res\":postprocess_output}]\n\n # Entry point of model serving, invoke by TorchServe \n # for prediction request \n   def handle(self, data, context):\n\n       model_input = self.preprocess(data)\n       model_output = self.inference(model_input)\n       return self.postprocess(model_output)\n```", "```py\n## Method one: package model by command line cli tool.\ntorch-model-archiver --model-name intent_classification --version 1.0 \\\n --model-file torchserve_model.py --serialized-file \\\n    workspace/MiniAutoML/{model_id}/model.pth \\\n --handler torchserve_handler.py --extra-files \\\nworkspace/MiniAutoML/{model_id}/vocab.pth,\n➥ workspace/MiniAutoML/{model_id}/manifest.json\n\n## Method two: package model in training code.\nmodel_archiver.archive(model_name=archive_model_name, \n  handler_file=handler, model_state_file=model_local_path,\n  extra_files=extra_files, model_version=config.MODEL_SERVING_VERSION,\n  dest_path=config.JOB_ID)\n```", "```py\n# 1\\. Save model in training code \nMODEL_DIR='tf_model'\nversion = \"1\"\nexport_path = os.path.join(MODEL_DIR, str(version))\nmodel.save(export_path, save_format=\"tf\")\n\n# 2\\. Start tensorflow serving service locally as a docker container\ndocker run -p 8501:8501 \n--mount type=bind,source=/workspace/tf_model,target=/models/model_a/ \n-e MODEL_NAME=model_a -t tensorflow/serving\n--model_config_file_poll_wait_seconds=60\n--model_config_file=/models/model_a/models.config \n\n# 3\\. Send predict request to local tensorflow serving docker container \n# The request url pattern to call a specific version of a model is\n   /v1/models/<model name>/versions/<version number> \njson_response = requests.post('http:/ /localhost:8501/\n ➥ v1/models/model_a/versions/1:predict', \n  data=data, headers=headers)\n```", "```py\nmodel_config_list {\n  config{\n    name: 'model_a'\n    base_path: '/models/model_a/'\n    model_platform: 'tensorflow'\n    model_version_policy{\n      specific{\n        versions:2    ❶\n        versions:3    ❷\n      }\n    }\n  }\n  config{\n    name: 'model_b'\n    base_path: '/models/model_b/'\n    model_platform: 'tensorflow'\n  }\n}\n```", "```py\n# 1\\. Create model store directory for torch serving\n# and copy model files (mar files) to it\nmkdir -p /tmp/model_store/torchserving\ncp sample_models/intent.mar /tmp/model_store/torchserving                       ❶\n\n# 2\\. Run the torch serving container\ndocker pull pytorch/torchserve:0.4.2-cpu\ndocker run --rm --shm-size=1g \\\n       --ulimit memlock=-1 \\\n       --ulimit stack=67108864 \\\n       -p8080:8080 \\\n       -p8081:8081 \\\n       -p8082:8082 \\\n       -p7070:7070 \\\n       -p7071:7071 \\\n       --mount type=bind,source=/tmp/model_store/torchserving,target=/tmp/models❷\npytorch/torchserve:0.4.2-cpu torchserve --model-store=/tmp/models\n\n# 3\\. Register intent model through torchserving management api\ncurl -X POST  \"http:/ /localhost:8081/models?url=                               ❸\n➥ intent_1.mar&initial_workers=1&model_name=intent\"                            ❸\n\n# 4\\. Query intent model in torch serving with default version.\ncurl --location --request GET 'http:/ /localhost:8080/predictions/intent' \\\n--header 'Content-Type: text/plain' \\\n--data-raw 'make a 10 minute timer'\n\n# 5\\. Query intent model in torch serving with specified version - 1.0\ncurl --location --request GET 'http:/ /localhost:8080/predictions/intent/1.0' \\\n--header 'Content-Type: text/plain' \\\n--data-raw 'make a 10 minute timer'\n```", "```py\n# 1\\. Scale up the worker number for the intent model. Default number is 1.\n# Set minimum worker count to 3 and maximum worker count to 6 \n# for version 1.0 of the intent model \ncurl -v -X PUT \"http:/ /localhost:8081/models/intent/1.0?min_worker=3&max_worker=6\"\n\n# 2\\. Use the describe model API to get detail runtime status of \n# default version of the intent model.\ncurl http:/ /localhost:8081/models/intent\n\n# 3\\. Use the describe model API to get detail runtime status of \n# specific version (1.0) of the intent model.\ncurl http:/ /localhost:8081/models/intent/1.0 \n```", "```py\nplatform: “pytorch_libtorch”    ❶\npytorch_libtorch                ❷\n  max_batch_size: 8             ❸\n  input [                       ❹\n    {\n      name: “input0”\n      data_type: TYPE_FP32\n      dims: [ 16 ]\n    },\n    {\n      name: “input1”\n      data_type: TYPE_FP32\n      dims: [ 16 ]\n    }\n  ]\n  output [                      ❺\n    {\n      name: “output0”\n      data_type: TYPE_FP32\n      dims: [ 16 ]\n    }\n  ]\n```", "```py\n#### Pytorch training code\n\n# 1\\. Define an instance of your model.\nModel = ...TorchModel()\n\n# 2\\. Switch the model to eval model\nmodel.eval()\n\n# 3\\. Build an example input of the model’s forward() method.\nExample = torch.rand(1, 3, 224, 224)\n\n# 4\\. Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\nTraced_script_module = torch.jit.trace(model, example)\n# 5\\. Save the TorchScript model\ntraced_script_module.save(“traced_torch_model.pt”)   \n```", "```py\napiVersion: serving.kserve.io/v1beta1             ❶\nkind: InferenceService\nmetadata:\n name: “torchserve”\nspec:\n predictor:\n   pytorch:                                       ❷\n     storageUri: gs://kfserving-examples/models   ❸\n          ➥ /torchserve/image_classifier         ❸\n```"]