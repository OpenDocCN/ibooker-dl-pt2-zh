["```js\ngit clone https://github.com/tensorflow/tfjs-examples.git\ncd tfjs-examples/jena-weather\nyarn\nyarn train-rnn --modelType baseline\n```", "```js\nCommonsense baseline mean absolute error: 0.290331\n```", "```js\ny = 0                             ***1***\nfor x in input_sequence:          ***2***\n  y = f(dot(W, x) + dot(U, y))    ***3***\n```", "```js\nfunction buildSimpleRNNModel(inputShape) {\n  const model = tf.sequential();\n  const rnnUnits = 32;                       ***1***\n  model.add(tf.layers.simpleRNN({            ***2***\n    units: rnnUnits,\n    inputShape\n  }));\n  model.add(tf.layers.dense({units: 1}));    ***3***\n  return model;\n}\n```", "```js\nyarn train-rnn --modelType simpleRNN --logDir /tmp/\n  jean-weather-simpleRNN-logs\n```", "```js\n import * as tf from '@tensorflow/tfjs-node';\n// Or '@tensorflow/tfjs-node-gpu'\n\n     // ...\n await model.fit(xs, ys, {\n   epochs,\n   callbacks: tf.node.tensorBoard('/path/to/my/logdir')\n });\n\n      // Or for fitDataset():\n await model.fitDataset(dataset, {\n   epochs,\n   batchesPerEpoch,\n   callbacks: tf.node.tensorBoard('/path/to/my/logdir')\n });\n```", "```js\nLayer (type)                 Output shape              Param #\n     =================================================================\n     simple_rnn_SimpleRNN1 (Simpl [null,32]                 1504\n     _________________________________________________________________\n     dense_Dense1 (Dense)         [null,1]                  33\n     =================================================================\nTotal params: 1537\n     Trainable params: 1537\n     Non-trainable params: 0\n     _________________________________________________________________\n```", "```js\nh = 0                                             ***1***\nfor x_i in input_sequence:                        ***2***\n  z = sigmoid(dot(W_z, x) + dot(U_z, h))          ***3***\n  r = sigmoid(dot(W_r, x) + dot(W_r, h))          ***4***\n  h_prime = tanh(dot(W, x) + dot(r, dot(U, h)))   ***5***\n  h = dot(1 - z, h) + dot(z, h_prime)             ***6***\n```", "```js\nfunction buildGRUModel(inputShape) {\n  const model = tf.sequential();\n  const rnnUnits = 32;                      ***1***\n  model.add(tf.layers.gru({                 ***2***\n    units: rnnUnits,\n    inputShape\n  }));\n  model.add(tf.layers.dense({units: 1}));   ***3***\n  return model;\n}\n```", "```js\nyarn train-rnn --modelType gru\n```", "```js\n    git clone https://github.com/tensorflow/tfjs-examples.git\n    cd tfjs-examples/sentiment\n    yarn\n    yarn train multihot\n```", "```js\nconst buffer = tf.buffer([sequences.length, numWords]);   ***1***\n     sequences.forEach((seq, i) => {                      ***2***\n  seq.forEach(wordIndex => {                              ***3***\n    if (wordIndex !== OOV_INDEX) {                        ***4***\n      buffer.set(1, i, wordIndex);                        ***5***\n    }\n  });\n});\n```", "```js\nconst model = tf.sequential();\nmodel.add(tf.layers.dense({             ***1***\n  units: 16,\n  activation: 'relu',\n  inputShape: [vocabularySize]          ***2***\n}));\nmodel.add(tf.layers.dense({\n  units: 16,\n  activation: 'relu'\n}));\nmodel.add(tf.layers.dense({\n  units: 1,\n  activation: 'sigmoid'                 ***3***\n}));\n```", "```js\nyarn train --maxLen 500 cnn\n```", "```js\nexport function padSequences(\n    sequences, maxLen,\n         padding = 'pre',\n         truncating = 'pre',\n          value = PAD_CHAR) {\n  return sequences.map(seq => {                       ***1***\n    if (seq.length > maxLen) {                        ***2***\n      if (truncating === 'pre') {                     ***3***\n        seq.splice(0, seq.length - maxLen);\n      } else {\n        seq.splice(maxLen, seq.length - maxLen);\n      }\n    }\n\n    if (seq.length < maxLen) {                        ***4***\n      const pad = [];\n      for (let i = 0; i < maxLen - seq.length; ++i) {\n        pad.push(value);                              ***5***\n      }\n      if (padding === 'pre') {                        ***6***\n        seq = pad.concat(seq);\n      } else {\n        seq = seq.concat(pad);\n      }\n    }\n\n    return seq;                                       ***7***\n  });\n}\n```", "```js\nconst model = tf.sequential();\nmodel.add(tf.layers.embedding({               ***1***\n  inputDim: vocabularySize,                   ***2***\n  outputDim: embeddingSize,\n  inputLength: maxLen\n}));\nmodel.add(tf.layers.dropout({rate: 0.5}));    ***3***\nmodel.add(tf.layers.conv1d({                  ***4***\n  filters: 250,\n  kernelSize: 5,\n  strides: 1,\n  padding: 'valid',\n  activation: 'relu'\n}));\nmodel.add(tf.layers.globalMaxPool1d({}));     ***5***\nmodel.add(tf.layers.dense({                   ***6***\n       units: 250,                            ***6***\n       activation: 'relu'                     ***6***\n     }));                                     ***6***\nmodel.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));\n\n________________________________________________________________\nLayer (type)                 Output shape              Param #\n=================================================================\nembedding_Embedding1 (Embedd [null,500,128]            1280000\n_________________________________________________________________\ndropout_Dropout1 (Dropout)   [null,500,128]            0\n_________________________________________________________________\nconv1d_Conv1D1 (Conv1D)      [null,496,250]            160250\n_________________________________________________________________\nglobal_max_pooling1d_GlobalM [null,250]                0\n_________________________________________________________________\ndense_Dense1 (Dense)         [null,250]                62750\n_________________________________________________________________\ndense_Dense2 (Dense)         [null,1]                  251\n=================================================================\nTotal params: 1503251\nTrainable params: 1503251\nNon-trainable params: 0\n_________________________________________________________________\n```", "```js\nyarn train --maxLen 500 lstm\n```", "```js\nyarn train --maxLen 500 cnn --epochs 2 --embeddingFilesPrefix\n             /tmp/imdb_embed\n```", "```js\npredict(text) {\n  const inputText =                                                     ***1***\n      text.trim().toLowerCase().replace(/(\\.|\\,|\\!)/g, '').split(' ');  ***1***\n  const sequence = inputText.map(word => {\n    let wordIndex =                                                     ***2***\n             this.wordIndex[word] + this.indexFrom;                     ***2***\n    if (wordIndex > this.vocabularySize) {\n      wordIndex = OOV_INDEX;                                            ***3***\n    }\n    return wordIndex;\n  });\n  const paddedSequence =                                                ***4***\n           padSequences([sequence], this.maxLen);                       ***4***\n  const input = tf.tensor2d(                                            ***5***\n           paddedSequence, [1, this.maxLen]);                           ***5***\n  const beginMs = performance.now();                                    ***6***\n  const predictOut = this.model.predict(input);                         ***7***\n  const score = predictOut.dataSync()[0];\n  predictOut.dispose();\n  const endMs = performance.now();\n\n  return {score: score, elapsed: (endMs - beginMs)};\n}\n```", "```js\n\"23Jan2015\", \"012315\", \"01/23/15\", \"1/23/15\",\n\"01/23/2015\", \"1/23/2015\", \"23-01-2015\", \"23-1-2015\",\n\"JAN 23, 15\", \"Jan 23, 2015\", \"23.01.2015\", \"23.1.2015\",\n\"2015.01.23\", \"2015.1.23\", \"20150123\", \"2015/01/23\",\n\"2015-01-23\", \"2015-1-23\"\n```", "```js\n    git clone https://github.com/tensorflow/tfjs-examples.git\n    cd tfjs-examples/sentiment\n    yarn\n    yarn train\n```", "```js\n    yarn train --gpu\n```", "```js\nyarn watch\n```", "```js\n  const embeddingDims = 64;\n  const lstmUnits = 64;\n```", "```js\n  const encoderInput = tf.input({shape: [inputLength]});\n  const decoderInput = tf.input({shape: [outputLength]});\n```", "```js\n  let encoder = tf.layers.embedding({\n    inputDim: inputVocabSize,\n    outputDim: embeddingDims,\n    inputLength,\n    maskZero: true\n  }).apply(encoderInput);\n```", "```js\n  encoder = tf.layers.lstm({\n    units: lstmUnits,\n    returnSequences: true\n  }).apply(encoder);\n```", "```js\n  const encoderLast = new GetLastTimestepLayer({\n    name: 'encoderLast'\n  }).apply(encoder);\n```", "```js\n  let decoder = tf.layers.embedding({\n    inputDim: outputVocabSize,\n    outputDim: embeddingDims,\n    inputLength: outputLength,\n    maskZero: true\n  }).apply(decoderInput);\n  decoder = tf.layers.lstm({\n    units: lstmUnits,\n    returnSequences: true\n  }).apply(decoder, {initialState: [encoderLast, encoderLast]});\n```", "```js\n  let attention = tf.layers.dot({axes: [2, 2]}).apply([decoder, encoder]);\n  attention = tf.layers.activation({\n    activation: 'softmax',\n    name: 'attention'\n  }).apply(attention);\n```", "```js\n  const context = tf.layers.dot({\n    axes: [2, 1],\n    name: 'context'\n  }).apply([attention, encoder]);\n```", "```js\n  const decoderCombinedContext =\n      tf.layers.concatenate().apply([context, decoder]);\n```", "```js\n  let output = tf.layers.timeDistributed({\n    layer: tf.layers.dense({\n      units: lstmUnits,\n      activation: 'tanh'\n    })\n  }).apply(decoderCombinedContext);\n  output = tf.layers.timeDistributed({\n    layer: tf.layers.dense({\n      units: outputVocabSize,\n      activation: 'softmax'\n    })\n  }).apply(output);\n```", "```js\n  const model = tf.model({\n    inputs: [encoderInput, decoderInput],\n    outputs: output\n  });\n```", "```js\n  model.compile({\n    loss: 'categoricalCrossentropy',\n    optimizer: 'adam'\n  });\n```", "```js\n          shuffledData.push(data[indices[i]]);\n        ```", "```js\n        samples.set(value, j, exampleRow, exampleCol++);\n        ```"]