- en: 3 Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Curse of dimensionality and its disadvantages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various methods of reducing dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular Value Decomposition (SVD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python solutions for both PCA and SVD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study on dimension reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Knowledge is a process of piling up facts; wisdom lies in their simplification.”
    – Martin H. Fischer
  prefs: []
  type: TYPE_NORMAL
- en: We face complex situations in life. Life throws multiple options at us and we
    choose a few viable ones from them. This decision of shortlisting is based on
    the significance, feasibility, utility, and perceived profit from each of the
    options. The ones which fit the bill are then chosen. A perfect example can be
    selecting your holiday destination. Based on the weather, travel time, safety,
    food, budget, and a number of other options, we choose a few where we would like
    to spend our next holiday. In this chapter, we are studying precisely the same
    – how to reduce the number of options, albeit in the data science and machine
    learning world.
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter, we covered major clustering algorithms. We also studied
    a case study over there. The datasets we generate and use in such real-world examples
    have a lot of variables. Sometimes, there can be more than 100 variables or *dimensions*
    in the data. But not all of them are important; not all of them are significant.
    Having a lot dimensions in the dataset is referred to as the “*Curse of Dimensionality*”.
    To perform any further analysis we choose a few from the list of all of the dimensions
    or variables. In this chapter, we are going to study the need for dimension reductions,
    various dimensionality techniques, and the respective pros and cons. We will dive
    deeper into the concepts of Principal Component Analysis (PCA) and SVD (Singular
    Value Decomposition), their mathematical foundation and complement with the Python
    implementation. And continuing our structure from the last chapter, we will examine
    a real-world case study in the telecommunication sector towards the end. There
    are other advanced dimensionality reduction techniques like t-SNE, and LDA which
    we will explore in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering and dimensionality reductions are the major categories of unsupervised
    learning. We studied major clustering methods in the last chapter and we will
    cover dimensionality reduction in this chapter. With these two solutions, we would
    cover a lot of ground in the unsupervised learning domain. But there are much
    more advanced topics to be covered, which are part of the latter chapters of the
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first understand what we mean by “*Curse of dimensionality*”.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are using the 3.6+ version of Python as used in the last chapters. Jupyter
    Notebook will be used in this chapter too.
  prefs: []
  type: TYPE_NORMAL
- en: All the datasets and code files are available at the GitHub repository at ([https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter3](main.html)).
    You need to install the following Python libraries to execute the `numpy`, `pandas`,
    `matplotlib`, `scipy`, `sklearn`. Since you have used the same packages in the
    last chapter, you need not install them again. CPU is good enough for execution,
    but if you face some computing problems, switch to GPU or Google Colab. You can
    refer to Appendix to the book if you face any issues in the installation of any
    of these packages.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s start by developing our understanding further with regard to “*Curse
    of dimensionality,*” in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Curse of Dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us continue with the holiday destination example we introduced earlier.
    The choice of destination is dependent on a number of parameters – safety, availability,
    food, nightlife, weather, budget, health, and so on. Having too many parameters
    to decide is a confusing thought. Let us understand by a real-life example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this: A retailer wishes to launch a new range of shoes in the market.
    And for that, a target group of customers have to be chosen. This target group
    will be reached through email, newsletter, etc. The business objective is to entice
    these customers to buy the newly launched shoes. From the entire customer base,
    the target group of customers can be chosen based on the variables like customer’s
    age, gender, pocket size, preferred category, average spend, frequency of shopping,
    and so on. These many variables or *dimensions* give us a hard time shortlisting
    the customers based on a sound data analysis technique. We would be analyzing
    too many parameters simultaneously, examining the impact of each on the shopping
    probability of the customer and hence it becomes too tedious and confusing a task.
    It is the *Curse of Dimensionality* problem we face in real-world data science
    projects. We can face the curse of dimensionality in one more situation wherein
    the number of observations is lesser than the number of variables. Consider a
    dataset where the number of observations is X, while the number of variables is
    more than X – in such a case we face the Curse of Dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: An easy method to understand any dataset is through visualization. Let’s visualize
    a dataset in a vector-space diagram. If we have only one attribute or feature
    in the dataset, we can represent it in one dimension. It is shown in Figure 3.1(i).
    For example, we might wish to capture only the height of an object using a single
    dimension. In case we have two attributes, we need two dimensions as shown in
    Figure 3.1(ii), wherein to get the area of an object we will require both length
    and breadth. In case we have three attributes, for example, to calculate the volume
    which requires length, breadth, and height, it requires a three-dimensional space,
    as shown in Figure 3.1(iii). And this requirement will continue to grow based
    on the number of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 (i) Only one dimension is required to represent the data points,
    for example, to represent the height of an object (ii) We need two dimensions
    to represent a data point. Each data point can correspond to the length and breadth
    of an object which can be used to calculate the area (iii) Three dimensions are
    required to show a point. Here, it can be length, breadth, and height which are
    required to get the volume of an object. This process continues based on the number
    of dimensions present in the data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_01](images/03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Now imagine if we have in total of 20 data points to be analyzed. If we have
    only one attribute, we can represent it as x[1], x[2], x[3], x[4], x[5] …. x[20]
    and hence a 20-dimensional space will be sufficient to represent these points.
    In the second example where we require two dimensions, we will require (x[1,]y[1]),
    (x[2,]y[2]), (x[3,]y[3]), (x[4,]y[4]), (x[5,]y[5])….. (x[20,]y[20]) or in other
    words 20*20 = 400 dimension space. For a three-dimensional one, we will represent
    a point as (x[1,]y[1,]z[1]), (x[2,]y[2,]z[2]), (x[3,]y[3,]z[3]), (x[4,]y[4,]z[4]),
    (x[5,]y[5,]z[5])….. (x[20,]y[20,]z[20]) and we will need 20*20*20 = 800 dimension
    space. And this process will continue.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it is quite easy for us to conclude that with an increase in the number
    of dimensions, the amount of space required to represent increases by leaps and
    bounds. It is referred to as the *Curse of Dimensionality*. The term was introduced
    by Richard E. Bellman and is used to refer to the problem of having too many variables
    in a dataset – some of which are significant while a lot may be of less importance.
  prefs: []
  type: TYPE_NORMAL
- en: There is another well-known theory known as *Hughes’ Phenomenon* shown in Figure
    3.2\. Generally, in data science and machine learning, we wish to have as many
    variables as possible to train our model. It is observed that the performance
    of the supervised learning classifier algorithm will increase to a certain limit
    and will peak with the most optimal number of variables. But, using the same amount
    of training data and with an increased number of dimensions, there is a decrease
    in the performance of a supervised classification algorithm. In other words, it
    is not advisable to have the variables in a dataset if they are not contributing
    to the accuracy of the solution. And we should remove such variables from the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 Hughes phenomenon shows that the performance of a machine learning
    model will improve initially with increase in the number of dimensions. But with
    a further increase, it leads to a decrease in the model’s performance.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_02](images/03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Increase in number of dimensions has the following impacts on the machine learning
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: As the model is dealing with an increased number of variables, the mathematical
    complexity increases. For example, in the case of the k-means clustering method
    we discussed in the last chapter – when we have a greater number of variables,
    the distance calculation between respective points will become complex. And hence,
    the overall model becomes more complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset generated in a larger-dimensional space can be much sparser as compared
    to a smaller number of variables. The dataset will be sparser as some of the variables
    will have missing values, NULLs, etc. The space is hence much emptier, the dataset
    is less dense, and a smaller number of variables have values associated with them.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With increased complexity in the model, the processing time required increases.
    The system feels the pressure to deal with so many dimensions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And the overall solution becomes more complex to comprehend and execute. Recall
    from Chapter 1 where we have discussed supervised learning algorithms. Due to
    the high number of dimensions, we might face the problem of overfitting in supervised
    learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a supervised learning model has good accuracy on training data but lesser
    accuracy on unseen data, it is referred to as Overfitting. Overfitting is a nuisance
    as the very aim of machine learning models is to work well on unseen datasets
    and overfitting defeats this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Let us relate things to a real-world example. Consider an insurance company
    offering different types of insurance policies like life insurance, vehicle insurance,
    health insurance, home insurance, etc. The company wishes to leverage data science
    and execute clustering use cases to increase the customer base and the total number
    of policies sold. They have customer details like age, gender, profession, policy
    amount, historical transactions, number of policies held, annual income, type
    of policy, number of historical defaults, etc. At the same time, let us assume
    that variables like whether the customer is left-handed or right-handed, wears
    black or brown shoes, the shampoo brand used, the color of hair, and favorite
    restaurant are also captured. If we include all the variables in the dataset,
    the total number of variables in the resultant dataset will be quite high. The
    distance calculation will be more complex for a k-means clustering algorithm,
    the processing time will increase and the overall solution will be quite a complex
    one.
  prefs: []
  type: TYPE_NORMAL
- en: It is also imperative to note that *not* all the dimensions or variables are
    significant. Hence, it is vital to filter the important ones from all the variables
    we have. Remember, nature always prefers simpler solutions! In the case discussed
    above, it is highly likely that variables like the color of the hair and favorite
    restaurant, etc. will not impact the outputs. So, it is in our best interest to
    reduce the number of dimensions to ease the complexity and reduce the computation
    time. At the same time, it is also vital to note that dimensionality reduction
    is not always desired. It depends on the type of dataset and the business problem
    we wish to resolve. We will explore it more when we work on the case study in
    subsequent sections of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   Curse of dimensionality refers to having a big size of data. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   Having a high number of variables will always increase the accuracy of
    a solution. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   How a large number of variables in a dataset impact the model?
  prefs: []
  type: TYPE_NORMAL
- en: We have established that having a lot of dimensions is a challenge for us. We
    are now examining the various methods to reduce the number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Dimension reduction methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We studied the disadvantages of having really high dimensional data in the last
    section. A lesser number of dimensions might result in a simpler structure for
    our data, which will be computationally efficient. At the same time, we should
    be careful with reducing the number of variables. The output of the dimension
    reduction method should be complete enough to represent the original data and
    should not lead to any information loss. In other words, if originally, we had
    for example 500 variables and we reduced it to 120 significant ones, still these
    120 *should* be robust enough to capture *almost* all the information. Let us
    understand using a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this: We wish to predict the amount of rainfall a city will receive
    in the next month. The rainfall prediction for that city might be dependent on
    temperature over a period of time, wind speed measurements, pressure, distance
    from the sea, elevation above sea level etc. These variables make sense if we
    wish to predict rainfall. At the same time, variables like the number of cinema
    halls in the city, whether the city is the capital of the country or the number
    of red cars in the city might not impact the prediction of rainfall. In such a
    case, if we do not use the number of cinema halls in the city to predict the amount
    of rainfall, it will not reduce the capability of the system. The solution in
    all probability, will be still able to perform quite well. And hence, in such
    a case no information will be lost by dropping such a variable and surely, we
    can drop it from the dataset. On the other hand, removing variables such as temperature
    or distance from the ocean very likely will negatively affect the prediction.
    It is a very simple example to highlight the need to reduce the number of variables.'
  prefs: []
  type: TYPE_NORMAL
- en: The dimensions or the number of variables can be reduced by a combination of
    manual and algorithm-based methods. But before studying them in detail, there
    are a few mathematical terms and components which we should be aware of before
    proceeding ahead, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Mathematical foundation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are quite a few mathematical terms that one must grab to develop a thorough
    understanding of dimensionality reduction methods.
  prefs: []
  type: TYPE_NORMAL
- en: We are trying to reduce the number of dimensions of a dataset. A dataset is
    nothing but a matrix of values – hence a lot of the concepts are related to matrix
    manipulation methods, geometrical representation of them, and performing transformations
    on such matrices. The mathematical concepts are discussed in the Appendix Mathematical
    Foundation of the book. You would also require an understanding of eigenvalues
    and eigenvectors. These concepts will be reused throughout the book and hence
    they have been put in the Appendix for quick reference. You are advised to go
    through them before proceeding. We will now explore a few manual methods for dimensionality
    reduction methods and then proceed to algorithm-based ones.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Manual methods of dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To tackle the curse of dimensionality, we wish to reduce the number of variables
    in a dataset. The reduction can be done by removing the variables from the dataset.
    Or a very simple solution for dimensionality reduction can be combining the variables
    which can be grouped logically or can be represented using a common mathematical
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, as shown in Table 3.1 below, the data can be from a retail store
    where different customers have generated different transactions. We will get the
    sales, number of invoices and the number of items bought for each customer over
    a period of time. In the table below, customer 1 has generated two invoices, bought
    5 items in total, and generated a total sale of 100.
  prefs: []
  type: TYPE_NORMAL
- en: If we wish to reduce the number of variables, we might combine three variables
    as two variables. Here, we have introduced variables ATV (average transaction
    value) and ABS (average basket size) wherein ATV = Sales/Invoices and ABS = NumberOfItems/Invoices.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the second table for Customer 1, we have ATV as 50 and ABS as 2.5\. Hence,
    the number of variables has been reduced from three to two. The process here is
    only an example to show how we can combine various variables. It does not mean
    that we should replace sales with ATV as a variable.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 In the first table, we have the sales, invoices and number of items
    as the variables. In the second table, they have been combined to create new variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T05](images/03_T05.png)'
  prefs: []
  type: TYPE_IMG
- en: And this process can continue to reduce the number of variables. Similarly,
    for a telecom subscriber, we will have the minutes of mobile calls made during
    30 days in a month. We can add them to create a single variable – *minutes used
    in a month*. The above examples are very basic ones to start with. Using the manual
    process, we can employ two other commonly used methods – manual selection and
    using correlation coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Manual feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Continuing from the rainfall prediction example we discussed in the last section
    – a data scientist might be able to drop a few variables. This will be based on
    a deep understanding of the business problem at hand and the corresponding dataset
    being used. However, it is an underlying assumption that the dataset is quite
    comprehensible for the data scientist and they understand the business domain
    well. Most of the time, the business stakeholders will be able to guide on such
    methods. It is also necessary that the variables are unique and not much of a
    dependency exists.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Table 3.2 below, we can remove a few of the variables which might
    not be useful for predicting rainfall.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 In the first table, we have all the variables present in the dataset.
    Using business logic, some of the variables which might be not much use have been
    discarded in the second table. But this is to be done with due caution. And the
    best way is to get guidance from the business stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T06](images/03_T06.png)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes, feature selection methods are also referred to as *wrapper methods*.
    Here, a machine learning model is wrapped or fitted with a subset of variables.
    In each iteration, we will get a different set of results. The set which generates
    the best results is selected for the final model.
  prefs: []
  type: TYPE_NORMAL
- en: The next methods are based on the correlation existing between various attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Correlation between two variables simply means that have a mutual relationship
    with each other. The change in the value of one variable will impact the value
    of another, which means that data points with similar values in one variable have
    similar values for the other variable. The variables which are highly correlated
    with each other are supplying similar information and hence one of them can be
    dropped.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation is described in detail in the Appendix Mathematical Foundation of
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for a retail store, the number of invoices generated in a day will
    be highly correlated with the amount of sales generated and hence, one of them
    can be dropped. Another example can be – students who study for a higher number
    of hours will have better grades than the ones who study less (mostly!).
  prefs: []
  type: TYPE_NORMAL
- en: But we should be careful in dropping the variables and should not trust correlation
    alone. The business context of a variable should be thoroughly understood before
    taking any decision.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good idea to discuss with the business stakeholders before dropping
    any variables from the study.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation-based methods are sometimes called *filter methods*. Using correlation
    coefficients, we can filter and choose the variables which are most significant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   We can drop a variable simply if we feel is not required. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   If two variables are correlated, ALWAYS drop one of them. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: Manual methods are easier solutions and can be executed quite efficiently. The
    dataset size is reduced and we can proceed ahead with the analysis. But manual
    methods are sometimes subjective and depend a lot on the business problem at hand.
    Many times, it is also not possible to employ manual methods for dimension reduction.
    In such situations, we have algorithm-based methods which we are studying in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Algorithm-based methods for reducing dimensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We examined manual methods in the last section. Continuing from there, we will
    examine algorithm-based methods in this section. The algorithm-based techniques
    are based on a more mathematical base and hence prove to be more scientific methods.
    In real-world business problems, we use a combination of both manual and algorithm-based
    techniques. Manual methods are straightforward to execute as compared to algorithm-based
    techniques. Also, we cannot comment on the comparison of both techniques, as they
    are based on different foundations. But at the same time, it is imperative that
    you put due diligence in the implementation of algorithm-based techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The major techniques used in dimensionality reductions are listed below. We
    will be exploring most of them in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Singular Value Decomposition (SVD)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis (LDA)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generalized Discriminant Analysis (GDA)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-negative matrix factorization (NMF)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multi-dimension scaling (MDS)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locally Linear Embeddings (LLE)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IsoMaps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: t-SNE (T-distributed Stochastic Neighbor Embedding)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These techniques are utilized for the common end goal – transform the data from
    a high-dimensional space to a low-dimensional one. Some of the data transformations
    are linear in nature, while some are non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to discuss Principal Component Analysis (PCA) and Singular Value
    Decomposition (SVD) in detail in this chapter. In the later chapters of the book,
    other major techniques are being explored. Perhaps, PCA is the most quoted dimensionality
    reduction method which we are exploring in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Principal Component Analysis (PCA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider this: You are working on a dataset that has 250 variables. It is almost
    impossible to visualize such a high-dimensional space. Some of the 250 variables
    might be correlated with each other, some of them might not be and there is a
    need to reduce the number of variables without losing much information. Principal
    Component Analysis or PCA allows us to mathematically select the most important
    features and leave the rest. PCA does reduce the number of dimensions but also
    preserves the most important relationships between the variables and the important
    structures in the dataset. Hence, the number of variables is reduced but the important
    information in the dataset is kept safe.'
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a projection of high-dimensional data in lower dimensions. In simpler
    terms, we are reducing a n-dimensional space into an m-dimensional one where n
    > m while maintaining the nature and the essence of the original dataset. In the
    process, the old variables are reduced to newer ones, while maintaining the crux
    of the original dataset The new variables thus created are called *Principal Components*.
    The principal components are a linear combination of the raw variables. As a result
    of this transformation, the first principal component captures the maximum randomness
    or the highest variance in the dataset. The second principal component created
    is orthogonal to the first component.
  prefs: []
  type: TYPE_NORMAL
- en: If two straight lines are orthogonal to each other, it means they are at an
    angle of 90⁰ to each other,
  prefs: []
  type: TYPE_NORMAL
- en: And the process continues to the third component and so on. Orthogonality allows
    us to maintain that there is no correlation between subsequent principal components.
  prefs: []
  type: TYPE_NORMAL
- en: PCA utilizes linear transformation of the dataset and such methods are sometimes
    referred to as feature projections. The resultant dataset or the projection is
    used for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand better by means of an example. In (Table 3.3) shown below,
    we have represented the total perceived value of a home using some variables.
    The variables are area (sq m), number of bedrooms, number of balconies, distance
    from the airport, distance from the train station, and so on – we have 100+ variables.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.3 The variables based on which a price of a house can be estimated
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T07](images/03_T07.png)'
  prefs: []
  type: TYPE_IMG
- en: We can combine some of the variables mathematically and logically. PCA will
    create a new variable which is a linear combination of some of the variables as
    shown in the example below. It will get the best *linear* combination of original
    variables so that the new_variable is able to capture the maximum variance of
    the dataset. The Equation 3.1 is only an example shown for illustration purposes
    wherein we are showing a new_variable created by a combination of other variables.
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 3.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: new_variable = a*area – b*bedrooms + c*distance – d*schools
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s understand the concept visually. In a vector-space diagram, we can
    represent the dataset as shown in Figure 3.3 below. The first figure represents
    the raw data where we can visualize the variables in an x-y diagram. As discussed
    above, we wish to create a linear combination of variables. Or in other words,
    we wish to create a mathematical equation that will be able to explain the relationship
    between x and y.
  prefs: []
  type: TYPE_NORMAL
- en: The output of such a process will be a straight line as shown in the second
    figure in Figure 3.3\. This straight line is sometimes referred to as the *Line
    of Best fit.* Using this line of best fit, we can predict a value of y for a given
    value of x. These predictions are nothing but the projections of data points on
    a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the actual value and the projections is the error as
    shown in the third figure in Figure 3.3 below. The total sum of these errors is
    called the total projection error.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 (i) The dataset can be represented in a vector-space diagram (ii)
    The straight line can be called as the line of best fit having the projections
    of all the data points on it. (iii) The difference between the actual value and
    the projections are the error terms.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_04](images/03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: There can be multiple options for this straight line as shown in Figure 3.4
    below. These different straight lines will have different errors and different
    values of variances captured.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 The data set can be captured by a number of lines, but not all the
    straight lines will be able to capture the maximum variance. The equation which
    gives the minimum error will be the chosen one.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_05](images/03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: The straight line which is able to capture the maximum variance will be chosen
    one. Or in other words, it is giving the minimum error. It will be the *first
    principal component* and the direction of maximum spread will be the *principal
    axis*.
  prefs: []
  type: TYPE_NORMAL
- en: The second principal component will be derived in a similar fashion. Since we
    know the first principal axis, we can subtract the variance along this principal
    axis from the total variance to get the residual variance. Or in other words,
    using the first principal component we would capture some variance in the dataset.
    But there will be a portion of the total variance in the dataset which is still
    unexplained by the first principal component. The portion of the total variance
    unexplained is the residual variance. Using the second principal component, we
    wish to capture as much variance as we can.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same process to capture the direction of maximum variance, we will
    get the second principal component. The second principal component can be at a
    number of angles with respect to the first one as shown in Figure 3.5\. It is
    mathematically proven that if the second principal component is orthogonal to
    the first principal component, it allows us to capture the maximum variance using
    the two principal components. In Figure 3.5 we can observe that the two principal
    components are at an angle of 90⁰ with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 (i) The first figure on the left is the first principal component.
    (ii) The second principal component can be at different angles with respect to
    the first principal component. We have to find the second principal which allows
    to capture the maximum variance (iii) To capture the maximum variance, the second
    principal component should be orthogonal to the first one and hence the combined
    variance captured in maximized.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_06](images/03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: And the process continues for the third, fourth principal component, and so
    on. With more principal components, the representation in a vector-space becomes
    difficult to visualize. You can think of a vector space diagram with more than
    three axes. Once all the principal components are derived, the dataset is projected
    onto these axes. The columns in this transformed dataset are the *principal components*.
    The principal components created will be lesser than the number of original variables
    and capture maximum information present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we examine the process of PCA in-depth, let’s study its important characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA aims to reduce the number of dimensions in the resultant dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA produces principal components which aim to reduce the noise in the dataset
    by maximizing the feature variance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, the principal components reduce the redundancy in the dataset.
    It is achieved by minimizing the covariance between the pairs of features.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The original variables no longer exist in the newly created dataset. Instead,
    new variables are created using these variables.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not necessary that the principal components will map one-to-one with all
    the variables present in the dataset. They are a new combination of the existing
    variables. And hence, they can be a combination of several different variables
    in one principal component (as shown in Equation 3.1).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The new features created from the dataset do not share the same column names.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The original variables might be correlated with each other, but the newly created
    variables are unrelated with each other.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of newly created variables is lesser than the original number of
    variables. We process to select the number of principal components has been described
    in the Python implementation section. After all, that is the whole purpose of
    dimensionality reduction.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If PCA has been used for reducing the number of variables in a training dataset,
    the testing/validation datasets have to be reduced by using PCA.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA is not synonymous with dimensionality reduction. It can be put into use
    for a number of other usages too. It is generally a PCA only for dimensionality
    reduction will be a misnomer for sure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now examine the approach used while implementing PCA and then we will
    develop a Python solution using PCA. Though we need not apply all the steps while
    we develop the codes, as the heavy lifting has already been done by the packages
    and libraries. The steps given below are hence taken care of by the packages,
    but still, it is imperative that you understand these steps to properly appreciate
    how PCA works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps followed in PCA are:'
  prefs: []
  type: TYPE_NORMAL
- en: In PCA, we start with **normalizing our dataset** as a first step. It ensures
    that all our variables have a common representation and become comparable. We
    have methods to perform the normalization in Python, which we will study when
    we develop the code. To explore more on normalizing the dataset, you can refer
    to the Appendix Mathematical Foundation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Get the covariance in** the normalized dataset. It allows us to study the
    relationship between the variables. We generally create a covariance matrix as
    shown in the Python example in the next section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then calculate the eigenvectors and eigenvalues of the covariance matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then sort the eigenvalues in decreasing order of eigenvalues. The eigenvectors
    corresponding to the maximum value of eigenvalues are chosen. The components hence
    chosen will be able to capture the maximum variance in the dataset. There are
    other methods to shortlist the principal components which we will explore while
    we develop the Python code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   PCA will result in the same number of variables in the dataset. TRUE or
    FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   PCA will be able to capture 100% information in the dataset. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   What is the logic of selecting principal components in PCA?
  prefs: []
  type: TYPE_NORMAL
- en: So, in essence, principal components are the linear combinations of the original
    variables. The weight in this linear combination is the eigenvector satisfying
    the error criteria of the least square method. We are studying Eigenvalue decomposition
    now and SVD is covered in the next section (3.6).
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 Eigenvalue Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We studied PCA in the last section where we said that principal components are
    the linear combination of the original variables. We will explore on the eigenvalue
    decomposition for PCA now.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of PCA, the eigenvector will represent the direction of the vector
    and the eigenvalue will be the variance that is captured along that eigenvector.
    It can be shown by means of Figure 3.6 below, where we are breaking the original
    nxn matrix into components.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 Using Eigenvalue decomposition, the original matrix can be broken
    into eigenvector matrix, eigenvalue matrix and an inverse of eigenvector matrix.
    We implement PCA using this methodology.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_07](images/03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Mathematically, we can show the relation by Equation 3.2
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 3.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A*v = λ*v
  prefs: []
  type: TYPE_NORMAL
- en: where A is a square matrix, v is the eigenvector and λ is the eigenvalue. Here,
    it is important to note that Eigenvector matrix is the orthonormal matrix and
    its columns are eigenvectors. Eigenvalue matrix is the diagonal matrix and its
    eigenvalues are the diagonal elements. The last component is the inverse of the
    eigenvector matrix. Once we have the eigenvalues and the eigenvectors, we can
    choose the significant eigenvectors for getting the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: We are presenting PCA and SVD as two separate methods in this book. Both of
    the methods are used to reduce high-dimensional data into fewer ones, and in the
    process retain the maximum information in the dataset. The difference between
    the two is – SVD exists for any sort of matrix (rectangular or square), whereas
    the eigen decomposition is possible only for square matrices. You will understand
    it better once we have covered SVD later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will now create a Python solution using eigenvalue decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Python solution using PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have studied the concepts of PCA and the process using eigenvalue decomposition.
    It is time for us to dive into Python and develop a PCA solution on a dataset.
    We will show you how to create eigenvectors and eigenvalues on the dataset. To
    implement the PCA algorithms, we will use the `sklearn` library. Libraries and
    packages provide a faster solution for implementing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We are using the Iris dataset for this problem. It is one of the most popular
    datasets used for machine learning problems. The dataset contains data of three
    iris species with 50 samples each and having properties of each flower – like
    petal length, sepal length etc. The objective of the problem is to predict the
    species using the properties of the flower. The independent variables hence are
    the flower properties whereas the variable “Species” is the target variable. The
    dataset and the code are checked-in at the GitHub repository. Here, we are using
    the inbuilt PCA functions which reduce the effort required to implement PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** Load all the necessary libraries first. We are going to use numpy,
    pandas, seaborn, matplotlib and sklearn. Note that we have imported PCA from sklearn.'
  prefs: []
  type: TYPE_NORMAL
- en: These are the most standard libraries. You will find that almost all the machine
    learning solutions would import these libraries in the solution notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2:** Load the dataset now. It is a .csv file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3:** We will now perform a basic check on the dataset – looking at the
    first five rows, shape of the data, spread of the variables etc. We are not performing
    an extensive EDA here as the steps are covered in Chapter 2\. The dataset has
    150 rows and 6 columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07a](images/03_07a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07b](images/03_07b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4:** Here we have to break the dataset into independent variables and
    target variable. X_variables here represent the independent variables which are
    in the first 4 columns of the dataset while y_variable is the target variable
    which is Species in this case and is the final column in the dataset. Recall,
    we wish to predict the Species of a flower using the other properties. Hence we
    have separated the target variable Species and other independent variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5:** We are now normalizing our dataset. The inbuilt method of StandardScalar()
    does the job for us quite easily.'
  prefs: []
  type: TYPE_NORMAL
- en: StandardScalar method normalizes the dataset for us. It subtracts the mean from
    the variable and divided by the standard deviation. For more details on normalization,
    refer to the Appendix Mathematical Foundation.
  prefs: []
  type: TYPE_NORMAL
- en: We invoke the method and then use it on our dataset to get the transformed dataset.
    Since, we are working on independent variables, we are using X_variables here.
    First we invoke the StandardScalar() method. And then we use fit_transform method.
    The fit_transform method first fits the transformers to X and Y and then returns
    a transformed version of X.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6:** We will now calculate the covariance matrix. And print it, the
    output is shown below. Getting the covariance matrix is straightforward using
    numpy.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07c](images/03_07c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 7:** Now in this step the eigenvalues are being calculated. Inside numpy
    library we have the inbuilt functionality to calculate the eigenvalues. We will
    then sort the eigenvalues in descending order. To shortlist the principal components,
    we can choose eigenvalues greater than 1\. This criterion is called *Kaiser criteria.*
    We are exploring other methods too.'
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalue represents how much a component is good as a summary of the data.
    If the eigenvalue is 1, it means that the component contains the same amount of
    information as a single variable. And hence we choose the eigenvalue which is
    greater than 1.
  prefs: []
  type: TYPE_NORMAL
- en: In this code, first we are getting the `eigen_values` and `eigen_vectors`. And
    then we are arranging then in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07d](images/03_07d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 8:** We will now invoke the PCA method from the `sklearn` library. The
    method is used to fit the data here. To be noted is, we have not yet determined
    the number of principal components we wish to use in this problem yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 9:** The principal components are now set. Let’s have a look at the
    variance explained by them. We can observe that the first component captures 72.77%
    variation, second captures 23.03% variation and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07e](images/03_07e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 10:** We are now plotting the components in a bar plot for better visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07f](images/03_07f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 11:** We are drawing a scree-plot to visualize the cumulative variance
    being explained by the principal components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07g](images/03_07g.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 12:** In this case study, if we choose the top two principal components
    as the final solutions as these two capture 95.08% of the total variance in the
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 13:** We will now plot the dataset with respect to two principal components.
    For that, it is a requirement that Species are tied-back to the actual values
    of the Species variable which are Iris-setosa, Iris-versicolor and Iris-virginica.
    Here, 0 is mapped to Iris-setosa, 1 is Iris-versicolor and 2 is Iris-virginica.
    In the code below, first the Species variable gets its values replaced by using
    the mapping discussed above.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 14:** We will now plot the results with respect to two principal components.
    The plot is showing the dataset reduced to two principal components we have just
    created. These principal components are able to capture 95.08% variance of the
    dataset. The first principal component represents the x-axis in the plot while
    the second principal component represents the y-axis in the plot. The color represents
    the various classes of Species.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![03_07h](images/03_07h.png)'
  prefs: []
  type: TYPE_IMG
- en: The above solution has reduced the number of components from four to 2 and still
    is able to retain most of the information. Here, we have examined three approaches
    to select the principal components – based on Kaiser criteria, the variance captured,
    and the scree plot.
  prefs: []
  type: TYPE_NORMAL
- en: Let us quickly analyze what we have achieved using PCA. Figure 3.7 shows two
    representations of the same dataset. The one on the left is the original dataset
    of X_variables. It has four variables and 150 rows. The right is the output of
    PCA. It has 150 rows but only two variables. Recall, we have reduced the number
    of dimensions from four to two. So the number of observations has remained as
    150 only while the number of variables has reduced from four to two.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 The figure on the left shows the original dataset which has 150 rows
    and 4 variables. After the implementation of PCA, the number of variables has
    been reduced to two. The number of rows remain the same as 150 which is shown
    by the length of pca_2d.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_08](images/03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Once we have reduced the number of components, we can continue to implement
    a supervised learning or an unsupervised learning solution. We can implement the
    above solution for any of the other real-world problems where we aim to reduce
    the number of dimensions. You will be exploring more on it in the case study section.
  prefs: []
  type: TYPE_NORMAL
- en: With this we have covered PCA. The Github repo contains a very interesting PCA
    decomposition with variables and corresponding plot. We will now explore Singular
    Value Decomposition (SVD) in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Singular Value Decomposition (SVD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last section we studied PCA. PCA transforms the data linearly and generate
    principal components which are not correlated with each other. But the process
    followed of eigenvalue decomposition can only be applied to *square matrices*.
    Whereas SVD can be implemented to any m x n matrix. We will study this in more
    detail now.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider we have a matrix A. The shape of A is m x n or it contains m
    rows and n columns. The transpose of A can be represented as A^T.
  prefs: []
  type: TYPE_NORMAL
- en: We can create two other matrices using A and A^T as *A A*^T and *A*^T*A*. These
    resultant matrices *A A*^T and *A*^T*A* have some special properties which are
    listed below. The mathematical proof of the properties is beyond the scope of
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The properties of *A A*^T and *A*^T*A* are:'
  prefs: []
  type: TYPE_NORMAL
- en: They are symmetric and square matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their eigenvalues are either positive or zero.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both A A^T and A^TA are having the same eigenvalue.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both A A^T and A^TA are having same rank as the original matrix A.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvectors of *A A*^T and *A*^T*A* are referred to as **singular vectors
    of A**. The square root of their eigenvalues are called **singular values**.
  prefs: []
  type: TYPE_NORMAL
- en: Since both of these matrices (A A^T and A^TA) are symmetrical, their eigenvectors
    are orthonormal to each other. In other words, being symmetrical - the eigenvectors
    are perpendicular to each other and can be of unit length.
  prefs: []
  type: TYPE_NORMAL
- en: Now, with this mathematical understanding, we can define SVD. As per the Singular
    Value Decomposition method, it is possible to factorize any matrix A as
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 3.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A = U * S * V^T
  prefs: []
  type: TYPE_NORMAL
- en: Here, A is the original matrix,
  prefs: []
  type: TYPE_NORMAL
- en: U and V are the orthogonal matrices with orthonormal eigenvectors taken from
    A A^T and A^TA respectively and
  prefs: []
  type: TYPE_NORMAL
- en: S is the diagonal matrix with r elements equal to the singular values.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, SVD can be said as an enhancement of the PCA methodology using
    eigenvalue decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Singular values are better and numerically more robust than eigenvalues decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: PCA was defined as the linear transformation of input variables using principal
    components. All those concepts of linear transformation, choosing the best components
    etc. remain the same. The major process steps remain similar, except in SVD we
    are using a slightly different approach wherein the eigenvalue decomposition has
    been replaced with using singular vectors and singular values. It is often advisable
    to use SVD when we have a sparse dataset, in the case of denser dataset PCA can
    be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  prefs: []
  type: TYPE_NORMAL
- en: 1.   SVD works on eigenvalue decomposition technique. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 2.   PCA is a much more robust methodology than SVD. TRUE or FALSE.
  prefs: []
  type: TYPE_NORMAL
- en: 3.   What are singular values and singular vectors in SVD?
  prefs: []
  type: TYPE_NORMAL
- en: We will now create a Python solution using SVD in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1 Python solution using SVD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case study, we are using the *mushrooms* dataset. This dataset contains
    descriptions of 23 species of grilled mushrooms. There are two classes – either
    the mushroom is “e” which means it is edible else the mushroom is “p” meaning
    it is poisonous.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** Import the libraries. We are importing'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2:** Import the dataset and check for shape, head etc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![03_08a](images/03_08a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 3:** As we can observe, the values are categorical in nature in the
    dataset. They have to be first encoded into numeric values. This is not the only
    approach we deal with categorical variables. There are other techniques too which
    we will explore throughout the book. We will dive deep into the techniques in
    the last chapter of the book.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we are invoking the LabelEncoder and then apply it to all the columns
    in the dataset. The LabelEncoder converts the categorical variables into numeric
    ones using one-hot encoding method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4:** Have a re-look at the dataset. All the categorical values have
    been converted to numeric ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![03_08b](images/03_08b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 5:** The next two steps are same as the last case study wherein we break
    the dataset into X_variables and y_label. And then the dataset is normalized.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6:** In this step, we implement the SVD. There is a method in numpy
    that implements SVD. The output is u, s and v, where u and v are the singular
    vectors and s is the singular value. If you wish you can analyze their respective
    shapes and dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7:** We know that singular values allow us to compute variance explained
    by each of the singular vectors. We will now analyze the % variance explained
    by each singular vector and plot it. The results are shown to three places of
    decimal. And then we are plotting the results as a histogram plot. On the x-axis
    we have the singular vectors while on the y-axis we have the % of variance explained.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![03_08c](images/03_08c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 8:** We will now create a data frame. This new data frame svd_df contains
    the first two singular vectors and the metadata. We are then printing the first
    5 rows using the head command.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![03_08d](images/03_08d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 9:** Similar to the last case study, we are replacing numeric values
    with actual class labels. 1 is edible while 0 is Poisonous.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10:** We are now plotting the variance explained by the two components.
    Here we have chosen only the first two components. You are advised to take the
    optimum number of components using the methods described in the last section and
    plot the respective scatter plots. Here, on the x-axis we have shown the first
    singular vector SV1 and on the y-axis we have shown the second singular vector
    SV2.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![03_08e](images/03_08e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe the distribution of the two classes with respect to the two components.
    The two classes – edible and poison are color coded as black and red respectively.
    As we have noted above, we have chosen only two components to show the impact
    using a visualization plot. You are advised to choose the optimum number of components
    using the methods described in the last case study and then visualize the results
    using different singular vectors. This solution can be used to reduce dimensions
    in a real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on SVD. We will now observe the positives and
    challenges with PCA and SVD in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Pros and cons of dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the initial sections of the chapter, we discussed the drawbacks the of curse
    of dimensionality. In the last few sections, we discovered PCA and SVD and implemented
    using Python. In the current section, we will examine the advantages and challenges
    we have with these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major advantages we get with implementing PCA or SVD are as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduced number of dimensions** lead to less complexity in the dataset. The
    correlated features are removed and are transformed. Treating correlated variables
    manually is a tough task which is quite manual and frustrating. Techniques like
    PCA and SVD do that job for us quite easily. The number of correlated features
    is minimized, and overall dimensions are reduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization of the dataset** is better if the number of dimensions is lesser.
    It is very difficult to visualize and depict a very-high dimensional dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy** of the machine learning model is improved if the correlated variables
    are removed. These variables do not add anything to the performance of the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The training time is reduced** as the dataset is less complex. Hence, lesser
    computation power is required, and lesser time is required.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting** is a nuisance in supervised machine learning models. It is
    a condition where the model is behaving very well on the training dataset but
    not so well on the testing/validation dataset. It means that the model may not
    be able to perform well on real-world unseen datasets. And it beats the entire
    purpose of building the machine learning model. PCA/SVD helps in tackling overfitting
    by reducing the number of variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the same time, there are a few challenges we face with dimensionality reduction
    techniques which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The new components created by PCA/SVD are **less interpretable**. They are the
    combination of the independent variables in the dataset and do not actually relate
    to the real-world, hence it can be difficult to relate them to the real-world
    scenario.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numeric variables** are required for PCA/SVD. And hence all the categorical
    variables have to be represented in numeric form.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalization/standardization** of the dataset is required to be done before
    the solution can be implemented.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There might be **information loss** when we use PCA or SVD. The principal components
    *cannot* replace the original dataset and hence there might be some loss of information
    while we implement these methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But despite a few challenges, PCA and SVD are used for reducing the dimensions
    in the dataset. They are one of the most popular methods and quite heavily used.
    At the same time, it is imperative to note that these are linear methods and we
    will be covering non-linear methods in the later part of the book.
  prefs: []
  type: TYPE_NORMAL
- en: With this we have covered the two important techniques used in dimensionality
    reduction. We will examine more advanced techniques in the later chapters. It
    is time to move to the case study which is the next section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Case study for dimension reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now explore a real-world case to relate the usage of PCA and SVD in
    real-world business scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this: You are working for a telecommunication service provider. You
    have a subscriber base, and you wish to cluster the consumers over a number of
    parameters. But the challenge can be the huge number of dimensions available to
    be analyzed.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective will be to reduce the number of attributes using dimension reduction
    algorithms. The consumer dataset can look like below.
  prefs: []
  type: TYPE_NORMAL
- en: Demographic details of the subscriber will consist of age, gender, occupation,
    household size, marital status etc. The list shown below is not exhaustive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Table 3.4 Demographic details of a subscriber like age, gender, marital status,
    household size, City etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T08](images/03_T08.png)'
  prefs: []
  type: TYPE_IMG
- en: Subscription details of the consumer might look like the table below. The list
    shown below is not exhaustive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Table 3.5 Subscription details of a subscriber like tenure, postpaid/prepaid
    connection etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T09](images/03_T09.png)'
  prefs: []
  type: TYPE_IMG
- en: The usage of the consumer will describe the minutes, call rates, data usages,
    services etc. The list shown below is not exhaustive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Table 3.6 Usage of a subscriber specifies the number of minutes used, SMS sent,
    data used, days spent in a network, national or international usage etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T10](images/03_T10.png)'
  prefs: []
  type: TYPE_IMG
- en: Payment and transaction details of the subscribers will talk about the various
    transactions made, the mode of payment, frequency of payments, days since last
    payment made etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Table 3.7 Transaction details of a subscriber showing all the details of amount,
    mode etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T11](images/03_T11.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataset can have many more attributes. So far, we have established that
    the number of variables involved are indeed high. Once we join all these datapoints,
    the number of dimensions in the final data can be very huge.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.8 The final dataset is a combination of all the above-mentioned datasets.
    It will be a big, really high-dimensional dataset which is to be analyzed
  prefs: []
  type: TYPE_NORMAL
- en: '![03_T12](images/03_T12.png)'
  prefs: []
  type: TYPE_IMG
- en: We have to reduce the number of attributes before we can proceed to any supervised
    or unsupervised solution. In this chapter, we are focusing on dimensionality reduction
    techniques and hence the steps are covering that aspect of the process. In the
    later chapters, we will examine the exploratory analysis in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we will perform a sanity check of the dataset and do the data
    cleaning. We will examine the number of data points, number of missing values,
    duplicates, junk values present etc. This will allow us to delete any variables
    which might be very sparse or contain not much information. For example, if the
    gender is available for only 0.01% of the customer base it might be a good idea
    to drop the variable. Or if all the customers have gender as male, the variable
    is not adding any new information to us and hence it can be discarded. Sometimes,
    using business logic a variable might be dropped from the dataset. An example
    has been discussed in the earlier sections. In this step, we might combine a few
    variables. For example, we might create a new variable as average transaction
    value by dividing total amount spent by total number of transactions. In this
    way, we will be able to reduce a few dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: A Python Jupyter notebook is available at the Github repository, wherein we
    have given a very detailed solution for data cleaning step.
  prefs: []
  type: TYPE_NORMAL
- en: Once we are done with the basic cleaning of the data, we start with the exploratory
    data analysis. As a part of exploratory analysis, we examine the spread of the
    variable, its distribution, mean/median/mode of numeric variables. This is sometimes
    referred to as *univariate analysis.* This step allows us to measure the spread
    of the variables, understand the central tendencies, examine the distribution
    of different classes for categorical variables and look for any anomalies in the
    values. For example, using the dataset mentioned above we will be interested to
    analyze the maximum/minimum/average data usage or the % distribution of gender
    or age. We would want to know the most popular method to make a transaction and
    we would also be interested to know maximum/minimum/average amount of the transactions.
    And this list goes on.
  prefs: []
  type: TYPE_NORMAL
- en: Then we explore the relationships between variables which is referred to as
    *bivariate analysis*. Crosstabs, distribution of data is a part of bivariate analysis.
    A correlation matrix is created during this step. Variables that are highly correlated
    are examined thoroughly. And based on business logic, one of them might be dropped.
    This step is useful to visualize and understand the behavior of one variable in
    the presence of other variables. We can examine their mutual relationships and
    the respective strength of the relationships. In this case study, we would answer
    the questions such as– do subscribers who use more data spend more time on network
    as compared to subscribers who send more SMS. Or hypothesis like – do the subscribers
    who make a transaction using online mode generate more revenue than the ones using
    cash. Or is there a relationship between gender/age with the data usage. Many
    such questions are answered during this phase of the project.
  prefs: []
  type: TYPE_NORMAL
- en: A Python Jupyter notebook is available at the Github repository, which provides
    detailed steps and code for the univariate and bivariate phase. Check it out!
  prefs: []
  type: TYPE_NORMAL
- en: At this step, we have a dataset which has a huge number of dimensions and we
    want to reduce the number of dimensions. Now it is good time to implement PCA
    or SVD. The techniques will reduce the number of dimensions and will make the
    dataset ready for the next steps in the process, as shown in Figure 3.8\. The
    figure is only representative in nature to depict the impact of dimensionality
    reduction methods. Notice how the large number of black lines in the left figure
    are getting reduced to lesser number of red lines in the right figure.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 A very high dimensional dataset will be reduced to a low dimensional
    one by using principal components which capture the maximum variance in the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![03_09](images/03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of dimensionality reduction methods will be a dataset with a lower
    number of variables. The dataset can be then used for supervised or unsupervised
    learning. We have already looked at the examples using Python in the earlier sections
    of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our case study on telecom subscribers. The case can be extended
    to any other domain like retail, BFSI, aviation, healthcare, manufacturing, etc.
  prefs: []
  type: TYPE_NORMAL
- en: We will now proceed to the summary of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.9 Closing Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is everywhere, in various forms, levels, dimensions, and with varying levels
    of complexity. It is often mentioned that “the more data the better. It is indeed
    true to a certain extent. But with a really high number of dimensions, it becomes
    quite a herculean task to make sense out of it. The analysis can become biased
    and really complex to deal with. We explored this curse of dimensionality in this
    third chapter. PCA/SVD are helpful to reduce this complexity. They make the dataset
    ready for the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: But dimensionality reduction is not as straightforward as it looks. It is not
    an easy task. But it is certainly a very rewarding one. And requires a combination
    of business acumen, logic, and common sense to deal with. The resultant dataset
    might still require some additional work. But it is a very good point for building
    a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: This marks the end of the third chapter. It also ends the part one of the book.
    In this part, we have covered the more basic algorithms. We started with the first
    chapter of the book, where we explored the fundamentals and basics of machine
    learning. In the second chapter we examined three algorithms for clustering. In
    this third chapter, we explored PCA and SVD.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the book, we are changing gears and studying more advanced
    topics. We are starting with association rules in the next chapter. Then we go
    into advanced clustering methods of time-series clustering, fuzzy clustering,
    GMM clustering, etc. It is followed by a chapter on advanced dimensionality reduction
    algorithms like t-SNE, LDA. And then to conclude the second part, we are examining
    unsupervised learning on text datasets. The third part of the book is even more
    advanced where we dive into neural network-based solutions and use image datasets.
    So still a long way to go! Stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: You can proceed to the question section now!
  prefs: []
  type: TYPE_NORMAL
- en: Practical next steps and suggested readings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use the vehicles dataset used in the last chapter for clustering and implement
    PCA and SVD on it. Compare the performance on clustering before and after implementing
    PCA and SVD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the datasets from the ([https://data.world/datasets/pca](datasets.html)).
    Here, you will find a lot of datasets like Federal plan cyber security, Pizza
    dataset etc. Compare the performance of PCA and SVD on these datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through the following papers on PCA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.sciencedirect.com/science/article/pii/009830049390090R](pii.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://web.stanford.edu/~hastie/Papers/spc_jcgs.pdf](Papers.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://web.cs.ucdavis.edu/~vemuri/papers/pcaVisualization.pdf](papers.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://cseweb.ucsd.edu/~ravir/papers/pca/pamifinal.pdf](pca.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through the following research papers on SVD
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://people.maths.ox.ac.uk/porterm/papers/s4.pdf](papers.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://papers.nips.cc/paper/3473-quic-svd-fast-svd-using-cosine-trees.pdf](paper.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1211.7102.pdf](pdf.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://glaros.dtc.umn.edu/gkhome/fetch/papers/sarwar_SVD.pdf](papers.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.10 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We understood that having a lot of dimensions in a data set gives rise to a
    problem known as the Curse of dimensionality. Because of the curse of dimensionality,
    the dataset becomes very complex to process and hence the processing time also
    increases a lot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also mentioned that there can be a number of techniques to tackle the problem
    of curse of dimensionality like PCA, LDA, SVD, Autoencoders, t-SNE, Isomaps etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We covered Principal Component Analysis (PCA) in detail where we studied that
    a principal component is a linear combination of various variables. Using this
    methodology, the total number of dimensions are reduced by having principal components
    which capture the maximum variance in the dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We then moved to Singular Value Decomposition where most of the process is the
    same as PCA except the eigenvalue decomposition in PCA has been replaced with
    using singular vectors and singular values in SVD.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We also studied that when we get the principal components, though we solve the
    problem of the curse of dimensionality, the original variables are lost.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we had the Python implementation of the techniques by using sklearn
    library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
