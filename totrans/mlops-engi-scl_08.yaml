- en: '6 Core PyTorch: Autograd, optimizers, and utilities'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 PyTorch 核心：Autograd、优化器和实用工具
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容如下
- en: Understanding automatic differentiation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自动微分
- en: Using automatic differentiation with PyTorch tensors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 张量进行自动微分
- en: Getting started with PyTorch SGD and Adam optimizers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 PyTorch SGD 和 Adam 优化器
- en: Applying PyTorch to linear regression with gradient descent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现带有梯度下降的线性回归
- en: Using data set batches for gradient descent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据集批次进行梯度下降
- en: PyTorch Dataset and DataLoader utility classes for batches
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 数据集和 DataLoader 工具类用于批量处理
- en: In chapter 5, you learned about the tensor, a core PyTorch data structure for
    n-dimensional arrays. The chapter illustrated the significant performance advantages
    of PyTorch tensors over native Python data structures for arrays and introduced
    PyTorch APIs for creating tensors as well as performing common operations on one
    or more tensors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章中，您学习了张量（tensor），这是 PyTorch 的核心数据结构，用于表示 n 维数组。该章节展示了 PyTorch 张量相对于原生
    Python 数据结构的数组的显著性能优势，并介绍了创建张量以及在一个或多个张量上执行常见操作的 PyTorch API。
- en: 'This chapter teaches another key feature of the PyTorch tensors: support for
    calculation of gradients using *automatic differentiation* (autodiff). Described
    as one of the major advances in scientific computing since 1970, autodiff is suprisingly
    simple and was invented by Seppo Linnainmaa, a master’s student at the University
    of Helsinki.[¹](#pgfId-1011886) The first part of this chapter introduces you
    to the fundamentals of autodiff by showing how you can implement the core algorithm
    for a scalar tensor using basic Python.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 PyTorch 张量的另一个关键特性：支持使用 *自动微分*（autodiff）进行梯度计算。自动微分被描述为自 1970 年以来科学计算中的一项重大进展，它出人意料地简单，由赫尔辛基大学的硕士研究生
    Seppo Linnainmaa 发明。本章的第一部分通过展示如何使用基本的 Python 实现标量张量的核心算法来向您介绍自动微分的基础知识。
- en: The remainder of this chapter explains how to use the autodiff feature of the
    PyTorch tensor APIs to calculate machine learning model gradients in a simple
    example of applying gradient descent to a linear regression problem based on a
    tiny, synthetic data set. In the process, you will learn the PyTorch autodiff
    APIs and how to use them to implement the standard sequence of steps used in machine
    learning with gradient descent. The chapter concludes by demonstrating the torch.optim
    package with various gradient descent optimizers and showing you how to take advantage
    of the optimizers as part of your machine learning code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '本章的其余部分将解释如何使用 PyTorch 张量 API 的自动微分功能来计算机器学习模型的梯度，以一个基于一个小的合成数据集的线性回归问题应用梯度下降的简单示例。在这个过程中，您将学习
    PyTorch 自动微分的 API，并学会如何使用它们来实现机器学习中使用梯度下降的标准步骤序列。本章最后展示了使用各种梯度下降优化器的 torch.optim
    包，并向您展示如何在您的机器学习代码中利用这些优化器。 '
- en: 6.1 Understanding the basics of autodiff
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 理解自动微分的基础知识
- en: This section introduces the ideas behind autodiff and teaches its fundamentals
    by walking you through a simple example of implementing autodiff using only core
    Python programming language constructs, without PyTorch. In the process, you will
    gain a deeper understanding of the PyTorch autodiff functionality and develop
    the knowledge that will help you troubleshoot PyTorch autodiff in your projects.
    In this section, you will see that while autodiff is surprisingly straightforward,
    it is an algorithm that supports complex applications of the calculus chain rule.
    In later sections, you will apply what you learned and use autodiff features of
    PyTorch tensors.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了自动微分的概念，并通过使用纯粹的 Python 编程语言构造，在没有使用 PyTorch 的情况下，通过一个简单的例子来教授其基础知识。在这个过程中，您将深入理解
    PyTorch 自动微分功能，并开发出使您能够在项目中解决 PyTorch 自动微分问题的知识。在本节中，您将看到自动微分虽然出奇地简单，但它是一个支持复杂应用微积分链式法则的算法。在后续章节中，您将应用所学知识，并使用
    PyTorch 张量的自动微分功能。
- en: The autodiff feature of PyTorch tensors is one of the core reasons the framework
    became popular for deep learning and for many machine learning algorithms that
    depend on gradient descent as well as related optimization techniques. While it
    is possible to use autodiff by treating it as a black box without fully understanding
    how it works, if you wish to develop the skills for troubleshooting autodiff in
    production scenarios, it is valuable to have at least a basic understanding of
    this critical PyTorch feature.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch张量的自动微分功能是该框架成为深度学习和许多依赖于梯度下降以及相关优化技术的机器学习算法流行的核心原因之一。虽然可以将自动微分视为一个黑盒子来使用，而不完全理解它的工作方式，但如果您希望开发用于在生产场景中排除自动微分问题的技巧，了解这个关键的PyTorch功能至少是有价值的。
- en: PyTorch implements an autodiff approach known as *reverse-mode accumulation
    automatic differentiation*, which is an efficient approach for computing gradients
    (defined in appendix A) of the kinds of loss functions that are commonly used
    in machine learning, including mean squared error and cross entropy. More precisely,
    PyTorch autodiff has O(n) computational complexity, where n is the total number
    of operations (e.g., addition or multiplication operations) in the function, as
    long as the function has more input than output variables.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch实现了一种名为*反向模式积累自动微分*的自动微分方法，这是一种高效的方法，用于计算常用于机器学习的损失函数（在附录A中定义）的梯度，包括均方误差和交叉熵。更准确地说，PyTorch自动微分具有O(n)的计算复杂度，其中n是函数中操作（如加法或乘法操作）的总数，只要函数的输入变量多于输出变量。
- en: If you are already familiar with reverse-mode accumulation automatic differentiation,
    feel free to skip to section 6.2, which explains how to use PyTorch autodiff APIs
    for machine learning. Otherwise, this section will help you gain a deeper understanding
    of the PyTorch autodiff API design and its use.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经熟悉反向模式积累自动微分，可以跳转到第6.2节，其中解释如何使用PyTorch自动微分API进行机器学习。否则，本节将帮助您更深入地了解PyTorch自动微分API设计及其用途。
- en: If you are just starting to learn about autodiff, you need to know that it is
    distinct from other popular differentiation techniques such as numeric or symbolic
    differentiation. Numeric differentiation is commonly taught in undergraduate computer
    science courses and is based on an approximation of ![006-01_EQ01](Images/006-01_EQ01.png).
    Unlike numeric differentiation, autodiff is numerically stable, meaning that it
    provides accurate values of gradients even at the extreme values of the differentiated
    functions and is resilient to the accumulation of small errors introduced by floating
    point number approximations to real numbers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您刚开始学习自动微分，需要知道它与其他流行的微分技术（如数值微分或符号微分）是不同的。数值微分通常在本科计算机科学课程中教授，基于对![006-01_EQ01]的近似。与数值微分不同，自动微分在数值上是稳定的，这意味着它在不同函数值的极端值时提供准确的梯度值，并且对实数的浮点数近似所引入的小误差的累积是有决策力的。
- en: Unlike symbolic differentiation, autodiff does not attempt to derive a symbolic
    expression of the differentiated function. As a result, autodiff typically requires
    less computation and memory. However, symbolic differentiation derives a differentiated
    function that can be applied across arbitrary input values, unlike autodiff, which
    differentiates a function for specific values of the function’s input variables
    one at a time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与符号微分不同，自动微分不尝试派生一个差分函数的符号表达式。因此，自动微分通常需要更少的计算和内存。然而，符号微分推导了一个可应用于任意输入值的差异函数，不像自动微分，它一次为函数的特定输入变量的值进行差异。
- en: A great way to understand autodiff is to implement a toy example of it yourself.
    In this section, you will implement autodiff for a trivial tensor, a scalar, add
    support for computing gradients of functions that use addition as well as multiplication,
    and then explore how you can use your implementation to differentiate common functions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 理解自动微分的一个好方法是自己实现一个玩具示例。在本节中，您将为一个微不足道的张量实现自动微分，一个纯量，添加支持计算使用加法和乘法的函数的梯度，然后探索如何使用您的实现来区分常见函数。
- en: To start, define a Scalar Python class, storing the value of the scalar (val)
    and its gradient (grad):[²](#pgfId-1012089)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，定义一个标量Python类，存储标量的值（val）和其梯度（grad）：[²](#pgfId-1012089)
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to better track the contents of the Scalar instances and to support
    a nicer printout of the instance values, let’s also add a __repr__ method, returning
    a string representation of the instance:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地跟踪标量实例的内容并支持更好的实例值输出打印，让我们也添加一个__repr__方法，返回实例的字符串表示形式：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this implementation in place, you can instantiate an object of the Scalar
    class, for example using Scalar(3.14).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个实现，您可以实例化一个标量类的对象，例如使用 Scalar(3.14)。
- en: Listing 6.1 grad attribute to store the gradient of the Scalar tensor
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 grad 属性用于存储标量张量的梯度
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once executed, this should return the output
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行，这个操作应该返回输出
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which corresponds to the string returned by the __repr__ method.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这与__repr__方法返回的字符串相对应。
- en: 'Next, let’s enable addition and multiplication of Scalar instances by overriding
    the corresponding Python methods. In reverse-mode autodiff, this is know as the
    implementation of the *forward* *pass* which simply computes the values from the
    Scalar operations:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过重写相应的Python方法来实现对标量实例的加法和乘法。在反向模式自动微分中，这被称为*前向传播* *过程*，它仅仅计算标量运算的值：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At this point, you can perform basic arithmetic on Scalar instances, so that
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可以对标量实例执行基本的算术运算，
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: correctly evaluates to
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正确计算为
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: and confirms that the implementation respects arithmetic precedence rules.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 并证实该实现遵守算术优先规则。
- en: The entire implementation at this point amounts to exactly a dozen lines of
    code that should be easy to understand. You are already more than halfway done,
    because this implementation correctly computes the forward pass of autodiff.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，整个实现只需大约十二行代码，应该很容易理解。您已经完成了一半以上的工作，因为这个实现正确计算了自动微分的前向传播。
- en: To support the *backward pass* that calculates and accumulates the gradients,
    you need to make a few small changes to the implementation. First, the Scalar
    class needs to be initialized with a backward function that is set to be a no-op
    by default ❶.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持计算和累积梯度的*反向传播*，您需要对实现做一些小的更改。首先，标量类需要用默认设置为一个空操作的反向函数进行初始化❶。
- en: Listing 6.2 backward method placeholder for backward pass support
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 反向传播支持的后向方法占位符
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '❶ Use lambda: None as the default implementation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ 使用 lambda: None 作为默认实现。'
- en: Surprisingly, this implementation is enough to start computing gradients of
    trivial linear functions. For instance, to find out the gradient of ![006-01_EQ02](Images/006-01_EQ02.png)
    for a linear function *y* = *x* at x = 2.0, you can start by evaluating
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这个实现足以开始计算琐碎线性函数的梯度。例如，要找出线性函数*y* = *x*在x = 2.0处的梯度![006-01_EQ02](Images/006-01_EQ02.png)，您可以从评估开始
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: which initializes the x variable to have a value of a Scalar(2.0) and declares
    the function *y* = *x*. Also, since it is such a simple case, the forward pass
    of computing y is just a no-op and does nothing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它将x变量初始化为一个Scalar(2.0)，并声明函数*y* = *x*。此外，由于这是一个非常简单的案例，计算y的前向传播只是一个空操作，什么也不做。
- en: 'Next, prior to using the backward function you have to perform two prerequisite
    steps: first, zero out the gradient of the variable (I will explain why shortly),
    and second, specify the gradient of the output y. Since x is a single variable
    in your function, zeroing out its gradient amounts to running'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在使用反向函数之前，您需要执行两个先决步骤：首先，将变量的梯度清零（我将很快解释为什么），其次，指定输出y的梯度。由于x是函数中的一个单独变量，清零其梯度就相当于运行
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you find the step of setting x.grad = 0.0 unnecessary since grad is already
    set to zero as part of the __init__ method, keep in mind that this example is
    for a trivial function, and as you later extend the implementation to more complex
    functions, the need for setting gradients to zero is going to become more clear.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您觉得设置x.grad = 0.0 这个步骤是不必要的，因为grad已经在__init__方法中设置为零，那么请记住，这个例子是针对一个琐碎函数的，当您稍后将实现扩展到更复杂的函数时，设置梯度为零的必要性会变得更加明显。
- en: 'The second step is to specify the value of the gradient of the output, y.grad,
    with respect to itself, which can be expressed as ![006-01_EQ02](Images/006-01_EQ02.png).
    Luckily, this value is trivial to figure out if you have ever divided a number
    by itself: y.grad is just 1.0.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是指定输出y的梯度值，即关于自身的![006-01_EQ02](Images/006-01_EQ02.png)。幸运的是，如果您曾经将一个数字除以自身，那么这个值就很容易找出：y.grad就是1.0。
- en: So, to perform reverse-mode accumulating autodiff on this trivial linear function,
    you simply execute
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要在这个简单线性函数上执行反向累积自动微分，你只需要执行以下操作：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: and then discover the value of ![006-01_EQ02](Images/006-01_EQ02.png) using
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: which correctly outputs 1.0.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出 ![006-01_EQ02](Images/006-01_EQ02.png) 的值，其结果正确显示为1.0。
- en: 'If you have been paying attention to the definition of y = x, you are well
    within your rights to protest that this entire sequence of calculations simply
    took the gradient from the y.grad = 1.0 statement and printed it back. If that’s
    your line of thinking, you are absolutely correct. Just as with the example of
    ![006-01_EQ03](Images/006-01_EQ03.png), when computing the gradient for ![006-01_EQ02](Images/006-01_EQ02.png),
    for a function y = x, the ratio of change to y, in terms of a change to x, is
    just 1.0. However, this simple example illustrates an important sequence of autodiff
    operations that stay the same even with complex functions:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直关注 y = x 的定义，你完全有权利提出反对，认为这整个计算过程只是将梯度从 y.grad = 1.0 语句中取出并打印出来。如果这是你的思维线路，那么你是绝对正确的。就像前面介绍
    ![006-01_EQ03](Images/006-01_EQ03.png) 的例子一样，当计算 ![006-01_EQ02](Images/006-01_EQ02.png)
    的梯度时，对于函数 y = x，y 相对于 x 的变化与 x 相对于 y 的变化的比率就是1.0。然而，这个简单的例子展示了一系列自动微分操作的重要顺序，即使对于复杂函数也是如此：
- en: Specifying the values of the variables
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定变量的值
- en: Specifying the output in terms of the variables (forward pass)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定变量的输出（前向传播）
- en: Ensuring the gradients of the variables are set to zero
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保变量的梯度设置为零
- en: Calling backward() to compute the gradients (backward pass)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用 backward() 来计算梯度（后向传播）
- en: 'If you are comfortable with the reasoning about differentiation so far, you
    should be ready to move on to computing gradients of more complex functions. With
    autodiff, computation of the gradients happens within the functions that implement
    mathematical operations. Let’s start with the easier one, addition:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对微分的推理感到舒适，那么你就可以进一步计算更复杂函数的梯度了。使用自动微分，梯度的计算发生在实现数学运算的函数内部。让我们从比较容易的加法开始：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that the direct computation, accumulation, and recursive computations
    of the gradient happen in the body of the backward function assigned to the Scalar
    produced by the addition operation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，梯度的直接计算、累积和递归计算发生在分配给加法操作所产生的 Scalar 对象的 backward 函数的主体中。
- en: 'To understand the logic behind self.grad += out.grad and the similar other.val
    += out.grad instruction, you can either apply basic rules of calculus or some
    straightforward reasoning about change. The relevant fact from calculus informs
    you that for a function *y* = *x* + *c*, where c is some constant, then ![006-01_EQ02](Images/006-01_EQ02.png)
    = 1.0\. This is nearly identical to the previous example with the computation
    of the gradient for y = x: despite adding a constant value to x, the ratio of
    change to y, in terms of a change to x, is still just 1.0. With respect to the
    code, this means that the amount of change contributed by self.grad to out.grad
    is identical to the value of out.grad.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 self.grad += out.grad 和类似的 other.val += out.grad 指令背后的逻辑，你可以应用微积分的基本规则或者进行一些有关变化的直观推理。微积分中的相关事实告诉我们，对于一个函数
    *y* = *x* + *c*，其中 *c* 是某个常数，那么 ![006-01_EQ02](Images/006-01_EQ02.png) = 1.0。这与之前计算
    y = x 的梯度的例子几乎完全相同：尽管给 x 添加了一个常数，但是 y 相对于 x 的变化与 x 相对于 y 的变化的比率仍然是 1.0。对于代码来说，这意味着
    self.grad 对 out.grad 贡献的变化量与 out.grad 的值是一样的。
- en: What about cases where the code is computing gradients for a function without
    a constant, in other words *y* = *x* + *z*, where both x and z are variables?
    In terms of the implementation, why should out.grad be treated as a constant when
    computing self.grad? The answer comes down to the definition of a gradient, or
    a partial derivative with respect to one variable at a time. Finding the gradient
    of self.grad is equivalent to answering the question “Assuming all variables,
    except for self.grad, stay constant, what is the ratio of a change in y to a change
    of self.grad?” Hence, when computing the gradient self.grad, other variables can
    be treated as constant values. This reasoning also applies when computing the
    gradient for other.grad, except in this case self.grad is treated as a constant.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于代码计算没有常数的函数的梯度的情况呢，换句话说*y* = *x* + *z*，其中x和z都是变量？在实现方面，当计算self.grad时，为什么out.grad应被视为常数呢？答案归结于梯度或关于一个变量的偏导数的定义。找到self.grad的梯度相当于回答问题“假设所有变量，除了self.grad，都保持不变，那么y对self.grad的变化率是多少？”因此，在计算梯度self.grad时，其他变量可以视为常量值。当计算other.grad的梯度时，这种推理也适用，唯一的区别是self.grad被视为常量。
- en: Also, note that as part of the calculation of the gradient in the __add__ method,
    both self.grad and other.grad are accumulating gradients using the += operator.
    Understanding this part of the autodiff is critical to understanding why the gradients
    need to be zeroed out before running the backward method. Simply put, if you invoke
    the backward method more than once, the values in the gradients will continue
    to accumulate, yielding undesirable results.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在__add__方法的梯度计算的一部分中，both self.grad和other.grad都使用+=运算符累积梯度。理解autodiff中的这部分是理解为什么在运行backward方法之前需要将梯度清零至关重要。简单地说，如果你多次调用backward方法，则梯度中的值将继续累积，导致不良结果。
- en: 'Last but not least, the lines of code self.backward() and other.backward()
    that invoke the backward method recursively ensure that the implementation of
    autodiff also handles function composition, such as f(g(x)). Recall that in the
    base case the backward method is just a no-op lambda: None function, which ensures
    that the recursive calls always terminate.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，调用backward方法递归触发的self.backward()和other.backward()代码行确保autodiff的实现也处理了函数组合，例如f(g(x))。请回忆，在基本情况下，backward方法只是一个无操作的lambda：None函数，它确保递归调用始终终止。
- en: 'To try out the __add__ implementation with a backward pass support, let’s look
    at a more complex example by redefining y as a sum of x values:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试带有后向传递支持的__add__实现，让我们通过将y重新定义为x值的和来查看更复杂的示例：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: From calculus, you may recall that the derivative of *y* = *x* + *x* = 2 * *x*
    is just 2.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从微积分中，你可能会记得*y* = *x* + *x* = 2 * *x*的导数只是2。
- en: 'Let’s confirm this using your implementation of the Scalar. Again, you need
    ensure that the gradients of x are zeroed out, initialize ![006-01_EQ03](Images/006-01_EQ03.png)
    = 1, and execute the backward function:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你的Scalar实现确认一下。同样，你需要确保x的梯度被清零，初始化 ![006-01_EQ03](Images/006-01_EQ03.png)
    = 1，然后执行后向函数：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: At this point, if you print out
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，如果你打印出来
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: it returns the correct value of
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回正确的值
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To get a better understanding of why ![006-01_EQ02](Images/006-01_EQ02.png)
    evaluated to 2.0, recall the definition of the backward function implemented in
    the __add__ method. Since y is defined as y = x + x, both self.grad and other.grad
    reference the same instance of the x variable within the backward method. Hence,
    a change to x translates to twice the change to y, or the gradient, ![006-01_EQ02](Images/006-01_EQ02.png)
    is 2.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解为什么![006-01_EQ02](Images/006-01_EQ02.png)评估为2.0，回想一下在__add__方法中实现的向后函数的定义。由于y定义为y
    = x + x，self.grad和other.grad都引用backward方法中x变量的同一实例。因此，对x的更改相当于对y或梯度的更改两倍，因此梯度![006-01_EQ02](Images/006-01_EQ02.png)为2。
- en: 'Next, let’s extend the implementation of the Scalar class to support gradient
    calculations when multiplying Scalars. The implementation is just six additional
    lines of code in the __mul__ function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们扩展Scalar类的实现，以支持在乘法Scalars时计算梯度。在__mul__函数中，实现只需要六行额外代码：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With multiplication, the logic behind the gradient derivation is more complex
    than with addition, so it is worthwhile to review it in more detail. Just as with
    addition, suppose you are trying to derive the gradient with respect to self.grad,
    which means that for the calculation, other.val can be considered a constant c.
    When computing the gradient of *y* = *c* * *x* with respect to x, the gradient
    is just c, meaning that for every change to x, y changes by c. When computing
    the gradient of the self.grad, c is just the value of other.val. Similarly, you
    can flip the calculation of the gradient for other.grad and treat self Scalar
    as a constant. This means that other.grad is a product of self.val and the out.grad.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法的梯度推导逻辑比加法更复杂，因此值得更详细地审查。与加法一样，假设你试图根据self.grad来推导梯度，这意味着在计算时，other.val可以被视为常数c。当计算
    *y* = *c* * *x* 关于 x 的梯度时，梯度就是c，这意味着对于x的每一个变化，y都会变化c。当计算self.grad的梯度时，c就是other.val的值。类似地，你可以翻转对other.grad的梯度计算，并将self标量视为常数。这意味着other.grad是self.val与out.grad的乘积。
- en: 'With this change in place, the entire implementation of the Scalar class is
    just the following 23 lines of code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个改变，标量类的整个实现只需以下23行代码：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To gain more confidence that the implementation is working correctly, you can
    try running the following test case:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更加确信实现正确，你可以尝试运行以下测试案例：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Repeat the earlier steps to zero out the gradients and specify the value of
    the output gradient as 1.0:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 重复早期步骤将梯度归零，并将输出梯度值指定为 1.0：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Using calculus rules, it is straightforward to figure out the expected result
    analytically: given *y* = *x*², the ![006-01_EQ02](Images/006-01_EQ02.png) = 2
    * x. So for x = 3.0, your implementation of the Scalar should return the gradient
    value of 6.0.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微积分规则，可以很容易地通过解析方法找出预期结果：给定 *y* = *x*²，则 ![006-01_EQ02](Images/006-01_EQ02.png)
    = 2 * x。因此对于 x = 3.0，你的标量实现应返回梯度值为 6.0。
- en: You can confirm this by printing out
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过打印输出来确认
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: which returns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 返回结果为
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The Scalar implementation also scales to more complex functions. Using *y* =
    *x*³ + 4**x* + 1 as an example, where the gradient ![006-01_EQ02](Images/006-01_EQ02.png)
    = 3 * *x*² + 4 so ![006-01_EQ02](Images/006-01_EQ02.png) = 31 when *x* = 3, you
    can implement this function y using your Scalar class by specifying
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 标量实现也可扩展到更复杂的函数。以*y* = *x*³ + 4**x* + 1为例，其中梯度 ![006-01_EQ02](Images/006-01_EQ02.png)
    = 3 * *x*² + 4，所以 ![006-01_EQ02](Images/006-01_EQ02.png) 在 *x* = 3 时等于 31，你可以通过指定如下代码来使用标量类实现这个函数y：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: and then running
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: to confirm that the implementation correctly returns the value of 31.0.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 确认实现正确返回值为 31.0。
- en: The implementation of autodiff for the Scalar is trivial compared to the features
    available with PyTorch tensors. However, it can give you a deeper understanding
    of the capabilities that PyTorch provides when you are computing gradients and
    shed light on when and why you need to use the seemingly magical zero_grad and
    backward functions of the PyTorch tensor autodiff APIs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标量（Scalar）的自动微分实现与PyTorch张量提供的功能相比较简单。然而，它可以让你更深入地了解PyTorch在计算梯度时提供的功能，并阐明为什么以及何时需要使用看似神奇的`zero_grad`和`backward`函数。
- en: 6.2 Linear regression using PyTorch automatic differentiation
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch自动微分进行线性回归
- en: This section builds on the explanation of the autodiff algorithm from section
    6.1 and introduces you to autodiff support in PyTorch. As a motivating example,
    the section walks you through the process of solving a single variable linear
    regression problem using PyTorch autodiff and a basic implementation of gradient
    descent. In the process, you will learn about using PyTorch autodiff APIs, practice
    implementing the forward and backward passes for a differentiable model, and prepare
    for a deeper dive into applying PyTorch in the upcoming chapters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节在介绍了第6.1节中自动微分算法的基础上，引入了PyTorch中的自动微分支持。作为一个激励性的例子，本节将带领你通过使用PyTorch自动微分和基本的梯度下降法解决单变量线性回归问题的过程。在这个过程中，你将学习如何使用PyTorch自动微分API，实践可微模型的前向和反向传播，并为在接下来的章节深入应用PyTorch做好准备。
- en: To illustrate the autodiff feature in PyTorch, let’s use it along with the gradient
    descent algorithm to solve the classic linear regression problem. To set up the
    problem, let’s generate some sample data,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明PyTorch中的自动微分特性，让我们结合梯度下降算法解决经典的线性回归问题。为了建立问题，让我们生成一些样本数据，
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'so that the variable X holds 100 values, evenly spaced in the range from –5
    to 5\. In this example, suppose that *y* = 2*x* + ε, where ε is a random number
    sampled from a standard normal distribution (ε ~ *N* (0,1)) so that y can be implemented
    with PyTorch as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以变量 X 包含100个值，均匀分布在范围从-5到5。在这个例子中，假设 *y* = 2*x* + ε，其中 ε 是从标准正态分布中抽样的随机数（ε
    ~ *N* (0,1)），以便 y 可以用 PyTorch 实现为：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The purpose of adding randn noise to the y function is to illustrate the ability
    of the algorithm to correctly estimate the slope of the line, in other words,
    to recover the value of 2.0 using just the training data tensors y, X.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 向 y 函数添加 randn 噪声的目的是说明算法正确估计线的斜率的能力，换句话说，仅使用训练数据张量 y、X 即可恢复值为2.0。
- en: At this point, if you were to graph the values of X along the horizontal axis
    and y on the vertical axis, you should expect to see a plot resembling the one
    shown in figure 6.1\. Of course, your specific values may vary if you have used
    a different random seed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，如果您将 X 的值沿水平轴和 y 的值沿垂直轴绘制成图表，您应该期望看到一个类似于图6.1的图。当然，如果您使用了不同的随机种子，您的具体值可能会有所不同。
- en: '![06-01](Images/06-01.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![06-01](Images/06-01.png)'
- en: Figure 6.1 A sample regression problem to explain the basics of PyTorch tensor
    APIs
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 用于解释 PyTorch 张量 API 基础知识的示例回归问题
- en: 'Next, to set up a differentiable model for the gradient descent algorithm you
    need to specify the model parameters along with an initial guess of the values
    of the parameters. For this simplified case of linear regression without a bias,
    the only model parameter you need to specify is the slope of the line. To initialize
    the parameter you can use the randn function sampling from the standard normal
    distribution:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，为了为梯度下降算法建立可微分模型，您需要指定模型参数以及参数值的初始猜测。对于这种简化的线性回归情况，没有偏差，您需要指定的唯一模型参数是线的斜率。为了初始化参数，您可以使用
    randn 函数从标准正态分布中抽样：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So far in this chapter you have not seen the requires_grad parameter used here
    by the randn tensor factory method to instantiate the value of w. In section 6.1,
    where I introduced the inner workings to the autodiff algorithm, you saw that
    calculation of the gradient requires additional memory and computation overhead
    for every data value in a tensor. For example, for every Scalar instance, autodiff
    required an additional grad attribute along with a definition of a recursive backward
    function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中您还没有看到 requires_grad 参数在此处被 randn 张量工厂方法用于实例化 w 值。在第6.1节中，我介绍了自动微分算法的内部工作原理，您看到计算梯度需要为张量中的每个数据值额外的内存和计算开销。例如，对于每个
    Scalar 实例，自动微分需要一个额外的 grad 属性以及一个递归反向函数的定义。
- en: For machine learning models, supporting autodiff can more than double the amount
    of memory a tensor needs. Hence, when instantiating a tensor using factory methods,
    PyTorch disables tensor autodiff by default, requiring you to use the requires_grad
    parameter to explicitly enable the support. However, if a tensor is derived from
    a tensor that has requires_grad enabled, then the derived tensor (known as *non-leaf*
    tensor) has requires_grad set to True automatically.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习模型，支持自动微分可能会使张量所需的内存量增加一倍以上。因此，当使用工厂方法实例化张量时，PyTorch 默认禁用张量自动微分，需要您使用
    requires_grad 参数显式启用支持。但是，如果一个张量是从启用 requires_grad 的张量派生的，那么派生张量（称为*非叶*张量）的 requires_grad
    将自动设置为 True。
- en: 'Once the model parameter w is initialized, the forward pass of the algorithm
    is ready to be implemented. In this case, the forward pass is to simply guess
    (predict) the y values using the value of the w parameter. The forward pass is
    implemented as a forward method that returns the value of the mean squared error
    of the predictions:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型参数 w 初始化完成，算法的前向传递就准备好了。在这种情况下，前向传递是简单地使用 w 参数猜测（预测）y 值。前向传递是作为一个返回预测的均方误差值的
    forward 方法实现的：
- en: '[PRE28]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Take a careful look at the body of the forward method to count all the tensors
    that get instantiated by PyTorch during the calculation of the mean squared error
    formula. The first tensor y_pred contains the predicted values for y based on
    a given value of w. The second tensor is y_pred - y and contains the individual
    errors of the predictions, while the third tensor contains the squared errors
    (y_pred - y) ** 2. Finally, the fourth tensor is computed using the mean function,
    which returns a scalar with the value of the mean squared error of the predictions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看前向方法的实现，去计算PyTorch在计算均方误差公式过程中实例化的所有张量的数量。第一个张量y_pred包含基于给定w值的y的预测值。第二个张量是y_pred
    - y，包含了预测值的个体误差，而第三个张量包含了平方误差(y_pred - y) ** 2。最后，使用mean函数计算第四个张量，返回一个标量，其值为预测值的均方误差。
- en: None of the four tensors instantiated in the forward method needed a manual
    specification of requires_grad = True because PyTorch automatically deduced that
    in order for the framework to support the computation of the gradients for the
    w tensor, it also needed to enable requires_grad for the non-leaf tensors derived
    from w. In general, given an arbitrary PyTorch tensor, you can check the values
    of its requires_grad attribute to determine whether it can be used for gradient
    calculations. For example, within the body of the forward method, y_pred.requires_grad
    returns True.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前向方法中实例化的四个张量均无需手动指定requires_grad = True，因为PyTorch会自动推断：为了支持对w张量的梯度计算，还需要启用来自w的非叶张量的requires_grad。一般来说，对于任意的PyTorch张量，你可以检查其requires_grad属性的值，以确定它是否可以用于梯度计算。例如，在前向方法的代码块中，y_pred.requires_grad返回True。
- en: In this chapter you have not yet worked with PyTorch tensor aggregation functions
    such as mean. In the forward method, the mean function simply computes an arithmetic
    mean of the tensor values (i.e., the mean of the squared errors) and returns the
    aggregated result as a scalar. In upcoming chapters you are going to learn more
    about mean and other PyTorch tensor aggregation functions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你还没有使用过PyTorch张量聚合函数，比如mean。在前向方法中，mean函数简单地计算张量值的算术平均值（即平方误差的平均值），并将聚合结果返回为标量。在接下来的章节中，你将学习更多关于mean和其他PyTorch张量聚合函数的内容。
- en: With the code for the forward pass in place, there is enough groundwork to complete
    the implementation of gradient descent using PyTorch autodiff. Recall that the
    values of y and X in the code are based on the equation *y* = 2*x* + ε. The following
    code performs 25 iterations of gradient descent to estimate the value of 2.0,
    which is used by the equation as a slope of the line.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有了前向传播代码，就可以完成使用PyTorch自动微分来实现梯度下降的工作。回想一下，代码中的y和X的值基于方程*y* = 2*x* + ε。以下代码执行25次梯度下降迭代来估算2.0的值，该值用作方程中的斜率。
- en: Listing 6.3 PyTorch autodiff for linear regression using gradient descent
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 使用梯度下降的PyTorch自动微分进行线性回归
- en: '[PRE29]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Empty (zero) the gradient of w.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清空w的梯度（置零）。
- en: 'You should expect the code to print out numbers close to the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该期望代码打印出接近以下数字的结果：
- en: '[PRE30]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the implementation of gradient descent, the learning rate is set arbitrarily
    to 0.03 and the number of gradient descent iterations to 25\. In upcoming chapters,
    you will learn more about hyperparameter tuning and more rigorous approaches for
    choosing the values for learning rate, and number of gradient descent iterations,
    as well as values for other machine learning hyperparameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降的实现中，将学习率任意设为0.03，并将梯度下降迭代次数设置为25。在接下来的章节中，你将学习更多关于超参数调整和更严格的方法来选择学习率、梯度下降迭代次数以及其他机器学习超参数的值。
- en: As you already know from section 6.1, when using autodiff it is critical to
    zero out gradients before using the backward function to accumulate updated gradient
    values. Note that in the case of PyTorch tensors, the grad attribute is zeroed
    out by setting it to None ❶ rather than to the value of 0. Once the mse_loss tensor
    is returned by the forward method, the gradients are updated by invoking the backward
    function. The gradient descent step amounts to the update of the w parameter data
    using the negative product of the learning rate and the updated gradient w.data
    -= LEARNING_RATE * w.grad.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从第 6.1 节已经了解的那样，在使用自动微分时，重要的是在使用 backward 函数累积更新的梯度值之前将梯度归零。请注意，在 PyTorch
    张量的情况下，grad 属性是通过将其设置为 None ❶ 而不是 0 的值来归零的。一旦 mse_loss 张量由 forward 方法返回，就会通过调用
    backward 函数来更新梯度。梯度下降步骤等同于使用学习率和更新后梯度的负乘积来更新 w 参数数据 w.data -= LEARNING_RATE *
    w.grad。
- en: Note that due to the noise in the values of y, you should not expect the gradient
    descent nor the analytical solution to linear regression to recover the exact
    value of 2.0 used for generating the data. To confirm this, you can use the PyTorch
    tensor APIs to calculate the analytical ordinary least squares solution based
    on the formula (*X^T* *X*)^(-1)*X^Ty*,
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于 y 值的噪声，不应期望梯度下降或线性回归的解析解能恢复用于生成数据的精确值 2.0。为了确认这一点，你可以使用 PyTorch 张量 API
    根据公式 (*X^T* *X*)^(-1)*X^Ty* 计算解析的最小二乘解，
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: which should return a value of roughly tensor(1.9876).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 应该返回大约 tensor(1.9876) 的值。
- en: 6.3 Transitioning to PyTorch optimizers for gradient descent
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 迁移到 PyTorch 优化器进行梯度下降
- en: This section covers the PyTorch torch.optim package and the Optimizer classes,
    including Adam and SGD (*stochastic gradient descent*), which you can re-use in
    your PyTorch-based machine learning models to improve how you train the model
    parameters.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 PyTorch 的 torch.optim 包和 Optimizer 类，包括 Adam 和 SGD（*随机梯度下降*），你可以在基于 PyTorch
    的机器学习模型中重新使用它们，以改进模型参数的训练方式。
- en: In addition to the torch.autograd automated differentiation framework, PyTorch
    also includes the torch.optim package with a collection of optimizers, which are
    algorithms that implement alternative optimization heuristics for updating the
    machine learning model parameters based on the loss function’s gradients. The
    details of the optimizer algorithms’ implementation are outside the scope of this
    book, but you should know that the PyTorch development team has been working diligently
    to maintain links in the PyTorch torch.optim package documentation to the relevant
    research papers that contain descriptions of the corresponding algorithm implementations.[³](#pgfId-1016733)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 torch.autograd 的自动微分框架外，PyTorch 还包括了 torch.optim 包，其中包含了一系列优化器，这些优化器是根据损失函数的梯度实现的替代优化启发式算法，用于更新机器学习模型参数。优化器算法的实现细节超出了本书的范围，但你应该知道，PyTorch
    开发团队一直在努力维护 PyTorch torch.optim 包文档中与相应算法实现相关的研究论文的链接。[³](#pgfId-1016733)
- en: 'The optimizer classes have been designed to be easily swappable, ensuring that
    you can experiment with alternative optimizers without having to change the rest
    of your machine learning model training code. Recall that in listing 6.3 you implemented
    a trivial version of linear regression using your own simple rule for updating
    the model parameter values based on the gradient and the learning rate:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器类被设计成易于交换，确保你可以在不必更改机器学习模型训练代码的情况下尝试替代优化器。回想一下，在列表 6.3 中，你实现了一个简单的线性回归版本，使用了自己简单的规则来根据梯度和学习率更新模型参数值：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Instead of hardcoding this rule yourself, you can re-use the equivalent update
    rule in the torch.optim.SGD optimizer by re-implementing the following code.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 不要自己硬编码这个规则，你可以通过重新实现以下代码，来重用 torch.optim.SGD 优化器中的等价更新规则。
- en: Listing 6.4 Linear regression using torch.optim optimizers
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 使用 torch.optim 优化器进行线性回归
- en: '[PRE33]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Instantiate SGD optimizer using an iterable of model parameter(s) [w].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用模型参数的可迭代对象 [w] 来实例化 SGD 优化器。
- en: ❷ Assume 25 epochs of gradient descent.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 假设进行 25 个周期的梯度下降。
- en: ❸ Perform a gradient update step using gradients computed by backward.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 backward 计算的梯度执行梯度更新步骤。
- en: ❹ Zero out the gradients of the model parameter(s) for the next iteration.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将模型参数的梯度归零，以备下一次迭代。
- en: This should output the model’s estimate of the slope of the line w as roughly
    2.0812765834924307. The changes needed to use the PyTorch optimizer are highlighted
    ❶, ❸, and ❹. Notice that when instantiating the optimizer ❶, you are providing
    the optimizer with a Python iterable (in this case a Python list) over the model
    parameters. After gradient descent computes the gradients (i.e., after the backward()
    method returns), the call to the optimizer’s step() method ❸ updates the model
    parameters based on the gradients. The call to the zero_grad() method of the optimizer
    ❹ clears (empties) the gradients to prepare the call to the backward() method
    in the next iteration of the for-loop.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出模型对线性斜率w的估计值大约为2.0812765834924307。使用PyTorch优化器所需的更改已经标注了❶、❸和❹。请注意，当实例化优化器❶时，您正在提供一个Python可迭代对象（在本例中是Python列表），用于模型参数。梯度下降计算出梯度后（即backward()方法返回后），对优化器的step()方法❸的调用基于梯度更新模型参数。优化器的zero_grad()方法❹的调用清除（清空）梯度，以准备下一次for循环迭代中backward()方法的调用。
- en: 'You may have encountered the Adam optimizer if you have prior experience training
    machine learning models. With the PyTorch library of optimizers, swapping the
    SGD optimizer ❶ for Adam is easy:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有训练机器学习模型的先验经验，可能已经遇到过Adam优化器。使用PyTorch优化器库，将SGD优化器❶替换为Adam很容易：
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In general, to use any PyTorch optimizer from the torch.optim package, you need
    to first instantiate one using the constructor,
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，要使用torch.optim包中的任何PyTorch优化器，您需要首先使用构造函数实例化一个，
- en: '[PRE35]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: where the params is an iterable of model parameters and defaults are the named
    arguments specific to an optimizer.[⁴](#pgfId-1017322)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中params是模型参数的可迭代对象，defaults是特定于优化器的命名参数的默认值。[⁴](#pgfId-1017322)
- en: Both SGD and Adam optimizers can be instantiated with additional configuration
    settings beyond the model parameters and the learning rate. However, these settings
    will be covered in more detail in chapter 11 on hyperparameter tuning. Until then,
    the examples will use the SGD optimizer since it is easier to both understand
    and to explain.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SGD和Adam优化器都可以使用除模型参数和学习率之外的其他配置设置来实例化。然而，这些设置将在第11章中的超参数调整中详细介绍。在那之前，示例将使用SGD优化器，因为它更容易理解和解释。
- en: 'As you progress to more complex training scenarios using gradient descent,
    it is useful to have clear and comprehensive terminology. As you can see from
    listing 6.4, training of a machine learning model by gradient descent consists
    of multiple iterations, where each iteration consists of actions that include
    the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当你逐渐进入使用梯度下降进行更复杂的训练场景时，清晰而全面的术语是很有用的。从列表6.4中可以看到，通过梯度下降训练机器学习模型包括多个迭代，每个迭代包括以下操作：
- en: '*Forward* pass, where you use the feature values and the model parameters to
    return the predicted outputs, for example y_pred = forward(X) from listing 6.4.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前向*传递，其中使用特征值和模型参数返回预测输出，例如来自列表6.4的y_pred = forward(X)。'
- en: '*Loss* calculation, where the predicted outputs and the actual outputs are
    used to determine the value of the loss function, for example mse = loss(y_pred,
    y) from listing 6.4.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失*计算，其中使用预测输出和实际输出来确定损失函数的值，例如来自列表6.4的mse = loss(y_pred, y)。'
- en: '*Backward* pass, where the reverse mode autodiff algorithm computes the gradients
    of the model parameters based on the calculations of the loss function, for example
    mse.backward() from listing 6.4.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向*传递，其中反向模式自动微分算法基于损失函数的计算计算模型参数的梯度，例如来自列表6.4的mse.backward()。'
- en: '*Parameter* or *weight updates*, where the model parameters are updated using
    the values of the gradients computed from the backward pass, which should be optimizer.step()
    if you are using the optimizers from the torch.optim package.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参数*或*权重更新*，其中使用从反向传递中计算得到的梯度值更新模型参数，如果您使用的是torch.optim包中的优化器，应该是optimizer.step()。'
- en: '*Clearing gradients*, where the gradient values in the model parameter PyTorch
    tensors are set to None to prevent automatic differentiation from accumulating
    gradient values across multiple iterations of gradient descent; if you are using
    the optimizers from the torch.optim package, this should be done using optimizer
    .zero_grad().'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*清除梯度*，其中将模型参数PyTorch张量中的梯度值设置为None，以防止自动微分在多次梯度下降迭代中累积梯度值；如果您使用的是torch.optim包中的优化器，则应使用optimizer.zero_grad()进行此操作。'
- en: In the industry, the terms *iteration* and *step of gradient descent* are often
    used interchangeably. Confusingly, the word “step” is also sometimes used to describe
    the specific action performed as part of gradient descent, for example a *backward
    step* or a *forward step*. Since PyTorch uses *step* to refer specifically to
    the action of updating the model parameters by the optimizer based on the gradients
    of the loss function, this book is going to stick with the PyTorch terminology.
    Keep in mind that some PyTorch frameworks, such as PyTorch Lightning, use *step*
    to mean *iteration*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在行业中，术语*迭代*和*梯度下降的步骤*通常互换使用。令人困惑的是，单词“步骤”有时也用来描述作为梯度下降的一部分执行的具体操作，例如*向后步骤*或*向前步骤*。由于PyTorch使用*步骤*来特指根据损失函数的梯度来更新模型参数的操作，本书将坚持使用PyTorch的术语。请记住，一些PyTorch的框架，如PyTorch
    Lightning，使用*步骤*来表示*迭代*。
- en: Before transitioning to the use of batches for model training with gradient
    descent, it is also useful to clarify the definition of the term *epoch*. In machine
    learning, an epoch describes one or more iterations of gradient descent needed
    to train (update) machine learning model parameters using every example in a data
    set exactly once. For example, listing 6.4 ❷ specifies that gradient descent should
    be performed for 25 epochs. The use of the word “epoch” also corresponds to *iterations*,
    for the simple reason that for every iteration of gradient descent all of the
    examples from the data set are used to calculate the gradients and update the
    weights of the model. However, as you will learn in the upcoming section on batches,
    performing an epoch of training may require multiple iterations of gradient descent.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在过渡到使用批次进行梯度下降的模型训练之前，还有必要明确术语*周期*的定义。在机器学习中，一个周期描述了使用每个数据集示例恰好一次来训练（更新）机器学习模型参数所需的一个或多个梯度下降迭代。例如，列表6.4❷指定了应该进行25个周期的梯度下降。使用词“周期”也对应于*迭代*，简单地原因是对于梯度下降的每次迭代，都会使用数据集中的所有示例来计算梯度和更新模型的权重。然而，正如您将在接下来关于批次的部分中了解到的，执行一个周期的训练可能需要多次梯度下降迭代。
- en: 6.4 Getting started with data set batches for gradient descent
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 开始使用数据集批次进行梯度下降
- en: 'This section teaches you about data set batches so that you can prepare for
    using data set batches for gradient descent with PyTorch. The concept of a data
    set batch for gradient descent is deceptively simple. A *batch* is just a non-empty
    collection (or, in mathematical terminology, a *multiset*) of examples randomly
    sampled[⁵](#pgfId-1019190) from a data set. Nevertheless, gradient descent using
    batches is nuanced and complex: mathematicians have even dedicated an area of
    research, known as *stochastic optimization*, to the topic. A training data set
    batch is more than just a sample of a training data set: in a single iteration
    of gradient descent, all the data examples from the training data set batch are
    used to update the gradients of a model.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向您介绍数据集批次（batches），以便您可以为使用PyTorch进行梯度下降做准备。梯度下降中数据集批次的概念看起来很简单。*批次*只是从数据集中随机抽样的一组非空示例（或在数学术语中，一个*多重集*）。尽管如此，使用批次的梯度下降是微妙而复杂的：数学家们甚至专门研究了这个领域，称为*随机优化*。训练数据集批次不仅仅是训练数据集的一个样本：在梯度下降的单个迭代中，训练数据集批次中的所有数据示例都用于更新模型的梯度。
- en: While you don’t need a PhD in mathematical optimization to use gradient descent
    with batches in PyTorch, it is worthwhile to have precise terminology related
    to batches and gradient descent to better navigate the complexity of the topic.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您不需要拥有数学优化的博士学位来使用PyTorch中的批量梯度下降，但具备与批次和梯度下降相关的精确术语能够更好地理解该主题的复杂性。
- en: The *batch size* is a positive (greater than zero) integer specifying the number
    of examples in a batch. Many machine learning research papers and online courses
    use the phrase *mini-batch gradient descent* to describe gradient descent with
    batch sizes greater than one. However, in PyTorch, the SGD (torch.optim.SGD) optimizer,
    as well as other optimizers in the torch.optim package, can be used with batch
    sizes ranging anywhere from 1 to the number of examples in the entire data set.
    Keep this in mind because often in machine learning literature the phrase *stochastic
    gradient descent* is used to describe gradient descent with a batch size of exactly
    one.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*批处理大小* 是一个正整数（大于零），指定了批处理中的示例数。许多机器学习研究论文和在线课程使用 *mini-batch 梯度下降* 这个短语来描述批处理大小大于一的梯度下降。然而，在
    PyTorch 中，SGD（torch.optim.SGD）优化器以及 torch.optim 包中的其他优化器可以使用从 1 到整个数据集示例数的任何批处理大小。要记住这一点，因为在机器学习文献中，*随机梯度下降*
    这个短语通常用来描述批处理大小恰好为一的梯度下降。'
- en: The choice of a batch size has as much to do with having enough memory in your
    machine learning compute nodes as with the machine learning performance of the
    gradient descent algorithm. This means that the upper bound on the batch size
    should take into account the amount of the available memory per node of your machine
    learning platform. The selection of the exact value for the batch size is covered
    in more detail in chapter 11 on hyperparameter tuning, but first you should know
    the maximum batch size that you can fit in the memory of your machine learning
    platform nodes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小的选择与您机器学习计算节点中的内存有关，也与梯度下降算法的机器学习性能有关。这意味着批处理大小的上限应考虑您机器学习平台节点的可用内存量。关于批处理大小的确切值的选择在第
    11 章的超参数调整中有更详细的介绍，但首先您应该知道您的机器学习平台节点内存中可以容纳的最大批处理大小。
- en: Much confusion about application of batches in PyTorch stems from lack of recognition
    that a batch should be treated as a fraction of data set size, which is the number
    of examples in a data set. A batch interpreted as a fraction is simply ![06-01_EQ06](Images/06-01_EQ06.png)
    so it is possible to categorize the choice of a batch size as producing either
    complete or incomplete batches, where a *complete batch* has a batch size that
    is an integer factor of the data set size, or more precisely
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对 PyTorch 中批处理的应用存在很多混淆，根源在于没有意识到批处理应该被视为数据集大小的一部分，即数据集中的示例数。将批处理解释为分数就是简单的
    ![06-01_EQ06](Images/06-01_EQ06.png)，因此可以将批处理大小的选择归类为生成完整或不完整批次，其中 *完整批次* 的批处理大小是数据集大小的整数因子，或者更准确地说
- en: '![06-01_EQ07](Images/06-01_EQ07.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![06-01_EQ07](Images/06-01_EQ07.png)'
- en: for some positive integer batch_count representing the number of batches in
    a data set.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个表示数据集中批次数的正整数 batch_count。
- en: 6.5 Data set batches with PyTorch Dataset and DataLoader
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 使用 PyTorch Dataset 和 DataLoader 的数据集批处理
- en: This section teaches you how to get started with using data set batches in PyTorch
    and how to use PyTorch utility classes that can help you manage and load your
    data sets as batches.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分教你如何开始使用 PyTorch 中的数据集批处理，并使用 PyTorch 实用程序类来帮助你管理和加载数据集作为批处理。
- en: The PyTorch framework provides a collection of data utility classes organized
    in the torch.utils.data package and implemented to support the use of data batches
    with gradient descent. The classes in the package, including DataLoader, Dataset,
    IterableDataset, and TensorDataset, are designed to work together to simplify
    the development of scalable machine learning model training processes, including
    scenarios where a data set does not fit in memory of a single node, and where
    a data set is used by multiple nodes in a distributed computing cluster. While
    the classes provide scalable implementations, that does not mean they are useful
    only for large data sets or with large computing clusters. As you will see in
    this section, the classes work fine (aside from a negligible overhead) with small,
    in-memory data sets.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 框架提供了一组数据实用程序类，组织在 torch.utils.data 包中，并实现了支持使用梯度下降的数据批处理。该包中的类，包括 DataLoader、Dataset、IterableDataset
    和 TensorDataset，旨在共同简化可扩展的机器学习模型训练过程的开发，包括数据集不适合单个节点内存的情况，以及数据集由分布式计算集群中的多个节点使用的情况。虽然这些类提供了可扩展的实现，但这并不意味着它们仅对大型数据集或大型计算集群有用。正如您将在本节中看到的，这些类可以很好地工作（除了可忽略的开销）与小型、内存中的数据集。
- en: Dataset is a highly reusable class and can support a variety of machine learning
    use cases based on map-style and iterable-style subclasses of the Dataset. The
    map-style Dataset is the original data set class from the PyTorch framework and
    is best suited for in-memory, index-addressable data sets. For example, if you
    were to implement your own map-style Dataset by subclassing PyTorch’s Dataset
    as a MapStyleDataset, you would have to implement the required __getitem__ and
    __len__ methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset是一个高度可重用的类，并且可以支持基于映射样式和可迭代样式的数据集子类的各种机器学习用例。映射样式数据集是PyTorch框架中的原始数据集类，最适合在内存中，可以通过索引进行寻址的数据集。例如，如果您要通过将PyTorch的数据集子类化为MapStyleDataset来实现自己的映射样式数据集，那么您必须实现所需的__getitem__和__len__方法。
- en: Listing 6.5 PyTorch map-style Dataset designed to be subclassed
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 设计为子类的PyTorch映射样式数据集
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Map style interface methods to retrieve a specific item from the data set
    . . .
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 映射样式接口方法，用于从数据集中检索特定项。. . .
- en: ❷ . . . and to return the total number of items in the entire data set.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ . . . 并返回整个数据集中项目的总数。
- en: 'Note that the map-style data set makes two assumptions as part of the interface:
    each example (item) from a data set is expected to be addressable by an index
    value ❶, and the size of the data set is expected to be known and available at
    any time ❷.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，作为接口的一部分，映射样式数据集做出了两个假设：预计可以通过索引值❶访问数据集中的每个示例（项），并且随时可以了解和获取数据集的大小❷。
- en: In most cases, if you are working with an in-memory data set, you can avoid
    having to implement your own map-style data set subclass, and instead re-use the
    TensorDataset class. The TensorDataset is also a part of the torch.utils.data
    package and implements the required Dataset methods by wrapping tensors or NumPy
    n-dimensional arrays. For example, to create a map-style training Dataset for
    the sample data values in tensors X and y, you can pass the data tensors directly
    to TensorDataset ❶.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，如果您使用的是内存数据集，您可以避免实现自己的映射样式数据集子类，而是重用TensorDataset类。TensorDataset也是torch.utils.data包的一部分，并通过包装张量或NumPy
    n维数组来实现所需的数据集方法。例如，要为张量X和y中的示例数据值创建映射样式的训练数据集，可以直接将数据张量传递给TensorDataset❶。
- en: Listing 6.6 TensorDataset simplifying batching of PyTorch tensors
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 简化批处理PyTorch张量的TensorDataset
- en: '[PRE37]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This allows you to fetch an example at index 0 from the data set by using the
    Python [0] syntax for the __getitem__ method,
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您可以使用Python [0]语法从数据集中获取索引为0的示例，使用__getitem__方法，
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: which outputs
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为
- en: '[PRE39]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: and allows you to confirm that the data set length is 100 using the __len__
    method on the train_ds data set,
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用train_ds数据集上的__len__方法验证数据集的长度为100，
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: where the Boolean expression in the assertion evaluates to True.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中断言中的布尔表达式的结果为True。
- en: When using an instance of a Dataset, the PyTorch code that implements the iterations
    of gradient descent should not access the data set directly but rather use an
    instance of DataLoader as an interface to the data. You will learn more about
    the rationale for using the DataLoader in the upcoming section on using GPUs with
    PyTorch.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用数据集的实例时，实现梯度下降迭代的PyTorch代码不应直接访问数据集，而是使用DataLoader的实例作为与数据交互的接口。您将在使用PyTorch和GPU的即将到来的部分中了解更多有关使用DataLoader的理由。
- en: At its core, a DataLoader is a wrapper around a Dataset, so by wrapping an instance
    of train_ds, described earlier, you can create a train_dl using
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: DataLoader本质上是对数据集的包装器，因此通过包装前面描述的train_ds实例，可以创建train_dl使用
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Note that by default, when a DataLoader is used with a map-style Dataset, each
    batch returned by a DataLoader instance is of size 1, meaning that the following
    expression evaluates to True:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，默认情况下，当DataLoader与映射样式的数据集一起使用时，DataLoader实例返回的每个批次的大小为1，这意味着以下表达式的结果为True：
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This default behavior can be easily modified by specifying the batch_size named
    parameter when instantiating the DataLoader so that the expression
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在实例化DataLoader时指定batch_size命名参数来轻松修改此默认行为，以便该表达式
- en: '[PRE43]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: evaluates to 25.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果为25。
- en: Both values of the batch_size, 1 and 25, produce complete batches. While all
    complete batches have an identical batch_size, an *incomplete batch* has fewer
    than batch_size examples. Specifically, given a batch_size and a data set, an
    incomplete batch may include as few as
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小的两个值，1和25，都会生成完整的批次。尽管所有完整的批次的批量大小相同，但*不完整的批次*包含的示例数少于批量大小。具体来说，根据批量大小和数据集，不完整的批次可以包含至少
- en: '*dataset_size* mod *batch_size*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*dataset_size* mod *batch_size*'
- en: examples, or in Python, dataset_size % batch_size.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 示例，或者在Python中，dataset_size % batch_size。
- en: 'For instance, when using a batch_size of 33, the following code produces an
    incomplete batch with a batch_size of 1 during the fourth iteration of the for-loop:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用batch_size为33时，在for循环的第四次迭代过程中，以下代码生成一个批次大小为1的不完整批次：
- en: '[PRE44]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This prints out the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下内容：
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'There are no universally accepted techniques for dealing with incomplete batches.
    While it is possible to try to prevent the incomplete batch problem, it may not
    have much value in practice: since batches are used when working with sufficiently
    large volumes of data such that the data sets are too large to fit in memory,
    ignoring or dropping incomplete batches are options if they do not have negative
    and measurable impact on the overall performance of a machine learning model.
    For example, the DataLoader class provides a drop_last option so that you can
    ignore the smaller, incomplete batches:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不完整的批次没有普遍接受的技术。虽然有可能尝试防止不完整批次的问题，但在实践中可能没有太多价值：因为批次是在处理足够大卷的数据时使用的，以致于数据集太大而无法放入内存中，因此如果不完整的批次对机器学习模型的整体性能没有负面和可衡量的影响，那么可以选择忽略或删除它们。例如，DataLoader类提供了一个drop_last选项，这样您就可以忽略较小的、不完整的批次：
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This outputs the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE47]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Nevertheless, the drop_last option for incomplete batches should be used judiciously
    when specifying a batch size, especially when working with batch sizes that are
    a large fraction of a data set. For example, consider a situation where the batch
    size is inadvertently set to be ![06-01_EQ09](Images/06-01_EQ09.png). Since this
    selection of the batch size yields two batches, the sole incomplete batch of the
    two, with a batch size of ![06-01_EQ10](Images/06-01_EQ10.png), is dropped when
    using the drop_last=True option, resulting in a waste of almost half of a data
    set!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在指定批大小时应谨慎使用不完整批次的drop_last选项，特别是在使用占数据集的大比例的批次大小时。例如，假设批次大小不慎设置为![06-01_EQ09](Images/06-01_EQ09.png)。由于此批次大小的选择产生两个批次，当使用drop_last=True选项时，两个中的单个不完整批次，批次大小为![06-01_EQ10](Images/06-01_EQ10.png)，被丢弃，导致将近一半的数据集浪费！
- en: 'It is possible to prevent incomplete batches by training for multiple epochs
    while concatenating the data set with itself and using the batch_size window as
    a *rolling window* over the data set. With this technique, the number of training
    epochs should be based on the least common multiple (lcm) of the batch size and
    the data set size:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过训练多个周期并将数据集与自身连接起来，使用batch_size窗口作为*滚动窗口*在数据集上滑动，从而防止不完整的批次。采用这种技术时，训练周期的数量应该基于批次大小和数据集大小的最小公倍数（lcm）：
- en: '![06-01_EQ11](Images/06-01_EQ11.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![06-01_EQ11](Images/06-01_EQ11.png)'
- en: To illustrate this approach, suppose for the sake of the example that you are
    working with a batch size of 12 and a data set size of 33, then
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种方法，假设仅为例子而工作的批次大小为12，数据集大小为33，则
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: outputs 4.0, indicating that the training data set needs to be concatenated
    with itself four times to achieve the four training epochs needed to avoid incomplete
    batches.[⁶](#pgfId-1021074)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 4.0，这表示训练数据集需要与自身连接四次，以实现所需的四个训练周期，以避免不完整的批次。
- en: How is a batch selected from a data set? Since a batch is intended to be statistically
    representative of the data set, the examples in the batch should be as independent
    of each other as possible. This means ensuring that the taxi fare examples within
    a batch are sampled randomly (without replacement) from the entire data set. In
    practice, the most straightforward way of achieving such random shuffling of the
    data set is to use the shuffle() method of the PySpark DataFrame API.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中选择批次是如何进行的？由于批次旨在统计代表数据集，批次中的示例应尽可能相互独立。这意味着确保批次中的出租车票价示例是从整个数据集中随机采样（无替换）的。在实践中，实现对数据集进行随机洗牌最直接的方法是使用PySpark
    DataFrame API的shuffle()方法。
- en: Since batches need to be statistically representative of the data set, you might
    be tempted to re-use the batch size based on the test data set size you discovered
    in chapter 4\. While the test data set size metric works as a *lower bound* for
    batch size, re-using its value isn’t the right decision since the test data set
    size was picked to be as small as possible while being statistically representative
    of the development data set. Chapter 11 describes in detail how to use a principled
    hyperparameter tuning approach to choose the right batch size with the lower and
    upper bound values introduced in this chapter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Dataset and DataLoader classes for gradient descent with batches
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section illustrates how to apply the Dataset and DataLoader classes using
    a minimalistic example to teach the concepts that also apply when using data set
    batches with more complex and realistic machine learning problems. To perform
    linear regression using gradient descent with batches using Dataset and DataLoader,
    you need to modify the solution from section 6.3.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Basic linear regression using gradient data set with batches
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Provide a data set interface to y and X using TensorDataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Create a DataLoader for the data set using a batch size of 1 (default).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Unpack each tuple of a batch of data while iterating over the DataLoader.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Perform the forward step over the batch of features to produce predictions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Calculate the loss using the batch of labels and the predictions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: This should output the estimated w as roughly 2.0812765834924307. Once the original
    tensors y and X are packaged into the map-style TensorDataset data set ❶, the
    resulting train_ds instance is further wrapped using a DataLoader to produce the
    train_dl.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: To use the batches with gradient descent, for each epoch, the code performs
    100 gradient descent iterations using individual batches returned in the for-loop
    by in train_dl ❸. The 100 iterations are performed per epoch since the original
    data set contains 100 examples and the default batch size of DataLoader is equal
    to 1\. Since the batch size of 1 produces complete batches (recall from the definition
    of a batch as a fraction of the data set),
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![06-01_EQ12](Images/06-01_EQ12.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: or alternatively, if you used batches with a batch size of 25 to
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: then
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![06-01_EQ13](Images/06-01_EQ13.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'The changes in listing 6.7 ❹ and ❺ are straightforward: instead of using the
    original data set, the code uses the batches returned by the train_dl instance.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In case you modified the batch size to a value that produces incomplete batches,
    for example by specifying batch size of 51,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: the second iteration of the inner for-loop will produce batches with 49 examples
    since DataLoader permits incomplete batches by default. In this specific case,
    this is not an issue, since the parameter of the model with the shape of torch.Size([])
    can broadcast to the incomplete batch with the shape of torch.Size([49]). However,
    in general, you must take care to align the shape of the model parameters to the
    shape of a batch. In chapter 7, you will learn from an example of aligning the
    model parameters to the batch shape for the DC taxi data set.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 内部 for 循环的第二次迭代将生成具有 49 个示例的批次，因为 DataLoader 默认允许不完整的批次。在这种特定情况下，这不是一个问题，因为具有
    torch.Size([]) 形状的模型参数可以与形状为 torch.Size([49]) 的不完整批次进行广播。然而，通常情况下，您必须注意将模型参数的形状与批次的形状对齐。在第
    7 章中，您将从对齐模型参数与 DC 出租车数据集批次形状的示例中学习。
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Automatic differentiation is a fundamental algorithm for simplifying complex
    chain rule applications.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动微分是简化复杂链式规则应用的基本算法。
- en: Python-native data structures can be used to demonstrate the basics of how to
    implement automatic differentiation for tensors.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 python 原生数据结构来演示如何为张量实现自动微分的基础知识。
- en: PyTorch tensors provide comprehensive support for automatic differentiation
    of tensor gradients.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 张量提供了全面的支持，用于对张量梯度进行自动微分。
- en: Optimizers from the torch.optim are a range of algorithms for optimizing parameters
    in machine learning models using gradient descent.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torch.optim 中的优化器是一系列用于使用梯度下降优化机器学习模型中参数的算法。
- en: PyTorch tensor automatic differentiation and optimizer APIs are central to machine
    learning with PyTorch.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 张量的自动微分和优化器 API 在使用 PyTorch 进行机器学习时是核心内容。
- en: Dataset and DataLoader interface classes simplify the use of batches in PyTorch
    code.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dataset 和 DataLoader 接口类简化了在 PyTorch 代码中使用批处理的过程。
- en: TensorDataset provides a ready-to-use implementation for in-memory data sets.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorDataset 提供了一个可以直接使用的内存中数据集的实现。
- en: '^(1.)The story of automatic differentiation and back propagation is described
    in detail here: [http://people.idsia .ch/~juergen/who-invented-backpropagation.html](http://people.idsia.ch/~juergen/who-invented-backpropagation.html).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)自动微分和反向传播的故事在这里有详细描述：[http://people.idsia.ch/~juergen/who-invented-backpropagation.html](http://people.idsia.ch/~juergen/who-invented-backpropagation.html)。
- en: ^(2.)In dense mathematical papers about automated differentiation, the Scalar
    class is known as a dual number and the grad as the adjoint.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)在有关自动微分的复杂数学论文中，标量类被称为双重数，grad 被称为伴随数。
- en: ^(3.)For example, the documentation for SGD is available from [http://mng.bz/zEpB](http://mng.bz/zEpB)
    and includes a link to both the relevant research paper and to the details of
    the SGD implementation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)例如，SGD 的文档可以从[http://mng.bz/zEpB](http://mng.bz/zEpB)获取，并包含指向相关研究论文和 SGD
    实现详细信息的链接。
- en: '^(4.)The base Optimizer class and its constructor are documented here: [https://pytorch.org/docs/stable/optim
    .html#torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)基础的 Optimizer 类和它的构造函数在这里有文档说明：[https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer)。
- en: ^(5.)More precisely, by using random sampling without replacement, or equivalently
    by randomly shuffling the order of examples in a data set.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)更精确地说，通过使用无替换的随机抽样，或者等效地对数据集中的示例顺序进行随机洗牌。
- en: ^(6.)Increasing the epoch count based on the least common multiple of the batch
    size and the data set size produces the minimum number of training epochs needed
    to avoid incomplete batches. It is also possible to train for any multiple of
    lcm(batch_size, data set_size) and avoid incomplete batches.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ^(6.)根据批处理大小和数据集大小的最小公倍数增加纪元数，可以产生足够的训练纪元，以避免不完整的批次。也可以训练任意 lcm(batch_size,
    data set_size) 的倍数，以避免不完整批次。
