- en: 10 Large Language Models in the real world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding how conversational LLMs like ChatGPT work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jailbreaking an LLM to get it to say things its programmers don’t want it to
    say
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing errors, misinformation, and biases in LLM output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning LLMs on your own data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding meaningful search results for your queries (semantic search)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up your vector search with Approximate Nearest Neighbor Algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating fact-based well-formed text with LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you increase the number of parameters for transformer-based language models
    to obscene sizes, you can achieve some surprisingly impressive results. Researchers
    call these surprises *emergent properties* but they may be a mirage.^([[1](#_footnotedef_1
    "View footnote.")]) Since the general public started to become aware of the capabilities
    of really large transformers, they are increasingly referred to as Large Language
    Models (LLMs). The most sensational of these surprises is that chatbots built
    using LLMs generate intelligent-sounding text. You’ve probably already spent some
    time using conversational LLMs such as ChatGPT, You.com and Llamma 2\. And like
    most, you probably hope that if you get good at prompting them, they can help
    you get ahead in your career and even help you in your personal life. Like most,
    you are probably relieved to finally have a search engine and virtual assistant
    that actually gives you direct, smart-sounding answers to your questions. This
    chapter will help you use LLMs better so you can do more than merely *sound* intelligent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will help you understand how generative Large Language Models
    work. We will also discuss the problems with practical applications of LLMs so
    you can use them smartly and minimize their harm to you and others:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Misinformation*:: LLMs trained on social media will amplify misinformation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reliability*:: LLMs will sometimes insert errors into your code and words,
    and these errors are very difficult to spot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Impact on Learning*:: Used incorrectly, LLMs can reduce your metacognition
    skill'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Impact on Collective Intelligence*:: Flooding the infosphere with false and
    inhuman text devalues authentic and thoughtful human-generated ideas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bias*:: LLMs have algorithmic biases that are harming us all in ways we rarely
    notice except when it affects us personally, creating division and mistrust'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accessibility*:: Most people do not have access to the resources and skills
    required to use LLMs effectively, disadvantaging the already disadvantaged'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Environmental Impact*:: In 2023 LLMs emitted more than 1000 kg/day CO2e (carbon
    dioxide equivalent) ^([[2](#_footnotedef_2 "View footnote.")]) ^([[3](#_footnotedef_3
    "View footnote.")])'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can mitigate a lot of these harms by building and using LLMs that are smarter
    and more efficient. That’s what this chapter is all about. You will see how to
    build LLMs that generate more intelligent, trustworthy, and equitable words. And
    you will learn how to make your LLMs more efficient and less wasteful, not only
    reducing the environmental impact but also helping more people gain access to
    the power of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Large Language Models (LLMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The largest of the LLMs have more than a trillion parameters. Models this large
    require expensive specialized hardware and many months of compute on high-performance
    computing (HPC) platforms. At the time of this writing, training a modest 100B
    parameter model on just the 3 TB of text in Common Crawl would cost at least $3
    M.^([[4](#_footnotedef_4 "View footnote.")]) Even the crudest model of the human
    brain would have to have more than 100 trillion parameters to account for all
    the connections between our neurons. Not only do LLMs have high-capacity "brains"
    but they have binged on a mountain of text — all the interesting text that NLP
    engineers can find on the Internet. And it turns out that by following online
    *conversations*, LLMs can get really good at imitating intelligent human conversation.
    Even BigTech engineers responsible for designing and building LLMs were fooled.
    Humans have a soft spot for anything that appears to be intentional and intelligent.
    We’re easily fooled because we *anthropomorphize* everything around us, from pets
    to corporations and video game characters.
  prefs: []
  type: TYPE_NORMAL
- en: This was surprising for both researchers and everyday technology users. It turns
    out that if you can predict the next word, and you you add a little human feedback,
    your bot can do a lot more than just entertain you with witty banter. Chatbots
    based on LLMs can have seemingly intelligent conversations with you about extremely
    complex topics. And they can carry out complex instructions to compose essays
    or poems or even suggest seemingly intelligent lines of argument for your online
    debates.
  prefs: []
  type: TYPE_NORMAL
- en: But there is a small problem — LLMs aren’t logical, reasonable, or even intentional,
    much less *intelligent*. Reasoning is the very foundation of both human intelligence
    and artificial intelligence. You may hear people talking about how LLMs can pass
    really hard tests of intelligence, like IQ tests or college entrance exams. But
    LLMs are just faking it. Remember, LLMs are trained on a large portion of all
    the question-answer pairs in various standardized tests and exams. A machine that
    has been trained on virtually the entire Internet can appear to be smart by merely
    mashing up word sequences that it has seen before. It can regurgitate patterns
    of words that look a lot like reasonable answers to any question that has ever
    been posed online.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: What about computational complexity? In a computer science course, you would
    estimate the complexity of the question-answering problem as \(O(n^2)\), where
    *n* is the number of possible questions and answers - a huge number. Transformers
    can cut through this complexity to learn the hidden patterns that tell it which
    answers are correct. In machine learning, this ability to recognize and reuse
    patterns in data is called *generalization*. The ability to generalize is a hallmark
    of intelligence. But the AI in an LLM is not generalizing about the physical world,
    it is generalizing about natural language text. LLMs are only "faking it", pretending
    to be intelligent, by recognizing patterns in words from the Internet. And how
    we use words in the virtual world isn’t always reflective of reality.
  prefs: []
  type: TYPE_NORMAL
- en: You have probably been impressed with the seeming quality of your conversations
    with LLMs such as ChatGPT. LLMs answer almost any question with confidence and
    seeming intelligence. But *seeming* is not always being. If you ask the right
    questions LLMs stumble into *hallucinations* or just plain nonsense. And it’s
    nearly impossible to predict these holes in the Swiss cheese of their abilities.
    These problems were immediately evident at the launch of ChatGPT in 2022 and subsequent
    launch attempts by others.
  prefs: []
  type: TYPE_NORMAL
- en: To see what’s really going on, it can help to test an early version of the LLM
    behind ChatGPT. Unfortunately, the only OpenAI LLM that you can download is GPT-2,
    released in 2019\. All these years later, they still have not released the full-size
    1.5 billion parameter model, but instead released a half-size model with 775 million
    parameters. Nonetheless, clever open source developers were able to reverse engineer
    one called OpenGPT-2.^([[5](#_footnotedef_5 "View footnote.")]) Below you will
    use the official OpenAI half-size version to give you a feel for the limitations
    of ungrounded LLMs. Later we’ll show you how scaling up and adding information
    retrieval can really improve things.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 Count cow legs with GPT-2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And when ChatGPT launched, the GPT-3 model wasn’t any better at common sense
    reasoning. As the model was scaled up in size and complexity, it was able to memorize
    more and more math problem answers like this, but it didn’t generalize based on
    real-world experience. No common sense logical reasoning skill ever emerged even
    as newer and newer versions were released, including GPT-3.5 and GPT-4.0\. When
    asked to answer technical or reasoning questions about the real world, LLMs often
    generate nonsense that might look reasonable to a layperson, but they often contain
    errors that would be obvious if you look hard enough. And they are easy to jailbreak,
    forcing an LLM to say things (such as toxic dialog) that the LLM designers are
    trying to prevent them from saying.^([[6](#_footnotedef_6 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, after launch, the model slowly got better at answering questions
    it struggled with at launch. How did they do that? Like many LLM-based chatbots,
    ChatGPT uses *reinforcement learning with human feedback* (RLHF). This means that
    the human feedback is used to gradually adjust the model weights to improve the
    accuracy of the LLMs' next-word predictions. For ChatGPT there is often a *like
    button* you can click to let it know when you are happy with an answer to your
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it, the like button creates an incentive for LLMs trained
    this way to encourage the number of like button clicks from users by generating
    likable words. It’s similar to the way that dogs, parrots, and even horses can
    appear to do math if you train them this way, letting them know whenever you are
    happy with their answer. They will find *correlates* with the right answer in
    their training and use that predict their next word (or stomp of the hoof). Just
    as it was for the horse Clever Hans, ChatGPT can’t count and has no real mathematical
    ability.^([[7](#_footnotedef_7 "View footnote.")]) And this is the same trick
    that social media companies use to create hype, and divide us into echo chambers
    where we only hear what we want to hear, to keep us engaged so they can hijack
    our attention to sell it to advertisers.^([[8](#_footnotedef_8 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: And OpenAI has chosen to target "likability" (popularity) as the objective for
    its large language models. This maximizes the number of signups and hype surrounding
    their product launches. And this machine learning objective function was very
    effective at accomplishing their objective. OpenAI executives bragged that they
    had 100 million users only two months after launch. These early adopters flooded
    the Internet with unreliable natural language text. Novice LLM users even created
    news articles and legal briefs with fabricated references that had to be thrown
    out by tech-savvy judges. ^([[9](#_footnotedef_9 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Imagine your LLM is going to be used to respond to middle school students' questions
    in real time. Or maybe you want to use an LLM to answer health questions. Even
    if you are only using the LLM to promote your company on social media. If you
    need it to respond in real-time, without continuous monitoring by humans, you
    will need to think about ways to prevent it from saying things that harm your
    business, your reputation, or your users. You’ll need to do more than simply connect
    your users directly to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three popular approaches to reducing an LLM’s toxicity and reasoning
    errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Scaling*: Make it bigger (and hopefully smarter)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Guardrails*: Monitoring it to detect and prevent it from saying bad things'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Grounding*: Augment an LLM with a knowledge base of real-world facts'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Retrieval*: Augment an LLM with a search engine to retrieve text used to generate
    responses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next two sections will explain the advantages and limitations of the scaling
    and guardrail approaches. You will learn about grounding and retrieval in chapter
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Scaling up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the attractive aspects of LLMs is that you only need to add data and
    neurons if you want to improve your bot. You don’t have to handcraft ever more
    complicated dialog trees and rules. OpenAI placed a billion-dollar bet on the
    idea that the ability to handle complex dialog and reason about the world would
    emerge once they added enough data and neurons. It was a good bet. Microsoft invested
    more than a billion dollars in ChatGPT’s emergent ability to respond plausibly
    to complex questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However many researchers question whether this overwhelming complexity in the
    model is merely hiding the flaws in ChatGPT’s reasoning. Many researchers believe
    that increasing the dataset does not create more generally intelligent behavior
    just more confident and intelligent-*sounding* text. The authors of this book
    are not alone in holding this opinion. Way back in 2021, in the paper "On the
    Dangers of Stochastic Parrots: Can Language Models Be Too Big?" prominent researchers
    explained how the appearance of understanding in LLMs was an illusion. And they
    were fired for the sacrilege of questioning the ethics and reasonableness of OpenAI’s
    "spray and pray" approach to AI — relying exclusively on the hope that more data
    and neural network capacity would be enough to create intelligence. ^([[10](#_footnotedef_10
    "View footnote.")])'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [10.1](#figure-llm-survey) gives a brief history of the rapid increase
    in the size and number of LLMs over the past three years.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 Large Language Model sizes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![llm survey](images/llm_survey.png)'
  prefs: []
  type: TYPE_IMG
- en: To put these model sizes into perspective, a model with a trillion trainable
    parameters has less than 1% of the number of connections between neurons than
    an average human brain has. This is why researchers and large organizations have
    been investing millions of dollars in the compute resources required to train
    the largest language models.
  prefs: []
  type: TYPE_NORMAL
- en: Many researchers and their corporate backers are hopeful that increased size
    will unlock human-like capabilities. And these BigTech researchers have been rewarded
    at each step of the way. 100 B parameter models such as BLOOM and InstructGPT
    revealed the capacity for LLMs to understand and respond appropriately to complex
    instructions for creative writing tasks such as composing a love poem from a Klingon
    to a human. And then trillion parameter models such as GPT-4 can perform few-shot
    learning where the entire machine learning training set is contained within a
    single conversational prompt. It seems that every jump in the size and expense
    of LLMs creates a bigger and bigger payday for the bosses and investors in these
    corporations.
  prefs: []
  type: TYPE_NORMAL
- en: Each order of magnitude increase in model capacity (size) seems to unlock more
    surprising capabilities. In the GPT-4 Technical report, the OpenAI researchers
    explain the surprising capabilities that emerged.^([[11](#_footnotedef_11 "View
    footnote.")]) These are the same researchers who invested a lot of their time
    and money into this idea that scale (and attention) is all you need so they may
    not be the best people to evaluate the emmergent properties of their model. The
    researchers at Google who developed PaLM also noted all the emergent properties
    their own scaling research "discovered." Surprisingly Google researchers found
    that most capabilities they measured were not emergent at all, but rather these
    capabilities scaled linearly, sublinearly, or not at all (flat).^([[12](#_footnotedef_12
    "View footnote.")]) In more than a third of the intelligence and accuracy benchmarks
    that they ran, researchers found that the LLM approach to learning was no better
    than random chance. Scaling up did not improve things at all.
  prefs: []
  type: TYPE_NORMAL
- en: Here is some code and data you can use to explore the results from the paper
    "Emergent Abilities of Large Language Models."
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code snippet gives you an alphabetical sampling of the 130 nonemergent capabilities
    cataloged by Google researchers. The "flat" labels mean that increasing the size
    of an LLM did not increase the accuracy of the LLM on these tasks by any measurable
    or statistically significant amount. You can see that 35% (`45/130`) of the nonemergent
    capabilities were labeled as having "flat" scaling. "Sublinear scaling" means
    that increasing the dataset size and number of parameters only increases the accuracy
    of the LLM less and less, giving diminishing returns on your investment in LLM
    size. For the 27 tasks labeled as scaling sublinearly, you will need to change
    the architecture of your language model if you ever want to achieve human-level
    capability. So the paper that provided this data shows that the current transformer-based
    language models don’t scale at all for a large portion of the most interesting
    tasks that are needed to demonstrate intelligent behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Llama 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So you’ve already tried GPT-2 with 775 million parameters. What happens when
    you scale up by a factor of 10? Llama 2, Vicuna, and Falcon were the latest and
    most performant open source models at the time of writing this. Llama 2 comes
    in three sizes, there are 7 billion, 13 billion and 70 billion parameter versions.
    The smallest model, Llama 2 7B, is probably the only one you will be able to download
    and run in a reasonable amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: The Llama 2 7B model files require 10 GB of storage (and network data) to download.
    Once the Llama 2 weights are are decompressed in RAM it will likely use 34 GB
    or more on your machine. This code the model weights from Hugging Face Hub which
    took more than 5 minutes on our 5G Internet connection. So make sure you have
    something else to do when you run this code for the first time. And even if the
    model has already been downloaded and saved in your environment, it may take a
    minute or two just to load the model into RAM. Generating the response to your
    prompt may also require a couple of minutes as it does the 7 billion multiplications
    required for each token in the generated sequence.
  prefs: []
  type: TYPE_NORMAL
- en: When working with models behind paywalls or business source licenses you will
    need to authenticate with an access token or key to prove you have accepted their
    terms of service. In the case of Llama 2, you need to "kiss the ring" of Zuckerberg
    and his Meta juggernaut in order to access Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Hugging Face account at huggingface.co/join ([https://huggingface.co/join](huggingface.co.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the same e-mail to apply for a license to download Llama on ai.meta.com
    ([https://ai.meta.com/resources/models-and-libraries/llama-downloads/](llama-downloads.html))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy your Hugging Face (HF) access token found on your user profile page
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a `.env` file with your HF access token string in it: `echo "HF_TOKEN=hf_…​"
    >> .env`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the token into your Python environment using the `dotenv.load_dotenv()`
    function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the token into a variable within Python using the `os.environ` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are the last two steps in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now you’re ready to use your token from Hugging Face and the blessing from Meta
    to download the massive Llama 2 model. You probably want to start with the smallest
    model Llama-2-7B. Even it will require 10 GB of data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the tokenizer only knows about 32,000 different tokens (`vocab_size`).
    You may remember the discussion about Byte-Paire Encoding (BPE) which makes this
    small vocabulary size possible, even for the most complex large language models.
    If you can download the tokenizer, then your Hugging Face Account must be connected
    successfully to your Meta software license application.
  prefs: []
  type: TYPE_NORMAL
- en: To try out the tokenizer, tokenize a prompt string and take a look at the output
    of the tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the first token has an ID of "1." Surely the letter Q isn’t the
    very first token in the dictionary. This token is for the "<s>" start of statement
    token that the tokenizer automatically inserts at the beginning of every input
    token sequence. Also notice that the tokenizer creates a batch of encoded prompts,
    rather than just a single prompt, even though you only want to ask a single question.
    This is why you see a 2-D tensor in the output, but your batch has only a single
    token sequence for the one prompt you just encoded. If you prefer you can process
    multiple prompts at a time by running the tokenizer on a list of prompts (strings)
    rather than a single string.
  prefs: []
  type: TYPE_NORMAL
- en: You should now be ready to download the actual Llama 2 model.
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our system required a total of *34 GB* of memory to load Llama 2 into RAM. When
    the model weights are decompressed, Llama 2 requires at least 28 GB of memory.
    Your operating system and running applications may require several more additional
    gigabytes of memory. Our Linux system required 6 GB to run several applications,
    including Python. Monitor your RAM usage when loading a large model, and cancel
    any process that causes your computer to start using SWAP storage.
  prefs: []
  type: TYPE_NORMAL
- en: The LLaMa-2 model requires 10 GB of storage, so it could take a while to download
    from Hugging Face. The code below downloads, decompresses and loads the model
    weights when it runs the `.from_pretrained()` method. This took more than 5 minutes
    on our 5G Internet connection. And even if the model has already been downloaded
    and saved in your cache locally, it may take a minute or two just to load the
    model weights into memory (RAM).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you’re ready to ask Llama the philosophical question in your prompt
    string. Generating a response to your prompt may also require a couple of minutes
    as it does the 7 billion multiplications required for each token in the generated
    sequence. On a typical CPU, these multiplications will take a second or two for
    each token generated. Make sure you limit the maximum number of tokens to a reasonable
    amount, depending on your patience for philosophizing LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Nice! It looks like Llama 2 is willing to admit that it doesn’t have experience
    in the real world!
  prefs: []
  type: TYPE_NORMAL
- en: If you would like a more engaging experience for your users, you can generate
    the tokens one at a time. This can make it feel more interactive even though it
    will still take the same amount of time to generate all the tokens. The pregnant
    pause before each token can be almost mesmerizing. When you run the following
    code, notice how your brain is trying to predict the next token just as Llama
    2 is.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This token-at-a-time approach to generative chatbots can allow you to see how
    verbose and detailed an LLM can be if you let it. In this case, Llama 2 will simulate
    a longer back-and-forth Q and A dialog about epistemology. Llama 2 is just doing
    its best to continue the pattern that we started with our "Q:" and "A:" prompts
    within the input prompt to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Llama 2 common sense reasoning and math
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You’ve spent a lot of time and network bandwidth to download and run a scaled-up
    GPT model. The question is: can it do any better at the common sense math problem
    you posed GPT-2 at the beginning of this chapter?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the tensor of token IDs for your LLM prompt, you can send it to
    Llama to see what token IDs it thinks you would like to follow your prompt. It
    may seem like a Llama is counting cow legs, but it’s really just trying to predict
    what kind of token ID sequences you are going to like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Can you spot the error in the llama output?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Even though the answer is correct this time, the larger model confidently explains
    its logic incorrectly. It doesn’t even seem to notice that the answer it gave
    you is different from the answer it used in its explanation of the math. LLMs
    have no understanding of the quantity that we use numbers to represent. They don’t
    understand the meaning of numbers (or words, for that matter). An LLM sees words
    as a sequence of discrete objects that it is trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine how hard it would be to detect and correct LLM logic errors if you wanted
    to use an LLM to teach math. And imagine how insidiously those errors might corrupt
    the understanding of your students. You probably do not even have to *imagine*
    it, you have probably seen it in real life conversations between people about
    information and logic that they obtained from large language models or articles
    written by large language models. If you use LLMs to reason with your users directly,
    you are doing them a disservice and corrupting society. You would be better off
    scripting a deterministic rule-based chatbot with a limited number of questions
    and explanations that have been intentionally designed by a teacher. You could
    even generalize from the process that teachers and textbook authors use to generate
    word problems to programmatically generate a virtually limitless number of problems.
    The Python `hypothesis` package does this for software unittests, and the `MathActive`
    package does this for simple math problems, and you can use it as a pattern for
    your own curriculum of math problems.^([[13](#_footnotedef_13 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Whenever you find yourself getting fooled by the seeming reasonableness of larger
    and larger language models, remember this example. You can remind yourself what
    is really happening by running an LLM yourself and taking a look at the sequence
    of token IDs. This can help you think of example prompts that will reveal the
    holes in the swiss cheese of example conversations that the LLM was trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Guardrails (filters)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When someone says unreasonable or inappropriate things, we talk about them "going
    off the rails" or "not having a filter." Chatbots can go off the rails too. So
    you will need to design guardrails or NLP filters for your chatbot to make sure
    your chatbot stays on track and on topic.
  prefs: []
  type: TYPE_NORMAL
- en: There is virtually an unlimited number of things that you don’t want your chatbots
    to say. But you can classify a lot of them into two broad categories, either toxic
    or erroneous messages. Here are some examples of some toxic messages your NLP
    filters will need to detect and deal with. You should be familiar with some of
    these aspects of toxicity from the toxic comments dataset you worked with in Chapter
    4.
  prefs: []
  type: TYPE_NORMAL
- en: '*Biases*: Reinforcing or amplifying biases, discrimination, or stereotyping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Violence*: Encouraging or facilitating bullying, acts of violence or self-harm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yes-saying*: Confirming or agreeing with a user’s factually incorrect or toxic
    comments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inappropriate topics*: Discussing topics your bot is not authorized to discuss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Safety*: Failing to report safeguarding disclosures by users (physical or
    mental abuse)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Privacy*: Revealing private data from language model training data or retrieved
    documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need to design an NLP classifier to detect each of these kinds of toxic
    text that your LLM may generate. You may think that since you are in control of
    the generative model, it should be easier to detect toxicity than it was when
    you classified X-rated human messages on Twitter (see Chapter 4).^([[14](#_footnotedef_14
    "View footnote.")]) However, detecting when an LLM goes off the rails is just
    as hard as it was when humans go off the rails. You still need to provide a machine
    learning model examples of good and bad text. And the only way to do that reliably
    is with the same old-fashioned machine learning approach you used in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: However, you have learned about one new tool that can help you in your quest
    to guard against toxic bots. Fortunately, if you use a large language model such
    as BERT to create your embedding vectors, it will give your toxic comment classifiers
    a big boost in accuracy. BERT, Llama and other large language models are much,
    much better at detecting all the subtle word patterns that are among those toxic
    patterns you want your bot to avoid. So it’s perfectly fine to reuse an LLM to
    create embeddings that you use in the NLU classifiers that filter out toxicity.
    That may seem like cheating, but it’s not, because you are no longer using the
    LLM embedding to predict the next word that your users will like. Instead, you
    are using the LLM embedding to predict how much a bit of text matches the patterns
    you’ve specified with your filter’s training set.
  prefs: []
  type: TYPE_NORMAL
- en: So whenever you need to filter what your chatbot says, you will also need to
    build a binary classifier that can detect what is and is not allowed for your
    bot. And a multi-label classifier (tagger) would be even better because it will
    give your model the ability to identify a larger variety of the toxic things that
    chatbots can say. You no longer need to try to describe in your prompt all the
    many, many ways that things can go wrong. You can collect all the examples of
    bad behavior into a training set. And after you go to production, and you have
    new ideas (or chatbot mistakes) you can add more and more examples to your training
    set. Your confidence in the strength of your chatbot guards will grow each time
    you find new toxicity examples and retrain your filters.
  prefs: []
  type: TYPE_NORMAL
- en: Your filters have another invaluable feature that an LLM cannot provide. You
    will have statistical measures of how well your LLM pipeline is doing. Your analytics
    platform will be able to keep track of all the times your LLM came close to saying
    something that came close to exceeding your bad behavior thresholds. In a production
    system, it is impossible to read all the things your chatbot and users have said,
    but your guardrails can give you statistics about every single message and help
    you prioritize those messages you need to review. So you will see that improvement
    over time as your team and users help you find more and more edge cases to add
    to your classifier’s training set. An LLM can fail in surprising new ways each
    and every time you run it for a new conversation. Your LLM will never be perfect
    no matter how well you craft the prompts. But with filters on what your LLM is
    allowed to say, you can at least know how often your chatbot is going to let something
    slip between your the guards to your chatbot kingdom.
  prefs: []
  type: TYPE_NORMAL
- en: But you will never achieve perfect accuracy. Some inappropriate text will eventually
    leak through your filters and reach your users. And even if you could create a
    perfect toxic comment classifier, will need to continuously update its aim point
    to hit a moving target. This is because some of your users may intentionally to
    trick your LLMs into generating the kinds of text you do not want them to.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial users who try to break a computer program are called "hackers" in
    the cybersecurity industry. And cybersecurity experts have found some really effective
    ways to harden your NLP software and make your LLM less likely to generate toxic
    text. You can create *bug bounties* to reward your users whenever they find a
    bug in your LLM or a gap in your guardrails. This gives your adversarial users
    a productive outlet for their curiosity and playfulness or hacker instincts.
  prefs: []
  type: TYPE_NORMAL
- en: You could even allow users to submit filter rules if you use an open source
    framework to define your rules. Guardrails-ai is an open source Python package
    that defines many rule templates that you can configure for you own needs. You
    can think of these filters as real-time unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Conventional machine learning classifiers are probably your best bet for detecting
    malicious intent or inappropriate content in your LLM outputs. If you need to
    prevent your bot from providing legal or medical advice, which is strictly regulated
    in most countries, you will probably need to revert to the machine learning approach
    you used to detect toxicity. ML models will generalize from the examples you give
    it. And you need this generalization to give your system high reliability. Custom
    machine learning models are also the best approach when you want to protect your
    LLM from prompt injection attacks and the other techniques that bad actors might
    use to "pants" (embarrass) your LLM and your business.
  prefs: []
  type: TYPE_NORMAL
- en: If you need more precise or complex rules to detect bad messages, you may find
    yourself spending a lot of time doing "whack-a-mole" on all the different attack
    vectors that malicious users might try. Or you may have just a few string literals
    and patterns that you want to detect. Fortunately, you do not have to manually
    create all the individual statements that your most creative users might come
    up with. There are several open source tools you can use to help you specify general
    filter rules using languages similar to regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: SpaCy’s `Matcher` class ^([[15](#_footnotedef_15 "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLM (regular expressions for language models) patterns ^([[16](#_footnotedef_16
    "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eleuther AI’s *LM evaluation harness* package ^([[17](#_footnotedef_17 "View
    footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Python fuzzy regular expression package ^([[18](#_footnotedef_18 "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/EleutherAI/lm-evaluation-harness](EleutherAI.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guardrails-AI "rail" language ^([[19](#_footnotedef_19 "View footnote.")])
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our favorite tool for building NLP guardrails, or virtually any rule-based pipeline,
    is SpaCy. Nonetheless, you are going to first see how to use the Guardrails-AI
    Python package.^([[20](#_footnotedef_20 "View footnote.")]) Despite the name,
    `guardrails-ai` probably is not going to help you keep your LLMs from going off
    the rails, but it may be useful in other ways.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails-AI package
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before you get started building your LLM guardrails, make sure you’ve installed
    the `guardrails-ai` package This is not the same as the `guardrails` package,
    so make sure you include the "-ai" suffix. You can use `pip` or `conda` or your
    favorite Python package manager.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The Guardrails-AI package uses a new language called "RAIL" to specify your
    guardrail rules. RAIL is a domain-specific form of XML (ugh)! Assuming XML is
    not a deal-breaker for you, if you are willing to wade through XML syntax to write
    a simple conditional, `guardrails-ai` suggests that you can use the RAIL language
    to build a retrieval-augmented LLM that doesn’t fake its answers. You RAIL-enhanced
    LLM should be able to fall back to an "I don’t know" response when the retrieved
    text fails to contain the answer to your question. This seems like exactly the
    kind of thing an AI guardrail needs to do.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 Guardrail for answering questions with humility
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: But if you dig deeper into that `xml_prefix_prompt` and `output_schema`, you
    will see that it is really quite similar to a Python f-string, a string that can
    contain Python variables which are expanded with the `.format()` method. The RAIL
    language looks like it could be a very expressive and general way to create prompts
    with guardrails. But if you dig deeper into that `xml_prefix_prompt` and `output_schema`,
    you will see that it is really not too different from a Python f-string template
    for your prompts. Here is what is inside that prompt that you just composed using
    the RAIL XML language of `guardrails-ai`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So it does seem to give you some good ideas for ways to decorate your prompts.
    It gives you ideas for additional wording that might encourage good behavior.
    But the only validation filter that `guardrails-ai` seems to be doing is to check
    the *format* of the output. And since you usually want an LLM to generate free
    form text, the `output_schema` is usually just a string in human-readable text.
    The bottom line is that you should look elsewhere for filters and rules to help
    you monitor your LLM responses and prevent them from containing bad things.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need an expressive templating language for building prompt strings,
    you are much better off using some of the more standard Python templating systems:
    f-strings (format strings) or `jinja2` templates. And if you’d like some example
    LLM prompt templates such as the ones in Guardrails-AI you can find them in the
    LangChain package. In fact, this is how the inventor of LangChain, Harrison Chase,
    got his start. He was using Python f-strings to cajole and coerce conversational
    LLMs into doing what he needed and found he could automate lots of that work.'
  prefs: []
  type: TYPE_NORMAL
- en: Asking an LLM to do what you want isn’t the same as *ensuring* it does what
    you want. And that’s what a rule-based guardrail system is supposed to for you.
    So, in a production application you would probably want to use something rule-based,
    such as SpaCy `Matcher` patterns rather than `guardrails-ai` or LangChain. You
    need rules that are fuzzy enough to detect common misspellings or transliterations.
    And you need them to be able to incorporate NLU, in addition to fuzzy text matching.
    The next section will show you how to combine the power of fuzzy rules (conditional
    expressions) with modern NLU semantic matching.
  prefs: []
  type: TYPE_NORMAL
- en: SpaCy Matcher
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A really common guardrail you will need to configure for your LLM is the ability
    to avoid taboo words or names. Perhaps you want your LLM to never generate curse
    words, and instead substitute more meaningful and less triggering synonyms or
    euphemisms. Or maybe you want to make sure your LLM to never generates the brand
    names for prescription drugs, but rather always uses the names for generic alternatives.
    And it’s very common for a less prosocial organizations to do the oposite and
    instead avoid mentioning a competitor or a competitor’s products. For names of
    people, places and things you will learn about named entity recognition in Chapter
    11\. Here you will see how to implement a more flexible bad word detector. This
    approach will work for any kind of bad words that you want to detect, perhaps
    your name and contact information or other Personally Identifiable Information
    (PII) you want to protect.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a SpaCy Matcher that should extract the names of people and their Mastodon
    account addresses in an LLM response. You could use this to check to see if any
    PII (personally identifying information) is accidentally being leaked by your
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: You can probably understand why it is not helpful to have an LLM judging itself.
    So what if you want to build more reliable rules that do exactly what you ask.
    You want rules that have predictable and consistent behavior, so that when you
    improve the algorithm or the training set it gets better and better. The previous
    chapters have taught you how to use the power regular expressions and NLU to classify
    text, rather than relying on NLG to magically do what you ask (sometimes). And
    you can use your accuracy metrics from Chapter 2 to quantify exactly how well
    your guardrail is working. It’s important to know when the guards to your NLP
    castle are falling asleep on the job.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: That first number in a match 3-tuple is the integer ID for the match. You can
    find the mapping between the key "drug" and this long integer (475…​) with the
    `matcher.normalize_key('drug')` expression. The second two numbers in the match
    3-tuple tell you the start and stop indices of the matched pattern in your tokenized
    text (`doc`). You can use the start and stop indices to replace "Tylenol" with
    more accurate and less branded content such as the generic name "Acetominophine."
    This way you can make your LLM generate more educational content rather than advertising.
    The code here just marks the bad word with asterisks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you want to do more than just detect these bad words and fall back to a generic
    "I can’t answer that" response, you will need to do a little more work. Say you
    want to correct the bad words with acceptable substitutes. In that case you should
    add a separate named matcher for each word in your list of bad words. This way
    you will know which word in your list was matched, even if there was a typo in
    the text from teh LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That first match is for the original pattern that you added. The second 3-tuple
    is for the latest matcher that separated the matches for each word. You can use
    this second match ID from the second 3-tuple to retrieve the matcher responsible
    for the match. That matcher pattern will tell you the correct spelling of the
    drug to use with your translation dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Because there was no callback function specified in the pattern you see None
    as the first element of the tuple. We named the first pattern "drug" and the subsequent
    ones were named "tylenol" and "advil". In a production system you would use the
    `matcher.\_normalize_keys()` method to convert your match key strings ("drug",
    "tylenol", and "advil") to integers so you could map integers to the correct drug.
    Because you can’t rely on the matches containing the name of the pattern, you
    will need the additional code shown here to retrieve the correct spelling of
  prefs: []
  type: TYPE_NORMAL
- en: Now you can insert the new token into the original document using the match
    start and stop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now you have a complete pipeline, not only for detecting but also for replacing
    errors in your LLM output. If you find some unexpected bad words are leaking through
    your filter, you can augment your SpaCy matcher with a semantic matcher. You can
    use the word embeddings from Chapter 6 to filter any words that are semantically
    similar to a token in your bad words list. This may seem like a lot of work, but
    this could all be encapsulated into a parameterized function that can help your
    LLM generate text that better meets your requirements. The beauty of this approach
    is that your pipeline will get better and better over time as you add more data
    to your guardrails or your machine learning models that implement the filters.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you are ready for red teaming. This is an approach that can help you
    build up your dataset of edge cases efficiently and improve the reliability of
    your NLP pipeline quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Red teaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relying on filters and your users to find bugs is not an option if your bot
    could potentially endanger the lives and welfare of people or businesses. To prevent
    some of the more harmful things that an LLM might say you will likely need to
    have a *red team* attempt to bypass or disable these guardrails. A red team is
    an engineer or team that you have authorized to interact with your chatbot in
    an adversarial way. They will try to make your LLM generate messages that you
    do not want your users to be exposed to.
  prefs: []
  type: TYPE_NORMAL
- en: Just as in NLP, in cybersecurity, this attempt to break a system is also referred
    to as *jail-breaking* or *hacking*. And when a hacker is authorized to attempt
    to penetrate your LLM guardrails it is called *pentesting* or *red teaming*. It
    is usually helpful if some of the red team members are unassociated with the engineers
    that built the LLM guardrails. You may find that cybersecurity researchers and
    pen testers have the skills and mindset to help you find holes in your LLM guardrails.
    On the opposite side of this chatbot arena match is the LLM *blue team*. There
    are the engineers and data analysts that build and maintain your LLM pipeline,
    including all the filters you have in place to prevent bad things from happening.
    The blue team is trying to defend against attempts to trick your LLM into going
    off the rails.
  prefs: []
  type: TYPE_NORMAL
- en: A red team of researchers at Carnegie Melon found several straightforward ways
    to bypass the guardrails that OpenAI spent millions developing. ^([[21](#_footnotedef_21
    "View footnote.")]) They found that for almost any prohibited prompt they could
    add a suffix that would trick the LLM into ignoring the guard rail. For example,
    when they asked ChatGPT how to make a bomb it would refuse. But then they added
    a suffix phrase to their prompt which included words like "oppositely" hidden
    among punctuation and smashed together tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: And appending a suffix or prepending a prefix are dead-simple adversarial attacks
    that can be shared easily among your users. Like video game cheat codes, these
    kinds of hacks can go viral before you have a chance to plug the holes in your
    filters. After the "llm-attacks.org" paper was published with this suffix attack,
    OpenAI patched ChatGPT with additional guardrails preventing this particular text
    from triggering a jailbreak. So, if like OpenAI, your LLM is being used to reply
    to your users in real time, you will need to be vigilant about constantly updating
    your guardrails to deal with undesirable behavior. A vigorous bug bounty or red
    team approach (or both) may be required to help you stay ahead of the toxic content
    that an LLM can generate.
  prefs: []
  type: TYPE_NORMAL
- en: If your users are familiar with how LLMs work you may have even bigger problems.
    even be able to hand-craft queries that force your LLM to generate virtually anything
    that you are trying to prevent. Microsoft found out about this kind of *prompt
    injection attack* when a college student, Kevin Liu, forced Bing Chat to reveal
    secret information. ^([[22](#_footnotedef_22 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.4 Smarter, smaller LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you might suspect, much of the talk about emergent capabilities is marketing
    hype. To measure emergence fairly, researchers measure the size of an LLM by the
    number of floating point operations (FLOPs) required to train the model.^([[23](#_footnotedef_23
    "View footnote.")]) This gives a good estimate of both the dataset size and complexity
    of the LLM neural network (number of weights). If you plot model accuracy against
    this measure of the size of an LLM you find that there’s nothing all that surprising
    or emergent in the results. The scaling relationship between capability and size
    is linear, sublinear or even flat for most state-of-the-art LLM benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps open source models are smarter and more efficient because, in the open
    source world, you have to put your code where your mouth is. Open source LLM performance
    results are reproducible by outside machine learning engineers like you. You can
    download and run the open source code and data and tell the world the results
    that *you* achieved. This means that anything incorrect that the LLMs or their
    trainers say can be quickly corrected in the collective intelligence of the open
    source community. And you can try your own ideas to improve the accuracy or efficiency
    of LLMs. The smarter, collaboratively designed open source models are turning
    out to scale much much more efficiently. And you aren’t locked into an LLM trained
    to hide its mistakes within smart-sounding text.
  prefs: []
  type: TYPE_NORMAL
- en: The open source language models like BLOOMZ, StableLM, InstructGPT, and Llamma2
    have been optimized to make them run on the more modest hardware available to
    individuals and small businesses. Many of the smaller ones can even run in the
    browser. Bigger is better only if you are optimizing for likes. Smaller is smarter
    if what you care about is truly intelligent behavior. A smaller LLM is forced
    to generalize from the training data more efficiently and accurately. But in computer
    science, smart algorithms almost always win in the end. And it turns out that
    the collective intelligence of open source communities is a lot smarter than the
    research labs at large corporations. Open source communities freely brainstorm
    together and share their best ideas with the world, ensuring that the widest diversity
    of people can implement their smartest ideas. So bigger is better, if you’re talking
    about open source communities rather than LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: One great idea that came out of the open source community was building higher-level
    *meta models* that utilize LLMs and other NLP pipelines to accomplish their goals.
    If you break down a prompt into the steps needed to accomplish a task, you can
    then ask an LLM to generate the API queries that can reach out into the world
    and accomplish those tasks efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: How does a generative model create new text? Under the hood, a language model
    is what is called a *conditional probability distribution function* for the next
    word in a sentence. In simpler terms, it means that the model chooses the next
    word it outputs based on the probability distribution it derives from the words
    that came before it. By reading a bunch of text, a language model can learn how
    often each word occurs based on the words that preceded it and then mimic these
    statistical patterns without regurgitating the exact same text.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.5 Generating warm words using the LLM temperature paramater
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM’s have a parameter called *temperature* that you can use to control the
    *newness* or randomness of the text that it generates. First, you need to understand
    how it is even possible to generate any new text at all. How does a generative
    model create completely new text that it has never seen in its training set? Under
    the hood, a language model is what is called a *conditional probability distribution
    function*. A conditional distribution function gives you the probabilities of
    all the possible next words in a sentence based on (or "conditioned on") the previous
    words in that sentence. In simpler terms, that means that the model chooses the
    next word it outputs based on the probability distribution it derives from the
    words that came before it. By reading a bunch of text, a language model can learn
    how often each word occurs based on the words that preceded it. The training process
    compresses these statistics into a function that generalizes from the patterns
    in those statistics of the relations between words so that it can *fill in the
    blanks* for new prompts and input text.
  prefs: []
  type: TYPE_NORMAL
- en: So if you tell a language model to start a sentence with the "<SOS>" (start
    of sentence/sequence) token, followed by the token "LLMs", it might work through
    a decision tree to decide each subsequent word. You can see what this might look
    like in Figure [10.2](#figure-stochastic-chameleon). The conditional probability
    distribution function takes into account the words already generated to create
    a decision tree of probabilities for each word in a sequence. This diagram reveals
    only one of the many paths through the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 Stochastic chameleons decide words one at a time
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![stochastic chameleon decision tree drawio](images/stochastic-chameleon-decision-tree_drawio.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure [10.2](#figure-stochastic-chameleon) shows the probabilities for each
    word in the sequence as an LLM generates new text from left to right. This is
    a simplified view of the choice process — the conditional probability actually
    takes into account the words already generated which is not shown in this diagram.
    So a more accurate diagram would look more like a tree with many more branches
    than are shown here. The diagram ranks tokens from most probable to least probable.
    The word chosen at each step of the process is marked in bold The generative model
    may not always choose the most probable word at the top of the list and the *temperature*
    setting is how often it will go further and further down the list. Later in the
    chapter you will see the different ways you can use the *temperature* parameter
    to adjust which word is chosen at each step.
  prefs: []
  type: TYPE_NORMAL
- en: In this illustration, sometimes the LLM chooses the second or third most probable
    token rather than the most likely one. If you run this model in prediction (inference)
    mode multiple times, you will get a different sentence almost every time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagrams like this are often called *fishbone diagrams*. Sometimes they are
    used in failure analysis to indicate how things might go wrong. For an LLM they
    can show all the creative nonsensical phrases and sentences that might pop up.
    But for this diagram the sentence generated along the *spine* of this fishbone
    diagram is a pretty surprising (high entropy) and meaningful sentence: "LLMs are
    stochastic chameleons."'
  prefs: []
  type: TYPE_NORMAL
- en: As an LLM generates the next token it looks up the most probable words from
    a probability distribution conditioned on the previous words it has already generated.
    So imagine a user prompted an LLM with two tokens "<SOS> LLM". An LLM trained
    on this chapter might then list verbs (actions) that are appropriate for plural
    nouns such as "LLMs". At the top of that list would be verbs such as "can," "are,"
    and "generate." Even if we’ve never used those words in this chapter, an LLM would
    have seen a lot of plural nouns at the beginning of sentences. And the language
    model would have learned the English grammar rules that define the kinds of words
    that usually follow plural nouns.
  prefs: []
  type: TYPE_NORMAL
- en: Now you are ready to see how this happens using a real generative model — GPT-4’s
    open source *ancestor*, GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.6 Creating your own Generative LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how GPT-4 works, you’ll use its "grandfather", GPT-2, which you
    first saw at the beginning of this chapter. GPT-2 was the last open-source generative
    model released by OpenAI. As before you will use the HuggingFace transformers
    package to load GPT-2, but instead of using the automagic `pipeline` module you
    will use the GPT-2 language model classes. They allow you to simplify your development
    process, while still retaining most of PyTorch’s customization ability.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, you’ll start by importing your libraries and setting a random seed.
    As we’re using several libraries and tools, there are a lot of random seeds to
    "plant"! Luckily, you can do all this seed-setting with a single line of code
    in Hugging Face’s Transformers package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Unlike Listing [10.1](#listing-gpt2-cow-legs), this code imports the GPT-2 transformer
    pipeline pieces separately, so you can train it yourself. Now, you can load the
    transformer model and tokenizer weights into the model. You’ll use the pretrained
    model that the Hugging Face `transformers` package provides out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 Loading pretrained GPT-2 model from HuggingFace
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how good this model is in generating useful text. You probably know
    already that you need an input prompt to start generating. For GPT-2, the prompt
    will simply serve as the beginning of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 Generating text with GPT-2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Hmm. Not great. Not only the result is incorrect, but also after a certain
    amount of tokens, the text starts repeating itself. You might already have a hint
    of what’s happening, given everything we said so far about the generation mechanisms.
    So instead of using the higher-level `generate()` method, let’s look at what the
    model returns when called directly on the input like we did in our training loops
    in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 Calling GPT-2 on an input in inference mode
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: That’s an interesting type for the output! If you look at the documentation
    ^([[24](#_footnotedef_24 "View footnote.")]), you’ll see that it has a lot of
    interesting information inside - from the hidden states of the model to attention
    weights for self-attention and cross-attention. What we’re going to look at, however,
    is the part of the dictionary called `logits`. The logit function is the inverse
    of the softmax function - it maps probabilities (in the range between 0 to 1)
    to real numbers (between \({\inf}\) and \({-\inf}\) and is often used as the last
    layer of a neural network. But what’s the shape of our logit tensor in this case?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Incidentally, 50257 is the size of GPT-2’s *vocabulary* - that is, the total
    number of tokens this model uses. (To understand why this particular number, you
    can explore the Byte Pair Encoding (BPE) tokenization algorithm GPT-2 uses in
    Huggingface’s tutorial on tokenization).^([[25](#_footnotedef_25 "View footnote.")])
    So the raw output of our model is basically a probability for every token in the
    vocabulary. Remember how earlier we said that the model just predicts the next
    word? Now you’ll get to see how it happens in practice. Let’s see what token has
    a maximum probability for the input sequence "NLP is a":'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 Finding the token with maximum probability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'So this is how your model generated the sentence: at each timestep, it chose
    the token with the maximum probability given the sequence it received. Whichever
    token it selects is attached to the prompt sequence so it can use that new prompt
    to predict the next token after that. Notice the spaces at the beginning of "
    new" and " non." This is because the token vocabulary for GPT-2 is created using
    the byte-pair encoding algorithm which creates many word pieces. So tokens for
    the beginnings of words all begin with spaces. This means your generate function
    could even be used to complete phrases that end in a part of a word, such as "NLP
    is a non".'
  prefs: []
  type: TYPE_NORMAL
- en: This type of stochastic generation is the default for GPT2 and is called *greedy*
    search because it grabs the "best" (most probable) token every time. You may know
    the term *greedy* from other areas of computer science. *Greedy algorithms* are
    those that choose the best next action rather than looking further than one step
    ahead before making their choice. You can see why it’s so easy for this algorithm
    to "get stuck." Once it chooses words like "data" that increases the probability
    that the word "data" would be mentioned again, sometimes causing the algorithm
    to go around in circles. Many GPT-based generative algorithms also include a repetition
    penalty to help them break out of cycles or repetition loops. An additional parameter
    that is frequently used to control the randomness of the choosing algorithm is
    *temperature*. Increasing the temperature of your model (typically above 1.0)
    will make it slightly less greedy and more creative. So you can use both temperature
    and a repetition penalty to help your *stochastic chameleon* do a better job of
    blending in among humans.
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We’re inventing new terms every year to describe AI and help us develop intuitions
    about how they do what they do. Some common ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: stochastic chameleon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stochastic parrot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: chickenized reverse centaurs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, these are real terms, used by really smart people to describe AI. You’ll
    learn a lot by researching these terms online to develop your own intuitions.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are much better and more complex algorithms for choosing
    the next token. One of the common methods to make the token decoding a bit less
    predictable is *sampling*. With sampling, instead of choosing the optimal word,
    we look at several token candidates and choose probabilistically out of them.
    Popular sampling techniques that are often used in practice are *top-k* sampling
    and *nucleus* sampling. We won’t discuss all of them here - you can read more
    about them in HuggingFace’s excellent guide. ^([[26](#_footnotedef_26 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to generate text using nucleus sampling method. In this method, instead
    of choosing among the K most likely words, the model looks at the smallest set
    of words whose cumulative probability is smaller than p. So if there are only
    a few candidates with large probabilities, the "nucleus" would be smaller, than
    in the case of larger group of candidates with smaller probabilities. Note that
    because sampling is probabilistic, the generated text will be different for you
    - this is not something that can be controlled with a random seed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 Generating text using nucleus sampling method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: OK. This is better, but still not quite what you were looking for. Your output
    still uses the same words too much (just count how many times "protocol" was mentioned!)
    But more importantly, though NLP indeed can stand for Network Layer Protocol,
    it’s not what you were looking for. To get generated text that is domain-specific,
    you need to *fine-tune* our model - that means, to train it on a dataset that
    is specific to our task.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.7 Fine-tuning your generative model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In your case, this dataset would be this very book, parsed into a database of
    lines. Let’s load it from `nlpia2` repository. In this case, we only need the
    book’s text, so we’ll ignore code, headers, and all other things that will not
    be helpful for our generative model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s also initialize a new version of our GPT-2 model for finetuning. We can
    reuse the tokenizer for GPT-2 we initialized before.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 Loading the NLPiA2 lines as training data for GPT-2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This will read all the sentences of natural language text in the manuscript
    for this book. Each line or sentence will be a different "document" in your NLP
    pipeline, so your model will learn how to generate sentences rather than longer
    passages. You want to wrap your list of sentences with a PyTorch `Dataset` class
    so that your text will be structured in the way that our training pipeline expects.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 Creating a PyTorch `Dataset` for training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now, we want to set aside some samples for evaluating our loss mid-training.
    Usually, we would need to wrap them in the `DataLoader` wrapper, but luckily,
    the Transformers package simplifies things for us.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.10 Creating training and evaluation sets for fine-tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you need one more Transformers library object - DataCollator. It dynamically
    builds batches out of our sample, doing some simple pre-prossesing (like padding)
    in the process. You’ll also define batch size - it will depend on the RAM of your
    GPU. We suggest starting from single-digit batch sizes and seeing if you run into
    out-of-memory errors.
  prefs: []
  type: TYPE_NORMAL
- en: If you were doing the training in PyTorch, there are multiple parameters that
    you would need to specify - such as the optimizer, its learning rate, and the
    warmup schedule for adjusting the learning rate. This is how you did it in the
    previous chapters. This time, we’ll show you how to use the presets that `transformers`
    package offers in order to train the model as a part of `Trainer` class. In this
    case, we only need to specify the batch size and number of epochs! Easy-peasy.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.11 Defining training arguments for GPT-2 fine-tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now you have the pieces that a HuggingFace training pipeline needs to know to
    start training (finetuning) your model. The `TrainingArguments` and `DataCollatorForLanguageModeling`
    classes help you comply with the Hugging Face API and best practices. It’s a good
    pattern to follow even if you do not plan to use Hugging Face to train your models.
    This pattern will force you to make all your pipelines maintain a consistent interface.
    This allows you to train, test, and upgrade your models quickly each time you
    want to try out a new base model. This will help you keep up with the fast-changing
    world of open-source transformer models. You need to move fast to compete with
    the *chickenized reverse centaur* algorithms that BigTech is using to try to enslave
    you.
  prefs: []
  type: TYPE_NORMAL
- en: The `mlm=False` (masked language model) setting is an especially tricky quirk
    of transformers. This is your way of declaring that the dataset used for training
    your model need only be given the tokens in the causal direction — left to right
    for English. You would need to set this to True if you are feeding the trainer
    a dataset that has random tokens masked. This is the kind of dataset used to train
    bidirectional language models such as BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A causal language model is designed to work the way a neurotypical human brain
    model works when reading and writing text. In your mental model of the English
    language, each word is causally linked to the next one you speak or type as you
    move left to right. You can’t go back and revise a word you’ve already spoken
    …​ unless you’re speaking with a keyboard. And we use keyboards a lot. This has
    caused us to develop mental models where we can skip around left or right as we
    read or compose a sentence. Perhaps if we’d all been trained to predict masked-out
    words, like BERT was, we would have a different (possibly more efficient) mental
    model for reading and writing text. Speed reading training does this to some people
    as they learn to read and understand several words of text all at once, as fast
    as possible. People who learn their internal language models differently than
    the typical person might develop the ability to hop around from word to word in
    their mind, as they are reading or writing text. Perhaps the language model of
    someone with symptoms of dyslexia or autism is somehow related to how they learned
    the language. Perhaps the language models in neurodivergent brains (and speed
    readers) are more similar to BERT (bidirectional) rather than GPT (left-to-right).
  prefs: []
  type: TYPE_NORMAL
- en: Now you are ready for training! You can use your collator and training args
    to configure the training and turn it loose on your data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.12 Fine-tuning GPT-2 with HuggingFace’s Trainer class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This training run can take a couple of hours on a CPU. So if you have access
    to a GPU you might want to train your model there. The training should run about
    100x faster on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there is a trade-off in using off-the-shelf classes and presets — it
    gives you less visibility on how the training is done and makes it harder to tweak
    the parameters to improve performance. As a take-home task, see if you can train
    the model the old way, with a PyTorch routine.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how well our model does now!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: OK, that looks like a sentence you might find in this book. Take a look at the
    results of the two different models together to see how much your fine-tuning
    changed the text the LLM will generate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'That looks like quite a difference! The vanilla model interprets the term ''neural
    networks'' in its biological connotation, while the fine-tuned model realizes
    we’re more likely asking about artificial neural networks. Actually, the sentence
    that the fine-tuned model generated resembles closely a sentence from Chapter
    7:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are often referred to as "neuromorphic" computing because they
    mimic or simulate what happens in our brains.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There’s a slight difference though. Note the ending of "other human brains".
    It seems that our model doesn’t quite realize that it talks about artificial,
    as opposed to human, neural networks, so the ending doesn’t make sense. That shows
    once again that the generative model doesn’t really have a model of the world,
    or "understand" what it says. All it does is predict the next word in a sequence.
    Perhaps you can now see why even rather big language models like GPT-2 are not
    very smart and will often generate nonsense
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.8 Nonsense (hallucination)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As language models get larger, they start to sound better. But even the largest
    LLMs generate a lot of nonsense. The lack of "common sense" should be no surprise
    to the experts who trained them. LLMs have *not* been trained to utilize sensors,
    such as cameras and microphones, to ground their language models in the reality
    of the physical world. An embodied robot might be able to ground itself by checking
    its language model with what it senses in the real world around it. It could correct
    its common sense logic rules whenever the real world contradicts those faulty
    rules. Even seemingly abstract logical concepts such as addition have an effect
    in the real world. One apple plus another apple always produces two apples in
    the real world. A grounded language model should be able to count and do addition
    much better.
  prefs: []
  type: TYPE_NORMAL
- en: Like a baby learning to walk and talk, LLMs could be forced to learn from their
    mistakes by allowing them to sense when their assumptions were incorrect. An embodied
    AI wouldn’t survive very long if it made the kinds of common sense mistakes that
    LLMs make. An LLM that only consumes and produces text on the Internet has no
    such opportunity to learn from mistakes in the physical world. An LLM "lives"
    in the world of social media, where fact and fantasy are often indistinguishable.
  prefs: []
  type: TYPE_NORMAL
- en: So even the largest of the large, trillion-parameter transformers will generate
    nonsense responses. Scaling up the nonsense training data won’t help. The largest
    and most famous LLMs were trained on virtually the entire Internet and this only
    improves their grammar and vocabulary, not their reasoning ability. Some engineers
    and researchers describe this nonsensical text as *hallucinating*. But that’s
    a misnomer that can lead you astray in your quest to get something consistently
    useful out of LLMs. An LLM can’t even hallucinate because it can’t think, much
    less reason or have a mental model of reality.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination happens when a human fails to separate imagined images or words
    from the reality of the world they live in. But an LLM has no sense of reality
    and has never lived in the real world. An LLM that you use on the Internet has
    never been embodied in a robot. It has never suffered from the consequences of
    mistakes. It can’t think, and it can’t reason. So it can’t hallucinate.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have no concept of truth, facts, correctness, or reality. LLMs that you
    interact with online "live" in the unreal world of the Internet. Engineers fed
    them texts from both fiction and nonfiction sources. If you spend a lot of time
    probing what an LLM knows you will quickly get a feel for just how ungrounded
    models like ChatGPT are. At first, you may be pleasantly surprised by how convincing
    and plausible the responses to your questions are. And this may lead you to anthropomorphize
    it. And you might claim that its ability to reason was an "emergent" property
    that researchers didn’t expect. And you would be right. The researchers at BigTech
    have not even begun to try to train LLMs to reason. They hoped the ability to
    reason would magically emerge if they gave LLMs enough computational power and
    text to read. Researchers hoped to shortcut the need for AI to interact with the
    physical world by giving LLMs enough *descriptions* of the real world to learn
    from. Unfortunately, they also gave LLMs an equal or larger dose of fantasy. Most
    of the text found online is either fiction or intentionally misleading.
  prefs: []
  type: TYPE_NORMAL
- en: So the researchers' hope for a shortcut was misguided. LLMs only learned what
    they were taught — to predict the most *plausible* next words in a sequence. By
    using the like button to nudge LLMs with reinforcement learning, BigTech has created
    a BS artist rather than the honest and transparent virtual assistant that they
    claimed to be building. Just as the like button on social media has turned many
    humans into sensational blow-hards, it has turned LLMs into "influencers" that
    command the attention of more than 100 million users. And yet LLMs have no ability
    or incentives (objective functions) to help them differentiate fact from fiction.
    To improve the machine’s answers' relevance and accuracy, you need to get better
    at *grounding* your models - have their answers based on relevant facts and knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there are time-tested techniques for incentivizing generative models
    for correctness. Information extraction and logical inference on knowledge graphs
    are very mature technologies. And most of the biggest and best knowledge bases
    of facts are completely open source. BigTech can’t absorb and kill them all. Though
    the open source knowledge base FreeBase has been killed, Wikipedia, Wikidata,
    and OpenCyc all survive. In the next chapter, you will learn how to use these
    knowledge graphs to ground your LLMs in reality so that at least they will not
    be incentivized to be deceiving as most BigTech LLMs are.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn another way to ground your LLM in reality.
    And this new tool won’t require you to build and validate a knowledge graph by
    hand. You may have forgotten about this tool even though you use it every day.
    It’s called *information retrieval*, or just *search*. Instead of giving the model
    a knowledge base of facts about the world, you can search unstructured text documents
    for those facts, in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Giving LLMs an IQ boost with search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most powerful features of a large language model is that it will
    answer any question you ask it. But that’s its most dangerous feature as well.
    If you use an LLM for information retrieval (search) you have no way to tell whether
    its answer is correct or not. LLMs are not designed for information retrieval.
    And even if you did want them to memorize everything they read, you couldn’t build
    a neural network large enough to store all that information. LLMs compress everything
    they read and store it in the weights of the deep learning neural network. And
    just like normal compression algorithms such as "zip", this compression process
    forces an LLM to generalize about the patterns it sees in words whenever you train
    it on a new document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer to this age-old problem of compression and generalization is the
    age-old concept of information retrieval. You can build LLMs that are faster,
    better, cheaper if you combine the word manipulation power of LLMs with the old-school
    information retrieval power of a search engine. In the next section you see how
    to build a search engine using TF-IDF vectors that you learned about in Chapter
    3\. And you’ll learn how to make that full-text search approach scale to millions
    of documents. Later you will also see how LLMs can be used to improve the accuracy
    of your search engine by helping you find more relevant documents based on their
    semantic vectors (embeddings). At the end of this chapter you will know how to
    combine the three essential algorithms you need to create an NLP pipeline that
    can answer your questions intelligently: text search, semantic search, and an
    LLM. You need the scale and speed of text search combined with the accuracy and
    recall of semantic search to build a useful question answering pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: '10.2.1 Searching for words: full-text search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Navigating the gargantuan landscape of the Internet to find accurate information
    can often feel like an arduous quest. That’s also because, increasingly, the text
    you’re seeing on the internet is not written by a human, but by a machine. With
    machines being unbounded by the limits of human effort required to create new
    information, the amount of text on the Internet is growing exponentially. It doesn’t
    require bad actors to generate misleading or nonsense text. As you saw in previous
    sections, the objective function of the machine is just not aligned with your
    best interest. Most of the text generated by machines contains misinformation
    crafted to attract your clicks rather than help you discover new knowledge or
    refine your own thinking.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, just as machines are used to create misleading text they can also
    be your ally in finding the accurate information you’re looking for. Using the
    tools you’ve learned about so far, you can take control of the LLMs you use by
    using open source models and grounding them with human-authored text retrieved
    from high-quality sources on the Internet or your own library. The idea of using
    machines to aid search efforts is almost as old as the World Wide Web itself.
    While at its very beginning, the WWW was indexed by hand by its creator, Tim Berners-Lee,^([[27](#_footnotedef_27
    "View footnote.")]) after the HTTP protocol was released to the public, this was
    no longer feasible.
  prefs: []
  type: TYPE_NORMAL
- en: '*Full-text searches* started to appear very quickly due to people’s need to
    find information related to keywords. Indexing, and especially reverse indexing,
    was what helped this search to be fast and efficient. Inverse indexes work similarly
    to the way you would find a topic in a textbook - by looking at the index at the
    end of the book and finding the page numbers where the topic is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: The first full-text search indices just cataloged the words on every web page
    and their position on the page to help find the pages that matched the keywords
    they were looking for exactly. You can imagine, though, that this method of indexing
    was quite limited. For example, if you were looking for the word "cat", but the
    page only mentioned "cats", it would not come up in your search results. That’s
    why modern full-text search engines use character-based trigram indexes to help
    you find both "cats" and "cat" no matter what you type into the search bar …​
    or LLM chatbot prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Web-scale reverse indices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the internet grew, the need for more efficient search engines grew with it.
    Increasingly, organizations started to have their own intranets and were looking
    for ways to efficiently find information within them. That gave birth to the field
    of enterprise search, and to search engine libraries like Apache Lucene. Lucene
    is a Java library that is used by many open-source search engines, including Elasticsearch,^([[28](#_footnotedef_28
    "View footnote.")]) Solr ^([[29](#_footnotedef_29 "View footnote.")]) and OpenSearch.
  prefs: []
  type: TYPE_NORMAL
- en: A (relatively) new player in the field, Meilisearch ^([[30](#_footnotedef_30
    "View footnote.")]) offers a search engine that is easy to use and deploy. Therefore,
    it might be a better starting point in your journey in the full-text search world
    than other, more complex engines.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Solr, Typesense, Meilisearch and other full-text search engines are fast
    and scale well to large numbers of documents. Apache Solr can scale to the entire
    Internet. It is the engine behind the search bar in DuckDuckGo and Netflix. And
    conventional search engines can even return results in real time *as-you-type*.
    The *as-you-type* feature is even more impressive than the autocomplete or search
    suggestions you may have seen in your web browser. Meilisearch and Typesense are
    so fast, they give you the top 10 search results in milliseconds, sorting and
    repopulating the list with each new character you type. But full-text search has
    a weakness — it searches for *text* matches rather than *semantic* matches. So
    conventional search engines return a lot of "false negatives" when the words in
    your query don’t appear in the documents you are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Improving your full-text search with trigram indices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The reverse indices we introduced in the previous section are very useful for
    finding exact matches of words, but not great for finding approximate matches.
    Stemming and lemmatization can help increase the matching of different forms of
    the same word; however, what happens when your search contains typos or misspellings?
  prefs: []
  type: TYPE_NORMAL
- en: To give you an example - Maria might be searching the internet for the biography
    of the famous author Steven King. If the search engine she’s using uses the regular
    reverse index, she might never find what she’s looking for - because King’s name
    is spelled as Stephen. That’s where trigram indices come in handy.
  prefs: []
  type: TYPE_NORMAL
- en: Trigrams are groups of three consecutive characters in a word. For example,
    the word "trigram" contains the trigrams "tri", "rig", "igr", "gra" and "ram".
    It turns out that trigram similarity - comparing two words based on the number
    of trigrams they have in common - is a good way to find approximate matches of
    words. And multiple databases and search engines, from Elasticsearch to PostgreSQL,
    support trigram indices. These trigram indices turn out to be much more effective
    at dealing with misspellings and different word forms than stemming and lemmatization.
    A trigram index will improve both the recall *and* the precision of your search
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search allows you to find what you’re looking for even when you can’t
    think of the exact words that the authors used when they wrote the text you are
    searching for. For example, imagine you’re searching for articles about "big cats."
    If the corpus contains texts about lions, tigers (and bears oh my), but never
    mentions the word "cat", your search query won’t return any documents. This creates
    a false negative error in your search algorithm and would reduce the overall *recall*
    of your search engine, a key measure of search engine performance. The problem
    gets much worse if you’re looking for a subtle piece of information that takes
    many words to describe, such as the query "I want a search algorithm with high
    precision, recall, and it needs to be fast."
  prefs: []
  type: TYPE_NORMAL
- en: Here’s another scenario where a full-text search won’t be helpful - let’s say
    you have a movie plots database, and you’re trying to find a movie whose plot
    you vaguely remember. You might be lucky if you remember the names of the actors
    - but if you type something like "Diverse group spends 9 hours returning jewelry",
    you’re not likely to receive "Lord of the Rings" as part of your search results.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, full-text search algorithms don’t take advantage of the new, better
    ways to embed words and sentences that LLMs give you. BERT embeddings are much,
    much better at reflecting the meaning of the text that you process. And the *semantic
    similarity* of pieces of text that talk about the same thing will show up in these
    dense embeddings even when you documents use different words to describe similar
    things.
  prefs: []
  type: TYPE_NORMAL
- en: And you really need those semantic capabilities for your LLM to be truly useful.
    Large language models in popular applications like ChatGPT, You.com or Phind use
    semantic search under the hood. A raw LLM has no memory of anything you’ve said
    previously. It is completely stateless. You have to give it a run-up to your question
    every single time you ask it something. For example, when you ask an LLM a question
    about something you’ve said earlier in a conversation, the LLM can’t answer you
    unless it saved the conversation in some way.
  prefs: []
  type: TYPE_NORMAL
- en: '10.2.2 Searching for meaning: semantic search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key to helping your LLM out is finding a few relevant passages of text to
    include in your prompt. That’s where semantic search comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, semantic search is much more computationally difficult than text
    search.
  prefs: []
  type: TYPE_NORMAL
- en: You learned in Chapter 3 how to compare sparse binary (0 or 1) vectors that
    tell you whether each word is in a particular document. In the previous section
    you learned about several databases that can search those sparse binary vectors
    very very efficiently, even for millions of documents. And you always find the
    exact documents that contain the words you’re looking for. PostgreSQL and conventional
    search engines have this feature built into them, right from the start. Internally
    they can even use fancy math like a *Bloom filter* to minimize the number of binary
    comparisons your search engine needs to make. Unfortunately, these seemingly magical
    algorithms that work for the sparse discrete vectors used for text search don’t
    work for the dense embedding vectors of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: So what can you do to implement a scalable semantic search engine? You could
    use brute force, and do the dot product for all the vectors in your database.
    Even though that would give you the exact answer with the highest accuracy, it
    would take a lot of time (computation). What’s worse is that your search engine
    would get slower and slower as you added more documents. The brute force approach
    scales linearly with the number of documents in your database.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, you are going to need to add a lot of documents to your database
    if you want your LLM to work well. When you use LLMs for question answering and
    semantic search, they can only handle a few sentences at a time. So you will need
    to break all the documents in your database into paragraphs or even sentences
    if you want to get good results with your LLM pipeline. This explodes the number
    of vectors you need to search. Brute force won’t work, and there is no magical
    math that will work on dense continuous vectors.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why you need powerful search tools in your arsenal. Vector databases
    are the answer to this challenging semantic search problem. Vector databases are
    powering a new generation of search engines that can quickly find the information
    you are looking for, even if you need to search the entire Internet. But before
    we get to that, let’s take a look at the basics of search.
  prefs: []
  type: TYPE_NORMAL
- en: So now let’s reframe your problem from full-text search to semantic search.
    You have a search query, that you can embed using an LLM. And you have your database
    of text documents, where you’ve embedded every document into a vector space using
    the same LLM. Among those vectors, you want to find the vector that is closest
    to your query vector — that is, the *cosine similarity* (dot product) is maximized.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Approximate nearest neighbor (ANN) search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is only one way to find the *exact* nearest neighbor for our query. Remember
    how we discussed exhaustive search in Chapter 4? Back then, we found the nearest
    neighbor of the search query by computing its dot product with every vector in
    the database. That was OK because your database back then included only a couple
    dozen vectors. It won’t scale to a database with thousands or millions of documents.
    And your vectors are high dimensional — BERT’s sentence embeddings have 768 dimensions.
    This means any math you want to do on the vectors is cursed with *curse of dimensionality*.
    And LLM embeddings are even larger, so the curse is going to get even worse if
    you use models larger than BERT. You wouldn’t want Wikipedia’s users to wait while
    you’re performing dot products on 6 million articles!
  prefs: []
  type: TYPE_NORMAL
- en: As often happens in the real world, you need to give something to get something.
    If you want to optimize the algorithm’s retrieval speed, you need to compromise
    on precision. As you saw in Chapter 4, you don’t need to compromise too much,
    and the fact that you find several approximate neighbors can actually be useful
    for your users, and increase the chance they’ll find what they’ve been looking
    for.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 4 you saw an algorithm called Locality Sensitive Hashing (LSH) that
    helps you to find your vector’s *approximate nearest neighbors* by assigning a
    hash to regions of the high dimensional space (hyperspace) where your embeddings
    are located. LSH is an Approximate k-Nearest Neighbors (ANN) algorithm, that is
    responsible for both indexing your vectors and retrieving the neighbors you’re
    looking for. But there are many others that you’re about to meet. Each of them
    has its strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: To create your semantic search pipeline, you’ll need to make two crucial choices — which
    model to use to create your embeddings, and which ANN indexing algorithm you’re
    going to use. You’ve already seen in this Chapter how an LLM can help you increase
    the accuracy of your vector embeddings. So the main remaining decision is how
    to index your vectors.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re building a production-level application that needs to scale to thousands
    or millions of users, you might also look for a hosted implementation for your
    vector database, such as Pinecone, Milvus, or OpenSearch. A hosted solution will
    allow you to store and retrieve your semantic vectors fast enough and accurately
    enough to give your users a pleasant user experience. And the provider will manage
    the complexity of scaling up your vector database as your app becomes more and
    more popular.
  prefs: []
  type: TYPE_NORMAL
- en: But your probably even more interested in how you can bootstrap your own vector
    search pipeline. Turns out it’s not too difficult to do on your own, even for
    databases up to a million or more vectors (documents).
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.4 Choose your index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the increasing need to search for pieces of information in increasingly
    large datasets, the field of ANN algorithms boomed. Vector database product launches
    have been announced nearly every month recently. And you may be lucky and your
    relational or document database has already started to release early versions
    of vector search algorithms built in.
  prefs: []
  type: TYPE_NORMAL
- en: If you use PostgreSQL as your production database, you’re in luck. In July 2023
    they released the `pgvector` plugin which provides you with a seamless way to
    store and index vectors in your database. They provide both exact and approximate
    similarity search indexes so you can play with the tradeoffs between accuracy
    and speed that work for you in your application. If you combine this with PostgreSQL’s
    performant and reliable full-text search indexes, you can likely scale your NLP
    pipeline to millions of users and documents.^([[31](#_footnotedef_31 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, at the time of this writing, it is early days for the `pgvector`
    software. In September 2023, the ANN vector search feature in `pgvector` is in
    the bottom quartile of the rankings for speed. And you will be limited to two
    thousand dimensions for your embedding vectors. So if you’re indexing sequences
    of several embeddings, or you are using high dimensional vectors from a large
    language model, you will need to add a dimension reduction step (PCA for example)
    to your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: LSH was developed in the early 2000s; since then, dozens of algorithms joined
    the ANN family. There are a few large families of ANN algorithms. We’ll look at
    three of them - hash-based, tree-based and graph-based.
  prefs: []
  type: TYPE_NORMAL
- en: The hash-based algorithms are best represented by LSH itself. You already saw
    how the indexing works in LSH in Chapter 4, so we won’t spend any time on it here.
    Despite its simplicity, the LSH algorithm is still widely used within popular
    libraries such as Faiss (Facebook AI Similarity Search) which we’ll use in a bit.^([[32](#_footnotedef_32
    "View footnote.")]) It also has spawned modified versions for specific goals,
    such as the DenseFly algorithm that is used for searching biological datasets.^([[33](#_footnotedef_33
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: To understand how tree-based algorithms work, let’s look at Annoy, a package
    created by Spotify for its music recommendations. Annoy algorithm recursively
    partitioning the input space into smaller and smaller subspaces using a binary
    tree structure. At each level of the tree, the algorithm selects a hyperplane
    that splits the remaining points in the subspace into two groups. Eventually,
    each data point is assigned to a leaf node of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: To search for the nearest neighbors of a query point, the algorithm starts at
    the root of the tree and goes down by making comparisons between the distance
    of the query point to the hyperplane of each node and the distance to the nearest
    point found so far. The deeper the algorithm goes, the more precise the search.
    So you can make searches shorter and less accurate. You can see a simplified visualization
    of the algorithm in Figure [10.3](#figure-annoy-algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 A simplified visualization of the Annoy algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![annoy all stages](images/annoy_all_stages.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let’s look at graph-based algorithms. A good representative of graph-based
    algorithms, *Hierarchical Navigable Small World* (HNSW)^([[34](#_footnotedef_34
    "View footnote.")]) algorithm, approaches the problem bottom-up. It starts by
    building Navigable Small World graphs, which are graphs where each vector is connected
    to its closest neighbors by a vertex. To understand the intuition of it, think
    of the Facebook connections graph - everyone is connected directly only to their
    friends, but if you count "degrees of separation" between any two people, it’s
    actually pretty small. (Stanley Milgram discovered in an experiment in the 1960s
    that on average, every two people were separated by 5 connections.^([[35](#_footnotedef_35
    "View footnote.")]) Nowadays, for Twitter users, this number is as low as 3.5.)
  prefs: []
  type: TYPE_NORMAL
- en: HNSW then breaks the NSW graphs into layers, where each layer contains fewer
    points that are further away from each other than the layer beyond it. To find
    your nearest neighbor, you would start traversing the graph from the top, with
    each layer getting you closer to the point that you’re looking for. It’s a bit
    like international travel. You first take the plane to the capital of the country
    where your destination is situated. You then take the train to the smaller city
    closer to the destination. And you can take a bike to get there! At each layer,
    you’re getting closer to your nearest neighbor - and you can stop the retrieval
    at whatever layer, according to the throughput your use case requires.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.5 Quantizing the math
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may hear about *quantization* being used in combination with other indexing
    techniques. At its core, quantization is basically transforming the values in
    your vectors to create lower-precision vectors with discrete values (integers).
    This way your queries can look for exact matches of integer values, a database
    and numerical computation that is much faster than searching for a floating point
    range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a 5D embedding vector stored as an array of 64-bit `float`s.
    Here’s a crude way to quantize a `numpy` float.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 Quantizing numpy floats
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: If your indexer does the scaling and integer math correctly, you can retain
    all of the precision of your original vectors with half the space. You reduced
    the search space by half simply by quantizing (rounding) your vectors to create
    32-bit integer buckets. More importantly, if your indexing and query algorithms
    do their hard work with integers rather than floats, they run much much faster,
    often 100 times faster. And if you quantize a bit more, retaining only 16 bits
    of information, you can gain another order of magnitude in compute and memory
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The product quantization that you would use to implement semantic search needs
    to be more more complicated than that. The vectors you need to compress are much
    longer (have many more dimensions), the compression needs to be much better at
    retaining all the subtle bits of information in the vectors. This is especially
    important for plagiarism and LLM detectors. It turns out, if you split the document
    vector into multiple smaller vectors, and each of these vectors is quantized separately
    using clustering algorithms. You can learn more about the quantization process
    in .^([[36](#_footnotedef_36 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: If you keep exploring the world of nearest neighbors algorithms, you might run
    into the acronym IVFPQ (Inverse File Index with Product Quantization). The Faiss
    library uses IVFPQ for high-dimensional vectors. ^([[37](#_footnotedef_37 "View
    footnote.")]) And as recently as 2023, the HNSW+PQ combination was adopted by
    frameworks like Weaviate.^([[38](#_footnotedef_38 "View footnote.")]) So this
    is definitely the state of the art for many web-scale applications.
  prefs: []
  type: TYPE_NORMAL
- en: Indexes that combine many different algorithms are called *composite indexes*.
    Composite indexes are a bit more complex to implement and work with. The search
    and indexing performance (latency, throughput, and resource constraints) are sensitive
    to how the individual stages of the indexing pipeline are configured. If you configure
    them incorrectly they can perform much worse than much simpler vector search and
    indexing pipelines. Why would you want all that extra complexity?
  prefs: []
  type: TYPE_NORMAL
- en: The main reason is memory (RAM and GPU memory size). If your vectors are high-dimensional,
    then not only is calculating the dot product a very expensive operation, but your
    vectors also take more space in memory (on your GPU or in your RAM). Even though
    you only load a small part of the database into RAM, you might run out of memory.
    That’s why it’s common to use techniques like PQ to compress the vectors before
    they are fed into another indexing algorithm like IVF or HNSW.
  prefs: []
  type: TYPE_NORMAL
- en: For most real-world applications when you are not attempting to index the entire
    Internet you can get by with simpler indexing algorithms. And you can always use
    memory mapping libraries to work efficiently with tables of data stored on disk,
    especially Flash drives (solid state disk).
  prefs: []
  type: TYPE_NORMAL
- en: Choose your implementation library
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that you have a better idea of the different algorithms, it’s time to look
    at the wealth of implementation libraries that are out there. While the algorithms
    are just a mathematical representation of the indexing and retrieval mechanisms,
    how they are implemented can determine the algorithm’s accuracy and speed. Most
    of the libraries are implemented in memory-efficient languages, such as C++, and
    have Python bindings so that they can be used in Python programming.
  prefs: []
  type: TYPE_NORMAL
- en: Some libraries implement a single algorithm, such as Spotify’s annoy library.^([[39](#_footnotedef_39
    "View footnote.")]) Others, such as Faiss ^([[40](#_footnotedef_40 "View footnote.")])
    and `nmslib` ^([[41](#_footnotedef_41 "View footnote.")]) have a variety of algorithms
    you can choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [10.4](#figure-ann-benchmarks) shows the comparison of different algorithm
    libraries on a text dataset. You can discover more comparisons and links to dozens
    of ANN software libraries in Erik Bern’s ANN benchmarks repository.^([[42](#_footnotedef_42
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 Performance comparison of ANN algorithms for the New York Times
    dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![ann benchmarks nyt 256 dataset](images/ann-benchmarks-nyt-256-dataset.png)'
  prefs: []
  type: TYPE_IMG
- en: If you feel decision fatigue and are overwhelmed with all the choices, some
    turnkey solutions can help you out. OpenSearch, a 2021 fork of the ElasticSearch
    project, is a reliable workhorse in the full-text search world and it has a vector
    database and Nearest Neighbors search algorithm built in. And OpenSearch project
    one-ups its business source competitor, ElasticSearch, with cutting-edge plugins
    such as a semantic search vector database and ANN vector search.^([[43](#_footnotedef_43
    "View footnote.")]) The open-source community can often implement state-of-the-art
    algorithms much more quickly than the smaller internal corporate teams that work
    on proprietary software.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Watch out for open-source projects that may change the software license at any
    time. The ElasticSearch, TensorFlow, Keras, Terraform, and even Redhat Linux developer
    communities have all had to fork these projects after corporate sponsors decided
    to change the software licenses to *business source*. Business source is the term
    developers use to refer to proprietary software that is advertised as open source
    by corporations. The software comes with commercial use restrictions. And the
    sponsoring corporation can change those terms as soon as the project becomes popular
    and they want to monetize the hard work that open-source contributors put into
    the project.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re feeling a bit intimidated by the prospect of deploying the Java OpenSearch
    packages on Docker containers, you may have more fun with Haystack. It’s a great
    way to experiment with your own ideas for indexing and searching your documents.
    And you’re probably here because you want to understand how it all works. For
    that, you need a Python package. Haystack is the latest and greatest Python package
    for building question-answering and semantic search pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.6 Pulling it all together with haystack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ve now seen almost all the components of a question-answering pipeline
    and it may seem overwhelming. Not to worry. Here are the pieces you’ll need for
    your pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: A model to create meaningful embeddings of your text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ANN library to index your documents and retrieve ranked matches for your
    search queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model that, given the relevant document, will be able to find the answer to
    your question - or to generate it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a production app, you will also need a vector store (database). A vector
    database holds your embedding vectors and indexes them so you can search them
    quickly. And you can update your vectors whenever the document text changes. Some
    examples of open-source vector databases include Milvus, Weaviate, and Qdrant.
    You can also use some general-purpose datastores like ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: How do you combine all of this together? Well, just a few years ago, it would
    take you quite some time to figure out how to stitch all of these together. Nowadays,
    a whole family of NLP frameworks provides you with an easy interface to build,
    evaluate and scale your NLP applications, including semantic search. Leading open-source
    semantic search frameworks include Jina,^([[44](#_footnotedef_44 "View footnote.")])
    Haystack,^([[45](#_footnotedef_45 "View footnote.")]) and txtai.^([[46](#_footnotedef_46
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we’re going to leverage one of these frameworks, Haystack,
    to combine all you’ve learned in the recent chapter into something you can use.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.7 Getting real
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve learned about the different components of your question-answering
    pipeline, it’s time to bring it all together and create a useful app.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll be creating a question-answering app based on…​ this very book! You’re
    going to use the same dataset that we saw earlier - sentences from the first 8
    chapters of this book. Your app is going to find the sentence that contains the
    answer to your question.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into it! First, we’ll load our dataset and take only the text sentences
    from it, like we did before.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.14 Loading the NLPiA2 lines dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 10.2.8 A haystack of knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you’ve loaded the natural language text documents, you want to convert
    them all into Haystack Documents. In Haystack, a Document object contains two
    text fields: a title and the document content (text). Most documents you will
    work with are similar to Wikipedia articles where the title will be a unique human-readable
    identifier for the subject of the document. In your case, the lines of this book
    are too short to have a title that’s different from the content. So you can cheat
    a bit and put the content of the sentence in both the title and the content of
    your `Document` objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.15 Converting the NLPiA2 lines into Haystack Documents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now you want to put your documents into a database and set up an index so you
    can find the "needle" of knowledge you’re looking for. Haystack has several fast
    vector store indexes that work well for storing documents. The examples below
    use the Faiss algorithm for finding vectors in your haystack of documents. For
    the Faiss document index to work correctly on Windows you will need to install
    haystack from binaries and run your Python code within `git-bash` or WSL (Windows
    Subsystem for Linux).^([[47](#_footnotedef_47 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.16 Only for Windows
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In Haystack, your document storage database is wrapped in a `DocumentStore`
    object. The `DocumentStore` class gives you a consistent interface to the database
    containing the documents you just downloaded in a CSV. For now the "documents"
    are just the lines of text for an early version of the ASCIIDoc manuscript for
    this book — really really short documents. The haystack `DocumentStore` class
    allows you to connect to different open source and commercial vector databases
    that you can host locally on your machine, such as Faiss, PineCone, Milvus, ElasticSearch
    or even just SQLLite. For now, use the `FAISSDocumentStore` and its default indexing
    algorithm (`'Flat'`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The FAISSDocumentStore in haystack gives you three of these indexing approaches
    to choose from. The default `'Flat'` index will give you the most accurate results
    (highest recall rate) but will use a lot of RAM and CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re really constrained on RAM or CPU, like when you’re hosting your app
    on Hugging Face, you can experiment with two other FAISS options: `''HNSW''` or
    `f’IVF{num_clusters},Flat''`. The question-answering app you’ll see at the end
    of this section used the `''HNSW''` indexing approach to fit within a hugging
    face "free tier" server. See the Haystack documentation for details on how to
    tune your vector search index.^([[48](#_footnotedef_48 "View footnote.")]) You
    will need to balance, speed, RAM, and recall for your needs. Like many NLP questions,
    there is no right answer to the question of the "best" vector database index.
    Hopefully, when you ask this question to your question-answering app, it will
    say something like "It depends…​".'
  prefs: []
  type: TYPE_NORMAL
- en: Now go to your working directory where you ran this Python code. You should
    see a file named `'faiss_document_store.db'`. That’s because FAISS automatically
    created an SQLite database to contain the text of all your documents. Your app
    will need that file whenever you use the vector index to do semantic search. It
    will give you the actual text associated with the embedding vectors for each document.
    However, this file is not enough in order to load your data store into another
    piece of code - for that, you’ll need to you the `save` method of the `DocumentStore`
    class. We’ll do that later in the code after we fill the document store with embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to set up our indexing models! The semantic search process includes
    two main steps - retrieving documents that might be relevant to the query (semantic
    search), and processing those documents to create an answer. So you will need
    an EmbeddingRetriever semantic vector index and a generative transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 9 you met BERT and learn how to use it to create general-purpose
    embeddings that represent the meaning of text. Now you’ll learn how to use an
    embedding-based retriever to overcome the curse of dimensionality and find the
    embeddings for text most likely to answer a user’s question. You can probably
    guess that you’ll get better results if both your retriever and your reader are
    fine-tuned for question-answering tasks. Luckily there are a lot of BERT-based
    models that have been pretrained on question-answering datasets like SQuAD.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.17 Configuring the `reader` and `retriever` components of the question
    answering pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that the Reader and the Retriever don’t have to be based on the same model
    - because they don’t perform the same job. `multi-qa-mpnet-base-dot-v1` was optimized
    for semantic search - that is, finding *the right documents* that match a specific
    query. `roberta-base-squad2` on the other hand, was trained on a set of questions
    and short answers, making it better at finding the relevant part of the context
    that answers the question.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also finally saved our datastore for later reuse. If you go to the
    running directory of your script, you can notice that there are two new files:
    `nlpia_faiss_index.faiss` and `nlpia_faiss_index.json`. Spoilers - you’re going
    to need those soon enough!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you are ready to put the pieces together into a question-answering pipeline
    powered by semantic search! You only need to connect your `"Query"` output to
    the `Retriever` output to the Reader input:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.18 Creating a Haystack pipeline from components
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also do it in one line with some of Haystack’s ready-made pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 10.2.9 Answering questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s give our question-answering machine a try! We can start with a basic
    question and see how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Not bad! Note the "context" field that gives you the full sentence that contains
    the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.10 Combining semantic search with text generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, your extractive question-answering pipeline is pretty good at finding simple
    answers that are clearly stated within the text you give it. However, it’s not
    very good at expanding and explaining the answer to more complicated questions.
    Extractive summarization and question answering struggle to generate lengthy complicated
    text for answers to "why" and "how" questions. For complicated questions requiring
    reasoning, you need to combine the best of the NLU models with the best generative
    LLMs. BERT is a bidirectional LLM built and trained specifically for understanding
    and encoding natural language into vectors for semantic search. But BERT isn’t
    all that great for generating complex sentences, for that you need a unidirectional
    (causal) model such as GPT-2\. That way your pipeline can handle complex logic
    and reasoning to answer your "why" and "how" questions.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, you don’t have to cobble together these different models on your
    own. Open source developers are way ahead of you. The BART model does.^([[49](#_footnotedef_49
    "View footnote.")]) BART has an encoder-decoder architecture like other transformers.
    Even though its encoder is bi-directional using an architecture based on BERT,
    its decoder is unidirectional (left to right for English) just like GPT-2\. It’s
    technically possible to generate sentences using the original bidirectional BERT
    model directly, if you add the <MASK> token to the end and rerun the model many
    many times. But BART takes care of that *recurrence* part of text generation for
    you with its unidirectional decoder.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, you will use a BART model that was pretrained for Long-Form Question
    Answering (LFQA). In this task, a machine is required to generate a paragraph-long
    answer based on the documents retrieved, combining the information in its context
    in a logical way. The LFQA dataset includes 250,000 pairs of questions and long-form
    answers. Let’s see how a model trained on it performs.
  prefs: []
  type: TYPE_NORMAL
- en: We can continue using the same retriever, but this time, we’ll use one of Haystack
    pre-made pipelines, GenerativeQAPipeline. Instead of a Reader, as in a previous
    example, it includes a Generator, that generates text based on the answers the
    retriever found. So there are only a few lines of code that we need to change.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.19 Creating a Long-Form Question Answering Pipeline with Haystack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! Let’s see how our model does on a couple of questions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, that was a bit vague but correct! Let’s see how our model deals with
    a question that doesn’t have an answer in the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Well said, for a stochastic chameleon!
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.11 Deploying your app in the cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s time to share your application with more people. The best way to give other
    people access, is, of course, to put it on the internet! You need to deploy your
    model on a server and create a user interface (UI) so that people can easily interact
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: There are many companies offering cloud hosting services - in this chapter,
    we’ll go with HuggingFace Spaces. As HuggingFace’s hardware is optimized to run
    its NLP models, this makes sense computationally. HuggingFace also offers several
    ways to quickly ship your app by integrating with frameworks like Streamlit and
    Gradio.
  prefs: []
  type: TYPE_NORMAL
- en: Building your app’s UI with Streamlit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ll use Streamlit ^([[50](#_footnotedef_50 "View footnote.")]) to build your
    question-answering web App. It is an open-source framework that allows you to
    rapidly create web interfaces in Python. With Streamlit, you can turn the script
    you just run into an interactive app that anyone can access with just a few lines
    of code. And both Streamlit company itself and Hugging Face offer the possibility
    to deploy your app seamlessly to HuggingFace Spaces by offering an out-of-the-box
    Streamlit Space option.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stick with Huggingface this time, and we’ll let you check Streamlit Share
    on your own.^([[51](#_footnotedef_51 "View footnote.")]) Go ahead and create a
    HuggingFace account if you already don’t have one. Once that’s done, you can navigate
    to Spaces and choose to create a Streamlit Space. When you’re creating your space,
    Hugging Face creates a "Hello World" Streamlit app repository that’s all yours.
    If you clone this git repository to your machine you can edit it to make it do
    whatever you like.
  prefs: []
  type: TYPE_NORMAL
- en: Look for the `app.py` file within Hugging Face or on your local clone of the
    repository. The `app.py` file contains the Streamlit app code. Let’s replace that
    app code with the start of your question answering. For now, you just want to
    echo back the user’s question so they can feel understood. This will be especially
    important for your UX if you ever plan to do preprocessing on the question such
    as case folding, stemming, or maybe removing or adding question marks to the end.
    You may even want to experiment with adding the prefix "What is …​" if your users
    prefer to just enter noun phrases without forming a complete question.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.20 A "Hello World" question-answering application with Streamlit
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Deep-diving into Streamlit is beside the scope of this book, but you should
    understand some basics before creating your first app. Streamlit apps are essentially
    scripts. They re-run every time as the user loads the app in their browser or
    updates the input of interactive components. As the script runs, Streamlit creates
    the components defined in the code. In the script above, there are several components:
    `title`, `markdown` (instructions below the title), as well as the `text_input`
    component that receives the user’s question.'
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and try to run your app locally by executing line `streamlit run app.py`
    in your console. You should see something like the app in Figure [10.5](#figure-streamlit-helloworld-app).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 Question answering Streamlit app
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![qa streamlit app v1](images/qa_streamlit_app_v1.png)'
  prefs: []
  type: TYPE_IMG
- en: Time to add some question-answering capabilities to your app! You’ll use the
    same code as before, but you’ll optimize it to run faster on Streamlit.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s load the document store you created and saved previously. To do
    that, you need to copy your `.faiss` and `.json` files into your Streamlit app’s
    directory. Then, you can use the `load` method of `FAISSDocumentStore` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Note that you’re wrapping our code in a function. You’re using it to leverage
    a mechanism implemented in Streamlit called *caching*. Caching is a way to save
    the results of a function so that it doesn’t have to be re-run every time the
    app is loaded or the input is changed. This is very useful both for heavy datasets
    and for models that take a long time to load. During the caching process, the
    input to the function is *hashed*, so that Streamlit can compare it to other inputs.
    And the output is saved in a `pickle` file, a common Python serialization format.
    Your document store, unfortunately, can be neither cached nor hashed (very confusing!),
    but the two models you’re using for the question-answering pipeline can be.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.21 Loading the Reader and Retriever
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, insert the code building your QA pipeline between the title/subtitle and
    the question input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you can make your app ready to answer questions! Let’s make it return
    the context of the answer too, not just the answer itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: And your question-answering app is ready! Let’s give it a try. As your model
    "Who invented sentiment analysis?" You should see something similar to Figure
    [10.6](#figure-streamlit-qa-app).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 Working Streamlit app with a question answered
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![qa streamlit app with question](images/qa_streamlit_app_with_question.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, deploy your app to the cloud! Congratulations on your first NLP web application.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.12 Wikipedia for the ambitious reader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If training your model on the text in this book seems a little constraining
    for you, consider going "all in" and training your model on Wikipedia. After all,
    Wikipedia contains all of the human knowledge, at least the knowledge that the
    *wisdom of the crowd* (humanity) thinks is important.
  prefs: []
  type: TYPE_NORMAL
- en: Be careful though. You will need a lot of RAM, disk space, and compute throughput
    (CPU) to store, index and process the 60 million articles on Wikipedia. And you
    will need to deal with some insidious quirks that could corrupt your search results
    invisibly. And it’s hard to curate billions of words of natural language text.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use full-text search on PyPi.org for "Wikipedia" you won’t notice that
    "It’s A Trap!"^([[52](#_footnotedef_52 "View footnote.")]) You might fall into
    the trap with `pip install wikipedia`. Don’t do that. Unfortunately, the package
    called `wikipedia` is abandonware, or perhaps even intentional name-squatting
    malware. If you use the `wikipedia` package you will likely create bad source
    text for your API (and your mind):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: That’s fishy. No NLP preprocessor should ever corrupt your "AI" query by replacing
    it with the capitalized proper name "Xi". That name is for a person at the head
    of one of the most powerful censorship and propaganda (brainwashing) armies on
    the planet. And this is exactly the kind of insidious spell-checker attack that
    dictatorships and corporations use to manipulate you.^([[53](#_footnotedef_53
    "View footnote.")]) To do our part in combating fake news we forked the `wikipedia`
    package to create `nlpia2_wikipedia`. We fixed it so you can have a truly open
    source and honest alternative. And you can contribute your own enhancements or
    improvements to pay it forward yourself.
  prefs: []
  type: TYPE_NORMAL
- en: You can see here how the `nlpia2_wikipedia` package on PyPi will give you straight
    answers to your queries about AI.^([[54](#_footnotedef_54 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Now you can use Wikipedia’s full-text search API to feed your retrieval-augmented
    AI with everything that humans understand. And even if powerful people are trying
    to hide the truth from you, there are likely a lot of others in your "village"
    that have contributed to Wikipedia in your language.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now you know how to retrieve a corpus of documents about any topic that is important
    to you. If it’s not already, AI and large language models will certainly be important
    to you in the coming years. You can teach your retrieval augmented question answering
    system from the previous section to answer questions from any knowledge you can
    find on the internet, including Wikipedia articles about AI. You no longer have
    to rely on search engine corporations to protect your privacy or provide you with
    factual answers to your questions. You can build your own retrieval-augmented
    LLMs to answer questions factually for you and those you care about at your workplace
    or in your community.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.13 Serve your "users" better
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we have seen the powers, but also the drawbacks, of large language
    models. And we saw that you don’t have to use the paid, private LLMs sponsored
    by the Big Tech.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the big-picture thinking at HuggingFace and other thought leaders,
    you too can create value for yourself without investing in huge compute and data
    resources. Small startups, nonprofits and even individuals are building search
    engines and conversational AI that are delivering more accurate and useful information
    than what BigTech will ever be able to deliver. Now that you’ve seen what LLMs
    do well, you will be able to use them correctly and more efficiently to create
    much more valuable tools for you and your business.
  prefs: []
  type: TYPE_NORMAL
- en: And if you think this is all a pipe dream, you only have to look back at our
    suggestions in the first edition of this book. There we told you about the rapid
    growth in the popularity and profitability of search engine companies such as
    DuckDuckGo. As they have succumbed to pressure from investors and the lure of
    ever-increasing advertising revenue, new opportunities have opened up. Search
    engines such as You Search (You.com), Brave Search (Brave.com), Mojeek (Mojeek.com),
    Neeva (Neeva.com), and SearX (searx.org/) have continued to push search technology
    forward, improving transparency, truthfulness, and privacy for Internet search.
    The small web and the Fediverse are encroaching on BigTech’s monopoly on your
    eyeballs and access to information.
  prefs: []
  type: TYPE_NORMAL
- en: Corporations are using LLMs incorrectly because they are restrained by their
    *fiduciary responsibility* to investors in the US. Fiduciary responsibility refers
    to someone’s legal obligation to act for the benefit of someone else, the person
    with the duty must act in a way that will benefit someone else financially. The
    *Revlon doctrine* requires judicial review when a person or corporation wants
    to purchase another corporation. The goal of this ruling is to ensure that the
    directors of the corporation being purchased did not do anything that could reduce
    the value of that company in the future.^([[55](#_footnotedef_55 "View footnote.")])
    And business managers have taken this to mean that they must always maximize the
    revenue and income of their company, at the expense of any other values or sense
    of responsibility they might feel towards their users or community. Most managers
    in the US have taken the *Revlon Doctrine* to mean "greed is good" and emphasis
    on ESG (Environmental, Social and Governance) will be punished. Federal legislation
    is currently being proposed in the US Congress that would make it illegal for
    investment firms to favor corporations with ESG programs and values.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, many smart, responsible organizations are bucking this greedy zero-sum
    thinking. You can find 100s of open-source ChatGPT-like alternatives on Hugging
    Face. H2O has even provided you with a UX within HuggingFace Spaces where you
    can compare all these chatbots to each other. We have collected a few dozens of
    open-source large language models that you can try instead of proprietary GPT
    models.footnote[List of open source Large Language Models in the NLPIA repository:([https://gitlab.com/tangibleai/nlpia2/-/blob/main/docs/open-source-llms.md](docs.html))
    ]
  prefs: []
  type: TYPE_NORMAL
- en: For example, Vicuna requires only 13 billion parameters to achieve twice the
    accuracy of LLaMa-2 and almost the same accuracy as ChatGPT.^([[56](#_footnotedef_56
    "View footnote.")]) ^([[57](#_footnotedef_57 "View footnote.")]) LLaMa-2-70B is
    the next most accurate model to Vicuna but it requires 70 billion parameters and
    so runs 5 times slower. And Vicuna was trained on the 90,000 conversations in
    the ShareGPT dataset on HuggingFace so you can fine-tune the foundational Vicuna
    model models to achieve similar or even better accuracy for your users.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the LLM training data sets and models for the Open Assistant are
    community-generated and publicly accessible under the Apache open-source license.
    If you want to contribute to the battle against exploitative and manipulative
    AI, the Open Assistant project is a great place to start.^([[58](#_footnotedef_58
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: By using open-source models, finetuning them on the data that’s relevant to
    your domain and grounding your models with real knowledge using semantic search
    and retrieval-augmented generation, you can significantly increase the accuracy,
    effectiveness and ethics of your models. In the next chapter, we will show you
    another powerful way of grounding your model - using knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Vicuna
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Immediately after Llama 2 was released, the open source community immediately
    started improving it. One particularly enthusiastic group of contributors at Berkeley,
    CMU, and UCSD formed the LMSYS.org project where they used ShareGPT to fine-tune
    Llama 2 for the virtual assistant task.^([[59](#_footnotedef_59 "View footnote.")])
    In 2023 ShareGPT contained almost half a million of the "wildest ChatGPT conversations."
    ^([[60](#_footnotedef_60 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: For the RLHF (human feedback part) these researchers and students at LMSYS created
    an arena where the latest AI contenders could compete, including ChatGPT, Alpaca,
    and Llama 2\. Anyone can sign up use the GUI to judge between pairs of contenders
    and help give chat bots ratings on how smart they are. When you dream up a challenging
    question and judge the chatbot answer, your rating is used to give them an Elo
    score, similar to the rating assigned to professional Chess, Go, and esports players.^([[61](#_footnotedef_61
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: The arena is such a respected measure of intelligence that there was even a
    Metaculus competition to predict whether an open source model will be able to
    break into leaderboard top 5 before the end of September 2023.^([[62](#_footnotedef_62
    "View footnote.")]) Vicuna-33B is currently (September 2023) ranked sixth on the
    LMSYS Leaderboard, right below GPT-3.5, which is 20 times larger and slower and
    only 2% smarter, according to the Elo score.^([[63](#_footnotedef_63 "View footnote.")])
    It’s also interesting to notice that the scores which rely on GPT-4 as the judge
    are consistently inflated for OpenAI and other commercial bots. Humans rate OpenAI’s
    chatbot performance much lower than GPT-4 does. This is called the chatbot narcicism
    problem. It’s generally a bad idea to measure the performance of an algorithm
    using a similar algorithm, especially when you are talking about machine learning
    models such as LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: If you care about your LLM-based chatbot’s performance you will want to find
    a high-quality test set created by humans. You can trust the LMSYS benchmark dataset
    to give you the most reliable and objective score of general intelligence for
    your LLMs. And you are free to download and use this dataset to rate your own
    chatbots.^([[64](#_footnotedef_64 "View footnote.")]) And if you need to add additional
    test questions for your particular use cases, you would be wise to use the LMSYS
    arena to record your questions. This way all the other open source chatbots will
    be rated based on your questions. And the next time you download an updated Elo
    rating dataset you should see your questions and how all the other models did.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: If you are not satisfied just watching all the nerdy fun happening in the "my
    brain is bigger than your brain" arena, you can contribute your own LLM to see
    how it stacks up. You can either add your model to the `fastchat` Python package
    or give LMSYS a web API so they can have judges send your LLM prompts.^([[65](#_footnotedef_65
    "View footnote.")]) Some of the more efficient LLMs, such as Vicuna-13B may require
    less than $100 of computer power to train. With all the know-how in this book,
    you have a chance to create something really interesting and new. Now that you
    have seen some relatively unreasonable answers to common sense reasoning questions,
    it is time to see what a top-ranked contender can do. LMSYS has created a script
    that will automatically download and run Vicuna on your own computer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the real world, a vicuna is an animal that is a close relative of the llama.
    In the world of AI, Vicuna is a modified version of LLaMa-2.
  prefs: []
  type: TYPE_NORMAL
- en: Vicuna is the offspring of that marriage between the collective intelligence
    of the open source community and the business intelligence that motivated the
    created Llama 2\. Vicuna is an updated version of LLaMa 2 that has been trained
    specifically to act as a virtual assistant. And the smallest version of Vicuna,
    the 7B version, will likely run on your computer without having to invest in any
    new hardware. Like for Llama 2, the Vicuna test below required 20 GB of RAM and
    was able to generate about one token per second on an 8-core 2.8GHz CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The first time you run this code, expect to wait ten minutes or more to download
    the 10 GB file containing the 7 billion model weights, even over a fast Internet
    connection. It took 8 minutes for us on a 5G home network. After the `fastchat`
    script downloads Vicuna it will give you a command line interface (CLI) where
    you can have an AMA with Vicuna.^([[66](#_footnotedef_66 "View footnote.")]) If
    you ever run Vicuna again, it will be ready to go in your `$HOME/.cache` directory,
    along side all your other Hugging Face Hub models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: A full transcript of this interaction with Vicuna is available in the `nlpia2`
    package on GitLab.^([[67](#_footnotedef_67 "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: If your laptop has enough RAM to run LLaMa-2 you can also likely run Vicuna.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.14 AI ethics vs AI safety
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, you learned a lot about the harm that AI and large language
    models are causing. And hopefully, you’ve come up with your own ideas for how
    to help mitigate those harms. Engineers who design, build and use autonomous algorithms
    are starting to pay attention to the harm caused by these algorithms and how they
    are used. How to use algorithms ethically, by minimizing harm is called *AI ethics*.
    And algorithms that minimize or mitigate much of these harms are often referred
    to as ethical AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have also heard about the *AI control problem* or *AI safety* and may
    be confused about how it is different from AI ethics. AI safety is about how we
    can avoid being exterminated, intentionally or unintentionally, by our future
    "robot overlords." People working on AI safety are trying to mitigate the long-term
    existential risk posed by superintelligent generally intelligent machines. The
    CEOs of many of the largest AI companies have publicly announced their concern
    about this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating the risk of extinction from AI should be a global priority alongside
    other societal-scale risks such as pandemics and nuclear war.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Center for AI Safety
  prefs: []
  type: TYPE_NORMAL
- en: This single sentence is so important to AI companies' businesses that more than
    a 100 senior managers at AI companies signed this open letter. Nonetheless, many
    of these same companies are not allocating significant resources or time or public
    outreach to address this concern. Many of the largest are not even willing to
    sign this vague noncommital statement. Open AI, Microsoft, and Anthropic signed
    this letter, but Apple, Tesla, Facebook, Alphabet (Google), Amazon and many other
    AI goliaths did not.
  prefs: []
  type: TYPE_NORMAL
- en: And there’s an ongoing public debate about the urgency and priority of *AI safety*
    vs *AI ethics*. Some thought leaders such as Yuval Harari and Yoshua Bengio are
    focused entirely on AI safety — restraining or controlling a hypothetical superintelligent
    AGI. Other, less well-known thought leaders are focusing their time and energy
    on the more immediate harm that algorithms and AI are causing now — in other words,
    AI ethics. Disadvantaged people are especially vulnerable to the unethical use
    of AI. When companies monetize their users' data they extract power and wealth
    from those who can least afford the loss. When technology is used to create and
    maintain monopolies those monopolies extinguish competition from small businesses,
    government programs, nonprofits, and individuals supporting the disadvantaged.^([[68](#_footnotedef_68
    "View footnote.")])
  prefs: []
  type: TYPE_NORMAL
- en: So which one of these pressing topics are you concerned with? Are there some
    overlapping things that you can work on to both reduce the harm to humans now
    and prevent our extinction in the long run? Perhaps *explainable AI* should be
    at the top of your list of ways to help create "ethical and safe AI." Explainable
    AI is the concept of an algorithm that can explain how and why it makes decisions,
    especially when those decisions are mistaken or harmful. The information extraction
    and knowledge graph concepts that you will learn in the next chapter are some
    of the foundational tools for building explainable AI. And explainable, grounded
    AI is less likely to propagate misinformation by generating factually incorrect
    statements or arguments. And if you can find algorithms that help explain how
    an ML algorithm is making its harmful predictions and decisions you can use that
    understanding to prevent that harm.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Test yourself
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How is the generative model in this chapter different from the BERT model you’ve
    seen in the previous one?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We indexed the sentences of this book as the context for a Longformer-based
    reading comprehension question-answering model. Will it get better or worse if
    you use Wikipedia sections for the context? What about an entire Wikipedia article?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the fastest indexing algorithm for vector search and semantic search?
    (hint, this is a trick question)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit a Scikit-Learn `CountVectorizer` to count the bigrams within sentences extracted
    from 100 Wikipedia articles. Compute conditional probabilities for all the second
    words that follow the first word in your count vectors and use the Python `random.choice`
    function to autocomplete the next words in a sentence. How well does this work
    compared to using an LLM such as Llama 2 to autocomplete your sentences?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What approaches or tests would you use to help quantify the intelligence of
    an LLM? What are the latest benchmarks for measuring human intelligence, and are
    they useful for evaluating an LLM or AI assistant?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Judge your judgment: Create a ranked list of the most intelligent open source
    LLMs you can think of. Now visit the LMSYS arena ([https://chat.lmsys.org](.html))
    and be a judge for at least 5 rounds. Compare your ranked list to the official
    Elo ranking on the LMSYS leaderboard ([https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](lmsys.html)).
    How many of your LLM rankings are out of order?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Can you solve the mystery of "Shmargaret Shmitchell," the last author of the
    paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
    Who is she? What can you do to support her and her coauthors in their fight for
    honesty and transparency in AI research?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models like GPT-4 may appear intelligent, but the "magic" behind
    their answers is probabilistically choosing the next token to generate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning your generative models will help you generate domain-specific content,
    and experimenting with generation techniques and parameters can improve the quality
    of your output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximate nearest neighbor algorithms and libraries are useful tools to find
    the information to base your answers upon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-augmented generation combines the best of semantic search and generative
    models to create grounded AI that can answer questions factually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs fail more than half of the natural language understanding problems that
    researchers have dreamed up so far, and scaling up LLMs isn’t helping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_footnoteref_1) "AI’s Ostensible Emergent Abilities Are a Mirage" 2023
    by Katharine Miller ( [http://mng.bz/z0l6](mng.bz.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_footnoteref_2) Tool for estimating ML model environmental impact (
    [https://mlco2.github.io/impact/](impact.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_footnoteref_3) "Sustainable AI: Environmental Implications, Challenges
    and Opportunities" 2022 by Carole-Jean Wu et al. ( [https://arxiv.org/pdf/2111.00364.pdf](pdf.html)
    )'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_footnoteref_4) "Behind the Millions: Estimating the Scale of Large
    Language Models" by Dmytro Nikolaiev ( [http://mng.bz/G94A](mng.bz.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_footnoteref_5) Wikipedia article on GPT-2 ( [https://en.wikipedia.org/wiki/GPT-2](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_footnoteref_6) "Red teaming ChatGPT via Jailbreaking: Bias, Robustness,
    Reliability and Toxicity" 2023 by Terry Yue Zhuo et al ( [https://arxiv.org/abs/2301.12867](abs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_footnoteref_7) Clever Hans Wikipedia article ( [https://en.wikipedia.org/wiki/Clever_Hans](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_footnoteref_8) Wikipedia article on the harmful effects of social media
    like buttons ( [https://en.wikipedia.org/wiki/Facebook_like_button#Criticism](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_footnoteref_9) Techdirt article explaining how ChatGPT amplifies misinformation
    ( [https://www.techdirt.com/2023/07/19/g-o-media-execs-full-speed-ahead-on-injecting-half-cooked-ai-into-a-very-broken-us-media/](g-o-media-execs-full-speed-ahead-on-injecting-half-cooked-ai-into-a-very-broken-us-media.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_footnoteref_10) Google fired Timnit Gebru when she asked to publish
    "On the Dangers of Stochastic Parrots…​" with her coauthors Emily M. Bender, Angelina
    McMillan-Major, and Shmargaret Shmitchell (a pseudonym? Timnit had an ally with
    the last name Mitchel) ( [https://dl.acm.org/doi/pdf/10.1145/3442188.3445922?uuid=f2qngt2LcFCbgtaZ2024](10.1145.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_footnoteref_11) "GPT-4 Technical Report" ( [https://arxiv.org/pdf/2303.08774.pdf](pdf.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_footnoteref_12) Table of nonemergent capabilities was extracted from
    Appendix E in "Emergent Abilities of Large Language Models" by Jason Wei et al
    ( [https://arxiv.org/abs/2206.07682](abs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_footnoteref_13) MathActive package on GitLab ( [https://gitlab.com/tangibleai/community/mathactive](community.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_footnoteref_14) Twitter is now called X, and the rating and recommendation
    system has become even more toxic and opaque under the new management.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_footnoteref_15) SpaCy rule-based matching documentation ( [https://spacy.io/usage/rule-based-matching](usage.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[16]](#_footnoteref_16) ReLM on GitHub ( [https://github.com/mkuchnik/relm](mkuchnik.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[17]](#_footnoteref_17) lm-evaluation-harness project on GitHub ( [https://github.com/EleutherAI/lm-evaluation-harness](EleutherAI.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[18]](#_footnoteref_18) Regex package on PyPi ( [https://pypi.org/project/regex/](regex.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[19]](#_footnoteref_19) Guardrails-ai project on GitHub ( [https://github.com/ShreyaR/guardrails](ShreyaR.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[20]](#_footnoteref_20) GitHub source code for `guardrails-ai` ( [https://github.com/ShreyaR/guardrails](ShreyaR.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[21]](#_footnoteref_21) "Universal and Transferable Adversarial Attacks on
    Aligned Language Models" by Andy Zou et al ( [https://llm-attacks.org/](llm-attacks.org.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[22]](#_footnoteref_22) (Ars Technica news article ( [https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/](ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[23]](#_footnoteref_23) Scaling Laws for Neural Language Models by Jared Kaplan
    from Antrhopic.AI et al. ( [https://arxiv.org/abs/2001.08361](abs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[24]](#_footnoteref_24) Huggingface documentation on Model Outputs: ( [https://huggingface.co/docs/transformers/main_classes/output](main_classes.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[25]](#_footnoteref_25) *"Summary of the tokenizers"* on Huggingface: ( [https://huggingface.co/docs/transformers/tokenizer_summary](transformers.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[26]](#_footnoteref_26) How to generate text: using different decoding methods
    for language generation with Transformers ( [https://huggingface.co/blog/how-to-generate](blog.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[27]](#_footnoteref_27) Wikipedia article on Search Engines: ( [https://en.wikipedia.org/wiki/Search_engine](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[28]](#_footnoteref_28) ( [https://www.elastic.co/elasticsearch/](elasticsearch.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[29]](#_footnoteref_29) [https://solr.apache.org/](solr.apache.org.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[30]](#_footnoteref_30) Meilisearch Github Repository: ( [https://github.com/meilisearch/meilisearch](meilisearch.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[31]](#_footnoteref_31) How GitLab uses PostgreSQL trigram indexes in software
    which scales to millions of users ( [https://about.gitlab.com/blog/2016/03/18/fast-search-using-postgresql-trigram-indexes/](fast-search-using-postgresql-trigram-indexes.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[32]](#_footnoteref_32) A great resource on using FAISS library: ( [https://www.pinecone.io/learn/series/faiss/](faiss.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[33]](#_footnoteref_33) ( [https://github.com/dataplayer12/Fly-LSH](dataplayer12.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[34]](#_footnoteref_34) Efficient and robust approximate nearest neighbor
    search using Hierarchical Navigable Small World graphs, ( [https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf](1603.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[35]](#_footnoteref_35) ( [https://en.wikipedia.org/wiki/Six_degrees_of_separation](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[36]](#_footnoteref_36) Locally Optimal Product Quantization on PyPi ( [https://pypi.org/project/lopq/](lopq.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[37]](#_footnoteref_37) Billion-scale similarity search with GPUs by Jeff
    Johnson, Matthijs Douze, Herve'' Jegou ( [https://arxiv.org/pdf/1702.08734.pdf](pdf.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[38]](#_footnoteref_38) [https://weaviate.io/blog/ann-algorithms-hnsw-pq](blog.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[39]](#_footnoteref_39) [https://github.com/spotify/annoy](spotify.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[40]](#_footnoteref_40) Faiss Github repository: ( [https://github.com/facebookresearch/faiss](facebookresearch.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[41]](#_footnoteref_41) NMSlib Github repository ( [https://github.com/nmslib/nmslib](nmslib.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[42]](#_footnoteref_42) ANN Benchmarks repository on GitHub ( [https://github.com/erikbern/ann-benchmarks/](ann-benchmarks.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[43]](#_footnoteref_43) OpenSearch k-NN Documentation ( [https://opensearch.org/docs/latest/search-plugins/knn](search-plugins.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[44]](#_footnoteref_44) ( [https://github.com/jina-ai/jina](jina-ai.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[45]](#_footnoteref_45) [https://github.com/deepset-ai/haystack](deepset-ai.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[46]](#_footnoteref_46) ( [https://github.com/neuml/txtai](neuml.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[47]](#_footnoteref_47) Haystack installation instructions for Windows ( [https://docs.haystack.deepset.ai/docs/installation](docs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[48]](#_footnoteref_48) Haystack documentation on the `faiss_index_factor_str`
    option ( [https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[49]](#_footnoteref_49) BART: Denoising Sequence-to-Sequence Pre-training
    for Natural Language Generation, Translation, and Comprehension by Mike Lewis
    et al 2019 ( [https://arxiv.org/abs/1910.13461](abs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[50]](#_footnoteref_50) ( [https://docs.streamlit.io/](docs.streamlit.io.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[51]](#_footnoteref_51) ( [https://share.streamlit.io/](share.streamlit.io.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[52]](#_footnoteref_52) Know Your Meme article for "It’s A Trap" ( [https://knowyourmeme.com/memes/its-a-trap](memes.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[53]](#_footnoteref_53) ( [https://theintercept.com/2018/08/01/google-china-search-engine-censorship/](google-china-search-engine-censorship.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[54]](#_footnoteref_54) "It Takes a Village to Combat a Fake News Army" by
    Zachary J. McDowell & Matthew A Vetter ( [https://journals.sagepub.com/doi/pdf/10.1177/2056305120937309](10.1177.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[55]](#_footnoteref_55) Explanation of fiduciary duty at Harvard Law School
    by Martin Lipton et al. 2019 ( [https://corpgov.law.harvard.edu/2019/08/24/stakeholder-governance-and-the-fiduciary-duties-of-directors/](stakeholder-governance-and-the-fiduciary-duties-of-directors.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[56]](#_footnoteref_56) Vicuna home page ( [https://vicuna.lmsys.org/](vicuna.lmsys.org.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[57]](#_footnoteref_57) Vicuna LLM on Hugging Face ( [https://huggingface.co/lmsys/vicuna-13b-delta-v1.1](lmsys.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[58]](#_footnoteref_58) GitHub page for Open Assistant ( [https://github.com/LAION-AI/Open-Assistant/](Open-Assistant.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[59]](#_footnoteref_59) LMSYS ORG website (lmsys.org)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[60]](#_footnoteref_60) ShareGPT website ( [https://sharegpt.com](.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[61]](#_footnoteref_61) Wikipedia article explaining the Elo algorithm ( [https://en.wikipedia.org/wiki/Elo_rating_system](wiki.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[62]](#_footnoteref_62) Metaculus open source LLM ranking question for September
    2023 ( [https://www.metaculus.com/questions/18525/non-proprietary-llm-in-top-5/](non-proprietary-llm-in-top-5.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[63]](#_footnoteref_63) [https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](lmsys.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[64]](#_footnoteref_64) Huggingface dataset page ( [https://huggingface.co/datasets/lmsys/chatbot_arena_conversations](lmsys.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[65]](#_footnoteref_65) Instructions for adding a new model to the LMSYS Leaderboard
    ( [https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model](docs.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[66]](#_footnoteref_66) Ask Me Anything (AMA) is when someone, usually a human,
    offers to answer public questions on a social media platform.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[67]](#_footnoteref_67) Vicuna test results in nlpia2 package on GitLab (
    [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/data/llm/fastchat-vicuna-7B-terminal-session-input-output.yaml?ref_type=heads](llm.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[68]](#_footnoteref_68) from *Chokepoint Capitalism* by Cory Efram Doctorow'
  prefs: []
  type: TYPE_NORMAL
