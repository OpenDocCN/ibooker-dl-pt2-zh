- en: Chapter 7\. Creating Text Embedding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text embedding models lie at the foundation of many powerful natural language
    processing applications. They lay the groundwork for empowering already impressive
    technologies such as text generation models. We have already used embedding models
    throughout this book in a number of applications, such as supervised classification,
    unsupervised classification, semantic search, and even giving memory to text generation
    models like ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: It is nearly impossible to overstate the importance of embedding models in the
    field as they are the driving power behind so many applications. As such, in this
    chapter, we will discuss a variety of ways that we can create and fine-tune an
    embedding model to increase its representative and semantic power.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by discovering what embedding models are and how they generally
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embeddings and embedding models have already been discussed in quite a number
    of chapters before (Chapters X, X, and X) thereby demonstrating their usefulness.
    Before going into training such a model, let’s recap what we have learned with
    embedding models before.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured textual data by itself is often quite hard to process. They are
    not values we can directly process, visualize and create actionable results from.
    We first have to convert this textual data to something that we can easily process,
    numeric representations. This process is often referred to as **embedding** the
    input to output usable vectors, namely **embeddings** as shown in [Figure 7-1](#fig_1_we_use_an_embedding_model_to_convert_textual_input).
  prefs: []
  type: TYPE_NORMAL
- en: '![We use an embedding model to convert textual input  such as documents  sentences  and
    phrases to numerical representations  called embeddings. ](assets/creating_text_embedding_models_249519_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. We use an embedding model to convert textual input, such as documents,
    sentences, and phrases to numerical representations, called embeddings.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This process of embedding the input is typically performed by an LLM, which
    we refer to as an *embedding model*. The main purpose of such a model is to be
    as accurate as possible in representing the textual data as an embedding.
  prefs: []
  type: TYPE_NORMAL
- en: However, what does it mean to be accurate in representation? Typically, we want
    to capture the *semantic nature*, the meaning, of documents. If we can capture
    the core of what the document communicates, we hope to have captured what the
    document is about. In practice, this means that we expect vectors of documents
    that are similar to one another to be similar, whereas the embeddings of documents
    that each discuss something entirely different should be dissimilar. This idea
    of semantic similarity is visualized in [Figure 7-2](#fig_2_the_idea_of_semantic_similarity_is_that_we_expect).
  prefs: []
  type: TYPE_NORMAL
- en: '![The idea of semantic similarity is that we expect textual data that have
    similar meaning are also closer to each other in n dimensional space. As an example  it
    is illustrated here in 2 dimensional space. Do note that this is a simplified
    example. While 2 dimensional visualization helps illustrate the proximity and
    similarity of embeddings  these embeddings typically reside in high dimensional
    spaces. ](assets/creating_text_embedding_models_249519_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. The idea of semantic similarity is that we expect textual data
    that have similar meaning are also closer to each other in n-dimensional space.
    As an example, it is illustrated here in 2-dimensional space. Do note that this
    is a simplified example. While 2-dimensional visualization helps illustrate the
    proximity and similarity of embeddings, these embeddings typically reside in high-dimensional
    spaces.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An embedding model, however, can be trained for a number of purposes. For example,
    when we are building a sentiment classifier, we are more interested in the sentiment
    of texts than their semantic similarity. As illustrated in [Figure 7-3](#fig_3_similarity_can_be_expressed_as_more_than_just_sema),
    we can fine-tune the model such that documents are closer based on their sentiment
    than their semantic nature.
  prefs: []
  type: TYPE_NORMAL
- en: Either way, an embedding model aims to learn what makes certain documents similar
    to one another and we can guide this process. By presenting the model with enough
    examples of semantically similar documents, we can steer towards semantics whereas
    using examples of sentiment would steer it in that direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarity can be expressed as more than just semantically. An embedding
    model can be trained to focus on sentiment similarity  the idea that documents
    with similar sentiments are closer to each other in n dimensional space than documents
    with dissimilar sentiments. In this figure  negative reviews  red  are close to
    one another and dissimilar to positive reviews  green .](assets/creating_text_embedding_models_249519_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Similarity can be expressed as more than just semantically. An
    embedding model can be trained to focus on sentiment similarity, the idea that
    documents with similar sentiments are closer to each other in n-dimensional space
    than documents with dissimilar sentiments. In this figure, negative reviews (red)
    are close to one another and dissimilar to positive reviews (green).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many ways in which we can train, fine-tune, and guide embedding models
    but one of the strongest and widely-used techniques is called contrastive learning.
  prefs: []
  type: TYPE_NORMAL
- en: What is Contrastive Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One major technique for both training and fine-tuning text embedding models
    is called contrastive learning. Contrastive learning is a technique that aims
    to train an embedding model such that similar documents are closer in vector space
    whilst dissimilar documents are further apart. We have seen this notion previously
    in Figures 13-X and Figure 13-X.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying idea of contrastive learning is that the best way to learn and
    model similarity/dissimilarity between documents is by feeding a model examples
    of similar and dissimilar pairs. In order to accurately capture the semantic nature
    of a document, it often needs to be contrasted with another document for a model
    to learn what makes it different or similar. This contrasting procedure is quite
    powerful and relates to the context in which documents are written. This high-level
    procedure is demonstrated in [Figure 7-4](#fig_4_contrastive_learning_aims_to_teach_an_embedding_mo).
  prefs: []
  type: TYPE_NORMAL
- en: '![Contrastive learning aims to teach an embedding model whether documents are
    similar or dissimilar. Contrastive learning does so by presenting groups of documents
    to a model that are similar or dissimilar to a certain degree. ](assets/creating_text_embedding_models_249519_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Contrastive learning aims to teach an embedding model whether documents
    are similar or dissimilar. Contrastive learning does so by presenting groups of
    documents to a model that are similar or dissimilar to a certain degree.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Another way to look at contrastive learning is through the nature of explanations.
    A nice example of this is an anecdotal story of a reporter asking a robber “Why
    did you rob a bank”, to which he answers “Because that is where the money is.”^([1](ch07.html#id298))
    Although a factually correct answer, the intent of the question was not why he
    robs banks specifically but why he robs at all. This is called contrastive explanation
    and refers to understanding a particular case, “Why P” in contrast to alternatives,
    “Why P and not Q?“^([2](ch07.html#id299)) In the example, the question could be
    interpreted in a number of ways and may be best modeled by providing an alternative:
    “Why did you rob a bank (P) instead of obeying the law (Q)?”.'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of alternatives to the understanding of a question also applies
    to how an embedding learns through contrastive learning. By showing a model similar
    and dissimilar pairs of documents, it starts to learn what makes something similar/dissimilar
    and more importantly, why.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could learn a model to understand what a dog is by letting
    it find features such as “tail”, “nose”, “four legs”, etc. This learning process
    can be quite difficult since features are often not well-defined and can be interpreted
    in a number of ways. A being with a “ tail”, “nose”, and “four legs” can also
    be a cat. To help the model steer toward what we are interested in, we essentially
    ask it “Why is this a dog and not a cat?”. By providing the contrast between two
    concepts, it starts to learn the features that define the concept but also the
    features that are not related. We further illustrate this concept of contrastive
    explanation in [Figure 7-5](#fig_5_explanations_are_typically_grounded_by_the_contras).
  prefs: []
  type: TYPE_NORMAL
- en: '![Explanations are typically grounded by the contrast of other possibilities.
    As such  we get more information when we frame a question as a contrast. The same
    applies to an embedding model. When we feed it with different contrasts  degrees
    of similarity   it starts to learn what makes things different from one another
    and thereby the distinctive characteristics of concepts.](assets/creating_text_embedding_models_249519_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Explanations are typically grounded by the contrast of other possibilities.
    As such, we get more information when we frame a question as a contrast. The same
    applies to an embedding model. When we feed it with different contrasts (degrees
    of similarity), it starts to learn what makes things different from one another
    and thereby the distinctive characteristics of concepts.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the earliest and most popular examples of contrastive learning in NLP
    is actually Word2Vec. The model learns word representations by training on individual
    words in a sentence. A word close to a target word in a sentence will be constructed
    as a positive pair whereas randomly sampled words constitute dissimilar pairs.
    In other words, positive examples of neighboring words are contrasted with randomly
    selected words that are not neighbors. Although not widely known, it is one of
    the first major breakthroughs in NLP that leverages contrastive learning with
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways we can apply contrastive learning to create text embedding
    models but the most well-known technique and framework is [sentence-transformers](https://github.com/UKPLab/sentence-transformers).
  prefs: []
  type: TYPE_NORMAL
- en: SBERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are many forms of contrastive learning, one framework that has
    popularized the technique within the Natural Language Processing community, is
    sentence-transformers. Its approach fixes a major problem with the original BERT
    implementation for creating sentence embeddings, namely its computational overhead.
    Before sentence-transformers, sentence embeddings were often used with BERT using
    an architectural structure called cross-encoders.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-encoders allow two sentences to be passed to the transformer network simultaneously
    to predict the extent to which the two sentences are similar. It does so by adding
    a classification head to the original architecture that can output a similarity
    score. However, the number of computations rises quickly when you want to find
    the highest pair in a collection of 10,000 sentences. That would require n·(n−1)/2
    = 49 995 000 inference computations and therefore generates significant overhead.
    Moreover, a cross-encoder generally does not generate embeddings, as shown in
    [Figure 7-6](#fig_6_the_architecture_of_a_cross_encoder_both_sentence).
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this overhead is by generating embeddings from a BERT model by
    averaging its output layer or using the [CLS] token. This, however, has shown
    to be worse than simply averaging word vectors, like GloVe.^([3](ch07.html#id300))
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture of a cross encoder. Both sentences are concatenated  separated
    with a  SEP  token  and fed to the model simultaneously. Instead of outputting
    embeddings  it outputs a similarity score between the input sentences. ](assets/creating_text_embedding_models_249519_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. The architecture of a cross-encoder. Both sentences are concatenated,
    separated with a <SEP> token, and fed to the model simultaneously. Instead of
    outputting embeddings, it outputs a similarity score between the input sentences.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead, the authors of sentence-transformers approached the problem differently
    and searched for a method that is fast and creates embeddings that can be compared
    semantically. The result is an elegant alternative to the original cross-encoder
    architecture. Unlike a cross-encoder, in sentence-transformers, the classification
    head is dropped, and instead mean pooling is used on the final output layer to
    generate an embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence-transformers are trained using a siamese architecture. In this architecture,
    as visualized in [Figure 7-7](#fig_7_the_architecture_of_the_original_sentence_transfor),
    we have two identical BERT models that share the same weights and neural architecture.
    Since the weights are identical for both BERT models, we can use a single model
    and feed it the sentences one after the other.
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture of the original sentence transformers model which leverages
    a siamese network  also called a bi encoder. BERT models with tied weights are
    fed the sentences from which embeddings are generated through the pooling of token
    embeddings. Then  models are optimized through the similarity of the sentence
    embeddings. ](assets/creating_text_embedding_models_249519_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. The architecture of the original sentence-transformers model which
    leverages a siamese network, also called a bi-encoder. BERT models with tied weights
    are fed the sentences from which embeddings are generated through the pooling
    of token embeddings. Then, models are optimized through the similarity of the
    sentence embeddings.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The optimization process of these pairs of sentences is done through the loss
    functions which can have a major impact on the model’s performance. During training,
    the embeddings for each sentence are concatenated together with the difference
    between the embeddings. Then, this resulting embedding is optimized through a
    softmax classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting architecture is also referred to as a bi-encoder or SBERT for
    sentence-BERT. Although a bi-encoder is quite fast and creates accurate sentence
    representations, cross-encoders generally achieve better performance than a bi-encoder
    but do not generate embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The bi-encoder, like a cross-encoder, leverages contrastive learning; by optimizing
    the (dis)similarity between pairs of sentences, the model will eventually learn
    the things that make the sentences what they are.
  prefs: []
  type: TYPE_NORMAL
- en: To perform contrastive learning, we need two things. First, we need data that
    constitute similar/dissimilar pairs. Second, we will need to define how the model
    defines and optimizes similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Embedding Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many methods through which an embedding model can be created but generally,
    we look towards contrastive learning. This is an important aspect of many embedding
    models as the process allows it to efficiently learn semantic representations.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not a free process. We will need to understand how to generate
    contrastive examples, how to train the model, and how to properly evaluate it.
  prefs: []
  type: TYPE_NORMAL
- en: Generating contrastive examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When pre-training your embedding model, you will often see data being used from
    Natural Language Inference (NLI) datasets. As we described in Chapter 2, NLI refers
    to the task of investigating whether, for a given premise, it entails the hypothesis
    (entailment), contradicts it (contradiction), or neither (neutral).
  prefs: []
  type: TYPE_NORMAL
- en: For example, when the premise is “He is in the cinema watching *Coco*” and the
    hypothesis “He is watching *Frozen* at home”, then these statements are contradictions.
    In contrast, when the premise is “He is in the cinema watching *Coco*” and the
    hypothesis “In the movie theater he is watching the Disney movie *Coco*”, then
    these statements are considered entailment. This principle is illustrated in [Figure 7-8](#fig_8_we_can_leverage_the_structure_of_nli_datasets_to_g).
  prefs: []
  type: TYPE_NORMAL
- en: '![We can leverage the structure of NLI datasets to generate negative examples  contradiction  and
    positive examples  entailments  for contrastive learning. ](assets/creating_text_embedding_models_249519_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. We can leverage the structure of NLI datasets to generate negative
    examples (contradiction) and positive examples (entailments) for contrastive learning.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you look closely at entailment and contradiction, then they describe the
    extent to which two inputs are similar to one another. As such, we can use NLI
    datasets to generate negative examples (contradictions) and positive examples
    (entailments) for contrastive learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Contrastive examples can also be generated if you have labeled data. In Chapter
    2, we used SetFit to perform few-shot classification using sentence-transformers.
    In SetFit, contrastive examples were generated by comparing sentences within classes
    (positive examples) and between classes (negative examples).
  prefs: []
  type: TYPE_NORMAL
- en: The data that we are going to be using throughout creating and fine-tuning embedding
    models are derived from the General Language Understanding Evaluation benchmark
    ([GLUE](https://gluebenchmark.com/)). This GLUE benchmark consists of nine language
    understanding tasks to evaluate and analyze model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of these tasks is the Multi-Genre Natural Language Inference (MNLI) corpus
    which is a collection of sentence pairs annotated with entailment (contradiction,
    neutral, entailment). We will be using this data to train our text embedding model.
    Let’s load the dataset using the `datasets` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we take a look at an example of entailment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After having loaded the dataset, we will need to process it in such a way that
    it can be read with sentence-transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Train model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our data loader with training examples, we will need to create
    our embedding model. We typically choose an existing sentence-transformer model
    and fine-tune that model but in this example, we are going to train an embedding
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: This means that we will have to define two things. First, a pre-trained transformer
    model that serves as embedding individual words. As we have seen in Chapter 2,
    the `“bert-base-uncased”` model is often used for tutorials. However, many others
    exist that also have been evaluated using [sentence-transformers](https://www.sbert.net/docs/training/overview.html#best-transformer-model).
    Most notably, `“microsoft/mpnet-base”` often gives good results when used as word
    embedding models. Second, we will need to define the pooling strategy. Averaging
    the word embeddings is typically used throughout most embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By default, all layers of an LLM in sentence-transformers are trainable. Although
    it is possible to freeze certain layers, it is generally not advised since the
    performance is often better when unfreezing all layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will need to define a loss function over which we will optimize the
    model. As mentioned at the beginning of this section, one of the first instances
    of sentence-transformers uses soft-max loss. For illustrative purposes, we are
    going to be using that for now but we will go into more performant losses later
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined our data, embedding model, and loss we can start training
    our model. We can do that using the `fit` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We train our model for a single epoch which takes roughly an hour or so on a
    V100 GPU. And… that’s it! We have now trained our own embedding model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The sentence-transformers framework allows for multi-task learning. The `train_objectives`
    parameter accepts a list of tuples which makes it possible to give it different
    datasets each with their own objective to optimize for. This means that we could
    give it the entire GLUE benchmark to train on.
  prefs: []
  type: TYPE_NORMAL
- en: We can perform a quick evaluation of the performance of our model. There are
    many tasks for doing, which we will go in-depth later, but a good one to start
    with is the Semantic Textual Similarity Benchmark (STSB) that is found in the
    GLUE dataset as we have seen before.
  prefs: []
  type: TYPE_NORMAL
- en: It is a collection of sentence pairs labeled, through human annotation, with
    similarity scores between 1 and 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can leverage this dataset to see how well our model scores on a semantic
    similarity task. First, we will need to process the STSB dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these samples to generate an evaluator using sentence-transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This evaluator allows us to evaluate any model, so let’s compare our trained
    model with its untrained variant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This training procedure improved the baseline score from 0.61 to 0.74!
  prefs: []
  type: TYPE_NORMAL
- en: In-depth Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good embedding model is more than just a good score on the STSB benchmark!
    As we have seen before, the GLUE benchmark has a number of tasks for which we
    can evaluate our embedding model. However, there exist many more benchmarks that
    allow for the evaluation of embedding models. To unify this evaluation procedure,
    the Massive Text Embedding Benchmark (MTEB)^([4](ch07.html#id301)) was developed.
    This MTEB spans 8 embedding tasks that cover 58 datasets and 112 languages.
  prefs: []
  type: TYPE_NORMAL
- en: To publicly compare the state-of-the-art embedding models, a [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    was created with the scores of each embedding model across all tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we inspect the results, we can see a number of evaluation metrics for
    this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The great thing about this evaluation benchmark is not only the diversity of
    the tasks and languages but that even the evaluation time is saved. Although many
    embedding models exist, we typically want those that are both accurate and have
    low latency. The tasks for which embedding models are used, like semantic search,
    often benefit from and require to have fast inference.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We trained [our model](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli)
    using SoftMaxLoss to illustrate how one of the first sentence-transformers models
    was trained. However, not only is there a large variety of loss functions to choose
    from, SoftMaxLoss is generally not advised as there are more performant losses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of going through every single loss function out there, there are two
    loss functions that are typically used and seem to perform generally well, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: Cosine Similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple Negatives Ranking Loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many more loss functions to choose from than just those discussed
    here. For example, a loss like MarginMSE works great for training or fine-tuning
    a cross-encoder. There are a number of interesting [loss functions](https://www.sbert.net/docs/package_reference/losses.html)
    implemented in the sentence-transformers framework.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine Similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Cosine Similarity Loss is an intuitive and easy-to-use loss that works across
    many different use cases and datasets. However, this loss is typically used in
    semantic textual similarity tasks. In these tasks, a similarity score is assigned
    to the pairs of texts over which we optimize the model.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of having strictly positive or negative pairs of sentences, we assume
    to have pairs of sentences that are similar or dissimilar to a certain degree.
    Typically, this value lies between 0 and 1 to indicate dissimilarity and similarity
    respectively ([Figure 7-9](#fig_9_the_cosine_similarity_loss_aims_to_minimize_the_co)).
  prefs: []
  type: TYPE_NORMAL
- en: '![The Cosine Similarity Loss aims to minimize the cosine distance between semantically
    similar sentences and to maximize the distance between semantically dissimilar
    sentences. ](assets/creating_text_embedding_models_249519_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. The Cosine Similarity Loss aims to minimize the cosine distance
    between semantically similar sentences and to maximize the distance between semantically
    dissimilar sentences.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Cosine Similarity Loss is straightforward–it calculates the cosine similarity
    between the two embeddings of the two texts and compares that to the labeled similarity
    score. The model will learn to recognize the degree of similarity between sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'A minimal example of training with Cosine Similarity Loss would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We are using the STSB dataset for this example. As we have seen before, they
    are pairs of sentences with annotated similarity scores that lend themselves naturally
    to the Cosine Similarity Loss.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: A score of 0.848 is a big improvement compared to the SoftMaxLoss example. However,
    since our training and evaluation are both on the same tasks in contrast to the
    SoftMaxLoss example, comparing them would not be fair.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Negatives Ranking Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiple Negatives Ranking (MNR^([5](ch07.html#id302))) loss, often referred
    to as InfoNCE^([6](ch07.html#id303)) or NTXentLoss,^([7](ch07.html#id304)) is
    a loss that, in principle, only uses positive pairs of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might have pairs of question/answer, image/image caption, paper
    title/paper abstract, etc. The great thing about these pairs is that we can be
    confident they are hard positive pairs. In MNR Loss ([Figure 7-10](#fig_10_multiple_negatives_ranking_loss_aims_to_minimize_t)),
    negative pairs are constructed by mixing a positive pair with another positive
    pair. In the example of a paper title and abstract, you would generate a negative
    pair by combining the title of a paper with a completely different abstract. These
    negatives are called in-batch negatives.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple Negatives Ranking Loss aims to minimize the distance between related
    pairs of text  such as questions and answers  and maximize the distance between
    unrelated pairs  such as questions and unrelated answers. ](assets/creating_text_embedding_models_249519_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. Multiple Negatives Ranking Loss aims to minimize the distance
    between related pairs of text, such as questions and answers, and maximize the
    distance between unrelated pairs, such as questions and unrelated answers.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After having generated these positive and negative pairs, we calculate their
    embeddings and apply cosine similarity. These similarity scores are then used
    to answer the question, are these pairs negative or positive? In other words,
    it is treated as a classification task and we can use cross-entropy loss to optimize
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a minimal example of training with MNR loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we used the same data as in our SoftMaxLoss example. Let’s compare
    the performance of this model with that of our previously trained models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Compared to our previously trained model with SoftMaxLoss (0.74), our model
    with MNR Loss (0.82) seems to be much more accurate!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Larger batch sizes tend to be better with MNR Loss as a larger batch makes the
    task more difficult. The reason for this is that the model needs to find the best
    matching sentence from a larger set of potential pairs of sentences. You can adapt
    the code to try out different batch sizes and get a feeling of its effects.
  prefs: []
  type: TYPE_NORMAL
- en: There is a downside to how we used this loss function. Since negatives are sampled
    from other question/answer pairs, these in-batch or “easy” negatives that we used
    could potentially be completely unrelated to the question. As a result, the embedding
    model’s task of then finding the right answer to a question becomes quite easy.
    Instead, we would like to have negatives that are very related to the question
    but not the right answer. These negatives are called hard negatives. Since this
    makes the task more difficult for the embedding model as it has to learn more
    nuanced representations, the embedding model’s performance generally improves
    quite a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In MNR Loss, cosine similarity is often used but could be replaced with the
    dot product instead. An advantage of using the dot product is that it tends to
    work better for longer texts as the dot product will increase. A disadvantage,
    however, is that it generally works less well for clustering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example of a hard negative is the following. Let’s assume we have the
    following question: “How many people live in Amsterdam?”. A related answer to
    this question would be: “Almost a million people live in Amsterdam”. To generate
    a good hard negative, we ideally want the answer to contain something about Amsterdam
    and the number of people living in this city. For example: “More than a million
    people live in Utrecht, which is more than Amsterdam.” This answer is unrelated
    to the question but very similar, so this would be a good hard negative. [Figure 7-11](#fig_11__negatives_png_an_easy_negative_is_typically_unre)
    illustrates the differences between easy and hard negatives.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ negatives.png  An easy negative is typically unrelated to both the question
    and answer. A semi hard negative has some similarities to the topic of the question
    and answer but is somewhat unrelated. A hard negative is very similar to the question
    but is generally the wrong answer. ](assets/creating_text_embedding_models_249519_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-11\. [negatives.png] An easy negative is typically unrelated to both
    the question and answer. A semi-hard negative has some similarities to the topic
    of the question and answer but is somewhat unrelated. A hard negative is very
    similar to the question but is generally the wrong answer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using hard negatives with this loss is rather straightforward, instead of passing
    two related texts to the `InputExample`, we pass three texts, two related texts,
    and the last text a hard-negative.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning an Embedding Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we went through the basics of training an embedding
    model from scratch and saw how we could leverage loss functions to further optimize
    its performance. This method, although quite powerful, requires creating an embedding
    model from scratch. This process can be quite costly and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, sentence-transformers have a number of pre-trained embedding models
    that we can use as a base for fine-tuning. They are trained on large amounts of
    data and already generate great performance out-of-the-box.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ways to fine-tune your model, depending on the data availability
    and domain. We will go through a number of them and demonstrate the strength of
    leveraging pre-trained embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most straightforward way to fine-tune an embedding model is to repeat the
    process of training our model as we did before, but replace the `'bert-base-uncased'`
    with a pre-trained sentence-transformers model. There are many to choose from
    but generally, the `'all-mpnet-base-v2'` ([https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html))
    performs well across many use cases (see [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we would only need to run the following to fine-tune our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the same data as we used to train our model in the Multiple Negatives
    Ranking Loss example. The code for fine-tuning your model is rather straightforward
    thanks to the incredibly well-built sentence-transformers package.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of using a pre-trained BERT model like `'``bert``-base-uncased'` or
    a possible out-of-domain model like `'all-mpnet-base-v2'`, you can also perform
    Masked Language Modeling on the pre-trained BERT model to first adapt it to your
    domain. Then, you can use this fine-tuned BERT model as the base for training
    your embedding model. This is a form of domain adaptation. You can find more about
    Masked Language Modeling in Chapter X.
  prefs: []
  type: TYPE_NORMAL
- en: The main difficulty of fine-tuning/training your model is finding the right
    data. With these models, we not only want to have very large datasets, the data
    in itself needs to be of high quality. Developing positive pairs is generally
    straightforward but adding hard negative pairs significantly increases the difficulty
    of creating quality data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Training on the data, as shown above, is a bit redundant since the model was
    already trained on a very similar NLI dataset. However, the procedure remains
    the same for fine-tuning it on your domain-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented SBERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A disadvantage of training or fine-tuning these embedding models is that they
    often require substantial training data. Many of these models are trained with
    more than a billion sentence pairs. Extracting such a high number of sentence
    pairs for your use case is generally not possible as in many cases, there is only
    little data available.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is a way to augment your data such that an embedding model
    can be fine-tuned when there is only a little labeled data available. This procedure
    is referred to as Augmented SBERT.^([8](ch07.html#id305))
  prefs: []
  type: TYPE_NORMAL
- en: In this procedure, we aim to augment the small amount of labeled data such that
    they can be used for regular training. It makes use of the slow and more accurate
    cross-encoder architecture (BERT) to augment and label a larger set of input pairs.
    These newly labeled pairs are then used for fine-tuning a bi-encoder (SBERT).
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Figure 7-12](#fig_12_augmented_sbert_works_through_training_a_high_perf),
    Augmented SBERT involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune a cross-encoder (BERT) using a small annotated dataset (gold dataset)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create new sentence pairs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Label new sentence pairs with the fine-tuned cross-encoder (silver dataset)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a bi-encoder (SBERT) on the extended dataset (gold + silver dataset)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, a gold dataset is a small but fully annotated dataset that holds the ground
    truth. A silver dataset is also fully annotated but is not necessarily the ground
    truth as it was generated through predictions of the cross-encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Augmented SBERT works through training a high performant cross encoder on
    a small gold dataset. Then  the trained cross encoder can be used to label an
    unlabeled dataset to generate the silver dataset which is much bigger than the
    gold dataset. Finally  both the gold and silver datasets are used to train the
    bi encoder.](assets/creating_text_embedding_models_249519_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-12\. Augmented SBERT works through training a high-performant cross-encoder
    on a small gold dataset. Then, the trained cross-encoder can be used to label
    an unlabeled dataset to generate the silver dataset which is much bigger than
    the gold dataset. Finally, both the gold and silver datasets are used to train
    the bi-encoder.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Before we get into the steps above, let us first prepare the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the train split of the STSB corpus as our gold dataset and use it to
    train our cross-encoder (step 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After having trained our cross-encoder, we can generate new candidate sentence
    pairs by simply random sampling 10 sentences for each input sentence (step 2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of randomly sampling the silver sentence pairs, we can also use pre-trained
    sentence-transformers. By retrieving the top-k sentences from the dataset using
    semantic search, the silver sentence pairs that we create tend to be more accurate.
    Although the sentence pairs are still chosen based on an approximation it is much
    better than random sampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-encoder that we trained in step 1 can then be used to label the candidate
    sentence pairs generated in the previous step to build up the silver dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a silver and gold dataset, we simply combine them and train
    our embedding model as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can run the above again with only the gold dataset and see how adding this
    silver dataset influences the performance. Training on only the gold dataset results
    in a performance of 0.804 whereas adding the silver dataset ups the performance
    to 0.830!
  prefs: []
  type: TYPE_NORMAL
- en: This method allows for increasing the datasets that you already have available
    without the need to manually label hundreds of thousands of sentence pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create an embedding model, we typically need labeled data. However, not all
    real-world datasets come with a nice set of labels that we can use. We would instead
    look for techniques to train the model without any pre-determined labels–unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised techniques for creating or fine-tuning an embedding model generally
    perform worse than their supervised alter-egos. Many approaches exist, like Simple
    Contrastive Learning of Sentence Embeddings (SimCSE)^([9](ch07.html#id306)), Contrastive
    Tension (CT)^([10](ch07.html#id307)), Tranformer-based Denoising AutoEncoder (TSDAE)^([11](ch07.html#id308)),
    and Generative Pseudo-Labeling (GPL)^([12](ch07.html#id309)) .
  prefs: []
  type: TYPE_NORMAL
- en: We will be going through two methods, TSDAE and GPL which we can even combine
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based Denoising AutoEncoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TSDAE is a very elegant approach to creating an embedding model with unsupervised
    learning. The method assumes that we have no labeled data at all and does not
    require us to artificially create labels.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying idea of TSDAE is that we add noise to the input sentence by removing
    a certain percentage of words from it. This “damaged” sentence is put through
    an encoder, with a pooling layer on top of it, to map it to a sentence embedding.
  prefs: []
  type: TYPE_NORMAL
- en: From this sentence embedding, a decoder tries to reconstruct the original sentence
    from the “damaged” sentence but without the artificial noise.
  prefs: []
  type: TYPE_NORMAL
- en: This method is very similar to Masked Language Modeling, where we try to reconstruct
    and learn certain masked words. Here, instead of reconstructing masked words,
    we try to reconstruct the entire sentence.
  prefs: []
  type: TYPE_NORMAL
- en: After training, we can use the encoder to generate embeddings from text since
    the decoder is only used for judging whether the embeddings can accurately reconstruct
    the original sentence ([Figure 7-13](#fig_13_tsdae_randomly_removes_words_from_an_input_sentenc)).
  prefs: []
  type: TYPE_NORMAL
- en: '![TSDAE randomly removes words from an input sentence which is passed through
    an encoder to generate a sentence embedding. From this sentence embedding  the
    original sentence is reconstructed. The main idea here is that the more accurate
    the sentence embedding is  the more accurate the reconstructed sentence will be.](assets/creating_text_embedding_models_249519_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-13\. TSDAE randomly removes words from an input sentence which is passed
    through an encoder to generate a sentence embedding. From this sentence embedding,
    the original sentence is reconstructed. The main idea here is that the more accurate
    the sentence embedding is, the more accurate the reconstructed sentence will be.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since we only need a bunch of sentences without any labels, training this model
    is straightforward. We start by defining our model as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that we are using CLS pooling instead of mean pooling. The authors found
    that there was little difference between them and since mean pooling loses positional
    information, they choose CLS pooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a bunch of sentences to be used for our model using the STSB
    dataset that we used before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The main difference between this and how we performed supervised modeling before
    is that we pass our data to the `DenoisingAutoEncoderDataset`. This will generate
    noise to the input during training. We also defined our encoder loss using the
    same base model as we used for our embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we only need to fit and evaluate our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After fitting our model, we get a score of 0.74 which is quite impressive considering
    we did all this training with unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Domain Adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have very little or no labeled data available, you typically use unsupervised
    learning to create your text embedding model. However, unsupervised techniques
    are generally outperformed by supervised techniques and have difficulty learning
    domain-specific concepts.
  prefs: []
  type: TYPE_NORMAL
- en: This is where *domain adaptation* comes in. Its goal is to update existing embedding
    models to a specific textual domain that contains different subjects from the
    source domain. [Figure 7-14](#fig_14_in_domain_adaptation_the_aim_is_to_create_and_gen)
    demonstrates how domains can differ in content.
  prefs: []
  type: TYPE_NORMAL
- en: '![In domain adaptation  the aim is to create and generalize an embedding model
    from one domain to another. The target domain  or out domain  generally contains
    words and subjects that were not found in the source domain or in domain. ](assets/creating_text_embedding_models_249519_14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-14\. In domain adaptation, the aim is to create and generalize an embedding
    model from one domain to another. The target domain, or out-domain, generally
    contains words and subjects that were not found in the source domain or in-domain.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One method for domain adaptation is called *adaptive pre-training*. You start
    by pre-training your domain-specific corpus using an unsupervised technique, such
    as the previously discussed TSDAE or even Masked Language Modeling. Then, as illustrated
    in [Figure 7-15](#fig_15_domain_adaptation_can_be_performed_with_adaptive_p),
    you fine-tune that model using a training dataset in your target domain.
  prefs: []
  type: TYPE_NORMAL
- en: This procedure leverages a pipeline that we have seen before, first using TSDAE
    to fine-tune an LLM or existing embedding model which is further fine-tuned using
    supervised or even augmented SBERT training.
  prefs: []
  type: TYPE_NORMAL
- en: This, however, can be computationally expensive since we must first pre-train
    our data on a large corpus and then use supervised learning with a labeled dataset.
    Typically, the labeled datasets need to be large and can require millions of training
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Domain adaptation can be performed with adaptive pre training and adaptive
    fine tuning. With adaptive pre training  we first apply unsupervised tuning of
    an LLM on the target domain after which we fine tune that model on labeled non
    domain specific data. With adaptive fine tuning  we take any existing pre training
    embedding model and fine tune it using unlabeled domain specific data.](assets/creating_text_embedding_models_249519_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-15\. Domain adaptation can be performed with adaptive pre-training
    and adaptive fine-tuning. With adaptive pre-training, we first apply unsupervised
    tuning of an LLM on the target domain after which we fine-tune that model on labeled
    non-domain specific data. With adaptive fine-tuning, we take any existing pre-training
    embedding model and fine-tune it using unlabeled domain-specific data.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead, we are going to be using a method that can be run on top of a pre-trained
    embedding model instead, namely *Generative Pseudo-Labeling*.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Pseudo-Labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative Pseudo-Labeling assumes that although we have data, none of it is
    labeled. It consists of three steps for generating labeled data that we can use
    for training an embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: First, for each unlabeled text that you have in your domain-specific data, we
    use a generative model, like T5, to generate a number of queries. These queries
    are generally questions that can be answered with part(s) of the input text. For
    example, when your text is “Coco is a movie produced by Pixar”, the model might
    generate a query like “Who produced the movie Coco?”. These are the positive examples
    that we generate as we illustrate in [Figure 7-16](#fig_16_in_the_first_step_of_gpl_s_pipeline_queries_are_g).
  prefs: []
  type: TYPE_NORMAL
- en: '![In the first step of GPL s pipeline  queries are generated with a generative
    model  like T5  for each unlabeled input text. These queries can be used as sentence
    pairs for labeling later on in the GPL pipeline.](assets/creating_text_embedding_models_249519_16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-16\. In the first step of GPL’s pipeline, queries are generated with
    a generative model, like T5, for each unlabeled input text. These queries can
    be used as sentence pairs for labeling later on in the GPL pipeline.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Second, we will also need to have negative examples for our model to learn.
    Preferably, a negative example is related to the query but is not the relevant
    answer. For example, a negative example for the query “Who produced the movie
    *Coco*?” would be “*The Lion King* was produced by Disney”.
  prefs: []
  type: TYPE_NORMAL
- en: To extract these negative examples, we use a pre-training embedding model to
    retrieve all texts that are relevant to the query. In [Figure 7-17](#fig_17_in_the_second_step_of_gpl_s_pipeline_negatives_ar),
    this second step shows how relevant texts are mined.
  prefs: []
  type: TYPE_NORMAL
- en: '![In the second step of GPL s pipeline  negatives are mined using a pre trained
    embedding model. This model generates embeddings for the input query and the corpus
    and finds those that relate to the input query. ](assets/creating_text_embedding_models_249519_17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-17\. In the second step of GPL’s pipeline, negatives are mined using
    a pre-trained embedding model. This model generates embeddings for the input query
    and the corpus and finds those that relate to the input query.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Third, after we generate the negative examples, we will need to score them.
    Some of the negative examples might end up being the actual answer or they might
    not be relevant, both of which we want to prevent. As demonstrated in [Figure 7-18](#fig_18_in_the_third_step_of_gpl_s_pipeline_we_use_a_cros),
    we can use a pre-trained cross-encoder to score the query/passage pairs that we
    have created through the previous two steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![In the third step of GPL s pipeline  we use a cross encoder to score all
    query passage pairs. The goal of this procedure is to filter out any false negatives.
    Some negatives might accidentally turn out to be positives. Instead of a negative
    vs. positive labeling procedure  using a scoring procedure allows for more nuance
    in the representations.](assets/creating_text_embedding_models_249519_18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-18\. In the third step of GPL’s pipeline, we use a cross-encoder to
    score all query/passage pairs. The goal of this procedure is to filter out any
    false negatives. Some negatives might accidentally turn out to be positives. Instead
    of a negative vs. positive labeling procedure, using a scoring procedure allows
    for more nuance in the representations.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After this final step, for each query, we now have a positive example (step
    1) and a mined negative (step 2). In other words, we have triplets that we can
    use for our training procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Running each step individually is quite a hassle but fortunately, there is a
    GPL package that abstracts that difficulty away. However, we first need to format
    our data such that the package can read it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We are again using the STSB dataset and we extract just a small portion of the
    data for training. The reason for doing so is that training can still be quite
    expensive. The authors of GPL had to train for roughly a day on a V100 GPU, so
    trying out with a smaller dataset is advised before scaling up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can import GPL and only need to run `train` to start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: There are a few sub-models worth noting that describe the steps as we have seen
    them before
  prefs: []
  type: TYPE_NORMAL
- en: '`generator` refers to *step 1* where we generate queries for our data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retriever` refers to *step 2* where we generate queries for our data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_encoder` refers to *step 3* where we generate queries for our data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`base_ckpt` refers to the final step of training our embedding model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After training, we can load and evaluate the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: With GPL, we managed to get a score of 0.82 without any labeled data at all!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have taken a look at creating and fine-tuning embedding
    models through contrastive learning, one of the most important components of training
    such models. Through both unsupervised and supervised techniques, we were able
    to create embedding models tuned to our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch07.html#id298-marker)) Alan Garfinkel. “Forms of Explanation: Rethinking
    the Questions in Social Theory.” (1981).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch07.html#id299-marker)) Tim Miller. “Contrastive explanation: A structural-model
    approach”.*The Knowledge Engineering Review* 36 (2021): e14.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch07.html#id300-marker)) Jeffrey Pennington, Richard, Socher, and Christopher
    D, Manning. “Glove: Global vectors for word representation.” In *Proceedings of
    the 2014 conference on empirical methods in natural language processing (EMNLP)* (pp.
    1532–1543). 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch07.html#id301-marker)) Muennighoff, Niklas, Nouamane, Tazi, Loïc, Magne,
    and Nils, Reimers. “MTEB: Massive text embedding benchmark”.*arXiv preprint arXiv:2210.07316* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch07.html#id302-marker)) Matthew Henderson, Rami, Al-Rfou, Brian, Strope,
    Yun-Hsuan, Sung, Lászl\'o, Lukacs, Ruiqi, Guo, Sanjiv, Kumar, Balint, Miklos,
    and Ray, Kurzweil. “Efficient natural language response suggestion for smart reply.”
    *arXiv preprint arXiv:1705.00652* (2017).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch07.html#id303-marker)) Oord, Aaron van den, Yazhe, Li, and Oriol, Vinyals.
    “Representation learning with contrastive predictive coding”.*arXiv preprint arXiv:1807.03748* (2018).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch07.html#id304-marker)) Ting Chn, Simon, Kornblith, Mohammad, Norouzi,
    and Geoffrey, Hinton. “A simple framework for contrastive learning of visual representations.”
    . In *International conference on machine learning* (pp. 1597–1607).2020.
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch07.html#id305-marker)) Thakur, Nandan, Nils, Reimers, Johannes, Daxenberger,
    and Iryna, Gurevych. “Augmented sbert: Data augmentation method for improving
    bi-encoders for pairwise sentence scoring tasks”.*arXiv preprint arXiv:2010.08240* (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch07.html#id306-marker)) Gao, Tianyu, Xingcheng, Yao, and Danqi, Chen.
    “Simcse: Simple contrastive learning of sentence embeddings”.*arXiv preprint arXiv:2104.08821* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch07.html#id307-marker)) Janson, Sverker, Evangelina, Gogoulou, Erik,
    Ylipäa, Amaru, Cuba Gyllensten, and Magnus, Sahlgren. “Semantic re-tuning with
    contrastive tension.” In *International Conference on Learning Representations,
    2021*.2021.
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch07.html#id308-marker)) Kexin Wang, Nils, Reimers, and Iryna, Gurevych.
    “Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised
    sentence embedding learning”.*arXiv preprint arXiv:2104.06979* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch07.html#id309-marker)) Kexin Wang, Nandan, Thakur, Nils, Reimers,
    and Iryna, Gurevych. “Gpl: Generative pseudo labeling for unsupervised domain
    adaptation of dense retrieval”.*arXiv preprint arXiv:2112.07577* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: About the Authors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Jay Alammar** is Director and Engineering Fellow at Cohere (pioneering provider
    of large language models as an API). In this role, he advises and educates enterprises
    and the developer community on using language models for practical use cases).
    Through his popular AI/ML blog, Jay has helped millions of researchers and engineers
    visually understand machine learning tools and concepts from the basic (ending
    up in the documentation of packages like NumPy and pandas) to the cutting-edge
    (Transformers, BERT, GPT-3, Stable Diffusion). Jay is also a co-creator of popular
    machine learning and natural language processing courses on Deeplearning.ai and
    Udacity.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Maarten Grootendorst** is a Senior Clinical Data Scientist at IKNL (Netherlands
    Comprehensive Cancer Organization). He holds master’s degrees in organizational
    psychology, clinical psychology, and data science which he leverages to communicate
    complex Machine Learning concepts to a wide audience. With his popular blogs,
    he has reached millions of readers by explaining the fundamentals of Artificial
    Intelligence–often from a psychological point of view. He is the author and maintainer
    of several open-source packages that rely on the strength of Large Language Models,
    such as BERTopic, PolyFuzz, and KeyBERT. His packages are downloaded millions
    of times and used by data professionals and organizations worldwide.'
  prefs: []
  type: TYPE_NORMAL
