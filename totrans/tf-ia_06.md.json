["```py\nimport tensorflow as tf\nimport numpy as np\n\nn_seq = 7\nx = tf.constant(np.random.normal(size=(1,n_seq,512)))\nWq = tf.Variable(np.random.normal(size=(512,512)))\nWk = tf.Variable (np.random.normal(size=(512,512)))\nWv = tf.Variable (np.random.normal(size=(512,512)))\n```", "```py\n>>> x.shape=(1, 7, 512)\n>>> Wq.shape=(1, 512)\n>>> Wk.shape=(1, 512)\n>>> Wv.shape=(1, 512)\n```", "```py\nimport tensorflow as tf\nimport tensorflow.keras.layers as layers\n\nclass SelfAttentionLayer(layers.Layer):\n\n    def __init__(self, d):\n        super(SelfAttentionLayer, self).__init__()\n        self.d = d                                                                ❶\n\n    def build(self, input_shape):\n        self.Wq = self.add_weight(                                                ❷\n            shape=(input_shape[-1], self.d), initializer='glorot_uniform',        ❷\n            trainable=True, dtype='float32'                                       ❷\n        )        \n        self.Wk = self.add_weight(                                                ❷\n            shape=(input_shape[-1], self.d), initializer='glorot_uniform',        ❷\n            trainable=True, dtype='float32'                                       ❷\n        )\n        self.Wv = self.add_weight(                                                ❷\n            shape=(input_shape[-1], self.d), initializer='glorot_uniform',        ❷\n            trainable=True, dtype='float32'                                       ❷\n        )\n\n    def call(self, q_x, k_x, v_x):\n        q = tf.matmul(q_x,self.Wq)                                                ❸\n        k = tf.matmul(k_x,self.Wk)                                                ❸\n        v = tf.matmul(v_x,self.Wv)                                                ❸\n\n        p = tf.nn.softmax(tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d))    ❹\n        h = tf.matmul(p, v)                                                       ❺\n        return h,p\n```", "```py\nlayer = SelfAttentionLayer(512)\nh, p = layer(x, x, x)\nprint(h.shape)\n```", "```py\n>>> (1, 7, 512)\n```", "```py\nx = tf.constant(np.random.normal(size=(1,10,256)))\n```", "```py\nimport tensorflow as tf\n\nclass SelfAttentionLayer(layers.Layer):\n\n    def __init__(self, d):\n        ...\n\n    def build(self, input_shape):\n        ...\n\n    def call(self, q_x, k_x, v_x, mask=None):   ❶\n        q = tf.matmul(x,self.Wq)\n        k = tf.matmul(x,self.Wk)\n        v = tf.matmul(x,self.Wv)\n\n        p = tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d)\n        p = tf.squeeze(p)\n        if mask is None:\n            p = tf.nn.softmax(p)                ❷\n        else:\n            p += mask * -1e9                    ❸\n            p = tf.nn.softmax(p)                ❸\n\n        h = tf.matmul(p, v)\n        return h,p\n```", "```py\nmask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n```", "```py\n>>> tf.Tensor(\n    [[0\\. 1\\. 1\\. 1\\. 1\\. 1\\. 1.]\n     [0\\. 0\\. 1\\. 1\\. 1\\. 1\\. 1.]\n     [0\\. 0\\. 0\\. 1\\. 1\\. 1\\. 1.]\n     [0\\. 0\\. 0\\. 0\\. 1\\. 1\\. 1.]\n     [0\\. 0\\. 0\\. 0\\. 0\\. 1\\. 1.]\n     [0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 1.]\n     [0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]], shape=(7, 7), dtype=float32)\n```", "```py\nlayer = SelfAttentionLayer(512)\nh, p = layer(x, x, x, mask)\nprint(p.numpy())\n```", "```py\n>>> [[1\\.    0\\.    0\\.    0\\.    0\\.    0\\.    0\\.   ]\n     [0.37  0.63  0\\.    0\\.    0\\.    0\\.    0\\.   ]\n     [0.051 0.764 0.185 0\\.    0\\.    0\\.    0\\.   ]\n     [0.138 0.263 0.072 0.526 0\\.    0\\.    0\\.   ]\n     [0.298 0.099 0.201 0.11  0.293 0\\.    0\\.   ]\n     [0.18  0.344 0.087 0.25  0.029 0.108 0\\.   ]\n     [0.044 0.044 0.125 0.284 0.351 0.106 0.045]]\n```", "```py\nmulti_attn_head = [SelfAttentionLayer(64) for i in range(8)]\noutputs = [head(x, x, x)[0] for head in multi_attn_head]\noutputs = tf.concat(outputs, axis=-1)\nprint(outputs.shape)\n```", "```py\n>>> (1, 7, 512)\n```", "```py\nimport tensorflow as tf\n\nclass FCLayer(layers.Layer):\n    def __init__(self, d1, d2):\n        super(FCLayer, self).__init__()\n        self.d1 = d1                                                       ❶\n        self.d2 = d2                                                       ❷\n\n    def build(self, input_shape):\n        self.W1 = self.add_weight(                                         ❸\n            shape=(input_shape[-1], self.d1), initializer='glorot_uniform',❸\n            trainable=True, dtype='float32'                                ❸\n        )\n        self.b1 = self.add_weight(                                         ❸\n            shape=(self.d1,), initializer='glorot_uniform',                ❸\n            trainable=True, dtype='float32'                                ❸\n        )        \n        self.W2 = self.add_weight(                                         ❸\n            shape=(input_shape[-1], self.d2), initializer='glorot_uniform',❸\n            trainable=True, dtype='float32'                                ❸\n        )\n        self.b2 = self.add_weight(                                         ❸\n            shape=(self.d2,), initializer='glorot_uniform',                ❸\n            trainable=True, dtype='float32'                                ❸\n        )  \n\n    def call(self, x):\n        ff1 = tf.nn.relu(tf.matmul(x,self.W1)+self.b1)                     ❹\n        ff2 = tf.matmul(ff1,self.W2)+self.b2                               ❺\n        return ff2\n```", "```py\nimport tensorflow as tf\nimport tensorflow.keras.layers as layers\n\nclass FCLayer(layers.Layer):\n\n    def __init__(self, d1, d2):\n        super(FCLayer, self).__init__()\n        self.dense_layer_1 = layer.Dense(d1, activation='relu')  ❶\n        self.dense_layer_2 = layers.Dense(d2)                    ❷\n\n    def call(self, x):\n        ff1 = self.dense_layer_1(x)                              ❸\n        ff2 = self.dense_layer_2(ff1)                            ❹\n        return ff2\n```", "```py\nimport tensorflow as tf\n\nclass EncoderLayer(layers.Layer):\n\n    def __init__(self, d, n_heads):\n        super(EncoderLayer, self).__init__()\n        self.d = d\n        self.d_head = int(d/n_heads) \n        self.n_heads = n_heads\n        self.attn_heads = [\n            SelfAttentionLayer(self.d_head) for i in range(self.n_heads)\n        ]                                           ❶\n        self.fc_layer = FCLayer(2048, self.d)       ❷\n\n    def call(self, x):\n        def compute_multihead_output(x):            ❸\n            outputs = [head(x, x, x)[0] for head in self.attn_heads] \n            outputs = tf.concat(outputs, axis=-1)\n            return outputs\n\n        h1 = compute_multihead_output(x)            ❹\n        y = self.fc_layer(h1)                       ❺\n\n        return y\n```", "```py\nimport tensorflow as tf\n\nclass DecoderLayer(layers.Layer):\n\n    def __init__(self, d, n_heads):\n        super(DecoderLayer, self).__init__()\n        self.d = d\n        self.d_head = int(d/n_heads)\n        self.dec_attn_heads = [\n            SelfAttentionLayer(self.d_head) for i in range(n_heads)\n        ]                                                           ❶\n        self.attn_heads = [\n            SelfAttentionLayer(self.d_head) for i in range(n_heads)\n        ]                                                           ❷\n        self.fc_layer = FCLayer(2048, self.d)                       ❸\n\n    def call(self, de_x, en_x, mask=None):\n        def compute_multihead_output(de_x, en_x, mask=None):        ❹\n            outputs = [\n                head(en_x, en_x, de_x, mask)[0] for head in \n➥ self.attn_heads]                                                 ❺\n            outputs = tf.concat(outputs, axis=-1)\n            return outputs\n\n        h1 = compute_multihead_output(de_x, de_x, mask)             ❻\n        h2 = compute_multihead_output(h1, en_x)                     ❼\n        y = self.fc_layer(h2)                                       ❽\n        return y\n```", "```py\nimport tensorflow as tf\n\nn_steps = 25                                                              ❶\nn_en_vocab = 300                                                          ❶\nn_de_vocab = 400                                                          ❶\nn_heads = 8                                                               ❶\nd = 512                                                                   ❶\nmask = 1 - tf.linalg.band_part(tf.ones((n_steps, n_steps)), -1, 0)        ❷\n\nen_inp = layers.Input(shape=(n_steps,))                                   ❸\nen_emb = layers.Embedding(n_en_vocab, 512, input_length=n_steps)(en_inp)  ❹\nen_out1 = EncoderLayer(d, n_heads)(en_emb)                                ❺\nen_out2 = EncoderLayer(d, n_heads)(en_out1)\n\nde_inp = layers.Input(shape=(n_steps,))                                   ❻\nde_emb = layers.Embedding(n_de_vocab, 512, input_length=n_steps)(de_inp)  ❼\nde_out1 = DecoderLayer(d, n_heads)(de_emb, en_out2, mask)                 ❽\nde_out2 = DecoderLayer(d, n_heads)(de_out1, en_out2, mask)\nde_pred = layers.Dense(n_de_vocab, activation='softmax')(de_out2)         ❾\n\ntransformer = models.Model(\n    inputs=[en_inp, de_inp], outputs=de_pred, name='MinTransformer'       ❿\n)\ntransformer.compile(\n    loss='categorical_crossentropy', optimizer='adam', metrics=['acc']\n)\n```", "```py\nlayers.Embedding(n_en_vocab, 512, input_length=n_steps)\n```", "```py\ntransformer = models.Model(\n    inputs=[en_inp, de_inp], outputs=de_pred, name=’MinTransformer’\n)\ntransformer.compile(\n    loss='categorical_crossentropy', optimizer='adam', metrics=['acc']\n)\n```", "```py\ntransformer.summary()\n```", "```py\nModel: \"MinTransformer\"\n_____________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                \n=============================================================================================\ninput_1 (InputLayer)            [(None, 25)]         0                                       \n_____________________________________________________________________________________________\nembedding (Embedding)           (None, 25, 512)      153600      input_1[0][0]               \n_____________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 25)]         0                                       \n_____________________________________________________________________________________________\nencoder_layer (EncoderLayer)    (None, 25, 512)      2886144     embedding[0][0]             \n_____________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 25, 512)      204800      input_2[0][0]               \n_____________________________________________________________________________________________\nencoder_layer_1 (EncoderLayer)  (None, 25, 512)      2886144     encoder_layer[0][0]         \n_____________________________________________________________________________________________\ndecoder_layer (DecoderLayer)    (None, 25, 512)      3672576     embedding_1[0][0]           \n                                                                 encoder_layer_1[0][0]       \n_____________________________________________________________________________________________\ndecoder_layer_1 (DecoderLayer)  (None, 25, 512)      3672576     decoder_layer[0][0]         \n                                                                 encoder_layer_1[0][0]       \n_____________________________________________________________________________________________\ndense (Dense)                   (None, 25, 400)      205200      decoder_layer_1[0][0]       \n=============================================================================================\nTotal params: 13,681,040\nTrainable params: 13,681,040\nNon-trainable params: 0\n_____________________________________________________________________________________________\n```", "```py\nWq = tf.Variable(np.random.normal(size=(256,512)))\nWk = tf.Variable (np.random.normal(size=(256,512)))\nWv = tf.Variable (np.random.normal(size=(256,512)))\n```", "```py\nmulti_attn_head = [SelfAttentionLayer(512) for i in range(8)]\noutputs = [head(x)[0] for head in multi_attn_head]\noutputs = tf.math.add_n(outputs)\n```"]