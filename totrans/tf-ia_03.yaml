- en: 2 TensorFlow 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: What TensorFlow 2 is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important data structures and operations in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common neural network related operations in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that TensorFlow is an end-to-end machine
    learning framework predominantly used for implementing deep neural networks. TensorFlow
    is skillful at converting these deep neural networks to computational graphs that
    run faster on optimized hardware (e.g., GPUs and TPUs). But keep in mind that
    this is not the only use for TensorFlow. Table 2.1 delineates other areas TensorFlow
    supports.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 Various features offered in TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: '| Probabilistic machine learning | TensorFlow supports implementing probabilistic
    machine learning models. For example, models like Bayesian neural networks can
    be implemented with a TensorFlow API ([https://www.tensorflow.org/probability](https://www.tensorflow.org/probability)).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Computer graphics-related computations | Computer graphic computations can
    be mostly achieved on GPUs (e.g., simulating various lighting effects, raytracing;
    [https://www.tensorflow.org/graphics](https://www.tensorflow.org/graphics)). |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow Hub: Reusable (pretrained) models | In deep learning we usually
    try to leverage models that have already been trained on large amounts of data
    for the downstream tasks we’re interested in solving. TensorFlow Hub is a repository
    in which such models implemented in TensorFlow are stored ([https://www.tensorflow.org/hub](https://www.tensorflow.org/hub)).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Visualize/debug TensorFlow models | TensorFlow provides a dashboard for visualizing
    and monitoring model performance and even visualizing data ([https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)).
    |'
  prefs: []
  type: TYPE_TB
- en: In the coming chapters, we will go on an exciting journey exploring the bells
    and whistles in TensorFlow and learning how to excel at things TensorFlow is good
    at. In other words, we will look at how to solve real-world problems with TensorFlow,
    such as image classification (i.e., recognizing objects in images), sentiment
    analysis (i.e., recognizing positive/negative tones in reviews/opinions), and
    so on. While solving these tasks, you will learn how to overcome real-world challenges
    such as overfitting and class imbalance that can easily throw a spanner in the
    works. This chapter specifically focuses on providing a strong foundational knowledge
    of TensorFlow before we head toward complex problems that can be solved with deep
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will implement a neural network in both TensorFlow 2 and TensorFlow
    1 and see how much TensorFlow has evolved in terms of user friendliness. Then
    we will learn about basic units (e.g., variables, tensors, and operations) provided
    in TensorFlow, which we must have a good understanding of in order to develop
    solutions. Finally, we will understand the details of several complex mathematical
    operations through a series of fun computer vision exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 First steps with TensorFlow 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s imagine you are taking a machine learning course and have been given
    an assignment to implement a *multilayer perceptron* (MLP) (i.e., a type of neural
    network) and compute the final output for a given datapoint using TensorFlow.
    You are new to TensorFlow, so you go to the library and start studying what TensorFlow
    is. While you research, you realize that TensorFlow has two major versions (1
    and 2) and decide to use the latest and greatest: TensorFlow 2\. You’ve already
    installed the required libraries, as outlined in appendix A.'
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, let’s learn about MLPs. An MLP (figure 2.1) is a simple neural
    network that has an input layer, one or more hidden layers, and an output layer.
    These networks are also called *fully connected networks*.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Some research only uses the term MLP to refer to a network made of multiple
    perceptrons ([http://mng.bz/y4lE](http://mng.bz/y4lE)) organized in a hierarchical
    structure. However, in this book, we will use the terms MLP and fully connected
    network interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: In each layer, we have weights and biases, which are used to compute the output
    of that layer. In our example, we have an input of size 4, a hidden layer with
    three nodes, and an output layer of size 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-01](../../OEBPS/Images/02-01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1 Depiction of a multilayer perceptron (MLP) or a fully connected
    network. There are three layers: an input layer, a hidden layer (that has weights
    and biases), and an output layer. The output layer produces normalized probabilities
    as the output using softmax activation.'
  prefs: []
  type: TYPE_NORMAL
- en: The input values (*x*) are transformed to hidden values (*h*) using the following
    computation
  prefs: []
  type: TYPE_NORMAL
- en: '*h* = *σ*(*x W*[1] + *b*[1])'
  prefs: []
  type: TYPE_NORMAL
- en: where *σ* is the sigmoid function. The sigmoid function is a simple nonlinear
    element-wise transformation, as shown as in figure 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-02](../../OEBPS/Images/02-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 A visualization of the sigmoidal activation function for different
    inputs
  prefs: []
  type: TYPE_NORMAL
- en: '*x* is a matrix of size 1 × 4 (i.e., one row and four columns), *W*[1] is a
    matrix of size 4 × 3 (i.e., four rows and three columns), and *b*[1] is 1 × 4
    (i.e., one row and four columns). This gives an h of size 1 × 3\. Finally, the
    output is computed as'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *softmax*(*h W*[2] + *b*[2])'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *W*[2] is a 3 × 2 matrix, and *b*[2] is a 1 × 2 matrix. Softmax activation
    normalizes the linear scores of the last layer (i.e., *h W*[2] + *b*[2]) to actual
    probabilities (i.e., values sum up to 1 along columns). Assuming an input vector
    x of length *K*, the softmax activation produces a *K*-long vector *y*. The *i*^(th)
    element of *y* is computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![02_02a](../../OEBPS/Images/02_02a.png)'
  prefs: []
  type: TYPE_IMG
- en: where *y[i]* is the *i*^(th) output element and *x*[i] is the *i*^(th) input
    element. As a concrete example, assume the final layer without the softmax activation
    produced,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Applying the softmax normalization converts these values to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how this can be implemented in TensorFlow 2\. You can find the code
    in the Jupyter notebook (Ch02-Fundamentals-of-TensorFlow-2/2.1.Tensorflow_Fundamentals.ipynb).
    How to install the necessary libraries and set up the development environment
    is delineated in appendix A. Initially, we need to import the required libraries
    using import statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define the input to the network (*x*) and the variables (or parameters)
    (i.e., *w*[1], *b*[1], *w*[2], and *b*[2]) of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, *x* is a simple NumPy array of size 1 × 4 (i.e., one row and four columns)
    that is filled with values from a normal distribution. Then we define the parameters
    of the network (i.e., weights and biases) as TensorFlow variables. A tf.Variable
    behaves similar to a typical Python variable. It has some value attached at the
    time of the definition and can change over time. tf.Variable is used to represent
    weights and biases of a neural network, which are changed during the optimization
    or the training procedure. When defining TensorFlow variables, we need to provide
    an initializer and a shape for the variables. Here we are using an initializer
    that randomly sample values from a normal distribution. Remember that *W*[1] is
    4 × 3 sized, *b*[1] is 1 × 3 sized, *W*[2] is 3 × 2 sized, and *b*[2] is 1 × 2
    sized, and that the shape argument for each of these is set accordingly. Next,
    we define the core computations of the MLP as a nice modular function. This way,
    we can easily reuse the function to compute hidden layer outputs of multiple layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, act is any nonlinear activation function of your choice (e.g., tf.nn.sigmoid).
    (You can look at various activation functions here: [https://www.tensorflow.org/api_docs/python/tf/nn](https://www.tensorflow.org/api_docs/python/tf/nn).
    Be mindful that not all of them are activation functions. The expression tf.matmul(x,W)+b
    elegantly wraps the core computations we saw earlier (i.e., *x W*[1] + *b*[1]
    and *h W*[2] + *b*[2]) to a reusable expression. Here, tf.matmul performs the
    matrix multiplication operation. This computation is illustrated in figure 2.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![02-03](../../OEBPS/Images/02-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 The matrix multiplication and bias addition illustrated for example
    input, weights, and bias
  prefs: []
  type: TYPE_NORMAL
- en: Having @tf.function on top of the function is a way for TensorFlow to know that
    this function contains TensorFlow code. We will discuss the purpose of @tf.function
    in more detail in the next section. This brings us to the final part of the code.
    As we have the inputs, all the parameters, and core computations defined, we can
    compute the final output of the network
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: which will output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, h and y are the resulting tensors (of type tf.Tensor) of various TensorFlow
    operations (e.g., tf.matmul). The exact values in the output might differ slightly
    (see the following listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Multilayer perceptron network with TensorFlow 2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Importing NumPy and TensorFlow libraries
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The input to the MLP (a NumPy array)
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The initializer used to initialize variables
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The parameters of layer 1 (w1 and b2) and layer 2 (w2 and b2)
  prefs: []
  type: TYPE_NORMAL
- en: ❺ This line tells TensorFlow’s AutoGraph to build the graph.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ MLP layer computation, which takes in an input, weights, bias, and a nonlinear
    activation
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Computing the first hidden layer output, h
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Computing the final output, y
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at what happens behind the scenes when TensorFlow runs the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 How does TensorFlow operate under the hood?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a typical TensorFlow program, there are two main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a data-flow graph encompassing the inputs, operations, and the outputs.
    In our exercise, the data-flow graph will represent how x, w1, b1, w2, b2, h,
    and y are related to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the graph by feeding values to the inputs and computing outputs. For
    example, if we need to compute h, we will feed a value (e.g., a NumPy array) to
    x and get the value of h.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorFlow 2 uses an execution style known as *imperative style execution*.
    In imperative style execution, declaration (defining the graph) and execution
    happen simultaneously. This is also known as *eagerly executing* code.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be wondering what a data-flow graph looks like. It is a term TensorFlow
    uses to describe the flow of computations you defined and is represented as a
    *directed acyclic graph* (DAG): a graph structure where arrows represent the data
    and nodes represent the operations. In other words, tf.Variable and tf.Tensor
    objects represent the edges in the graph, whereas operations (e.g., tf.matmul)
    represent the nodes. For example, the data-flow graph for'
  prefs: []
  type: TYPE_NORMAL
- en: '*h = x W*[1] + *b*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: would look like figure 2.4\. Then, at runtime, you could get the value of y
    by feeding values to x, as y is dependent on the input x.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-04](../../OEBPS/Images/02-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 An example computational graph. The various elements here are covered
    in more detail in section 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: How does TensorFlow know to create the data-flow graph? You might have noticed
    the line starting with the symbol @ hanging on top of the forward(...) function.
    This is known as a *decorator* in Python language. The @tf.function decorator
    takes in a function that performs various TensorFlow operations, traces all the
    steps, and turns that into a data-flow graph. How cool is that? This encourages
    the user to write modular code while enabling the computational advantages of
    a data-flow graph. This feature in TensorFlow 2 is known appropriately as AutoGraph
    ([https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function)).
  prefs: []
  type: TYPE_NORMAL
- en: What is a decorator?
  prefs: []
  type: TYPE_NORMAL
- en: 'A decorator modifies the behavior of a function by wrapping it, which happens
    before/after the function is invoked. A good example of a decorator is logging
    the inputs and outputs of a function whenever it is invoked. Here’s how you would
    use decorators for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: as expected. Therefore, when you add the @tf.function decorator, it essentially
    modifies the behavior of the invoked function by building a computational graph
    of the computations happening within the given function.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram in figure 2.5 depicts the execution path of a TensorFlow 2 program.
    The first time the functions a(...) and b(...) are invoked, the data-flow graph
    is created. Then, inputs passed to the functions will be fed to the graph and
    obtain the outputs you are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-05](../../OEBPS/Images/02-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 Typical execution of a TensorFlow 2 program. In the first run, TensorFlow
    traces all functions annotated with @tf.function and builds the data-flow graph.
    In the subsequent runs, corresponding values are fed to the graph (according to
    the function call) and the results are retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: AutoGraph
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoGraph is a great feature in TensorFlow that reduces the developer’s workload
    by working hard behind the scene. To build true appreciation for the feature,
    read more at [https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function).
    Though it is quite amazing, AutoGraph is not a silver bullet. Therefore, it is
    important to understand its advantages as well as its limitations and caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: AutoGraph will provide a performance boost if your code consists of lots of
    repetitive operations (e.g., training a neural network for many iterations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoGraph might slow you down if you run many different operations that only
    run once; because you run the operation only once, building the graph is just
    an overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be careful of what you include inside the function you are exposing to AutoGraph.
    For example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy arrays and Python lists will be converted to tf.constant objects.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: for loops will be unwrapped during function tracing, which might result in large
    graphs that eventually run out of memory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow 1, the predecessor of TensorFlow 2, used an execution style known
    as *declarative graph-based execution*, which consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly define a data-flow graph using various symbolic elements (e.g., placeholder
    inputs, variables, and operations) of what you need to achieve. Unlike in TensorFlow
    2, these do not hold values at declaration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explicitly write code to run the defined graph and obtain or evaluate results.
    You can feed actual values to the previously defined symbolic elements at runtime
    and execute the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is very different from TensorFlow 2, which hides all the intricacies of
    the data-flow graph by automatically building it in the background. In TensorFlow
    1, you have to explicitly build the graph and then execute it, leading to code
    that’s more complex and difficult to read. Table 2.2 summarizes the differences
    between TensorFlow 1 and TensorFlow 2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.2 Differences between TensorFlow 1 and TensorFlow 2
  prefs: []
  type: TYPE_NORMAL
- en: '| **TensorFlow 1** | **TensorFlow 2** |'
  prefs: []
  type: TYPE_TB
- en: '| Does not use eager execution by default | Uses eager execution by default
    |'
  prefs: []
  type: TYPE_TB
- en: '| Uses symbolic placeholders to represent inputs to the graph | Directly feeds
    actual data (e.g., NumPy arrays) to the data-flow graph |'
  prefs: []
  type: TYPE_TB
- en: '| Difficult to debug as results are not evaluated imperatively | Easy to debug
    as operations are evaluated imperatively |'
  prefs: []
  type: TYPE_TB
- en: '| Needs to explicitly and manually create the data-flow graph | Has AutoGraph
    functionality, which traces TensorFlow operations and creates the graph automatically
    |'
  prefs: []
  type: TYPE_TB
- en: '| Does not encourage object-oriented programming, as it forces you to define
    the computational graph in advance | Encourages object-oriented programming |'
  prefs: []
  type: TYPE_TB
- en: '| Results in poor readability of code due to having separate graph definition
    and runtime code | Has better readability of code |'
  prefs: []
  type: TYPE_TB
- en: In the next section, we discuss the basic building blocks of TensorFlow that
    set the foundation for writing TensorFlow programs.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs: []
  type: TYPE_NORMAL
- en: Given the following code,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: where should the tf.function decorator go?
  prefs: []
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any of above
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.2 TensorFlow building blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen the core differences between TensorFlow 1 and TensorFlow 2\. While
    doing this, you were exposed to various data structures (e.g., tf.Variable) and
    operations (e.g., tf.matmul) exposed by the TensorFlow API. Let’s now see where
    and how you might use these data structures and operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow 2, there are three major basic elements we need to learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: tf.Variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tf.Tensor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tf.Operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have already seen all of these being used. For example, from the previous
    MLP example, we have these elements, as shown in table 2.3\. Having knowledge
    of these primitive components is helpful in understanding more abstract components,
    such as a Keras layer and model objects, and will be discussed later.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.3 tf.Variable, tf.Tensor, and tf.Operation entities from the MLP example
  prefs: []
  type: TYPE_NORMAL
- en: '| **Element** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| tf.Variable | w1*,* b1*,* w2 and b2 |'
  prefs: []
  type: TYPE_TB
- en: '| tf.Tensor | h and y |'
  prefs: []
  type: TYPE_TB
- en: '| tf.Operation | tf.matmul |'
  prefs: []
  type: TYPE_TB
- en: It is important to firmly grok these basic elements of TensorFlow for several
    reasons. The main reason is that everything you see in this book, from this point
    on, is built on top of these elements. For example, if you are using a high-level
    API like Keras to build a model, it still uses tf.Variable, tf.Tensor, and tf.Operation
    entities to do the computations. Therefore, it is important to know how to use
    these elements and what you can and cannot achieve with them. The other benefit
    is that the errors returned by TensorFlow are usually presented to you using these
    elements. So, this knowledge will also help us understand errors and resolve them
    quickly as we develop more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Understanding tf.Variable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When building a typical machine learning model, you have two types of data:'
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters that change over time (mutable) as the model is optimized with
    regard to a chosen loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs of the model that are static given data and model parameters (immutable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'tf.Variable is ideal for defining model parameters, as they are initialized
    with some value and can change the value over time. A TensorFlow variable must
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A shape (size of each dimension of the variable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An initial value (e.g., randomly initialized from values sampled from a normal
    distribution)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data type (e.g., int32, float32)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can define a TensorFlow variable as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: initial_value contains the initial value provided to the model. This is typically
    provided using a variable initializer provided in the tf.keras.initializers submodule
    (the full list of initializers can be found at [http://mng.bz/M2Nm](http://mng.bz/M2Nm)).
    For example, if you want to initialize the variable randomly with a 2D matrix
    having four rows and three columns using a uniform distribution, you can pass
    tf.keras.initializers.RandomUniform()([4,3]). You must provide a value to the
    initial_value argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: trainable parameter accepts a Boolean value (i.e., True or False) as the input.
    Setting the trainable parameter to True allows the model parameters to be changed
    by means of gradient descent. Setting the trainable parameter to False will freeze
    the layer so that the values cannot be changed using gradient descent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dtype specifies the data type of the data contained in the variable. If unspecified,
    this defaults to the data type provided to the initial_value argument (typically
    float32).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how we can define TensorFlow variables. First, make sure you have
    imported the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can define a TensorFlow variable with one dimension of size 4 with a constant
    value of 2 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, tf.constant(2.0, shape=[4]) produces a vector of four elements having
    a value 2.0, which then is used as the initial value of tf.Variable. You can also
    define a TensorFlow variable with a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, np.ones(shape=[4,3]) generates a matrix of shape [4,3], and all the elements
    have a value of 1\. The next code snippet defines a TensorFlow variable with three
    dimensions (3×4×5) with random normal initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can see that if we print a tf.Variable it is possible to see its
    attributes such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape of the variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data type of the variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial value of the variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also convert your tf.Variable to a NumPy array with a single line using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can then validate the result yourself by printing the Python variable arr
    using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: which will return
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: A key characteristic of a tf.Variable is that you can change the value of its
    elements as required even after it is initialized. For example, to manipulate
    individual elements or slices of a tf.Variable, you can use the assign() operation
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of this exercise, let us assume the following TensorFlow variable,
    which is a matrix initialized with zeros that has four rows and three columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can change the element in the first (i.e., index 0) row and third (i.e.,
    index 2) column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: NOTE Remember that Python uses zero-based indexing. This means that indexing
    starts from zero (not one). For example, if you want to get the second element
    of a vector vec, you would use vec[1].
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also change values using slicing as follows. Here, we are changing
    the values that lie in the last two rows and first two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This results in
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 2
  prefs: []
  type: TYPE_NORMAL
- en: Can you write the code to create a tf.Variable that has the following values
    and has type int16? You can use np.array() for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 2.2.2 Understanding tf.Tensor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen, tf.Tensor is the output of performing a TensorFlow operation
    on some data (e.g., on a tf.Variable or a tf.Tensor). tf.Tensor objects are heavily
    used when defining machine learning models, as they are used to store inputs,
    interim outputs of layers, and final outputs of the model. So far, we have looked
    mostly at vectors (one dimension) and matrices (two dimension). However, there’s
    nothing stopping us from creating n-dimensional data structures. Such an n-dimensional
    data structure is known as a *tensor*. Table 2.4 shows a few examples of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.4 Examples of tensors
  prefs: []
  type: TYPE_NORMAL
- en: '| **Description** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| A 2D tensor with two rows and four columns |'
  prefs: []
  type: TYPE_TB
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| A 4D tensor of size 2 × 3 × 2 × 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Tensors also have axes. Each dimension of the tensor is considered an axis.
    Figure 2.6 depicts the axes of a 3D tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-06](../../OEBPS/Images/02-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 A 2 × 4 × 3 tensor with the three axes. The first axis (axis 0) is
    the row dimension, the second axis (axis 1) is the column axis, and the final
    axis (axis 2) is the depth axis.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, a tensor can also have just a single dimension (i.e., vector) or
    be a scalar. An important distinction to make is how the terms *tensor* and tf.Tensor
    are used. We will use *tensor/vector/scalar* to refer to a tensor when we are
    discussing mathematical aspects of our models. We will refer to any data-related
    output produced by our TensorFlow code as a tf.Tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we will discuss a few instances where you will end up with a tf.Tensor.
    For example, you can produce a tf.Tensor by multiplying a tf.Variable with a constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If you analyze the type of the object produced after the previous operation
    using print(type(b).__name__), you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'EagerTensor is a class inherited from tf.Tensor. It is a special type of tf.Tensor,
    the value of which is evaluated eagerly (i.e., immediately after defined). You
    can verify that EagerTensor is, in fact, a tf.Tensor by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You can also produce a tf.Tensor by adding a tf.Tensor to another tf.Tensor
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: where print(c) will yield
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, tf.constant() is used to produce tf.Tensor objects a and b. By adding
    a and b, you will get a tensor c of type tf.Tensor. As before, you can validate
    this claim by running
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The key difference between a tf.Variable and a tf.Tensor is that tf.Variable
    allows its values to change even after the variable is initialized (known as a
    mutable structure). However, once you initialize a tf.Tensor, you cannot change
    it during the lifetime of the execution (known as an *immutable data structure*).
    tf.Variable is a mutable data structure, whereas tf.Tensor is an immutable data
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what happens if you try to change the value of a tf.Tensor after
    it’s initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Clearly, TensorFlow isn’t amused by our rebellious act of trying to modify tf.Tensor
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Zoo
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow has an arsenal of different Tensor types for attacking various problems.
    Here are a few different Tensor types available in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: RaggedTensor—A type of data used for variable sequence-length data sets that
    cannot be represented as a matrix efficiently
  prefs: []
  type: TYPE_NORMAL
- en: TensorArray—A dynamic-sized data structure that can start small and stretch
    as more data is added (similar to a Python list)
  prefs: []
  type: TYPE_NORMAL
- en: SparseTensor—A type of data used to represent sparse data (e.g., a user-by-movie
    rating matrix)
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will discuss some of the popular TensorFlow operations.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  prefs: []
  type: TYPE_NORMAL
- en: Can you write the code to create a tf.Tensor that is initialized with values
    sampled from a normal distribution and that has the shape 4 × 1 × 5? You can use
    np.random .normal() for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Understanding tf.Operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The backbone of TensorFlow that allows you to do useful things with the data
    are the operations available. For example, one of the core operations in a deep
    network is matrix multiplication, which makes TensorFlow a great tool for implementing
    core operations. Like matrix multiplication, TensorFlow offers a wide range of
    low-level operations that can be used in TensorFlow. A full list of operations
    available via the TensorFlow API can be found at [http://mng.bz/aDWY](http://mng.bz/aDWY).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss some popular arithmetic operations you have at your disposal.
    First, you have basic arithmetic operations such as addition, subtraction, multiplication,
    and division. You can perform these just like you would with normal Python variables.
    To demonstrate this, let’s assume the following vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We can look at what a and b look like by executing the following
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: which gives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Performing addition on a and b
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: gives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Performing multiplication on a and b
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: gives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You can also do logical comparisons between tensors. Assuming
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: and checking for element-wise equality
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: gives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Checking less than or equal elements
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: gives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you have reduction operators that allow you to reduce a tensor (e.g.,
    minimum/ maximum/sum/product) on a specific axis or all axes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, a is a tf.Tensor that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s first get the sum of all elements of this tensor. In other words, reduce
    the tensor on all axes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s get the product on axis 0 (i.e., element-wise product of each row
    of a):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now get the minimum over multiple axes (i.e., 0 and 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: You can see that whenever you perform a reduction operation on a certain dimension,
    you are losing that dimension. For example, if you have a tensor of size [6,4,2]
    and reduce that tensor on axis 1 (i.e., second axis), you will have a tensor of
    size [6,2]. In certain instances, you need to keep this dimension there while
    reducing the tensor (resulting in a [6,1,2]-shaped tensor). One such instance
    is to make your tensor broadcast compatible with another tensor ([http://mng.bz/g4Zn](http://mng.bz/g4Zn)).
    *Broadcasting* is a term used to describe how scientific computation tools (e.g.,
    NumPy/TensorFlow) treat tensors during arithmetic operations. In such situations,
    you can set the keepdims parameter to True (which defaults to False). You can
    see the difference in the shape of the final output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: which produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Several other important functions are outlined in table 2.5.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.5 Mathematical functions offered in TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: '| tf.argmax | Description | Computes the index of a maximum value on a given
    axis. For example, the following example shows how to compute tf.argmax on axis
    0. |'
  prefs: []
  type: TYPE_TB
- en: '| Usage | d = tf.constant([[1,2,3],[3,4,5],[6,5,4]])d_max1 = tf.argmax(d, axis=0)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Result | tf.Tensor ([2,2,0]) |'
  prefs: []
  type: TYPE_TB
- en: '| tf.argmin | Description | Computes the index of a minimum value on a given
    axis. For example, the following example shows how to compute tf.argmin on axis
    1. |'
  prefs: []
  type: TYPE_TB
- en: '| Usage | d = tf.constant([[1,2,3],[3,4,5],[6,5,4]])d_min1 = tf.argmin(d, axis=1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Result | tf.Tensor([[0],[0],[0]]) |'
  prefs: []
  type: TYPE_TB
- en: '| tf.cumsum | Description | Computes the cumulative sum of a vector or a tensor
    on a given axis |'
  prefs: []
  type: TYPE_TB
- en: '| Usage | e = tf.constant([1,2,3,4,5])e_cumsum = tf.cumsum(e) |'
  prefs: []
  type: TYPE_TB
- en: '| Result | tf.Tensor([1,3,6,10,15]) |'
  prefs: []
  type: TYPE_TB
- en: We conclude our discussion about basic primitives of TensorFlow here. Next we
    will discuss some of the computations that are commonly used in neural network
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 4
  prefs: []
  type: TYPE_NORMAL
- en: There is another function for computing mean called tf.reduce_mean(). Given
    the tf.Tensor object a, which contains the following values, can you compute the
    mean for each column?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 2.3 Neural network-related computations in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we will talk about some key low-level operations that underpin deep neural
    networks. Let’s say you are taking a computer vision class at school. For your
    assignment, you have to manipulate an image using various mathematical operations
    to achieve various effects. We will be using the famous image of a baboon (figure
    2.7), which is a popular choice for computer vision problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-07](../../OEBPS/Images/02-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 Image of a baboon
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Matrix multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your first task is to convert the image from RGB to grayscale. For this, you
    must employ matrix multiplication. Let’s first understand what matrix multiplication
    is.
  prefs: []
  type: TYPE_NORMAL
- en: Story of Lena
  prefs: []
  type: TYPE_NORMAL
- en: Though we are using an image of a baboon for the exercises, there’s a long-standing
    tradition of using Lena’s (a Swedish model) photo to demonstrate various computer
    vision algorithms. There is a very interesting backstory behind how this became
    a norm for computer vision problems, which you can read at [http://mng.bz/enrZ](http://mng.bz/enrZ).
  prefs: []
  type: TYPE_NORMAL
- en: You perform matrix multiplication between two tensors using the tf.matmul()
    function. For two matrices, tf.matmul() performs matrix multiplication (e.g.,
    if you have a of size [4,3] and b of size [3,2], matrix multiplication results
    in a [4,2] tensor. Figure 2.8 illustrates the matrix multiplication operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-08](../../OEBPS/Images/02-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 Matrix multiplication between a 4 × 3 matrix and 3 × 2 matrix, resulting
    in a 4 × 2 matrix
  prefs: []
  type: TYPE_NORMAL
- en: More generally, if you have an n x m matrix (a) and a m x p matrix (b), the
    result of matrix multiplication c is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![02_08a](../../OEBPS/Images/02_08a.png)'
  prefs: []
  type: TYPE_IMG
- en: However, if you have high-dimensional tensors a and b, the sum product over
    the last axis of a and second-to-last axis of b will be performed. Both a and
    b tensors need to have identical dimensionality except for the last two axes.
    For example, if you have a tensor a of size [3,5,7] and b of size [3,7,8], the
    result would be a [3,5,8]-sized tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to our problem, given three RGB pixels, you can convert it to a
    grayscale pixel using
  prefs: []
  type: TYPE_NORMAL
- en: 0.3 * R + 0.59 * G + 0.11 * B
  prefs: []
  type: TYPE_NORMAL
- en: This is a common operation for converting any RGB image to grayscale ([http://mng.bz/p2M0](http://mng.bz/p2M0)),
    which can be important depending on the problem at hand. For example, to recognize
    digits from images, color is not so important. By converting images to grayscale,
    you are essentially helping the model by reducing the size of the input (one channel
    instead of three) and by removing noisy features (i.e., color information).
  prefs: []
  type: TYPE_NORMAL
- en: Given a 512 × 512 × 3 image, if you multiply that with a 3 × 1 array representing
    the weights provided, you will get the grayscale image of size 512 × 512 × 1\.
    Then we need to remove the last dimension of the grayscale image (as it is one),
    and we end up with a matrix of size 512 × 512\. For this you can use the tf.squeeze()
    function, which removes any dimensions that are of size one (see the next listing).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Converting an RGB image to grayscale using matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: ❶ PIL is a Python library for basic image manipulation
  prefs: []
  type: TYPE_NORMAL
- en: ❷ The RGB image of size 512 × 512 × 3 loaded as a NumPy array
  prefs: []
  type: TYPE_NORMAL
- en: ❸ The NumPy array is converted to a tf.Tensor.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The RGB weights as a 3 × 1 array
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Performing matrix multiplication to get the black-and-white image
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Getting rid of the last dimension, which is 1
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication is an important operation in fully connected networks
    as well. To go from an input layer to a hidden layer, we employ matrix multiplication
    and add operation. For the moment, we will ignore the nonlinear activation, as
    it is just an element-wise transformation. Figure 2.9 visualizes the hidden layer
    computation of the MLP you built earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-09](../../OEBPS/Images/02-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 An illustration of the computations taking place in a hidden layer.
    x is the input (1 × 4), W is the weight matrix (4 × 3), b is the bias (1 × 3),
    and finally, h is the output (1 × 3).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Convolution operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next task is to implement an edge-detection algorithm. Knowing that you
    can detect edges using the convolution operation, you also want to show your skills
    off by achieving this with TensorFlow. The good news is, you can!
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation is essential in convolutional neural networks, which
    are deep networks heavily utilized for image-related machine learning tasks (e.g.,
    image classification, object detection). A convolution operation shifts a *window*
    (also known as a *filter* or a *kernel*) over the data while producing a single
    value at every position. The convolution window will have some value at each location.
    And at a given position, the values in the convolution window are element-wise
    multiplied and summed over with what’s overlapping with that window in the data
    to produce the final value for that location. The convolution operation is shown
    in figure 2.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-10](../../OEBPS/Images/02-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 Computational steps of the convolution operation
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the values you choose for the convolution window, you can produce
    some unique effects. You can try out some popular kernels at [https://setosa.io/ev/image-kernels/](https://setosa.io/ev/image-kernels/).
    Edge detection is also a popular computer vision technique that can be achieved
    using the convolution operation. TensorFlow provides the tf.nn.convolution() function
    to perform convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we have our black-and-white image of the baboon stored as a tf.Tensor
    in the variable x. x is a matrix of size 512 × 512\. Let’s create a new variable
    y from this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s define our edge detection filter. We will use an edge detection
    filter known as an *approximate Laplacian filter*, which is a 3 × 3 matrix filled
    with value -1 except for the middle-most value, which is 8\. Note how the sum
    of the kernel is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to reshape y and filter, because the tf.nn.convolution() function
    accepts a very specifically shaped input and a filter. The first constraint is
    that your y and filter should have the same rank. Rank here refers to the number
    of dimensionalities in the data. Here we have rank 2 tensors and will perform
    2D convolution. To perform 2D convolution, both the input and the kernel need
    to be of rank 4\. Therefore, we need to reshape the input and the kernel in a
    few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Add two more dimensions at the beginning and end of the input. The dimension
    at the beginning represents the batch dimension, and the last dimension represents
    the channel dimension (e.g., RGB channels of an image). Though the values will
    be 1 in our example, we still need those dimensions to be present (e.g., an image
    of size [512,512] will be reshaped to [1,512,512,1]).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add two more dimensions of size 1 at the end of the filter. These new dimensions
    represent the incoming and outgoing channels. We have a single channel (i.e.,
    grayscale) coming in, and we want to produce a single channel (i.e., grayscale)
    as well (e.g., a kernel of size [3,3] will be reshaped to [3,3,1,1]).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NOTE Rank of a tensor refers to the number of dimensions of that tensor. This
    is different from the rank of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t worry if you don’t fully understand why we added these additional dimensions.
    This will make more sense when we discuss the convolution operation in the context
    of convolutional neural networks in a later chapter. For now, you only need to
    understand the high-level behavior of the convolution operation. In TensorFlow,
    you can reshape y and filter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, y is a 512 × 512 tensor. The expression tf.reshape(y, [1,512,512,1])
    converts y (i.e., a 2D tenor) to a 4D tensor of size 1 × 512 × 512 × 1\. Similarly,
    the filter (i.e., a 2D tensor of size 3 × 3) is reshaped to a 4D tensor of size
    3 × 3 × 1 × 1\. Note that the total number of elements is unchanged during the
    reshaping. Now you can compute the convolution output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: You can visualize the result of edge detection and compare that to the original
    image, as shown in figure 2.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-11](../../OEBPS/Images/02-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 Original black-and-white image versus result of edge detection
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss another operation known as the pooling
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Pooling operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are off to the next task, which is to resize the image resultant after edge
    detection by halving the width and height of the image. For example, if we have
    a 512 × 512 image and need to rescale it to 256 × 256, the *pooling operation*
    is the best way to achieve this easily. The pooling (or sub-sampling) operation
    is commonly used in convolutional neural networks for this reason: to reduce the
    size of the output so fewer parameters can be used to learn from data.'
  prefs: []
  type: TYPE_NORMAL
- en: Why is it called the pooling operation?
  prefs: []
  type: TYPE_NORMAL
- en: The reason why the sub-sampling operation is also called “pooling” probably
    has its roots in the word’s meaning, as well as in statistics. The word *pooling*
    is used to describe combining things into a single entity, which is exactly what
    is done in this operation (e.g., by means of averaging or taking maximum). In
    statistics, you will find the term *pooled variance*, which is a weighted average
    of the variance between two populations ([http://mng.bz/OGdO](http://mng.bz/OGdO)),
    essentially combining two variances into a single variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, you can call the tf.nn.max_pool() function to perform max pooling
    and tf.nn.avg_pool() for average pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The pooling operation is another commonly found operation in convolutional neural
    networks and works similarly to the convolution operation. But unlike the convolution
    operation, the pooling operation does not have values in the kernel. At a given
    location, it takes either the average or maximum value of what’s overlapping the
    kernel in the data. The operation that produces the average at a given location
    is known as average pooling, whereas the operation that produces the maximum is
    known as max pooling. Figure 2.12 illustrates the max pooling operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-12](../../OEBPS/Images/02-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 Max-pooling operation. The pooling window goes from one position
    to another on the image, while producing a single value (i.e., maximum value in
    the image overlapping the pooling window) at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have y_conv, which is a 4D tensor having the shape [1,510,510,1]. You might
    notice that the dimensions are slightly smaller than the original image size (i.e.,
    512). This is because, when doing convolution with a window of size c x c (without
    extra padding) on an image having h height and w width, the resulting image has
    the dimensions h-c+1 and w-c+1. We can perform pooling as shown. You can perform
    either average pooling or max pooling with the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in two images, z_avg and z_max; both have the shape [1,255,255,1].
    In order to keep just the height and width dimensions and remove redundant dimensions
    of size 1, we use the tf.squeeze() function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: You can plot z_avg and z_max using matplotlib (a plotting library in Python)
    and get the result shown in figure 2.13\. The code is provided in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![02-13](../../OEBPS/Images/02-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 Result after edge detection versus result after average or max pooling
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 shows the different effects we get with different types of pooling.
    If you look closely, you will see that average pooling results in more consistent
    and continuous lines, whereas max pooling results in a noisier image.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, unlike in the convolution operation, we are not providing a filter
    (or a kernel), as the pooling operation doesn’t have a kernel. But we need to
    pass in the dimensions of the window. These dimensions represent the corresponding
    dimensions of the input (i.e., it is a [batch dimension, height, width, channels]
    window). In addition to that, we are also passing two arguments, stride and padding.
    We will discuss these in detail in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5
  prefs: []
  type: TYPE_NORMAL
- en: You are given a grayscale image img of size 256 × 256 and a convolution filter
    f of size 5 × 5\. Can you write the tf.reshape() function calls and the tf.nn.convolution()
    operation? What would be the size of the output?
  prefs: []
  type: TYPE_NORMAL
- en: Great work! Now you know most common operations used in deep learning networks.
    We will end our discussion about TensorFlow basics here. In the next chapter,
    we will discuss a high-level API available in TensorFlow called Keras, which is
    particularly useful for model building.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow is an end-to-end machine learning framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow provides an ecosystem facilitating model prototyping, model building,
    model monitoring, and model serving.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow 1 uses declarative graph execution style (define then run), whereas
    TensorFlow 2 uses imperative graph execution style (define by run).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow provides three main building blocks: tf.Variable (for values that
    change over time), tf.Tensor (values that are fixed over time), and tf.Operation
    (transformations performed on tf.Variable and tf.Tensor objects).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow provides several operations that are used to build neural networks
    such as tf.matmul, tf.nn.convolution, and tf.nn.max_pool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use tf.matmul to convert an RGB image to grayscale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use tf.nn.convolution to detect edges in an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use tf.nn.max_pool to resize an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Exercise 1:** 2'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 2:** tf.Variable(np.array([[1,2,3],[4,3,2]], dtype=”int16”)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 3:** tf.constant(np.random.normal(size=[4,1,5]))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 4:** tf.reduce_mean(a, axis=1)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise 5:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The shape of the final output would be [1,252,252,1]. The resulting size of
    the convolution operation is image size - convolution window size + 1.
  prefs: []
  type: TYPE_NORMAL
