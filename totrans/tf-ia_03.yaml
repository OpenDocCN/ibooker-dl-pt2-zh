- en: 2 TensorFlow 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 TensorFlow 2
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了
- en: What TensorFlow 2 is
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2是什么
- en: Important data structures and operations in TensorFlow
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow中的重要数据结构和操作
- en: Common neural network related operations in TensorFlow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow中常见的与神经网络相关的操作
- en: In the previous chapter, we learned that TensorFlow is an end-to-end machine
    learning framework predominantly used for implementing deep neural networks. TensorFlow
    is skillful at converting these deep neural networks to computational graphs that
    run faster on optimized hardware (e.g., GPUs and TPUs). But keep in mind that
    this is not the only use for TensorFlow. Table 2.1 delineates other areas TensorFlow
    supports.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到TensorFlow是一种端到端的机器学习框架，主要用于实现深度神经网络。TensorFlow擅长将这些深度神经网络转换为在优化硬件上（例如GPU和TPU）运行更快的计算图。但请记住，这不是TensorFlow的唯一用途。表2.1概述了TensorFlow支持的其他领域。
- en: Table 2.1 Various features offered in TensorFlow
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 TensorFlow提供的各种功能
- en: '| Probabilistic machine learning | TensorFlow supports implementing probabilistic
    machine learning models. For example, models like Bayesian neural networks can
    be implemented with a TensorFlow API ([https://www.tensorflow.org/probability](https://www.tensorflow.org/probability)).
    |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 概率机器学习 | TensorFlow支持实现概率机器学习模型。例如，可以使用TensorFlow API实现贝叶斯神经网络等模型（[https://www.tensorflow.org/probability](https://www.tensorflow.org/probability)）。'
- en: '| Computer graphics-related computations | Computer graphic computations can
    be mostly achieved on GPUs (e.g., simulating various lighting effects, raytracing;
    [https://www.tensorflow.org/graphics](https://www.tensorflow.org/graphics)). |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 与计算机图形有关的计算 | 计算机图形计算大部分可以使用GPU实现（例如模拟各种光照效果、光线追踪；[https://www.tensorflow.org/graphics](https://www.tensorflow.org/graphics)）。'
- en: '| TensorFlow Hub: Reusable (pretrained) models | In deep learning we usually
    try to leverage models that have already been trained on large amounts of data
    for the downstream tasks we’re interested in solving. TensorFlow Hub is a repository
    in which such models implemented in TensorFlow are stored ([https://www.tensorflow.org/hub](https://www.tensorflow.org/hub)).
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow Hub：可重用（预训练的）模型 | 在深度学习中，我们通常试图利用已经在大量数据上训练过的模型来解决我们感兴趣的下游任务。TensorFlow
    Hub是一个存放这种用TensorFlow实现的模型的仓库（[https://www.tensorflow.org/hub](https://www.tensorflow.org/hub)）。'
- en: '| Visualize/debug TensorFlow models | TensorFlow provides a dashboard for visualizing
    and monitoring model performance and even visualizing data ([https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)).
    |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 可视化/调试TensorFlow模型 | TensorFlow提供了一个仪表板，用于可视化和监控模型性能，甚至可视化数据（[https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)）。'
- en: In the coming chapters, we will go on an exciting journey exploring the bells
    and whistles in TensorFlow and learning how to excel at things TensorFlow is good
    at. In other words, we will look at how to solve real-world problems with TensorFlow,
    such as image classification (i.e., recognizing objects in images), sentiment
    analysis (i.e., recognizing positive/negative tones in reviews/opinions), and
    so on. While solving these tasks, you will learn how to overcome real-world challenges
    such as overfitting and class imbalance that can easily throw a spanner in the
    works. This chapter specifically focuses on providing a strong foundational knowledge
    of TensorFlow before we head toward complex problems that can be solved with deep
    networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将展开一次充满惊喜的旅程，探索TensorFlow中的花里胡哨的东西，并学习如何在TensorFlow擅长的领域中表现出色。换句话说，我们将学习如何使用TensorFlow解决现实世界的问题，例如图像分类（即在图像中识别对象）、情感分析（即识别评论/意见中的正面/负面情绪）等等。在解决这些任务的同时，您将学习如何克服过拟合和类别不平衡等现实世界中可能会出现的挑战，这些问题很容易妨碍我们的进展。本章将特别关注在我们进入可以使用深度网络解决的复杂问题之前，为TensorFlow提供扎实的基础知识。
- en: First, we will implement a neural network in both TensorFlow 2 and TensorFlow
    1 and see how much TensorFlow has evolved in terms of user friendliness. Then
    we will learn about basic units (e.g., variables, tensors, and operations) provided
    in TensorFlow, which we must have a good understanding of in order to develop
    solutions. Finally, we will understand the details of several complex mathematical
    operations through a series of fun computer vision exercises.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在TensorFlow 2和TensorFlow 1中实现一个神经网络，看看TensorFlow在用户友好性方面发展了多少。然后，我们将了解TensorFlow提供的基本单元（例如变量、张量和操作），我们必须对此有很好的理解才能开发解决方案。最后，我们将通过一系列有趣的计算机视觉练习来理解几个复杂的数学操作的细节。
- en: 2.1 First steps with TensorFlow 2
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 TensorFlow 2初步
- en: 'Let’s imagine you are taking a machine learning course and have been given
    an assignment to implement a *multilayer perceptron* (MLP) (i.e., a type of neural
    network) and compute the final output for a given datapoint using TensorFlow.
    You are new to TensorFlow, so you go to the library and start studying what TensorFlow
    is. While you research, you realize that TensorFlow has two major versions (1
    and 2) and decide to use the latest and greatest: TensorFlow 2\. You’ve already
    installed the required libraries, as outlined in appendix A.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在参加一门机器学习课程，并被要求使用TensorFlow实现一个*多层感知机*（MLP）（即一种神经网络类型），并针对给定的数据点计算最终输出。你对TensorFlow还不熟悉，所以你去图书馆开始研究TensorFlow是什么。在研究过程中，你意识到TensorFlow有两个主要版本（1和2），决定使用最新最好的：TensorFlow
    2\. 你已经按附录A中的要求安装了所需的库。
- en: Before moving on, let’s learn about MLPs. An MLP (figure 2.1) is a simple neural
    network that has an input layer, one or more hidden layers, and an output layer.
    These networks are also called *fully connected networks*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们了解一下MLP。MLP（图2.1）是一个简单的神经网络，它有一个输入层，一个或多个隐藏层和一个输出层。这些网络也被称为*全连接网络*。
- en: NOTE Some research only uses the term MLP to refer to a network made of multiple
    perceptrons ([http://mng.bz/y4lE](http://mng.bz/y4lE)) organized in a hierarchical
    structure. However, in this book, we will use the terms MLP and fully connected
    network interchangeably.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注 Some research only uses the term MLP to refer to a network made of multiple
    perceptrons ([http://mng.bz/y4lE](http://mng.bz/y4lE)) organized in a hierarchical
    structure. However, in this book, we will use the terms MLP and fully connected
    network interchangeably.
- en: In each layer, we have weights and biases, which are used to compute the output
    of that layer. In our example, we have an input of size 4, a hidden layer with
    three nodes, and an output layer of size 2.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层中，我们有权重和偏置，用于计算该层的输出。在我们的例子中，我们有一个大小为4的输入，一个具有三个节点的隐藏层和一个大小为2的输出层。
- en: '![02-01](../../OEBPS/Images/02-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![02-01](../../OEBPS/Images/02-01.png)'
- en: 'Figure 2.1 Depiction of a multilayer perceptron (MLP) or a fully connected
    network. There are three layers: an input layer, a hidden layer (that has weights
    and biases), and an output layer. The output layer produces normalized probabilities
    as the output using softmax activation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 多层感知机（MLP）或全连接网络的示意图。有三层：一个输入层，一个隐藏层（带有权重和偏置），一个输出层。输出层使用softmax激活产生归一化的概率作为输出。
- en: The input values (*x*) are transformed to hidden values (*h*) using the following
    computation
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输入值(*x*)经过以下计算转换为隐藏值(*h*)：
- en: '*h* = *σ*(*x W*[1] + *b*[1])'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*h* = *σ*(*x W*[1] + *b*[1])'
- en: where *σ* is the sigmoid function. The sigmoid function is a simple nonlinear
    element-wise transformation, as shown as in figure 2.2.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*σ*是sigmoid函数。Sigmoid函数是一个简单的非线性逐元素变换，如图2.2所示。
- en: '![02-02](../../OEBPS/Images/02-02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![02-02](../../OEBPS/Images/02-02.png)'
- en: Figure 2.2 A visualization of the sigmoidal activation function for different
    inputs
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 sigmoid激活函数对不同输入的可视化
- en: '*x* is a matrix of size 1 × 4 (i.e., one row and four columns), *W*[1] is a
    matrix of size 4 × 3 (i.e., four rows and three columns), and *b*[1] is 1 × 4
    (i.e., one row and four columns). This gives an h of size 1 × 3\. Finally, the
    output is computed as'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*是一个大小为1 × 4的矩阵（即一行四列），*W*[1]是一个大小为4 × 3的矩阵（即四行三列），*b*[1]是1 × 4的矩阵（即一行四列）。这给出了一个大小为1
    × 3的*h*。最后，输出计算为'
- en: '*y* = *softmax*(*h W*[2] + *b*[2])'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *softmax*(*h W*[2] + *b*[2])'
- en: Here, *W*[2] is a 3 × 2 matrix, and *b*[2] is a 1 × 2 matrix. Softmax activation
    normalizes the linear scores of the last layer (i.e., *h W*[2] + *b*[2]) to actual
    probabilities (i.e., values sum up to 1 along columns). Assuming an input vector
    x of length *K*, the softmax activation produces a *K*-long vector *y*. The *i*^(th)
    element of *y* is computed as
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*[2]是一个3 × 2的矩阵，*b*[2]是一个1 × 2的矩阵。Softmax激活将最后一层的线性分数（即*h W*[2] + *b*[2]）归一化为实际概率（即沿列求和的值等于1）。假设输入向量*x*的长度为*K*，softmax激活产生一个*K*长的向量*y*。*y*的第*i*个元素计算如下：
- en: '![02_02a](../../OEBPS/Images/02_02a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![02_02a](../../OEBPS/Images/02_02a.png)'
- en: where *y[i]* is the *i*^(th) output element and *x*[i] is the *i*^(th) input
    element. As a concrete example, assume the final layer without the softmax activation
    produced,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y[i]*是第*i*个输出元素，*x*[i]是第*i*个输入元素。作为一个具体的例子，假设最终层没有softmax激活产生，
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Applying the softmax normalization converts these values to
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 应用softmax归一化将这些值转换为
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s see how this can be implemented in TensorFlow 2\. You can find the code
    in the Jupyter notebook (Ch02-Fundamentals-of-TensorFlow-2/2.1.Tensorflow_Fundamentals.ipynb).
    How to install the necessary libraries and set up the development environment
    is delineated in appendix A. Initially, we need to import the required libraries
    using import statements:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在TensorFlow 2中实现这一点。您可以在Jupyter笔记本（Ch02-Fundamentals-of-TensorFlow-2/2.1.Tensorflow_Fundamentals.ipynb）中找到代码。如何安装必要的库和设置开发环境在附录A中描述。最初，我们需要使用导入语句导入所需的库：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we define the input to the network (*x*) and the variables (or parameters)
    (i.e., *w*[1], *b*[1], *w*[2], and *b*[2]) of the network:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义网络的输入（*x*）和变量（或参数）（即*w*[1]、*b*[1]、*w*[2]和*b*[2]）：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, *x* is a simple NumPy array of size 1 × 4 (i.e., one row and four columns)
    that is filled with values from a normal distribution. Then we define the parameters
    of the network (i.e., weights and biases) as TensorFlow variables. A tf.Variable
    behaves similar to a typical Python variable. It has some value attached at the
    time of the definition and can change over time. tf.Variable is used to represent
    weights and biases of a neural network, which are changed during the optimization
    or the training procedure. When defining TensorFlow variables, we need to provide
    an initializer and a shape for the variables. Here we are using an initializer
    that randomly sample values from a normal distribution. Remember that *W*[1] is
    4 × 3 sized, *b*[1] is 1 × 3 sized, *W*[2] is 3 × 2 sized, and *b*[2] is 1 × 2
    sized, and that the shape argument for each of these is set accordingly. Next,
    we define the core computations of the MLP as a nice modular function. This way,
    we can easily reuse the function to compute hidden layer outputs of multiple layers:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*是一个大小为1×4（即一行四列）的简单NumPy数组，其值来自正常分布。然后，我们将网络的参数（即权重和偏差）定义为TensorFlow变量。tf.Variable的行为类似于典型的Python变量。在定义时会附加一些值，并且随着时间的推移可能会发生变化。tf.Variable用于表示神经网络的权重和偏差，在优化或训练过程中会更改这些参数。定义TensorFlow变量时，需要为变量提供一个初始化器和一个形状。在这里，我们使用从正态分布随机抽样值的初始化器。请记住，*W*[1]大小为4×3，*b*[1]大小为1×3，*W*[2]大小为3×2，*b*[2]大小为1×2，每个参数的形状参数都相应地进行了设置。接下来，我们将多层感知器网络的核心计算定义为一个漂亮的模块化函数。这样，我们可以轻松地重用该函数来计算多层的隐藏层输出：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, act is any nonlinear activation function of your choice (e.g., tf.nn.sigmoid).
    (You can look at various activation functions here: [https://www.tensorflow.org/api_docs/python/tf/nn](https://www.tensorflow.org/api_docs/python/tf/nn).
    Be mindful that not all of them are activation functions. The expression tf.matmul(x,W)+b
    elegantly wraps the core computations we saw earlier (i.e., *x W*[1] + *b*[1]
    and *h W*[2] + *b*[2]) to a reusable expression. Here, tf.matmul performs the
    matrix multiplication operation. This computation is illustrated in figure 2.3.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，act是您选择的任何非线性激活函数（例如tf.nn.sigmoid）。（您可以在此处查看各种激活函数：[https://www.tensorflow.org/api_docs/python/tf/nn](https://www.tensorflow.org/api_docs/python/tf/nn)。要注意的是，并非所有函数都是激活函数。表达式tf.matmul(x,W)+b优雅地封装了我们之前看到的核心计算（即*x
    W*[1]+ *b*[1]和 *h W*[2]+*b*[2]）到可重用的表达式中。在这里，tf.matmul执行矩阵乘法运算。该计算在图2.3中说明。
- en: '![02-03](../../OEBPS/Images/02-03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![02-03](../../OEBPS/Images/02-03.png)'
- en: Figure 2.3 The matrix multiplication and bias addition illustrated for example
    input, weights, and bias
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 矩阵乘法和偏置加法的示例输入、权重和偏差说明
- en: Having @tf.function on top of the function is a way for TensorFlow to know that
    this function contains TensorFlow code. We will discuss the purpose of @tf.function
    in more detail in the next section. This brings us to the final part of the code.
    As we have the inputs, all the parameters, and core computations defined, we can
    compute the final output of the network
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将@tf.function放在函数的顶部是TensorFlow知道该函数包含TensorFlow代码的一种方式。我们将在下一部分更详细地讨论@tf.function的目的。这将带我们进入代码的最后部分。由于我们已经定义了输入、所有参数和核心计算，因此可以计算网络的最终输出。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: which will output
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将会是：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, h and y are the resulting tensors (of type tf.Tensor) of various TensorFlow
    operations (e.g., tf.matmul). The exact values in the output might differ slightly
    (see the following listing).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，h 和y是各种TensorFlow操作（例如tf.matmul）的结果张量（类型为tf.Tensor）。输出中的确切值可能会略有不同（请见下面的列表）。
- en: Listing 2.1 Multilayer perceptron network with TensorFlow 2
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 使用TensorFlow 2的多层感知器网络
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Importing NumPy and TensorFlow libraries
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❶导入NumPy和TensorFlow库
- en: ❷ The input to the MLP (a NumPy array)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ MLP的输入（一个NumPy数组）
- en: ❸ The initializer used to initialize variables
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于初始化变量的初始化器
- en: ❹ The parameters of layer 1 (w1 and b2) and layer 2 (w2 and b2)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第一层（w1 和 b2）和第二层（w2 和 b2）的参数
- en: ❺ This line tells TensorFlow’s AutoGraph to build the graph.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 这行告诉 TensorFlow 的 AutoGraph 构建图形。
- en: ❻ MLP layer computation, which takes in an input, weights, bias, and a nonlinear
    activation
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ MLP 层计算，它接受输入、权重、偏置和非线性激活
- en: ❼ Computing the first hidden layer output, h
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算第一个隐藏层的输出 h
- en: ❽ Computing the final output, y
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 计算最终输出 y
- en: Next, we will look at what happens behind the scenes when TensorFlow runs the
    code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看 TensorFlow 运行代码时背后发生了什么。
- en: 2.1.1 How does TensorFlow operate under the hood?
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 TensorFlow 在底层是如何运行的？
- en: 'In a typical TensorFlow program, there are two main steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的 TensorFlow 程序中，有两个主要步骤：
- en: Define a data-flow graph encompassing the inputs, operations, and the outputs.
    In our exercise, the data-flow graph will represent how x, w1, b1, w2, b2, h,
    and y are related to each other.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个涵盖输入、操作和输出的数据流图。在我们的练习中，数据流图将表示 x、w1、b1、w2、b2、h 和 y 之间的关系。
- en: Execute the graph by feeding values to the inputs and computing outputs. For
    example, if we need to compute h, we will feed a value (e.g., a NumPy array) to
    x and get the value of h.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为输入提供值并计算输出来执行图形。例如，如果我们需要计算 h，则将一个值（例如 NumPy 数组）馈送到 x 并获取 h 的值。
- en: TensorFlow 2 uses an execution style known as *imperative style execution*.
    In imperative style execution, declaration (defining the graph) and execution
    happen simultaneously. This is also known as *eagerly executing* code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2 使用一种称为*命令式执行*的执行样式。在命令式执行中，声明（定义图形）和执行同时发生。这也被称为*急切执行*代码。
- en: 'You might be wondering what a data-flow graph looks like. It is a term TensorFlow
    uses to describe the flow of computations you defined and is represented as a
    *directed acyclic graph* (DAG): a graph structure where arrows represent the data
    and nodes represent the operations. In other words, tf.Variable and tf.Tensor
    objects represent the edges in the graph, whereas operations (e.g., tf.matmul)
    represent the nodes. For example, the data-flow graph for'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道数据流图是什么样的。这是 TensorFlow 用来描述您定义的计算流程的术语，并表示为*有向无环图*（DAG）：箭头表示数据，节点表示操作。换句话说，tf.Variable
    和 tf.Tensor 对象表示图中的边，而操作（例如 tf.matmul）表示节点。例如，对于
- en: '*h = x W*[1] + *b*[1]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*h = x W*[1] + *b*[1]'
- en: would look like figure 2.4\. Then, at runtime, you could get the value of y
    by feeding values to x, as y is dependent on the input x.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将看起来像图 2.4。然后，在运行时，您可以通过向 x 提供值来获取 y 的值，因为 y 依赖于输入 x。
- en: '![02-04](../../OEBPS/Images/02-04.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![02-04](../../OEBPS/Images/02-04.png)'
- en: Figure 2.4 An example computational graph. The various elements here are covered
    in more detail in section 2.2.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 一个示例计算图。这里的各个元素将在 2.2 节中更详细地讨论。
- en: How does TensorFlow know to create the data-flow graph? You might have noticed
    the line starting with the symbol @ hanging on top of the forward(...) function.
    This is known as a *decorator* in Python language. The @tf.function decorator
    takes in a function that performs various TensorFlow operations, traces all the
    steps, and turns that into a data-flow graph. How cool is that? This encourages
    the user to write modular code while enabling the computational advantages of
    a data-flow graph. This feature in TensorFlow 2 is known appropriately as AutoGraph
    ([https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 如何知道创建数据流图？您可能已经注意到以@符号开头的行悬挂在 forward(...) 函数的顶部。这在 Python 语言中称为*装饰器*。@tf.function
    装饰器接受执行各种 TensorFlow 操作的函数，跟踪所有步骤，并将其转换为数据流图。这是多么酷？这鼓励用户编写模块化代码，同时实现数据流图的计算优势。TensorFlow
    2 中这个功能被称为 AutoGraph（[https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function)）。
- en: What is a decorator?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是装饰器？
- en: 'A decorator modifies the behavior of a function by wrapping it, which happens
    before/after the function is invoked. A good example of a decorator is logging
    the inputs and outputs of a function whenever it is invoked. Here’s how you would
    use decorators for this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 装饰器通过包装函数来修改函数的行为，这发生在函数被调用之前/之后。一个很好的装饰器示例是在每次调用函数时记录输入和输出。下面是如何使用装饰器的示例：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will output
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: as expected. Therefore, when you add the @tf.function decorator, it essentially
    modifies the behavior of the invoked function by building a computational graph
    of the computations happening within the given function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的。因此，当您添加 @tf.function 装饰器时，它实际上修改了调用函数的行为，通过构建给定函数内发生的计算的计算图。
- en: The diagram in figure 2.5 depicts the execution path of a TensorFlow 2 program.
    The first time the functions a(...) and b(...) are invoked, the data-flow graph
    is created. Then, inputs passed to the functions will be fed to the graph and
    obtain the outputs you are interested in.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5中的图解描述了 TensorFlow 2 程序的执行路径。第一次调用函数 a(...) 和 b(...) 时，将创建数据流图。然后，将输入传递给函数，以将输入传递给图并获取您感兴趣的输出。
- en: '![02-05](../../OEBPS/Images/02-05.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![02-05](../../OEBPS/Images/02-05.png)'
- en: Figure 2.5 Typical execution of a TensorFlow 2 program. In the first run, TensorFlow
    traces all functions annotated with @tf.function and builds the data-flow graph.
    In the subsequent runs, corresponding values are fed to the graph (according to
    the function call) and the results are retrieved.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 TensorFlow 2程序的典型执行。在第一次运行时，TensorFlow 会跟踪所有使用 @tf.function 注释的函数，并构建数据流图。在后续运行中，根据函数调用传递相应的值给图，并检索结果。
- en: AutoGraph
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGraph
- en: 'AutoGraph is a great feature in TensorFlow that reduces the developer’s workload
    by working hard behind the scene. To build true appreciation for the feature,
    read more at [https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function).
    Though it is quite amazing, AutoGraph is not a silver bullet. Therefore, it is
    important to understand its advantages as well as its limitations and caveats:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGraph 是 TensorFlow 中的一个很棒的功能，通过在幕后努力工作，减轻了开发者的工作量。要真正欣赏这个功能，请阅读更多内容请访问[https://www.tensorflow.org/guide/function](https://www.tensorflow.org/guide/function)。虽然它相当令人惊叹，但
    AutoGraph 不是万能药。因此，了解其优点以及限制和注意事项非常重要：
- en: AutoGraph will provide a performance boost if your code consists of lots of
    repetitive operations (e.g., training a neural network for many iterations).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的代码包含大量重复操作（例如，多次迭代训练神经网络），AutoGraph 将提供性能提升。
- en: AutoGraph might slow you down if you run many different operations that only
    run once; because you run the operation only once, building the graph is just
    an overhead.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您运行多个仅运行一次的不同操作，则 AutoGraph 可能会减慢您的速度；因为您仅运行一次操作，构建图仅是一种开销。
- en: Be careful of what you include inside the function you are exposing to AutoGraph.
    For example
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要注意将什么包含在您向 AutoGraph 公开的函数内。例如
- en: NumPy arrays and Python lists will be converted to tf.constant objects.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 数组和 Python 列表将被转换为 tf.constant 对象。
- en: for loops will be unwrapped during function tracing, which might result in large
    graphs that eventually run out of memory.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在函数跟踪期间将展开 for 循环，这可能导致大型图最终耗尽内存。
- en: 'TensorFlow 1, the predecessor of TensorFlow 2, used an execution style known
    as *declarative graph-based execution*, which consists of two steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 1，TensorFlow 2 的前身，使用了一种称为*声明式基于图的执行*的执行风格，它包含两个步骤：
- en: Explicitly define a data-flow graph using various symbolic elements (e.g., placeholder
    inputs, variables, and operations) of what you need to achieve. Unlike in TensorFlow
    2, these do not hold values at declaration.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 明确定义一个数据流图，使用各种符号元素（例如占位符输入、变量和操作），以实现你所需的功能。与 TensorFlow 2 不同，这些在声明时不会保存值。
- en: Explicitly write code to run the defined graph and obtain or evaluate results.
    You can feed actual values to the previously defined symbolic elements at runtime
    and execute the graph.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 明确编写代码来运行定义的图，并获取或评估结果。您可以在运行时向先前定义的符号元素提供实际值，并执行图。
- en: This is very different from TensorFlow 2, which hides all the intricacies of
    the data-flow graph by automatically building it in the background. In TensorFlow
    1, you have to explicitly build the graph and then execute it, leading to code
    that’s more complex and difficult to read. Table 2.2 summarizes the differences
    between TensorFlow 1 and TensorFlow 2.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 TensorFlow 2 非常不同，后者隐藏了数据流图的所有复杂性，通过自动在后台构建它。在 TensorFlow 1 中，您必须显式构建图，然后执行它，导致代码更加复杂且难以阅读。表2.2总结了TensorFlow
    1和TensorFlow 2之间的区别。
- en: Table 2.2 Differences between TensorFlow 1 and TensorFlow 2
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.2 TensorFlow 1和TensorFlow 2之间的区别
- en: '| **TensorFlow 1** | **TensorFlow 2** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **TensorFlow 1** | **TensorFlow 2** |'
- en: '| Does not use eager execution by default | Uses eager execution by default
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 默认情况下不使用急切执行 | 默认情况下使用急切执行 |'
- en: '| Uses symbolic placeholders to represent inputs to the graph | Directly feeds
    actual data (e.g., NumPy arrays) to the data-flow graph |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 使用符号占位符表示图形的输入 | 直接将实际数据（例如，NumPy数组）提供给数据流图 |'
- en: '| Difficult to debug as results are not evaluated imperatively | Easy to debug
    as operations are evaluated imperatively |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 由于结果不是按命令式评估，因此难以调试 | 由于操作是按命令式评估的，因此易于调试 |'
- en: '| Needs to explicitly and manually create the data-flow graph | Has AutoGraph
    functionality, which traces TensorFlow operations and creates the graph automatically
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 需要显式手动创建数据流图 | 具有AutoGraph功能，可以跟踪TensorFlow操作并自动创建图形 |'
- en: '| Does not encourage object-oriented programming, as it forces you to define
    the computational graph in advance | Encourages object-oriented programming |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 不鼓励面向对象编程，因为它强制您提前定义计算图 | 鼓励面向对象编程 |'
- en: '| Results in poor readability of code due to having separate graph definition
    and runtime code | Has better readability of code |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 由于具有单独的图形定义和运行时代码，代码的可读性较差 | 具有更好的代码可读性 |'
- en: In the next section, we discuss the basic building blocks of TensorFlow that
    set the foundation for writing TensorFlow programs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论TensorFlow的基本构建模块，为编写TensorFlow程序奠定基础。
- en: Exercise 1
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1
- en: Given the following code,
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 给定以下代码，
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: where should the tf.function decorator go?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: tf.function装饰器应该放在哪里？
- en: A
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: C
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: Any of above
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以上任何一项
- en: 2.2 TensorFlow building blocks
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 TensorFlow构建模块
- en: We have seen the core differences between TensorFlow 1 and TensorFlow 2\. While
    doing this, you were exposed to various data structures (e.g., tf.Variable) and
    operations (e.g., tf.matmul) exposed by the TensorFlow API. Let’s now see where
    and how you might use these data structures and operations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了TensorFlow 1和TensorFlow 2之间的核心差异。在此过程中，您接触到了TensorFlow API公开的各种数据结构（例如，tf.Variable）和操作（例如，tf.matmul）。现在让我们看看在哪里以及如何使用这些数据结构和操作。
- en: 'In TensorFlow 2, there are three major basic elements we need to learn about:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 2中，我们需要了解三个主要的基本元素：
- en: tf.Variable
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.Variable
- en: tf.Tensor
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.Tensor
- en: tf.Operation
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.Operation
- en: You have already seen all of these being used. For example, from the previous
    MLP example, we have these elements, as shown in table 2.3\. Having knowledge
    of these primitive components is helpful in understanding more abstract components,
    such as a Keras layer and model objects, and will be discussed later.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到所有这些被使用了。例如，从前面的MLP示例中，我们有这些元素，如表2.3所示。了解这些基本组件有助于理解更抽象的组件，例如Keras层和模型对象，稍后将进行讨论。
- en: Table 2.3 tf.Variable, tf.Tensor, and tf.Operation entities from the MLP example
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.3 MLP示例中的tf.Variable、tf.Tensor和tf.Operation实体
- en: '| **Element** | **Example** |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **元素** | **示例** |'
- en: '| tf.Variable | w1*,* b1*,* w2 and b2 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| tf.Variable | w1*,* b1*,* w2 和 b2 |'
- en: '| tf.Tensor | h and y |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| tf.Tensor | h 和 y |'
- en: '| tf.Operation | tf.matmul |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| tf.Operation | tf.matmul |'
- en: It is important to firmly grok these basic elements of TensorFlow for several
    reasons. The main reason is that everything you see in this book, from this point
    on, is built on top of these elements. For example, if you are using a high-level
    API like Keras to build a model, it still uses tf.Variable, tf.Tensor, and tf.Operation
    entities to do the computations. Therefore, it is important to know how to use
    these elements and what you can and cannot achieve with them. The other benefit
    is that the errors returned by TensorFlow are usually presented to you using these
    elements. So, this knowledge will also help us understand errors and resolve them
    quickly as we develop more complex models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 牢牢掌握TensorFlow的这些基本元素非常重要，原因有几个。主要原因是，从现在开始，您在本书中看到的所有内容都是基于这些元素构建的。例如，如果您使用像Keras这样的高级API构建模型，它仍然使用tf.Variable、tf.Tensor和tf.Operation实体来进行计算。因此，了解如何使用这些元素以及您可以实现什么和不能实现什么非常重要。另一个好处是，TensorFlow返回的错误通常使用这些元素呈现给您。因此，这些知识还将帮助我们理解错误并在开发更复杂的模型时迅速解决它们。
- en: 2.2.1 Understanding tf.Variable
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 理解tf.Variable
- en: 'When building a typical machine learning model, you have two types of data:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 构建典型的机器学习模型时，您有两种类型的数据：
- en: Model parameters that change over time (mutable) as the model is optimized with
    regard to a chosen loss function
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数随时间变化（可变），因为模型针对所选损失函数进行了优化。
- en: Outputs of the model that are static given data and model parameters (immutable)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型输出是给定数据和模型参数的静态值（不可变）
- en: 'tf.Variable is ideal for defining model parameters, as they are initialized
    with some value and can change the value over time. A TensorFlow variable must
    have the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Variable 是定义模型参数的理想选择，因为它们被初始化为某个值，并且可以随着时间改变其值。一个 TensorFlow 变量必须具有以下内容：
- en: A shape (size of each dimension of the variable)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形状（变量的每个维度的大小）
- en: An initial value (e.g., randomly initialized from values sampled from a normal
    distribution)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始值（例如，从正态分布中抽样的随机初始化）
- en: A data type (e.g., int32, float32)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型（例如 int32、float32）
- en: You can define a TensorFlow variable as follows
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以如下定义一个 TensorFlow 变量
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: where
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: initial_value contains the initial value provided to the model. This is typically
    provided using a variable initializer provided in the tf.keras.initializers submodule
    (the full list of initializers can be found at [http://mng.bz/M2Nm](http://mng.bz/M2Nm)).
    For example, if you want to initialize the variable randomly with a 2D matrix
    having four rows and three columns using a uniform distribution, you can pass
    tf.keras.initializers.RandomUniform()([4,3]). You must provide a value to the
    initial_value argument.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始值包含提供给模型的初始值。通常使用 tf.keras.initializers 子模块中提供的变量初始化器提供（完整的初始化器列表可以在 [http://mng.bz/M2Nm](http://mng.bz/M2Nm)
    找到）。例如，如果你想使用均匀分布随机初始化一个包含四行三列的二维矩阵的变量，你可以传递 tf.keras.initializers.RandomUniform()([4,3])。你必须为
    initial_value 参数提供一个值。
- en: trainable parameter accepts a Boolean value (i.e., True or False) as the input.
    Setting the trainable parameter to True allows the model parameters to be changed
    by means of gradient descent. Setting the trainable parameter to False will freeze
    the layer so that the values cannot be changed using gradient descent.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: trainable 参数接受布尔值（即 True 或 False）作为输入。将 trainable 参数设置为 True 允许通过梯度下降更改模型参数。将
    trainable 参数设置为 False 将冻结层，以使值不能使用梯度下降进行更改。
- en: dtype specifies the data type of the data contained in the variable. If unspecified,
    this defaults to the data type provided to the initial_value argument (typically
    float32).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dtype 指定变量中包含的数据的数据类型。如果未指定，这将默认为提供给 initial_value 参数的数据类型（通常为 float32）。
- en: 'Let’s see how we can define TensorFlow variables. First, make sure you have
    imported the following libraries:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何定义 TensorFlow 变量。首先，请确保已导入以下库：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can define a TensorFlow variable with one dimension of size 4 with a constant
    value of 2 as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以如下定义一个大小为 4 的一维 TensorFlow 变量，其常量值为 2：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, tf.constant(2.0, shape=[4]) produces a vector of four elements having
    a value 2.0, which then is used as the initial value of tf.Variable. You can also
    define a TensorFlow variable with a NumPy array:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，tf.constant(2.0, shape=[4]) 生成一个有四个元素且值为 2.0 的向量，然后将其用作 tf.Variable 的初始值。你也可以使用
    NumPy 数组定义一个 TensorFlow 变量：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, np.ones(shape=[4,3]) generates a matrix of shape [4,3], and all the elements
    have a value of 1\. The next code snippet defines a TensorFlow variable with three
    dimensions (3×4×5) with random normal initialization:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，np.ones(shape=[4,3]) 生成一个形状为 [4,3] 的矩阵，所有元素的值都为 1。下一个代码片段定义了一个具有随机正态初始化的三维（3×4×5）
    TensorFlow 变量：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, you can see that if we print a tf.Variable it is possible to see its
    attributes such as the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到如果我们打印一个 tf.Variable，可以看到它的属性，如下所示：
- en: The name of the variable
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量的名称
- en: The shape of the variable
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量的形状
- en: The data type of the variable
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量的数据类型
- en: The initial value of the variable
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量的初始值
- en: You can also convert your tf.Variable to a NumPy array with a single line using
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用一行代码将你的 tf.Variable 转换为 NumPy 数组
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can then validate the result yourself by printing the Python variable arr
    using
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以通过打印 Python 变量 arr 来验证结果
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: which will return
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: A key characteristic of a tf.Variable is that you can change the value of its
    elements as required even after it is initialized. For example, to manipulate
    individual elements or slices of a tf.Variable, you can use the assign() operation
    as follows.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Variable 的一个关键特点是，即使在初始化后，你也可以根据需要更改其元素的值。例如，要操作 tf.Variable 的单个元素或片段，你可以使用
    assign() 操作如下。
- en: 'For the purpose of this exercise, let us assume the following TensorFlow variable,
    which is a matrix initialized with zeros that has four rows and three columns:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本练习的目的，让我们假设以下 TensorFlow 变量，它是一个由零初始化的矩阵，有四行三列：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can change the element in the first (i.e., index 0) row and third (i.e.,
    index 2) column as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以如下更改第一行（即索引为 0）和第三列（即索引为 2）中的元素：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will produce the following array:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生下列数组：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: NOTE Remember that Python uses zero-based indexing. This means that indexing
    starts from zero (not one). For example, if you want to get the second element
    of a vector vec, you would use vec[1].
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Python 使用以零为基数的索引。这意味着索引从零开始（而不是从一开始）。例如，如果你要获取向量 vec 的第二个元素，你应该使用 vec[1]。
- en: 'You can also change values using slicing as follows. Here, we are changing
    the values that lie in the last two rows and first two columns:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用切片改变数值。例如，下面我们就将最后两行和前两列的数值改为另外一些数：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This results in
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Exercise 2
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2
- en: Can you write the code to create a tf.Variable that has the following values
    and has type int16? You can use np.array() for this purpose.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请编写代码创建一个 tf.Variable，其数值为下面的数值，并且类型为 int16。你可以使用 np.array() 来完成该任务。
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 2.2.2 Understanding tf.Tensor
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 理解 tf.Tensor
- en: As we have seen, tf.Tensor is the output of performing a TensorFlow operation
    on some data (e.g., on a tf.Variable or a tf.Tensor). tf.Tensor objects are heavily
    used when defining machine learning models, as they are used to store inputs,
    interim outputs of layers, and final outputs of the model. So far, we have looked
    mostly at vectors (one dimension) and matrices (two dimension). However, there’s
    nothing stopping us from creating n-dimensional data structures. Such an n-dimensional
    data structure is known as a *tensor*. Table 2.4 shows a few examples of tensors.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，tf.Tensor 是对某些数据进行 TensorFlow 操作后得到的输出（例如，对 tf.Variable 或者 tf.Tensor
    进行操作）。在定义机器学习模型时，tf.Tensor 对象广泛应用于存储输入、层的中间输出、以及模型的最终输出。到目前为止，我们主要看了向量（一维）和矩阵（二维）。但是，我们也可以创建
    n 维数据结构。这样的一个 n 维数据结构被称为一个 *张量*。表 2.4 展示了一些张量的示例。
- en: Table 2.4 Examples of tensors
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.4 张量的示例
- en: '| **Description** | **Example** |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| **描述** | **示例** |'
- en: '| A 2D tensor with two rows and four columns |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 一个 2 × 4 的二维张量 |'
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| A 4D tensor of size 2 × 3 × 2 × 1 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 一个大小为 2 × 3 × 2 × 1 的四维张量 |'
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Tensors also have axes. Each dimension of the tensor is considered an axis.
    Figure 2.6 depicts the axes of a 3D tensor.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 张量也有轴，张量的每个维度都被认为是一个轴。图 2.6 描述了一个 3D 张量的轴。
- en: '![02-06](../../OEBPS/Images/02-06.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![02-06](../../OEBPS/Images/02-06.png)'
- en: Figure 2.6 A 2 × 4 × 3 tensor with the three axes. The first axis (axis 0) is
    the row dimension, the second axis (axis 1) is the column axis, and the final
    axis (axis 2) is the depth axis.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 一个 2 × 4 × 3 张量，包含三个轴。第一个轴（axis 0）是行维度，第二个轴（axis 1）是列维度，最后一个轴（axis 2）是深度维度。
- en: Technically, a tensor can also have just a single dimension (i.e., vector) or
    be a scalar. An important distinction to make is how the terms *tensor* and tf.Tensor
    are used. We will use *tensor/vector/scalar* to refer to a tensor when we are
    discussing mathematical aspects of our models. We will refer to any data-related
    output produced by our TensorFlow code as a tf.Tensor.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，张量也可以只有一个维度（即向量）或者只是一个标量。但是需要区分 *tensor* 和 tf.Tensor。在讨论模型的数学方面时我们会使用 *tensor/vector/scalar*，而我们在提到
    TensorFlow 代码所输出的任何数据相关输出时都会使用 tf.Tensor。
- en: 'Next we will discuss a few instances where you will end up with a tf.Tensor.
    For example, you can produce a tf.Tensor by multiplying a tf.Variable with a constant:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将讨论一些会产生 tf.Tensor 的情况。例如，你可以通过一个 tf.Variable 和一个常数相乘来产生一个 tf.Tensor：
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If you analyze the type of the object produced after the previous operation
    using print(type(b).__name__), you will see the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 print(type(b).__name__) 分析前面操作生成的对象类型，你会看到下面的输出：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'EagerTensor is a class inherited from tf.Tensor. It is a special type of tf.Tensor,
    the value of which is evaluated eagerly (i.e., immediately after defined). You
    can verify that EagerTensor is, in fact, a tf.Tensor by executing the following
    command:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: EagerTensor 是从 tf.Tensor 继承而来的一个类。它是一种特殊类型的 tf.Tensor，其值在定义后会立即得到计算。你可以通过执行下列命令验证
    EagerTensor 实际上是 tf.Tensor：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can also produce a tf.Tensor by adding a tf.Tensor to another tf.Tensor
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过将一个 tf.Tensor 加上另一个 tf.Tensor 来创建一个 tf.Tensor。
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: where print(c) will yield
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: print(c) 将打印出下列结果：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, tf.constant() is used to produce tf.Tensor objects a and b. By adding
    a and b, you will get a tensor c of type tf.Tensor. As before, you can validate
    this claim by running
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，tf.constant() 用于创建 tf.Tensor 对象 a 和 b。通过将 a 和 b 加在一起，你将得到一个类型为 tf.Tensor
    的张量 c。如之前所述，可以通过运行如下代码验证该张量：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The key difference between a tf.Variable and a tf.Tensor is that tf.Variable
    allows its values to change even after the variable is initialized (known as a
    mutable structure). However, once you initialize a tf.Tensor, you cannot change
    it during the lifetime of the execution (known as an *immutable data structure*).
    tf.Variable is a mutable data structure, whereas tf.Tensor is an immutable data
    structure.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Variable和tf.Tensor之间的关键区别在于，tf.Variable允许其值在变量初始化后发生更改（称为可变结构）。然而，一旦您初始化了一个tf.Tensor，在执行的生命周期中您就无法更改它（称为*不可变数据结构*）。tf.Variable是一种可变数据结构，而tf.Tensor是一种不可变数据结构。
- en: 'Let’s see what happens if you try to change the value of a tf.Tensor after
    it’s initialized:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果尝试在初始化后更改tf.Tensor的值会发生什么：
- en: '[PRE33]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You will get the following error:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您将收到以下错误：
- en: '[PRE34]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Clearly, TensorFlow isn’t amused by our rebellious act of trying to modify tf.Tensor
    objects.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，TensorFlow对我们尝试修改tf.Tensor对象的叛逆行为并不感兴趣。
- en: Tensor Zoo
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 张量动物园
- en: 'TensorFlow has an arsenal of different Tensor types for attacking various problems.
    Here are a few different Tensor types available in TensorFlow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow有各种不同的张量类型，用于解决各种问题。以下是TensorFlow中可用的一些不同的张量类型：
- en: RaggedTensor—A type of data used for variable sequence-length data sets that
    cannot be represented as a matrix efficiently
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: RaggedTensor——一种用于不能有效表示为矩阵的可变序列长度数据集的数据类型
- en: TensorArray—A dynamic-sized data structure that can start small and stretch
    as more data is added (similar to a Python list)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: TensorArray——一种动态大小的数据结构，可以从小开始，并随着添加更多数据而伸展（类似于Python列表）
- en: SparseTensor—A type of data used to represent sparse data (e.g., a user-by-movie
    rating matrix)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SparseTensor——一种用于表示稀疏数据的数据类型（例如，用户-电影评分矩阵）
- en: In the next subsection, we will discuss some of the popular TensorFlow operations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将讨论一些流行的TensorFlow操作。
- en: Exercise 3
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3
- en: Can you write the code to create a tf.Tensor that is initialized with values
    sampled from a normal distribution and that has the shape 4 × 1 × 5? You can use
    np.random .normal() for this purpose.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你能写出创建初始化为从正态分布中抽样的值并且形状为4 × 1 × 5的tf.Tensor的代码吗？您可以使用np.random.normal()来实现这个目的。
- en: 2.2.3 Understanding tf.Operation
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 理解tf.Operation
- en: The backbone of TensorFlow that allows you to do useful things with the data
    are the operations available. For example, one of the core operations in a deep
    network is matrix multiplication, which makes TensorFlow a great tool for implementing
    core operations. Like matrix multiplication, TensorFlow offers a wide range of
    low-level operations that can be used in TensorFlow. A full list of operations
    available via the TensorFlow API can be found at [http://mng.bz/aDWY](http://mng.bz/aDWY).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的骨干是操作，它允许您对数据进行有用的操作。例如，深度网络中的核心操作之一是矩阵乘法，这使得TensorFlow成为实现核心操作的强大工具。就像矩阵乘法一样，TensorFlow提供了许多低级操作，可用于TensorFlow。可以在[TensorFlow
    API](http://mng.bz/aDWY)中找到可用操作的完整列表。
- en: 'Let’s discuss some popular arithmetic operations you have at your disposal.
    First, you have basic arithmetic operations such as addition, subtraction, multiplication,
    and division. You can perform these just like you would with normal Python variables.
    To demonstrate this, let’s assume the following vectors:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些您可以使用的流行算术操作。首先，您有基本的算术操作，如加法、减法、乘法和除法。您可以像对待普通Python变量一样执行这些操作。为了演示这一点，让我们假设以下向量：
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can look at what a and b look like by executing the following
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行以下操作来查看a和b的样子
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: which gives
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Performing addition on a and b
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对a和b执行加法
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: gives
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 提供
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Performing multiplication on a and b
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对a和b执行乘法
- en: '[PRE40]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: gives
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 提供
- en: '[PRE41]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You can also do logical comparisons between tensors. Assuming
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在张量之间进行逻辑比较。假设
- en: '[PRE42]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: and checking for element-wise equality
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 并检查逐元素相等性
- en: '[PRE43]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: gives
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 提供
- en: '[PRE44]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Checking less than or equal elements
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 检查小于或等于元素
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: gives
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 提供
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, you have reduction operators that allow you to reduce a tensor (e.g.,
    minimum/ maximum/sum/product) on a specific axis or all axes:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您有减少运算符，允许您在特定轴或所有轴上减少张量（例如，最小值/最大值/和/乘积）：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Here, a is a tf.Tensor that looks like this:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，a是一个看起来像这样的tf.Tensor：
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s first get the sum of all elements of this tensor. In other words, reduce
    the tensor on all axes:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先获取此张量的所有元素的总和。换句话说，在所有轴上减少张量：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This produces
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, let’s get the product on axis 0 (i.e., element-wise product of each row
    of a):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在轴0上获取产品（即，对a的每一行进行逐元素乘积）：
- en: '[PRE51]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This produces
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生
- en: '[PRE52]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will now get the minimum over multiple axes (i.e., 0 and 1):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在多个轴（即0和1）上获取最小值：
- en: '[PRE53]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This produces
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: You can see that whenever you perform a reduction operation on a certain dimension,
    you are losing that dimension. For example, if you have a tensor of size [6,4,2]
    and reduce that tensor on axis 1 (i.e., second axis), you will have a tensor of
    size [6,2]. In certain instances, you need to keep this dimension there while
    reducing the tensor (resulting in a [6,1,2]-shaped tensor). One such instance
    is to make your tensor broadcast compatible with another tensor ([http://mng.bz/g4Zn](http://mng.bz/g4Zn)).
    *Broadcasting* is a term used to describe how scientific computation tools (e.g.,
    NumPy/TensorFlow) treat tensors during arithmetic operations. In such situations,
    you can set the keepdims parameter to True (which defaults to False). You can
    see the difference in the shape of the final output
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，无论何时在某个维度上执行缩减操作，你都会失去该维度。例如，如果你有一个大小为[6,4,2]的张量，并且在轴1上缩减该张量（即第二个轴），你将得到一个大小为[6,2]的张量。在某些情况下，你需要在缩减张量的同时保留该维度（导致一个[6,1,2]形状的张量）。一个这样的情况是使你的张量广播兼容另一个张量（[http://mng.bz/g4Zn](http://mng.bz/g4Zn)）。*广播*是一个术语，用来描述科学计算工具（例如NumPy/TensorFlow）在算术操作期间如何处理张量。在这种情况下，你可以将keepdims参数设置为True（默认为False）。你可以看到最终输出的形状的差异
- en: '[PRE55]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: which produces
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生
- en: '[PRE56]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This produces
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生
- en: '[PRE57]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Several other important functions are outlined in table 2.5.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.5中概述了其他几个重要的函数。
- en: Table 2.5 Mathematical functions offered in TensorFlow
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.5 TensorFlow提供的数学函数
- en: '| tf.argmax | Description | Computes the index of a maximum value on a given
    axis. For example, the following example shows how to compute tf.argmax on axis
    0. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| tf.argmax | 描述 | 计算给定轴上最大值的索引。例如，以下示例显示了如何在轴0上计算tf.argmax。 |'
- en: '| Usage | d = tf.constant([[1,2,3],[3,4,5],[6,5,4]])d_max1 = tf.argmax(d, axis=0)
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 用法 | d = tf.constant([[1,2,3],[3,4,5],[6,5,4]])d_max1 = tf.argmax(d, axis=0)
    |'
- en: '| Result | tf.Tensor ([2,2,0]) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 结果 | tf.Tensor ([2,2,0]) |'
- en: '| tf.argmin | Description | Computes the index of a minimum value on a given
    axis. For example, the following example shows how to compute tf.argmin on axis
    1. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| tf.argmin | 描述 | 计算给定轴上最小值的索引。例如，以下示例显示了如何在轴1上计算tf.argmin。 |'
- en: '| Usage | d = tf.constant([[1,2,3],[3,4,5],[6,5,4]])d_min1 = tf.argmin(d, axis=1)
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 用法 | d = tf.constant([[1,2,3],[3,4,5],[6,5,4]])d_min1 = tf.argmin(d, axis=1)
    |'
- en: '| Result | tf.Tensor([[0],[0],[0]]) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 结果 | tf.Tensor([[0],[0],[0]]) |'
- en: '| tf.cumsum | Description | Computes the cumulative sum of a vector or a tensor
    on a given axis |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| tf.cumsum | 描述 | 计算给定轴上向量或张量的累积和 |'
- en: '| Usage | e = tf.constant([1,2,3,4,5])e_cumsum = tf.cumsum(e) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 用法 | e = tf.constant([1,2,3,4,5])e_cumsum = tf.cumsum(e) |'
- en: '| Result | tf.Tensor([1,3,6,10,15]) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 结果 | tf.Tensor([1,3,6,10,15]) |'
- en: We conclude our discussion about basic primitives of TensorFlow here. Next we
    will discuss some of the computations that are commonly used in neural network
    models.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里结束了对TensorFlow基本原语的讨论。接下来我们将讨论在神经网络模型中常用的一些计算。
- en: Exercise 4
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4
- en: There is another function for computing mean called tf.reduce_mean(). Given
    the tf.Tensor object a, which contains the following values, can you compute the
    mean for each column?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个计算平均值的函数叫做tf.reduce_mean()。给定包含以下值的tf.Tensor对象a，你能计算每列的平均值吗？
- en: '[PRE58]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 2.3 Neural network-related computations in TensorFlow
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 TensorFlow中与神经网络相关的计算
- en: Here we will talk about some key low-level operations that underpin deep neural
    networks. Let’s say you are taking a computer vision class at school. For your
    assignment, you have to manipulate an image using various mathematical operations
    to achieve various effects. We will be using the famous image of a baboon (figure
    2.7), which is a popular choice for computer vision problems.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将讨论一些支撑深度神经网络的关键低级操作。假设你在学校学习计算机视觉课程。对于你的作业，你必须使用各种数学运算来操作图像，以实现各种效果。我们将使用一张著名的狒狒图像（图2.7），这是计算机视觉问题的常见选择。
- en: '![02-07](../../OEBPS/Images/02-07.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![02-07](../../OEBPS/Images/02-07.png)'
- en: Figure 2.7 Image of a baboon
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 狒狒的图像
- en: 2.3.1 Matrix multiplication
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 矩阵乘法
- en: Your first task is to convert the image from RGB to grayscale. For this, you
    must employ matrix multiplication. Let’s first understand what matrix multiplication
    is.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一个任务是将图像从RGB转换为灰度。为此，你必须使用矩阵乘法。让我们首先了解什么是矩阵乘法。
- en: Story of Lena
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Lena的故事
- en: Though we are using an image of a baboon for the exercises, there’s a long-standing
    tradition of using Lena’s (a Swedish model) photo to demonstrate various computer
    vision algorithms. There is a very interesting backstory behind how this became
    a norm for computer vision problems, which you can read at [http://mng.bz/enrZ](http://mng.bz/enrZ).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在练习中使用了狒狒的图像，但长期以来，一直有一个传统，即使用Lena（一位瑞典模特）的照片来演示各种计算机视觉算法。关于这是如何成为计算机视觉问题的规范的背后有一个非常有趣的故事，您可以在[http://mng.bz/enrZ](http://mng.bz/enrZ)上阅读。
- en: You perform matrix multiplication between two tensors using the tf.matmul()
    function. For two matrices, tf.matmul() performs matrix multiplication (e.g.,
    if you have a of size [4,3] and b of size [3,2], matrix multiplication results
    in a [4,2] tensor. Figure 2.8 illustrates the matrix multiplication operation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 使用tf.matmul()函数在两个张量之间执行矩阵乘法。对于两个矩阵，tf.matmul()执行矩阵乘法（例如，如果您有大小为[4,3]和大小为[3,2]的矩阵，矩阵乘法将得到一个[4,2]张量）。图2.8说明了矩阵乘法操作。
- en: '![02-08](../../OEBPS/Images/02-08.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![02-08](../../OEBPS/Images/02-08.png)'
- en: Figure 2.8 Matrix multiplication between a 4 × 3 matrix and 3 × 2 matrix, resulting
    in a 4 × 2 matrix
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 在一个4×3矩阵和一个3×2矩阵之间进行矩阵乘法，得到一个4×2矩阵。
- en: More generally, if you have an n x m matrix (a) and a m x p matrix (b), the
    result of matrix multiplication c is given by
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，如果您有一个n×m矩阵（a）和一个m×p矩阵（b），则矩阵乘法c的结果如下：
- en: '![02_08a](../../OEBPS/Images/02_08a.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![02_08a](../../OEBPS/Images/02_08a.png)'
- en: However, if you have high-dimensional tensors a and b, the sum product over
    the last axis of a and second-to-last axis of b will be performed. Both a and
    b tensors need to have identical dimensionality except for the last two axes.
    For example, if you have a tensor a of size [3,5,7] and b of size [3,7,8], the
    result would be a [3,5,8]-sized tensor.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您有高维张量a和b，则将在a的最后一个轴上和b的倒数第二个轴上执行总乘积。a和b张量的维度除了最后两个轴外都需要相同。例如，如果您有一个大小为[3,5,7]的张量a和大小为[3,7,8]的张量b，则结果将是一个大小为[3,5,8]的张量。
- en: Coming back to our problem, given three RGB pixels, you can convert it to a
    grayscale pixel using
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的问题，给定三个RGB像素，您可以使用以下方法将其转换为灰度像素。
- en: 0.3 * R + 0.59 * G + 0.11 * B
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 0.3 * R + 0.59 * G + 0.11 * B
- en: This is a common operation for converting any RGB image to grayscale ([http://mng.bz/p2M0](http://mng.bz/p2M0)),
    which can be important depending on the problem at hand. For example, to recognize
    digits from images, color is not so important. By converting images to grayscale,
    you are essentially helping the model by reducing the size of the input (one channel
    instead of three) and by removing noisy features (i.e., color information).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将任何RGB图像转换为灰度图像的常见操作（[http://mng.bz/p2M0](http://mng.bz/p2M0)），这取决于手头的问题是否重要。例如，要从图像中识别数字，颜色并不那么重要。通过将图像转换为灰度，您实质上通过减少输入的大小（一个通道而不是三个）并去除噪声特征（即，颜色信息），从而帮助模型。
- en: Given a 512 × 512 × 3 image, if you multiply that with a 3 × 1 array representing
    the weights provided, you will get the grayscale image of size 512 × 512 × 1\.
    Then we need to remove the last dimension of the grayscale image (as it is one),
    and we end up with a matrix of size 512 × 512\. For this you can use the tf.squeeze()
    function, which removes any dimensions that are of size one (see the next listing).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个512×512×3的图像，如果您将其与代表所提供权重的3×1数组相乘，您将得到大小为512×512×1的灰度图像。然后，我们需要删除灰度图像的最后一个维度（因为它是一个），最终得到大小为512×512的矩阵。对此，您可以使用tf.squeeze()函数，该函数删除大小为一的任何维度（请参阅下一个列表）。
- en: Listing 2.2 Converting an RGB image to grayscale using matrix multiplication
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 使用矩阵乘法将RGB图像转换为灰度图像
- en: '[PRE59]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: ❶ PIL is a Python library for basic image manipulation
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ PIL是用于基本图像处理的Python库。
- en: ❷ The RGB image of size 512 × 512 × 3 loaded as a NumPy array
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 大小为512×512×3的RGB图像被加载为NumPy数组。
- en: ❸ The NumPy array is converted to a tf.Tensor.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将NumPy数组转换为tf.Tensor。
- en: ❹ The RGB weights as a 3 × 1 array
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 作为一个3×1数组的RGB权重
- en: ❺ Performing matrix multiplication to get the black-and-white image
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行矩阵乘法以获得黑白图像
- en: ❻ Getting rid of the last dimension, which is 1
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 去掉最后一个维度，即1
- en: Matrix multiplication is an important operation in fully connected networks
    as well. To go from an input layer to a hidden layer, we employ matrix multiplication
    and add operation. For the moment, we will ignore the nonlinear activation, as
    it is just an element-wise transformation. Figure 2.9 visualizes the hidden layer
    computation of the MLP you built earlier.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法在全连接网络中也是一项重要操作。为了从输入层到隐藏层，我们需要使用矩阵乘法和加法。暂时忽略非线性激活，因为它只是一个逐元素的转换。图2.9可视化了您之前构建的MLP的隐藏层计算。
- en: '![02-09](../../OEBPS/Images/02-09.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![02-09](../../OEBPS/Images/02-09.png)'
- en: Figure 2.9 An illustration of the computations taking place in a hidden layer.
    x is the input (1 × 4), W is the weight matrix (4 × 3), b is the bias (1 × 3),
    and finally, h is the output (1 × 3).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 计算发生在隐藏层的插图。x 是输入（1×4），W 是权重矩阵（4×3），b 是偏差（1×3），最终，h 是输出（1×3）。
- en: 2.3.2 Convolution operation
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 卷积操作
- en: The next task is to implement an edge-detection algorithm. Knowing that you
    can detect edges using the convolution operation, you also want to show your skills
    off by achieving this with TensorFlow. The good news is, you can!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的任务是实现边缘检测算法。知道可以使用卷积操作检测边缘后，您还想使用 TensorFlow 展示自己的技能。好消息是，您可以做到！
- en: The convolution operation is essential in convolutional neural networks, which
    are deep networks heavily utilized for image-related machine learning tasks (e.g.,
    image classification, object detection). A convolution operation shifts a *window*
    (also known as a *filter* or a *kernel*) over the data while producing a single
    value at every position. The convolution window will have some value at each location.
    And at a given position, the values in the convolution window are element-wise
    multiplied and summed over with what’s overlapping with that window in the data
    to produce the final value for that location. The convolution operation is shown
    in figure 2.10.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作在卷积神经网络中非常重要，卷积神经网络是用于图像相关的机器学习任务（例如图像分类、物体检测）的深度网络。卷积操作将 *窗口*（也称为*filter*或*kernel*）移动到数据上，同时在每个位置产生单个值。卷积窗口在每个位置都有一些值。对于给定位置，卷积窗口中的值是元素乘积并与数据中与该窗口重叠的部分相加，以产生该位置的最终值。卷积操作如图2.10所示。
- en: '![02-10](../../OEBPS/Images/02-10.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![02-10](../../OEBPS/Images/02-10.png)'
- en: Figure 2.10 Computational steps of the convolution operation
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 卷积操作的计算步骤
- en: Depending on the values you choose for the convolution window, you can produce
    some unique effects. You can try out some popular kernels at [https://setosa.io/ev/image-kernels/](https://setosa.io/ev/image-kernels/).
    Edge detection is also a popular computer vision technique that can be achieved
    using the convolution operation. TensorFlow provides the tf.nn.convolution() function
    to perform convolution.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您选择的卷积窗口的值，您可以产生一些独特的效果。您可以尝试在[https://setosa.io/ev/image-kernels/](https://setosa.io/ev/image-kernels/)上尝试一些流行的核。边缘检测也是一种流行的计算机视觉技术，可以使用卷积操作来实现。TensorFlow提供了tf.nn.convolution()函数来执行卷积。
- en: 'Initially, we have our black-and-white image of the baboon stored as a tf.Tensor
    in the variable x. x is a matrix of size 512 × 512\. Let’s create a new variable
    y from this:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将黑白狒狒图片存储在tf.Tensor中，并将其输入变量命名为x，x是一个大小为512×512的矩阵。现在，让我们从中创建一个名为y的新变量：
- en: '[PRE60]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, let’s define our edge detection filter. We will use an edge detection
    filter known as an *approximate Laplacian filter*, which is a 3 × 3 matrix filled
    with value -1 except for the middle-most value, which is 8\. Note how the sum
    of the kernel is zero:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义我们的边缘检测filter。我们将使用一种名为 *近似拉普拉斯滤波器* 的边缘检测滤波器，它是一个填有 -1 值的 3 × 3 矩阵，除了最中间的值是
    8 外。请注意，内核的总和为零：
- en: '[PRE61]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, we need to reshape y and filter, because the tf.nn.convolution() function
    accepts a very specifically shaped input and a filter. The first constraint is
    that your y and filter should have the same rank. Rank here refers to the number
    of dimensionalities in the data. Here we have rank 2 tensors and will perform
    2D convolution. To perform 2D convolution, both the input and the kernel need
    to be of rank 4\. Therefore, we need to reshape the input and the kernel in a
    few steps:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要对 y 和 filter 进行reshape，因为 tf.nn.convolution() 函数接受具有非常特定形状的输入和filter。第一个约束是y和
    filter 应具有相同的rank。在这里，rank 指数据中的维数个数。我们这里有rank2的张量，将进行二维卷积。要执行 2D 卷积，输入和kernel
    都需要是 rank 4。因此我们需要对输入和kernel 进行几步重塑：
- en: Add two more dimensions at the beginning and end of the input. The dimension
    at the beginning represents the batch dimension, and the last dimension represents
    the channel dimension (e.g., RGB channels of an image). Though the values will
    be 1 in our example, we still need those dimensions to be present (e.g., an image
    of size [512,512] will be reshaped to [1,512,512,1]).
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在输入的开始和结尾添加两个更多的维度。开始的维度表示批量维度，最后的维度表示通道维度（例如，图像的RGB通道）。虽然在我们的示例中值为1，但我们仍然需要这些维度存在（例如，一个大小为[512,512]的图像将重塑为[1,512,512,1]）。
- en: Add two more dimensions of size 1 at the end of the filter. These new dimensions
    represent the incoming and outgoing channels. We have a single channel (i.e.,
    grayscale) coming in, and we want to produce a single channel (i.e., grayscale)
    as well (e.g., a kernel of size [3,3] will be reshaped to [3,3,1,1]).
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在filter的末尾添加两个大小为1的额外维度。这些新维度表示输入和输出通道。我们有一个单通道（即灰度）输入，我们也希望产生一个单通道（即灰度）的输出（例如，一个大小为[3,3]的核将被重塑为[3,3,1,1]）。
- en: NOTE Rank of a tensor refers to the number of dimensions of that tensor. This
    is different from the rank of a matrix.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 张量的阶数是指该张量的维数。这与矩阵的秩不同。
- en: 'Don’t worry if you don’t fully understand why we added these additional dimensions.
    This will make more sense when we discuss the convolution operation in the context
    of convolutional neural networks in a later chapter. For now, you only need to
    understand the high-level behavior of the convolution operation. In TensorFlow,
    you can reshape y and filter as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不完全明白为什么我们添加了这些额外的维度，不要担心。当我们在后面的章节中讨论卷积神经网络中的卷积操作时，这将更有意义。现在，你只需要理解卷积操作的高级行为就可以了。在TensorFlow中，你可以按如下方式重塑y和filter：
- en: '[PRE62]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Here, y is a 512 × 512 tensor. The expression tf.reshape(y, [1,512,512,1])
    converts y (i.e., a 2D tenor) to a 4D tensor of size 1 × 512 × 512 × 1\. Similarly,
    the filter (i.e., a 2D tensor of size 3 × 3) is reshaped to a 4D tensor of size
    3 × 3 × 1 × 1\. Note that the total number of elements is unchanged during the
    reshaping. Now you can compute the convolution output as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，y是一个512×512的张量。表达式tf.reshape(y, [1,512,512,1])将y（即一个2D张量）重塑为一个4D张量，大小为1×512×512×1。同样，filter（即一个大小为3×3的2D张量）被重塑为一个4D张量，大小为3×3×1×1。请注意，在重塑过程中元素的总数保持不变。现在你可以计算卷积输出如下：
- en: '[PRE63]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: You can visualize the result of edge detection and compare that to the original
    image, as shown in figure 2.11.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将边缘检测的结果可视化并将其与原始图像进行比较，如图2.11所示。
- en: '![02-11](../../OEBPS/Images/02-11.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![02-11](../../OEBPS/Images/02-11.png)'
- en: Figure 2.11 Original black-and-white image versus result of edge detection
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 原始黑白图像与边缘检测结果的比较
- en: In the next section, we will discuss another operation known as the pooling
    operation.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论另一种操作，即pooling操作。
- en: 2.3.3 Pooling operation
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 Pooling操作
- en: 'We are off to the next task, which is to resize the image resultant after edge
    detection by halving the width and height of the image. For example, if we have
    a 512 × 512 image and need to rescale it to 256 × 256, the *pooling operation*
    is the best way to achieve this easily. The pooling (or sub-sampling) operation
    is commonly used in convolutional neural networks for this reason: to reduce the
    size of the output so fewer parameters can be used to learn from data.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的任务是将经过边缘检测后的图像的宽度和高度减半。例如，如果我们有一个512×512的图像，并且需要将其调整为256×256，*pooling操作*是实现这一目标的最佳方式。出于这个原因，*pooling（或子采样）操作*在卷积神经网络中常被用于减小输出的尺寸，以便可以使用更少的参数从数据中学习。
- en: Why is it called the pooling operation?
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它被称为pooling操作？
- en: The reason why the sub-sampling operation is also called “pooling” probably
    has its roots in the word’s meaning, as well as in statistics. The word *pooling*
    is used to describe combining things into a single entity, which is exactly what
    is done in this operation (e.g., by means of averaging or taking maximum). In
    statistics, you will find the term *pooled variance*, which is a weighted average
    of the variance between two populations ([http://mng.bz/OGdO](http://mng.bz/OGdO)),
    essentially combining two variances into a single variance.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 子采样操作之所以也被称为“pooling”可能是因为这个词的含义以及统计学的原因。*pooling*一词用于描述将事物合并为一个单一实体，这也正是此操作所做的（例如通过平均值或取最大值）。在统计学中，你会发现术语*pooled
    variance*，它是两个群体之间方差的加权平均值（[http://mng.bz/OGdO](http://mng.bz/OGdO)），本质上将两个方差合并为一个方差。
- en: 'In TensorFlow, you can call the tf.nn.max_pool() function to perform max pooling
    and tf.nn.avg_pool() for average pooling:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，您可以调用tf.nn.max_pool()函数进行最大池化，调用tf.nn.avg_pool()函数进行平均池化：
- en: '[PRE64]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The pooling operation is another commonly found operation in convolutional neural
    networks and works similarly to the convolution operation. But unlike the convolution
    operation, the pooling operation does not have values in the kernel. At a given
    location, it takes either the average or maximum value of what’s overlapping the
    kernel in the data. The operation that produces the average at a given location
    is known as average pooling, whereas the operation that produces the maximum is
    known as max pooling. Figure 2.12 illustrates the max pooling operation.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作是卷积神经网络中常见的另一种操作，它的工作原理与卷积操作类似。但与卷积操作不同的是，池化操作的内核中没有值。在给定位置，池化操作取得与数据中内核重叠的部分的平均值或最大值。在给定位置产生平均值的操作称为平均池化，而产生最大值的操作称为最大池化。图2.12说明了最大池化操作。
- en: '![02-12](../../OEBPS/Images/02-12.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![02-12](../../OEBPS/Images/02-12.png)'
- en: Figure 2.12 Max-pooling operation. The pooling window goes from one position
    to another on the image, while producing a single value (i.e., maximum value in
    the image overlapping the pooling window) at a time.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12：最大池化操作。池化窗口在图像上从一个位置移动到另一个位置，同时一次产生一个值（即与池化窗口重叠的图像中的最大值）。
- en: 'We have y_conv, which is a 4D tensor having the shape [1,510,510,1]. You might
    notice that the dimensions are slightly smaller than the original image size (i.e.,
    512). This is because, when doing convolution with a window of size c x c (without
    extra padding) on an image having h height and w width, the resulting image has
    the dimensions h-c+1 and w-c+1. We can perform pooling as shown. You can perform
    either average pooling or max pooling with the following functions:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个形状为[1,510,510,1]的4D张量y_conv。你可能会注意到，这些维度略小于原始图像的大小（即512）。这是因为，在对具有h高度和w宽度的图像进行没有额外填充的大小为c×c的窗口卷积时，得到的图像的维度为h-c+1和w-c+1。我们可以进行如下所示的池化操作。您可以使用以下函数进行平均池化或最大池化：
- en: '[PRE65]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This will result in two images, z_avg and z_max; both have the shape [1,255,255,1].
    In order to keep just the height and width dimensions and remove redundant dimensions
    of size 1, we use the tf.squeeze() function:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到两个图像，z_avg和z_max；它们的形状都是[1,255,255,1]。为了仅保留高度和宽度维度并移除大小为1的冗余维度，我们使用tf.squeeze()函数：
- en: '[PRE66]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: You can plot z_avg and z_max using matplotlib (a plotting library in Python)
    and get the result shown in figure 2.13\. The code is provided in the notebook.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Python的绘图库matplotlib绘制z_avg和z_max并获得图2.13中所示的结果。代码已在笔记本中提供。
- en: '![02-13](../../OEBPS/Images/02-13.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![02-13](../../OEBPS/Images/02-13.png)'
- en: Figure 2.13 Result after edge detection versus result after average or max pooling
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：边缘检测后的结果与平均或最大池化后的结果
- en: Figure 2.13 shows the different effects we get with different types of pooling.
    If you look closely, you will see that average pooling results in more consistent
    and continuous lines, whereas max pooling results in a noisier image.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13显示了不同类型池化的效果。仔细观察，您会看到平均池化结果更加一致和连续，而最大池化结果则更加嘈杂。
- en: Note that, unlike in the convolution operation, we are not providing a filter
    (or a kernel), as the pooling operation doesn’t have a kernel. But we need to
    pass in the dimensions of the window. These dimensions represent the corresponding
    dimensions of the input (i.e., it is a [batch dimension, height, width, channels]
    window). In addition to that, we are also passing two arguments, stride and padding.
    We will discuss these in detail in a later chapter.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与卷积操作不同，我们没有提供滤波器（或内核），因为池化操作没有滤波器。但是我们需要传递窗口的维度。这些维度表示输入的相应维度（即它是一个[batch维度、高度、宽度、通道]的窗口）。除此之外，我们还传递了两个参数：stride和padding。我们将在后面的章节中详细讨论这些参数。
- en: Exercise 5
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5
- en: You are given a grayscale image img of size 256 × 256 and a convolution filter
    f of size 5 × 5\. Can you write the tf.reshape() function calls and the tf.nn.convolution()
    operation? What would be the size of the output?
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个大小为256×256的灰度图像img和一个大小为5×5的卷积滤波器f。你能编写tf.reshape()函数调用和tf.nn.convolution()操作吗？输出的大小会是多少？
- en: Great work! Now you know most common operations used in deep learning networks.
    We will end our discussion about TensorFlow basics here. In the next chapter,
    we will discuss a high-level API available in TensorFlow called Keras, which is
    particularly useful for model building.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！现在你已经了解了深度学习网络中最常用的操作。我们将在此结束关于 TensorFlow 基础知识的讨论。在下一章中，我们将讨论 TensorFlow
    中提供的一个高级 API，称为 Keras，它对于模型构建特别有用。
- en: Summary
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow is an end-to-end machine learning framework.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 是一个端到端的机器学习框架。
- en: TensorFlow provides an ecosystem facilitating model prototyping, model building,
    model monitoring, and model serving.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 提供了一个生态系统，便于模型原型设计、模型构建、模型监控和模型提供。
- en: TensorFlow 1 uses declarative graph execution style (define then run), whereas
    TensorFlow 2 uses imperative graph execution style (define by run).
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 1 使用声明式图执行风格（先定义，然后运行），而 TensorFlow 2 使用命令式图执行风格（运行时定义）。
- en: 'TensorFlow provides three main building blocks: tf.Variable (for values that
    change over time), tf.Tensor (values that are fixed over time), and tf.Operation
    (transformations performed on tf.Variable and tf.Tensor objects).'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 提供了三个主要构建模块：tf.Variable（用于随时间变化的值）、tf.Tensor（随时间固定的值）和tf.Operation（对tf.Variable和tf.Tensor对象执行的转换）。
- en: TensorFlow provides several operations that are used to build neural networks
    such as tf.matmul, tf.nn.convolution, and tf.nn.max_pool.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 提供了几个用于构建神经网络的操作，如 tf.matmul、tf.nn.convolution 和 tf.nn.max_pool。
- en: You can use tf.matmul to convert an RGB image to grayscale.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 tf.matmul 将 RGB 图像转换为灰度图像。
- en: You can use tf.nn.convolution to detect edges in an image.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 tf.nn.convolution 来检测图像中的边缘。
- en: You can use tf.nn.max_pool to resize an image.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 tf.nn.max_pool 来调整图像大小。
- en: Answers to exercises
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1:** 2'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1：** 2'
- en: '**Exercise 2:** tf.Variable(np.array([[1,2,3],[4,3,2]], dtype=”int16”)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2：** tf.Variable(np.array([[1,2,3],[4,3,2]], dtype=”int16”)'
- en: '**Exercise 3:** tf.constant(np.random.normal(size=[4,1,5]))'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3：** tf.constant(np.random.normal(size=[4,1,5]))'
- en: '**Exercise 4:** tf.reduce_mean(a, axis=1)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4：** tf.reduce_mean(a, axis=1)'
- en: '**Exercise 5:**'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 5：**'
- en: '[PRE67]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The shape of the final output would be [1,252,252,1]. The resulting size of
    the convolution operation is image size - convolution window size + 1.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出的形状将是 [1,252,252,1]。卷积操作的结果大小为图像大小 - 卷积窗口大小 + 1。
