["```py\n# Step 1: define an objective function\ndef objective(args):\n  model = train(args)                                   ❶\n  return evaluate(model)                                ❶\n\n# Step 2 define search space for hyperparameters \nspace = hp.choice('classifier_type', [                  ❷\n  {\n    'type': 'naive_bayes',\n  },\n  {\n    'type': 'svm',\n    'C': hp.lognormal('svm_C', 0, 1),                   ❸\n    'kernel': hp.choice('svm_kernel', [                 ❸\n      {'ktype': 'linear'},                              ❸\n      {'ktype': 'RBF',                                  ❸\n       'width': hp.lognormal('svm_rbf_width', 0, 1)},   ❸\n    ]),\n  },\n  {\n    'type': 'dtree',\n    'criterion': hp.choice('dtree_criterion', \n      ['gini', 'entropy']),\n    'max_depth': hp.choice('dtree_max_depth',\n      [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),\n    'min_samples_split': hp.qlognormal(\n      'dtree_min_samples_split', 2, 1, 1),\n  },\n  ])\n\n# Step 3 start the hpo process execution \nbest = fmin(objective, space, algo=tpe.suggest,\n ➥ max_evals=100)                            ❹\n```", "```py\n# Step 1: define an objective function\ndef objective(trial):\n\n  regressor_name = trial.suggest_categorical(                  ❶\n    'classifier', ['SVR', 'RandomForest'])                     ❶\n  if regressor_name == 'SVR':\n    svr_c = trial.suggest_float(                               ❷\n      'svr_c', 1e-10, 1e10, log=True)                          ❷\n    regressor_obj = sklearn.svm.SVR(C=svr_c)                   ❷\n  else:\n    rf_max_depth = trial.suggest_int('rf_max_depth', 2, 32)    ❸\n    regressor_obj = sklearn.ensemble\n      .RandomForestRegressor(max_depth=rf_max_depth)\n\n  X_train, X_val, y_train, y_val = \\\n    sklearn.model_selection.train_test_split(X, y, random_state=0)\n\n  regressor_obj.fit(X_train, y_train)                          ❹\n  y_pred = regressor_obj.predict(X_val)\n\n  error = sklearn.metrics\n    .mean_squared_error(y_val, y_pred)                         ❺\n  return error                                                 ❺\n\n# Step 2: Set up HPO by creating a new study.\nstudy = optuna.create_study() \n\n# Step 3: Invoke HPO process \nstudy.optimize(objective, n_trials=100)  \n```", "```py\n# Step 1: define objective_function\ndef objective_function(config):\n  model = ConvNet()                                      ❶\n  model.to(device)\n\n  optimizer = optim.SGD(                                 ❷\n    model.parameters(), lr=config[\"lr\"],                 ❷\n    momentum=config[\"momentum\"])                         ❷\n  for i in range(10):\n    train(model, optimizer, train_loader)                ❸\n    acc = test(model, test_loader)\n\n    tune.report(mean_accuracy=acc)                       ❹\n\n# Step 2: define search space for each hyperparameter\nsearch_space = {\n   \"lr\": tune.sample_from(lambda spec: \n      10**(-10 * np.random.rand())),\n   \"momentum\": tune.uniform(0.1, 0.9)                    ❺\n}\n\n# Uncomment this to enable distributed execution\n# `ray.init(address=\"auto\")`\n\n# Step 3: start the HPO execution\nanalysis = tune.run(\n   objective_function,\n   num_samples=20,\n   scheduler=ASHAScheduler(metric=\"mean_accuracy\", mode=\"max\"),\n   config=search_space)\n\n# check HPO progress and result\n# obtain a trial dataframe from all run trials \n# of this `tune.run` call.\ndfs = analysis.trial_dataframes\n```"]