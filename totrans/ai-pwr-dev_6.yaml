- en: 6 Testing, Assessing, and Explaining with Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Drafting unit tests with ease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating integration tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining code quality and coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing software complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating code and text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter will explore a critical aspect of software engineering: testing.
    The act of testing software serves multiple essential purposes. First and foremost,
    it aids in the identification of bugs, errors, and issues that could potentially
    impact the software''s functionality, usability, or performance. Furthermore,
    it ensures that the software adheres to the required quality standards. By conducting
    thorough tests, we can verify if the software meets the specified requirements,
    functions as intended, and produces the expected outcomes. Through comprehensive
    testing, developers can evaluate the software''s reliability, accuracy, efficiency,
    security, and compatibility across various platforms and environments. Detecting
    and resolving software defects early in the development process can result in
    significant time and cost savings.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have finished formulating our tests, we will evaluate our code's quality.
    You will be introduced to several metrics that prove helpful in assessing software
    quality and complexity. Additionally, if we need clarification on the purpose
    of our code or are reviewing it for the first time, we will seek an explanation
    to ensure a thorough understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Testing, Testing…One, Two, Three Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing plays a vital role in software engineering; therefore, we will explore
    various types of testing in detail. This includes unit tests, integration tests,
    and behavior tests. To start, we will leverage Copilot Chat to assist us in creating
    a *unit test*.
  prefs: []
  type: TYPE_NORMAL
- en: Unit Testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A unit test is a type of testing that focuses on testing individual components
    or units of code to ensure they function correctly in isolation. Developers usually
    perform it and help identify bugs and issues within specific software units.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Unit Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will create unit tests to test our software components.
    There are several unit testing frameworks available for Python. Each has its unique
    features and is suitable for different scenarios. We will examine each of them
    briefly before settling on a specific framework based off of the recommendation
    provided by our AI tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first framework is `unittest`: This is Python''s standard library for creating
    unit tests. It comes bundled with Python and doesn''t need to be installed separately.
    `unittest` provides a rich set of assertions and is great for writing simple to
    complex test cases, but it can be quite verbose. `unittest` is a good choice for
    writing basic unit tests, especially if you don''t want to introduce additional
    dependencies in your project. It''s useful in any scenario where you need to confirm
    the functionality of individual units of code in isolation from the rest of the
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s examine `pytest`: `pytest` is a popular third-party library that
    can be used for unit testing, although it''s versatile enough to handle more than
    just unit tests. It requires less boilerplate code than `unittest` and has powerful
    features like fixtures for setup and teardown, parameterized testing, and the
    ability to run `unittest` and nose test suites. p`ytest` is great for both simple
    and complex unit test cases. It''s also useful for functional and integration
    tests. If you value simplicity and ease of use, and your project is not restricted
    to using only the Python standard library, `pytest` is an excellent choice.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have `nose2`, the successor to the deprecated "`nose`" testing framework.
    It extends `unittest` and makes testing easier. It's known for its test discovery
    feature, which automatically finds your project's tests so you don't have to manually
    list them.`nose2` is good for larger projects where test discovery can save time.
    Like `pytest`, it can run `unittest` test suites, so it's also a good choice if
    you're migrating from `unittest` but want to keep your existing tests.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there is `doctest`. This is another module that's part of the Python
    standard library. It's not a full-featured testing framework like the others,
    but it allows you to write tests directly in your docstrings. `doctest` is best
    suited for simple cases where you want to demonstrate how to use a function or
    module and confirm that the example code works as expected. It's a great way to
    ensure your documentation stays up to date with your code.
  prefs: []
  type: TYPE_NORMAL
- en: Copilot Chat recommended that we use `unittest`, so we shall. You might try
    to ask it yourself and see if it makes a different conclusion, which it might,
    depending on how you frame the question.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will define test cases by creating classes that inherit from the `unittest.TestCase`
    class. In writing our first unit test we will uncover a rather pernicious bug
    that has thus far eluded us. We shall get to that shortly. Finally, we are going
    to test the correctness of the `DepartmentStatisticsVisitor` class.
  prefs: []
  type: TYPE_NORMAL
- en: We start with a prompt asking Copilot Chat to create a unit test for the `DepartmentStatisticsVisitor.`
    The response will be a reminder of the importance of context. Copilot Chat, given
    that we began a new chat session, will attempt to create this unit test, but it
    lacks the definition of any of our classes.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 The first attempt at defining a unit test for `DepartmentStatisticsVisitor`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We should proceed by inputting definitions into Copilot Chat so that it can
    refine the test case. We can tell it more of the visitor class, as shown in the
    following snippet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With this updated information, Copilot Chat is about to better estimate how
    our unit test should function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 The first attempt at defining a unit test for `DepartmentStatisticsVisitor`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The unit test is close to what is needed; however, the definition is Asset is
    incorrect. We should enter the proper definition from our asset.py class. Copilot
    Chat will continue to refine the unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 The next attempt at defining a unit test for `DepartmentStatisticsVisitor
    n`ow that Copilot Chat is aware of how to define `Asset`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is closer, we should also remind Copilot Chat that the `Asset` class also
    has a class called `FundingDetails`. Once we provide it with the definition of
    `FundingDetails` and inform Copilot Chat that it should be part of `Asset`, we
    get closer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 The third attempt at a unit test, which comes reasonably close
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: All that remains is to manually update the import statements.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let this exercise be a cautionary tale in the usage of Large Language Models;
    coercing them to behave with respects to highly focused tasks can be very challenging
    to the point of not being worth the effort. In the real-world, no one would blame
    you for abandoning the prompt-engineering and go back to just coding out this
    test. However, with some persistence you could be able to build out a template
    library that you could use to build a suite of unit tests for similarly shaped
    classes. One additional note is that Copilot Chat can and will generate a test
    for the file in the editor window if you instruct it to “Generate (a) unit test
    for my code,” however, it will mock all objects/properties that are not directly
    part of the class under test. Depending on what you are attempting to test, the
    utility of this feature might be questionable.
  prefs: []
  type: TYPE_NORMAL
- en: What we discover, when we attempt to run this test, is that there is a *circular
    dependency* between visitor, asset, funding details, and depreciation strategy.
    A circular dependency is a situation in which two or more modules or components
    depend on each other directly or indirectly. In our case, when Python tries to
    instantiate the `Asset`, it to load the definition of `FundingDetails`.
  prefs: []
  type: TYPE_NORMAL
- en: We fix this by moving away from a direct instantiation or reference to the `FundingDetails`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 The updated `Asset` class that does not directly reference the `FundingDetails`
    class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We will need to do the same to the `FundingDetails` class. It should not directly
    reference the `DepreciationStrategy` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 The updated `FundingDetails` class that does not directly reference
    the `DepreciationStrategy` class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we saw, we were able to create a unit test using Copilot Chat. However, we
    would likely have been able to create it with more ease had we written it without
    Copilot. The tool is surprisingly good at providing you guidance as to when and
    how to test your code, but the implantation (at least currently) leaves something
    to be desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the real world, we would continue to add unit tests to build up a substantial
    body of tests. How many tests is substantial, you ask? We will explore this shortly.
    However, we should first turn our attention to the next type of test: the *integration
    test*.'
  prefs: []
  type: TYPE_NORMAL
- en: Integration Testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Integration testing involves testing the interaction between different components
    or modules of the software to ensure they work together seamlessly. It verifies
    that the integrated system functions as expected and detects any inconsistencies
    or communication problems between modules.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Integration Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will develop an integration test that will allow us to test
    the end-to-end system. Thankfully, `fastapi` comes with its own test client, which
    will aid us in the creation of this test.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by copying in the definition of our `AssetController` into the Copilot
    Chat window. We can then ask Copilot Chat how to create a integration test for
    this controller. Given that we included the routes in the definition, Copilot
    Chat should be able to provide us with a accurate integration tests. We will need
    to specify that we will use the `InMemoryAssetRepository` class or fix this after
    the test has been generated.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 The Copilot Chat generated integration test of the `AssetController`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now turn our attention to the final type of testing that we examine:
    *behavior testing*.'
  prefs: []
  type: TYPE_NORMAL
- en: Behavior testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Behavior testing is a type of testing that focuses on the behavior of a system
    as a whole, from the perspective of an end user. Behavior testing is typically
    used to test the functionality of a system and to ensure that it meets the requirements
    and specifications that have been defined for it.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Behavior Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All testing, in some sense is behavior testing, as tests verify the behavior
    of the system. However, behavior testing in unique in some respects. Let’s summarize
    the different types of testing that we have encountered thus far and contrast
    this against behavior testes.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing is a type of testing that focuses on testing individual units or
    components of a system in isolation, typically using automated tests. Unit tests
    are designed to test the functionality of individual functions or methods, and
    to ensure that they behave correctly under a variety of conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing, on the other hand, is a type of testing that focuses on
    testing the interactions between different components or units of a system. Integration
    testing is typically used to test the interfaces between different components
    or units, and to ensure that they work together correctly. Integration testing
    can be performed manually or using automated tests, and it typically involves
    testing the interactions between different components or units of a system, rather
    than the system as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral testing focus is on defining the behavior of the software in terms
    of user stories or scenarios. These scenarios are written in a specific format
    called "Given-When-Then" (GWT) and are used to drive the development process.
    The GWT format describes the preconditions (Given), the actions (When), and the
    expected outcomes (Then) of a particular scenario.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress with our testing, we may find that some behavior or components
    are difficult to set up in our test. Additionally, we may find it trickly to isolate
    the behavior of a particular object or module, and to test the interactions between
    different objects. To address this limitation, we can use a *mock object*.
  prefs: []
  type: TYPE_NORMAL
- en: Mock Objects
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A mock object is a test double that simulates the behavior of a real object
    in a controlled way. They can also be used to simulate error conditions or edge
    cases that are difficult to reproduce with real objects. Mock objects can be created
    manually using a mocking library such as `unittest.mock` or `pytest-mock`. These
    libraries provide functions and classes for creating and configuring mock objects.
    Mock objects can be configured to return specific values or raise specific exceptions
    when their methods are called. They can also be used to record the calls made
    to their methods, so that you can verify that the correct methods were called
    with the correct parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We will incorporate mock objects into the prompt to create a behavior test for
    the `AssetManager`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 The prompt to set up a behavior test for the `AssetManager`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Copilot Chat will tell us that we need to create a new file (called a feature
    file) called `asset_manager.feature`. In this file, we will define the `AssetManager's`
    attributes (and any supporting classes). For example, in the feature file defines
    a scenario that adds two assets to the `AssetManager`, confirms that the total
    cost of all assets is $4000.00, and mocks the `AssetLocationMediator` class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 The definition of the `asset_manager.feature` file according to
    Copilot Chat
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We should create a directory in the root of our project called `features`. We
    should place this file in that directory. Additionally, we should add a subdirectory
    called `steps`. We will get the `steps` in just a moment.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a new Python file called `steps.py` `and place it in the`
    `steps` `directory.` This Python file defines the step implementations for the
    scenario in the feature file. The `AssetManager`, `InMemoryAssetRepository`, `AssetLocationMediator`,
    and `Asset` classes are imported, and six functions are defined to correspond
    to the steps in the scenario. These functions use the `context` object to store
    and retrieve data between steps. The `Mock` class from the `unittest.mock` library
    is used to create a mock object for the `AssetLocationMediator` class. The `Asset`
    class is instantiated using named parameters to clarify which parameter belong
    to which attribute. If we ask Copilot Chat for the contents of the steps.py file,
    we will see output similar to the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 The implementation details for our behavior test as defined in `steps.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you did not have it installed already, you should Install the `behave` library
    using pip: `pip install behave.` Additionally, you should add it to the `requirements.txt`
    file to ensure that it will get pulled in when we later build a deployable version
    of this application. We will run the behavior test by issuing the following command
    from the root directory of our project.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 Running the behavior test with the resultant output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In this section, we have laid a foundation for good software development by
    using three types of tests: unit, integration, and behavior. Now, one may quibble
    that it came very late in the development lifecycle of this project. One would
    not be wrong. In the real world, we would develop our tests as we develop our
    code. Some might argue that we should build our test before our code. You may
    or may not hold this belief, but either way you should test early, and test often.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section of the book, we will dive into some metrics that can be
    used to determine the overall quality of our software, and we will ask Copilot
    to help us assess the quality of our code thus far.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Assessing Quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the performance, reliability, maintainability, and overall quality
    of software applications is a crucial aspect of software engineering. This chapter
    will delve into the fascinating and intricate domain of software quality metrics
    – the quantitative standards and benchmarks that guide our understanding of the
    quality of a software system.
  prefs: []
  type: TYPE_NORMAL
- en: Software quality metrics are essential tools that allow stakeholders – developers,
    testers, managers, and users – to assess a software product's state, identifying
    its strengths and areas for improvement. They provide an empirical foundation
    for various processes such as product development, testing, debugging, maintenance,
    and improvement initiatives. By quantifying specific characteristics of the software,
    these metrics provide a tangible means to understand the otherwise abstract concept
    of software quality.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore several important categories of software quality
    metrics, including product metrics, process metrics, and project metrics. We'll
    analyze their significance, methodologies for their calculation, and how they
    can be effectively utilized to evaluate and enhance software quality. This exploration
    will include both static metrics, which are applied to the static software system,
    and dynamic metrics, which assess the system's behavior during execution.
  prefs: []
  type: TYPE_NORMAL
- en: Software quality metrics not only contribute to the technical soundness of a
    software system but also help ensure customer satisfaction, profitability, and
    long-term business success. Therefore, developing an understanding of these metrics
    is invaluable to anyone involved in the field of software development, from engineers
    and project managers to executives and software users.
  prefs: []
  type: TYPE_NORMAL
- en: In this section we will examine a few common measures of complexity and maintainability
    of the class or code. Complex software can be difficult to comprehend. This makes
    it challenging for developers, particularly new ones, to grasp how different parts
    of the software interact with each other. This can slow down the onboarding process
    and development time.
  prefs: []
  type: TYPE_NORMAL
- en: Complex code often leads to higher maintenance efforts. When code is complex,
    modifications or bug fixes can take longer because it's harder to predict the
    impacts of changing a single piece of the system. This can result in higher costs
    over the software's lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Complex software tends to be more error prone. Because it's harder to understand,
    developers are more likely to introduce bugs when making changes. Also, complex
    code can have many interdependencies, where a change in one area may have unexpected
    effects elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the software, the more test cases are required to achieve thorough
    testing. It might also be harder to write these test cases due to the complexity
    of the logic involved.
  prefs: []
  type: TYPE_NORMAL
- en: Writing simple and maintainable code should be one of our highest priorities.
    Observing the change in the metric that accompanies our code should aid us in
    this endeavor. Toward this objective, the first metric that we can (and should)
    use is the *cyclomatic complexity*.
  prefs: []
  type: TYPE_NORMAL
- en: Cyclomatic Complexity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Cyclomatic Complexity is a metric that quantifies the number of independent
    paths through a software module. It measures the complexity of decision-making
    within the code, including loops, conditionals, and branches. A higher cyclomatic
    complexity value indicates increased complexity and suggests the potential for
    more bugs and challenges in understanding and maintaining the code.
  prefs: []
  type: TYPE_NORMAL
- en: Within the file department_visitor.py, enter the prompt in snippet 6.3 anywhere
    within this file. Copilot will immediately output the answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Copilot will tell that the complexity of this class is 1\. You may or may not
    be aware of the meaning of this value. If it is the latter, you can ask Copilot
    to elaborate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Copilot informs us that cyclomatic complexity is good if it is low. Intuatively
    this makes sense. Code with low complexity means that it is simpiler to understand
    and therefore understand and reason about. It is likely easier to maintain as
    well. The next metric that we will explore is the *Halstead Complexity measures.*
  prefs: []
  type: TYPE_NORMAL
- en: Halstead Complexity Measures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Halstead complexity measures assess the complexity of a software program based
    on the number of unique operators and operands used in the code. These measures
    include metrics such as program length (N1), program vocabulary (n1), volume (V),
    difficulty (D), effort (E), and others. These metrics provide insights into the
    size and cognitive complexity of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to last time, we will start with a prompt asking Copilot to determine
    the Halstead Complexity Measure for our visitor class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You may want to continue this Q&A session for a while to see what information
    can be gleamed from Copilot. Once you are ready to continue, there is one more
    metric to explore: the *maintainability index.*'
  prefs: []
  type: TYPE_NORMAL
- en: Maintainability Index
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Maintainability Index is a composite metric that combines several factors,
    including cyclomatic complexity, lines of code, and Halstead complexity measures,
    to provide an overall measure of software maintainability. A higher maintainability
    index suggests easier maintenance and potentially lower complexity.
  prefs: []
  type: TYPE_NORMAL
- en: You should start a similar discussion for the maintainability index in the visitor
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If we get a low maintainability index, we could refactor to reduce this number.
    A metric is useful in that it gives us a nail to hang our hat on; that is, we
    can take that measure and perform some action to improve it. Metrics move us beyond
    the pure aesthetics or subjectivity of the individual. A metric is real, actionable
    data. But Copilot has (at least) one more trick up its proverbial sleeve. Copilot
    is capable of doing more than just writing and assessing our code, it can also
    address the code’s flaws. Let’s bug hunt.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Hunting for bugs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will use an elementary (albeit quite contrived) example
    to demonstrate how we can use Copilot to find and fix the issues in our code.
    This code is supposed to loop over the list of integers and calculate the sum.
    However, there is a “blink and you’ll miss it” bug. The sum is assigned the value
    of i, rather than adding the value of i to the running total.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Simple loop over a list of integers and calculate the sum
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To debug this issue, we will introduce a new tool: Copilot Labs. Prior to Copilot
    Chat, Copilot Labs was the only means by which certain features were available
    in our IDE, VS Code specifically. For example, we would need to use Copilot Labs
    to find and fix bugs. The main advantage that Copilot Labs still has today, is
    that it can access the highlighted contents of your editor pane. This feature
    allows Copilot Labs to operate directly on the editable code in your IDE. Once
    you install the extension into your IDE, you should see a Copilot Labs toolkit
    on the left side of your IDE. Should you need a reminder on how to install an
    extension into your IDE, please consult appendices A to C for instructions on
    installing one.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 The Copilot Labs toolkit menu, which includes options for finding
    and fixing bugs. The toolkit also provides facilities to enhance your code as
    well as document it.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a computer Description automatically generated with low confidence](images/06_img_0001.png)'
  prefs: []
  type: TYPE_IMG
- en: We shall temporarily change the contents of the main.py file to the code listed
    in Listing 6.9\. Once you have made this change, highlight the code and depress
    the Fix Bug button within the Copilot Labs toolkit. You should see output like
    that in Figure 6.2\. Copilot Labs was able to determine the issue within this
    code and provided a suggestion as to how to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 Copilot Labs, using the GPT model, has identified the bug and how
    to address this bug
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a computer Description automatically generated with low confidence](images/06_img_0002.png)'
  prefs: []
  type: TYPE_IMG
- en: Alternatively, we could have copied this code into ChatGPT and asked it to find
    the bug. However, it is arguable that this is less convenient as you would have
    to know there was a bug in your code before asking ChatGPT to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Covering Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Code coverage* is a measure of how much of your code is being exercised by
    your tests. It is typically expressed as a percentage and represents the proportion
    of your code that is executed by your tests.'
  prefs: []
  type: TYPE_NORMAL
- en: Code coverage can be used as a metric to evaluate the effectiveness of your
    tests. If your code coverage is low, it may indicate that parts of your code are
    not being tested, which could lead to uncaught bugs and other issues. Alternatively,
    with high code coverage, you should rest assured that your code is well-tested.
    This does not guarantee that your code is bug-free, but it should indicate a high
    degree of confidence that if there are bugs that should be caught in a test.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the code coverage in our Python project, we will use the code coverage
    tool provided in the `coverage` library coverage. The `coverage` library works
    by instrumenting our code to collect coverage data as it runs. It can collect
    coverage data for any Python code, including tests, scripts, and modules. By using
    a code coverage tool like coverage, we can better understand how much of our code
    is being exercised by our tests and identify areas of our code that may need more
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s install coverage using pip: `pip install coverage.` Next, let’s
    run our tests with coverage: `coverage run -m pytest.` This will run your tests
    and collect coverage data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will generate a coverage report. The coverage report will show the
    code coverage for each file in our project. We create a text-based coverage report
    using this command: `coverage report` or generate an HTML version of the report
    using the command: `coverage html`. The HTML version of the report would be in
    the htmlcov directory. Figure 6.3 shows the coverage report.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 The code coverage report showing the coverage for each file in our
    Information Technology Asset Management system project
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a computer Description automatically generated with medium
    confidence](images/06_img_0003.png)'
  prefs: []
  type: TYPE_IMG
- en: Code coverage of 70% is a good start. In the real world, we would continue working
    with our team and generative AI pals to bring this measure up into the high 90s.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will transition to a new topic: using generative AI to describe a code listing
    to us.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Transliterating code – from code to descriptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often one is handed existing source code. Determining the exact entry point
    of this code, the purpose of the code, and the overall structure of a brownfield
    project can be challenging. Thankfully, this is one of the areas in which generative
    AIs truly excel: translating code into textual descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, we shall copy the (buggy) code from the previous section into
    the Copilot Chat dialog box, prefixed with the prompt, “**What does this code
    do?**”
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 The buggy code from the last section with a prompt asking Copilot
    Chat to explain this code to us
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a computer Description automatically generated with medium
    confidence](images/06_img_0004.png)'
  prefs: []
  type: TYPE_IMG
- en: What is striking about this explanation is that Copilot Chat detected that there
    is a bug in the code and offered a suggestion as to how to fix this bug.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 Copilot Chat explains the code’s purpose, identifies the bug, and
    suggests a fix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone Description automatically generated with low
    confidence](images/06_img_0005.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will ask it something more challenging. We will ask it to explain the
    behavior test we set up earlier in the chapter. Copy the code into the Copilot
    Chat window and ask it what this class does. You will likely receive a response
    similar to the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Copilot Chat explains the behavior test that we wrote previously
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 6.6 Translating from one language to another
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As remote as this may seem, you may find yourself looking through a code base
    written in a programming language with only a passing familiarity. Alternatively,
    you might find yourself pining for a specific function from another language to
    be available in your current language. For example, what would the `Asset` and
    `FundingDetails` classes look like rewritten in JavaScript? Well, let’s ask Copilot
    Chat.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.13 Copilot Chat translates our `Asset` and `FundingDetails` classes
    to JavaScript
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We might assume that translating a class or two is too easy. Well, we will attempt
    something more complex. **What would the** `AssetManager` **behavior test look
    like if written in JavaScript?**
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.14 Copilot Chat translates our `AssetManager` behavior test into JavaScript
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, you might wish to have GPT rewrite the code in idiomatic JavaScript.
    You need only say: “Write `AssetManager` in idiomatic JavaScript.” Copilot Chat
    will inform you that it will define the `AssetManager` class using the `class`
    syntax introduced in ES6.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.15 The `AssetManager` classe rewritten in idiomatic JavaScript
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s an example of what the `AssetManager` behavior test might look like
    in JavaScript using the Jest testing framework (which Copilot Chat included without
    our suggestion):'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.16 Copilot Chat rewrites our `AssetManager` test in idiomatic JavaScript
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use the Jest testing framework to define a set of behavior
    tests for the `AssetManager` class. We are using the `describe` library to determine
    test suites and individual tests.
  prefs: []
  type: TYPE_NORMAL
- en: In each test, we create an instance of the `AssetManager` class and perform
    some operation on it, such as adding or removing an asset. We are then using the
    expect function to make assertions about the behavior of the `AssetManager` class,
    such as checking that an asset was added to the asset list or that an asset was
    removed from the asset list.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the syntax for defining classes and constructors is slightly different
    in JavaScript compared to Python, and we are using the required function to import
    the AssetManager and Asset classes from their respective modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, for the big finale: We will start a new ChatGPT 4.0 session and paste
    in our Mermaid diagram from Chapter 3\. Lead with the prompt, “**Here is a mermaid
    diagram of my application. Please implement in Golang.**” Marvel as it returns
    the code, rendered in Golang.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.17 Copilot Chat rewrites our `Asset` class in idiomatic Golang
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, tell it to switch to Java.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.18 Copilot Chat rewrites our `Asset` class in idiomatic Java
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 6.7 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unit testing: Focuses on testing individual components or units of code to
    identify bugs and issues within specific units. Unit tests will be the most numerous
    in your codebase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Integration testing: Tests the interaction between different components or
    modules of the software to ensure seamless integration and detect communication
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behavior testing: Tests a system''s functionality from an end user''s perspective,
    ensuring it meets requirements and specifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mock objects: Simulate the behavior of natural objects in a controlled way,
    useful for testing and simulating error conditions. Mock objects are especially
    good at mimicking parts of the system which are needed for the test to run, but
    outside the scope of the test. For example, if your class had a constructor argument
    for a database, but you do not want to test the database directly, since the data
    may change, causing your test to be inconclusive, non-repeatable, or non-deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cyclomatic Complexity: Measures the number of independent paths through a software
    module, indicating complexity and potential for bugs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Halstead complexity measures: Assess software complexity based on unique operators
    and operands, providing insights into code size and cognitive complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maintainability Index: Combines factors like cyclomatic complexity, lines of
    code, and Halstead measures to evaluate software maintainability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code coverage: Metric for evaluating test effectiveness, indicating the extent
    to which code is tested and the potential for uncaught bugs. Generally, higher
    is better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language familiarity: Needing to navigate code in an unfamiliar programming
    language or desiring features from another language in the current one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
