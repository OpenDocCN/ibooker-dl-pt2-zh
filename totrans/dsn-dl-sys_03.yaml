- en: 3 Model training service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 模型训练服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章涵盖了
- en: Designing principles for building a training service
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立训练服务的设计原则
- en: Explaining the deep learning training code pattern
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释深度学习训练代码模式
- en: Touring a sample training service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参观一个示例训练服务
- en: Using an open source training service, such as Kubeflow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开源训练服务，如 Kubeflow
- en: Deciding when to use a public cloud training service
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定何时使用公共云训练服务
- en: The task of model training in machine learning is not the exclusive responsibility
    of researchers and data scientists. Yes, their work on training the algorithms
    is crucial because they define the model architecture and the training plan. But
    just like physicists need a software system to control the electron-positron collider
    to test their particle theories, data scientists need an effective software system
    to manage the expensive computation resources, such as GPU, CPU, and memory, to
    execute the training code. This system of managing compute resources and executing
    training code is known as the *model training service*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的模型训练任务不是研究人员和数据科学家的专属责任。是的，他们对算法的训练工作至关重要，因为他们定义了模型架构和训练计划。但就像物理学家需要一个软件系统来控制电子-正电子对撞机来测试他们的粒子理论一样，数据科学家需要一个有效的软件系统来管理昂贵的计算资源，如
    GPU、CPU 和内存，以执行训练代码。这个管理计算资源和执行训练代码的系统被称为*模型训练服务*。
- en: Building a high-quality model depends not only on the training algorithm but
    also on the compute resources and the system that executes the training. A good
    training service can make model training much faster and more reliable and can
    also reduce the average model-building cost. When the dataset or model architecture
    is massive, using a training service to manage the distributed computation is
    your only option.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 构建高质量的模型不仅取决于训练算法，还取决于计算资源和执行训练的系统。一个好的训练服务可以使模型训练速度更快、更可靠，同时还可以降低平均模型构建成本。当数据集或模型架构非常庞大时，使用训练服务来管理分布式计算是你唯一的选择。
- en: In this chapter, we first examine the training service’s value proposition and
    design principles, and then we meet our sample training service. This sample service
    not only shows you how to apply the design principles in practice but also teaches
    you how the training service interacts with arbitrary training code. Next, we
    introduce several open source training applications that you can use to set up
    your own training service quickly. We end with a discussion on when to use a public
    cloud training system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们首先考察了训练服务的价值主张和设计原则，然后我们遇到了我们的示例训练服务。这个示例服务不仅向你展示了如何将设计原则应用于实践，还教你训练服务如何与任意训练代码交互。接下来，我们介绍了几个开源训练应用程序，你可以用它们快速建立自己的训练服务。最后，我们讨论了何时使用公共云训练系统。
- en: This chapter focuses on designing and building effective training services *from
    a software engineer’s perspective, not a data scientist’s*. So we don’t expect
    you to be familiar with any deep learning theories or frameworks. Section 3.2,
    on the deep learning algorithm code pattern, is all the preparation you need to
    understand the training code in this chapter. The training code is not our main
    focus here; we wrote it only for demonstration purposes, so we have something
    on which to demonstrate the sample training service.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论了从软件工程师的角度而不是数据科学家的角度设计和构建有效的训练服务。因此，我们不希望你熟悉任何深度学习理论或框架。第3.2节关于深度学习算法代码模式，是你理解本章训练代码所需的全部准备工作。训练代码并不是我们在这里的主要关注点；我们只是为了演示目的而编写了它，所以我们有东西可以演示示例训练服务。
- en: 'Model training topics often intimidate engineers. One common misunderstanding
    is that model training is all about training algorithms and research. By reading
    this chapter, I hope you will not only learn how to design and build training
    services but also absorb this message: the success of model training is built
    on two pillars, algorithms and system engineering. The model training activities
    in an organization cannot scale without a good training system. So we, as software
    engineers, have a lot to contribute to this field.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的话题经常让工程师感到害怕。一个常见的误解是，模型训练全部都是关于训练算法和研究的。通过阅读这一章，我希望你不仅能学会如何设计和构建训练服务，还能吸收到这样一条信息：模型训练的成功建立在两个支柱上，即算法和系统工程。组织中的模型训练活动如果没有良好的训练系统，就无法扩展。因此，作为软件工程师，我们有很多可以为这个领域做出的贡献。
- en: '3.1 Model training service: Design overview'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 模型训练服务：设计概述
- en: 'In an enterprise environment, there are two roles involved in deep learning
    model training: data scientists, who develop model training algorithms (in TensorFlow,
    PyTorch, or other frameworks), and platform engineers, who build and maintain
    the system that runs the model training code in remote and shared server farms.
    We call this system the model training service.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中，深度学习模型训练涉及两种角色：开发模型训练算法的数据科学家（使用TensorFlow、PyTorch或其他框架），以及构建和维护在远程和共享服务器群中运行模型训练代码的系统的平台工程师。我们称这个系统为模型训练服务。
- en: A model training service works as a training infrastructure to execute the model
    training code (algorithm) in a dedicated environment; it handles both training
    job scheduling and compute resource management. Figure 3.1 shows a high-level
    workflow in which the model training service runs a model training code to produce
    a model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练服务作为一个训练基础设施，在专用环境中执行模型训练代码（算法）；它处理训练作业调度和计算资源管理。图3.1展示了一个高级工作流程，其中模型训练服务运行模型训练代码以生成一个模型。
- en: '![](../Images/03-01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-01.png)'
- en: Figure 3.1 A high-level workflow for executing model training via a training
    service. In step 1, the data scientist submits a training request with training
    code to the training service, which creates a job in the job queue. In step 2,
    the model training service allocates compute resources to execute the training
    job (training code). In step 3, the job produces a model when the training execution
    completes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 通过训练服务执行模型训练的高级工作流程。在步骤1中，数据科学家向训练服务提交带有训练代码的训练请求，该服务将在作业队列中创建一个作业。在步骤2中，模型训练服务分配计算资源来执行训练作业（训练代码）。在步骤3中，当训练执行完成时，作业产生一个模型。
- en: The most common question asked about this component is why we would need to
    write a service to do model training. For many people, it seems much easier to
    write a simple bash script to execute the training code (algorithm) locally or
    remotely, such as with an Amazon Elastic Cloud Computing (Amazon EC2) instance.
    The rationale behind building a training service, however, goes beyond just launching
    a training computation. We will discuss it in detail in the next section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个组件最常见的问题是为什么我们需要编写一个服务来进行模型训练。对许多人来说，似乎更容易编写一个简单的bash脚本在本地或远程执行训练代码（算法），比如在Amazon弹性云计算（Amazon
    EC2）实例上。然而，构建训练服务的理由不仅仅是启动训练计算。我们将在下一节详细讨论它。
- en: 3.1.1 Why use a service for model training?
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 为什么使用模型训练的服务？
- en: 'Imagine you lead a data science team, and you need to assign the team’s precious
    compute resources wisely to the team members Alex, Bob, and Kevin. The computing
    resource needs to be allocated in a way that all team members can complete their
    model training tasks within a time limit and a budget. Figure 3.2 paints two approaches
    for allocating the compute resources: dedicated and shared.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你领导一个数据科学团队，你需要明智地为团队成员Alex、Bob和Kevin分配团队宝贵的计算资源。计算资源需要以一种所有团队成员都能在时间限制和预算内完成他们的模型训练任务的方式分配。图3.2展示了分配计算资源的两种方法：专用和共享。
- en: '![](../Images/03-02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-02.png)'
- en: 'Figure 3.2 Different compute resource allocation strategies: dedicated vs.
    shared'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 不同的计算资源分配策略：专用 vs. 共享
- en: The first option, dedicated, is to exclusively assign a powerful workstation
    to each member of the team. This is the simplest approach but clearly not an economic
    one because when Alex is not running his training code, his server sits idle and
    neither Bob nor Kevin can use it. So, in this approach, our budget is underutilized.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项，专用，是为每个团队成员独家分配一台强大的工作站。这是最简单的方法，但显然不是经济的，因为当Alex不运行他的训练代码时，他的服务器处于空闲状态，Bob和Kevin都无法使用它。因此，在这种方法中，我们的预算被低效利用。
- en: Another problem with the dedicated approach is that it cannot scale. When Alex
    wants to train a large model or a model with a large dataset, he will need multiple
    machines. And training machines are normally expensive; because of the complexity
    of the deep learning model architecture, even a decent size neural network requires
    a GPU with large memory. In this case, we must assign more dedicated servers to
    Alex, which exacerbates the inefficient resource allocation problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 专用方法的另一个问题是它无法扩展。当Alex需要训练一个大模型或者一个数据集庞大的模型时，他将需要多台机器。而且训练机器通常很昂贵；由于深度学习模型架构的复杂性，即使是一个体量适中的神经网络也需要具有较大内存的GPU。在这种情况下，我们必须为Alex分配更多专用服务器，这加剧了资源分配效率低下的问题。
- en: The second option, shared compute resources, is to build an elastic server group
    (the size of the group is adjustable) and share it with all members. This approach
    is obviously more economical because we use fewer servers to achieve the same
    result, which maximizes our resource utilization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项，共享计算资源，是建立一个弹性服务器组（组大小可调整）并与所有成员共享。这种方法显然更经济，因为我们使用更少的服务器来实现相同的结果，从而最大化了我们的资源利用率。
- en: It’s not a hard decision to choose a sharing strategy because it greatly reduces
    the cost of our training cluster. But the sharing approach requires proper management,
    such as queuing user requests if there is a sudden burst of training requests,
    babysitting each training execution and intervening (restarting or aborting) when
    necessary (training progress is stuck), and scaling up or scaling down our cluster
    according to the real-time system usage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 选择共享策略并不是一个困难的决定，因为它大大降低了我们训练集群的成本。但是共享方法需要适当的管理，例如如果突然出现大量的训练请求，则排队用户请求，监控每个训练执行并在必要时进行干预（重新启动或中止）（训练进度停滞），并根据实时系统使用情况扩展或缩减我们的集群。
- en: Script vs. service
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本与服务
- en: Now let’s revisit the previous script versus service discussion. In a model
    training context, *training script* refers to using shell scripts to orchestrate
    different training activities in a shared server cluster. A training service is
    a remote process that communicates over the network using HTTP (hypertext transfer
    protocol) or gRPC (gRPC Remote Procedure Call). As data scientists, Alex and Bob
    send training requests to the service, and the service orchestrates these requests
    and manages the training executions on the shared servers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们重新审视之前关于脚本与服务的讨论。在模型训练的背景下，*训练脚本* 是指使用 shell 脚本在共享服务器集群中编排不同的训练活动。训练服务是一个远程过程，它通过
    HTTP（超文本传输协议）或 gRPC（gRPC 远程过程调用）进行网络通信。作为数据科学家，Alex 和 Bob 向服务发送训练请求，而服务则编排这些请求并管理共享服务器上的训练执行。
- en: The script approach may work for a single-person scenario but will prove difficult
    in a shared-resource environment. Besides executing training code, we need to
    take care of other important elements, such as setting up the environment, ensuring
    data compliance, and troubleshooting model performance. For example, environment
    setup requires that the library dependencies of the training framework and training
    code are installed properly on the training server before starting model training.
    Data compliance requires that the sensitive training data (user credit card numbers,
    payment records) is protected with restricted access. And performance troubleshooting
    requires that everything used in training, including dataset IDs and versions,
    training code versions, and hyperparameters, is tracked for model reproduction
    purposes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用脚本方法可能适用于单人场景，但在共享资源环境中会变得困难。除了执行训练代码之外，我们还需要关注其他重要因素，比如设置环境、确保数据合规性以及排除模型性能问题。例如，环境设置要求在开始模型训练之前，在训练服务器上正确安装训练框架和训练代码的库依赖项。数据合规性要求对敏感的训练数据（用户信用卡号、支付记录）进行受限访问的保护。性能故障排除要求对训练中使用的所有内容进行跟踪，包括数据集
    ID 和版本、训练代码版本以及超参数，以便进行模型再现。
- en: It’s hard to imagine addressing these requirements in shell scripts and having
    the model training executed in a reliable, repeatable, and scalable fashion. This
    is why most models trained in production nowadays are produced by thoughtfully
    designed model training services.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 很难想象用 shell 脚本解决这些要求，并且以可靠、可重复和可扩展的方式执行模型训练。这就是为什么如今大多数在生产中训练的模型都是通过深思熟虑设计的模型训练服务生成的原因。
- en: Benefits of having a model training service
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练服务的好处
- en: 'From the previous discussion, we can imagine a model training service’s value
    proposition as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的讨论中，我们可以想象一个模型训练服务的价值主张如下：
- en: Saturates computing resources and reduces model training costs
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 饱和计算资源并降低模型训练成本
- en: Expedites model development by building models in a fast (more resources available)
    and reliable way
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以快速（可用资源更多）和可靠的方式构建模型来加快模型开发
- en: Enforces data compliance by executing training in a confined environment
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在受限环境中执行训练来强制执行数据合规性
- en: Facilitates model performance troubleshooting
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 促进模型性能故障排除
- en: 3.1.2 Training service design principles
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 模型训练服务设计原则
- en: Before we look at our sample training service, let’s look at the four design
    principles we can use to evaluate a model training system.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看我们的示例训练服务之前，让我们看一下可以用来评估模型训练系统的四个设计原则。
- en: 'Principle 1: Provides a unified API and is agnostic about actual training code'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 原则1：提供统一的 API，不关心实际的训练代码
- en: Having only one public API to train models with different kinds of training
    algorithms makes the training service easy to use. Whether it’s object detection
    training, voice recognition training, or text-intent classification training,
    we can use sample APIs to trigger the model training execution. Future algorithm
    performance A/B tests can also be easily implemented by having a single training
    API.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个公共 API 来训练不同种类的训练算法使得训练服务易于使用。无论是目标检测训练、语音识别训练还是文本意图分类训练，我们都可以使用示例 API 触发模型训练执行。未来算法性能的
    A/B 测试也可以通过一个单一的训练 API 轻松实现。
- en: Training code-agnostic means that the training service defines a clear mechanism
    or protocol for how it executes a training algorithm (code). It establishes, for
    instance, how the service passes in variables to the training code/process, how
    the training code obtains the training dataset, and where the trained model and
    metrics are uploaded. As long as training code follows this protocol, it doesn’t
    matter how it’s implemented, what its model architecture is, or which training
    libraries it uses.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码不易装配意味着训练服务定义了一种执行训练算法（代码）的清晰机制或协议。例如，它确定了服务如何将变量传递给训练代码/进程，训练代码如何获取训练数据集，以及训练后的模型和指标上传到何处。只要训练代码遵循这个协议，无论它是如何实现的、其模型架构是什么或使用哪些训练库，都不会有任何问题。
- en: 'Principle 2: Builds a model with high performance and low costs'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原则2：构建具有高性能和低成本的模型
- en: A good training service should set cost-effectiveness as a priority. Cost-effectiveness
    can provide methods to shorten the model training execution time and improve the
    utilization rate of the compute resources. For instance, a modern training service
    can reduce time and hardware costs by supporting various distributed training
    methods, offering good job-schedule management to saturate the server farm, and
    alerting users when a training process goes off the original plan so it can be
    terminated early.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的训练服务应该将成本效益作为优先考虑。成本效益可以提供缩短模型训练执行时间和提高计算资源利用率的方法。例如，一种现代的训练服务可以通过支持各种分布式训练方法、提供良好的作业调度管理来饱和服务器群，以及在训练过程偏离原始计划时向用户发出警报，从而降低时间和硬件成本。
- en: 'Principle 3: Supports model reproducibility'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 原则3：支持模型可重现性
- en: A service should produce the same model if given the same inputs. This is not
    only important for debugging and performance troubleshooting but also builds trustworthiness
    in the system. Remember, we will build business logic based on the model prediction
    result. We might, for instance, employ a classification model to predict a user’s
    credibility and then make loan-approval decisions based on it. We can’t trust
    the entire loan-approval application unless we can repeatedly produce models of
    the same quality.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个服务如果给出相同的输入应该会产生相同的模型。这不仅对调试和性能故障排除很重要，而且还建立了系统的可信度。记住，我们将根据模型预测结果构建业务逻辑。例如，我们可以使用分类模型来预测用户的信用并根据此作出贷款批准决策。除非我们能够反复产生相同质量的模型，否则就无法信任整个贷款批准申请。
- en: 'Principle 4: Supports robust, isolated, and elastic compute management'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原则4：支持鲁棒、隔离和弹性计算管理
- en: Modern deep learning models, such as language understanding models, take a long
    time to train (more than a week). If the training process is interrupted or gets
    aborted in the middle for some random OS failures, all the time and computation
    expenses are wasted. A matured training service should handle the training job
    robustness (failover, failure recovery), resource isolation, and elastic resource
    management (ability to adjust the number of resources), so it can make sure its
    training job execution will complete successfully in a variety of situations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习模型，如语言理解模型，需要很长时间的训练（超过一周）。如果训练过程在中途被中断或因某些随机操作系统故障而中止，所有的时间和计算费用都会白白浪费。一个成熟的训练服务应该处理训练工作的鲁棒性（故障转移、故障恢复）、资源隔离和弹性资源管理（能够调整资源数量），以确保其训练作业可以在各种情况下成功完成执行。
- en: After discussing all the important abstract concepts, let’s tackle how to design
    and build a model training service. In the next two sections, we will learn a
    general code pattern for deep learning code and an example of a model training
    service.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了所有重要的抽象概念之后，让我们来解决如何设计和构建模型训练服务。在接下来的两节中，我们将学习深度学习代码的一般代码模式以及模型训练服务的示例。
- en: 3.2 Deep learning training code pattern
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 深度学习训练代码模式
- en: Deep learning algorithms can be complicated and often intimidating for engineers.
    Fortunately, as software engineers designing the platform for deep learning systems,
    we don’t need to master these algorithms for our daily work. We do, however, need
    to be familiar with the general code pattern of these algorithms. With a high-level
    understanding of the model training code pattern, we can comfortably treat model
    training code as a black box. In this section, we’ll introduce you to the general
    pattern.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法可能对工程师来说复杂且常常令人望而生畏。幸运的是，作为设计深度学习系统平台的软件工程师，我们不需要掌握这些算法来进行日常工作。但是，我们需要熟悉这些算法的一般代码模式。通过对模型训练代码模式的高层次理解，我们可以将模型训练代码视为黑盒子。在本节中，我们将向您介绍一般模式。
- en: 3.2.1 Model training workflow
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 模型训练工作流程
- en: In a nutshell, most deep learning models are trained through an iterative learning
    process. The process repeats the same set of computation steps in many iterations,
    and in every iteration, it tries to update the weights and biases of the neural
    network to get the algorithm output (prediction result) closer to the training
    targets in the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，大多数深度学习模型通过迭代学习过程进行训练。该过程在许多迭代中重复相同的计算步骤，并且在每次迭代中，它试图更新神经网络的权重和偏差，以使算法输出（预测结果）更接近数据集中的训练目标。
- en: To measure how well a neural network models the given data and uses it to update
    the weights of the neural network to get better results, a loss function is defined
    to calculate the deviations of the algorithm output from the actual results. The
    output of the loss function is named LOSS.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量神经网络模拟给定数据的能力并使用它来更新神经网络的权重以获得更好的结果，定义了一个损失函数来计算算法输出与实际结果之间的偏差。损失函数的输出称为
    LOSS。
- en: So, you can see the entire iterative training process as a repeating effort
    to reduce the loss value. Eventually, when the loss value meets our training goal
    or it can’t be reduced any further, then the training completes. The training
    output is the neural network and its weights, but we generally refer to it simply
    as a model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以将整个迭代训练过程视为不断努力减少损失值。最终，当损失值达到我们的训练目标或无法进一步减少时，训练完成。训练输出是神经网络及其权重，但我们通常简称为模型。
- en: '![](../Images/03-03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-03.png)'
- en: Figure 3.3 General steps of a deep learning model training workflow
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 深度学习模型训练工作流程的一般步骤
- en: 'Figure 3.3 illustrates the general model training steps. Because neural networks
    cannot load the entire dataset at once due to memory limitations, we usually regroup
    the dataset into small batches (mini-batches) before training begins. In step
    1, the mini-batch examples are fed to the neural network, and the network calculates
    the prediction result for each example. In step 2, we pass in the predicted results
    and the expected value (training labels) to the loss function to compute the loss
    value, which indicates the deviation between the current learning and the target
    data pattern. In step 3, a process called backpropagation calculates gradients
    for each of the neural network’s parameters with the loss value. These gradients
    are used to update the model parameters, so the model can get a better prediction
    accuracy in the next training loop. In step 4, The neural network’s parameters
    (weights and biases) are updated by a selected optimization algorithm, such as
    stochastic gradient descent and its variants. The gradients (from step 3) and
    learning rate are the input parameters for the optimization algorithm. The model
    accuracy is supposed to improve after this model update step. Finally, in step
    5, training completes and the network and its parameters are saved as the final
    model file. The training is completed under either of the two following conditions:
    finishing the expected training runs or reaching the expected model accuracy.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3说明了一般的模型训练步骤。由于神经网络由于内存限制无法一次性加载整个数据集，因此我们通常在训练开始之前将数据集重新分组成小批量（mini-batches）。在步骤1中，将小批量示例馈送到神经网络中，并且网络计算每个示例的预测结果。在步骤2中，我们将预测结果和期望值（训练标签）传递给损失函数以计算损失值，该损失值表示当前学习与目标数据模式之间的偏差。在步骤3中，一个称为反向传播的过程计算出每个神经网络参数的梯度与损失值。这些梯度用于更新模型参数，以便模型可以在下一个训练循环中获得更好的预测准确性。在步骤4中，选择的优化算法（如随机梯度下降及其变种）更新神经网络的参数（权重和偏差）。梯度（来自步骤3）和学习率是优化算法的输入参数。模型更新步骤后，模型准确性应该会提高。最后，在步骤5中，训练完成，网络及其参数保存为最终模型文件。训练在以下两种情况下完成：完成预期的训练运行或达到预期的模型准确度。
- en: Although there are different types of model architectures, including recurrent
    neural networks (RNNs), convolutional neural networks (CNNs), and autoencoders,
    their model training processes all follow this same pattern; only the model network
    differs. Also, abstracting model training code to the previously repeated general
    steps is the foundation for running distributed training. This is because, no
    matter how the model architecture is different, we can train them in a common
    training strategy. We will discuss distributed training in detail in the next
    chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有不同类型的模型架构，包括循环神经网络（RNNs）、卷积神经网络（CNNs）和自编码器，但它们的模型训练过程都遵循相同的模式；只有模型网络不同。此外，将模型训练代码抽象为先前重复的一般步骤是进行分布式训练的基础。这是因为，无论模型架构如何不同，我们都可以使用共同的训练策略对它们进行训练。我们将在下一章详细讨论分布式训练。
- en: 3.2.2 Dockerize model training code as a black box
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2：将模型训练代码Docker化为黑匣子
- en: With the previously discussed training pattern in mind, we can view deep learning
    training code as a black box. No matter what model architecture and training algorithm
    a training code implements, we can execute it the same way in a training service.
    To run the training code anywhere in the training cluster and create isolation
    for each training execution, we can pack the training code and its dependent libraries
    into a Docker image and run it as a container (figure 3.4).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前讨论的训练模式的基础上，我们可以将深度学习训练代码视为一个黑匣子。无论训练代码实现了什么样的模型架构和训练算法，我们都可以在训练服务中以相同的方式执行它。为了在训练集群中的任何位置运行训练代码并为每个训练执行创建隔离，我们可以将训练代码及其依赖库打包到一个Docker镜像中，并将其作为容器运行（见图3.4）。
- en: '![](../Images/03-04.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-04.png)'
- en: Figure 3.4 A training service launches a Docker container to perform model training
    instead of running the training code directly as a process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：一个训练服务启动一个Docker容器来执行模型训练，而不是直接运行训练代码作为一个进程。
- en: In figure 3.4, by Dockerizing the training code, the training service can execute
    a model training by simply launching a Docker container. Because the service is
    agnostic about what’s inside the container, the training service can execute all
    different code in this standard method. This is much simpler than letting the
    training service spawn a process to execute model training because the training
    service needs to set up the various environments and dependent packages for each
    training code. Another benefit of Dockerization is it decouples the training service
    and training code, which enables data scientists and platform engineers to focus
    on model algorithm development and training execution performance, respectively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 3.4 中，通过将训练代码 Docker 化，训练服务可以通过简单地启动一个 Docker 容器来执行模型训练。因为服务对容器内部的内容无所知，所以训练服务可以以这种标准方法执行所有不同的代码。这比让训练服务生成一个进程来执行模型训练要简单得多，因为训练服务需要为每个训练代码设置各种环境和依赖包。Docker
    化的另一个好处是它将训练服务和训练代码解耦，这使得数据科学家和平台工程师可以分别专注于模型算法开发和训练执行性能。
- en: You may wonder how a training service communicates with training code if they
    are agnostic to each other. The key is to define a communication protocol; this
    protocol delineates which parameters and their data format a training service
    passes to training code. These parameters include dataset, hyperparameters, model
    saving location, metrics saving location, and more. We will see a concrete example
    in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对训练服务如何与训练代码通信而对彼此不知情感到奇怪，那么关键在于定义通信协议；该协议界定了训练服务传递给训练代码的参数及其数据格式。这些参数包括数据集、超参数、模型保存位置、指标保存位置等等。我们将在下一节看到一个具体的例子。
- en: 3.3 A sample model training service
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 一个样本模型训练服务
- en: As we now know, most deep learning training codes follow the same pattern (figure
    3.3), and they can be Dockerized and executed in a unified fashion (figure 3.4).
    Let’s take a closer look at a concrete example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如今我们知道，大多数深度学习训练代码遵循相同的模式（图 3.3），它们可以以统一的方式进行 Docker 化和执行（图 3.4）。让我们仔细看一个具体的例子。
- en: To demonstrate the concept and design principles we introduced so far, we built
    a sample service that implements the basic production scenarios of model training—receiving
    a training request, launching a training execution in a Docker container, and
    tracking its execution progress. Although the scenarios are quite simple—a few
    hundred lines of code—they demonstrate the key concepts we discussed in previous
    sections, including using a unified API, Dockerized training code, and communication
    protocol between the training service and training container.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示我们迄今介绍的概念和设计原则，我们构建了一个示例服务，实现了模型训练的基本生产场景——接收训练请求，在 Docker 容器中启动训练执行，并跟踪其执行进度。虽然这些场景相当简单——几百行代码——但它们展示了我们在前几节中讨论的关键概念，包括使用统一的
    API、Docker 化的训练代码以及训练服务和训练容器之间的通信协议。
- en: 'Note To show the key pieces clearly, the service is built in a slim fashion.
    Model training metadata (such as running jobs and waiting jobs) is tracked in
    memory instead of a database, and the training jobs are executed in the local
    Docker engine directly. By removing lots of intermediate layers, you will have
    a straight view of two key areas: training job management and the communication
    between the training service and training code (Docker container).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰地展示关键部分，该服务以精简的方式构建。模型训练元数据（如运行任务和等待任务）被跟踪在内存中而不是数据库中，并且训练任务直接在本地 Docker
    引擎中执行。通过删除许多中间层，您将直接查看到两个关键区域：训练任务管理和训练服务与训练代码（Docker 容器）之间的通信。
- en: 3.3.1 Play with the service
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 与服务交互
- en: Before we look at service design and implementation, let’s see how we can play
    with it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看服务设计和实现之前，让我们看看我们如何操作它。
- en: Note Please follow the GitHub instructions to run this lab. We highlight only
    the major steps and key commands for how to run the sample service to avoid the
    lengthy pages of code and execution outputs, so the concept can be demonstrated
    clearly. To run this lab, follow the instructions in the “single trainer demo”
    doc (`training-service/single_trainer_demo.md`) in the orca3/MiniAutoML Git repository,
    which also captures the desired outputs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照 GitHub 说明运行此实验。我们仅强调了运行示例服务的主要步骤和关键命令，以避免冗长的代码页面和执行输出，以便清晰地演示概念。要运行此实验，请按照
    orca3/MiniAutoML Git 存储库中的“单个训练器演示”文档 (`training-service/single_trainer_demo.md`)
    中的说明操作，该文档还包含所需的输出。
- en: 'First, we start the service with `scripts/ts-001-start-server.sh`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `scripts/ts-001-start-server.sh` 启动服务：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After launching the training service Docker container, we can send a gRPC request
    to kick off a model training execution (`scripts/ts-002-start-run.sh` `<dataset`
    `id>`). See a sample gRPC request as follows.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动训练服务 Docker 容器后，我们可以发送一个 gRPC 请求来启动模型训练执行（`scripts/ts-002-start-run.sh` `<dataset`
    `id>`）。请参见以下示例 gRPC 请求。
- en: 'Listing 3.1 Calling training service API: Submitting a training job'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 调用训练服务 API：提交训练作业
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Training algorithm; also the name of the training Docker image
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练算法；也是训练 Docker 镜像的名称
- en: ❷ Version hash of the training dataset
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练数据集的版本哈希值。
- en: ❸ Training hyperparameters
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练超参数
- en: 'Once the job is submitted successfully, we can use the returned job ID from
    the `train` API to query the progress of the training execution (`scripts/ts-003-check-run.sh`
    `<job` `id>`); see the following example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦作业成功提交，我们就可以使用从 `train` API 返回的作业 ID 来查询训练执行的进度（`scripts/ts-003-check-run.sh`
    `<job` `id>`）；请参见以下示例：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Uses the job ID returned by the train API
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用由训练 API 返回的作业 ID。
- en: As you can see, by calling two gRPC APIs, we can kick off deep learning training
    and track its progress. Now, let’s look at the design and implementation of this
    sample training service.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，通过调用两个 gRPC API，我们可以启动深度学习训练并跟踪其进度。现在，让我们来看看这个示例训练服务的设计和实现。
- en: Note Check out appendix A if you encounter any problems. Scripts in section
    A.2 automate both dataset preparation and model training. If you want to see a
    working model training example, read the lab portion of that section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 如果您遇到任何问题，请查看附录 A。A.2 节的脚本自动化了数据集准备和模型训练。如果您想看到一个工作模型训练示例，请阅读该部分的实验部分。
- en: 3.3.2 Service design overview
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 服务设计概述
- en: Let’s use Alex (a data scientist) and Tang (a developer) to show how the service
    functions. To use the training service to train a model, Alex needs to write training
    code (for example, a neural network algorithm) and build the code into a Docker
    image. This Docker image needs to be published to an artifact repository, so the
    training service can pull the image and run it as a container. Inside the Docker
    container, the training code will be executed by a bash script.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 Alex（一位数据科学家）和 Tang（一位开发人员）来展示服务的功能。要使用训练服务来训练一个模型，Alex 需要编写训练代码（例如，一个神经网络算法）并将代码构建成一个
    Docker 镜像。这个 Docker 镜像需要发布到一个 artifact 存储库，以便训练服务可以拉取镜像并将其作为容器运行。在 Docker 容器内部，训练代码将由一个
    bash 脚本执行。
- en: To provide an example, we wrote a sample intent classification training code
    in PyTorch, built the code into a Docker image, and pushed it to the Docker hub
    ([https://hub.docker.com/u/orca3](https://hub.docker.com/u/orca3)). We will explain
    it again in section 3.3.6.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个示例，我们用 PyTorch 编写了一个样本意图分类训练代码，将代码构建成一个 Docker 镜像，并将其推送到 Docker hub（[https://hub.docker.com/u/orca3](https://hub.docker.com/u/orca3)）。我们将在第
    3.3.6 节再次解释它。
- en: 'note In real-world scenarios, the training Docker image creation, publication,
    and consumption are done automatically. A sample scenario could be as follows:
    step 1, Alex checks his training code into a Git repository; step 2, a preconfigured
    program—for example, a Jenkins pipeline—is triggered to build a Docker image from
    this repo; step 3, the pipeline also publishes the Docker image to a Docker image
    artifactory, for example, JFrog Artifactory; and step 4, Alex sends a training
    request, and then the training service pulls the training images from the artifactory
    and begins model training.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在实际场景中，训练 Docker 镜像的创建、发布和消费都是自动完成的。一个示例场景可能如下：第一步，Alex 将他的训练代码提交到 Git 存储库；第二步，一个预配置的程序——例如
    Jenkins 流水线——被触发以从这个存储库构建一个 Docker 镜像；第三步，流水线还将 Docker 镜像发布到 Docker 镜像工厂，例如 JFrog
    Artifactory；第四步，Alex 发送一个训练请求，然后训练服务从工厂拉取训练镜像并开始模型训练。
- en: 'When Alex finishes the training code development, he can start to use the service
    to run his training code. The entire workflow is as follows: step 1.a, Alex submits
    a training request to our sample training service. The request defines the training
    code—a Docker image and tag. When the training service receives a training request,
    it creates a job in the queue and returns the job ID to Alex for future job tracking;
    step 1.b, Alex can query the training service to get the training progress in
    realtime; step 2, the service launches a training job as a Docker container in
    the local Docker engine to execute the model training; and step 3, the training
    code in the Docker container uploads training metrics to the metadata store during
    training and the final model when training completes.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当Alex完成培训代码开发后，他可以开始使用服务运行他的培训代码。整个工作流程如下：步骤1.a，Alex向我们的样本培训服务提交培训请求。请求定义了培训代码——一个Docker镜像和标签。当培训服务收到培训请求时，它会在队列中创建一个作业，并将作业ID返回给Alex以供将来跟踪作业；步骤1.b，Alex可以查询培训服务以实时获取培训进度；步骤2，服务以Docker容器的形式在本地Docker引擎中启动一个训练作业来执行模型训练；步骤3，Docker容器中的培训代码在训练期间上传培训指标到元数据存储以及在培训完成时上传最终模型。
- en: Note Model evaluation is the step we did not mention in the aforementioned model
    training workflow. After the model is trained, Alex (data scientist) will look
    at the training metrics, reported by the training service, to validate the model's
    quality. To evaluate the model quality, Alex can check the prediction failure
    rate, gradients, and loss-value graphs. As model evaluation is usually a data
    scientist’s responsibility, we will not cover it in this book, but we will discuss
    in chapter 8 how model training metrics are collected and stored.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 模型评估是我们在前述模型训练工作流程中未提及的步骤。在模型训练完成后，Alex（数据科学家）将查看培训服务报告的培训指标，以验证模型的质量。为了评估模型质量，Alex可以检查预测失败率、梯度和损失值图。由于模型评估通常是数据科学家的责任，所以我们不会在本书中涉及此内容，但我们会在第8章中讨论模型训练指标是如何收集和存储的。
- en: The entire training workflow is self-serve; Alex can manage the model training
    entirely by himself. Tang develops the training service and maintains the system,
    but the system is agnostic about the training code developed by Alex. Tang’s focus
    is not the model’s accuracy but the availability and efficiency of the system.
    See figure 3.5 for the user workflow we just described.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 整个培训工作流程是自助式的；Alex可以完全自己管理模型训练。Tang开发了培训服务并维护系统，但系统对Alex开发的培训代码是不可知的。Tang的重点不是模型的准确性，而是系统的可用性和效率。请参见图3.5，了解我们刚刚描述的用户工作流程。
- en: '![](../Images/03-05.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-05.png)'
- en: 'Figure 3.5 A high-level service design and user workflow: user training requests
    are queued, and the Docker job tracker picks up jobs from the queue and launches
    Docker containers to run model training.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 高级服务设计和用户工作流程：用户培训请求被排队，Docker作业跟踪器从队列中提取作业，并启动Docker容器来运行模型训练。
- en: 'Having seen the user workflow, let’s look at two key components: memory store
    and Docker job tracker. The memory store uses the following four data structures
    (maps) to organize requests (jobs): job queue, job launch list, running list,
    and finalized list. Each of these maps represents jobs in a different running
    status. We implement the job tracking store in memory just for simplicity; ideally,
    we should use a database.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 看到了用户工作流程，让我们看看两个关键组件：内存存储和Docker作业跟踪器。内存存储使用以下四种数据结构（映射）来组织请求（作业）：作业队列、启动列表、运行列表和完成列表。这些映射中的每一个都代表了不同运行状态的作业。我们之所以在内存中实现作业跟踪存储，只是为了简单起见；理想情况下，我们应该使用数据库。
- en: The Docker job tracker handles the actual job execution in the Docker engine;
    it periodically monitors the job queue in the memory store. When there is capacity
    in the Docker engine, the tracker will launch a Docker container from the job
    queue and keep monitoring the container execution. In our example, we use the
    local Docker engine, so the service can run on your local. But it can be easily
    configured to a remote Docker engine as well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Docker作业跟踪器处理Docker引擎中的实际作业执行；它定期监视内存存储中的作业队列。当Docker引擎有空闲容量时，跟踪器将从作业队列中启动一个Docker容器，并继续监视容器的执行。在我们的示例中，我们使用本地Docker引擎，所以服务可以在您的本地运行。但它也可以很容易地配置到远程Docker引擎上。
- en: After launching a training container, based on the execution status, the Docker
    job tracker moves the job object from the job queue to other job lists, such as
    the job launching list, running list, and `finalizedJobs` list. In section 3.4.4,
    we will discuss this process in detail.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 启动培训容器后，基于执行状态，Docker作业跟踪器将作业对象从作业队列移动到其他作业列表，如作业启动列表、运行列表和`finalizedJobs`列表。在第3.4.4节中，我们将详细讨论这个过程。
- en: Note Consider that a dataset will be split in a training container (at training
    time). It is valid to split datasets during dataset building or model training,
    and both processes have pros and cons. But either way, it will not affect the
    design of the training service significantly. For simplicity, we assume in this
    sample training service that the algorithm code will split the dataset into train,
    validate, and test subsets.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 考虑到训练时间，可能会在培训容器（在培训时）中分割数据集。在数据集构建或模型训练期间拆分数据集都是有效的，但两个过程都有各自的优点和缺点。但无论哪种方式，都不会对训练服务的设计产生重大影响。为简单起见，在此示例培训服务中，我们假设算法代码将数据集拆分为训练集、验证集和测试集。
- en: 3.3.3 Training service API
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 培训服务API
- en: 'Having seen the overview, let’s dive into the public gRPC APIs (`grpc-contract/
    src/main/proto/training_service.proto`) to gain a deeper understanding of the
    service. There are two APIs in the training service: `Train` and `GetTrainingStatus`.
    The `Train` API is for submitting training requests, and the `GetTrainingStatus`
    API is for fetching the training execution status. See the API definition in the
    following listing.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了概述后，让我们深入了解公共gRPC API (`grpc-contract/ src/main/proto/training_service.proto`)，以更深入地理解该服务。培训服务中有两个API：`Train`
    和 `GetTrainingStatus`。`Train` API用于提交培训请求，而`GetTrainingStatus` API用于获取培训执行状态。请参见以下清单中的API定义。
- en: Listing 3.2 A model training service gRPC interface
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 清单3.2 模型培训服务gRPC接口
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Defines the dataset, training algorithm, and extra parameters for the model-building
    request
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ 定义了模型构建请求的数据集、训练算法和额外参数 '
- en: 'From the gRPC interface in listing 3.2, to use the `Train` API, we need to
    provide the following information as `TrainingJobMetadata`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从清单3.2的gRPC接口中，为使用`Train` API，我们需要提供以下信息作为`TrainingJobMetadata`：
- en: '`dataset_id`—The dataset ID in the dataset management service'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_id`—数据集管理服务中的数据集ID'
- en: '`train_data_version_hash`—The hash version of the dataset that is used in training'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_data_version_hash`—用于培训的数据集的散列版本'
- en: '`name`—Training job name'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`—培训作业名称'
- en: '`algorithm`—Specify which training algorithm to use to train the dataset. This
    algorithm string needs to be one of our predefined algorithms. Internally, the
    training service will find the Docker image associated with this algorithm to
    execute training.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`algorithm`—指定使用哪个培训算法来训练数据集。该算法字符串需要是我们预定义算法之一。在内部，培训服务将查找与该算法关联的Docker镜像以执行培训。'
- en: '`parameters`—Training hyperparameters that we pass directly to the training
    container, such as number of epochs, batch size, etc.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parameters`—我们直接传递给训练容器的训练超参数，如训练轮数、批量大小等。'
- en: Once the `Train` API receives a training request, the service puts the request
    to the job queue and returns an ID (`job_id`) for the caller to reference the
    job. This `job_id` can be used with the `GetTrainingStatus` API to check the training
    status. Now that we have seen the API definitions, let’s look at their implementations
    in the next two sections.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`Train` API收到一个培训请求，服务将把请求放入作业队列，并返回一个ID (`job_id`)供调用者引用该作业。这个`job_id`可以与`GetTrainingStatus`
    API一起使用，以检查培训状态。现在我们已经看到了API定义，让我们在接下来的两个章节中看看它们的具体实现。
- en: 3.3.4 Launching a new training job
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 启动新的培训作业
- en: When a user calls the `Train` API, a training request is added to the job queue
    of the memory store, and then the Docker job tracker handles the actual job execution
    in another thread. This logic will be explained in the next three listings (3.3–3.5).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户调用`Train` API时，培训请求将被添加到内存存储的作业队列中，然后Docker作业跟踪器会在另一个线程中处理实际的作业执行。这个逻辑将在接下来的三个清单（3.3-3.5）中解释。
- en: Receiving training requests
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接收培训请求
- en: First, a new training request will be added to the job-waiting queue and will
    be assigned a job ID for future reference; see code (`training-service/src/main/`
    `java/org/orca3/miniAutoML/training/TrainingService.java`) as follows.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一个新的培训请求将被添加到作业等待队列中，并分配一个作业ID供将来参考；参见代码(`training-service/src/main/` `java/org/orca3/miniAutoML/training/TrainingService.java`)如下。
- en: Listing 3.3 Submitting training request implementation
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 3.3 提交培训请求的实现代码
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Implements train API
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实现了训练API
- en: ❷ Enqueues the training request
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Returns the job ID
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Four different job lists to track job status
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Generates job ID
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Starts the job at the waiting queue
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Launching a training job (container)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Once the job is in the waiting queue, the Docker job tracker will process it
    when there are enough system resources. Figure 3.6 shows the entire process. The
    Docker job tracker monitors the job-waiting queue and picks up the first available
    job when there is enough capacity in the local Docker engine (step 1 in figure
    3.6). Then the Docker job tracker executes the model training the job by launching
    a Docker container (step 2). After the container launches successfully, the tracker
    moves the job object from the job queue to the launching list queue (step 3).
    The code implementation for figure 3.6 (`training-service/src/main/java/org/orca3/miniAutoML/training/
    tracker/DockerTracker.java`) is highlighted in listing 3.4.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03-06.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6 The training job launching workflow: the Docker job tracker launches
    training containers from the job queue when it has the capacity.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.4 Launching a training container with DockerTracker
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Checks the system’s capacity
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Launches the training Docker container
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Converts training parameters into environment variables
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Builds the Docker launch command
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Sets the Docker image name; the value is from the algorithm name parameter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Passes the training parameter to the Docker container as environment variables
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Runs the Docker container
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the training parameters defined in the `train`
    API request are passed to the training container (training code) as environment
    variables by the `launch` function in code listing 3.4.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Tracking training progress
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: During the last step, the Docker job tracker continues tracking each job by
    monitoring its container’s execution status. When it detects a container status
    change, the job tracker moves the container’s job object to the corresponding
    job list in the memory store.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The job tracker will query the Docker runtime to fetch the container’s status.
    For example, if a job’s Docker container starts running, the job tracker will
    detect the change and put the job in the “running job list”; if a job’s Docker
    container finishes, the job is then moved to the “finalized jobs list” by the
    job tracker. The job tracker will stop checking the job status once it’s placed
    on the “finalized jobs list,” which means the training is completed. Figure 3.7
    depicts this job tracking workflow. Code listing 3.5 highlights the implementation
    of this job tracking process.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03-07.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 The Docker job tracker monitors the Docker container execution status
    and updates the job queues.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.5 DockerTracker monitoring Docker and updating the job status
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Checks container status for all jobs in the launching job list
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Queries the container’s execution status
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Checks the container status for all jobs in the running job list
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.5 Updating and fetching job status
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you have seen how a training request is executed in the training service,
    let’s move on to the last stop of the code tour: obtaining the training execution
    status. After launching a training job, we can query the `GetTrainingStatus` API
    to get the training status. As a reminder, we are reposting figure 3.5, presented
    here as figure 3.8, which shows the service’s high-level design, as follows.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03-08.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 A high-level service design and user workflow
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Based on figure 3.8, we can see that obtaining the training status needs only
    one step, 1.b. Also, the latest status of a training job can be determined by
    finding which job list (in the memory store) contains the `jobId`. See the following
    code for querying the status of a training job/request (`training-service/src/main/java/org/orca3/miniAutoML/training/TrainingService.java`).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.6 Training status implementation
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Searches the job in the finalized job list
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Searches the job in the launching job list
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Searches the job in the running job list
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: ❹ The job is still in the waiting job queue.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Because the Docker job tracker moves the job to the corresponding job list in
    real time, we can rely on using the job queue type to determine a training job
    status.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.6 The intent classification model training code
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until now, we have been working with the training service code. Now let’s look
    at the last piece, the model training code. Please do not be intimidated by the
    deep learning algorithms here. The purpose of this code example is to show you
    a concrete example of how a training service interacts with the model training
    code. Figure 3.9 draws the workflow of the sample intent classification training
    code.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03-09.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 The intent classification training code workflow first reads all
    the input parameters from environment variables and then downloads the dataset,
    processes it, and starts the training loop. At the end, it uploads the output
    model file.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Our sample training code trains a three-layer neural network to perform intent
    classification. It first obtains all the input parameters from environment variables
    that are passed by our training service (see section 3.3.4). The input parameters
    include hyperparameters (epoch number, learning rate, etc.), dataset download
    settings (MinIO server address, dataset ID, version hash), and model upload settings.
    Next, the training code downloads and parses the dataset and starts the iterative
    learning process. In the last step, the code uploads the generated model and training
    metrics to the metadata store. The following code listing highlights the major
    steps mentioned previously (`train-service/text-classification/train.py` and `train-service/text-classification/Dockerfile`).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.7 Intent classification model training code and Docker file
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note We hope our sample training code demonstrates how a deep learning training
    code follows a common pattern. With Dockerization and a clear protocol to pass
    in parameters, a training service can execute varieties of training code, regardless
    of the training framework or model architecture.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们希望我们的示例训练代码演示了深度学习训练代码遵循常见模式。通过容器化以及传递参数的清晰协议，训练服务可以执行各种训练代码，而不论训练框架或模型架构。
- en: 3.3.7 Training job management
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.7 训练作业管理
- en: 'In section 3.1.2, we mentioned that a good training service should address
    computation isolation and provide on-demand computing resources (principle 4).
    This isolation has two meanings: training process execution isolation and resource
    consumption isolation. Because we Dockerize the training process, the process
    execution isolation is guaranteed by the Docker engine. But we still have to handle
    the resource consumption isolation ourselves.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3.1.2 节中，我们提到良好的训练服务应该解决计算隔离并提供按需计算资源（原则 4）。这种隔离有两重含义：训练过程的执行隔离和资源消耗隔离。由于我们使用
    Docker 对训练过程进行容器化，所以这个执行隔离是由 Docker 引擎保证的。但是资源消耗隔离仍然需要我们自己处理。
- en: 'Imagine three users (A, B, and C) from different teams submitting training
    requests to our training service. If user A submits 100 training requests and
    then users B and C both submit one request each, user B’s and C’s requests will
    sit in the waiting job queue for a while until all of user A’s training requests
    complete. This is what happens when we treat the training cluster as a playing
    field for everyone: one heavy-use case will dominate the job scheduling and resource
    consumption.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，来自不同团队的三个用户（A、B 和 C）向我们的训练服务提交训练请求。如果用户 A 提交了 100 个训练请求，然后用户 B 和 C 都各自提交了一个请求，那么用户
    B 和 C 的请求将在等待作业队列中等待一段时间，直到用户 A 的所有训练请求完成。当我们将训练集群视为每个人的游戏场时，这就是发生的情况：一个重度使用情况会主导作业调度和资源消耗。
- en: To solve this resource competition problem, we need to set boundaries within
    the training cluster for different teams and users. We could create machine pools
    inside the training cluster to create resource consumption isolation. Each team
    or user can be assigned to a dedicated machine pool, each pool has its own GPU
    and machines, and the pool size depends on the project needs and training usage.
    Also, each machine pool can have a dedicated job queue, so the heavy users won’t
    affect other users. Figure 3.9 shows this approach at work.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决资源竞争问题，我们需要在训练集群内为不同团队和用户设置边界。我们可以在训练集群内创建机器池以实现资源消耗隔离。每个团队或用户可以分配到一个专用的机器池，每个池都有自己的
    GPU 和机器，池的大小取决于项目需求和训练使用情况。此外，每个机器池可以有一个专门的作业队列，因此重度用户不会影响其他用户。图 3.9 展示了这种方法的运作方式。
- en: Note The resource segregation approach, like the server pools method we just
    mentioned, may not be efficient on the resource utilization front. For example,
    server pool A may be extremely busy, whereas server pool B may be idle. It is
    possible to define the size of each server pool in a range instead of a fixed
    number, such as a minimum of 5 servers and a maximum of 10 to improve resource
    utilization. Additional logic that either shuffles servers between pools or provisions
    new servers can then be applied.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 资源隔离方法，像我们刚刚提到的服务器池方法，在资源利用方面可能不够高效。例如，服务器池 A 可能非常忙，而服务器池 B 可能处于空闲状态。可以定义每个服务器池的大小为一个范围，而不是一个固定数字，例如最小
    5 台服务器、最大 10 台服务器，以提高资源利用率。然后可以应用额外的逻辑，以在服务器之间进行移动或提供新服务器。
- en: The ideal approach for implementing figure 3.10 is to use Kubernetes. Kubernetes
    allows you to create multiple virtual clusters backed by the same physical cluster,
    which is called a namespace. Kubernetes namespace is a lightweight machine pool
    that consumes very few system resources.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 实现图 3.10 的理想方法是使用 Kubernetes。Kubernetes 允许您创建由相同物理集群支持的多个虚拟集群，称为命名空间。Kubernetes
    命名空间是一个消耗非常少系统资源的轻量级机器池。
- en: '![](../Images/03-10.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-10.png)'
- en: Figure 3.10 Creating machine pools within the training cluster to set up the
    resource consumption boundary for different users.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 在训练集群内创建机器池，以为不同用户设置资源消耗边界。
- en: If you are using Kubernetes to manage your service environment and your computing
    cluster, setting up such isolation is fairly easy. First, you create a namespace
    with a resource quota, such as the number of CPUs, memory size, and GPU counts;
    then, define the user and its namespace mapping in the training service.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用 Kubernetes 管理服务环境和计算集群，那么设置此类隔离非常容易。首先，您需要创建一个拥有资源配额的命名空间，例如 CPU 数量、内存大小和
    GPU 数量；然后，在训练服务中定义用户及其命名空间的映射关系。
- en: Now, when a user submits a training request, the training service first finds
    the right namespace for the user by checking the user information from the request
    and then calls the Kubernetes API to place the training executable in the namespace.
    Because Kubernetes tracks the system usage in real time, it knows whether a namespace
    has enough capacity, and it will reject the job launch request if the namespace
    is fully loaded.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当用户提交一个训练请求时，训练服务首先通过检查请求中的用户信息找到正确的命名空间，然后调用 Kubernetes API 将训练可执行文件放置在该命名空间中。由于
    Kubernetes 实时跟踪系统的使用情况，它知道一个命名空间是否有足够的容量，如果命名空间已满，它将拒绝作业启动请求。
- en: As you can see, by using Kubernetes to manage training clusters, we can offload
    the resource capacity tracking and resource isolation management from the training
    service. This is one of the reasons why Kubernetes is a good choice for building
    training cluster management for deep learning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，通过使用 Kubernetes 管理训练集群，我们可以将资源容量跟踪和资源隔离管理从训练服务中卸载出来。这是选择 Kubernetes 构建深度学习训练集群管理的一个原因。
- en: 3.3.8 Troubleshooting metrics
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.8 故障排除指标
- en: 'What we didn’t demo in this sample service is metrics. In general, metrics
    are measures of quantitative assessment commonly used for assessing, comparing,
    and tracking performance or production. For deep learning training specifically,
    we usually define two types of metrics: model training execution metrics and model
    performance metrics.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例服务中，我们没有演示指标的使用。通常，指标是用于评估、比较和跟踪性能或生产的定量评估的常见度量。对于深度学习训练，我们通常定义两种类型的指标：模型训练执行指标和模型性能指标。
- en: Model training execution metrics include resource saturation rate, training
    jobs’ execution availability, average training job execution time, and job failure
    rate. We check these metrics to make sure the training service is healthy and
    functioning and that our user’s daily activities are healthy. As an example, we
    expect service availability to exceed 99.99% and training job failure rate to
    be less than 0.1%.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练执行指标包括资源饱和率、训练作业执行可用性、平均训练作业执行时间和作业失败率。我们检查这些指标，以确保训练服务健康运行，并且用户的日常活动正常。例如，我们期望服务可用性超过
    99.99% ，训练作业失败率小于 0.1% 。
- en: Model performance metrics measure the quality of the model learning. It includes
    a loss value and evaluation score for each training iteration (epoch) and the
    final model evaluation results, such as accuracy, precision, and F1 score.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能指标衡量模型学习的质量。它包括每个训练迭代（epoch）的损失值和评估分数，以及最终的模型评估结果，如准确率、精度和 F1 分数。
- en: For model performance–related metrics, we need to store the metrics in a more
    organized way, so we can use a unified method to search for information and compare
    performance between different training runs easily. We will discuss this in more
    detail in chapter 8.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与模型性能相关的指标，我们需要以更有组织的方式存储这些指标，以便能够使用统一的方法轻松搜索信息并比较不同训练运行之间的性能。我们将在第8章对此进行更详细的讨论。
- en: 3.3.9 Supporting new algorithm or new version
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.9 支持新算法或新版本
- en: Now let’s discuss how to onboard more training code to our sample training service.
    In the current implementation, we define a naive mapping between the user training
    request and the training code, using the `algorithm` variable in the request to
    find the training image. The underlying rule is the `algorithm` variable must
    equal a Docker image name; otherwise, the training service can’t find the right
    image to run model training.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论如何将更多的训练代码接入到我们的示例训练服务中。在当前的实现中，我们使用请求中的 `algorithm` 变量来定义用户训练请求与训练代码之间的映射关系，使用
    `algorithm` 变量在请求中找到对应的训练镜像。底层规则是 `algorithm` 变量必须等于 Docker 镜像名称，否则训练服务无法找到正确的镜像来运行模型训练。
- en: Use our intent classification training as an example. First, we need to Dockerize
    our intent training Python code to a Docker image and name it “intent-classification.”
    Then, when the user sends a training request with the parameter `algorithm='intent-classification'`,
    the Docker job tracker will use the algorithm name (intent-classification) to
    search local Docker repository for the “intent-classification” training image
    and run the image as a training container.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '以我们的意图分类训练为例。首先，我们需要将意图训练Python代码Docker化为Docker镜像并将其命名为“intent-classification”。然后，当用户使用`algorithm
    = ''intent-classification''`参数发送训练请求时，Docker作业跟踪器将使用算法名称（即intent-classification）在本地Docker仓库中搜索“intent-classification”训练镜像并将镜像运行为训练容器。 '
- en: This approach is definitely oversimplified, but it exemplifies how we work with
    data scientists to define a formal contract for mapping user training requests
    to the actual training code. In practice, the training service should provide
    a set of APIs to allow data scientists to register training code in a self-serve
    manner.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法肯定过于简化了，但它演示了我们如何与数据科学家一起定义将用户训练请求映射到实际训练代码的正式协议。在实践中，训练服务应该提供一组API，允许数据科学家以自助方式注册训练代码。
- en: One possible approach is to define an algorithm name and training code mapping
    in a database and add some API to manage this mapping. The proposed APIs can be
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '一种可能的方法是在数据库中定义算法名称和训练代码映射，添加一些API来管理这个映射。建议的API可以是 '
- en: '`createAlgorithmMapping(string` `algorithmName,` `string` `image,` `string`
    `version)`'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createAlgorithmMapping(string` `algorithmName,` `string` `image,` `string`
    `version)` '
- en: '`updateAlgorithmVersion(string` `algorithmName,` `string` `image,` `string`
    `version)`'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`updateAlgorithmVersion(string` `algorithmName,` `string` `image,` `string`
    `version)`'
- en: If data scientists want to add a new algorithm type, they would call the `createAlgorithmMapping`
    API to register the new training image with a new algorithm name into the training
    service. Our users just need to use this new algorithm name in the training request
    to kick off model training with this new algorithm.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '如果数据科学家想要添加新的算法类型，他们可以调用`createAlgorithmMapping` API来向训练服务注册新的训练图像和新的算法名称。我们的用户只需要在训练请求中使用这个新的算法名称即可使用这个新的算法开始模型训练。 '
- en: If data scientists want to release a newer version of an existing algorithm,
    they can call the `updateAlgorithmVersion` API to update the mapping. Our users
    will still have the same algorithm name (such as intent-classification) to send
    requests, but they won’t be aware that the training code has upgraded to a different
    version behind the scenes. Also, it is worth pointing out that the service’s public
    API won’t be affected by adding new training algorithms; only a new parameter
    value is used.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据科学家想要发布现有算法的新版本，他们可以调用`updateAlgorithmVersion` API来更新映射。我们的用户仍然会有相同的算法名称（如意图分类），发送请求，但他们不会意识到训练代码在幕后升级到不同的版本。同时，值得指出的是，服务的公共API不会受到添加新训练算法的影响；只有一个新参数值被使用。
- en: '3.4 Kubeflow training operators: An open source approach'
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4. Kubeflow训练操作符：开源方法
- en: After seeing our sample training service, let’s look at an open source training
    service. In this section, we will discuss a set of open source training operators
    from the Kubeflow project. These training operators work out of the box and can
    be set up independently in any Kubernetes cluster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 看完我们的示例训练服务后，让我们看看一个开源的训练服务。在本节中，我们将讨论来自Kubeflow项目的一组开源训练操作符。这些训练操作符可立即使用，并且可以在任何Kubernetes集群中独立设置。
- en: Kubeflow is a mature, open source machine learning system built for production
    use cases. We briefly discuss it in appendix B.4 along with Amazon SageMaker and
    Google Vertex AI. We recommend Kubeflow training operators because they are well
    designed and offer high-quality training that’s scalable, distributable, and robust.
    We will first talk about the high-level system design and then discuss how to
    integrate these training operators into your own deep learning system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow是一个成熟、开源的机器学习系统，适用于生产环境。我们在附录B.4中简要介绍了它，以及亚马逊SageMaker和Google Vertex
    AI。我们推荐使用Kubeflow、可扩展、可分布式和稳健性高的训练操作符。我们将首先讨论高级系统设计，然后讨论如何将这些训练操作符集成到自己的深度学习系统中。
- en: What Is Kubeflow?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是Kubeflow？
- en: Kubeflow is an open source machine learning platform (originated from Google)
    for developing and deploying production-level machine learning models. You can
    view Kubeflow as the open source version of Amazon SageMaker, but it runs natively
    on Kubernetes, so it’s cloud agnostic. Kubeflow integrates a full list of machine
    learning features into one system—from notebooks and pipelines to training and
    serving.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow是一个开源的机器学习平台（源自谷歌），用于开发和部署生产级别的机器学习模型。你可以将Kubeflow视为Amazon SageMaker的开源版本，但它原生运行在Kubernetes上，因此它是云无关的。Kubeflow将完整的机器学习功能集成到一个系统中——从Notebooks和管道到训练和服务。
- en: I highly recommend that you pay attention to Kubeflow projects, even if you
    have no interest in using it. Kubeflow is a well-designed and fairly advanced
    deep learning platform; its feature list covers the entire machine learning life
    cycle. By reviewing its use cases, design, and code, you will gain a deep understanding
    of modern deep learning platforms.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不打算使用它，我强烈建议你关注Kubeflow项目。Kubeflow是一个设计良好且相当先进的深度学习平台；它的功能列表涵盖了整个机器学习生命周期。通过审查其用例、设计和代码，你将深入了解现代深度学习平台。
- en: Also, because Kubeflow is built natively on top of Kubernetes, you can easily
    set up the whole system on your local or production environment. If you are not
    interested in borrowing the entire system, you can also port some of its components—such
    as training operators or hyperparameter optimization services—which can work by
    themselves in any Kubernetes environment out of the box.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为Kubeflow是在Kubernetes的原生基础上构建的，你可以轻松地在你的本地或生产环境中设置整个系统。如果你不感兴趣借鉴整个系统，你也可以移植其中一些组件——如训练操作器或超参数优化服务——它们可以自行在任何Kubernetes环境中开箱即用。
- en: 3.4.1 Kubeflow training operators
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 Kubeflow训练操作器
- en: Kubeflow offers a set of training operators, such as TensorFlow operator, PyTorch
    operator, MXNet operator, and MPI operator. These operators cover all the major
    training frameworks. Each operator has the knowledge to launch and monitor the
    training code (container) written in a specific type of training framework.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow提供了一组训练操作器，例如TensorFlow操作器、PyTorch操作器、MXNet操作器和MPI操作器。这些操作器涵盖了所有主要训练框架。每个操作器都有知识可以启动和监视用特定类型的训练框架编写的训练代码（容器）。
- en: 'If you plan to run model training in Kubernetes cluster and want to set up
    your own training service to reduce the operation cost, Kubeflow training operators
    are the perfect choice. Here are the three reasons:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划在Kubernetes集群中运行模型训练，并希望设置自己的训练服务以减少操作成本，Kubeflow训练操作器是完美的选择。以下是三个原因：
- en: '*Easy install and low maintenance*—Kubeflow operators work out of the box;
    you can make them work in your cluster by issuing a few lines of Kubernetes commands.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轻松安装和低维护*——Kubeflow操作器开箱即用；通过几行Kubernetes命令，你可以使它们在你的集群中工作。'
- en: '*Compatible with most training algorithms and frameworks*—As long as you containerize
    your training code, you can use Kubeflow operators to execute it.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*兼容大多数训练算法和框架*——只要你将训练代码容器化，就可以使用Kubeflow操作器执行它。'
- en: '*Easy integration to existing systems*—Because Kubeflow training operators
    follow the Kubernetes operator design pattern, you can use Kubernetes’s declarative
    HTTP API to submit the training job request and check the job running status and
    result. You can also use RESTful queries to interact with these operators.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易于集成到现有系统*——由于Kubeflow训练操作器遵循Kubernetes操作器设计模式，因此你可以使用Kubernetes的声明性HTTP API提交训练作业请求并检查作业运行状态和结果。你也可以使用RESTful查询与这些操作器交互。'
- en: 3.4.2 Kubernetes operator/controller pattern
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 Kubernetes操作器/控制器模式
- en: Kubeflow training operators follow the Kubernetes operator (controller) design
    pattern. If we understand this pattern, running Kubeflow training operators and
    reading their source code is straightforward. Figure 3.11 shows the controller
    pattern design graph.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow训练操作器遵循Kubernetes操作器（控制器）设计模式。如果我们理解了此模式，那么运行Kubeflow训练操作器并阅读其源代码就很简单了。图3.11显示了控制器模式的设计图。
- en: '![](../Images/03-11.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-11.png)'
- en: Figure 3.11 The Kubernetes operator/controller pattern runs an infinite control
    loop that watches the actual state (on the right) and desired state (on the left)
    of certain Kubernetes resources and tries to move the actual state to the desired
    one.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 Kubernetes操作器/控制器模式运行无限控制循环，观察某些Kubernetes资源的实际状态（在右侧）和期望状态（在左侧），并尝试将实际状态移动到期望状态。
- en: Everything in Kubernetes is built around resource objects and controllers. Kubernetes’
    resource objects such as Pods, Namespaces, and ConfigMaps are conceptual objects
    that persist entities (data structures) that represent the state (desired and
    current) of your cluster. A controller is a control loop that makes changes to
    the actual system resources to bring your cluster from the current state closer
    to the desired one, which is defined in resource objects.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的所有内容都围绕着资源对象和控制器构建。Kubernetes 的资源对象，如 Pods、Namespaces 和 ConfigMaps，是持久化实体（数据结构），代表着集群的状态（期望的和当前的）。控制器是一个控制循环，它对实际的系统资源进行更改，以将您的集群从当前状态带到更接近期望状态，这在资源对象中定义。
- en: 'Note Kubernetes pods are the smallest deployable units of computing that you
    can create and manage in Kubernetes. Pods can be viewed as “logical hosts” that
    run one or more Docker containers. A detailed explanation of the Kubernetes concepts,
    such as Namespaces and ConfigMaps, can be found at the official website: [https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Kubernetes pod 是您可以在 Kubernetes 中创建和管理的最小部署单元。Pod 可以被视为运行一个或多个 Docker 容器的“逻辑主机”。有关
    Kubernetes 概念的详细解释，例如 Namespaces 和 ConfigMaps，可以在官方网站找到：[https://kubernetes.io/docs/concepts/](https://kubernetes.io/docs/concepts/)
- en: 'For example, when a user applies the Kubernetes command to create a pod, it
    will create a pod resource object (a data structure) in the cluster, which contains
    the desired states: two Docker containers and one disk volume. When the controller
    detects this new resource object, it will provision the actual resource in the
    cluster and run the two Docker containers and attach the disk. Next, it will update
    the pod resource object with the latest actual status. Users can query the Kubernetes
    API to get the updated information from this pod resource object. When the user
    deletes this pod resource object, the controller will remove the actual Docker
    containers because the desired state is changed to zero.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当用户应用 Kubernetes 命令来创建一个 pod 时，它将在集群中创建一个 pod 资源对象（一个数据结构），其中包含所需的状态：两个 Docker
    容器和一个磁盘卷。当控制器检测到这个新的资源对象时，它将在集群中提供实际的资源，并运行两个 Docker 容器并附加磁盘。接下来，它将更新 pod 资源对象的最新实际状态。用户可以查询
    Kubernetes API 来获取此 pod 资源对象的更新信息。当用户删除此 pod 资源对象时，控制器将删除实际的 Docker 容器，因为所需状态已更改为零。
- en: To extend Kubernetes easily, Kubernetes allows users to define customer resource
    definition (CRD) objects and register customized controllers to handle these CRD
    objects, which are called operators. If you want to learn more about controllers/
    operators, you can read the “Kubernetes/sample-controller” GitHub repository,
    which implements a simple controller for watching a CRD object. This sample controller
    code can help you to understand operator/controller patterns, and this understanding
    is very useful for reading the Kubeflow training operator source code.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松扩展 Kubernetes，Kubernetes 允许用户定义自定义资源定义（CRD）对象，并注册定制的控制器来处理这些 CRD 对象，称为操作器。如果您想了解更多关于控制器/操作器的信息，可以阅读
    “Kubernetes/sample-controller” GitHub 存储库，该存储库实现了用于监视 CRD 对象的简单控制器。这个示例控制器代码可以帮助您理解操作器/控制器模式，这种理解对于阅读
    Kubeflow 训练操作器源代码非常有用。
- en: Note The terms *controller* and *operator* are used interchangeably in this
    section.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本节中，“控制器”和“操作器”这两个术语是可以互换使用的。
- en: 3.4.3 Kubeflow training operator design
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 Kubeflow 训练操作器设计
- en: Kubeflow training operators (TensorFlow operator, PyTorch operator, MPI operator)
    follow the Kubernetes operator design. Each training operator watches its own
    kind of customer resource definition object—such as `TFJob`, `PyTorchJob`, and
    `MPIJob`—and creates the actual Kubernetes resources to run the training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 训练操作器（TensorFlow 操作器、PyTorch 操作器、MPI 操作器）遵循 Kubernetes 操作器设计。每个训练操作器都会监视其自己类型的客户资源定义对象
    —— 例如 `TFJob`、`PyTorchJob` 和 `MPIJob` —— 并创建实际的 Kubernetes 资源来运行训练。
- en: For example, the TensorFlow operator processes any `TFJob` CRD object generated
    in the cluster and creates the actual services/pods based on the `TFJob` spec.
    It synchronizes `TFJob` objects’ resource requests with the actual Kubernetes
    resources, such as services and pods, and continuously strives to make the observed
    state match the desired state. See a visual workflow in figure 3.12.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，TensorFlow 操作器处理在集群中生成的任何 `TFJob` CRD 对象，并根据 `TFJob` 规范创建实际的服务/ pod。它将 `TFJob`
    对象的资源请求与实际的 Kubernetes 资源（例如服务和 pod）同步，并不断努力使观察到的状态与期望的状态匹配。在图 3.12 中可以看到一个视觉工作流程。
- en: '![](../Images/03-12.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-12.png)'
- en: Figure 3.12 The Kubeflow training operator workflow. A user first creates a
    `TFJob` CRD object that defines a training request, and then the TensorFlow operator
    detects this object and creates actual pods to execute the TensorFlow training
    image. The TensorFlow operator also monitors the pod status and updates its status
    to the `TFJob` CRD object. The same workflow applies to the PyTorch operator.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 Kubeflow 训练操作器的工作流程。用户首先创建一个 `TFJob` CRD 对象，该对象定义了一个训练请求，然后 TensorFlow
    操作器检测到此对象，并创建实际的 pod 来执行 TensorFlow 训练图像。TensorFlow 操作器还监视 pod 的状态，并将其状态更新到 `TFJob`
    CRD 对象中。相同的工作流程也适用于 PyTorch 操作器。
- en: Each operator can run training pods for its own type of training framework.
    For example, the TensorFlow operator knows how to set up a distributed training
    pod group for training code written in TensorFlow. The operator reads the user
    request from the CRD definition, creates training pods, and passes the correct
    environment variables and command-line arguments to each training pod/container.
    You can check out the `reconcileJobs` and `reconcilePods` functions in each operator’s
    code for further details.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 每个操作器都可以为其自己类型的训练框架运行训练 pod。例如，TensorFlow 操作器知道如何为 TensorFlow 编写的训练代码设置分布式训练
    pod 组。操作器从 CRD 定义中读取用户请求，创建训练 pod，并将正确的环境变量和命令行参数传递给每个训练 pod/container。您可以查看每个操作器代码中的
    `reconcileJobs` 和 `reconcilePods` 函数以了解更多详细信息。
- en: Each Kubeflow operator also handles job queue management. Because Kubeflow operators
    follow the Kubernetes operator pattern and create Kubernetes resources at the
    pod level, the training pod failover is handled nicely. For example, when a pod
    fails unexpectedly, the current pod number becomes one less than the desired pod
    number. In this situation, the `reconcilePods` logic in the operator will create
    a new pod in the cluster to make sure the actual pod number is equal to the desired
    number defined in the CRD object, thus addressing failover.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Kubeflow 操作器还处理作业队列管理。因为 Kubeflow 操作器遵循 Kubernetes 操作器模式，并在 pod 级别创建 Kubernetes
    资源，所以训练 pod 的故障切换处理得很好。例如，当一个 pod 意外失败时，当前 pod 数量会减少一个，小于 CRD 对象中定义的期望 pod 数量。在这种情况下，操作器中的
    `reconcilePods` 逻辑将在集群中创建一个新的 pod，以确保实际的 pod 数量等于 CRD 对象中定义的期望数量，从而解决故障切换问题。
- en: Note At the time of writing this book, the TensorFlow operator was becoming
    the all-in-one Kubeflow operator. It aims to simplify running distributed or nondistributed
    TensorFlow/PyTorch/MXNet/XGBoost jobs on Kubernetes. No matter how it ends up,
    it will be built on top of the design we mention here but simply more convenient
    to use.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在编写本书时，TensorFlow 操作器正在成为全能的 Kubeflow 操作器。它旨在简化在 Kubernetes 上运行分布式或非分布式 TensorFlow/PyTorch/MXNet/XGBoost
    作业。无论最终的结果如何，它都是基于我们在这里提到的设计构建的，只是使用起来更加方便。
- en: 3.4.4 How to use Kubeflow training operators
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 如何使用 Kubeflow 训练操作器
- en: In this section, we will use the PyTorch operator as an example for training
    a PyTorch model in four steps. Because all Kubeflow training operators follow
    the same usage pattern, these steps are applicable to other operators as well.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将以 PyTorch 操作器作为示例，分四个步骤训练一个 PyTorch 模型。因为所有的 Kubeflow 训练操作器都遵循相同的使用模式，这些步骤也适用于其他操作器。
- en: 'First, install the stand-alone PyTorch operator and `PyTorchJob` CRD in your
    Kubernetes cluster. You can find detailed instructions in the developer guide
    from the PyTorch operator Git repository. After installation, you can find a training
    operator pod running and a CRD definition created in your Kubernetes cluster.
    See the CRD query command as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在您的 Kubernetes 集群中安装独立的 PyTorch 操作器和 `PyTorchJob` CRD。您可以在 PyTorch 操作器的 Git
    存储库的开发人员指南中找到详细的安装说明。安装完成后，您可以在您的 Kubernetes 集群中找到一个正在运行的训练操作器 pod，并创建一个 CRD 定义。查看如下的
    CRD 查询命令：
- en: '[PRE9]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Lists all CRD definitions
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列出所有的 CRD 定义
- en: ❷ PyTorchJob CRD is created in Kubernetes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Kubernetes 中创建了 PyTorchJob CRD。
- en: Note The training operator installation can be confusing because the README
    suggests that you install the entire Kubeflow to run these operators, but this
    isn’t necessary. Each training operator can be installed individually, which is
    how we recommend handling it. Please check the development guide or the setup
    script at [https://github.com/kubeflow/pytorch-operator/blob/master/scripts/setup-pytorch-operator.sh](https://github.com/kubeflow/pytorch-operator/blob/master/scripts/setup-pytorch-operator.sh).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Next, update your training container to read the parameter input from environment
    variables and command-line arguments. You can pass in these parameters later in
    the CRD object.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Third, create a `PyTorchJob` CRD object to define our training request. You
    can create this CRD object by first writing a YAML file (e.g., pytorchCRD.yaml)
    and then running `kubectl` `create` `-f` `pytorchCRD.yaml` in your Kubernetes
    cluster. The PT-operator will detect this newly created CRD object, put it into
    the controller’s job queue, and try to allocate resources (Kubernetes pods) to
    run the training. Listing 3.8 shows a sample `PyTorchJob` CRD.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.8 A sample PyTorch CRD object
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The name of the CRD
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Trains job name
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Defines training group specs
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Numbers of master pods
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Numbers of worker pods
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Defines training container configuration
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Defines environment variable for each training pod
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Defines the command-line parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is monitoring. You can obtain the training status by using the
    `kubectl` `get` `-o` `yaml` `pytorchjobs` command, which will list the details
    of all the `pytorchjobs` types of CRD objects. Because the controller of the PyTorch
    operator will continue updating the latest training information back to the CRD
    object, we can read the current status from it. For example, the following command
    will make the `PyTorchJob` type CRD object with the name equal to `pytorch-demo`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note In the previous sample, we used the Kubernetes command `kubectl` to interact
    with the PyTorch operator. But we could also send RESTful requests to the cluster’s
    Kubernetes API to create a training job CRD object and query its status. The newly
    created CRD object will then trigger training actions in the controller. This
    means Kubeflow training operators can be easily integrated into other systems.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 How to integrate these operators into an existing system
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From section 3.4.3, we see that the operators’ CRD objects act as the gateway
    APIs to trigger training operations and the source of truth of the training status.
    Therefore, we can integrate these training operators into any system by building
    a web service as a wrapper on top of the operator CRD objects. This wrapper service
    has two responsibilities: first, it converts training requests in your system
    to the CRUD (create, read, update, and delete) operation on the CRD (training
    job) objects; second, it queries training status by reading the CRD objects. See
    the main workflow in figure 3.13.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03-13.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 Integrating Kubeflow training operators into an existing deep learning
    system as training backend. The wrapper service can transform training requests
    to CRD objects and fetch the training status from the CRD objects.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 3.13, the front part of the existing system is untouched—for example,
    the front door website. At the computation backend, we changed the internal components
    and talked to the wrapper training service to execute model training. The wrapper
    service does three things: first, it manages the job queue; second, it translates
    the training request from the existing format to the Kubeflow training operators’
    CRD objects; and third, it fetches the training status from CRD objects. With
    this approach, by adding the wrapper service, we can adopt Kubeflow training operators
    easily as the training backend for any existing deep learning platform/systems.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Building a production-quality training system from scratch requires a lot of
    effort. You need to know not only every nuance of different training frameworks
    but also how to handle the reliability and scalability challenges on the engineering
    side. Therefore, we highly recommend adopting Kubeflow training operators if you
    decide to run model training in Kubernetes. It’s an out-of-the-box solution and
    can be ported to an existing system easily.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 When to use the public cloud
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Major public cloud vendors like Amazon, Google, and Microsoft provide their
    deep learning platforms such as Amazon SageMaker, Google Vertex AI, and Azure
    Machine Learning Studio out of the box. All these systems claim to offer fully
    managed services that support the entire machine learning workflow to train and
    deploy machine learning models quickly. In fact, they cover not only model training
    but also data processing and storage, versioning, troubleshooting, operating,
    and more.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’re not going to talk about which cloud solution is the best;
    instead, we want to share our thoughts on when to use them. When we propose building
    services inside our company, such as training services or hyperparameter tuning
    services, we often hear questions like “Can we use SageMaker? I heard they have
    a feature . . .” or “Can you build a wrapper on top of Google Vertex AI? I heard. . . .”
    These questions are sometimes valid and sometimes not. What you can afford really
    depends on the stage of your business.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 When to use a public cloud solution
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you run a startup or want to validate your business idea quickly, using the
    public cloud AI platform is a good option. It handles all the underlying infrastructure
    management and provides a standard workflow for you to follow. As long as the
    predefined methods work for you, you can focus on developing your business logic,
    collecting data, and implementing model algorithms. The real benefit is the time
    saved on building your own infrastructure, so you can “fail early and learn fast.”
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to use the public cloud AI platforms is that you have only a
    few deep learning scenarios, and they fit the public cloud’s standard-use case
    well. In this event, it isn’t worth the resources to build a complicated deep
    learning system for just a few applications.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 When to build your own training service
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let’s talk about situations when you need to build your own training approach.
    If you have any of the following five requirements for your system, building your
    own training service is the way to go.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Being cloud agnostic
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: You can’t use Amazon SageMaker or Google Vertex AI platforms if you want your
    application to be cloud agnostic because these systems are cloud specific. Being
    cloud agnostic is important when your service stores customer data because some
    potential customers have specific requirements on which cloud they *don’t* want
    to put their data in. You want your application to have the capability of running
    on various cloud infrastructures seamlessly.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The common method of building a cloud-agnostic system on public clouds is to
    *only* use the foundation services, such as virtual machines (VM) and storage,
    and build your application logic on top of it. Using model training as an example,
    when using Amazon Web Services, we first set up a Kubernetes cluster (Amazon Elastic
    Kubernetes Service (Amazon EKS)) by using Amazon EC2 service to manage the computing
    resources and then build our own training service with the Kubernetes interfaces
    to launch training jobs. In this way, when we need to migrate to Google Cloud
    (GCP), we can simply apply our training service to the GCP Kubernetes cluster
    (Google Kubernetes Engine) instead of Amazon EKS, and most of the service remains
    unchanged.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Reducing infrastructure cost
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Using the cloud provider’s AI platform will charge you premium dollars compared
    to operating on your own services. You may not care so much about your bill at
    the prototyping phase, but after the product is released, you certainly should.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon SageMaker as an example, at the time this book was written (2022),
    SageMaker charged $0.461 per hour for an m5.2xlarge type (eight virtual CPUs,
    32 GB memory) machine. If you launch an Amazon EC2 instance (VM) on this hardware
    spec directly, it charges $0.384 per hour. By building your own training service
    and operating on the Amazon EC2 instances directly, you save nearly 20% on average
    for model building. If a company has multiple teams doing model training on a
    daily basis, a self-built training system will give you an edge over your competitors.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Customization
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Although the cloud AI platform gives you a lot of options for the workflow configuration,
    they are still black-box approaches. Because they are the one-for-all approach,
    these AI platforms focus on the most common scenarios. But there are always exceptions
    that you need to customize for your business; it won’t be a good experience when
    there aren’t many choices.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Another problem for the cloud AI platform is it always has a delay in adopting
    new technologies. For example, you have to wait for the SageMaker team’s decision
    on whether to support a training method and when to support it, and sometimes
    that decision is not agreeable to you. Deep learning is a rapidly developing space.
    Building your own training service can help you to adopt the latest research and
    pivot quickly, which will give you an edge over the fierce competition.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Passing compliance audits
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: To be qualified to run some businesses, you need to obtain certificates for
    compliance laws and regulations—for example, HIPAA (Healthcare Insurance Portability
    and Accountability Act) or CCPA (California Consumer Privacy Act). These certifications
    require that you provide evidence not only that your code meets these requirements
    but also that the infrastructure on which your application runs is compliant.
    If your application is built on Amazon SageMaker and Google Vertex AI platforms,
    they also need to be in compliance. As cloud vendors are a black box, running
    through compliance checklists and providing evidence is an unpleasant task.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and authorization
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Integrating authentication and authorization functionality into cloud AI platforms
    and in-house auth services (on-premises) requires a lot of effort. Many companies
    have their own version of auth services to authenticate and authorize user requests.
    If we adopt SageMaker as the AI platform and expose it to different internal services
    for various business purposes, bridging SageMaker auth management with the in-house
    user auth management services is not going to be easy. Instead, building on-premises
    training services is a lot easier because we can change our API freely and simply
    integrate it into existing auth services.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary goal of the training service is to manage the computing resources
    and training executions.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A sophisticated training service follows four principles: it supports all kinds
    of model training code through a unified interface; it reduces training cost;
    it supports model reproducibility; and it has high scalability and availability
    and handles compute isolation.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the general model training code pattern allows us to treat the
    code as a black box from the perspective of the training service.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerization is the key to using a generic method to handle the diversities
    of deep learning training methods and frameworks.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By Dockerizing training code and defining clear communication protocol, a training
    service can treat training code as a black box and execute the training on a single
    device or distributively. This also benefits data scientists because they can
    focus on model algorithm development without worrying about training execution.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeflow training operators are a set of Kubernetes-based open source training
    applications. These operators work out of the box, and they can be easily integrated
    into any existing systems as a model training backend. Kubeflow training operators
    support both distributed and nondistributed training.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using public cloud training services can help to build deep learning applications
    quickly. On the other hand, building your own training services can reduce training
    operation costs, provide more customized options, and remain cloud agnostic.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
