["```py\ntrain_df = pd.DataFrame(data=[train_y,train_x]).T \ntest_df = pd.DataFrame(data=[test_y,test_x]).T\n```", "```py\ntrain_df.shape\ntest_df.shape\n```", "```py\ndata_lm = TextLMDataBunch.from_df(train_df = train_df, valid_df = test_df, path = \"\")\n```", "```py\ndata_clas = TextClasDataBunch.from_df(path = \"\", train_df = train_df, valid_df = test_df, vocab=data_lm.train_ds.vocab)\n```", "```py\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)     ❶\n```", "```py\nlearn.lr_find()                        ❶\nlearn.recorder.plot(suggestion=True)   ❷\n```", "```py\nrate = learn.recorder.min_grad_lr    ❶\nprint(rate)                          ❷\n```", "```py\nlearn.fit_one_cycle(1, rate)      ❶\n```", "```py\nlearn.unfreeze()                               ❶\nlearn.fit_one_cycle(1, slice(rate/100,rate))   ❷\n```", "```py\nlearn.save_encoder('fine-tuned_language_model')\n```", "```py\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3)   ❶\nlearn.load_encoder('fine-tuned_language_model')                       ❷\n```", "```py\nlearn.lr_find()                           ❶\nlearn.recorder.plot(suggestion=True)      ❷\n```", "```py\nrate = learn.recorder.min_grad_lr      ❶\nlearn.fit_one_cycle(1, rate)           ❷\n```", "```py\ndepth = 2                           ❶\nfor i in range(1,depth+1):          ❷\n    learn.freeze_to(-i)             ❸\n    learn.fit_one_cycle(1, rate)    ❹\n```", "```py\nfrom transformers import DistilBertTokenizerFast                                                           ❶\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-multilingual-cased\")                  ❷\n```", "```py\nfrom transformers import DistilBertForMaskedLM                                                         ❶\n\nmodel = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-multilingual-cased\")                    ❷\n\nprint(\"Number of parameters in DistilmBERT model:\")\nprint(model.num_parameters())\n```", "```py\nfrom transformers import LineByLineTextDataset\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"../input/jw300entw/jw300.en-tw.tw\",    ❶\n    block_size=128)                                   ❷\n```", "```py\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True, mlm_probability=0.15)        ❶\n```", "```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"twidistilmbert\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_gpu_train_batch_size=16,\n    save_total_limit=1,\n)\n```", "```py\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n    prediction_loss_only=True)\n```", "```py\nimport time\nstart = time.time()\ntrainer.train()\nend = time.time()\nprint(\"Number of seconds for training:\")\nprint((end-start))\n```", "```py\ntrainer.save_model(\"twidistilmbert\")\n```", "```py\nfrom transformers import pipeline\n\nfill_mask = pipeline(                                ❶\n    \"fill-mask\",\n    model=\"twidistilmbert\",\n    tokenizer=tokenizer)\n\nprint(fill_mask(\"Eyi de ɔhaw kɛse baa [MASK] hɔ.\"))   ❷\n```", "```py\n[{'sequence': '[CLS] Eyi de ɔhaw kɛse baa fie hɔ. [SEP]', 'score': 0.31311026215553284, 'token': 29959}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa me hɔ. [SEP]', 'score': 0.09322386980056763, 'token': 10911}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa ne hɔ. [SEP]', 'score': 0.05879712104797363, 'token': 10554}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa too hɔ. [SEP]', 'score': 0.052420321851968765, 'token': 16683}, {'sequence': '[CLS] Eyi de ɔhaw kɛse baa no hɔ. [SEP]', 'score': 0.04025224596261978, 'token': 10192}]\n```"]