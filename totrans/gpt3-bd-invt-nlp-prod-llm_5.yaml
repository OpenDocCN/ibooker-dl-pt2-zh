- en: Chapter 5\. GPT-3 for Corporations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。企业的 GPT-3
- en: 'When a new innovation or technical shift happens, big corporations are usually
    the last to adopt it. Their hierarchical structures are composed of various authoritarian
    levels, and standard processes of legal approvals and paperwork often limit freedom
    to experiment, making it difficult for enterprises to be early adopters. But this
    doesn’t seem to be the case with GPT-3\. As soon as the API was released, corporations
    started experimenting with it. However, they ran into a significant barrier: data
    privacy.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当新的创新或技术转变发生时，大型企业通常是最后采用的。它们的等级结构由各种专制级别组成，而法律批准和文书工作的标准流程通常限制了试验自由，使得企业难以成为早期采用者。但是对于
    GPT-3 来说似乎并非如此。一旦 API 发布，企业就开始尝试使用它。然而，它们遇到了一个重大障碍：数据隐私。
- en: In its simplest form, all a language model does is to predict the next word,
    given a series of previous words. As you learned in [Chapter 2](ch02.xhtml#using_the_openai_api),
    OpenAI has devised several techniques to transform the functioning of language
    models like GPT-3 from simple next-word prediction to more useful NLP tasks such
    as answering questions, summarizing documents, and generating context-specific
    text. Typically, the best results are achieved by fine-tuning a language model
    or conditioning it to mimic a particular behavior by providing it with a few examples
    using domain-specific data. You can provide examples with the training prompt,
    but a more robust solution is to create a custom-trained model using the fine-tuning
    API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，语言模型所做的就是预测下一个单词，给定一系列先前的单词。正如你在[第 2 章](ch02.xhtml#using_the_openai_api)中学到的那样，OpenAI
    已经设计了几种技术，将语言模型（如 GPT-3）的功能从简单的下一个单词预测转变为更有用的 NLP 任务，如回答问题、摘要文件和生成特定上下文的文本。通常，通过微调语言模型或使其模仿特定行为来实现最佳结果，方法是使用领域特定数据提供几个示例来对其进行条件化。您可以使用训练提示提供示例，但更健壮的解决方案是使用微调
    API 创建定制训练模型。
- en: OpenAI offers GPT-3 in the form of an open-ended API, where users provide input
    data and the API returns output data. Properly securing, handling, and processing
    user data is a key concern for corporations looking to use GPT-3\. OpenAI’s Welinder
    notes that, while enterprise leaders have expressed a variety of concerns about
    GPT-3, “SOC2 compliance, geofencing, and the ability to run the API within a private
    network were the biggest of them.”
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 以开放式 API 的形式提供 GPT-3，用户提供输入数据，API 返回输出数据。对于打算使用 GPT-3 的企业来说，正确保护、处理和处理用户数据是一个关键问题。OpenAI
    的 Welinder 指出，尽管企业领导人对 GPT-3 表达了各种关注，“SOC2 合规性、地理围栏和能够在私人网络内运行 API 的能力是其中最大的关注点。”
- en: OpenAI’s measures for model safety and misuse are thus designed to cover a wide
    range of issues under the umbrella of data privacy and security. For example,
    Bram Adams, founder of Stenography, tells us about the privacy and security aspects
    of the OpenAI API. “As it stands, Stenography is a pass-through API—it’s like
    a toll road. So that people will pass in their code, which is passed as-is to
    the OpenAI API without saving or logging it anywhere.” Outside of those guardrails,
    Stenography is a superset of [OpenAI’s Terms of Use](https://oreil.ly/qjxIM).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 有关模型安全性和滥用的措施旨在涵盖数据隐私和安全的各种问题。例如，Stenography 的创始人 Bram Adams 告诉我们关于 OpenAI
    API 的隐私和安全方面。“目前，Stenography 是一个直通 API——就像一条收费公路。这样人们将会把他们的代码传递过去，而不会在任何地方保存或记录。”在这些防护栏之外，Stenography
    是[OpenAI 使用条款](https://oreil.ly/qjxIM)的一个超集。
- en: 'We talked to representatives of several corporations about what’s stopping
    them from using the OpenAI API in production. Most highlighted two common concerns:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与几家公司的代表谈过，了解了阻止他们在生产中使用 OpenAI API 的原因。大多数人都强调了两个常见的关注点：
- en: The GPT-3 API endpoint exposed by OpenAI should not retain or save any part
    of the training data provided to it as part of the model fine-tuning/training
    process.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 提供的 GPT-3 API 端点不应保留或保存作为模型微调/训练过程的一部分提供给它的任何训练数据的任何部分。
- en: Before sending their data to the OpenAI API, companies want to make sure that
    there’s no way for a third party to extract or access the data by providing any
    input to the API.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将其数据发送到 OpenAI API 之前，公司希望确保第三方无法通过向 API 提供任何输入来提取或访问数据。
- en: OpenAI responded to the above customer concerns and questions around data handling
    and privacy by offering security reviews, enterprise contracts, data processing
    agreements, third-party security certification efforts, and more. Some of the
    issues that customers and OpenAI discussed included whether the customer’s data
    can be used to improve OpenAI models, which may improve performance in the customer’s
    desired use cases but comes with concerns around data privacy and internal compliance
    obligations; limits around the storage and retention of customer data; and obligations
    regarding security handling and processing of data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 回应了客户关于数据处理和隐私的担忧和问题，提供了安全审查、企业合同、数据处理协议、第三方安全认证工作等。一些客户和 OpenAI 讨论的问题包括客户的数据是否可以用于改善
    OpenAI 模型，这可能会提高客户期望用例的性能，但会引发有关数据隐私和内部合规义务的担忧；关于客户数据的存储和保留限制；以及关于数据安全处理和处理的义务。
- en: The rest of this chapter delves into three case studies that show how global
    enterprises like GitHub, Microsoft, and Algolia are navigating these questions
    and using GPT-3 at scale. You’ll also learn how OpenAI has adapted to the demand
    for enterprise-grade products by collaborating on Microsoft Azure OpenAI Service.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将深入探讨三个案例研究，展示了像 GitHub、Microsoft 和 Algolia 这样的全球企业如何处理这些问题，并且如何规模化使用
    GPT-3。您还将了解到 OpenAI 如何通过与 Microsoft Azure OpenAI 服务的合作来适应企业级产品的需求。
- en: 'Case Study: GitHub Copilot'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：GitHub Copilot
- en: Let’s start this journey with GitHub Copilot, one of the hottest products of
    2021\. GitHub Copilot ([Figure 5-1](#github_copilot)) is a first-of-its-kind AI
    pair programmer that helps users write code faster and with much less work. Oege
    de Moor, vice president of GitHub Next, says the mission is “to reach all developers,
    with an ultimate goal to make programming accessible to everyone.” Automating
    mundane tasks, like writing redundant code and writing unit test cases, allows
    developers to “focus on the truly creative part of the job, which involves deciding
    what the software should actually do” and to “think more about the product concept
    rather than being stuck in figuring out the code.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 GitHub Copilot 开始这段旅程，这是 2021 年最热门的产品之一。GitHub Copilot（[图5-1](#github_copilot)）是一款首创的
    AI 协作编程工具，可帮助用户更快地编写代码，减少工作量。GitHub Next 副总裁 Oege de Moor 表示，该产品的使命是“触及所有开发者，最终目标是使编程变得无障碍”。自动化琐碎任务，如编写冗余代码和编写单元测试用例，使开发者能够“专注于工作的真正创造性部分，即决定软件实际应该做什么”，以及“更多地思考产品概念，而不是陷在代码中难以解脱”。
- en: 'As Awan told us: “I’m excited to work on more side projects now, because I
    know I’ll have the help of GitHub Copilot. It’s almost like I have a cofounder
    now. Codex and Copilot are writing 2 to 10% of my code, something like that. So
    it has already made me 2 to 10% more accelerated. And all of this is on an exponential
    scale. So what will GPT-3 be like next year? What will Codex be like next year?
    I may be 30% more accelerated.”'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Awan 告诉我们的：“我很兴奋能够在更多的副业项目上工作，因为我知道我将得到 GitHub Copilot 的帮助。现在我几乎就像有了一个联合创始人。Codex
    和 Copilot 大约写了我的代码的 2 到 10%，类似这样。所以它已经让我加速了 2 到 10%。而且所有这些都是呈指数增长的。那么明年 GPT-3
    会是什么样子？明年 Codex 会是什么样子？我可能会加速 30%。”
- en: Let’s dive into the inner workings of Copilot.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解 Copilot 的内部工作原理。
- en: '![](Images/gpt3_0501.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0501.png)'
- en: Figure 5-1\. GitHub Copilot
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1。GitHub Copilot
- en: How It Works
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理
- en: GitHub Copilot draws context from the code you’re working on, based on things
    like docstrings, comments, and function names. It then automatically suggests
    the next line, or even entire functions, right inside your editor to produce boilerplate
    code and suggest test cases that match the code implementation. It works with
    a broad set of frameworks and programming languages by using a plugin to the user’s
    code editor, making it nearly language-agnostic as well as lightweight and easy
    to use.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot 根据您正在处理的代码提取上下文，基于诸如文档字符串、注释和函数名称等内容。然后，它会在编辑器内自动建议下一行，甚至整个函数，以生成样板代码并建议匹配代码实现的测试用例。它通过使用用户代码编辑器的插件与广泛的框架和编程语言一起工作，使其几乎与语言无关，同时轻量且易于使用。
- en: 'OpenAI research scientist Harri Edwards notes that Copilot is also a useful
    tool for programmers working in a new language or framework: “Trying to code in
    an unfamiliar language by googling everything is like navigating a foreign country
    with just a phrasebook. Using GitHub Copilot is like hiring an interpreter.”'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 研究科学家哈里·爱德华兹指出，Copilot 对于在新语言或框架中工作的程序员也是一个有用的工具：“试图通过搜索一切来编写不熟悉的语言的代码，就像只带着一本短语书在陌生的国家里导航。使用
    GitHub Copilot 就像雇佣了一名翻译。”
- en: GitHub Copilot is powered by OpenAI’s Codex, a descendant of the GPT-3 model
    that, as we noted in [Chapter 4](ch04.xhtml#gpt_three_as_a_launchpad_for_next_gener),
    is designed specifically to interpret and write code. “GitHub is home to more
    than 73 million developers, [and] includes a massive amount of public data that
    embodies the collective knowledge of the community,” says de Moor. That translates
    to billions of lines of publicly available code for Codex to train on. It understands
    both programming and human languages.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot 由 OpenAI 的 Codex 提供支持，这是 GPT-3 模型的后裔，正如我们在 [Chapter 4](ch04.xhtml#gpt_three_as_a_launchpad_for_next_gener)
    中所述，它专门设计用于解释和编写代码。“GitHub 是超过 7300 万开发者的家园，[包括] 包含了社区集体知识的大量公开数据，”德莫尔说。这意味着 Codex
    有数十亿行可供训练的公开可用代码。它理解编程语言和人类语言。
- en: 'Codex draws on supporting comments or instructions in simple English to come
    up with relevant code as seen in [Figure 5-2](#how_github_copilot_works_left_parenthes).
    The Copilot editor extension intelligently chooses which context to send to the
    GitHub Copilot service, which in turn runs OpenAI’s Codex model to synthesize
    suggestions. Even though Copilot generates the code, the user is still in charge:
    you can cycle through suggested options, choose which to accept or reject, and
    manually edit the suggested code. GitHub Copilot adapts to the edits you make
    and matches your coding style. De Moor explains, “It links natural language with
    source code so you can use it in both directions. You can use the source code
    to generate comments or you can use the comments to generate the source code,
    making it immensely powerful.”'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Codex 依靠支持性评论或简单英语指令来生成相关代码，如 [Figure 5-2](#how_github_copilot_works_left_parenthes)
    所示。Copilot 编辑器扩展智能地选择要发送到 GitHub Copilot 服务的上下文，后者运行 OpenAI 的 Codex 模型来合成建议。尽管
    Copilot 生成代码，用户仍然有掌控权：你可以浏览建议的选项，选择接受或拒绝，以及手动编辑建议的代码。GitHub Copilot 会根据你所做的编辑进行调整，并匹配你的编码风格。德莫尔解释说：“它将自然语言与源代码链接起来，这样你就可以在两个方向上使用它。你可以使用源代码生成注释，也可以使用注释生成源代码，使其具有极大的功能。”
- en: '![](Images/gpt3_0502.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0502.png'
- en: 'Figure 5-2\. How GitHub Copilot works (source: GitHub Copilot)'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. GitHub Copilot 的工作原理（来源：GitHub Copilot）
- en: This functionality has also indirectly changed how developers write code. When
    they know that their code comments in human languages, like English, will be part
    of the model’s training, they write “better and more accurate comments in order
    to get better results from Copilot,” says de Moor.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能间接地改变了开发者编写代码的方式。当他们知道他们的代码注释将成为模型训练的一部分时，他们会“写出更好、更准确的注释，以获得 Copilot 更好的结果，”德莫尔说。
- en: Many critics worry that putting this tool in the hands of people who can’t judge
    the quality of code may result in introducing bugs or errors in the codebase.
    Contrary to that opinion de Moor tells us, “We have received a lot of feedback
    from developers that Copilot makes them write better and more efficient code.”
    In the current technical preview, Copilot can only help you write code if you
    understand how different pieces in software work, where you can tell Copilot precisely
    what it is that you want it to do. Copilot encourages healthy developer practices,
    like writing more accurate comments, and rewards developers with better code generation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 许多评论家担心，将这个工具交给不能判断代码质量的人可能会导致在代码库中引入错误或漏洞。与此相反，德莫尔告诉我们：“我们收到了很多开发者的反馈，说 Copilot
    让他们写出更好、更高效的代码。”在当前的技术预览中，Copilot 只有在你理解软件中不同部分如何运作时才能帮助你编写代码，你可以准确告诉 Copilot
    你希望它做什么。Copilot 鼓励健康的开发者实践，比如写更准确的注释，并以更好的代码生成奖励开发者。
- en: Copilot is not just limited to the general rules of programming; it can also
    figure out the details of specific fields, such as writing programs to compose
    music. To do that you need to understand music theory. “Seeing how Copilot has
    somehow picked it up from its immensely large training data is just amazing,”
    de Moor adds.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot 不仅仅局限于编程的一般规则；它还可以弄清楚特定领域的细节，比如编写用于创作音乐的程序。要做到这一点，你需要了解音乐理论。“看到 Copilot
    如何从其庞大的训练数据中获得这些知识真是令人惊讶，”德·穆尔补充道。
- en: Developing Copilot
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发 Copilot
- en: De Moor says one of the challenges of designing Copilot was creating the right
    user experience, one that “lets you use this model in a collaborative way without
    being intrusive.” The goal is for it to feel like working with a programming partner
    or coworker who “knows more about the mundane coding stuff so you can focus more
    on creating the important stuff.” Developers are constantly searching for existing
    solutions to problems and often refer to Stack Overflow, search engines, and blogs
    to find implementation and code syntax details—which means lots of moving back
    and forth between editor and browser. As de Moor points out, “As a developer,
    you are more productive when you can stay in your environment and just think about
    the problem rather than switching context all the time.” This is why GitHub’s
    team designed Copilot to deliver suggestions inside the development environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 德·穆尔表示，设计 Copilot 的一个挑战之一是创建正确的用户体验，即“让您以一种不引人注目的方式与这个模型进行协作”。目标是让它感觉像是与一个“了解琐碎编码内容的编程伙伴或同事一起工作，这样你就可以更多地专注于创建重要的东西”。开发人员经常在搜索现有问题的解决方案，并经常参考
    Stack Overflow、搜索引擎和博客以查找实现和代码语法细节——这意味着在编辑器和浏览器之间频繁切换。正如德·穆尔指出的，“作为开发人员，当你可以留在你的环境中，并且只是思考问题而不是一直切换上下文时，你会更有生产力。”这就是为什么
    GitHub 团队设计 Copilot 来在开发环境内提供建议的原因。
- en: 'No-Code/Low-Code: Simplifying Software Development?'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无代码/低代码：简化软件开发？
- en: Right now, developing software-related products or services requires a technical
    or scientific background—for example, you have to learn at least one programming
    language. And that’s just a start. Even to develop a minimum viable product (MVP)
    with conventional techniques you have to understand the different elements of
    software engineering involved in developing both the frontend (how the user interacts
    with the software) and the backend (how the processing logic works). This creates
    a barrier to entry for those who don’t come from a technical or engineering background.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开发与软件相关的产品或服务需要具备技术或科学背景——例如，你必须至少学习一种编程语言。而这只是一个开始。即使是用传统技术开发一个最小可行产品（MVP），你也必须了解涉及软件工程的不同元素，包括开发前端（用户如何与软件交互）和后端（处理逻辑运作方式）。这为那些没有技术或工程背景的人设置了一道入门障碍。
- en: De Moor sees Copilot as a step toward making technology more accessible and
    inclusive. If developers “have to worry less and less about the development details
    and just explain the design, explain the purpose of what [they] want to do,” and
    let Copilot handle the details, many more people will be able to use these tools
    to create new products and services.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 德·穆尔将 Copilot 视为使技术更加可访问和包容的一步。如果开发人员“越来越不必担心开发细节，只需要解释设计，解释他们想要做的事情的目的”，然后让
    Copilot 处理细节，那么将有更多的人能够使用这些工具来创建新产品和服务。
- en: There are already several no-code programming platforms, but many users find
    their limits constricting, in essence “heavily simplifying the programming experience”
    by making it “more visual, more graphical, and easy to use,” according to de Moor.
    “These things are great to get started but unfortunately, it comes up with a limit
    on the things that are possible to build using those platforms.” De Moor argues
    that Copilot is equally easy to use but provides far more options by using fully
    operational programming tools rather than simplified versions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有几个无代码编程平台了，但许多用户发现它们的限制很严格，本质上通过使编程体验“更加视觉化、更加图形化和更易于使用”来“大大简化编程体验”，德·穆尔说。“这些东西很适合入门，但不幸的是，它们限制了使用这些平台构建的东西的可能性。”德·穆尔认为
    Copilot 同样易于使用，但通过使用完全操作性的编程工具而不是简化版本，提供了更多选项。
- en: Scaling with the API
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 API 进行扩展
- en: Scaling in terms of language models has been undervalued for so long because
    of theoretical concepts like [Occam’s Razor](https://oreil.ly/M7fEu) and vanishing
    results when you expand the neural network to a significant size. With conventional
    deep learning, it has always been a norm to keep the model size small with fewer
    parameters to avoid the problem of vanishing gradients and introducing complexity
    in the model training process. Occam’s Razor, which implies “a simple model is
    the best model,” has been sacred in the AI community since its inception. This
    principle has been a center of reference for training new models, which has discouraged
    people from experimenting with scale.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从语言模型的规模扩展方面来看，长期以来一直被低估，因为理论概念，比如[奥卡姆剃刀](https://oreil.ly/M7fEu)和当你将神经网络扩展到一个显著规模时结果会消失。通过传统的深度学习，一直以来都是规模小，参数较少的模型是一种规范，以避免消失梯度问题，并在模型训练过程中引入复杂性。自从成立以来，“简单模型是最好的模型”的奥卡姆剃刀在人工智能社区中一直是神圣不可侵犯的原则。这个原则一直是训练新模型的参考中心，这阻碍了人们对规模进行实验的积极性。
- en: In 2020, when OpenAI released its marquee language model GPT-3, the potential
    of scaling came into the limelight and the common conception of the AI community
    started to shift. People started realizing that the “gift of scale” can give rise
    to a more generalized artificial intelligence, where a single model like GPT-3
    can perform an array of tasks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2020 年，当 OpenAI 推出其标志性语言模型 GPT-3 时，扩展的潜力开始受到关注，人们对人工智能社区的普遍观念开始发生变化。人们开始意识到“规模的赠予”可以产生更广义的人工智能，其中像
    GPT-3 这样的单一模型可以执行一系列任务。
- en: Hosting and managing a model like GPT-3 requires sophistication on many different
    levels, including the optimization of model architecture, its deployment, and
    how the general public can access it. De Moor tells us, “When we launched Copilot,
    it was using the OpenAI API infrastructure in the initial phases, and then we
    had this explosion of response after the launch with so many people signing up
    and wanting to use the product.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 托管和管理像 GPT-3 这样的模型需要在许多不同层面上的精湛技术，包括模型架构的优化、其部署以及公众如何访问它。德·莫尔告诉我们：“当我们推出 Copilot
    时，它在最初阶段使用的是 OpenAI API 基础设施，然后在推出后，我们迎来了大量的回应，有那么多人注册并想要使用这个产品。”
- en: Although the API was capable of handling large numbers of requests, the actual
    number of requests and their frequency still surprised the OpenAI team. De Moor
    and his team “realized the need [for] a more efficient and bigger infrastructure
    for deployment and, fortunately, it was about [that] time that Microsoft Azure
    OpenAI came to light” allowing them to make the required switch to Azure deployment
    infrastructure.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 API 能够处理大量的请求，但实际的请求数量及其频率仍然让 OpenAI 团队感到惊讶。德·莫尔和他的团队“意识到需要一个更高效、更大的基础设施来进行部署，幸运的是，微软
    Azure OpenAI 正好在那个时候浮出水面”，使他们能够进行所需的 Azure 部署基础设施转换。
- en: When we asked about the experience of building and scaling Copilot, de Moor
    shares, “Early on we had this misled belief that accuracy is the single most important
    thing that matters, but sometime later into the product journey, we realized that
    it’s actually a trade-off between the powerful AI model and [a] flawless user
    experience.” The Copilot team quickly realized that there is a trade-off between
    speed and the accuracy of suggestions as is the case with any deep learning model
    of sufficient scale.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们询问关于构建和扩展 Copilot 的经验时，德·莫尔分享道：“早期我们曾有一个误导性的信念，即准确性是唯一最重要的事情，但在产品发展的过程中，我们意识到实际上是强大的人工智能模型和[无瑕的用户体验之间存在着权衡。”
    Copilot 团队很快意识到，速度与建议准确性之间存在权衡，就像任何足够规模的深度学习模型一样。
- en: 'Generally, the more layers a deep learning model has, the more accurate it
    will be. However, more layers also means it will be slower to run. The Copilot
    team had to somehow find a balance between the two, as de Moor explains: “Our
    use case required the model to deliver the response at lightning-fast speed with
    multiple alternative suggestions; if it’s not fast enough, users can easily outpace
    the model and write the code themselves. So, we found that a slightly less powerful
    model that gives the responses quickly while maintaining the quality of results”
    was the answer.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，深度学习模型的层数越多，它的准确性就越高。然而，更多的层也意味着运行速度会变慢。Copilot 团队不得不在两者之间找到平衡，正如德·莫尔所解释的那样：“我们的用例要求模型以闪电般的速度提供响应，并提供多个备选建议；如果速度不够快，用户很容易就会超过模型并自己编写代码。因此，我们发现一个略微弱一些的模型，可以快速给出响应，同时保持结果的质量”是答案。
- en: The rapid user adoption and interest in GitHub Copilot took everyone in the
    team by surprise, but it didn’t end there. Because of the usefulness of the product
    and the quality of code suggestions, the team saw exponential growth in the amount
    of code generated using Copilot where on average, “35% of newly written code is
    being suggested by Copilot. This number will increase going forward as we get
    closer to finding the right balance between model capabilities and the speed of
    suggestions” says de Moor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对 GitHub Copilot 的快速用户采用和兴趣让团队中的每个人都感到惊讶，但事情并没有结束。由于产品的实用性和代码建议的质量，团队看到使用 Copilot
    生成的代码量呈指数增长，平均“35%的新编写代码是由 Copilot 提出的。随着我们在模型能力和建议速度之间找到正确平衡的不断接近，这个数字将继续增加。”
    德摩尔说。
- en: 'When asked about the data security and privacy aspect of code submitted as
    part of the request to Copilot, de Moor tells us, “Copilot architecture is designed
    in a way that when a user types the code into the Copilot, there would not be
    any possibility of code leaking between one user to another. GitHub Copilot is
    a code synthesizer, not a search engine: the vast majority of the code that it
    suggests is uniquely generated and has never been seen before. We found that about
    0.1% of the time, the suggestion may contain some snippets that are verbatim from
    the training set.”'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 被问及作为向 Copilot 提交请求的一部分提交的代码的数据安全性和隐私方面时，德摩尔告诉我们，“Copilot 的架构是设计成这样的，当用户将代码输入
    Copilot 时，不会有任何代码泄露给其他用户的可能性。GitHub Copilot 是一个代码合成器，而不是一个搜索引擎：它建议的绝大部分代码都是独一无二的生成的，从未被见过。我们发现，大约有
    0.1% 的时间，建议可能包含一些直接来自训练集的代码片段。”
- en: What’s Next for GitHub Copilot?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GitHub Copilot 的下一步是什么？
- en: De Moor sees a great potential for Copilot to assist in code review as well
    as writing. “Think of an automated code reviewer where it automatically looks
    at your changes and makes suggestions to make your code better and more efficient.
    The code review process at GitHub today consists of human reviewers, and we’re
    also exploring the idea of Copilot reviews.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 德摩尔认为 Copilot 在代码审查和编写方面有很大潜力。他说：“想象一下自动化的代码审查器，它自动查看您的更改并提出建议，使您的代码更好、更高效。如今
    GitHub 的代码审查过程由人工审阅者进行，我们也正在探索 Copilot 审查的想法。”
- en: Another feature under exploration is code explanation. De Moor explains that
    users can select a code snippet and “Copilot can explain it in simple English.”
    This has potential as a useful learning tool. In addition, de Moor says, Copilot
    hopes to provide tools that assist in “translation of code from one programming
    language to another.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 探索中的另一个功能是代码解释。德摩尔解释说，用户可以选择一段代码片段，“Copilot 可以用简单的英语解释它。” 这有潜力作为一个有用的学习工具。此外，德摩尔表示，Copilot
    希望提供帮助工具，用于“将代码从一种编程语言翻译成另一种编程语言”。
- en: Copilot has opened the world of unlimited opportunities not just for the developers
    but also for anyone who wants to get creative and build a piece of software to
    bring their ideas to reality. Prior to GitHub Copilot and OpenAI’s Codex, features
    like generating production-grade code, AI-assisted code review, and the translation
    of code from one language to another had been a far-fetched dream. The advent
    of LLMs combined with no-code and low-code platforms will enable people to unleash
    their creativity and build interesting and unexpected applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot 为开发人员以及任何想要发挥创造力并构建一款软件来实现其想法的人打开了无限的机会世界。在 GitHub Copilot 和 OpenAI
    的 Codex 出现之前，生成生产级代码、AI 辅助代码审查以及将代码从一种语言翻译成另一种语言等功能一直是遥不可及的梦想。LLM 的出现与无代码和低代码平台的结合将使人们能够释放创造力，构建有趣而意想不到的应用。
- en: 'Case Study: Algolia Answers'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：Algolia Answers
- en: Algolia is a renowned search solutions provider with clients spanning Fortune
    500 companies to a new generation of start-ups. It offers a symbolic, keyword-based
    search API that can be integrated with any existing product or application. In
    2020, Algolia partnered with OpenAI to connect GPT-3 with its already existing
    search technology. The next-generation product offering resulted in Algolia Answers,
    which enables clients to build an intelligent, semantics-driven, single-search
    endpoint for search queries. “We build the technology that other companies use,”
    says Dustin Coates, product manager at Algolia.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia 是一家知名的搜索解决方案提供商，客户跨越了财富 500 强公司到新一代的初创企业。它提供了一个符号化、基于关键字的搜索 API，可以与任何现有产品或应用程序集成。在
    2020 年，Algolia 与 OpenAI 合作，将 GPT-3 与其已有的搜索技术连接起来。下一代产品是 Algolia Answers，它使客户能够构建智能的、基于语义的、单一的搜索终端，用于搜索查询。“我们构建的是其他公司使用的技术，”Algolia
    的产品经理 Dustin Coates 说。
- en: Coates says that what his team means by *intelligent search* is along the lines
    of “You search for something and you get back the response right away—not just
    you get back to the record, you get back to the article—but you get back to what’s
    actually answering the question.” In short, it’s “a search experience where people
    don’t have to type exactly what the words are.”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Coates 说，他的团队所说的 *智能搜索* 是指“您搜索一些东西，然后立即得到响应—不仅是得到记录，而是得到文章—而是得到实际回答问题的内容。”简而言之，它是“一个搜索体验，人们不必准确输入单词。”
- en: Evaluating NLP Options
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估自然语言处理选项
- en: Algolia set a dedicated team to work in this area. When OpenAI reached out to
    them to find out if Algolia might be interested in GPT-3, Coates’s team compared
    it to competing technologies. Algolia ML engineer Claire Helme-Guizon, a member
    of the original Algolia Answers team, explains, “We worked on BERT-like models,
    to optimize for speed, DistilBERT, and with more stable models like RoBERTa along
    with different variants of GPT-3 like DaVinci, Ada, etc.” They created a rating
    system to compare the quality of different models and understand their strengths
    and weaknesses. They found, Coates says, that GPT-3 “performed really well in
    terms of the quality of the search results returned.” Speed and cost were weaknesses,
    but the API was ultimately a deciding factor since it allowed Algolia to use the
    model without having to maintain its infrastructure. Algolia asked existing clients
    whether they might be interested in such a search experience, and the response
    was very positive.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia 成立了一个专门的团队来处理这个领域。当 OpenAI 联系他们询问 Algolia 是否对 GPT-3 感兴趣时，Coates 的团队将其与竞争技术进行了比较。Algolia
    机器学习工程师 Claire Helme-Guizon，也是原始 Algolia Answers 团队的成员，解释说：“我们研究了类似 BERT 的模型，以优化速度，DistilBERT，以及像
    RoBERTa 这样更稳定的模型，以及 GPT-3 的不同变体，如 DaVinci、Ada 等。”他们创建了一个评分系统来比较不同模型的质量，并了解它们的优势和劣势。他们发现，Coates
    说，GPT-3 在搜索结果质量方面“表现得非常好。”速度和成本是弱点，但 API 最终是一个决定性因素，因为它允许 Algolia 在不必维护其基础架构的情况下使用该模型。Algolia
    询问了现有客户是否可能对这样的搜索体验感兴趣，回应非常积极。
- en: 'Even with that quality of results, Algolia still had plenty of questions: How
    would it work for the customers? Would the architecture be scalable? Was it financially
    feasible? To answer them, Coates explains, “We sculpted specific use cases that
    had longer textual content,” such as publishing and help desks.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 即使拥有这样的结果质量，Algolia 仍然有很多问题：它对客户会如何运作？架构是否可扩展？财务上可行吗？为了回答这些问题，Coates 解释说：“我们设计了具有较长文本内容的特定用例”，比如发布和帮助台。
- en: For some use cases, it’s good enough to rely solely on GPT-3 to get the search
    results, but for other complex use cases, it may be necessary to integrate GPT-3
    with other models. GPT-3, being trained on data up to a certain point in time,
    struggles with use cases involving freshness, popularity, or personalized results.
    When it comes to the quality of results, the Algolia team was challenged by the
    fact that semantic similarity scores generated by GPT-3 were not the only metric
    that mattered to their customers. They needed to somehow blend the similarity
    scores with other measures to ensure that the clients got satisfactory results.
    So they introduced other open source models to highlight the best results in combination
    with GPT-3.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些使用案例，仅依靠GPT-3获得搜索结果已经足够，但对于其他复杂的使用案例，可能需要将GPT-3与其他模型集成。由于GPT-3是根据某个时间点之前的数据进行训练的，因此在涉及新鲜度、流行度或个性化结果的使用案例方面会遇到困难。在结果质量方面，Algolia团队面临的挑战在于，由GPT-3生成的语义相似度分数并不是他们客户关心的唯一指标。他们需要以某种方式将相似度分数与其他指标结合起来，以确保客户获得满意的结果。因此，他们引入了其他开源模型，以与GPT-3结合展示最佳结果。
- en: Data Privacy
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据隐私
- en: The biggest challenges Algolia faced while introducing this novel technology,
    Coates says, were legal ones. “Getting through legal and security and procurements
    was maybe the hardest thing we did in this entire project because you’re sending
    this customer data and it’s feeding this ML model. How do we delete that data?
    How do we make sure it’s GDPR compliant?^([1](ch05.xhtml#ch01fn9)) How do we handle
    all of these things? How do we know that OpenAI isn’t going to take this data
    and feed everyone else’s model with it as well? So there were a lot of questions
    that needed to be answered and a lot of agreements that needed to be put into
    place.”
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 引入这项新技术时Algolia面临的最大挑战之一是法律问题。Coates表示：“在整个项目中，通过法律、安全和采购可能是我们做的最困难的事情之一，因为你正在发送这些客户数据并为这个ML模型提供数据。我们如何删除这些数据？我们如何确保其符合GDPR？^([1](ch05.xhtml#ch01fn9))我们如何处理所有这些事情？我们如何知道OpenAI不会拿取这些数据并将其馈送到其他所有模型中去？因此，有很多问题需要回答，也有很多协议需要制定。”
- en: Cost
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Most of the GPT-3 use cases that we’ve seen so far are business-to-consumer
    (B2C) products, but for a business-to-business (B2B) company like Algolia, the
    game is different. Not only do they need OpenAI’s pricing to work for them, but
    they also need to optimize their pricing for clients, so that they “can be profitable
    and have customers still be interested in what [they’re] building.”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们见过的大多数GPT-3使用案例都是面向消费者的业务（B2C）产品，但对于像Algolia这样的企业对企业（B2B）公司来说，游戏规则是不同的。他们不仅需要OpenAI的定价适合他们，而且还需要优化他们的定价以适应客户，这样他们“才能盈利并且客户仍然对我们正在构建的内容感兴趣。”
- en: In the search solutions business, success is measured on the basis of throughputs.
    So it naturally makes sense to think about the tradeoff between quality, cost,
    and speed. Coates says, “Even before we knew the costs, Ada was the right model
    for us because of the speed. But even if, let’s say, Davinci was fast enough,
    we may have still gotten down to Ada just because of the cost measures.”
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索解决方案业务中，成功是以吞吐量为基础衡量的。因此，自然而然地应该考虑质量、成本和速度之间的权衡。Coates表示：“即使在我们知道成本之前，Ada对我们来说也是正确的模型，因为速度很快。但是即使，比方说，Davinci足够快，我们可能仍然会选择Ada，只是因为成本措施。”
- en: Helme-Guizon notes that the factors affecting cost include “the number of tokens,
    and the number of documents you are sending and their length.” Algolia’s approach
    was to build “the smallest possible context windows”—meaning the amount of data
    sent to the API at one time—that would still be “relevant enough in terms of quality.”
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Helme-Guizon指出，影响成本的因素包括“token的数量以及您发送的文件数量和长度”。Algolia的方法是构建“尽可能小的上下文窗口”，即一次发送到API的数据量，“在质量上仍然足够相关”。
- en: So how did they solve this problem? Coates explains, “We started with OpenAI
    before they had announced pricing, and we had gone far enough and had seen that
    the quality was good enough from what we could see elsewhere, without knowing
    what the pricing was. So it was quite some sleepless nights, not knowing what
    the pricing was. And then once we knew the pricing, [it was a matter of] figuring
    out how to bring that cost down. Because when we first saw the pricing, we weren’t
    sure if we were going to [be able to] make it work.”
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那么他们是如何解决这个问题的呢？Coates 解释道：“我们在 OpenAI 公布价格之前就开始了合作，我们已经走得足够远，并且从我们在其他地方所见到的东西来看，质量是足够好的，而不知道价格是多少。所以我们有了一段相当多的失眠夜晚，不知道价格是多少。然后一旦我们知道价格，[就是]想出如何降低成本。因为当我们第一次看到价格时，我们不确定我们是否能够使其发挥作用。”
- en: They did put a lot of work into optimizing the price for their use case as,
    according to Coates, pricing will be “a universal challenge” for everyone trying
    to build their business on top of GPT-3\. So, it is highly recommended to start
    thinking about price optimization in the very early stages of product development.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 他们确实在优化他们的使用情况下的价格方面投入了大量工作，因为根据 Coates 的说法，价格将是每个试图在 GPT-3 基础上构建业务的人的“一个普遍性挑战”。因此，在产品开发的早期阶段就开始考虑价格优化是非常推荐的。
- en: Speed and Latency
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度和延迟
- en: Speed is of particular importance to Algolia; the company promises its clients
    lightning-fast search capabilities with delays limited to just milliseconds. When
    the team evaluated OpenAI’s proposal, they were happy with the quality of results,
    but GPT-3’s latency was unacceptable. “In our traditional search, the results
    come back round trip [in] less than 50 milliseconds,” Coates says. “We’re searching
    across hundreds of millions of documents and it has to be in real-time. When we
    worked with OpenAI early on, each of those queries took minutes.”
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 速度对于 Algolia 来说尤为重要；该公司承诺为客户提供毫秒级延迟的闪电般快速搜索能力。当团队评估了 OpenAI 的提议时，他们对结果的质量感到满意，但
    GPT-3 的延迟是无法接受的。“在我们传统的搜索中，结果往返时间少于 50 毫秒，” Coates 说。“我们正在跨越数亿个文档进行搜索，而且必须是实时的。当我们早期与
    OpenAI 合作时，每个查询都需要几分钟。”
- en: Algolia did decide to give GPT-3 a shot and began an initial phase of experimentation
    and beta rollout for Algolia Answers. However, bringing down latency and monetary
    costs required a lot of effort. “We started out at around 300 milliseconds, sometimes
    400, total latency,” Coates recalls, “which we had to bring down to somewhere
    in the range of 50 to 100 milliseconds for it to be feasible for our clients to
    use.” Ultimately, Algolia came up with semantic highlighting, a technique that
    uses a trained question-answering model on top of GPT-3 to perform mini searches
    and figure out the correct answer. The combination of GPT-3 with other open source
    models resulted in reduced overall latency. The quality of their results are better,
    Helme-Guizon adds, because “the models are trained to find the answers, not just
    the words that are related to one another.”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia 确实决定尝试使用 GPT-3，并开始了 Algolia Answers 的初始实验阶段和 beta 推出。然而，降低延迟和货币成本需要大量的努力。“我们最初的总延迟约为
    300 毫秒，有时达到 400 毫秒，” Coates 回忆道，“我们不得不将其降低到 50 到 100 毫秒的范围内，以便我们的客户可以使用它。”最终，Algolia
    提出了语义高亮，这是一种利用在 GPT-3 之上训练的问答模型执行小型搜索并找出正确答案的技术。将 GPT-3 与其他开源模型结合使用导致了总体延迟的降低。他们的结果质量更好，Helme-Guizon
    补充道，因为“模型被训练来找到答案，而不仅仅是相关的单词。”
- en: A key aspect of Algolia Answers’ architecture, Helme-Guizon says, is *reader
    retrieval architecture*, in which an AI reader is “going through the subset of
    documents and reading them, understanding them with reference to the query using
    Ada, and giving us a confidence score for the semantic value.” While this was
    “a good first solution,” she adds, it has a lot of challenges, “especially latency,
    because you have that dependency where you cannot process the first batch and
    the second batch together” asynchronously.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia Answers 架构的一个关键方面，Helme-Guizon 表示，是*阅读器检索架构*，在其中，AI 阅读器“浏览子文档并阅读它们，使用
    Ada 理解它们，并根据查询为我们提供语义值的置信度评分。”尽管这是“一个很好的第一解决方案，”她补充道，但它面临着许多挑战，“特别是延迟，因为您有一个依赖性，您无法异步地处理第一批和第二批。”
- en: 'GPT-3 uses the embedding from the predictions to compute c*osine similarity*,
    a mathematical metric used to determine how similar two documents are, irrespective
    of their size. Coates sums up these challenges: First, “you can’t send too many
    documents or else the response is going to be too slow or the cost is going to
    be too high monetarily.” The second is casting “a net wide enough to fetch all
    the relevant documents while keeping time and costs under control.”'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 使用从预测中得出的嵌入来计算余弦相似度，这是一种用于确定两个文档相似程度的数学指标，而不考虑它们的大小。Coates 总结了这些挑战：首先，“你不能发送太多的文档，否则响应会太慢，或者成本会在经济上太高。”第二个是“放置一个足够广泛的网，以获取所有相关文档，同时保持时间和成本的控制。”
- en: Lessons Learned
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所学到的经验
- en: So, if Algolia Answers had to start from scratch today, what would it do differently?
    “Working with GPT-3 can be overwhelming at times,” Coates says. “We would have
    asked some of the first-principle questions in the early stages of product development,
    like, ‘Are we willing to take a hit in terms of semantic understanding because
    we take such an increase for everything else?’ I think we would have thought a
    lot more about the latency and the confluence of different ranking factors early
    on.” He adds that he could see the project “going back to a BERT-based model.
    We might say that the raw quality isn’t the same as what we’re going to get out
    of GPT-3\. There’s no denying that. But I think that as much as we fell in love
    with the technology, we uncovered customer problems that we weren’t solving, and
    the technology has to follow the customer problems, not the other way around.”
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果 Algolia Answers 今天不得不从零开始，它会有什么不同的做法呢？“有时候与 GPT-3 一起工作会让人感到不知所措，” Coates
    说道。“我们会在产品开发的早期阶段提出一些首要问题，比如，‘我们是否愿意在语义理解上受到打击，因为我们为其他方面的提升付出了如此巨大的代价？’我认为我们会更多地考虑延迟和不同排名因素的交汇。”他补充说，他可以看到这个项目“回归到基于
    BERT 的模型。我们可能会说，原始质量并不像我们从 GPT-3 中得到的那样。这是无可否认的。但我认为，尽管我们对技术产生了浓厚的兴趣，但我们发现了一些我们没有解决的客户问题，技术必须追随客户问题的步伐，而不是反过来。”
- en: So what is Algolia’s take on the future of search? “We don’t believe that anyone
    has truly solved blending textual relevance and semantic relevance. It’s a very
    difficult problem because you can have situations where things are textually relevant,
    but don’t really answer the question,” says Coates. He envisions “a marriage of
    the more traditional, textual base, the more understandable and explainable side
    of it, with these more advanced language models.”
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 Algolia 对搜索未来的看法是什么呢？“我们不认为任何人真正解决了文本相关性和语义相关性的融合问题。这是一个非常困难的问题，因为有时候情况可能是文本相关的，但实际上并不能回答问题，”
    Coates 说道。他设想着“更传统的、文本基础、更可理解和可解释的一面，与这些更先进的语言模型相结合”。
- en: 'Case Study: Microsoft Azure OpenAI Service'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：微软 Azure OpenAI 服务
- en: Algolia has matured on the OpenAI API, but soon the company wanted to expand
    its business in Europe, which meant it needed GDPR compliance. It began working
    with Microsoft, which was launching its Azure OpenAI Service. In the next case
    study, we’ll take a look at that service.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Algolia 在 OpenAI API 上已经成熟了，但很快公司希望在欧洲扩大业务，这意味着它需要 GDPR 的合规性。它开始与微软合作，后者正在推出其
    Azure OpenAI 服务。在下一个案例研究中，我们将看看那项服务。
- en: A Partnership That Was Meant to Be
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注定要成为的伙伴关系
- en: Microsoft and OpenAI announced a partnership in 2019, with the goal of giving
    Microsoft Azure customers access to GPT-3’s capabilities. The partnership is based
    on the shared vision of wanting to ensure that AI and AGI are deployed safely
    and securely. Microsoft invested a billion dollars in OpenAI, funding the launch
    of the API, which runs on Azure. The partnership culminates in shipping the API
    to provide more people access to large language models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 微软和 OpenAI 在 2019 年宣布了一项合作伙伴关系，其目标是让微软 Azure 客户访问 GPT-3 的能力。这个合作伙伴关系基于共同的愿景，即希望确保
    AI 和 AGI 的安全部署。微软向 OpenAI 投资了十亿美元，资助了 API 的推出，该 API 运行在 Azure 上。这一合作伙伴关系的最终目标是向更多人提供访问大型语言模型的机会。
- en: Dominic Divakaruni, Principal Group Product Manager and Head of Azure OpenAI
    Service, says he’s always thought of this collaboration as a partnership that
    feels like it was meant to be, noting that Microsoft CEO Satya Nadella and OpenAI
    CEO Sam Altman have both spoken often about ensuring that the benefits of AI are
    accessible and widely distributed. Both companies are also concerned with safety
    in AI innovation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Dominic Divakaruni，Azure OpenAI 服务负责人和主要产品经理，表示他一直把这次合作看作是一种天作之合的伙伴关系，指出微软CEO萨提亚·纳德拉和OpenAI
    CEO Sam Altman经常谈论确保人工智能的好处能够被广泛获取和分配。两家公司都关注人工智能创新中的安全性。
- en: The goal, Divakaruni says, “was to leverage each other’s strengths,” in particular
    OpenAI’s user experience and modeling progress and Microsoft’s existing relationships
    with companies, large salesforce, and cloud infrastructure. Given its customer
    base, the team at Microsoft Azure understands enterprise cloud customers’ fundamental
    requirements in terms of compliance, certifications, network security, and related
    issues.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Divakaruni 表示，目标是“充分利用彼此的优势”，特别是 OpenAI 在用户体验和建模方面取得的进展以及微软在企业、大型销售团队和云基础设施方面的现有关系。考虑到其客户群体，微软
    Azure 团队了解企业云客户在合规性、认证、网络安全等方面的基本要求。
- en: For Microsoft, the interest in GPT-3 begins largely with it breaking new ground
    and being available before any other model from the LLM category. Another crucial
    factor in Microsoft’s investment is that it gained the ability to use OpenAI’s
    intellectual property assets exclusively. Although GPT-3 alternatives are available,
    Divakaruni says that the centralization of the OpenAI API is unique. He notes
    that models for services such as text analytics or translation require “quite
    a bit of work” on a cloud provider’s part to adapt into an API service. OpenAI,
    however, offers “the same API used for various tasks” rather than “bespoke APIs
    that are created for particular tasks.”
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微软来说，对 GPT-3 的兴趣主要在于它开创了新局面，并且在 LLM 类别中首先可用。微软投资的另一个关键因素是，它获得了独家使用 OpenAI
    的知识产权资产的能力。虽然存在 GPT-3 的替代方案，但 Divakaruni 表示 OpenAI API 的集中化是独一无二的。他指出，用于文本分析或翻译等服务的模型需要云提供商做“相当多的工作”才能适应
    API 服务。然而，OpenAI 提供的是“用于各种任务的相同 API”，而不是“为特定任务创建的定制 API”。
- en: An Azure-Native OpenAI API
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个 Azure 本地化的 OpenAI API
- en: 'OpenAI knew that it would be essential to scale the cloud fundamentals. From
    the inception of the OpenAI API, the idea has always been to have an instantiation
    of the API within Azure as well, in order to reach more customers. Divakaruni
    mentions that there are more similarities than differences between the OpenAI
    API and Azure OpenAI Service platforms. From a technology perspective, the objective
    is very similar: to provide people with the same API and access to the same models.
    The shape of the Azure OpenAI Service is going to be more Azure native, but Microsoft
    wants to match the developer experience of OpenAI customers, especially as some
    of them graduate from the OpenAI API into the Azure OpenAI Service.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 知道扩展云基础设施至关重要。从OpenAI API 的起源开始，一直都有在 Azure 中有一个 API 实例的想法，以便能够触及更多的客户。Divakaruni
    提到 OpenAI API 和 Azure OpenAI 服务平台之间的相似性比差异性更多。从技术角度来看，目标非常相似：为人们提供相同的 API 和相同模型的访问权限。Azure
    OpenAI 服务的形态将更加 Azure 本地化，但微软希望能够匹配 OpenAI 客户的开发者体验，尤其是一些客户从 OpenAI API 迁移到 Azure
    OpenAI 服务时。
- en: At the time of writing this book, we have captured the Azure OpenAI Service
    team still kicking off the platform, with lots to be fixed before they broadly
    release it to customers. OpenAI Service is now adding more and more models to
    its service; the goal is to eventually reach parity or to be only a few months
    behind OpenAI API in terms of the models available.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本书时，我们已经捕捉到 Azure OpenAI 服务团队正在启动平台，还有许多问题需要解决，然后才能广泛向客户发布。OpenAI 服务现在正在为其服务添加越来越多的模型；其目标是最终在可用模型方面达到或只落后于
    OpenAI API 几个月。
- en: Resource Management
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源管理
- en: One difference between the two services is in how they handle resource management.
    A *resource* is a manageable item that is available through the service (whether
    it is the OpenAI API or Microsoft Azure). In the context of OpenAI, examples of
    resources would be an API account or a pool of credits associated with an account.
    Azure offers a more complex set of resources, such as virtual machines, storage
    accounts, databases, virtual networks, subscriptions, and management groups.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 两种服务之间的一个区别在于它们如何处理资源管理。*资源*是通过服务可管理的项目（无论是 OpenAI API 还是 Microsoft Azure）。在
    OpenAI 的上下文中，资源的示例可能是一个 API 账户或与账户关联的积分池。Azure 提供了更复杂的资源集，如虚拟机、存储账户、数据库、虚拟网络、订阅和管理组。
- en: While OpenAI offers a single API account per organization, within Azure companies
    can create multiple different resources, which they can track, monitor, and allocate
    to different cost centers. “It’s just another Azure resource in general,” says
    Christopher Hoder, senior program manager at Microsoft Azure OpenAI Service, which
    makes it easy to use out of the box.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 OpenAI 为每个组织提供单个 API 账户，在 Azure 中公司可以创建多个不同的资源，可以跟踪、监视和分配给不同的成本中心。“它通常只是
    Azure 的另一个资源，”微软 Azure OpenAI 服务的高级项目经理 Christopher Hoder 说，这使得它易于开箱即用。
- en: '*Resource management* within Azure is a deployment and management functionality
    that enables customers to create, update, and delete resources in Azure accounts.
    It comes with features like access control, locks, and tags to secure and organize
    customer resources after deployment.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 内的*资源管理*是一种部署和管理功能，使客户能够在 Azure 账户中创建、更新和删除资源。它带有访问控制、锁定和标记等功能，用于在部署后保护和组织客户资源。
- en: Azure has several layers of resource management that allow companies and organizations
    to better manage pricing and resources, Hoder says. At a high level, there is
    an organizational Azure account, and within that account, there are multiple Azure
    subscriptions. Within that, there are resource groups, and then the resources
    themselves. “All of those can be monitored and segmented and access controlled,”
    Hoder adds, which becomes especially important for deployments at scale.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 有几个资源管理层，允许公司和组织更好地管理定价和资源，Hoder说。在高层次上，有一个组织的 Azure 账户，而在该账户内，有多个 Azure
    订阅。在其中，有资源组，然后是资源本身。“所有这些都可以进行监控、分段和访问控制，”Hoder 补充说，这在大规模部署时尤其重要。
- en: Security and Data Privacy
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性和数据隐私
- en: 'While Microsoft hasn’t said much publicly about its security so far, Divakaruni
    told us that the company is focused on three main points: content filters, monitoring
    of abuse, and a safety-first approach. The team is working on more safety-enforcing
    elements and plans to use customer feedback to understand which of these elements
    will be the most meaningful for users before they officially launch.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微软迄今未公开多少关于其安全性的信息，但 Divakaruni 告诉我们，公司专注于三个主要点：内容过滤器、滥用监控和以安全为先的方法。团队正在致力于更多强制安全性的元素，并计划利用客户反馈了解在它们正式推出之前哪些元素对用户最有意义。
- en: The team is also working on documentation that lays out the architecture of
    how the privacy policy is implemented, which will be shared with customers to
    provide assurances that Microsoft is protecting customer data while ensuring that
    its obligations for responsibly using artificial intelligence are maintained.
    “Lots of customers that come to us have concerns about the way it is currently
    implemented on OpenAI, because it is more open, and we are addressing [those concerns],”
    says Divakaruni.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 团队还在努力编写文档，详细说明隐私政策如何实施，将与客户分享，以确保微软在保护客户数据的同时履行负责任使用人工智能的义务。“许多来找我们的客户对目前 OpenAI
    的实施方式有所顾虑，因为它更加开放，我们正在解决[这些顾虑]，”Divakaruni说。
- en: Content filters have been introduced in the form of PII (personally identifiable
    information) filters that block sexual and other types of content, the scope of
    which is still being established. “The philosophy there is providing the customers
    the right knobs to adjust and iterate the content for their particular domain,”
    Divakaruni says.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 内容过滤器已经以 PII（个人可识别信息）过滤器的形式引入，它可以阻止性和其他类型的内容，其范围仍在确定中。“在这里的理念是为客户提供正确的旋钮，以调整和迭代适用于他们特定领域的内容，”Divakaruni说。
- en: Microsoft’s enterprise customers are demanding with regard to security. The
    Azure OpenAI API Service team is leveraging the work it’s done for other products,
    such as Bing and Office. Microsoft has a history of model development and pushing
    the envelope. “Office has provided language products for a while. So there is
    a pretty extensive content moderation capability…and we have a science team dedicated
    to building out filters that are appropriate for these models in this space,”
    says Divakaruni.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的企业客户在安全性方面要求严格。Azure OpenAI API Service 团队正在利用其为 Bing 和 Office 等其他产品所做的工作。微软在模型开发和突破方面有着悠久的历史。
    “Office 已经提供了语言产品一段时间了。因此，在这个领域我们有非常丰富的内容审核能力…我们有一个专门的科学团队致力于构建适合这些模型的过滤器。” Divakaruni
    说。
- en: OpenAI API users often request *geofencing*, a technology that sets a virtual
    boundary around a real-world geographical area, creating silos to keep the data
    in a particular location. If data is moved outside the specified radius, it can
    trigger an action in a geo-enabled phone or other portable electronic device.
    For example, it can alert administrators when a person enters or exits the geofence,
    and then generate an alert to the user’s mobile device in the form of a push notification
    or email. Geofencing enables businesses to accurately track, market to, and effectively
    alert administrators. Azure’s geofencing feature is still a work in progress,
    but Divakaruni says that it’s been implemented on an experimental basis for a
    few select customers, such as GitHub Copilot.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API 的用户经常请求 *地理围栏*，这是一种在真实地理区域周围设定虚拟边界的技术，创建分隔区域以保持数据在特定位置。如果数据移动到指定半径外，它可以在启用地理的电话或其他便携式电子设备上触发操作。例如，它可以在一个人进入或离开地理围栏时通知管理员，然后以推送通知或电子邮件的形式向用户的移动设备生成警报。地理围栏使企业能够准确跟踪、营销和有效地通知管理员。Azure
    的地理围栏功能仍在不断完善中，但 Divakaruni 表示它已经在试验性基础上为一些特定客户实施，例如 GitHub Copilot。
- en: Model-as-a-Service at the Enterprise Level
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 企业级的模型即服务
- en: While Azure OpenAI Service has been engaged with a lot of big enterprise customers
    on the platform, Microsoft isn’t ready to discuss them publicly, citing privacy
    concerns and the sensitivity of public opinion. What it can mention now are examples
    of its internal services. GitHub Copilot started off on the OpenAI API but now,
    mostly for scale reasons, has transitioned to Azure OpenAI Service. Other examples
    of internal services running on Azure are Dynamics 365 Customer Service, Power
    Apps, ML to code, and Power BI services.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI Service 已经与许多大型企业客户在平台上展开合作，但微软尚未准备公开讨论它们，引用隐私问题和公众舆论的敏感性。它现在可以提及的是一些内部服务的例子。GitHub
    Copilot 最初在 OpenAI API 上启动，但现在，主要出于规模原因，已经转移到 Azure OpenAI Service 上。在 Azure 上运行的其他内部服务的例子包括
    Dynamics 365 客户服务、Power Apps、ML to code 和 Power BI 服务。
- en: Divakaruni says they’re seeing a lot of interest from financial services industries
    and traditional enterprises looking to enhance their customer experience. “There
    is a lot of text information to process and there’s a lot of need for summarization
    and helping analysts, for example, quickly zero in on the text that is relevant
    and meaningful for them. The customer service industry, I think, is a big untapped
    domain as well. There’s a vast amount of information that is locked in audio,
    which can be transcribed, in call center information that could [yield] meaningful
    insights for a company that is trying to improve their customer experience.” Another
    set of use cases they are seeing is companies accelerating their developer productivity
    by training GPT-3 for their internal APIs and software development kits to make
    these tools more accessible to their employees.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Divakaruni 表示，他们看到了金融服务行业和传统企业对提升客户体验的极大兴趣。 “有大量的文本信息需要处理，分析师需要摘要并快速地确定对他们来说相关和有意义的文本。我认为客户服务行业也是一个巨大的未开发领域。有大量的信息被锁定在音频中，可以转录，以及在呼叫中心信息中，这些信息可能为试图改善客户体验的公司提供有意义的洞察。”
    他们看到的另一组使用案例是公司通过训练 GPT-3 来加速他们的内部 API 和软件开发工具的开发人员生产率，从而使这些工具对其员工更易于访问。
- en: Divakaruni notes that many businesses whose core strength is not in AI or ML
    want to apply AI in ways that add meaningful value to their business processes
    or enhance their customer experience. They leverage Microsoft’s field strength
    to help them build solutions. The Azure OpenAI Service team fully expects its
    sophisticated model-as-a-service approach to become mainstream, Hoder says. He
    notes that Microsoft provides its ready-to-use experience by embedding it into
    consumer applications such as Office and Dynamics. Customers that need more unique
    or tailored support go down a layer to services like the Power platform, which
    is aimed at business users and developers, providing no-code or low-code ways
    to tailor machine learning and AI. “If you go a little bit lower, a little bit
    more customized, a little bit more developer-focused, you end up at Cognitive
    Services. This has really been our model to provide AI capabilities through REST
    API–based services. And now we’re introducing a more granular layer with OpenAI
    Service.… And then at the bottom layer, we have the data science–focused tooling
    with Azure Machine Learning,” Hoder explains.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Divakaruni指出，许多其核心优势不在于人工智能或机器学习的企业希望以能够为其业务流程增加有意义价值或增强客户体验的方式应用人工智能。他们利用微软的实地优势来帮助他们构建解决方案。Azure
    OpenAI 服务团队完全预计其复杂的模型即服务方法将成为主流，Hoder表示。他指出，微软通过将其嵌入到诸如Office和Dynamics之类的消费者应用程序中来提供其即用即用的体验。需要更独特或定制支持的客户会向下一层转向像Power平台这样的服务，该平台面向业务用户和开发人员，提供无代码或低代码的方式来定制机器学习和人工智能。“如果你再往下走一点，再定制一点，再侧重于开发人员一点，你就会到达认知服务。这确实是我们通过基于REST
    API的服务提供人工智能能力的模型。现在我们正在引入一个更精细的层次，即OpenAI服务……然后在底层，我们有专注于数据科学的工具与Azure机器学习，”
    Hoder解释道。
- en: Microsoft sees a big customer demand for Azure OpenAI Service but also can vouch
    for its success so far with other services, such as speech-recognition services
    and the form recognizers. “We see a lot of demand for the ability to take an image,
    extract information in a structured way, and extract tables and other information
    from PDFs to do automated data ingestion, and then combine analytics and search
    capabilities.” Hoder says. (See, for example, this [case study](https://oreil.ly/1QA4i)
    of how customers are using Microsoft’s REST API-based AI/ML services.)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 微软看到了对Azure OpenAI服务的大量客户需求，但也可以证明其迄今为止在其他服务方面的成功，例如语音识别服务和表单识别器。“我们看到了对于能够拍摄图像，以结构化方式提取信息，并从PDF中提取表格和其他信息以进行自动数据摄入，然后结合分析和搜索功能的需求。”Hoder说。（例如，参见这个[案例研究](https://oreil.ly/1QA4i)，了解客户如何使用基于微软REST
    API的人工智能/机器学习服务。）
- en: Other Microsoft AI and ML Services
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他微软人工智能和机器学习服务
- en: 'Will Azure OpenAI Service affect other AI/ML services from Microsoft’s product
    line such as Azure Machine Learning Studio? Divakaruni tells us that there is
    a place for both on the market: “It’s definitely not a winner take all. There
    is a need for multiple solutions in the market that provide for specific customer
    requirements.” Customers’ requirements may differ substantially. They might need
    to generate and then label data specific to their particular use case. They can
    build a model from scratch using platforms like Azure ML Studio or SageMaker,
    and then train a distilled, smaller model for that purpose.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI服务会影响微软产品线中的其他人工智能/机器学习服务，如Azure Machine Learning Studio吗？Divakaruni告诉我们市场上两者都有位置：“绝对不是一家独大。市场上需要多种提供特定客户需求的解决方案。”客户的需求可能有很大不同。他们可能需要生成然后标记与其特定用例相关的数据。他们可以使用Azure
    ML Studio或SageMaker等平台从头开始构建模型，然后为此目的训练一个精简的、较小的模型。
- en: 'Of course, that’s a niche that’s not accessible to most people. Hoder notes
    that bringing data science capabilities to customers “broadens access; it democratizes
    it.” Divakaruni agrees: “You’ll increasingly see a trend toward the larger, most
    sophisticated models being exposed through services, as opposed to people” building
    their own. Why? “The fundamental truth is that it takes a tremendous amount of
    compute and lots of data to train these models. The companies that have the means
    to develop these models are unfortunately few. But it’s our responsibility, as
    we do [have the means], to make them available for the world.”'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是大多数人无法接触到的一个小众领域。Hoder指出，将数据科学能力带给客户“扩大了接入范围；使之民主化。”Divakaruni同意：“你将越来越多地看到一个趋势，即更大、更复杂的模型通过服务来暴露，而不是由人们自己构建。”
    为什么？“根本事实是，训练这些模型需要大量的计算和大量的数据。能够开发这些模型的公司可惜是少数。但是我们有能力，就应该让它们为世界所用。”
- en: Generally, data science teams from companies that can afford costly resources
    strongly prefer to build their own intellectual property for their specific use
    cases, using lower-level ML platforms like Azure Machine Learning Studio. That
    demand, Divakaruni argues, is unlikely to disappear.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，那些能够承担昂贵资源的公司的数据科学团队更倾向于为其特定用例构建自己的知识产权，使用像Azure Machine Learning Studio这样的低级ML平台。
    Divakaruni认为，这种需求不太可能消失。
- en: Advice for Enterprises
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 企业建议
- en: 'Enterprises investigating the Azure OpenAI Service, Divakaruni says, can approach
    it much as they would when investigating any other cloud service: you start with
    what makes the most sense for you and then look to see if the various technologies
    meet your needs. “While the technology is cool and that certainly has a wow factor,
    you still have to start with, ‘where can this be most applicable for me as a business,
    for my group?’ And then look to solve that with a set of technologies.”'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 调查Azure OpenAI服务的企业，Divakaruni说，可以像调查任何其他云服务一样进行：您首先从对您最有意义的地方开始，然后查看各种技术是否满足您的需求。“虽然技术很酷，当然有一个哇因素，但你仍然必须从‘这对我作为一个企业，对我的团队最有用在哪里？’开始，然后寻找一套技术来解决这个问题。”
- en: 'The next step is to examine how to get from experimentation into production:
    “What are the other things that you need to build?” Divakaruni refers to this
    step as an “application glue that someone needs to inject around, making sure
    these models actually behave and can be used in a live application scenario.”
    That’s a nontrivial task, but enterprises need to think about this to understand
    what kind of investment a GPT-3-based application will require. Divakaruni advises
    asking, “Is this model actually producing things that are relevant when you have
    automation around? The use of the capability, when it’s actually built into an
    application—is it doing what it’s supposed to be doing?”'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是研究如何从实验进入生产阶段：“您需要构建的其他东西是什么？” Divakaruni将这一步称为“应用程序粘合剂，某人需要在周围注入，确保这些模型实际上能够行为正常，并且可以在实时应用场景中使用。”这是一项非常重要的任务，但企业需要考虑这一点，以了解基于GPT-3的应用程序将需要什么样的投资。
    Divakaruni建议问：“当您进行自动化时，这个模型是否真的产生了相关的东西？当它实际上内置到一个应用程序中时，这个功能的使用——它是否在做它应该做的事情？”
- en: 'OpenAI or Azure OpenAI Service: Which Should You Use?'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用OpenAI还是Azure OpenAI服务：您应该使用哪个？
- en: 'The question for companies interested in exploring GPT-3, then, is this: OpenAI
    API or Azure OpenAI Service? Divakaruni maintains that the OpenAI API version
    is more suitable for companies that are exploring their options but don’t have
    any specific project implementation in mind. In terms of access, OpenAI is definitely
    farther along, with its Playground making it easier for individual users and companies
    to experiment. The OpenAI API also allows access to the latest experimental models
    and API endpoints that expand the API’s capabilities.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有兴趣探索GPT-3的公司来说，问题是：OpenAI API还是Azure OpenAI服务？Divakaruni认为，OpenAI API版本更适合那些正在探索各种选择但没有任何具体项目实施想法的公司。在访问方面，OpenAI显然走得更远，其游乐场使个人用户和公司更容易进行实验。
    OpenAI API还允许访问最新的实验模型和扩展API功能的API端点。
- en: Azure OpenAI Service, on the other hand, is targeting a cohort of users with
    production use cases who “graduate” from the OpenAI API or need to meet different
    compliance and privacy regulations The two organizations encourage customers to
    experiment and validate their use cases, and then firm them up with the OpenAI
    API. If that platform meets their needs, Microsoft is encouraging customers to
    stay on it, but when their production needs become more mature and they start
    to need more compliance, they should consider transitioning to Azure.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Azure OpenAI 服务针对的是具有生产用例的用户群体，这些用户从 OpenAI API “毕业”，或者需要满足不同的合规和隐私规定。两个组织鼓励客户进行实验和验证其用例，然后使用
    OpenAI API 确定它们。如果该平台满足其需求，微软鼓励客户继续使用它，但当他们的生产需求变得更加成熟，开始需要更多的合规性时，他们应考虑过渡到 Azure。
- en: Conclusion
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you saw how corporations are using GPT-3-based products at
    scale and how the new Microsoft Azure OpenAI Service is paving the way for enterprises
    interested in becoming part of the GPT-3 ecosystem. We have dived into the nuances
    of scaling a GPT-3-powered product and shared some tips from the journey of large-scale,
    enterprise-grade products. In [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and),
    we’ll look at some of the controversies and challenges surrounding the OpenAI
    API and LLMs more generally.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，您了解了企业如何大规模使用基于 GPT-3 的产品，以及新的微软 Azure OpenAI 服务如何为有意成为 GPT-3 生态系统的一部分的企业铺平道路。我们深入探讨了扩展基于
    GPT-3 的产品的微妙之处，并分享了一些来自大规模、企业级产品旅程的建议。在[第六章](ch06.xhtml#challengescomma_controversiescomma_and)中，我们将讨论围绕
    OpenAI API 和 LLMs 的一些争议和挑战。  '
- en: ^([1](ch05.xhtml#ch01fn9-marker)) The European Union’s [General Data Protection
    Regulation](https://gdpr.eu/tag/gdpr) requirements prohibit companies from hiding
    behind illegible terms and conditions that are difficult to understand. GDPR requires
    companies to clearly define their data privacy policies and make them easily accessible.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#ch01fn9-marker)) 欧盟的[《通用数据保护条例》](https://gdpr.eu/tag/gdpr)要求公司不得隐藏在难以理解的条款和条件背后。GDPR要求公司明确定义其数据隐私政策，并使其易于访问。
