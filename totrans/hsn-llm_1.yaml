- en: Chapter 2\. Semantic Search
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章\. 语义搜索
- en: 'Search was one of the first Large Language Model (LLM) applications to see
    broad industry adoption. Months after the release of the seminal [BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
    paper, Google announced it was using it to power Google Search and that it [represented](https://blog.google/products/search/search-language-understanding-bert/)
    “one of the biggest leaps forward in the history of Search”. Not to be outdone,
    Microsoft Bing also [stated](https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/)
    that “Starting from April of this year, we used large transformer models to deliver
    the largest quality improvements to our Bing customers in the past year”.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索是第一个被广泛采用的“大语言模型”（LLM）应用之一。在开创性论文[BERT：用于语言理解的深度双向变换器预训练](https://arxiv.org/abs/1810.04805)发布几个月后，谷歌宣布它在使用此模型来增强谷歌搜索，并且它[代表了](https://blog.google/products/search/search-language-understanding-bert/)“搜索历史上最大的飞跃之一”。微软必应也[表示](https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/)“从今年四月开始，我们使用大型变换器模型为我们的必应客户带来了过去一年最大的质量改进”。
- en: This is a clear testament to the power and usefulness of these models. Their
    addition instantly and massively improves some of the most mature, well-maintained
    systems that billions of people around the planet rely on. The ability they add
    is called *semantic search*, which enables searching by meaning, and not simply
    keyword matching.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地证明了这些模型的强大和实用性。它们的加入瞬间大幅提升了一些最成熟、维护良好的系统，这些系统是全球数十亿人依赖的。它们增加的能力称为*语义搜索*，使得根据意义进行搜索，而不仅仅是关键词匹配。
- en: In this chapter, we’ll discuss three major ways of using language models to
    power search systems. We’ll go over code examples where you can use these capabilities
    to power your own applications. Note that this is not only useful for web search,
    but that search is a major component of most apps and products. So our focus will
    not be just on building a web search engine, but rather on your own dataset. This
    capability powers lots of other exciting LLM applications that build on top of
    search (e.g., retrieval-augmented generation, or document question answering).
    Let’s start by looking at these three ways of using LLMs for semantic search.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论使用语言模型增强搜索系统的三种主要方法。我们将介绍代码示例，您可以利用这些功能来增强自己的应用程序。请注意，这不仅对网页搜索有用，搜索还是大多数应用程序和产品的重要组成部分。因此，我们的重点不仅是构建一个网页搜索引擎，而是关注您自己的数据集。此功能为许多其他基于搜索的激动人心的LLM应用提供动力（例如，检索增强生成或文档问答）。让我们开始看看这三种使用LLM进行语义搜索的方法。
- en: Three Major Categories of Language-Model-based Search Systems
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于语言模型的搜索系统的三大类。
- en: 'There’s a lot of research on how to best use LLMs for search. Three broad categories
    of these models are:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何最好地使用LLM进行搜索的研究很多。这些模型的三大类是：
- en: 1- Dense Retrieval
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 密集检索
- en: Say that a user types a search query into a search engine. Dense retrieval systems
    rely on the concept of embeddings, the same concept we’ve encountered in the previous
    chapters, and turn the search problem into retrieving the nearest neighbors of
    the search query (after both the query and the documents are converted into embeddings).
    [Figure 2-1](#fig_1_dense_retrieval_is_one_of_the_key_types_of_semanti) shows
    how dense retrieval takes a search query, consults its archive of texts, and outputs
    a set of relevant results.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设用户在搜索引擎中输入搜索查询。密集检索系统依赖于嵌入的概念，这是我们在前面的章节中遇到的相同概念，并将搜索问题转化为检索搜索查询的最近邻（在查询和文档都转换为嵌入后）。[图2-1](#fig_1_dense_retrieval_is_one_of_the_key_types_of_semanti)展示了密集检索如何获取搜索查询，查阅其文本档案，并输出一组相关结果。
- en: '![Dense retrieval is one of the key types of semantic search  relying on the
    similarity of text embeddings to retrieve relevant results](assets/semantic_search_888356_01.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![密集检索是语义搜索的关键类型之一，依赖文本嵌入的相似性来检索相关结果](assets/semantic_search_888356_01.png)'
- en: Figure 2-1\. Dense retrieval is one of the key types of semantic search, relying
    on the similarity of text embeddings to retrieve relevant results
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 密集检索是语义搜索的关键类型之一，依赖文本嵌入的相似性来检索相关结果。
- en: 2- Reranking
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 重新排序
- en: 'These systems are pipelines of multiple steps. A Reranking LLM is one of these
    steps and is tasked with scoring the relevance of a subset of results against
    the query, and then the order of results is changed based on these scores. [Figure 2-2](#fig_2_rerankers_the_second_key_type_of_semantic_search)
    shows how rerankers are different from dense retrieval in that they take an additional
    input: a set of search results from a previous step in the search pipeline.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统是多个步骤的管道。重排序 LLM 是这些步骤之一，负责对结果子集相对于查询的相关性进行评分，然后根据这些评分更改结果的顺序。[图 2-2](#fig_2_rerankers_the_second_key_type_of_semantic_search)
    显示了重排序器如何不同于密集检索，因为它们需要额外的输入：来自搜索管道前一步的搜索结果集。
- en: '![Rerankers  the second key type of semantic search  take a search query and
    a collection of results  and re order them by relevance  often resulting in vastly
    improved results.](assets/semantic_search_888356_02.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![重排序器：第二种关键的语义搜索类型，接收搜索查询和结果集合，并根据相关性重新排序，通常会显著改善结果。](assets/semantic_search_888356_02.png)'
- en: Figure 2-2\. Rerankers, the second key type of semantic search, take a search
    query and a collection of results, and re-order them by relevance, often resulting
    in vastly improved results.
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 重排序器，第二种关键的语义搜索类型，接收搜索查询和结果集合，并根据相关性重新排序，通常会显著改善结果。
- en: 3- Generative Search
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 3- 生成搜索
- en: The growing LLM capability of text generation led to a new batch of search systems
    that include a generation model that simply generates an answer in response to
    a query. [Figure 2-3](#fig_3_generative_search_formulates_an_answer_to_a_questi)
    shows a generative search example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不断增长的文本生成 LLM 能力导致了一批新的搜索系统，其中包括一个生成模型，它简单地对查询生成答案。[图 2-3](#fig_3_generative_search_formulates_an_answer_to_a_questi)
    显示了一个生成搜索的例子。
- en: '![Generative search formulates an answer to a question and cites its information
    sources.](assets/semantic_search_888356_03.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![生成搜索针对问题生成答案并引用其信息来源。](assets/semantic_search_888356_03.png)'
- en: Figure 2-3\. Generative search formulates an answer to a question and cites
    its information sources.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 生成搜索针对问题生成答案并引用其信息来源。
- en: All three concepts are powerful and can be used together in the same pipeline.
    The rest of the chapter covers these three types of systems in more detail. While
    these are the major categories, they are not the only LLM applications in the
    domain of search.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种概念都很强大，可以在同一流程中结合使用。本章其余部分将更详细地介绍这三种系统。虽然这些是主要类别，但它们并不是搜索领域中唯一的 LLM 应用。
- en: Dense Retrieval
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密集检索
- en: Recall that embeddings turn text into numeric representations. Those can be
    thought of as points in space as we can see in [Figure 2-4](#fig_4_the_intuition_of_embeddings_each_text_is_a_point).
    Points that are close together mean that the text they represent is similar. So
    in this example, text 1 and text 2 are similar to each other (because they are
    near each other), and different from text 3 (because it’s farther away).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，嵌入将文本转换为数字表示。这些可以被视为空间中的点，如我们在[图 2-4](#fig_4_the_intuition_of_embeddings_each_text_is_a_point)中所见。接近的点意味着它们所代表的文本是相似的。因此在这个例子中，文本
    1 和文本 2 彼此相似（因为它们靠近），而与文本 3 不同（因为它更远）。
- en: '![The intuition of embeddings  each text is a point  texts with similar meaning
    are close to each other.](assets/semantic_search_888356_04.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![嵌入的直观理解：每段文本都是一个点，含义相似的文本彼此接近。](assets/semantic_search_888356_04.png)'
- en: 'Figure 2-4\. The intuition of embeddings: each text is a point, texts with
    similar meaning are close to each other.'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 嵌入的直观理解：每段文本都是一个点，含义相似的文本彼此接近。
- en: This is the property that is used to build search systems. In this scenario,
    when a user enters a search query, we embed the query, thus projecting it into
    the same space as our text archive. Then we simply find the nearest documents
    to the query in that space, and those would be the search results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于构建搜索系统的属性。在这种情况下，当用户输入搜索查询时，我们将查询嵌入，从而将其投影到与我们的文本档案相同的空间中。然后，我们只需在该空间中找到与查询最接近的文档，这些文档就是搜索结果。
- en: '![Dense retrieval relies on the property that search queries will be close
    to their relevant results.](assets/semantic_search_888356_05.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![密集检索依赖于搜索查询与相关结果之间的接近性。](assets/semantic_search_888356_05.png)'
- en: Figure 2-5\. Dense retrieval relies on the property that search queries will
    be close to their relevant results.
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 密集检索依赖于搜索查询与相关结果之间的接近性。
- en: 'Judging by the distances in [Figure 2-5](#fig_5_dense_retrieval_relies_on_the_property_that_search),
    “text 2” is the best result for this query, followed by “text 1”. Two questions
    could arise here, however:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[图2-5](#fig_5_dense_retrieval_relies_on_the_property_that_search)中的距离，“文本2”是这个查询的最佳结果，其次是“文本1”。但是，这里可能会出现两个问题：
- en: Should text 3 even be returned as a result? That’s a decision for you, the system
    designer. It’s sometimes desirable to have a max threshold of similarity score
    to filter out irrelevant results (in case the corpus has no relevant results for
    the query).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 文本3是否应该被返回作为结果？这是你作为系统设计者的决定。有时需要设置一个最大相似度分数的阈值，以过滤掉不相关的结果（以防语料库中没有与查询相关的结果）。
- en: Are a query and its best result semantically similar? Not always. This is why
    language models need to be trained on question-answer pairs to become better at
    retrieval. This process is explained in more detail in chapter 13.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 查询及其最佳结果在语义上相似吗？不一定。这就是为什么语言模型需要在问答对上进行训练，以便在检索方面变得更好的原因。这个过程在第13章中有更详细的说明。
- en: Dense Retrieval Example
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密集检索示例
- en: 'Let’s take a look at a dense retrieval example by using Cohere to search the
    Wikipedia page for the film *Interstellar*. In this example, we will do the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用Cohere搜索维基百科关于电影*星际穿越*的页面来看一个密集检索示例。在这个示例中，我们将执行以下操作：
- en: Get the text we want to make searchable, apply some light processing to chunk
    it into sentences.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取我们想要使其可搜索的文本，对其进行一些轻处理以将其拆分成句子。
- en: Embed the sentences
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入句子
- en: Build the search index
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建搜索索引
- en: Search and see the results
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索并查看结果
- en: 'To start, we’ll need to install the libraries we’ll need for the example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装示例所需的库：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Get your Cohere API key by signing up at https://cohere.ai/. Paste it in the
    cell below. You will not have to pay anything to run through this example.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在https://cohere.ai/注册获取你的Cohere API密钥。将其粘贴到下面的单元格中。你在运行这个示例时无需支付任何费用。
- en: 'Let’s import the datasets we’ll need:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入所需的数据集：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Getting the text Archive
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取文本档案
- en: Let’s use the first section of the Wikipedia article on the film *Interstellar*.
    https://en.wikipedia.org/wiki/Interstellar_(film). We’ll get the text, then break
    it into sentences.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们使用维基百科关于电影*星际穿越*的第一部分。https://en.wikipedia.org/wiki/Interstellar_(film)。我们将获取文本，然后将其拆分成句子。
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Embed the texts
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入文本
- en: Let’s now embed the texts. We’ll send them to the Cohere API, and get back a
    vector for each text.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们现在嵌入文本。我们将把它们发送到Cohere API，并为每个文本返回一个向量。
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Which outputs:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: (15, 4096)
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (15, 4096)
- en: Indicating that we have 15 vectors, each one is of size 4096.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表明我们有15个向量，每个向量的大小为4096。
- en: Build The Search Index
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建搜索索引
- en: Before we can search, we need to build a search index. An index stores the embeddings
    and is optimized to quickly retrieve the nearest neighbors even if we have a very
    large number of points.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们可以搜索之前，我们需要构建一个搜索索引。索引存储嵌入，并被优化为快速检索最近邻，即使我们有非常大量的点。
- en: '[PRE4]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Search the index
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索索引
- en: We can now search the dataset using any query we want. We simply embed the query,
    and present its embedding to the index, which will retrieve the most similar texts.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们可以使用任何我们想要的查询来搜索数据集。我们只需嵌入查询，并将其嵌入呈现给索引，索引将检索出最相似的文本。
- en: 'Let’s define our search function:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们定义我们的搜索函数：
- en: '[PRE5]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now ready to write a query and search the texts!
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在准备好写查询并搜索文本了！
- en: '[PRE6]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Which produces the output:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这产生的输出为：
- en: '[PRE7]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|  |'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE8]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE9]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE11]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE12]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE13]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE14]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE15]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE16]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE17]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE18]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: The first result has the least distance, and so is the most similar to the query.
    Looking at it, it answers the question perfectly. Notice that this wouldn’t have
    been possible if we were only doing keyword search because the top result did
    not include the words “much” or “make”.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个结果的距离最小，因此与查询最相似。查看它，它完美地回答了问题。请注意，如果我们仅进行关键字搜索，这是不可能的，因为最佳结果中不包含“much”或“make”这两个词。
- en: 'To further illustrate the capabilities of dense retrieval, here’s a list of
    queries and the top result for each one:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明密集检索的能力，这里有一个查询列表及每个查询的最佳结果：
- en: 'Query: “Tell me about the $$$?”'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：“告诉我关于$$$的事情？”
- en: 'Top result: The film had a worldwide gross over $677 million (and $773 million
    with subsequent re-releases), making it the tenth-highest grossing film of 2014'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳结果：这部电影在全球的总票房超过6.77亿美元（与后来的重映一起为7.73亿美元），成为2014年票房第十高的电影。
- en: 'Distance: 1.244138'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 距离：1.244138
- en: 'Query: “Which actors are involved?”'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：“哪些演员参与了？”
- en: 'Top result: It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain,
    Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部结果：它由马修·麦康纳、安妮·海瑟薇、杰西卡·查斯坦、比尔·欧文、艾伦·伯斯廷、马特·达蒙和迈克尔·凯恩主演。
- en: 'Distance: 0.917728'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 距离：0.917728
- en: 'Query: “How was the movie released?”'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：“这部电影是如何上映的？”
- en: 'Top result: In the United States, it was first released on film stock, expanding
    to venues using digital projectors'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部结果：在美国，它最初是在胶卷上发布的，扩展到使用数字放映机的场所。
- en: 'Distance: 0.871881'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 距离：0.871881
- en: Caveats of Dense Retrieval
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密集检索的注意事项
- en: 'It’s useful to be aware of some of the drawbacks of dense retrieval and how
    to address them. What happens, for example, if the texts don’t contain the answer?
    We still get results and their distances. For example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 了解密集检索的一些缺点及其解决方法是有用的。例如，如果文本中不包含答案，会发生什么？我们仍然会得到结果及其距离。例如：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In cases like this, one possible heuristic is to set a threshold level -- a
    maximum distance for relevance, for example. A lot of search systems present the
    user with the best info they can get, and leave it up to the user to decide if
    it’s relevant or not. Tracking the information of whether the user clicked on
    a result (and were satisfied by it), can improve future versions of the search
    system.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个可能的启发式方法是设定一个阈值水平——例如，相关性的最大距离。许多搜索系统向用户提供他们能获取的最佳信息，并由用户决定其相关性。跟踪用户是否点击了结果（并对此感到满意）的信息，可以改善未来版本的搜索系统。
- en: Another caveat of dense retrieval is cases where a user wants to find an exact
    match to text they’re looking for. That’s a case that’s perfect for keyword matching.
    That’s one reason why hybrid search, which includes both semantic search and keyword
    search, is used.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 密集检索的另一个注意事项是用户希望找到与其正在寻找的文本完全匹配的情况。这种情况非常适合关键词匹配。这也是为什么同时包括语义搜索和关键词搜索的混合搜索被使用的原因之一。
- en: Dense retrieval systems also find it challenging to work properly in domains
    other than the ones that they were trained on. So for example if you train a retrieval
    model on internet and Wikipedia data, and then deploy it on legal texts (without
    having enough legal data as part of the training set), the model will not work
    as well in that legal domain.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 密集检索系统在训练以外的领域中正常工作也面临挑战。例如，如果你在互联网和维基百科数据上训练检索模型，然后在法律文本上部署（而训练集中没有足够的法律数据），模型在法律领域的表现将不佳。
- en: 'The final thing we’d like to point out is that this is a case where each sentence
    contained a piece of information, and we showed queries that specifically ask
    those for that information. What about questions whose answers span multiple sentences?
    This shows one of the important design parameters of dense retrieval systems:
    what is the best way to chunk long texts? And why do we need to chunk them in
    the first place?'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想指出的最后一点是，这是一个每个句子包含一条信息的情况，我们展示了具体询问这些信息的查询。那么，对于答案跨越多个句子的提问呢？这显示了密集检索系统的一个重要设计参数：分块长文本的最佳方法是什么？我们为什么要首先进行分块？
- en: Chunking Long Texts
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长文本分块
- en: One limitation of Transformer language models is that they are limited in context
    sizes. Meaning we cannot feed them very long texts that go above a certain number
    of words or tokens that the model supports. So how do we embed long texts?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer语言模型的一个限制是它们的上下文大小有限。这意味着我们不能输入超过模型支持的某个字数或标记数量的非常长的文本。那么我们如何嵌入长文本呢？
- en: There are several possible ways, and two possible approaches shown in [Figure 2-6](#fig_6_it_s_possible_to_create_one_vector_representing_an)
    include indexing one vector per document, and indexing multiple vectors per document.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种可能的方法，图[2-6](#fig_6_it_s_possible_to_create_one_vector_representing_an)中展示的两种可能方法包括每个文档索引一个向量，以及每个文档索引多个向量。
- en: '![It s possible to create one vector representing an entire document  but it
    s better for longer documents to be split into smaller chunks that get their own
    embeddings.](assets/semantic_search_888356_06.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![可以创建一个向量表示整个文档，但对于较长的文档，将其分割成较小的块以获取各自的嵌入更好。](assets/semantic_search_888356_06.png)'
- en: Figure 2-6\. It’s possible to create one vector representing an entire document,
    but it’s better for longer documents to be split into smaller chunks that get
    their own embeddings.
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 可以创建一个向量来表示整个文档，但对于较长的文档，将其分割成较小的块以获取各自的嵌入更好。
- en: One vector per document
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个文档一个向量
- en: 'In this approach, we use a single vector to represent the whole document. The
    possibilities here include:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们使用单个向量来表示整个文档。这里的可能性包括：
- en: 'Embedding only a representative part of the document and ignoring the rest
    of the text. This may mean embedding only the title, or only the beginning of
    the document. This is useful to get quickly started with building a demo but it
    leaves a lot of information unindexed and so unsearchable. As an approach, it
    may work better for documents where the beginning captures the main points of
    a document (think: Wikipedia article). But it’s really not the best approach for
    a real system.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅嵌入文档的代表部分而忽略其余文本。这可能意味着仅嵌入标题或文档的开头。这对于快速开始构建演示非常有用，但会留下大量未索引的信息，因此不可搜索。作为一种方法，它可能更适合于那些开头捕获文档主要观点的文档（例如：维基百科文章）。但这并不是一个真正系统的最佳方法。
- en: Embedding the document in chunks, embedding those chunks, and then aggregating
    those chunks into a single vector. The usual method of aggregation here is to
    average those vectors. A downside of this approach is that it results in a highly
    compressed vector that loses a lot of the information in the document.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文档分块、嵌入这些块，然后将这些块聚合为单个向量。这里常用的聚合方法是对这些向量取平均。该方法的一个缺点是会产生一个高度压缩的向量，导致文档中大量信息丢失。
- en: This approach can satisfy some information needs, but not others. A lot of the
    time, a search is for a specific piece of information contained in an article,
    which is better captured if the concept had its own vector.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以满足某些信息需求，但不能满足其他需求。很多时候，搜索的是包含在文章中的特定信息，如果该概念有自己的向量，捕获效果会更好。
- en: Multiple vectors per document
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个文档多个向量
- en: In this approach, we chunk the document into smaller pieces, and embed those
    chunks. Our search index then becomes that of chunk embeddings, not entire document
    embeddings.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们将文档分块为更小的部分，并嵌入这些块。我们的搜索索引因此变为块嵌入，而不是整个文档的嵌入。
- en: The chunking approach is better because it has full coverage of the text and
    because the vectors tend to capture individual concepts inside the text. This
    leads to a more expressive search index. Figure X-3 shows a number of possible
    approaches.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 分块方法更好，因为它全面覆盖了文本，并且向量倾向于捕获文本中的单个概念。这导致了更具表现力的搜索索引。图X-3展示了一些可能的方法。
- en: '![A number of possible options for chunking a document for embedding.](assets/semantic_search_888356_07.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![多种用于嵌入文档的分块选项。](assets/semantic_search_888356_07.png)'
- en: Figure 2-7\. A number of possible options for chunking a document for embedding.
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7\. 多种用于嵌入文档的分块选项。
- en: 'The best way of chunking a long text will depend on the types of texts and
    queries your system anticipates. Approaches include:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对长文本的最佳分块方式将取决于系统预期的文本类型和查询。方法包括：
- en: Each sentence is a chunk. The issue here is this could be too granular and the
    vectors don’t capture enough of the context.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个句子是一个块。这里的问题是这可能过于细化，向量无法捕获足够的上下文。
- en: Each paragraph is a chunk. This is great if the text is made up of short paragraphs.
    Otherwise, it may be that every 4-8 sentences are a chunk.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个段落是一个块。如果文本由短段落组成，这很棒。否则，可能每4-8句话是一个块。
- en: 'Some chunks derive a lot of their meaning from the text around them. So we
    can incorporate some context via:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些块的意义来自于周围的文本。因此，我们可以通过以下方式结合一些上下文：
- en: Adding the title of the document to the chunk
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文档的标题添加到块中。
- en: Adding some of the text before and after them to the chunk. This way, the chunks
    can overlap so they include some surrounding text. This is what we can see in
    [Figure 2-8](#fig_8_chunking_the_text_into_overlapping_segments_is_one).
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在块中添加一些前后的文本。这样，块可以重叠，从而包括一些周围文本。这就是我们在[图2-8](#fig_8_chunking_the_text_into_overlapping_segments_is_one)中看到的。
- en: '![Chunking the text into overlapping segments is one strategy to retain more
    of the context around different segments.](assets/semantic_search_888356_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![将文本分块为重叠片段是一种保留不同片段周围更多上下文的策略。](assets/semantic_search_888356_08.png)'
- en: Figure 2-8\. Chunking the text into overlapping segments is one strategy to
    retain more of the context around different segments.
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8\. 将文本分块为重叠片段是一种保留不同片段周围更多上下文的策略。
- en: Expect more chunking strategies to arise as the field develops -- some of which
    may even use LLMs to dynamically split a text into meaningful chunks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随着该领域的发展，预计会出现更多的分块策略——其中一些甚至可能使用LLM动态地将文本分割成有意义的块。
- en: Nearest Neighbor Search vs. Vector Databases
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近邻搜索与向量数据库
- en: The most straightforward way to find the nearest neighbors is to calculate the
    distances between the query and the archive. That can easily be done with NumPy
    and is a reasonable approach if you have thousands or tens of thousands of vectors
    in your archive.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最近邻的最简单方法是计算查询与档案之间的距离。这可以很容易地用NumPy实现，如果你的档案中有成千上万或几万个向量，这也是一种合理的方法。
- en: As you scale beyond to the millions of vectors, an optimized approach for the
    retrieval is to rely on approximate nearest neighbor search libraries like Annoy
    or FAISS. These allow you to retrieve results from massive indexes in milliseconds
    and some of them can scale to GPUs and clusters of machines to serve very large
    indices.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当你扩展到数百万个向量时，优化检索的方式是依赖于近似最近邻搜索库，如Annoy或FAISS。这些库允许你在毫秒内从巨大的索引中检索结果，有些可以扩展到GPU和机器集群，以服务非常大的索引。
- en: Another class of vector retrieval systems are vector databases like Weaviate
    or Pinecone. A vector database allows you to add or delete vectors without having
    to rebuild the index. They also provide ways to filter your search or customize
    it in ways beyond merely vector distances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类向量检索系统是像Weaviate或Pinecone这样的向量数据库。向量数据库允许你添加或删除向量，而无需重建索引。它们还提供了超越单纯向量距离的搜索过滤或自定义的方法。
- en: Fine-tuning embedding models for dense retrieval
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为密集检索微调嵌入模型
- en: Just like we’ve seen in the text classification chapter, we can improve the
    performance of an LLM on a task using fine-tuning. Just like in that case, retrieval
    needs to optimize text embeddings and not simply token embeddings. The process
    for this finetuning is to get training data composed of queries and relevant results.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在文本分类章节中看到的那样，我们可以通过微调提高大型语言模型在某项任务上的表现。和那种情况一样，检索需要优化文本嵌入，而不仅仅是令牌嵌入。这个微调过程的目标是获取由查询和相关结果组成的训练数据。
- en: 'Looking at one example from our dataset, the sentence “Interstellar premiered
    on October 26, 2014, in Los Angeles.”. Two possible queries where this is a relevant
    result are:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 看一个来自我们数据集的例子，句子“《星际穿越》于2014年10月26日在洛杉矶首映。”。两个可能的相关查询是：
- en: 'Relevant Query 1: “Interstellar release date”'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关查询1：“《星际穿越》发布日期”
- en: 'Relevant Query 2: “When did Interstellar premier”'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关查询2：“《星际穿越》什么时候首映”
- en: The fine-tuning process aims to make the embeddings of these queries close to
    the embedding of the resulting sentence. It also needs to see negative examples
    of queries that are not relevant to the sentence, for example.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 微调过程的目的是使这些查询的嵌入接近结果句子的嵌入。它还需要看到与句子不相关的查询的负示例。
- en: 'Irrelevant Query: “Interstellar cast”'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无关查询：“星际穿越演员表”
- en: Having these examples, we now have three pairs - two positive pairs and one
    negative pair. Let’s assume, as we can see in [Figure 2-9](#fig_9_before_fine_tuning_the_embeddings_of_both_relevan),
    that before fine-tuning, all three queries have the same distance from the result
    document. That’s not far-fetched because they all talk about Interstellar.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些示例，我们现在有三对——两对正样本和一对负样本。假设，如我们在[图2-9](#fig_9_before_fine_tuning_the_embeddings_of_both_relevan)中看到的，微调之前，这三条查询与结果文档的距离相同。这并不牵强，因为它们都是在谈论《星际穿越》。
- en: '![Before fine tuning  the embeddings of both relevant and irrelevant queries
    may be close to a particular document. ](assets/semantic_search_888356_09.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![在微调之前，相关和无关查询的嵌入可能接近某个特定文档。](assets/semantic_search_888356_09.png)'
- en: Figure 2-9\. Before fine-tuning, the embeddings of both relevant and irrelevant
    queries may be close to a particular document.
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9。微调之前，相关和无关查询的嵌入可能接近某个特定文档。
- en: The fine-tuning step works to make the relevant queries closer to the document
    and at the same time making irrelevant queries farther from the document. We can
    see this effect in [Figure 2-10](#fig_10_after_the_fine_tuning_process_the_text_embedding).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 微调步骤的目的是使相关查询更靠近文档，同时使无关查询远离文档。我们可以在[图2-10](#fig_10_after_the_fine_tuning_process_the_text_embedding)中看到这一效果。
- en: '![After the fine tuning process  the text embedding model becomes better at
    this search task by incorporating how we define relevance on our dataset using
    the examples we provided of relevant and irrelevant documents.](assets/semantic_search_888356_10.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![经过微调过程后，文本嵌入模型在这个搜索任务中变得更有效，因为它结合了我们如何定义数据集中相关性，使用了我们提供的相关和无关文档的示例。](assets/semantic_search_888356_10.png)'
- en: Figure 2-10\. After the fine-tuning process, the text embedding model becomes
    better at this search task by incorporating how we define relevance on our dataset
    using the examples we provided of relevant and irrelevant documents.
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reranking
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of companies have already built search systems. For those companies, an
    easier way to incorporate language models is as a final step inside their search
    pipeline. This step is tasked with changing the order of the search results based
    on relevance to the search query. This one step can vastly improve search results
    and it’s in fact what Microsoft Bing added to achieve the improvements to the
    search results using BERT-like models.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-11](#fig_11_llm_rerankers_operate_as_a_part_of_a_search_pipeli) shows
    the structure of a rerank search system serving as the second stage in a two-stage
    search system.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![LLM Rerankers operate as a part of a search pipeline with the goal of re
    ordering a number of shortlisted search results by relevance](assets/semantic_search_888356_11.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. LLM Rerankers operate as a part of a search pipeline with the
    goal of re-ordering a number of shortlisted search results by relevance
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reranking Example
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A reranker takes in the search query and a number of search results, and returns
    the optimal ordering of these documents so the most relevant ones to the query
    are higher in ranking.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Cohere’s [Rerank](https://docs.cohere.com/reference/rerank-1) endpoint is a
    simple way to start using a first reranker. We simply pass it the query and texts,
    and get the results back. We don’t need to train or tune it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can print these results:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This shows the reranker is much more confident about the first result, assigning
    it a relevance score of 0.92 while the other results are scored much lower in
    relevance.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: More often, however, our index would have thousands or millions of entries,
    and we need to shortlist, say one hundred or one thousand results and then present
    those to the reranker. This shortlisting is called the *first stage* of the search
    pipeline.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The dense retriever example we looked at in the previous section is one possible
    first-stage retriever. In practice, the first stage can also be a search system
    that incorporates both keyword search as well as dense retrieval.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Open Source Retrieval and Reranking with Sentence Transformers
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to locally setup retrieval and reranking on your own machine, then
    you can use the Sentence Transformers library. Refer to the documentation in https://www.sbert.net/
    for setup. Check the [*Retrieve & Re-Rank* section](https://www.sbert.net/examples/applications/retrieve_rerank/README.html)
    for instructions and code examples for how to conduct these steps in the library.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: How Reranking Models Work
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One popular way of building LLM search rerankers present the query and each
    result to an LLM working as a *cross-encoder*. Meaning that a query and possible
    result are presented to the model at the same time allowing the model to view
    the full text of both these texts before it assigns a relevance score. This method
    is described in more detail in a paper titled [*Multi-Stage Document Ranking with
    BERT*](https://arxiv.org/abs/1910.14424) and is sometimes referred to as monoBERT.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的构建 LLM 搜索重排序器的方法是将查询和每个结果同时呈现给作为*交叉编码器*工作的 LLM。这意味着查询和可能的结果同时呈现给模型，使其在分配相关性得分之前能够查看这两段文本的完整内容。该方法在一篇题为
    [*多阶段文档排名与 BERT*](https://arxiv.org/abs/1910.14424) 的论文中有更详细的描述，有时被称为 monoBERT。
- en: This formulation of search as relevance scoring basically boils down to being
    a classification problem. Given those inputs, the model outputs a score from 0-1
    where 0 is irrelevant and 1 is highly relevant. This should be familiar from looking
    at the Classification chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将搜索形式化为相关性评分基本上归结为分类问题。给定这些输入，模型输出一个从 0 到 1 的得分，其中 0 是不相关，1 是高度相关。这应该在查看分类章节时是熟悉的。
- en: 'To learn more about the development of using LLMs for search, [*Pretrained
    Transformers for Text Ranking: BERT and Beyond*](https://arxiv.org/abs/2010.06467)
    is a highly recommended look at the developments of these models until about 2021.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于使用 LLM 进行搜索的发展，可以参考 [*预训练变换器进行文本排名：BERT 及其后续*](https://arxiv.org/abs/2010.06467)，这是对这些模型直到
    2021 年的发展进行的高度推荐的观察。
- en: Generative Search
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成搜索
- en: You may have noticed that dense retrieval and reranking both use representation
    language models, and not generative language models. That’s because they’re better
    optimized for these tasks than generative models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，密集检索和重排序都使用表示语言模型，而不是生成语言模型。这是因为它们在这些任务上比生成模型更优化。
- en: At a certain scale, however, generative LLMs started to seem more and more capable
    of a form of useful information retrieval. People started asking models like ChatGPT
    questions and sometimes got relevant answers. The media started painting this
    as a threat to Google which seems to have started an arms race in using language
    models for search. Microsoft [launched](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/)
    Bing AI, powered by generative models. Google launched [Bard](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/),
    its own answer in this space.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某个规模上，生成 LLM 开始显得越来越能够进行有用的信息检索。人们开始向像 ChatGPT 这样的模型提问，有时得到了相关的答案。媒体开始将其描绘为对谷歌的威胁，这似乎引发了一场在搜索中使用语言模型的军备竞赛。微软
    [推出](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/)
    了由生成模型驱动的 Bing AI。谷歌推出了 [Bard](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/)，这是它在这个领域的回应。
- en: What is Generative Search?
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是生成搜索？
- en: Generative search systems include a text generation step in the search pipeline.
    At the moment, however, generative LLMs aren’t reliable information retrievers
    and are prone to generate coherent, yet often incorrect, text in response to questions
    they don’t know the answer to.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 生成搜索系统在搜索流程中包括文本生成步骤。然而，目前，生成 LLM 不是可靠的信息检索工具，容易生成连贯但通常不正确的文本来回应它们不知道答案的问题。
- en: The first batch of generative search systems is using search models as simply
    a summarization step at the end of the search pipeline. We can see an example
    in [Figure 2-12](#fig_12_generative_search_formulates_answers_and_summaries).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第一批生成搜索系统仅将搜索模型作为搜索流程末尾的总结步骤。我们可以在 [图 2-12](#fig_12_generative_search_formulates_answers_and_summaries)
    中看到一个例子。
- en: '![Generative search formulates answers and summaries at the end of a search
    pipeline while citing its sources  returned by the previous steps in the search
    system .](assets/semantic_search_888356_12.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![生成搜索在搜索流程的末尾制定答案和摘要，同时引用之前步骤返回的源。](assets/semantic_search_888356_12.png)'
- en: Figure 2-12\. Generative search formulates answers and summaries at the end
    of a search pipeline while citing its sources (returned by the previous steps
    in the search system).
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 生成搜索在搜索流程的末尾制定答案和摘要，同时引用其来源（由搜索系统之前的步骤返回）。
- en: Until the time of this writing, however, language models excel at generating
    coherent text but they are not reliable in retrieving facts. They don’t yet really
    know what they know or don’t know, and tend to answer lots of questions with coherent
    text that can be incorrect. This is often referred to as *hallucination*. Because
    of it, and for the fact that search is a use case that often relies on facts or
    referencing existing documents, generative search models are trained to cite their
    sources and include links to them in their answers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在撰写本文时，语言模型在生成连贯文本方面表现出色，但在检索事实时并不可靠。它们尚未真正知道自己知道或不知道什么，往往用连贯的文本回答许多问题，但可能是错误的。这通常被称为*幻觉*。因此，由于搜索常常依赖于事实或引用现有文档，生成搜索模型被训练以引用其来源并在答案中包含链接。
- en: Generative search is still in its infancy and is expected to improve with time.
    It draws from a machine learning research area called retrieval-augmented generation.
    Notable systems in the field include [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/),
    [RETRO](https://jalammar.github.io/illustrated-retrieval-transformer/), [Atlas](https://arxiv.org/pdf/2208.03299.pdf),
    amongst others.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 生成搜索仍处于初期阶段，预计会随着时间的推移而改善。它源自一个叫做检索增强生成的机器学习研究领域。该领域的显著系统包括[RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)、[RETRO](https://jalammar.github.io/illustrated-retrieval-transformer/)和[Atlas](https://arxiv.org/pdf/2208.03299.pdf)等。
- en: Other LLM applications in search
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索中其他LLM应用
- en: 'In addition to these three categories, there are plenty of other ways to use
    LLMs to power or improve search systems. Examples include:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三类之外，还有很多其他方式可以使用LLM来推动或改善搜索系统。例子包括：
- en: Generating synthetic data to improve embedding models. This includes methods
    like [GenQ](https://www.pinecone.io/learn/genq/) and [InPars-v2](https://arxiv.org/abs/2301.01820)
    that look at documents, generate possible queries and questions about those documents,
    then use that generated data to fine-tune a retrieval system.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成数据以改进嵌入模型。这包括像[GenQ](https://www.pinecone.io/learn/genq/)和[InPars-v2](https://arxiv.org/abs/2301.01820)等方法，它们查看文档，生成关于这些文档的可能查询和问题，然后使用生成的数据微调检索系统。
- en: 'The growing reasoning capabilities of text generation models are leading to
    search systems that can tackle complex questions and queries by breaking them
    down into multiple sub-queries that are tackled in sequence, leading up to a final
    answer of the original question. One method in this category is described in [*Demonstrate-Search-Predict:
    Composing retrieval and language models for knowledge-intensive NLP*](https://arxiv.org/abs/2212.14024).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文本生成模型日益增长的推理能力使搜索系统能够通过将复杂问题和查询分解为多个子查询来逐步解决，最终得到原始问题的答案。该类别中的一种方法在[*Demonstrate-Search-Predict:
    组合检索和语言模型以进行知识密集型NLP*](https://arxiv.org/abs/2212.14024)中有所描述。'
- en: Evaluation metrics
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: 'Semantic search is evaluated using metrics from the Information Retrieval (IR)
    field. Let’s discuss two of these popular metrics: Mean Average Precision (MAP),
    and Normalized Discounted Cumulative Gain (nDCG).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索使用信息检索（IR）领域的指标进行评估。让我们讨论这两个流行指标：平均精确度（MAP）和标准化折扣累积增益（nDCG）。
- en: Evaluating search systems needs three major [components](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html),
    a text archive, a set of queries, and relevance judgments indicating which documents
    are relevant for each query. We see these components in FIgure 3-13.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 评估搜索系统需要三个主要[组件](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)：文本档案、一组查询和相关性判断，指示哪些文档与每个查询相关。我们在图3-13中看到了这些组件。
- en: '![To evaluate search systems  we need a test suite including queries and relevance
    judgements indicating which documents in our archive are relevant for each query.](assets/semantic_search_888356_13.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![要评估搜索系统，我们需要一个测试套件，包括查询和相关性判断，指示我们档案中的哪些文档与每个查询相关。](assets/semantic_search_888356_13.png)'
- en: Figure 2-13\. To evaluate search systems, we need a test suite including queries
    and relevance judgements indicating which documents in our archive are relevant
    for each query.
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-13。要评估搜索系统，我们需要一个测试套件，包括查询和相关性判断，指示我们档案中的哪些文档与每个查询相关。
- en: Using this test suite, we can proceed to explore evaluating search systems.
    Let’s start with a simple example, let’s assume we pass Query 1 to two different
    search systems. And get two sets of results. Say we limit the number of results
    to three results only as we can see in [Figure 2-14](#fig_14_to_compare_two_search_systems_we_pass_the_same_qu).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个测试套件，我们可以开始探索评估搜索系统。让我们从一个简单的例子开始，假设我们将查询1传递给两个不同的搜索系统，并获得两个结果集。假设我们将结果数量限制为仅三个，如在[图2-14](#fig_14_to_compare_two_search_systems_we_pass_the_same_qu)中所示。
- en: '![To compare two search systems  we pass the same query from our test suite
    to both systems and look at their top results](assets/semantic_search_888356_14.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![为了比较两个搜索系统，我们将测试套件中的同一查询传递给两个系统，并查看它们的顶部结果。](assets/semantic_search_888356_14.png)'
- en: Figure 2-14\. To compare two search systems, we pass the same query from our
    test suite to both systems and look at their top results
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14。为了比较两个搜索系统，我们将测试套件中的同一查询传递给两个系统，并查看它们的顶部结果。
- en: To tell which is a better system, we turn the relevance judgments that we have
    for the query. [Figure 2-15](#fig_15_looking_at_the_relevance_judgements_from_our_test)
    shows which of the returned results are relevant.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了判断哪个系统更好，我们查看针对该查询的相关性判断。[图2-15](#fig_15_looking_at_the_relevance_judgements_from_our_test)显示了哪些返回的结果是相关的。
- en: '![Looking at the relevance judgements from our test suite  we can see that
    System 1 did a better job than System 2.](assets/semantic_search_888356_15.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![从我们的测试套件查看相关性判断，我们可以看到系统1比系统2表现更好。](assets/semantic_search_888356_15.png)'
- en: Figure 2-15\. Looking at the relevance judgements from our test suite, we can
    see that System 1 did a better job than System 2.
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-15。从我们的测试套件查看相关性判断，我们可以看到系统1比系统2表现更好。
- en: This shows us a clear case where system 1 is better than system 2\. Intuitively,
    we may just count how many relevant results each system retrieved. System A got
    two out of three correctly, and System 2 got only one out of three correctly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我们展示了一个清晰的案例，系统1优于系统2。直观上，我们可能只计算每个系统检索到的相关结果数量。系统A正确获取了三个中的两个，而系统2仅正确获取了三个中的一个。
- en: But what about a case like Figure3-16 where both systems only get one relevant
    result out of three, but they’re in different positions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于像图3-16这样的情况，两个系统都只获得了三个中的一个相关结果，但它们的位置不同，该如何处理呢？
- en: '![We need a scoring system that rewards system 1 for assigning a high position
    to a relevant result    even though both systems retrieved only one relevant result
    in their top three results.](assets/semantic_search_888356_16.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![我们需要一个评分系统，该系统奖励系统1为相关结果分配高位，即使两个系统在其前三个结果中仅检索到一个相关结果。](assets/semantic_search_888356_16.png)'
- en: Figure 2-16\. We need a scoring system that rewards system 1 for assigning a
    high position to a relevant result -- even though both systems retrieved only
    one relevant result in their top three results.
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-16。我们需要一个评分系统，该系统奖励系统1为相关结果分配高位，即使两个系统在其前三个结果中仅检索到一个相关结果。
- en: In this case, we can intuit that System 1 did a better job than system 2 because
    the result in the first position (the most important position) is correct. But
    how can we assign a number or score to how much better that result is? Mean Average
    Precision is a measure that is able to quantify this distinction.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以直观地判断系统1比系统2表现更好，因为第一个位置（最重要的位置）的结果是正确的。但我们如何为该结果的优越性分配一个数字或评分呢？均值平均精度是一个能够量化这种区别的度量。
- en: One common way to assign numeric scores in this scenario is Average Precision,
    which evaluates System 1’s result for the query to be 0.6 and System 2’s to be
    0.1\. So let’s see how Average Precision is calculated to evaluate one set of
    results, and then how it’s aggregated to evaluate a system across all the queries
    in the test suite.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，分配数字评分的一个常见方法是平均精度，它评估系统1对该查询的结果为0.6，而系统2的结果为0.1。因此，让我们看看如何计算平均精度来评估一组结果，然后如何将其聚合以评估整个测试套件中的系统。
- en: Mean Average Precision (MAP)
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均值平均精度（MAP）
- en: To score system 1 on this query, we need to calculate multiple scores first.
    Since we are looking at only three results, we’ll need to look at three scores
    - one associated with each position.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对系统1进行评分，我们需要首先计算多个分数。由于我们只关注三个结果，因此我们需要查看三个分数——与每个位置相关联的一个分数。
- en: 'The first one is easy, looking at only the first result, we calculate the precision
    score: we divide the number of correct results by the total number of results
    (correct and incorrect). [Figure 2-17](#fig_17__to_calculate_mean_average_precision_we_start_by)
    shows that in this case, we have one correct result out of one (since we’re only
    looking at the first position now). So precision here is 1/1 = 1.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个很简单，只看第一个结果，我们计算精度分数：将正确结果的数量除以结果的总数（正确和不正确）。[图 2-17](#fig_17__to_calculate_mean_average_precision_we_start_by)显示，在这种情况下，我们在第一个位置上有一个正确结果。因此，精度为
    1/1 = 1。
- en: '![  To calculate Mean Average Precision  we start by calculating precision
    at each position  starting by position  1.](assets/semantic_search_888356_17.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![  计算均值平均精度时，我们从计算每个位置的精度开始，从第 1 个位置开始。](assets/semantic_search_888356_17.png)'
- en: 'Figure 2-17\. To calculate Mean Average Precision, we start by calculating
    precision at each position, starting by position #1.'
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-17\. 计算均值平均精度时，我们从计算每个位置的精度开始，从第 1 个位置开始。
- en: We need to continue calculating precision results for the rest of the position.
    The calculation at the second position looks at both the first and second position.
    The precision score here is 1 (one out of two results being correct) divided by
    2 (two results we’re evaluating) = 0.5.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要继续计算其余位置的精度结果。第二个位置的计算考虑了第一个和第二个位置。这里的精度分数为 1（两个结果中有一个是正确的）除以 2（我们正在评估的两个结果）=
    0.5。
- en: '[Figure 2-18](#fig_18_caption_to_come) continues the calculation for the second
    and third positions. It then goes one step further -- having calculated the precision
    for each position, we average them to arrive at an Average Precision score of
    0.61.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-18](#fig_18_caption_to_come)继续计算第二和第三个位置的精度。接下来更进一步——在计算每个位置的精度后，我们将它们平均得到平均精度分数为
    0.61。'
- en: '![Caption to come ](assets/semantic_search_888356_18.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![标题待补充](assets/semantic_search_888356_18.png)'
- en: Figure 2-18\. Caption to come
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-18\. 标题待补充
- en: This calculation shows the average precision for a single query and its results.
    If we calculate the average precision for System 1 on all the queries in our test
    suite and get their mean, we arrive at the Mean Average Precision score that we
    can use to compare System 1 to other systems across all the queries in our test
    suite.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 该计算显示了单个查询及其结果的平均精度。如果我们计算系统 1 在测试套件中所有查询的平均精度并得出它们的均值，我们可以得到均值平均精度分数，从而可以将系统
    1 与测试套件中其他系统进行比较。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we looked at different ways of using language models to improve
    existing search systems and even be the core of new, more powerful search systems.
    These include:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用语言模型来改善现有搜索系统的不同方法，甚至作为新型、更强大搜索系统的核心。这些包括：
- en: Dense retrieval, which relies on the similarity of text embeddings. These are
    systems that embed a search query and retrieve the documents with the nearest
    embeddings to the query’s embedding.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集检索依赖于文本嵌入的相似性。这些系统将搜索查询嵌入并检索与查询嵌入最接近的文档。
- en: Rerankers, systems (like monoBERT) that look at a query and candidate results,
    and scores the relevance of each document to that query. These relevance scores
    are then used to order the shortlisted results according to their relevance to
    the query often producing an improved results ranking.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排序器（如 monoBERT），这些系统查看查询和候选结果，并对每个文档与该查询的相关性进行评分。这些相关性评分用于根据其与查询的相关性对入围结果进行排序，通常能产生改进的结果排名。
- en: Generative search, where search systems that have a generative LLM at the end
    of the pipeline to formulate an answer based on retrieved documents while citing
    its sources.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式搜索，指的是在管道末端具有生成性 LLM 的搜索系统，基于检索到的文档来形成答案，同时引用其来源。
- en: We also looked at one of the possible methods of evaluating search systems.
    Mean Average Precision allows us to score search systems to be able to compare
    across a test suite of queries and their known relevance to the test queries.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了一种可能的搜索系统评估方法。均值平均精度允许我们为搜索系统评分，以便在查询的测试套件及其已知相关性之间进行比较。
