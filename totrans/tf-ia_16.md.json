["```py\n(John, angry), (John, threw), (John, a), (threw, angry), (threw, John), (threw, a), (threw, pizza), ..., (at, a), (at, pizza), (at, me)\n```", "```py\nRalph loves his tennis ball. It likes to chase the ball\nRalph loves his tennis ball. Ralph likes to chase it\n```", "```py\nimport tensorflow as tf\n\n# Defining some hyperparameters\nn_steps = 25 # Sequence length\nn_en_vocab = 300 # Encoder's vocabulary size\nn_heads = 8 # Number of attention heads\nd = 512 # The feature dimensionality of each layer\n\n# Encoder input layer\nen_inp = tf.keras.layers.Input(shape=(n_steps,))\n# Encoder input embedddings\nen_emb = tf.keras.layers.Embedding(\n    n_en_vocab, d, input_length=n_steps\n)(en_inp)\n\n# Two encoder layers\nen_out1 = EncoderLayer(d, n_heads)(en_emb)\nen_out2 = EncoderLayer(d, n_heads)(en_out1)\n\nmodel = tf.keras.models.Model(inputs=en_inp, output=en_out2)\n```", "```py\nI went to the bakery to buy bread\n```", "```py\nI went to the [MASK] to buy bread\n```", "```py\nQuestion: What color is the ball?\nParagraph: Tippy is a dog. She loves to play with her red ball.\n```", "```py\n[CLS] What color is the ball [SEP] Tippy is a dog She loves to play with her red ball [SEP]\n```", "```py\nham    Go until jurong point, crazy.. Available only in bugis n great \n➥ world la e buffet... Cine there got amore wat...\nham    Ok lar... Joking wif u oni...\nspam        Free entry in 2 a wkly comp to win FA Cup final tkts 21st \n➥ May 2005 ...\n```", "```py\ninputs = []                                 ❶\nlabels = []                                 ❷\n\nn_ham, n_spam = 0,0                         ❸\nwith open(os.path.join('data', 'SMSSpamCollection'), 'r') as f:\n    for r in f:                             ❹\n\n        if r.startswith('ham'):             ❺\n            label = 0                       ❻\n            txt = r[4:]                     ❼\n            n_ham += 1                      ❽\n        # Spam input\n        elif r.startswith('spam'):          ❾\n            label = 1                       ❿\n            txt = r[5:]                     ⓫\n            n_spam += 1\n        inputs.append(txt)                  ⓬\n        labels.append(label)                ⓭\n\n# Convert them to arrays\ninputs = np.array(inputs).reshape(-1,1)     ⓮\nlabels = np.array(labels)                   ⓯\n```", "```py\nfrom imblearn.under_sampling import  NearMiss, RandomUnderSampler\nimport numpy as np\n```", "```py\nn=100 # Number of instances for each class for test/validation sets\nrandom_seed = 4321\n```", "```py\nrus = RandomUnderSampler(\n    sampling_strategy={0:n, 1:n}, random_state=random_seed\n)\n```", "```py\nrus.fit_resample(inputs, labels)\n```", "```py\ntest_inds = rus.sample_indices_\ntest_x, test_y = inputs[test_inds], np.array(labels)[test_inds]\n```", "```py\nrest_inds = [i for i in range(inputs.shape[0]) if i not in test_inds]\nrest_x, rest_y = inputs[rest_inds], labels[rest_inds]\n```", "```py\nrus.fit_resample(rest_x, rest_y)\nvalid_inds = rus.sample_indices_\nvalid_x, valid_y = rest_x[valid_inds], rest_y[valid_inds]\n```", "```py\ntrain_inds = [i for i in range(rest_x.shape[0]) if i not in valid_inds]\ntrain_x, train_y = rest_x[train_inds], rest_y[train_inds]\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncountvec = CountVectorizer()\ntrain_bow = countvec.fit_transform(train_x.reshape(-1).tolist())\n```", "```py\nfrom imblearn.under_sampling import  NearMiss\n\noss = NearMiss()\nx_res, y_res = oss.fit_resample(train_bow, train_y)\ntrain_inds = oss.sample_indices_\n\ntrain_x, train_y = train_x[train_inds], train_y[train_inds]\n```", "```py\nTest dataset size\n1    100\n0    100\ndtype: int64\n\nValid dataset size\n1    100\n0    100\ndtype: int64\nTrain dataset size\n1    547\n0    547\ndtype: int64\n```", "```py\nimport tensorflow_models as tfm\n```", "```py\nvocab_file = os.path.join(\"data\", \"vocab.txt\")\n\ndo_lower_case = True\n\ntokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(\n    vocab_file=vocab_file, lower_case=do_lower_case\n)\n```", "```py\ntokens = tf.reshape(\n    tokenizer([\"She sells seashells by the seashore\"]), [-1])\nprint(\"Tokens IDs generated by BERT: {}\".format(tokens))\nids = [tokenizer._vocab[tid] for tid in tokens] \nprint(\"Tokens generated by BERT: {}\".format(ids))\n```", "```py\nTokens IDs generated by BERT: [ 2016 15187 11915 18223  2015  2011  1996 11915 16892]\nTokens generated by BERT: ['she', 'sells', 'seas', '##hell', '##s', 'by', 'the', 'seas', '##hore']\n```", "```py\nspecial_tokens = ['[CLS]', '[SEP]', '[MASK]', '[PAD]']\nids = [tokenizer._vocab.index(tok) for tok in special_tokens]\nfor t, i in zip(special_tokens, ids):\n    print(\"Token: {} has ID: {}\".format(t, i))\n```", "```py\nToken: [CLS] has ID: 101\nToken: [SEP] has ID: 102\nToken: [MASK] has ID: 103\nToken: [PAD] has ID: 0\n```", "```py\ndef encode_sentence(s):\n    \"\"\" Encode a given sentence by tokenizing it and adding special tokens \"\"\"\n\n    tokens = list(\n        tf.reshape(tokenizer([\"CLS\" + s + \"[SEP]\"]), [-1])\n    )                   ❶\n    return tokens       ❷\n```", "```py\nencode_sentence(\"I like ice cream\")\n```", "```py\n[101, 1045, 2066, 3256, 6949, 102]\n```", "```py\n\"I like ice cream\"\n```", "```py\n{\n    'input_word_ids': [[ 101, 1045, 2066, 3256, 6949,  102,    0,    0]], \n    'input_mask': [[1., 1., 1., 1., 1., 1., 0., 0.]], \n    'input_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0]]\n}\n```", "```py\ndef get_bert_inputs(tokenizer, docs,max_seq_len=None):\n    \"\"\" Generate inputs for BERT using a set of documents \"\"\"\n\n    packer = tfm.nlp.layers.BertPackInputs(                      ❶\n        seq_length=max_seq_length,\n        special_tokens_dict = tokenizer.get_special_tokens_dict()\n    )\n\n    packed = packer(tokenizer(docs))                             ❷\n\n    packed_numpy = dict(\n        [(k, v.numpy()) for k,v in packed.items()]               ❸\n    )\n    # Final output\n    return packed_numpy                                          ❹\n```", "```py\ntrain_inputs = get_bert_inputs(train_x, max_seq_len=80)\nvalid_inputs = get_bert_inputs(valid_x, max_seq_len=80)\ntest_inputs = get_bert_inputs(test_x, max_seq_len=80)\n```", "```py\ntrain_inds = np.random.permutation(len(train_inputs[\"input_word_ids\"]))\ntrain_inputs = dict(\n    [(k, v[train_inds]) for k, v in train_inputs.items()]\n)\ntrain_y = train_y[train_inds]\n```", "```py\nimport tensorflow_hub as hub\n\nhub_bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\nmax_seq_length = 60\n# Contains input token ids\ninput_word_ids = tf.keras.layers.Input(\n    shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\"\n)\n# Contains input mask values\ninput_mask = tf.keras.layers.Input(\n    shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\"\n)\ninput_type_ids = tf.keras.layers.Input(\n    shape=(max_seq_length,), dtype=tf.int32, name=\"input_type_ids\"\n)\n\n# BERT encoder downloaded from TF hub\nbert_layer = hub.KerasLayer(hub_bert_url, trainable=True)\n\n# get the output of the encoder\noutput = bert_layer({\n    \"input_word_ids\":input_word_ids, \n    \"input_mask\": input_mask, \n    \"input_type_ids\": input_type_ids\n})\n\n# Define the final encoder as with the Functional API\nhub_encoder = tf.keras.models.Model(\n    inputs={\n        \"input_word_ids\": input_word_ids, \n        \"input_mask\": input_mask, \n        \"input_type_ids\": input_type_ids\n    }, \n    outputs={\n        \"sequence_output\": output[\"sequence_output\"], \n        \"pooled_output\": output[\"pooled_output\"]\n    }\n)\n```", "```py\n# Generating a classifier and the encoder\nbert_classifier = tfm.nlp.models.BertClassifier(\n    network=hub_encoder, num_classes=2\n)\n```", "```py\nimport yaml\n\nwith open(os.path.join(\"data\", \"bert_en_uncased_base.yaml\"), 'r') as stream:\n    config_dict = yaml.safe_load(stream)['task']['model']['encoder']['bert']\n\nencoder_config = tfm.nlp.encoders.EncoderConfig({\n    'type':'bert',\n    'bert': config_dict\n})\n\nbert_encoder = tfm.nlp.encoders.build_encoder(encoder_config)\n\nbert_classifier = tfm.nlp.models.BertClassifier(\n    network=bert_encoder, num_classes=2\n)\n```", "```py\ncheckpoint = tf.train.Checkpoint(encoder=bert_encoder)\ncheckpoint.read(<path to .ckpt>).assert_consumed()\n```", "```py\nepochs = 3\nbatch_size = 56\neval_batch_size = 56\n\ntrain_data_size = train_x.shape[0]\nsteps_per_epoch = int(train_data_size / batch_size)\nnum_train_steps = steps_per_epoch * epochs\nwarmup_steps = int(num_train_steps * 0.1)\n\ninit_lr = 3e-6\nend_lr = 0.0\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=init_lr,\n    end_learning_rate=end_lr,\n    decay_steps=num_train_steps)\n\nwarmup_schedule = tfm.optimization.lr_schedule.LinearWarmup(\n    warmup_learning_rate = 1e-10,\n    after_warmup_lr_sched = linear_decay,\n    warmup_steps = warmup_steps\n)\n\noptimizer = tf.keras.optimizers.experimental.Adam(\n    learning_rate = warmup_schedule\n)\n```", "```py\nmetrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', \n➥ dtype=tf.float32)]\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nhub_classifier.compile(\n    optimizer=optimizer,\n    loss=loss,\n    metrics=metrics)\n```", "```py\nhub_classifier.fit(\n      x=train_inputs, \n      y=train_y,\n      validation_data=(valid_inputs, valid_y),\n      validation_batch_size=eval_batch_size,\n      batch_size=batch_size,\n      epochs=epochs)\n```", "```py\nEpoch 1/3\n18/18 [==============================] - 544s 29s/step - loss: 0.7082 - \n➥ accuracy: 0.4555 - val_loss: 0.6764 - val_accuracy: 0.5150\nEpoch 2/3\n18/18 [==============================] - 518s 29s/step - loss: 0.6645 - \n➥ accuracy: 0.6589 - val_loss: 0.6480 - val_accuracy: 0.8150\nEpoch 3/3\n18/18 [==============================] - 518s 29s/step - loss: 0.6414 - \n➥ accuracy: 0.7608 - val_loss: 0.6391 - val_accuracy: 0.8550\n```", "```py\nhub_classifier.evaluate(test_inputs, test_y)\n```", "```py\n7/7 [==============================] - 22s 3s/step - loss: 0.6432 - accuracy: 0.7950\n```", "```py\n[CLS], What, did, the, dog, barked, at, [SEP], The, dog, barked, at, the, \n➥ mailman, [SEP]\n```", "```py\nfrom datasets import load_dataset\ndataset = load_dataset(\"squad\")\n```", "```py\nprint(dataset)\n```", "```py\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 10570\n    })\n})\n```", "```py\ndataset[\"train\"][\"answers\"][:5]\n```", "```py\n[{'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n {'answer_start': [188], 'text': ['a copper statue of Christ']},\n {'answer_start': [279], 'text': ['the Main Building']},\n {'answer_start': [381], 'text': ['a Marian place of prayer and reflection']},\n {'answer_start': [92], 'text': ['a golden statue of the Virgin Mary']}]\n```", "```py\ndef correct_indices_add_end_idx(answers, contexts):\n    \"\"\" Correct the answer index of the samples (if wrong) \"\"\"\n\n    n_correct, n_fix = 0, 0                                       ❶\n    fixed_answers = []                                            ❷\n    for answer, context in zip(answers, contexts):                ❸\n\n        gold_text = answer['text'][0]                             ❹\n        answer['text'] = gold_text                                ❹\n        start_idx = answer['answer_start'][0]                     ❺\n        answer['answer_start'] = start_idx                        ❺\n        if start_idx <0 or len(gold_text.strip())==0:\n            print(answer)\n        end_idx = start_idx + len(gold_text)                      ❻\n\n        # sometimes squad answers are off by a character or two - fix this\n        if context[start_idx:end_idx] == gold_text:               ❼\n            answer['answer_end'] = end_idx\n            n_correct += 1\n        elif context[start_idx-1:end_idx-1] == gold_text:         ❽\n            answer['answer_start'] = start_idx - 1\n            answer['answer_end'] = end_idx - 1     \n            n_fix += 1\n        elif context[start_idx-2:end_idx-2] == gold_text:         ❾\n            answer['answer_start'] = start_idx - 2\n            answer['answer_end'] = end_idx - 2 \n            n_fix +=1\n\n        fixed_answers.append(answer)\n\n    print(                                                        ❿\n        \"\\t{}/{} examples had the correct answer indices\".format(\n            n_correct, len(answers)\n        )\n    )\n    print(                                                        ❿\n        \"\\t{}/{} examples had the wrong answer indices\".format(\n            n_fix, len(answers)\n        )\n    )\n    return fixed_answers, contexts                                ⓫\n```", "```py\ntrain_questions = dataset[\"train\"][\"question\"]\ntrain_answers, train_contexts = correct_indices_add_end_idx(\n    dataset[\"train\"][\"answers\"], dataset[\"train\"][\"context\"]\n)\n\ntest_questions = dataset[\"validation\"][\"question\"]\ntest_answers, test_contexts = correct_indices_add_end_idx(\n    dataset[\"validation\"][\"answers\"], dataset[\"validation\"][\"context\"]\n)\n```", "```py\nfrom transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n```", "```py\ncontext = \"This is the context\"\nquestion = \"This is the question\"\n\ntoken_ids = tokenizer(context, question, return_tensors='tf')\nprint(token_ids)\n```", "```py\n{'input_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=\narray([[ 101, 2023, 2003, 1996, 6123,  102, 2023, 2003, 1996, 3160,  102]],\n      dtype=int32)>, \n 'attention_mask': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>\n}\n```", "```py\nprint(tokenizer.convert_ids_to_tokens(token_ids['input_ids'].numpy()[0]))\n```", "```py\n['[CLS]', 'this', 'is', 'the', 'context', '[SEP]', 'this', 'is', 'the', 'question', '[SEP]']\n```", "```py\ntrain_encodings = tokenizer(                                          ❶\n    train_contexts, train_questions, truncation=True, padding=True, \n➥ return_tensors='tf' \n)\nprint(\n    \"train_encodings.shape: {}\".format(train_encodings[\"input_ids\"].shape)\n)\n\ntest_encodings = tokenizer(\n    test_contexts, test_questions, truncation=True, padding=True, \n➥ return_tensors='tf'                                                ❷\n)\nprint(\"test_encodings.shape: {}\".format(test_encodings[\"input_ids\"].shape))\n```", "```py\ntrain_encodings.shape: (87599, 512)\ntest_encodings.shape: (10570, 512)\n```", "```py\ndef update_char_to_token_positions_inplace(encodings, answers):\n    start_positions = []\n    end_positions = []\n    n_updates = 0\n\n    for i in range(len(answers)):                                     ❶\n        start_positions.append(\n            encodings.char_to_token(i, answers[i]['answer_start'])    ❷\n        )\n        end_positions.append(\n            encodings.char_to_token(i, answers[i]['answer_end'] - 1)  ❷\n        )\n\n        if start_positions[-1] is None or end_positions[-1] is None:\n            n_updates += 1                                            ❸\n\n        # if start position is None, the answer passage has been truncated\n        # In the guide, \n➥ https:/ /huggingface.co/transformers/custom_datasets.xhtml#qa-squad\n        # they set it to model_max_length, but this will result in NaN \n➥ losses as the last\n        # available label is model_max_length-1 (zero-indexed)\n        if start_positions[-1] is None:        \n            start_positions[-1] = tokenizer.model_max_length -1       ❹\n\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length -1         ❺\n\n    print(\"{}/{} had answers truncated\".format(n_updates, len(answers)))\n    encodings.update({\n        'start_positions': start_positions, 'end_positions': end_positions\n    })                                                                ❻\n\nupdate_char_to_token_positions_inplace(train_encodings, train_answers)\nupdate_char_to_token_positions_inplace(test_encodings, test_answers)\n```", "```py\n10/87599 had answers truncated\n8/10570 had answers truncated\n```", "```py\nimport tensorflow as tf\n```", "```py\ndef data_gen(input_ids, attention_mask, start_positions, end_positions):\n    for inps, attn, start_pos, end_pos in zip(\n        input_ids, attention_mask, start_positions, end_positions\n    ):\n\n        yield (inps, attn), (start_pos, end_pos)\n```", "```py\nfrom functools import partial\n\ntrain_data_gen = partial(\n    data_gen,\n    input_ids=train_encodings['input_ids'],\n    attention_mask=train_encodings['attention_mask'],\n    start_positions=train_encodings['start_positions'],  \n    end_positions=train_encodings['end_positions']\n)\n```", "```py\ntrain_dataset = tf.data.Dataset.from_generator(\n    train_data_gen, output_types=(('int32', 'int32'), ('int32', 'int32'))\n)\n```", "```py\ntrain_dataset = train_dataset.shuffle(20000)\n```", "```py\nvalid_dataset = train_dataset.take(10000)\nvalid_dataset = valid_dataset.batch(8)\n```", "```py\ntrain_dataset = train_dataset.skip(10000)\ntrain_dataset = train_dataset.batch(8)\n```", "```py\ntest_data_gen = partial(data_gen,\n    input_ids=test_encodings['input_ids'], \n    attention_mask=test_encodings['attention_mask'],\n    start_positions=test_encodings['start_positions'], \n    end_positions=test_encodings['end_positions']\n)\ntest_dataset = tf.data.Dataset.from_generator(\n    test_data_gen, output_types=(('int32', 'int32'), ('int32', 'int32'))\n)\ntest_dataset = test_dataset.batch(8)\n```", "```py\nfrom transformers import TFDistilBertForQuestionAnswering\n```", "```py\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\nTypeError: The two structures don't have the same sequence type. \nInput structure has type <class 'tuple'>, while shallow structure has type \n<class 'transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput'>.\n```", "```py\nfrom transformers import DistilBertConfig, TFDistilBertForQuestionAnswering\n\nconfig = DistilBertConfig.from_pretrained(\n    \"distilbert-base-uncased\", return_dict=False\n)\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\n    \"distilbert-base-uncased\", config=config\n)\n```", "```py\ndef tf_wrap_model(model):\n    \"\"\" Wraps the huggingface's model with in the Keras Functional API \"\"\"\n\n    # Define inputs\n    input_ids = tf.keras.layers.Input(\n        [None,], dtype=tf.int32, name=\"input_ids\"\n    )                                                  ❶\n    attention_mask = tf.keras.layers.Input(\n      [None,], dtype=tf.int32, name=\"attention_mask\"   ❷\n    )\n\n   out = model([input_ids, attention_mask])            ❸\n\n   wrap_model = tf.keras.models.Model(\n     [input_ids, attention_mask], \n     outputs=(out.start_logits, out.end_logits)        ❹\n   )\n\n   return wrap_model\n```", "```py\nmodel_v2 = tf_wrap_model(model)\n```", "```py\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nacc = tf.keras.metrics.SparseCategoricalAccuracy()\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n\nmodel_v2.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n```", "```py\nmodel_v2.fit(\n    train_dataset, \n    validation_data=valid_dataset,    \n    epochs=3\n)\n```", "```py\nEpoch 1/3\nWARNING:tensorflow:The parameters `output_attentions`, \n➥ `output_hidden_states` and `use_cache` cannot be updated when calling a \n➥ model.They have to be set to True/False in the config object (i.e.: \n➥ `config=XConfig.from_pretrained('name', output_attentions=True)`).\n\nWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode \n➥ and will always be set to `True`.\n\n9700/9700 [==============================] - 3308s 340ms/step - loss: \n➥ 4.3138 - tf_distil_bert_for_question_answering_loss: 2.2146 - \n➥ tf_distil_bert_for_question_answering_1_loss: 2.0992 - \n➥ tf_distil_bert_for_question_answering_sparse_categorical_accuracy: \n➥ 0.4180 - \n➥ tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy: \n➥ 0.4487 - val_loss: 2.3849 - \n➥ val_tf_distil_bert_for_question_answering_loss: 1.2053 - \n➥ val_tf_distil_bert_for_question_answering_1_loss: 1.1796 - \n➥ val_tf_distil_bert_for_question_answering_sparse_categorical_accuracy: \n➥ 0.6681 - \n➥ val_tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy\n➥ : 0.6909\n\n...\n\nEpoch 3/3\n9700/9700 [==============================] - 3293s 339ms/step - loss: \n➥ 1.6349 - tf_distil_bert_for_question_answering_loss: 0.8647 - \n➥ tf_distil_bert_for_question_answering_1_loss: 0.7703 - \n➥ tf_distil_bert_for_question_answering_sparse_categorical_accuracy: \n➥ 0.7294 - \n➥ tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy: \n➥ 0.7672 - val_loss: 2.4049 - \n➥ val_tf_distil_bert_for_question_answering_loss: 1.2048 - \n➥ val_tf_distil_bert_for_question_answering_1_loss: 1.2001 - \n➥ val_tf_distil_bert_for_question_answering_sparse_categorical_accuracy: \n➥ 0.6975 - \n➥ val_tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy\n➥ : 0.7200\n```", "```py\nimport os\n\n# Create folders\nif not os.path.exists('models'):\n    os.makedirs('models')\nif not os.path.exists('tokenizers'):\n    os.makedirs('tokenizers')\n\ntokenizer.save_pretrained(os.path.join('tokenizers', 'distilbert_qa'))\n\nmodel_v2.get_layer(\n    \"tf_distil_bert_for_question_answering\").save_pretrained(\n        os.path.join('models', 'distilbert_qa')\n    )\n)\n```", "```py\nmodel_v2.evaluate(test_dataset)\n```", "```py\n1322/1322 [==============================] - 166s 126ms/step - loss: 2.4756 \n➥ - tf_distil_bert_for_question_answering_loss: 1.2702 - \n➥ tf_distil_bert_for_question_answering_1_loss: 1.2054 - \n➥ tf_distil_bert_for_question_answering_sparse_categorical_accuracy: \n➥ 0.6577 - \n➥ tf_distil_bert_for_question_answering_1_sparse_categorical_accuracy: \n➥ 0.6942\n```", "```py\ni = 5\n\nsample_q = test_questions[i]                                          ❶\nsample_c = test_contexts[i]                                           ❶\nsample_a = test_answers[i]                                            ❶\nsample_input = (\n    test_encodings[\"input_ids\"][i:i+1], \n    test_encodings[\"attention_mask\"][i:i+1]\n)\n\ndef ask_bert(sample_input, tokenizer):\n\n    out = model_v2.predict(sample_input)                              ❷\n    pred_ans_start = tf.argmax(out[0][0])                             ❸\n    pred_ans_end = tf.argmax(out[1][0])                               ❹\n    print(\n        \"{}-{} token ids contain the answer\".format(\n            pred_ans_start, pred_ans_end\n        )\n    )\n    ans_tokens = sample_input[0][0][pred_ans_start:pred_ans_end+1]    ❺\n\n    return \" \".join(tokenizer.convert_ids_to_tokens(ans_tokens))      ❻\n\nprint(\"Question\")                                                     ❼\nprint(\"\\t\", sample_q, \"\\n\")                                           ❼\nprint(\"Context\")                                                      ❼\nprint(\"\\t\", sample_c, \"\\n\")                                           ❼\nprint(\"Answer (char indexed)\")                                        ❼\nprint(\"\\t\", sample_a, \"\\n\")                                           ❼\nprint('='*50,'\\n')\n\nsample_pred_ans = ask_bert(sample_input, tokenizer)                   ❽\n\nprint(\"Answer (predicted)\")                                           ❾\nprint(sample_pred_ans)                                                ❾\nprint('='*50,'\\n')\n```", "```py\nQuestion\n     What was the theme of Super Bowl 50? \n\nContext\n     Super Bowl 50 was an American football game to determine the \n➥ champion of the National Football League (NFL) for the 2015 season. The \n➥ American Football Conference (AFC) champion Denver Broncos defeated the \n➥ National Football Conference (NFC) champion Carolina Panthers 24-10 to \n➥ earn their third Super Bowl title. The game was played on February 7, \n➥ 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, \n➥ California. As this was the 50th Super Bowl, the league emphasized the \n➥ \"golden anniversary\" with various gold-themed initiatives, as well as \n➥ temporarily suspending the tradition of naming each Super Bowl game \n➥ with Roman numerals (under which the game would have been known as \n➥ \"Super Bowl L\"), so that the logo could prominently feature the Arabic \n➥ numerals 50\\. \n\nAnswer (char indexed)\n     {'answer_start': 487, 'text': '\"golden anniversary\"', \n➥ 'answer_end': 507} \n\n================================================== \n\n98-99 token ids contain the answer\nAnswer (predicted)\ngolden anniversary\n================================================== \n```", "```py\nimport torch\nfrom bertviz import head_view\n```", "```py\nconfig = BertConfig.from_pretrained(\n    'bert-base-uncased', output_attentions=True\n)\nbert = TFBertModel.from_pretrained(\n    \"bert-base-uncased\", config=config\n)\n```", "```py\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```", "```py\nhead_view(\n    [torch.from_numpy(layer_attn.numpy()) for layer_attn in output.attentions],\n    encoded_tokens\n)\n```", "```py\n<model>.from_pretrained(<model_tag>, *\"*a*\"*= *\"*abc*\"*)\n```", "```py\nimport tensorflow as tf\n\n# Defining some hyperparameters\nn_steps = 25 # Sequence length\nn_en_vocab = 300 # Encoder's vocabulary size\nn_heads = 8 # Number of attention heads\nd = 512 # The feature dimensionality of each layer\n\n# Encoder input layer\nen_inp = tf.keras.layers.Input(shape=(n_steps,))\n# Encoder input embedddings\nen_emb = tf.keras.layers.Embedding(\n    n_en_vocab, 512, input_length=n_steps\n)(en_inp)\n\npos_inp = tf.constant(\n    [[p/(10000**(2*i/d)) for p in range(d)] for i in range(n_steps)]\n)\npos_inp = tf.expand_dims(pos_inp, axis=0)\nen_pos_emb = tf.math.sin(pos_inp)\n\nen_final_emb = en_emb + en_pos_emb\n\n# Two encoder layers\nen_out1 = EncoderLayer(d, n_heads)(en_emb)\nen_out2 = EncoderLayer(d, n_heads)(en_out1)\n\nmodel = tf.keras.models.Model(inputs=en_inp, output=en_out2)\n```", "```py\nhub_classifier, hub_encoder = bert.bert_models.classifier_model(\n    bert_config=bert_config, hub_module_url=bert_url, num_labels=5\n)\n```", "```py\nfrom transformers import TFDistilBertForTokenClassification\n\nmodel = TFDistilBertForTokenClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=7\n*)*\n```"]