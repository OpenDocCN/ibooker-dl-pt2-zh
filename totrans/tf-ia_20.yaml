- en: appendix B Computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'B.1 Grad-CAM: Interpreting computer vision models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Grad-CAM (which stands for gradient class activation map) was introduced in
    chapter 7 and is a model interpretation technique introduced for deep neural networks
    by Ramprasaath R. Selvaraju et al. in “Grad-CAM: Visual Explanations from Deep
    Networks via Gradient-based Localization” ([https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)).
    Deep networks are notorious for their inexplicable nature and are thus termed
    *black boxes*. Therefore, we must do some analysis and ensure that the model is
    working as intended.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s refresh our memory on the model we implemented in chapter 7: a pretrained
    Inception-based model called InceptionResNet v2, topped with a softmax classifier
    that has 200 nodes (i.e., the same as the number of classes in our image classification
    data set, TinyImageNet; see the following listing).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.1 The InceptionResNet v2 model we defined in chapter 7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Define a model using the Sequential API.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Define an input layer to take in a 224 × 224 × 3-sized batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ Download and use the pretrained InceptionResNetV2 model (without the built-in
    classifier).
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Add a dropout layer.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Add a new classifier layer that has 200 nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you print the summary of this model, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the InceptionResNet v2 model is considered a single layer in
    our model. In other words, it’s a nested model, where the outer model (sequential)
    has an inner model (inception_resnet_v2). But we need more transparency, as we
    are going to access a particular layer inside the inception_resnet_v2 model in
    order to implement Grad-CAM. Therefore, we are going to “unwrap” or remove this
    nesting and have the model described by layers only. We can achieve this using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Essentially what we are doing is taking the existing model and changing the
    input of it slightly. After taking the existing model, we change the input to
    the input layer of the inception_resnet_v2 model. With that, we define a new model
    (which essentially uses the same parameters as the old model). Then you will see
    the following output. There are no more models within models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to make one more change: introduce a new output to our model.
    Remember that we used the functional API to define our model. This means we can
    define multiple outputs in our model. The output we need is the feature maps produced
    by the last convolutional layer in the inception_resnet_v2 model. This is a core
    part of the Grad-CAM computations. You can get the layer name of the last convolutional
    layer by looking at the model summary of the unwrapped model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With our model ready, let’s move on to the data. We will use the validation
    data set to inspect our model. Particularly, we will write a function (listing
    B.2) that takes in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: image_path (str)—Path to an image in the data set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: val_df (pd.DataFrame)—A pandas DataFrame that contains a mapping from an image
    name to wnid (i.e., a WordNet ID). Remember that a wnid is a special coding used
    to identify a specific class of objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: class_indices (dict)—A wnid (string) to class (integer between 0-199) mapping.
    This keeps information about which wnid is represented by which index in the final
    output layer of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: words (pd.DataFrame)—A pandas DataFrame that contains a mapping from a wnid
    to a human-readable description of the class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing B.2 Retrieving the transformed image, class index, and human-readable
    label
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Reads in the val_annotations.txt. This will create a data frame that has a
    mapping from an image filename to a wnid (i.e., WordNet ID).
  prefs: []
  type: TYPE_NORMAL
- en: ❷ Load the class indices that map a wnid to a class index (integer).
  prefs: []
  type: TYPE_NORMAL
- en: ❸ This will create a data frame that has a mapping from a wnid to a class description.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ Loads the image given by the filepath. First, we add an extra dimension to
    represent the batch dimension.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Resize the image to a 224 × 224-sized image.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Bring image pixels to a range of [-1, 1].
  prefs: []
  type: TYPE_NORMAL
- en: ❼ If the image is grayscale, repeat the image three times across the channel
    dimension to have the same format as an RGB image.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Get the wnid of the image.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Get the class index of the image.
  prefs: []
  type: TYPE_NORMAL
- en: ❿ Get the string label of the class.
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ Run the function for an example image.
  prefs: []
  type: TYPE_NORMAL
- en: The get_image_class_label() function takes the arguments specified and loads
    the image given by the image_path. First, we resize the image to a 224 × 224-sized
    image. We also add an extra dimension at the beginning to represent the image
    as a batch of one image. Then it performs a specific numerical transformation
    (i.e., divide element-wise by 127.5 and subtract 1). This is a special transformation
    that is used to train the InceptionResNet v2 model. Afterward, we get the class
    index (i.e., an integer) and the human-readable label of that class using the
    data frames and class_indices we passed into the function. Finally, it returns
    the transformed image, the class index, and the label of the class the image belongs
    to.
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows how the Grad-CAMs are computed for images. We will use
    10 images to compute Grad-CAMs for each individually.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.3 Computing Grad-CAM for 10 images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ❶ Get the normalized input, class(int), and label (string) for each image.
  prefs: []
  type: TYPE_NORMAL
- en: ❷ We compute the output of the model in the GradientTape context.
  prefs: []
  type: TYPE_NORMAL
- en: ❸ This will enable us to later access the gradients that appeared during the
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: ❹ We only take the loss corresponding to the class index of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: ❺ Get the gradients of the loss with respect to the last convolutional feature
    map.
  prefs: []
  type: TYPE_NORMAL
- en: ❻ Compute and apply weights.
  prefs: []
  type: TYPE_NORMAL
- en: ❼ Collapse the feature maps to a single channel to get the final heatmap.
  prefs: []
  type: TYPE_NORMAL
- en: ❽ Normalize the values to be in the range of 0-255.
  prefs: []
  type: TYPE_NORMAL
- en: ❾ Store the computed GradCAMs in a dictionary to visualize later.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the Grad-CAM for one image, we follow the following procedure. First,
    we get the transformed image, class index, and label for a given image path.
  prefs: []
  type: TYPE_NORMAL
- en: Next is the most important step of this computation! You know that, given an
    image and a label, the final loss is computed as the sum of class-specific losses
    for all the available classes. That is, if you imagine a one-hot encoded label
    and a probability vector output by the model, we compute element-wise loss between
    each output node. Here each node represents a single class. To compute the gradient
    map, we first compute the gradients of the class-specific loss only for the true
    label of that image, with respect to the output of the last convolutional layer.
    This gives a tensor the same size as the output of the last convolutional layer.
    It is important to note the difference between the typical loss we use and the
    loss used here. Typically, we sum the loss across all classes, whereas in Grad-CAM,
    we only consider the loss of the specific node that corresponds to the true class
    of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Note how we are computing the gradients. We use something called the GradientTape
    *(*[http://mng.bz/wo1Q](http://mng.bz/wo1Q)*)*. It’s an innovative piece of technology
    from TensorFlow. Whenever you compute something in the context of a GradientTape,
    it will record the gradients of all those computations. This means that when we
    compute the output in the context of a GradientTape, we can access the gradients
    of that computation later.
  prefs: []
  type: TYPE_NORMAL
- en: Then we do a few more transformations. First, we compute weights for each channel
    of the output feature map. The weights are simply the mean value of that feature
    map. The feature map values are multiplied by those weights. We then sum the output
    across all the channels. This means we will get an output with a width and height
    that have a single channel. This is essentially a heatmap, where a high value
    indicates more importance at a given pixel. To clip negative values to 0, we then
    apply ReLU activation on the output. As the final normalization step, we bring
    all the values to a range of 0-255 so that we can superimpose this on the actual
    image as a heatmap. Then it’s a simple matter of using the matplotlib library
    to plot the images and overlap the Grad-CAM outputs we generated on top of the
    images. If you want to see the code for this, please refer to the Ch07-Improving-CNNs-and-Explaining/7.3.Interpreting_CNNs_
    Grad-CAM.ipynb notebook. The final output will look like figure B.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![B-1](../../OEBPS/Images/B-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figrue B.1 Visualization of the Grad-CAM output for several probe images. The
    redder an area in the image, the more the model focuses on that part of the image.
    You can see that our model has learned to understand some complex scenes and separate
    the model that it needs to focus on.
  prefs: []
  type: TYPE_NORMAL
- en: 'B.2 Image segmentation: U-Net model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In chapter 8, we discussed the DeepLab v3: an image segmentation model. In
    this section we will discuss a different image segmentation model known as U-Net.
    It has a different architecture compared to a DeepLab model and is quite commonly
    used in the rea world. Therefore, it’s a model worth learning about.'
  prefs: []
  type: TYPE_NORMAL
- en: B.2.1 Understanding and defining the U-Net model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The U-Net model is essentially two mirrored fully convolutional networks that
    act as the encoder and the decoder, with some additional connections that connect
    parts of the encoder to parts of the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Background of U-Net
  prefs: []
  type: TYPE_NORMAL
- en: 'U-Net was introduced in the paper “U-Net: Convolution Networks for Biomedical
    Image Segmentation” ([https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf))
    and has its origins in biomedical image segmentation. The name U-net is derived
    from what the network looks like. It is still a popular pick for segmentation
    tasks in biology/medicine domains and has been shown to work well for more general-purpose
    tasks as well.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will look at the original U-Net model introduced in the paper. Later,
    we will slightly change the direction of our discussion to make it more suited
    to the problem at hand. The original model consumed a 572 × 572 × 1-sized image
    (i.e., a grayscale image) and outputted a 392 × 392 × 2-sized image. The network
    was trained to identify/segment cell boundaries from bodies. Therefore, the two
    channels in the output represent a binary output of whether the pixel belongs
    to a cell boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder consists of several downsampling modules, which gradually downsample
    the input. A downsampling module consists of two convolution layers and one max
    pooling layer. Specifically, a downsampling module comprises
  prefs: []
  type: TYPE_NORMAL
- en: A 3 × 3 convolution layer (with valid padding) × 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 2 × 2 max-pooling layer (except in the last downsampling module)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A series of such downsampling layers brings the 572 × 572 × 1-sized input to
    a 28 × 28 × 1024-sized output.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the decoder consists of several upsampling layers. Specifically, each
    decoder upsampling module consists of
  prefs: []
  type: TYPE_NORMAL
- en: A 2 × 2 transpose convolution layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 3 × 3 convolution layer (with valid padding) × 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might already be wondering, what is a transpose convolution layer? Transpose
    convolution is what you get if you reverse the computations happening in a convolution
    layer. Instead of the convolution operation reducing the size of the output (i.e.,
    using strides), transpose convolution *increases* the size of the output (i.e.,
    upsamples the input). This is also known as *fractional striding*, as increasing
    the stride leads to larger outputs when using transpose convolution. This is illustrated
    in figure B.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![B-2](../../OEBPS/Images/B-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.2 Standard convolution versus transpose convolution. A positive stride
    on standard convolution leads to a smaller output, whereas a positive stride on
    transpose convolution leads to a bigger image.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are skip connections that connect interim layers of the encoder
    to interim layers of the decoder. This is an important architectural design, as
    this provides the much needed spatial/contextual information to the decoder that
    otherwise would have been lost. Particularly, the output of the encoder’s i^(th)
    level output is concatenated to the output of the decoder’s n-i^(th) level input
    (e.g., the output of the first level [of size 568 × 568 × 64] is concatenated
    to the input of the last level of the decoder [of size 392 × 392 × 64]; figure
    B.3). In order to do so, the encoder’s output first needs to be cropped slightly
    to match the corresponding decoder layer’s output.
  prefs: []
  type: TYPE_NORMAL
- en: '![B-3](../../OEBPS/Images/B-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.3 The original U-Net model proposed. The light blocks represent the
    encoder, and the dark blocks represent the decoder. The vertical numbers represent
    the size of the output (height and width) at a given position, and the number
    on top represents the number of filters.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.2 What’s better than an encoder? A pretrained encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you use the original network as is for the Pascal VOC data set, you will
    likely be very disappointed with its performance. There could be several reasons
    behind this behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: The data in the Pascal VOC is much more complex than what the original U-Net
    was designed for. For example, as opposed to the images containing simple cell
    structures in black and white, we have RGB images containing complex scenes in
    the real world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a fully convolution network, U-Net is highly regularized (due to the small
    number of parameters). This number of parameters is not enough to solve the complex
    task we have with adequate accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a network that started with a random initialization, it needs to learn to
    solve the task without the pretrained knowledge from a pretrained model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line with this reasoning, let’s discuss a few changes that we will make to
    the original U-Net architecture. We will be implementing a U-Net network that
    has
  prefs: []
  type: TYPE_NORMAL
- en: A pretrained encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A larger number of filters in each decoder module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pretrained encoder we will use is a ResNet-50 model ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)).
    It is one of the pioneering residual networks that made waves in the computer
    vision community a few years ago. We will look at ResNet-50 on the surface only,
    as we will discuss this model in detail in our section on the DeepLab v3 model.
    The ResNet-50 model consists of several convolution blocks, followed by a global
    average pooling layer and a fully connected final prediction layer with softmax
    activation. The convolution block is the innovative part of the model (denoted
    by B in figure B.4). The original model has 16 convolution blocks organized into
    5 groups. We will only use the first 13 blocks (i.e., first 4 groups). A single
    block consists of three convolution layers (1 × 1 convolution layer with stride
    2, 3 × 3 convolution layer, and 1 × 1 convolution layer), batch normalization,
    and residual connections, as shown in figure B.4\. We discussed residual connections
    in depth in chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: '![B-4](../../OEBPS/Images/B-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.4 The modified U-Net architecture (best viewed in color). This version
    of U-Net has the first four blocks of the ResNet-50 model as the encoder, and
    the decoder specifications (e.g., number of filters) are increased to match the
    specifications of the matching encoder layers.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the modified U-Net
  prefs: []
  type: TYPE_NORMAL
- en: 'With a sound conceptual understanding of the model and its different components,
    it’s time to implement it in Keras. We will use the Keras functional API. First,
    we define the encoder part of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we discuss the bells and whistles of the decoder. The decoder consists
    of several upsampling layers, which serve two important functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Upsampling the input to the layer to a larger output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copying, cropping, and concatenating the matched encoder input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function shown in the following listing encapsulates the computations we
    outlined.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.4 The upsampling layer of the modified UNet’s decoder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s analyze the function we wrote. It takes the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: input—The input to the layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: copy_and_crop—The input that is copied across from the encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: filters—The number of output filters after performing transpose convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we perform transpose convolution as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The Conv2DTranspose has identical syntax to the Conv2D we have used many times.
    It has a number of filters, a kernel size (height and width), strides (height
    and width), activation, and padding (defaults to valid). We will compute the crop
    parameters depending on the size of the transpose convolution output and the encoder’s
    input. Then we perform cropping using the Keras layer Cropping2D as required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we first compute how much to crop from one side by subtracting the encoder’s
    size from the upsampled output conv1_out. Then, if the size is greater than zero,
    cropped_copy is computed by passing the crop_side as a parameter to a Cropping2D
    Keras layer. The cropped encoder’s output and the upsampled conv1_out is then
    concatenated to produce a single tensor. This goes through two 3 × 3 convolution
    layers with ReLU activation and valid padding to produce the final output. We
    now define the decoder fully (see the next listing). The decoder consists of three
    upsampling layers, which consume the output of the previous layer, as well as
    an encoder output that is copied across.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.5 The decoder of the modified U-Net model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Copying an interim output of a predefined model across is not something we have
    done previously. Therefore, it is worth investigating further. We don’t have the
    luxury of resorting to previously defined variables that represent the encoders’
    outputs because this is a predefined model we downloaded through Keras, without
    the references to actual variables that were used in creating the model.
  prefs: []
  type: TYPE_NORMAL
- en: But accessing intermediate outputs and using them to create new connections
    is not that difficult. All you need to know is the name of the layer that you
    want to access. This can be done by looking at the output of encoder.summary().
    For example, here (according to figure B.4) we get the last outputs of the conv3,
    conv2, and conv1 modules. To get the output of conv3_block4_out, all you need
    to do is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: and pass that to the upsample_conv layer we just defined. The ability to perform
    such complex manipulations is a testament to how flexible the Keras functional
    API is. Finally, you can define the holistic modified U-Net model in the function
    unet_ pretrained_encoder(), as shown in the next listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.6 Full modified U-Net model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: What’s happening here is quite clear. We first define a 512 × 512 × 3-sized
    input that is passed to the encoder. Our encoder is a ResNet-50 model without
    the top prediction layer or global pooling. Next, we define the decoder, which
    takes the conv4_block6_out layer’s output as the input (i.e., final output of
    the conv4 block of the ResNet-50 model) and then upsamples it gradually using
    transpose convolution operations. Moreover, the decoder copies, crops, and concatenates
    matching encoder layers. We also define a 1 × 1 convolution layer that produces
    the final output. Finally, we define an end-to-end model using the Keras functional
    API.
  prefs: []
  type: TYPE_NORMAL
