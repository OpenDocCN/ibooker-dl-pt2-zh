- en: appendix B Computer vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B 计算机视觉
- en: 'B.1 Grad-CAM: Interpreting computer vision models'
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 Grad-CAM：解释计算机视觉模型
- en: 'Grad-CAM (which stands for gradient class activation map) was introduced in
    chapter 7 and is a model interpretation technique introduced for deep neural networks
    by Ramprasaath R. Selvaraju et al. in “Grad-CAM: Visual Explanations from Deep
    Networks via Gradient-based Localization” ([https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)).
    Deep networks are notorious for their inexplicable nature and are thus termed
    *black boxes*. Therefore, we must do some analysis and ensure that the model is
    working as intended.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'Grad-CAM（代表梯度类激活映射）在第 7 章介绍过，是由 Ramprasaath R. Selvaraju 等人在“Grad-CAM: Visual
    Explanations from Deep Networks via Gradient-based Localization”（[https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)）中介绍的一种深度神经网络模型解释技术。深度网络以其难以解释的特性而臭名昭著，因此被称为*黑盒子*。因此，我们必须进行一些分析，并确保模型按预期工作。'
- en: 'Let’s refresh our memory on the model we implemented in chapter 7: a pretrained
    Inception-based model called InceptionResNet v2, topped with a softmax classifier
    that has 200 nodes (i.e., the same as the number of classes in our image classification
    data set, TinyImageNet; see the following listing).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在第 7 章实现的模型上刷新一下记忆：一个名为 InceptionResNet v2 的预训练模型，其顶部是一个具有 200 个节点的 softmax
    分类器（即我们的图像分类数据集 TinyImageNet 中的类别数量相同；请参阅下面的列表）。
- en: Listing B.1 The InceptionResNet v2 model we defined in chapter 7
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 B.1 我们在第 7 章定义的 InceptionResNet v2 模型
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Define a model using the Sequential API.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 Sequential API 定义一个模型。
- en: ❷ Define an input layer to take in a 224 × 224 × 3-sized batch of images.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义一个输入层来接收大小为 224 × 224 × 3 的图像批次。
- en: ❸ Download and use the pretrained InceptionResNetV2 model (without the built-in
    classifier).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 下载并使用预训练的 InceptionResNetV2 模型（不包括内置分类器）。
- en: ❹ Add a dropout layer.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个 dropout 层。
- en: ❺ Add a new classifier layer that has 200 nodes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加一个具有 200 个节点的新分类器层。
- en: 'If you print the summary of this model, you will get the following output:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印此模型的摘要，你将得到以下输出：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see, the InceptionResNet v2 model is considered a single layer in
    our model. In other words, it’s a nested model, where the outer model (sequential)
    has an inner model (inception_resnet_v2). But we need more transparency, as we
    are going to access a particular layer inside the inception_resnet_v2 model in
    order to implement Grad-CAM. Therefore, we are going to “unwrap” or remove this
    nesting and have the model described by layers only. We can achieve this using
    the following code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，InceptionResNet v2 模型被视为我们模型中的单个层。换句话说，它是一个嵌套模型，其中外层模型（sequential）有一个内层模型（inception_resnet_v2）。但是我们需要更多的透明度，因为我们将要访问
    inception_resnet_v2 模型内的特定层，以实现 Grad-CAM。因此，我们将“解开”或移除此嵌套，并且只描述模型的层。我们可以使用以下代码实现这一点：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Essentially what we are doing is taking the existing model and changing the
    input of it slightly. After taking the existing model, we change the input to
    the input layer of the inception_resnet_v2 model. With that, we define a new model
    (which essentially uses the same parameters as the old model). Then you will see
    the following output. There are no more models within models:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上我们正在做的是取现有模型并略微更改其输入。在取得现有模型后，我们将输入更改为 inception_resnet_v2 模型的输入层。然后，我们定义一个新模型（本质上使用与旧模型相同的参数）。然后你将看到以下输出。没有更多的模型在模型内部：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we are going to make one more change: introduce a new output to our model.
    Remember that we used the functional API to define our model. This means we can
    define multiple outputs in our model. The output we need is the feature maps produced
    by the last convolutional layer in the inception_resnet_v2 model. This is a core
    part of the Grad-CAM computations. You can get the layer name of the last convolutional
    layer by looking at the model summary of the unwrapped model:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行一次更改：向我们的模型引入一个新输出。请记住，我们使用功能 API 来定义我们的模型。这意味着我们可以在我们的模型中定义多个输出。我们需要的输出是
    inception_resnet_v2 模型中最后一个卷积层产生的特征图。这是 Grad-CAM 计算的核心部分。您可以通过查看解开模型的模型摘要来获得最后一个卷积层的层名称：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With our model ready, let’s move on to the data. We will use the validation
    data set to inspect our model. Particularly, we will write a function (listing
    B.2) that takes in the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的模型准备好后，让我们转向数据。我们将使用验证数据集来检查我们的模型。特别地，我们将编写一个函数（见清单 B.2）来接收以下内容：
- en: image_path (str)—Path to an image in the data set.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: image_path（str）- 数据集中图像的路径。
- en: val_df (pd.DataFrame)—A pandas DataFrame that contains a mapping from an image
    name to wnid (i.e., a WordNet ID). Remember that a wnid is a special coding used
    to identify a specific class of objects.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: val_df（pd.DataFrame）—一个包含从图像名称到wnid（即WordNet ID）的映射的pandas数据框。请记住，wnid是用于识别特定对象类的特殊编码。
- en: class_indices (dict)—A wnid (string) to class (integer between 0-199) mapping.
    This keeps information about which wnid is represented by which index in the final
    output layer of the model.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: class_indices（dict）—一个wnid（字符串）到类别（0-199之间的整数）的映射。这保留了关于哪个wnid在模型的最终输出层中由哪个索引表示的信息。
- en: words (pd.DataFrame)—A pandas DataFrame that contains a mapping from a wnid
    to a human-readable description of the class.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: words（pd.DataFrame）—一个包含从wnid到类别的可读描述的映射的pandas数据框。
- en: Listing B.2 Retrieving the transformed image, class index, and human-readable
    label
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 B.2 检索转换后的图像、类别索引和人类可读标签
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Reads in the val_annotations.txt. This will create a data frame that has a
    mapping from an image filename to a wnid (i.e., WordNet ID).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取val_annotations.txt。这将创建一个数据框，其中包含从图像文件名到wnid（即WordNet ID）的映射。
- en: ❷ Load the class indices that map a wnid to a class index (integer).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载将wnid映射到类索引（整数）的类索引。
- en: ❸ This will create a data frame that has a mapping from a wnid to a class description.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这将创建一个数据框，其中包含从wnid到类描述的映射。
- en: ❹ Loads the image given by the filepath. First, we add an extra dimension to
    represent the batch dimension.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 加载由文件路径给出的图像。首先，我们添加一个额外的维度来表示批次维度。
- en: ❺ Resize the image to a 224 × 224-sized image.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将图像调整大小为224×224大小的图像。
- en: ❻ Bring image pixels to a range of [-1, 1].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将图像像素值调整到[-1, 1]的范围内。
- en: ❼ If the image is grayscale, repeat the image three times across the channel
    dimension to have the same format as an RGB image.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 如果图像是灰度的，则在通道维度上将图像重复三次，以与RGB图像具有相同的格式。
- en: ❽ Get the wnid of the image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 获取图像的wnid。
- en: ❾ Get the class index of the image.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 获取图像的类别索引。
- en: ❿ Get the string label of the class.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 获取类的字符串标签。
- en: ⓫ Run the function for an example image.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 对一个示例图像运行该函数。
- en: The get_image_class_label() function takes the arguments specified and loads
    the image given by the image_path. First, we resize the image to a 224 × 224-sized
    image. We also add an extra dimension at the beginning to represent the image
    as a batch of one image. Then it performs a specific numerical transformation
    (i.e., divide element-wise by 127.5 and subtract 1). This is a special transformation
    that is used to train the InceptionResNet v2 model. Afterward, we get the class
    index (i.e., an integer) and the human-readable label of that class using the
    data frames and class_indices we passed into the function. Finally, it returns
    the transformed image, the class index, and the label of the class the image belongs
    to.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: get_image_class_label()函数使用指定的参数并加载由image_path给出的图像。首先，我们将图像调整大小为224×224大小的图像。我们还在开始时添加了一个额外的维度来表示图像作为一个图像批次。然后，它执行特定的数值转换（即，逐元素除以127.5并减去1）。这是用于训练InceptionResNet
    v2模型的特殊转换。然后，我们使用传递给函数的数据框和class_indices获取该类的类索引（即，整数）和人类可读的标签。最后，它返回转换后的图像、类索引和图像所属类的标签。
- en: The next listing shows how the Grad-CAMs are computed for images. We will use
    10 images to compute Grad-CAMs for each individually.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个清单显示了如何为图像计算Grad-CAMs。我们将使用10个图像分别计算Grad-CAMs。
- en: Listing B.3 Computing Grad-CAM for 10 images
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 B.3 为10个图像计算Grad-CAM
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Get the normalized input, class(int), and label (string) for each image.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取每个图像的标准化输入、类别（整数）和标签（字符串）。
- en: ❷ We compute the output of the model in the GradientTape context.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在GradientTape环境中计算模型的输出。
- en: ❸ This will enable us to later access the gradients that appeared during the
    computation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这将使我们能够稍后访问在计算过程中出现的梯度。
- en: ❹ We only take the loss corresponding to the class index of the input image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们只考虑输入图像的类索引对应的损失。
- en: ❺ Get the gradients of the loss with respect to the last convolutional feature
    map.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取与最后一个卷积特征图相关的损失的梯度。
- en: ❻ Compute and apply weights.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算并应用权重。
- en: ❼ Collapse the feature maps to a single channel to get the final heatmap.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将特征图折叠为单个通道，以获得最终的热图。
- en: ❽ Normalize the values to be in the range of 0-255.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将值归一化为0-255的范围内。
- en: ❾ Store the computed GradCAMs in a dictionary to visualize later.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 将计算出的GradCAMs存储在字典中以便稍后可视化。
- en: To compute the Grad-CAM for one image, we follow the following procedure. First,
    we get the transformed image, class index, and label for a given image path.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算一张图像的 Grad-CAM，我们遵循以下步骤。首先，我们获得给定图像路径的转换图像、类索引和标签。
- en: Next is the most important step of this computation! You know that, given an
    image and a label, the final loss is computed as the sum of class-specific losses
    for all the available classes. That is, if you imagine a one-hot encoded label
    and a probability vector output by the model, we compute element-wise loss between
    each output node. Here each node represents a single class. To compute the gradient
    map, we first compute the gradients of the class-specific loss only for the true
    label of that image, with respect to the output of the last convolutional layer.
    This gives a tensor the same size as the output of the last convolutional layer.
    It is important to note the difference between the typical loss we use and the
    loss used here. Typically, we sum the loss across all classes, whereas in Grad-CAM,
    we only consider the loss of the specific node that corresponds to the true class
    of the input.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是此计算的最重要步骤！您知道，给定一张图像和一个标签，最终损失被计算为所有可用类别的类特定损失的总和。也就是说，如果您想象一个独热编码的标签和模型输出的概率向量，我们计算每个输出节点之间的损失。这里每个节点代表一个单独的类别。为了计算梯度映射，我们首先仅针对该图像的真实标签计算类特定损失的梯度，关于最后一个卷积层的输出。这给出了一个与最后一个卷积层的输出大小相同的张量。重要的是注意我们使用的典型损失和这里使用的损失之间的区别。通常，我们将所有类别的损失求和，而在
    Grad-CAM 中，我们仅考虑与输入的真实类对应的特定节点的损失。
- en: Note how we are computing the gradients. We use something called the GradientTape
    *(*[http://mng.bz/wo1Q](http://mng.bz/wo1Q)*)*. It’s an innovative piece of technology
    from TensorFlow. Whenever you compute something in the context of a GradientTape,
    it will record the gradients of all those computations. This means that when we
    compute the output in the context of a GradientTape, we can access the gradients
    of that computation later.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何计算梯度。我们使用了一种称为 GradientTape 的东西（*[http://mng.bz/wo1Q](http://mng.bz/wo1Q)*）。这是
    TensorFlow 中的一项创新技术。每当在 GradientTape 的上下文中计算某些东西时，它将记录所有这些计算的梯度。这意味着当我们在 GradientTape
    的上下文中计算输出时，我们可以稍后访问该计算的梯度。
- en: Then we do a few more transformations. First, we compute weights for each channel
    of the output feature map. The weights are simply the mean value of that feature
    map. The feature map values are multiplied by those weights. We then sum the output
    across all the channels. This means we will get an output with a width and height
    that have a single channel. This is essentially a heatmap, where a high value
    indicates more importance at a given pixel. To clip negative values to 0, we then
    apply ReLU activation on the output. As the final normalization step, we bring
    all the values to a range of 0-255 so that we can superimpose this on the actual
    image as a heatmap. Then it’s a simple matter of using the matplotlib library
    to plot the images and overlap the Grad-CAM outputs we generated on top of the
    images. If you want to see the code for this, please refer to the Ch07-Improving-CNNs-and-Explaining/7.3.Interpreting_CNNs_
    Grad-CAM.ipynb notebook. The final output will look like figure B.1.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进行一些额外的转换。首先，我们计算输出特征图的每个通道的权重。这些权重简单地是该特征图的均值。然后将特征图的值乘以这些权重。然后我们对所有通道的输出进行求和。这意味着我们将得到一个宽度和高度都为一个通道的输出。这本质上是一个热图，其中高值表示在给定像素处更重要。为了将负值剪切为
    0，然后在输出上应用 ReLU 激活。作为最终的归一化步骤，我们将所有值带到 0-255 的范围，以便我们可以将其作为热图叠加在实际图像上。然后只需使用 matplotlib
    库绘制图像，并将我们生成的 Grad-CAM 输出叠加在图像上。如果您想查看此代码，请参阅 Ch07-Improving-CNNs-and-Explaining/7.3.Interpreting_CNNs_
    Grad-CAM.ipynb 笔记本。最终输出将如图 B.1 所示。
- en: '![B-1](../../OEBPS/Images/B-1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![B-1](../../OEBPS/Images/B-1.png)'
- en: Figrue B.1 Visualization of the Grad-CAM output for several probe images. The
    redder an area in the image, the more the model focuses on that part of the image.
    You can see that our model has learned to understand some complex scenes and separate
    the model that it needs to focus on.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.1 几个探测图像的 Grad-CAM 输出可视化。图像中越红的区域，模型就越关注该图像的那部分。您可以看到我们的模型已经学会了理解一些复杂的场景，并分离出需要关注的模型。
- en: 'B.2 Image segmentation: U-Net model'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 图像分割：U-Net 模型
- en: 'In chapter 8, we discussed the DeepLab v3: an image segmentation model. In
    this section we will discuss a different image segmentation model known as U-Net.
    It has a different architecture compared to a DeepLab model and is quite commonly
    used in the rea world. Therefore, it’s a model worth learning about.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 章中，我们讨论了 DeepLab v3：一个图像分割模型。在本节中，我们将讨论另一个被称为 U-Net 的图像分割模型。它的架构与 DeepLab
    模型不同，并且在实际中非常常用。因此，这是一个值得学习的模型。
- en: B.2.1 Understanding and defining the U-Net model
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.1 理解和定义 U-Net 模型
- en: The U-Net model is essentially two mirrored fully convolutional networks that
    act as the encoder and the decoder, with some additional connections that connect
    parts of the encoder to parts of the decoder.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 模型本质上是两个镜像反射的全卷积网络，充当编码器和解码器，还有一些额外的连接，将编码器的部分连接到解码器的部分。
- en: Background of U-Net
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 的背景
- en: 'U-Net was introduced in the paper “U-Net: Convolution Networks for Biomedical
    Image Segmentation” ([https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf))
    and has its origins in biomedical image segmentation. The name U-net is derived
    from what the network looks like. It is still a popular pick for segmentation
    tasks in biology/medicine domains and has been shown to work well for more general-purpose
    tasks as well.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'U-Net 是在论文“U-Net: Convolution Networks for Biomedical Image Segmentation”([https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf))中被介绍的，其起源于生物医学图像分割。U-Net
    的名称源自于网络的形状。它仍然是生物学/医学领域分割任务中常用的选择，并且已经被证明在更一般的任务中也能很好地工作。'
- en: First, we will look at the original U-Net model introduced in the paper. Later,
    we will slightly change the direction of our discussion to make it more suited
    to the problem at hand. The original model consumed a 572 × 572 × 1-sized image
    (i.e., a grayscale image) and outputted a 392 × 392 × 2-sized image. The network
    was trained to identify/segment cell boundaries from bodies. Therefore, the two
    channels in the output represent a binary output of whether the pixel belongs
    to a cell boundary.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看一下原始的 U-Net 模型，该模型在论文中被介绍。然后，我们稍微调整我们的讨论方向，使其更适合当前问题。原始模型使用的是一个 572 ×
    572 × 1 大小的图像（即灰度图像），并输出一个 392 × 392 × 2 大小的图像。该网络经过训练，可以识别/分割细胞边界。因此，输出中的两个通道表示像素是否属于细胞边界的二进制输出。
- en: The encoder consists of several downsampling modules, which gradually downsample
    the input. A downsampling module consists of two convolution layers and one max
    pooling layer. Specifically, a downsampling module comprises
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由多个下采样模块组成，逐渐对输入进行下采样。一个下采样模块包括两个卷积层和一个最大池化层。具体来说，一个下采样模块包括：
- en: A 3 × 3 convolution layer (with valid padding) × 2
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 3 × 3 卷积层（valid padding）× 2
- en: A 2 × 2 max-pooling layer (except in the last downsampling module)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 2 × 2 最大池化层（除了最后一个下采样模块）
- en: A series of such downsampling layers brings the 572 × 572 × 1-sized input to
    a 28 × 28 × 1024-sized output.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列这样的下采样层将大小为 572 × 572 × 1 的输入转换为大小为 28 × 28 × 1024 的输出。
- en: Next, the decoder consists of several upsampling layers. Specifically, each
    decoder upsampling module consists of
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，解码器由多个上采样层组成。具体来说，每个解码器上采样模块包括：
- en: A 2 × 2 transpose convolution layer
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 2 × 2 转置卷积层
- en: A 3 × 3 convolution layer (with valid padding) × 2
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 3 × 3 卷积层（valid padding）× 2
- en: You might already be wondering, what is a transpose convolution layer? Transpose
    convolution is what you get if you reverse the computations happening in a convolution
    layer. Instead of the convolution operation reducing the size of the output (i.e.,
    using strides), transpose convolution *increases* the size of the output (i.e.,
    upsamples the input). This is also known as *fractional striding*, as increasing
    the stride leads to larger outputs when using transpose convolution. This is illustrated
    in figure B.2.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经想知道，什么是转置卷积层？转置卷积是反向计算卷积层中发生的计算得到的结果。转置卷积不是缩小输出的卷积运算（即使用步长），而是增加输出的大小（即上采样输入）。这也被称为*分数步长*，因为使用转置卷积时，增加步长会产生更大的输出。如图
    B.2 所示。
- en: '![B-2](../../OEBPS/Images/B-2.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![B-2](../../OEBPS/Images/B-2.png)'
- en: Figure B.2 Standard convolution versus transpose convolution. A positive stride
    on standard convolution leads to a smaller output, whereas a positive stride on
    transpose convolution leads to a bigger image.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.2 标准卷积与转置卷积的对比。标准卷积的正向步长会导致输出更小，而转置卷积的正向步长会导致输出更大的图像。
- en: Finally, there are skip connections that connect interim layers of the encoder
    to interim layers of the decoder. This is an important architectural design, as
    this provides the much needed spatial/contextual information to the decoder that
    otherwise would have been lost. Particularly, the output of the encoder’s i^(th)
    level output is concatenated to the output of the decoder’s n-i^(th) level input
    (e.g., the output of the first level [of size 568 × 568 × 64] is concatenated
    to the input of the last level of the decoder [of size 392 × 392 × 64]; figure
    B.3). In order to do so, the encoder’s output first needs to be cropped slightly
    to match the corresponding decoder layer’s output.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有跳跃连接将编码器的中间层连接到解码器的中间层。这是一个重要的架构设计，因为它为解码器提供了所需的空间/上下文信息，否则这些信息将会丢失。特别地，编码器的第i^(th)级输出与解码器的第n-i^(th)级输入连接起来（例如，第一级的输出[大小为568
    × 568 × 64]被连接到解码器的最后一级输入上 [大小为392 × 392 × 64]；图B.3）。为了做到这一点，编码器的输出首先需要稍微裁剪一下，以匹配相应的解码器层的输出。
- en: '![B-3](../../OEBPS/Images/B-3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![B-3](../../OEBPS/Images/B-3.png)'
- en: Figure B.3 The original U-Net model proposed. The light blocks represent the
    encoder, and the dark blocks represent the decoder. The vertical numbers represent
    the size of the output (height and width) at a given position, and the number
    on top represents the number of filters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.3 原始U-Net模型。浅色块代表编码器，深色块代表解码器。垂直数字表示给定位置的输出大小（高度和宽度），顶部的数字表示滤波器数量。
- en: B.2.2 What’s better than an encoder? A pretrained encoder
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.2 比编码器更好的是什么？一个预训练的编码器
- en: 'If you use the original network as is for the Pascal VOC data set, you will
    likely be very disappointed with its performance. There could be several reasons
    behind this behavior:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接将原始网络用于Pascal VOC数据集，你可能会对其性能感到非常失望。这种行为背后可能有几个原因：
- en: The data in the Pascal VOC is much more complex than what the original U-Net
    was designed for. For example, as opposed to the images containing simple cell
    structures in black and white, we have RGB images containing complex scenes in
    the real world.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascal VOC中的数据比原始U-Net设计的要复杂得多。例如，与黑白图像中包含简单细胞结构不同，我们有包含现实世界复杂场景的RGB图像。
- en: As a fully convolution network, U-Net is highly regularized (due to the small
    number of parameters). This number of parameters is not enough to solve the complex
    task we have with adequate accuracy.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一个完全卷积网络，U-Net具有很高的正则化程度（由于参数数量较少）。这个参数数量不足以以足够的准确度解决我们所面临的复杂任务。
- en: As a network that started with a random initialization, it needs to learn to
    solve the task without the pretrained knowledge from a pretrained model.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一个从随机初始化开始的网络，它需要学会在没有来自预训练模型的预训练知识的情况下解决任务。
- en: In line with this reasoning, let’s discuss a few changes that we will make to
    the original U-Net architecture. We will be implementing a U-Net network that
    has
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这种推理，让我们讨论一下我们将对原始U-Net架构进行的一些改变。我们将实现一个具有
- en: A pretrained encoder
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练的编码器
- en: A larger number of filters in each decoder module
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个解码器模块中的滤波器数量更多
- en: The pretrained encoder we will use is a ResNet-50 model ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)).
    It is one of the pioneering residual networks that made waves in the computer
    vision community a few years ago. We will look at ResNet-50 on the surface only,
    as we will discuss this model in detail in our section on the DeepLab v3 model.
    The ResNet-50 model consists of several convolution blocks, followed by a global
    average pooling layer and a fully connected final prediction layer with softmax
    activation. The convolution block is the innovative part of the model (denoted
    by B in figure B.4). The original model has 16 convolution blocks organized into
    5 groups. We will only use the first 13 blocks (i.e., first 4 groups). A single
    block consists of three convolution layers (1 × 1 convolution layer with stride
    2, 3 × 3 convolution layer, and 1 × 1 convolution layer), batch normalization,
    and residual connections, as shown in figure B.4\. We discussed residual connections
    in depth in chapter 7.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的预训练编码器是一个 ResNet-50 模型（[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)）。几年前，它是计算机视觉社区中引起轰动的开创性残差网络之一。我们只会简单地介绍
    ResNet-50，因为我们将在 DeepLab v3 模型的部分详细讨论该模型。ResNet-50 模型由多个卷积块组成，后跟一个全局平均池化层和一个具有
    softmax 激活的完全连接的最终预测层。卷积块是该模型的创新部分（在图 B.4 中用 B 表示）。原始模型有 16 个卷积块组织成 5 组。我们将仅使用前
    13 个块（即前 4 组）。单个块由三个卷积层（步幅为 2 的 1 × 1 卷积层、3 × 3 卷积层和 1 × 1 卷积层）、批量归一化和残差连接组成，如图
    B.4 所示。我们在第 7 章深入讨论了残差连接。
- en: '![B-4](../../OEBPS/Images/B-4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![B-4](../../OEBPS/Images/B-4.png)'
- en: Figure B.4 The modified U-Net architecture (best viewed in color). This version
    of U-Net has the first four blocks of the ResNet-50 model as the encoder, and
    the decoder specifications (e.g., number of filters) are increased to match the
    specifications of the matching encoder layers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.4 修改后的 U-Net 架构（最佳查看彩色）。此版本的 U-Net 将 ResNet-50 模型的前四个块作为编码器，并将解码器规格（例如，滤波器数量）增加到与匹配的编码器层的规格相匹配。
- en: Implementing the modified U-Net
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实现修改后的 U-Net
- en: 'With a sound conceptual understanding of the model and its different components,
    it’s time to implement it in Keras. We will use the Keras functional API. First,
    we define the encoder part of the network:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对模型及其不同组件进行深入的概念理解，是时候在 Keras 中实现它了。我们将使用 Keras 函数式 API。首先，我们定义网络的编码器部分：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we discuss the bells and whistles of the decoder. The decoder consists
    of several upsampling layers, which serve two important functions:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论解码器的花哨之处。解码器由多个上采样层组成，这些层具有两个重要功能：
- en: Upsampling the input to the layer to a larger output
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入上采样到更大的输出
- en: Copying, cropping, and concatenating the matched encoder input
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制、裁剪和连接匹配的编码器输入
- en: The function shown in the following listing encapsulates the computations we
    outlined.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表中显示的函数封装了我们概述的计算。
- en: Listing B.4 The upsampling layer of the modified UNet’s decoder
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.4 修改后的 UNet 解码器的上采样层
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s analyze the function we wrote. It takes the following arguments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析我们编写的函数。它接受以下参数：
- en: input—The input to the layer
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入—层的输入
- en: copy_and_crop—The input that is copied across from the encoder
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: copy_and_crop—从编码器复制过来的输入
- en: filters—The number of output filters after performing transpose convolution
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: filters—执行转置卷积后的输出滤波器数量
- en: First, we perform transpose convolution as
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们执行转置卷积，如下所示：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The Conv2DTranspose has identical syntax to the Conv2D we have used many times.
    It has a number of filters, a kernel size (height and width), strides (height
    and width), activation, and padding (defaults to valid). We will compute the crop
    parameters depending on the size of the transpose convolution output and the encoder’s
    input. Then we perform cropping using the Keras layer Cropping2D as required:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Conv2DTranspose 的语法与我们多次使用的 Conv2D 相同。它有一些滤波器、卷积核大小（高度和宽度）、步长（高度和宽度）、激活函数和填充（默认为
    valid）。我们将根据转置卷积输出的大小和编码器的输入来计算裁剪参数。然后，根据需要使用 Keras 层 Cropping2D 进行裁剪：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we first compute how much to crop from one side by subtracting the encoder’s
    size from the upsampled output conv1_out. Then, if the size is greater than zero,
    cropped_copy is computed by passing the crop_side as a parameter to a Cropping2D
    Keras layer. The cropped encoder’s output and the upsampled conv1_out is then
    concatenated to produce a single tensor. This goes through two 3 × 3 convolution
    layers with ReLU activation and valid padding to produce the final output. We
    now define the decoder fully (see the next listing). The decoder consists of three
    upsampling layers, which consume the output of the previous layer, as well as
    an encoder output that is copied across.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先计算从一侧裁剪多少，方法是从上采样输出 conv1_out 中减去编码器的大小。然后，如果大小大于零，则通过将 crop_side 作为参数传递给
    Cropping2D Keras 层来计算 cropped_copy。然后将裁剪后的编码器输出和上采样的 conv1_out 连接起来以产生单个张量。这通过两个具有
    ReLU 激活和有效填充的 3 × 3 卷积层产生最终输出。我们现在完全定义解码器（请参见下一个清单）。解码器由三个上采样层组成，这些层使用前一层的输出以及复制的编码器输出。
- en: Listing B.5 The decoder of the modified U-Net model
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 B.5 修改后的 U-Net 模型的解码器
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Copying an interim output of a predefined model across is not something we have
    done previously. Therefore, it is worth investigating further. We don’t have the
    luxury of resorting to previously defined variables that represent the encoders’
    outputs because this is a predefined model we downloaded through Keras, without
    the references to actual variables that were used in creating the model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 跨越预定义模型的中间输出的复制不是我们以前做过的事情。因此，值得进一步调查。我们不能够诉诸于先前定义的代表编码器输出的变量，因为这是一个通过 Keras
    下载的预定义模型，没有用于创建模型的实际变量的引用。
- en: But accessing intermediate outputs and using them to create new connections
    is not that difficult. All you need to know is the name of the layer that you
    want to access. This can be done by looking at the output of encoder.summary().
    For example, here (according to figure B.4) we get the last outputs of the conv3,
    conv2, and conv1 modules. To get the output of conv3_block4_out, all you need
    to do is
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但是访问中间输出并使用它们创建新连接并不那么困难。你只需要知道要访问的层的名称即可。这可以通过查看 encoder.summary() 的输出来完成。例如，在这里（根据图
    B.4），我们获得了 conv3、conv2 和 conv1 模块的最后输出。要获取 conv3_block4_out 的输出，你需要做的就是
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: and pass that to the upsample_conv layer we just defined. The ability to perform
    such complex manipulations is a testament to how flexible the Keras functional
    API is. Finally, you can define the holistic modified U-Net model in the function
    unet_ pretrained_encoder(), as shown in the next listing.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 将其传递给我们刚刚定义的上采样卷积层。能够执行这样复杂的操作证明了 Keras 函数 API 有多么灵活。最后，你可以在下一个清单中的函数 unet_pretrained_encoder()
    中定义完整修改后的 U-Net 模型。
- en: Listing B.6 Full modified U-Net model
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 B.6 完整修改后的 U-Net 模型
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: What’s happening here is quite clear. We first define a 512 × 512 × 3-sized
    input that is passed to the encoder. Our encoder is a ResNet-50 model without
    the top prediction layer or global pooling. Next, we define the decoder, which
    takes the conv4_block6_out layer’s output as the input (i.e., final output of
    the conv4 block of the ResNet-50 model) and then upsamples it gradually using
    transpose convolution operations. Moreover, the decoder copies, crops, and concatenates
    matching encoder layers. We also define a 1 × 1 convolution layer that produces
    the final output. Finally, we define an end-to-end model using the Keras functional
    API.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况非常清楚。我们首先定义一个大小为 512 × 512 × 3 的输入，将其传递给编码器。我们的编码器是一个没有顶部预测层或全局池化的 ResNet-50
    模型。接下来，我们定义解码器，它将 conv4_block6_out 层的输出作为输入（即 ResNet-50 模型的 conv4 块的最终输出），然后逐渐使用转置卷积操作上采样它。此外，解码器复制、裁剪和连接匹配的编码器层。我们还定义一个产生最终输出的
    1 × 1 卷积层。最后，我们使用 Keras 函数 API 定义端到端模型。
