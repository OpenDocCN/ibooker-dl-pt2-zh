["```py\n# Load data from huggingface\nfrom datasets import load_dataset\ndataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]\n\n# Extract specific metadata\nabstracts = dataset[\"Abstracts\"]\nyears = dataset[\"Years\"]\ncategories = dataset[\"Categories\"]\ntitles = dataset[\"Titles\"]\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\n# We load our model\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# The abstracts are converted to vector representations\nembeddings = model.encode(abstracts)\n```", "```py\nfrom umap import UMAP\n\n# We instantiate our UMAP model\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n# We fit and transform our embeddings to reduce them\nreduced_embeddings = umap_model.fit_transform(embeddings)\n```", "```py\nfrom hdbscan import HDBSCAN\n\n# We instantiate our HDBSCAN model\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom')\n\n# We fit our model and extract the cluster labels\nhdbscan_model.fit(reduced_embeddings)\nlabels = hdbscan_model.labels_\n```", "```py\nimport seaborn as sns\n\n# Reduce 384-dimensional embeddings to 2 dimensions for easier visualization\nreduced_embeddings = UMAP(n_neighbors=15, n_components=2, \nmin_dist=0.0, metric='cosine').fit_transform(embeddings)\ndf = pd.DataFrame(np.hstack([reduced_embeddings, clusters.reshape(-1, 1)]),\n     columns=[\"x\", \"y\", \"cluster\"]).sort_values(\"cluster\")\n\n# Visualize clusters\ndf.cluster = df.cluster.astype(int).astype(str)\nsns.scatterplot(data=df, x='x', y='y', hue='cluster', \n   linewidth=0, legend=False, s=3, alpha=0.3)\n```", "```py\n>>> for index in np.where(labels==1)[0][:3]:\n>>>    print(abstracts[index])\n Sarcasm is considered one of the most difficult problem in sentiment\nanalysis. In our ob-servation on Indonesian social media, for cer-tain topics,\npeople tend to criticize something using sarcasm. Here, we proposed two\nadditional features to detect sarcasm after a common sentiment analysis is\ncon...\n\n  Automatic sarcasm detection is the task of predicting sarcasm in text. This\nis a crucial step to sentiment analysis, considering prevalence and challenges\nof sarcasm in sentiment-bearing text. Beginning with an approach that used\nspeech-based features, sarcasm detection has witnessed great interes...\n\n  We introduce a deep neural network for automated sarcasm detection. Recent\nwork has emphasized the need for models to capitalize on contextual features,\nbeyond lexical and syntactic cues present in utterances. For example, different\nspeakers will tend to employ sarcasm regarding different subjects...\n```", "```py\nfrom bertopic import BERTopic\n\n# Instantiate our topic model\ntopic_model = BERTopic()\n\n# Fit our topic model on a list of documents\ntopic_model.fit(documents)\n```", "```py\nfrom umap import UMAP\nfrom hdbscan import HDBSCAN\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom bertopic import BERTopic\nfrom bertopic.representation import KeyBERTInspired\nfrom bertopic.vectorizers import ClassTfidfTransformer\n```", "```py\n# Step 1 - Extract embeddings (blue block)\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Step 2 - Reduce dimensionality (red block)\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n# Step 3 - Cluster reduced embeddings (green block)\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n# Step 4 - Tokenize topics (yellow block)\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n# Step 5 - Create topic representation (grey block)\nctfidf_model = ClassTfidfTransformer()\n\n# Step 6 - (Optional) Fine-tune topic representations with \n# a `bertopic.representation` model (purple block)\nrepresentation_model = KeyBERTInspired()\n# Combine the steps and build our own topic model\ntopic_model = BERTopic(\n  embedding_model=embedding_model,          *# Step 1 - Extract embeddings*\n  umap_model=umap_model,                    *# Step 2 - Reduce dimensionality*\n  hdbscan_model=hdbscan_model,              *# Step 3 - Cluster reduced embeddings*\n  vectorizer_model=vectorizer_model,        *# Step 4 - Tokenize topics*\n  ctfidf_model=ctfidf_model,                *# Step 5 - Extract topic words*\n  representation_model=representation_model *# Step 6 - Fine-tune topics*\n)\n```", "```py\n# Load data from huggingface\nfrom datasets import load_dataset\ndataset = load_dataset(\"maartengr/arxiv_nlp\")\n\n# Extract specific metadata\nabstracts = dataset[\"Abstracts\"]\nyears = dataset[\"Years\"]\ncategories = dataset[\"Categories\"]\ntitles = dataset[\"Titles\"]\n```", "```py\n# Train our topic model in only three lines of code\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic()\ntopics, probs = topic_model.fit_transform(abstracts)\n```", "```py\nfrom umap import UMAP\nfrom bertopic import BERTopic\n\n# Using a custom UMAP model\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n\n# Train our model\ntopic_model = BERTopic(umap_model=umap_model)\ntopics, probs = topic_model.fit_transform(abstracts)\n```", "```py\n>>> topic_model.get_topic_info()\nTopic    Count    Name\n0    -1    11648    -1_of_the_and_to\n1    0    1554    0_question_answer_questions_qa\n2    1    620    1_hate_offensive_toxic_detection\n3    2    578    2_summarization_summaries_summary_abstractive\n4    3    568    3_parsing_parser_dependency_amr\n...    ...    ...    ...\n317    316    10    316_prf_search_conversational_spoke\n318    317    10    317_crowdsourcing_workers_annotators_underline\n319    318    10    318_curriculum_nmt_translation_dcl\n320    319    10    319_botsim_menu_user_dialogue\n321    320    10    320_color_colors_ib_naming\n```", "```py\n>>> topic_model.get_topic(2)\n[('summarization', 0.029974019692323675),\n ('summaries', 0.018938088406361412),\n ('summary', 0.018019112468622436),\n ('abstractive', 0.015758156442697138),\n ('document', 0.011038627359130419),\n ('extractive', 0.010607624721836042),\n ('rouge', 0.00936377058925341),\n ('factual', 0.005651676100789188),\n ('sentences', 0.005262910357048789),\n ('mds', 0.005050565343932314)]\n```", "```py\n>>> topic_model.find_topics(\"topic modeling\")\n([17, 128, 116, 6, 235],\n [0.6753638370140129,\n  0.40951682679389345,\n  0.3985390076544335,\n  0.37922002441932795,\n  0.3769700288091359])\n```", "```py\n>>> topic_model.get_topic(17)\n[('topic', 0.0503756681079549),\n ('topics', 0.02834246786579726),\n ('lda', 0.015441277604137684),\n ('latent', 0.011458141214781893),\n ('documents', 0.01013764950401255),\n ('document', 0.009854201885298964),\n ('dirichlet', 0.009521114618288628),\n ('modeling', 0.008775384549157435),\n ('allocation', 0.0077508974418589605),\n ('clustering', 0.005909325849593925)]\n```", "```py\n>>> topics[titles.index('BERTopic: Neural topic modeling with a class-based TF-IDF procedure')]\n17\n```", "```py\nindex = titles.index('BERTopic: Neural topic modeling with a class-based TF-IDF procedure')\n\n# Calculate the topic distributions on a token-level\ntopic_distr, topic_token_distr = topic_model.approximate_distribution(abstracts[index][:90], calculate_tokens=True)\ndf = topic_model.visualize_approximate_distribution(abstracts[index][:90], topic_token_distr[0])\ndf\n```", "```py\ntopic_model.visualize_topics()\n```", "```py\n# Visualize a selection of topics and documents\ntopic_model.visualize_documents(titles, \n      topics=[0, 1, 2, 3, 4, 6, 7, 10, 12, \n 13, 16, 33, 40, 45, 46, 65])\n```", "```py\ntopic_model.visualize_barchart(topics=list(range(50, 58, 1)))\n```", "```py\n# Save original representations\nfrom copy import deepcopy\noriginal_topics = deepcopy(topic_model.topic_representations_)\n```", "```py\ndef topic_differences(model, original_topics, max_length=75, nr_topics=10):\n  \"\"\" For the first 10 topics, show the differences in \n  topic representations between two models \"\"\"\n  for topic in range(nr_topics):\n\n    # Extract top 5 words per topic per model\n    og_words = \" | \".join(list(zip(*original_topics[topic]))[0][:5])\n    new_words = \" | \".join(list(zip(*model.get_topic(topic)))[0][:5])\n\n    # Print a 'before' and 'after'\n    whitespaces = \" \" * (max_length - len(og_words))\n    print(f\"Topic: {topic}    {og_words}{whitespaces}-->     {new_words}\")\n```", "```py\n# KeyBERTInspired\nfrom bertopic.representation import KeyBERTInspired\nrepresentation_model = KeyBERTInspired()\n\n# Update our topic representations\nnew_topic_model.update_topics(abstracts, representation_model=representation_model)\n\n# Show topic differences\ntopic_differences(topic_model, new_topic_model)\n```", "```py\n# Part-of-Speech tagging\nfrom bertopic.representation import PartOfSpeech\nrepresentation_model = PartOfSpeech(\"en_core_web_sm\")\n\n# Use the representation model in BERTopic on top of the default pipeline\ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n# Show topic differences\ntopic_differences(topic_model, original_topics)\n```", "```py\n# Maximal Marginal Relevance\nfrom bertopic.representation import MaximalMarginalRelevance\nrepresentation_model = MaximalMarginalRelevance(diversity=0.5)\n\n# Use the representation model in BERTopic on top of the default pipeline\ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n# Show topic differences\ntopic_differences(topic_model, original_topics)\n```", "```py\nprompt = \"\"\"\nI have a topic that contains the following documents: \\n[DOCUMENTS]\nThe topic is described by the following keywords: [KEYWORDS]\n\nBased on the above information, give a short label of the topic.\n\"\"\"\n```", "```py\nI have a topic that contains the following documents: \\n[DOCUMENTS]\n```", "```py\nThe topic is described by the following keywords: [KEYWORDS]\n```", "```py\nBased on the above information, give a short label of the topic.\n```", "```py\n\"\"\"\nI have a topic that contains the following documents: \n- Our videos are also made possible by your support on patreon.co.\n- If you want to help us make more videos, you can do so on patreon.com or get one of our posters from our shop.\n- If you want to help us make more videos, you can do so there.\n- And if you want to support us in our endeavor to survive in the world of online video, and make more videos, you can do so on patreon.com.\n\nThe topic is described by the following keywords: videos video you our support want this us channel patreon make on we if facebook to patreoncom can for and more watch \n\nBased on the above information, give a short label of the topic.\n\"\"\"\n```", "```py\nfrom transformers import pipeline\nfrom bertopic.representation import TextGeneration\n\n# Text2Text Generation with Flan-T5\ngenerator = pipeline('text2text-generation', model='google/flan-t5-xl')\nrepresentation_model = TextGeneration(generator)\n\n# Use the representation model in BERTopic on top of the default pipeline\ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n# Show topic differences\ntopic_differences(topic_model, original_topics)\n```", "```py\nfrom bertopic.representation import OpenAI\n\n# OpenAI Representation Model\nprompt = \"\"\"\nI have a topic that contains the following documents: \\n[DOCUMENTS]\nThe topic is described by the following keywords: [KEYWORDS]\n\nBased on the information above, extract a short topic label in the following format:\ntopic: <topic label>\n\"\"\"\nrepresentation_model = OpenAI(model=\"gpt-3.5-turbo\", delay_in_seconds=10, chat=True)\n\n# Use the representation model in BERTopic on top of the default pipeline\ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n# Show topic differences\ntopic_differences(topic_model, original_topics)\n```", "```py\nimport cohere\nfrom bertopic.representation import Cohere\n\n# Cohere Representation Model\nco = cohere.Client(my_api_key)\nrepresentation_model = Cohere(co)\n\n# Use the representation model in BERTopic on top of the default pipeline\ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n# Show topic differences\ntopic_differences(topic_model, original_topics)\n```", "```py\nfrom langchain.llms import OpenAI\nfrom langchain.chains.question_answering import load_qa_chain\nfrom bertopic.representation import LangChain\n\n# Langchain representation model\nchain = load_qa_chain(OpenAI(temperature=0, openai_api_key=MY_API_KEY), chain_type=\"stuff\")\nrepresentation_model = LangChain(chain)\n\n# Use the representation model in BERTopic on top of the default pipeline\ntopic_model.update_topics(abstracts, representation_model=representation_model)\n\n# Show topic differences\ntopic_differences(topic_model, original_topics)\n```"]