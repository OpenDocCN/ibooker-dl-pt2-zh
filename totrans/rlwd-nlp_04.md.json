["```py\n(4\n  (2 (2 Steven) (2 Spielberg))\n    (4\n      (2 (2 brings) (3 us))\n      (4 (2 another) (4 masterpiece))))\n\n(1\n  (2 It)\n  (1\n    (1 (2 (2 's) (1 not))\n      (4 (2 a) (4 (4 great) (2 (2 monster) (2 movie)))))\n    (2 .)))\n```", "```py\npip install allennlp==2.5.0\npip install allennlp-models==2.5.0\n```", "```py\nfrom itertools import chain\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training import GradientDescentTrainer\nfrom allennlp.training.metrics import CategoricalAccuracy, F1Measure\nfrom allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import \\    StanfordSentimentTreeBankDatasetReader\n```", "```py\nEMBEDDING_DIM = 128\nHIDDEN_DIM = 128\n```", "```py\nreader = StanfordSentimentTreeBankDatasetReader()\ntrain_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt'\ndev_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt'\n```", "```py\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(reader, train_path,\n                                           batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(reader, dev_path,\n                                           batch_sampler=sampler)\n```", "```py\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(),\n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\n```", "```py\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_DIM)\n```", "```py\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n```", "```py\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n```", "```py\nself.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n                              out_features=vocab.get_vocab_size('labels'))\n```", "```py\nclass LstmClassifier(Model):\n    def __init__(self,\n                 word_embeddings: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n        super().__init__(vocab)\n        self.word_embeddings = word_embeddings\n\n        self.encoder = encoder\n\n        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n                                      out_features=vocab.get_vocab_size('labels'))\n\n        self.loss_function = torch.nn.CrossEntropyLoss()         ❶\n\n    def forward(self,                                            ❷\n                tokens: Dict[str, torch.Tensor],\n                label: torch.Tensor = None) -> torch.Tensor:\n        mask = get_text_field_mask(tokens)\n\n        embeddings = self.word_embeddings(tokens)\n        encoder_out = self.encoder(embeddings, mask)\n        logits = self.linear(encoder_out)\n\n        output = {\"logits\": logits}\n        if label is not None:\n            self.accuracy(logits, label)\n            self.f1_measure(logits, label)\n            output[\"loss\"] = self.loss_function(logits, label)   ❸\n\n        return output\n```", "```py\nself.loss_function = torch.nn.CrossEntropyLoss()\n```", "```py\noutput[\"loss\"] = self.loss_function(logits, label)\n```", "```py\noptimizer = optim.Adam(model.parameters())\n```", "```py\nMAX_EPOCHS = 100\nmodel = Model()\n\nfor epoch in range(MAX_EPOCHS):\n    for instance, label in train_set:\n        prediction = model.forward(instance)\n        loss = loss_function(prediction, label)\n        new_model = optimizer(model, loss)\n        model = new_model\n```", "```py\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(reader, dev_path, batch_sampler=sampler)\n```", "```py\nmodel = LstmClassifier(word_embeddings, encoder, vocab)\n```", "```py\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\n\ntrainer.train()\n```", "```py\n   def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {'accuracy': self.accuracy.get_metric(reset),\n                **self.f1_measure.get_metric(reset)}\n```", "```py\n    self.accuracy = CategoricalAccuracy()\n    self.f1_measure = F1Measure(positive_index)\n```", "```py\naccuracy: 0.7268, precision: 0.8206, recall: 0.8703, f1: 0.8448, batch_loss: 0.7609, loss: 0.7194 ||: 100%|##########| 267/267 [00:13<00:00, 19.28it/s]\naccuracy: 0.3460, precision: 0.3476, recall: 0.3939, f1: 0.3693, batch_loss: 1.5834, loss: 1.9942 ||: 100%|##########| 35/35 [00:00<00:00, 119.53it/s]\n```", "```py\npredictor = SentenceClassifierPredictor(model, dataset_reader=reader)\nlogits = predictor.predict('This is the best movie ever!')['logits']\n```", "```py\nlabel_id = np.argmax(logits)\nprint(model.vocab.get_token_from_index(label_id, 'labels'))\n```", "```py\ngit clone https:/./github.com/allenai/allennlp-server\npip install —editable allennlp-server\n```", "```py\n$ allennlp serve \\ \n    --archive-path examples/sentiment/model/model.tar.gz \\\n    --include-package examples.sentiment.sst_classifier \\\n    --predictor sentence_classifier_predictor \\\n    --field-name sentence\n```", "```py\ncurl -d '{\"sentence\": \"This is the best movie ever!\"}'\n    -H \"Content-Type: application/json\" \\\n    -X POST http:/./localhost:8000/predict\n```", "```py\n{\"logits\":[-0.2549717128276825,-0.35388273000717163,\n-0.0826418399810791,0.7183976173400879,0.23161858320236206]}\n```"]