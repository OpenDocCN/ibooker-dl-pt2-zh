- en: Chapter 9\. Deep learning for sequences and text
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章。 序列和文本的深度学习
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章包括*'
- en: How sequential data differs from nonsequential data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序数据与非顺序数据有何不同
- en: Which deep-learning techniques are suitable for problems that involve sequential
    data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些深度学习技术适用于涉及序列数据的问题
- en: How to represent text data in deep learning, including with one-hot encoding,
    multi-hot encoding, and word embedding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在深度学习中表示文本数据，包括独热编码，多热编码和词嵌入
- en: What RNNs are and why they are suitable for sequential problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是循环神经网络，以及为什么它们适合于顺序问题
- en: What 1D convolution is and why it is an attractive alternative to RNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是一维卷积，以及为什么它是循环神经网络的一个有吸引力的替代品
- en: The unique properties of sequence-to-sequence tasks and how to use the attention
    mechanism to solve them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列任务的独特特性以及如何使用注意力机制来解决它们
- en: 'This chapter focuses on problems involving sequential data. The essence of
    sequential data is the ordering of its elements. As you may have realized, we’ve
    dealt with sequential data before. Specifically, the Jena-weather data we introduced
    in [chapter 7](kindle_split_019.html#ch07) is sequential. The data can be represented
    as an array of arrays of numbers. Order certainly matters for the outer array
    because the measurements come in over time. If you reverse the order of the outer
    array—for instance, a rising air-pressure trend becomes a falling one—it has completely
    different implications if you are trying to predict future weather. Sequential
    data is everywhere in life: stock prices, electrocardiogram (ECG) readings, strings
    of characters in software code, consecutive frames of a video, and sequences of
    actions taken by a robot. Contrast those with nonsequential data such as the iris
    flowers in [chapter 3](kindle_split_014.html#ch03): it doesn’t matter if you alter
    the order of the four numeric features (sepal and petal length and width).^([[1](#ch09fn1)])'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍涉及序列数据的问题。序列数据的本质是其元素的排序。您可能已经意识到，我们之前已经处理过序列数据。具体来说，我们在[第七章](kindle_split_019.html#ch07)介绍的Jena-weather数据是序列数据。数据可以表示为数字数组的数组。外部数组的顺序当然很重要，因为测量是随着时间的推移而进行的。如果您改变外部数组的顺序——例如，上升的气压趋势变成下降的气压趋势——如果您尝试预测未来的天气，它就具有完全不同的含义。序列数据无处不在：股票价格，心电图（ECG）读数，软件代码中的字符串，视频的连续帧以及机器人采取的行动序列。将这些与非序列数据相对比，比如[第三章](kindle_split_014.html#ch03)中的鸢尾花：如果您改变这四个数字特征（萼片和花瓣的长度和宽度）的顺序并不重要。^([[1](#ch09fn1)])
- en: ¹
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Convince yourself that this is indeed the case in exercise 1 at the end of the
    chapter.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 说服自己这确实是事实，练习本章末尾的练习1。
- en: The first part of the chapter will introduce a fascinating type of model we
    mentioned in [chapter 1](kindle_split_011.html#ch01)—RNNs, or recurrent neural
    networks, which are designed specifically to learn from sequential data. We will
    build the intuition for what special features of RNNs make these models sensitive
    to the ordering of elements and the information it bears.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分将介绍我们在[第一章](kindle_split_011.html#ch01)中提到的一种引人入胜的模型——循环神经网络（RNNs），它们专门设计用于从序列数据中学习。我们将理解循环神经网络的特殊特性，以及这些模型敏感于元素的排序和相关信息的直觉。
- en: 'The second part of the chapter will talk about a special kind of sequential
    data: text, which is perhaps the most ubiquitous sequential data (especially in
    the web environment!). We will start by examining how text is represented in deep
    learning and how to apply RNNs on such representations. We will then move on to
    1D convnets and talk about why they are also powerful at processing text and how
    they can be attractive alternatives to RNNs for certain types of problems.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第二部分将讨论一种特殊的序列数据：文本，这可能是最常见的序列数据（尤其是在网络环境中！）。我们将首先研究深度学习中如何表示文本以及如何在这些表示上应用循环神经网络。然后我们将转向一维卷积神经网络，并讨论它们为何也在处理文本时非常强大，以及它们如何对某些类型的问题是循环神经网络的有吸引力的替代品。
- en: In the last part of the chapter, we will go a step further and explore sequence-based
    tasks that are slightly more complex than predicting a number or a class. In particular,
    we will venture into sequence-to-sequence tasks, which involve predicting an output
    sequence from an input one. We will use an example to illustrate how to solve
    basic sequence-to-sequence tasks with a new model architecture called the *attention
    mechanism*, which is becoming more and more important in the field of deep-learning-based
    natural language processing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一部分，我们将进一步探讨比预测数字或类别稍微复杂一点的基于序列的任务。特别是，我们将涉及序列到序列的任务，这涉及从输入序列预测输出序列。我们将用一个例子来说明如何使用一种新的模型架构——*注意机制*来解决基本的序列到序列任务，这在基于深度学习的自然语言处理领域变得越来越重要。
- en: By the end of this chapter, you should be familiar with common types of sequential
    data in deep learning, how they are presented as tensors, and how to use TensorFlow.js
    to write basic RNNs, 1D convnets, and attention networks to solve machine-learning
    tasks involving sequential data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您应该熟悉深度学习中常见类型的顺序数据，它们如何呈现为张量，以及如何使用TensorFlow.js编写基本的RNN、1D卷积网络和注意网络来解决涉及顺序数据的机器学习任务。
- en: The layers and models you will see in this chapter are among the most complex
    in this book. This is the cost that comes with their enhanced capacity for sequential-learning
    tasks. You may find some of them hard to grasp the first time you read about them,
    even though we strive to present them in a fashion that’s as intuitive as possible,
    with the help of diagrams and pseudo-code. If that’s the case, try playing with
    the example code and working through the exercises provided at the end of the
    chapter. In our experience, the hands-on experience makes it much easier to internalize
    complex concepts and architectures like the ones that appear in this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中您将看到的层和模型是本书中最复杂的。这是它们为顺序学习任务增强容量所付出的代价。即使我们努力以尽可能直观的方式呈现它们，配以图表和伪代码的帮助，您第一次阅读时可能会觉得其中一些很难理解。如果是这样，请尝试运行示例代码并完成章末提供的练习。根据我们的经验，实践经验使得内化复杂概念和架构变得更加容易，就像本章中出现的那些一样。
- en: '9.1\. Second attempt at weather prediction: Introducing RNNs'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 天气预测的第二次尝试：引入RNN
- en: The models we built for the Jena-weather problem in [chapter 8](kindle_split_020.html#ch08)
    threw away the order information. In this section, we will tell you why that’s
    the case and how we can bring the order information back by using RNNs. This will
    allow us to achieve superior prediction accuracies in the temperature-prediction
    task.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第8章](kindle_split_020.html#ch08)中为Jena天气问题构建的模型丢弃了顺序信息。在本节中，我们将告诉您为什么会这样，并且我们如何通过使用RNN将顺序信息带回来。这将使我们能够在温度预测任务中实现更准确的预测。
- en: 9.1.1\. Why dense layers fail to model sequential order
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1\. 为什么密集层无法建模顺序
- en: Since we described the Jena-weather dataset in detail in the previous chapter,
    we will go over the dataset and the related machine-learning task only briefly
    here. The task involves predicting the temperature 24 hours from a certain moment
    in time by using readings from 14 weather instruments (such as temperature, air
    pressure, and wind speed) over a 10-day period leading up to the moment. The instrument
    readings are taken at regular intervals of 10 minutes, but we downsample them
    by a factor of 6 to once per hour for the sake of manageable model size and training
    time. So, each training example comes with a feature tensor of shape `[240, 14]`,
    where 240 is the number of time steps over the 10-day period, and 14 is the number
    of different weather-instrument readings.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在上一章节中已经详细描述了Jena天气数据集，所以在这里我们将仅简要讨论数据集和相关的机器学习任务。该任务涉及使用过去10天内一段时间内的14个天气仪器（如温度、气压和风速）的读数来预测从某一时刻开始的24小时后的温度。仪器读数以10分钟的固定间隔进行，但我们将其降采样6倍，以每小时一次，以便使模型大小和训练时间可管理。因此，每个训练示例都带有一个形状为`[240,
    14]`的特征张量，其中240是10天内的时间步数，14是不同天气仪器读数的数量。
- en: 'When we tried a linear-regression model and an MLP on the task in the previous
    chapter, we flattened the 2D input features to 1D by using a `tf.layers.flatten`
    layer (see [listing 8.2](kindle_split_020.html#ch08ex02) and [figure 8.2](kindle_split_020.html#ch08fig02)).
    The flattening step was necessary because both the linear regressor and the MLP
    used dense layers to handle the input data, and dense layers require the input
    data to be 1D for each input example. This means that the information from all
    the time steps is mixed together in a way that erases the significance of which
    step comes first and which one next, which time step follows which other one,
    how far apart two time steps are, and so forth. In other words, it doesn’t matter
    how we order the 240 time steps when we flatten the 2D tensor of shape `[240,
    14]` into the 1D tensor of shape `[3360]` as long as we are consistent between
    training and inference. You can confirm this point experimentally in exercise
    1 at the end of this chapter. But from a theoretical point of view, this lack
    of sensitivity to the order of data elements can be understood in the following
    way. At the core of a dense layer is a set of linear equations, each of which
    multiplies every input feature value [*x*[1], *x*[2], …, *x[n]*] with a tunable
    coefficient from the kernel [*k*[1], *k*[2], …, *k[n]*]:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章的任务中，当我们尝试了一个线性回归模型和一个MLP时，我们使用了`tf.layers.flatten`层将2D输入特征展平为1D（参见[清单 8.2](kindle_split_020.html#ch08ex02)和[图
    8.2](kindle_split_020.html#ch08fig02)）。展平步骤是必要的，因为线性回归器和MLP都使用了密集层来处理输入数据，而密集层要求每个输入示例的输入数据为1D。这意味着所有时间步的信息以一种方式混合在一起，使得哪个时间步首先出现，接下来是哪个时间步，一个时间步距离另一个时间步有多远等等的重要性被抹去了。换句话说，当我们将形状为`[240,
    14]`的2D张量展平为形状为`[3360]`的1D张量时，我们如何对240个时间步进行排序并不重要，只要我们在训练和推断之间保持一致即可。您可以在本章末尾的练习1中通过实验证实这一点。但从理论上讲，这种对数据元素顺序缺乏敏感性的缺点可以用以下方式理解。在密集层的核心是一组线性方程，每个方程都将每个输入特征值[*x*[1]，*x*[2]，…，*x[n]*]与来自核[*k*[1]，*k*[2]，…，*k[n]*]的可调系数相乘：
- en: equation 9.1\.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 9.1\.
- en: '![](09eqa01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](09eqa01.jpg)'
- en: '[Figure 9.1](#ch09fig01) provides a visual representation of how a dense layer
    works: the paths leading from the input elements to the output of the layer are
    graphically symmetric with one another, reflecting the mathematical symmetry in
    [equation 9.1](#ch09equ01). The symmetry is *undesirable* when we deal with sequential
    data because it renders the model blind to the order among the elements.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.1](#ch09fig01) 提供了密集层的工作原理的可视化表示：从输入元素到层输出的路径在图形上对称，反映了[方程式 9.1](#ch09equ01)
    中的数学对称性。当我们处理序列数据时，这种对称性是*不可取的*，因为它使模型对元素之间的顺序视而不见。'
- en: Figure 9.1\. The internal architecture of a dense layer. The multiplication
    and addition performed by a dense layer is symmetric with respect to its inputs.
    Contrast this with a simpleRNN layer ([figure 9.2](#ch09fig02)), which breaks
    the symmetry by introducing step-by-step computation. Note that we assume the
    input has only four elements and omit the bias terms for simplicity. Also, we
    show the operations for only one output unit of the dense layer. The remaining
    units are represented as the stack of obscured boxes in the background.
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.1\. 密集层的内部架构。密集层执行的乘法和加法与其输入对称。与简单RNN层（[图 9.2](#ch09fig02)）相比，它通过引入逐步计算来打破对称性。请注意，我们假设输入只有四个元素，出于简单起见，省略了偏置项。此外，我们仅显示了密集层的一个输出单元的操作。其余的单元被表示为背景中的一堆模糊的框。
- en: '![](09fig01_alt.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig01_alt.jpg)'
- en: 'In fact, there is an easy way to show that our dense-layer-based approach (the
    MLPs, even with regularization) did not provide a very good solution to the temperature-prediction
    problem: comparing its accuracy with the accuracy we can obtain from a commonsense,
    non-machine-learning approach.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有一个简单的方法可以显示，我们基于密集层的方法（即MLP，即使加入正则化）并没有很好地解决温度预测问题：将其准确性与我们从常识、非机器学习方法中获得的准确性进行比较。
- en: What is the commonsense approach we are speaking of? Predict the temperature
    as the last temperature reading in the input features. To put this simply, just
    pretend that the temperature 24 hours from now will be the same as the temperature
    right now! This approach makes “gut sense” because we know from everyday experience
    that the temperature tomorrow tends to be close to the temperature today (that
    is, at exactly the same time of day). It is a very simple algorithm and gives
    a reasonable guess that should beat all other similarly simple algorithms (such
    as predicting the temperature as the temperature from 48 hours ago).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的常识方法是什么？将温度预测为输入特征中的最后一个温度读数。简单地说，就假装从现在起 24 小时后的温度会与当前温度相同！这种方法是“直觉上合理”的，因为我们从日常经验中知道，明天的温度往往接近于今天的温度（也就是说，在同一天的同一时间）。这是一个非常简单的算法，并提供了一个合理的猜测，应该能击败所有其他类似简单的算法（例如，将温度预测为
    48 小时前的温度）。
- en: 'The jena-weather directory of tfjs-examples that we used in [chapter 8](kindle_split_020.html#ch08)
    provides a command for you to assess the accuracy of this commonsense approach:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第 8 章](kindle_split_020.html#ch08) 中使用的 tfjs-examples 的 jena-weather 目录提供了一个命令，用于评估这种常识方法的准确性：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `yarn train-rnn` command calls the script train-rnn.js and performs computation
    in the Node.js-based backend environment.^([[2](#ch09fn2)]) We will come back
    to this mode of operation when we explore RNNs shortly. The command should give
    you the following screen output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn train-rnn` 命令调用了 train-rnn.js 脚本，并在基于 Node.js 的后端环境中执行计算。^([[2](#ch09fn2)])
    我们将在不久的将来回到这种操作模式，当我们探索 RNN 时。该命令应该会给出以下屏幕输出：'
- en: ²
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The code that implements this commonsense, non-machine-learning approach is
    in the function named `getBaselineMeanAbsoluteError()` in jena-weather/models.js.
    It uses the `forEachAsync()` method of the `Dataset` object to iterate through
    all batches of the validation subset, compute the MAE loss for each batch, and
    accumulate all the losses to obtain the final loss.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实现这种常识、非机器学习方法的代码位于 jena-weather/models.js 中名为 `getBaselineMeanAbsoluteError()`
    的函数中。它使用 `Dataset` 对象的 `forEachAsync()` 方法来遍历验证子集的所有批次，计算每个批次的 MAE 损失，并累积所有损失以获得最终损失。
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, the simple non-machine-learning approach yields a mean absolute prediction
    error of about 0.29 (in normalized terms), which is about equal to (if not slightly
    better than) the best validation error we got from the MLP in [chapter 8](kindle_split_020.html#ch08)
    (see [figure 8.4](kindle_split_020.html#ch08fig04)). In other words, the MLP,
    with or without regularization, wasn’t able to beat the accuracy from the commonsense
    baseline method reliably!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简单的非机器学习方法产生了约为 0.29（以归一化术语表示）的平均绝对预测误差，这与我们从 [第 8 章](kindle_split_020.html#ch08)
    中 MLP 获得的最佳验证误差相当（见 [图 8.4](kindle_split_020.html#ch08fig04)）。换句话说，MLP，无论是否进行正则化，都无法可靠地击败来自常识基线方法的准确性！
- en: 'Such observations are not uncommon in machine learning: it’s not always easy
    for machine learning to beat a commonsense approach. In order to beat it, the
    machine-learning model sometimes needs to be carefully designed or tuned through
    hyperparameter optimization. Our observation also underlines how important it
    is to create a non-machine-learning baseline for comparison when working on a
    machine-learning problem. We certainly want to avoid wasting all the effort on
    building a machine-learning algorithm that can’t even beat a much simpler and
    computationally cheaper baseline! Can we beat the baseline in the temperature-prediction
    problem? The answer is yes, and we will rely on RNNs to do that. Let’s now take
    a look at how RNNs capture and process sequential order.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的观察在机器学习中并不少见：机器学习并不总是能够击败常识方法。为了击败它，机器学习模型有时需要通过超参数优化进行精心设计或调整。我们的观察还强调了在处理机器学习问题时创建非机器学习基准进行比较的重要性。当然，我们肯定要避免将所有的精力都浪费在构建一个甚至连一个简单且计算成本更低的基线都无法击败的机器学习算法上！我们能够在温度预测问题中击败基线吗？答案是肯定的，我们将依靠
    RNN 来做到这一点。现在让我们来看看 RNN 如何捕捉和处理序列顺序。
- en: 9.1.2\. How RNNs model sequential order
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2\. RNNs 如何建模序列顺序
- en: Panel A of [figure 9.2](#ch09fig02) shows the internal structure of an RNN layer
    by using a short, four-item sequence. There are several variants of RNN layers
    out there, and the diagram shows the simplest variant, which is referred to as
    simpleRNN and is available in TensorFlow.js as the `tf.layers.simpleRNN()` factory
    function. We will talk about more complicated RNN variants later in this chapter,
    but for now we will focus on simpleRNN.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.2](#ch09fig02)的A面通过使用一个简短的四项序列显示了RNN层的内部结构。有几种RNN层的变体，图表显示了最简单的变体，称为SimpleRNN，并且在TensorFlow.js中可用作`tf.layers.simpleRNN()`工厂函数。我们稍后将在本章中讨论更复杂的RNN变体，但现在我们将专注于SimpleRNN。'
- en: Figure 9.2\. The “unrolled” (panel A) and “rolled” (panel B) representations
    of the internal structure of simpleRNN. The rolled view (panel B) represents the
    same algorithm as the unrolled one, albeit in a more succinct form. It illustrates
    simpleRNN’s sequential processing of input data in a more concise fashion. In
    the rolled representation in panel B, the connection that goes back from output
    (*y*) into the model itself is the reason why such layers are called *recurrent*.
    As in [figure 9.1](#ch09fig01), we display only four input elements and omit the
    bias terms for simplicity.
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2. SimpleRNN内部结构的“展开”（A面）和“卷曲”（B面）表示。卷曲视图（B面）以更简洁的形式表示与展开视图相同的算法。它以更简洁的方式说明了SimpleRNN对输入数据的顺序处理。在面板B中的卷曲表示中，从输出（*y*）返回到模型本身的连接是这些层被称为*循环*的原因。与[图9.1](#ch09fig01)中一样，我们仅显示了四个输入元素，并简化了偏差项。
- en: '![](09fig02_alt.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig02_alt.jpg)'
- en: The diagram shows how the time slices of the input (*x*[1], *x*[2], *x*[3],
    …) are processed step-by-step. At each step, *x[i]* is processed by a function
    (*f*()), represented as the rectangular box at the center of the diagram. This
    yields an output (*y[i]*) that gets combined with the next input slice (*x[i]*[+1])
    as the input to the *f*() at the next step. It is important to note that even
    though the diagram shows four separate boxes with function definitions in them,
    they in fact represent the same function. This function (*f*()) is called the
    *cell* of the RNN layer. It is used in an iterative fashion during the invocation
    of the RNN layer. Therefore, an RNN layer can be viewed as “an RNN cell wrapped
    in a `for` loop.”^([[3](#ch09fn3)])
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了输入的时间片段（*x*[1]，*x*[2]，*x*[3]，…）是如何逐步处理的。在每一步中，*x[i]* 通过一个函数（*f*()）进行处理，该函数表示为图表中心的矩形框。这产生了一个输出（*y[i]*），它与下一个输入片段（*x[i]*[+1]）结合，作为下一步
    *f*() 的输入。重要的是要注意，即使图表显示了四个具有函数定义的单独框，它们实际上表示相同的函数。这个函数（*f*()）称为RNN层的*cell*。在调用RNN层期间，它以迭代的方式使用。因此，可以将RNN层视为“在`for`循环中包装的RNN单元。”^([[3](#ch09fn3)])
- en: ³
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Quote attributed to Eugene Brevdo.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 引述于 Eugene Brevdo。
- en: 'Comparing the structure of simpleRNN and that of the dense layer ([figure 9.1](#ch09fig01)),
    we can see two major differences:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 比较SimpleRNN的结构和密集层的结构（[图9.1](#ch09fig01)），我们可以看到两个主要区别：
- en: SimpleRNN processes the input elements (time steps) one step at a time. This
    reflects the sequential nature of the inputs, something a dense layer can’t do.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SimpleRNN逐步处理输入元素（时间步）。这反映了输入的顺序性，这是密集层无法做到的。
- en: 'In simpleRNN, the processing at every input time step generates an output (*y[i]*).
    The output from a previous time step (for example, *y*[1]) is used by the layer
    when it processes the next time step (such as *x*[2]). This is the reason behind
    the “recurrent” part of the name RNN: the output from previous time steps flows
    back and becomes an input for later time steps. Recurrence doesn’t happen in layer
    types such as dense, conv2d, and maxPooling2d. Those layers don’t involve output
    information flowing back and hence are referred to as *feedforward* layers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SimpleRNN中，每个输入时间步的处理都会生成一个输出（*y[i]*）。前一个时间步的输出（例如，*y*[1]）在处理下一个时间步（例如 *x*[2]）时由层使用。这就是RNN名称中“循环”部分的原因：来自先前时间步的输出会流回并成为后续时间步的输入。在诸如dense、conv2d和maxPooling2d之类的层类型中不会发生递归。这些层不涉及输出信息的回流，因此被称为*前馈*层。
- en: Due to these unique features, simpleRNN breaks the symmetry between the input
    elements. It is sensitive to the order of the input elements. If you reorder the
    elements of a sequential input, the output will be altered as a result. This distinguishes
    simpleRNN from a dense layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些独特的特征，SimpleRNN打破了输入元素之间的对称性。它对输入元素的顺序敏感。如果重新排列顺序输入的元素，则输出将随之而改变。这使SimpleRNN与密集层有所区别。
- en: Panel B of [figure 9.2](#ch09fig02) is a more abstract representation of simpleRNN.
    It is referred to as a *rolled* RNN diagram, versus the *unrolled* diagram in
    panel A, because it “rolls” all time steps into a single loop. The rolled diagram
    corresponds nicely to a `for` loop in programming languages, which is actually
    how simpleRNN and other types of RNNs are implemented under the hood in TensorFlow.js.
    But instead of showing the real code, let’s look at the much shorter pseudo-code
    for simpleRNN in the following listing, which you can view as the implementation
    of the simpleRNN architecture shown in [figure 9.2](#ch09fig02). This will help
    you focus on the essence of how the RNN layer works.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.2](#ch09fig02) 的 B 面板是对简单循环神经网络的更抽象的表示。它被称为 *rolled* RNN 图表，与 A 面板中的 *unrolled*
    图表相对应，因为它将所有时间步骤“卷”成一个循环。滚动图表很好地对应于编程语言中的 `for` 循环，这实际上是 TensorFlow.js 中实现 simpleRNN
    和其他类型的 RNN 的方式。但是，与其显示真实的代码，不如看一下下面的简单RNN的伪代码，您可以将其视为[图 9.2](#ch09fig02)中所示的 simpleRNN
    结构的实现。这将帮助您专注于RNN层的工作原理的本质。'
- en: Listing 9.1\. Pseudo-code for the internal computation of simpleRNN
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1\. simpleRNN 的内部计算的伪代码
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** y corresponds to the y in [figure 9.2](#ch09fig02). The state is initialized
    to zeros in the beginning.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** y 对应于[图 9.2](#ch09fig02)中的 y。状态在开始时被初始化为零。'
- en: '***2*** x corresponds to the x in [figure 9.2](#ch09fig02). The for loop iterates
    over all time steps of the input sequence.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** x 对应于[图 9.2](#ch09fig02)中的 x。for 循环遍历输入序列的所有时间步。'
- en: '***3*** W and U are the weight matrices for the input and the state (that is,
    the output that loops back and becomes the recurrent input), respectively. This
    is also where the output for time step i becomes the state (recurrent input) for
    time step i + 1.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** W 和 U 分别是输入和状态的权重矩阵（即，回路回传并成为重复输入的输出）。这也是时间步骤 i 的输出成为时间步骤 i + 1 的状态（重复输入）的地方。'
- en: In [listing 9.1](#ch09ex01), you can see that the output at time step `i` becomes
    the “state” for the next time step (next iteration). *State* is an important concept
    for RNNs. It is how an RNN “remembers” what happened in the steps of the input
    sequence it has already seen. In the `for` loop, this memory state gets combined
    with future input steps and becomes the new memory state. This gives the simpleRNN
    the ability to react to the same input element differently depending on what elements
    have appeared in the sequence before. This type of memory-based sensitivity is
    at the heart of sequential processing. As a simple example, if you are trying
    to decode Morse code (made of dots and dashes), the meaning of a dash depends
    on the sequence of dots and dashes that go before (and after) it. As another example,
    in English, the word *last* can have completely different meanings depending on
    what words go before it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 9.1](#ch09ex01)中，您可以看到时间步 `i` 的输出成为下一个时间步（下一个迭代）的“状态”。*State* 是RNN的一个重要概念。这是RNN“记住”已经看过的输入序列步骤的方式。在
    `for` 循环中，这个记忆状态与未来的输入步骤结合起来，并成为新的记忆状态。这使得 simpleRNN 能够根据之前序列中出现的元素来不同地处理相同的输入元素。这种基于记忆的敏感性是顺序处理的核心。作为一个简单的例子，如果您试图解码莫尔斯电码（由点和短划组成），则短划的含义取决于先前（以及之后）的点和短划的序列。另一个例子，在英语中，单词
    *last* 可以根据之前的单词有完全不同的含义。
- en: SimpleRNN is appropriately named because its output and state are the same thing.
    Later, we will explore more complex and powerful RNN architectures. Some of these
    have output and state as two separate things; others even have multiple states.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SimpleRNN 的命名适当，因为其输出和状态是相同的东西。稍后，我们将探索更复杂和更强大的RNN体系结构。其中一些具有输出和状态作为两个单独的东西；其他甚至具有多个状态。
- en: Another thing worth noting about RNNs is that the `for` loop enables them to
    process input sequences made of an arbitrary number of input steps. This is something
    that can’t be done through flattening a sequential input and feeding it to a dense
    layer because a dense layer can only take a fixed input shape.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RNN的另一件值得注意的事情是 `for` 循环使它们能够处理由任意数量的输入步骤组成的输入序列。这是通过将序列输入展平并将其馈送到密集层中无法完成的，因为密集层只能接受固定的输入形状。
- en: 'Furthermore, the `for` loop reflects another important property of RNNs: *parameter
    sharing*. What we mean by this is the fact that the same weight parameters (`W`
    and `U`) are used in all time steps. The alternative is to have a unique value
    of `W` (and `U`) for every time step. That would be undesirable because 1) it
    limits the number of time steps that can be processed by the RNN, and 2) it leads
    to a dramatic increase in the number of tunable parameters, which will increase
    the amount of computation and the likelihood of overfitting during training. Therefore,
    the RNN layers are similar to conv2d layers in convnets in that they use parameter
    sharing to achieve efficient computation and protect against overfitting—although
    the recurrent and conv2d layers achieve parameter sharing in different ways. While
    conv2d layers exploit the translational invariance along spatial dimensions, RNN
    layers exploit translational invariance along the *time* dimension.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`for` 循环反映了 RNN 的另一个重要属性：*参数共享*。我们所说的是，相同的权重参数（`W` 和 `U`）在所有时间步中都被使用。另一种选择是对每个时间步使用唯一的
    `W`（和 `U`）值。这是不可取的，因为 1）它限制了 RNN 可以处理的时间步数，2）它导致可调参数数量的显著增加，这将增加计算量并增加训练期间过拟合的可能性。因此，RNN
    层类似于 convnets 中的 conv2d 层，它们使用参数共享来实现高效计算并防止过拟合——尽管循环和 conv2d 层以不同的方式实现参数共享。虽然
    conv2d 层利用了沿空间维度的平移不变性，但 RNN 层利用了沿*时间*维度的平移不变性。
- en: '[Figure 9.2](#ch09fig02) shows what happens in a simpleRNN during inference
    time (the forward pass). It doesn’t show how the weight parameters (`W` and `U`)
    are updated during training (the backward pass). However, the training of RNNs
    follows the same backpropagation rules that we introduced in [section 2.2.2](kindle_split_013.html#ch02lev2sec9)
    ([figure 2.8](kindle_split_013.html#ch02fig08))—that is, starting from the loss,
    backtracking the list of operations, taking their derivatives, and accumulating
    gradient values through them. Mathematically, the backward pass on a recurrent
    network is basically the same as that on a feedforward one. The only difference
    is that the backward pass of an RNN layer goes backwards in time, on an unrolled
    graph like the one in panel A of [figure 9.2](#ch09fig02). This is why the process
    of training RNNs is sometimes referred to as *backpropagation through time* (BPTT).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.2](#ch09fig02)显示了在推断时间（前向传播）中简单RNN中发生的情况。它并不显示在训练期间（后向传播）如何更新权重参数（`W`
    和 `U`）。然而，RNN 的训练遵循我们在 [2.2.2 节](kindle_split_013.html#ch02lev2sec9)（[图 2.8](kindle_split_013.html#ch02fig08)）中介绍的相同反向传播规则——即从损失开始，回溯操作列表，取其导数，并通过它们累积梯度值。数学上，递归网络上的后向传播基本上与前向传播相同。唯一的区别是
    RNN 层的反向传播沿时间倒退，在像 [图 9.2](#ch09fig02) 面板 A 中的展开图中。这就是为什么有时将训练 RNN 的过程称为*时间反向传播*（BPTT）。'
- en: SimpleRNN in action
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SimpleRNN 的实现
- en: That’s enough abstract musing about simpleRNN and RNNs in general. Let’s now
    look at how to create a simpleRNN layer and include it in a model object, so we
    can use it to predict temperatures more accurately than before. The code in [listing
    9.2](#ch09ex02) (excerpted from jena-weather/train-rnn.js) is how this is done.
    For all the internal complexity of the simpleRNN layer, the model itself is fairly
    simple. It has only two layers. The first one is simpleRNN, configured to have
    32 units. The second one is a dense layer that uses the default linear activation
    to generate continuous numerical predictions for the temperature. Note that because
    the model starts with an RNN, it is no longer necessary to flatten the sequential
    input (compare this with [listing 8.3](kindle_split_020.html#ch08ex03) in the
    previous chapter, when we created MLPs for the same problem). In fact, if we put
    a flatten layer before the simpleRNN layer, an error would be thrown because RNN
    layers in TensorFlow.js expect their inputs to be at least 3D (including the batch
    dimension).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 simpleRNN 和 RNN 总体的抽象思考已经足够了。现在让我们看看如何创建一个 simpleRNN 层并将其包含在模型对象中，这样我们就可以比以前更准确地预测温度了。[清单
    9.2](#ch09ex02) 中的代码（从 jena-weather/train-rnn.js 中摘录）就是这样做的。尽管 simpleRNN 层的内部复杂性很高，但模型本身相当简单。它只有两层。第一层是
    simpleRNN，配置为具有 32 个单元。第二个是使用默认线性激活生成温度的连续数值预测的密集层。请注意，因为模型以一个 RNN 开始，所以不再需要展平序列输入（与前一章中为同一问题创建
    MLPs 时进行比较时）。实际上，如果我们在 simpleRNN 层之前放置一个 flatten 层，将会抛出错误，因为 TensorFlow.js 中的
    RNN 层期望它们的输入至少是 3D（包括批处理维度）。
- en: Listing 9.2\. Creating a simpleRNN-based model for the temperature-prediction
    problem
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 9.2 创建用于温度预测问题的基于 simpleRNN 的模型
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1*** The hard-coded unit count of the simpleRNN layer is a value that works
    well, determined through hand-tuning of the hyperparameter.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** simpleRNN 层的硬编码单元数是通过超参数的手工调整得到的一个很好的值。'
- en: '***2*** The first layer of the model is a simpleRNN layer. There is no need
    to flatten the sequential input, which has a shape of [null, 240, 14].'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 模型的第一层是一个 simpleRNN 层。不需要对顺序输入进行展平，其形状为 [null, 240, 14]。'
- en: '***3*** We end the model with a dense layer with a single unit and the default
    linear activation for the regression problem.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 我们用一个具有单个单元且默认线性激活函数的密集层来结束模型，这适用于回归问题。'
- en: 'To see the simpleRNN model in action, use the following command:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看 simpleRNN 模型的运行情况，请使用以下命令：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The RNN model is trained in the backend environment using tfjs-node. Due to
    the amount of computation involved in the BPTT-based RNN training, it would be
    much harder and slower, if not impossible, to train the same model in the resource-restricted
    browser environment. If you have a CUDA environment set up properly, you can add
    the `--gpu` flag to the command to get a further boost in training speed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 模型在后端环境中使用 tfjs-node 进行训练。由于基于 BPTT 的 RNN 训练涉及到大量计算，如果在资源受限的浏览器环境中训练相同的模型将会更加困难和缓慢，甚至不可能完成。如果您已经正确设置了
    CUDA 环境，您可以在命令中添加 `--gpu` 标志来进一步提高训练速度。
- en: The `--logDir` flag in the previous command causes the model-training process
    to log the loss values to the specified directory. You can load and plot the loss
    curves in the browser using a tool called TensorBoard. [Figure 9.3](#ch09fig03)
    is a screenshot from TensorBoard. At the level of JavaScript code, this is achieved
    by configuring the `tf.LayersModel.fit()` call with a special callback that points
    to the log directory. [Info box 9.1](#ch09sb01) contains further information on
    how this is done.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令中的 `--logDir` 标志使得模型训练过程将损失值记录到指定的目录中。可以使用一个名为 TensorBoard 的工具在浏览器中加载并绘制损失曲线。[图
    9.3](#ch09fig03) 是 TensorBoard 的一个截图。在 JavaScript 代码级别，通过使用指向日志目录的特殊回调函数来配置 `tf.LayersModel.fit()`
    调用来实现这个功能。[信息框 9.1](#ch09sb01) 中包含了关于如何实现这一功能的进一步信息。
- en: Figure 9.3\. MAE loss curves from the simpleRNN model built for the Jena-temperature-prediction
    problem. This chart is a screenshot from TensorBoard serving the logs from the
    Node.js-based training of the simpleRNN model.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.3 Jena-temperature-prediction 问题的 simpleRNN 模型的 MAE 损失曲线。该图是 TensorBoard
    的一个截图，显示了基于 Node.js 进行的 simpleRNN 模型训练的日志。
- en: '![](09fig03_alt.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig03_alt.jpg)'
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Using the TensorBoard callbacks to monitor long-running model training in
    Node.js**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 TensorBoard 回调函数在 Node.js 中监控长时间的模型训练**'
- en: In [chapter 8](kindle_split_020.html#ch08), we introduced callbacks from the
    tfjs-vis library that help you monitor `tf.LayersModel.fit()` calls in the browser.
    However, tfjs-vis is a browser-only library and is not applicable to Node.js.
    By default, `tf.LayersModel.fit()` in tfjs-node (or tfjs-node-gpu) renders progress
    bars and displays loss and timing metrics in the terminal. While this is lightweight
    and informative, text and numbers are often a less intuitive and less visually
    appealing way to monitor long-running model training than a GUI. For example,
    small changes in the loss value over an extensive period of time, which is often
    what we are looking for during late stages of model training, are much easier
    to spot in a chart (with properly set scales and grids) than in a body of text.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 8 章](kindle_split_020.html#ch08) 中，我们介绍了来自 tfjs-vis 库的回调函数，以帮助您在浏览器中监控
    `tf.LayersModel.fit()` 的调用。然而，tfjs-vis 是仅适用于浏览器的库，不适用于 Node.js。在 tfjs-node（或 tfjs-node-gpu）中，默认情况下，`tf.LayersModel.fit()`
    在终端中以进度条形式呈现，并显示损失和时间指标。虽然这种方式简洁明了而且信息量大，但文字和数字往往不如图形界面直观和吸引人。例如，对于模型训练后期我们经常寻找的损失值的微小变化，使用图表（具有适当的刻度和网格线）要比使用一段文本更容易发现。
- en: 'Luckily, a tool called *TensorBoard* can help us in the backend environment.
    TensorBoard was originally designed for TensorFlow (Python), but tfjs-node and
    tfjs-node-gpu can write data in a compatible format that can be ingested by TensorBoard.
    To log loss and metric values to TensorBoard from a `tf.LayersModel.fit()` or
    `tf.LayersModel.fitDataset()` call, follow this pattern:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一个名为 *TensorBoard* 的工具可以帮助我们在后端环境中完成这项工作。TensorBoard 最初是为 TensorFlow（Python）设计的，但
    tfjs-node 和 tfjs-node-gpu 可以以兼容格式写入数据，这些数据可以被 TensorBoard 处理。要将损失和指标值记录到 TensorBoard
    以用于 `tf.LayersModel.fit()` 或 `tf.LayersModel.fitDataset()` 的调用中，请按照下列模式操作：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These calls will write the loss values, along with any metrics configured during
    the `compile()` call, to the directory /path/to/my/logdir. To view the logs in
    the browser,
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些调用会将损失值和在`compile()`调用期间配置的任何指标写入目录/path/to/my/logdir。要在浏览器中查看日志，
- en: Open a separate terminal.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个单独的终端。
- en: 'Install TensorBoard with the following command (unless it’s already installed):
    `pip install tensorboard`'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装 TensorBoard（如果尚未安装）：`pip install tensorboard`
- en: 'Start the backend server of TensorBoard, and point it to the log directory
    specified during the callback creation: `tensorboard --logdir /path/to/my/logdir`'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 TensorBoard 的后端服务器，并指向在回调创建过程中指定的日志目录：`tensorboard --logdir /path/to/my/logdir`
- en: In the web browser, navigate to the http:// URL displayed by the TensorBoard
    process. Then the loss and metric charts such as those shown in [figures 9.3](#ch09fig03)
    and [9.5](#ch09fig05) will appear in the beautiful web UI of TensorBoard.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Web浏览器中，导航至TensorBoard进程显示的http:// URL。然后，类似于[figures 9.3](#ch09fig03)和[9.5](#ch09fig05)中显示的损失和指标图表将出现在TensorBoard的美观Web
    UI中。
- en: '|  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The text summary of the simpleRNN model created by [listing 9.2](#ch09ex02)
    looks like the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[listing 9.2](#ch09ex02)创建的simpleRNN模型的文本摘要如下：'
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It has significantly fewer weight parameters than the MLP we used before (1,537
    versus 107,585, or a reduction by a factor of 70), but it achieves a lower validation
    MAE loss (that is, more accurate predictions) than the MLP during training (0.271
    versus 0.289). This small but solid reduction in the temperature-prediction error
    highlights the power of parameter sharing based on temporal invariance and the
    advantages of RNNs in learning from sequence data like the weather data we are
    dealing with.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它的权重参数明显少于我们之前使用的MLP（1,537与107,585相比，减少了70倍），但在训练过程中实现了更低的验证MAE损失（即更准确的预测）（0.271与0.289）。这种对温度预测误差的小但明显的减少突显了基于时间不变性的参数共享的强大力量以及RNN在学习诸如我们处理的天气数据之类的序列数据方面的优势。
- en: You might have noticed that even though simpleRNN involves a relatively small
    number of weight parameters, its training and inference take much longer compared
    to feedforward models such as MLP. This is a major shortcoming of RNNs, one in
    which it is impossible to parallelize the operations over the time steps. Such
    parallelization is not achievable because subsequent steps depend on the state
    values computed in previous steps (see [figure 9.2](#ch09fig02) and the pseudo-code
    in [listing 9.1](#ch09ex01)). If we use the Big-O notation, the forward pass on
    an RNN takes an O(*n*) amount of time, where *n* is the number of input time steps.
    The backward pass (BPTT) takes another O(*n*) amount of time. The input future
    of the Jena-weather problem consists of a large number of (240) time steps, which
    leads to the slow training time seen previously. This is the main reason why we
    train the model in tfjs-node instead of in the browser.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，即使simpleRNN涉及相对少量的权重参数，与MLP等前馈模型相比，其训练和推断时间要长得多。这是RNN的一个主要缺点，即无法在时间步长上并行化操作。这种并行化是不可实现的，因为后续步骤依赖于先前步骤中计算的状态值（参见[figure
    9.2](#ch09fig02)和[listing 9.1](#ch09ex01)中的伪代码）。如果使用大O符号表示，RNN的前向传递需要O（*n*）时间，其中*n*是输入时间步的数量。后向传递（BPTT）需要另外O（*n*）时间。耶拿天气问题的输入未来包含大量（240）时间步，这导致了之前看到的较慢的训练时间。这也是为什么我们在tfjs-node而不是在浏览器中训练模型的主要原因。
- en: This situation of RNNs is in contrast to feedforward layers such as dense and
    conv2d. In those layers, computation can be parallelized among the input elements
    because the operation on one element does not depend on the result from another
    input element. This allows such feedforward layers to take less than O(*n*) time
    (in some cases close to O(1)) to execute their forward and backward passes with
    the help of GPU acceleration. In [section 9.2](#ch09lev1sec2), we will explore
    some more parallelizable sequential modeling approaches such as 1D convolution.
    However, it is still important to be familiar with RNNs because they are sensitive
    to sequential positions in a way that 1D convolution isn’t (more on that later).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的情况与dense和conv2d等前馈层形成鲜明对比。在这些层中，计算可以在输入元素之间并行化，因为对一个元素的操作不依赖于另一个输入元素的结果。这使得这些前馈层在执行它们的正向和反向传播时可以在O（*n*）时间内花费较少的时间（在某些情况下接近O（1）），借助GPU加速。在[section
    9.2](#ch09lev1sec2)中，我们将探索一些更多可并行化的序列建模方法，比如1D卷积。然而，熟悉RNN仍然是重要的，因为它们对于序列位置是敏感的，而1D卷积不是（稍后讨论）。
- en: 'Gated recurrent unit: A more sophisticated type of RNN'
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）：一种更复杂的 RNN 类型
- en: 'SimpleRNN isn’t the only recurrent layer available in TensorFlow.js. There
    are two others: Gated Recurrent Unit (GRU^([[4](#ch09fn4)])) and LSTM (which you’ll
    recall stands for Long Short-Term Memory^([[5](#ch09fn5)])). In most practical
    use cases, you’ll probably want to use one of these two. SimpleRNN is too simplistic
    for most real problems, despite the fact that it is computationally much cheaper
    and has an easier-to-understand internal mechanism than GRU and LSTM. There is
    a major issue with simpleRNN: although it is theoretically able to retain at time
    `t` information about inputs seen many time steps before, such long-term dependencies
    are hard to learn in practice.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SimpleRNN 并不是 TensorFlow.js 中唯一的循环层。还有两个循环层可用：门控循环单元 (GRU^([[4](#ch09fn4)]))
    和 LSTM（Long Short-Term Memory 的缩写^([[5](#ch09fn5)])）。在大多数实际应用中，你可能会想要使用这两种模型中的一种。SimpleRNN
    对于大多数真实问题而言过于简单，尽管其计算成本更低并且其内部机制比 GRU 和 LSTM 更容易理解。但是，简单RNN 存在一个主要问题：尽管理论上来说，simpleRNN
    能够在时间 `t` 保留对于多个时间步长前的输入信息，但是在实践中，学习这种长期依赖关系非常困难。
- en: ⁴
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kyunghyun Cho et al., “Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation,” 2014, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078).
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kyunghyun Cho 等人在 2014
- en: ⁵
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” *Neural Computation*,
    vol. 9, no. 8, 1997, pp. 1735–1780.
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sepp Hochreiter 和 Jürgen Schmidhuber 在 1997 年发表的论文《Long Short-Term Memory》中提出了
    LSTM 模型，这篇论文发表在《Neural Computation》杂志的第 9 卷第 8 期上，页码从 1735 至 1780。
- en: 'This is due to the *vanishing-gradient problem*, an effect similar to what
    is observed in feedforward networks that are many layers deep: As you keep adding
    layers to a network, the size of the gradients backpropagated from the loss function
    to the early layers gets smaller and smaller. Henceforth, the updates to the weights
    get smaller and smaller, to the point where the network eventually becomes untrainable.
    For RNNs, the large number of time steps plays the role of the many layers in
    this problem. GRU and LSTM are RNNs designed to solve the vanishing-gradient problem,
    and GRU is the simpler of the two. Let’s look at how GRU does that.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于*梯度消失问题*，这是一种类似于前馈网络深度很深时观察到的效应的影响：随着你向网络中添加越来越多的层，从损失函数向早期层反向传播的梯度大小会越来越小。因此，权重的更新也越来越小，直到网络最终变得无法训练。对于
    RNN，大量的时间步骤在此问题中扮演了许多层的角色。GRU 和 LSTM 是为解决梯度消失问题而设计的 RNN，GRU 是两者中更简单的一种。让我们看看 GRU
    是如何解决这个问题的。
- en: Compared to simpleRNN, GRU has a more complex internal structure. [Figure 9.4](#ch09fig04)
    shows a rolled representation of a GRU’s internal structure. Compared with the
    same rolled representation of simpleRNN (panel B of [figure 9.2](#ch09fig02)),
    it contains more nuts and bolts. The input (*x*) and the output/state (referred
    to as *h* by the convention in the RNN literature) pass through *four* equations
    to give rise to the new output/state. Compare this with simpleRNN, which involves
    only *one* equation. This complexity is also reflected in the pseudo-code in [listing
    9.3](#ch09ex03), which can be viewed as an implementation of the mechanisms of
    [figure 9.4](#ch09fig04). We omit the bias terms in the pseudo-code for simplicity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与 simpleRNN 相比，GRU 具有更复杂的内部结构。[图 9.4](#ch09fig04) 显示了 GRU 的内部结构的滚动表示。与 simpleRNN
    的相同滚动表示进行比较（[图 9.2](#ch09fig02) 的面板 B），它包含了更多的细节。输入 (*x*) 和输出 / 状态（按照 RNN 文献中的约定称为
    *h*）通过 *四个* 等式生成新的输出 / 状态。相比之下，simpleRNN 仅涉及 *一个* 等式。这种复杂性也体现在 [清单 9.3](#ch09ex03)
    中的伪代码中，可以将其视为 [图 9.4](#ch09fig04) 中机制的一种实现。为简单起见，我们省略了伪代码中的偏置项。
- en: Figure 9.4\. A rolled representation of the GRU cell, a more complex and powerful
    RNN layer type than simpleRNN. This is a rolled representation, comparable to
    panel B of [figure 9.2](#ch09fig02). Note that we omit the bias terms in the equations
    for simplicity. The dashed lines indicate feedback connections from the output
    of the GRU cell (*h*) to the same cell in subsequent time steps.
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.4 门控循环单元（GRU）的滚动表示，一种比 simpleRNN 更复杂、更强大的 RNN 层类型。这是一个滚动表示，与 [图 9.2](#ch09fig02)
    中的面板 B 相似。请注意，我们为了简单起见，在等式中省略了偏置项。虚线表示了从 GRU 单元的输出 (*h*) 到下一个时间步的同一单元的反馈连接。
- en: '![](09fig04_alt.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig04_alt.jpg)'
- en: Listing 9.3\. Pseudo-code for a GRU layer
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 9.3 Pseudo-code for a GRU layer
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1*** This is the h in [figure 9.4](#ch09fig04). As in simpleRNN, the state
    is initialized to zero in the beginning.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 这是 [图 9.4](#ch09fig04) 中的 h。和 simpleRNN 一样，在最开始状态被初始化为零。'
- en: '***2*** This for loop iterates over all time steps of the input sequence.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 这个 for 循环遍历输入序列的所有时间步。'
- en: '***3*** z is called the update gate.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** z 被称为更新门。'
- en: '***4*** r is called the reset gate.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** r 被称为重置门。'
- en: '***5*** h_prime is the temporary state of the current state.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** h_prime 是当前状态的临时状态。'
- en: '***6*** h_prime (current temporary state) and h (previous state) are combined
    in a weighted fashion (z being the weight) to form the new state.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** h_prime (当前临时状态) 和 h (上一个状态) 以加权方式结合（z 为权值）形成新状态。'
- en: 'Of all the internal details of GRU, we highlight the two most important ones:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GRU 的所有内部细节中，我们要强调两个最重要的方面：
- en: GRU makes it easy to carry information across many time steps. This is achieved
    by the intermediate quantity *z*, which is referred to as the *update gate*. Because
    of the update gate, GRU can learn to carry the same state across many time steps
    with minimal changes. In particular, in the equation (1 - *z*) · *h* + *z* · *h*',
    if the value of *z* is 0, then the state *h* will simply be copied from the current
    time step to the next. The ability to perform wholesale carrying like this is
    an important part of how GRU combats the vanishing-gradient problem. The reset
    gate *z* is calculated as a linear combination of the input *x* and the current
    state *h*, followed by a sigmoid nonlinearity.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GRU 可以轻松地在许多时间步之间传递信息。这是通过中间量 *z* 实现的，它被称为*更新门*。由于更新门的存在，GRU 可以学习以最小的变化在许多时间步内传递相同的状态。特别地，在等式
    (1 - *z*) · *h* + *z* · *h' 中，如果 *z* 的值为 0，则状态 *h* 将简单地从当前时间步复制到下一个时间步。这种整体传递的能力对于
    GRU 如何解决消失梯度问题至关重要。重置门 *z* 被计算为输入 *x* 和当前状态 *h* 的线性组合，然后经过一个 sigmoid 非线性函数。
- en: In addition to the update gate *z*, another “gate” in GRU is the so-called *reset
    gate*, *r*. Like the update gate *z*, *r* is calculated as a sigmoid nonlinearity
    operating on a linear combination of the input and the current state `h`. The
    reset gate controls how much of the current state to “forget.” In particular,
    in the equation tanh(*W* · *x* `+` *r* · *U* · *h*`)`, if the value of *r* becomes
    0, then the effect of the current state *h* gets erased; and if (1 - *z*`)` in
    the downstream equation is close to zero as well, then the influence of the current
    state *h* on the next state will be minimized. So, *r* and *z* work together to
    enable the GRU to learn to forget the history, or a part of it, under the appropriate
    conditions. For instance, suppose we’re trying to classify a movie review as positive
    or negative. The review may start by saying “this movie is pretty enjoyable,”
    but halfway through the review, it then reads “however, the movie isn’t as good
    as other movies based on similar ideas.” At this point, the memory regarding the
    initial praise should be largely forgotten, because it is the later part of the
    review that should weigh more in determining the final sentiment-analysis result
    of this review.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了更新门 *z*，GRU 中的另一个“门”被称为所谓的*重置门*，*r*。像更新门 *z* 一样，*r* 被计算为对输入和当前状态 `h` 的线性组合进行
    sigmoid 非线性函数操作。重置门控制有多少当前状态需要“遗忘”。特别地，在等式 tanh(*W* · *x* `+` *r* · *U* · *h*`)`
    中，如果 *r* 的值变为 0，则当前状态 *h* 的影响被抹除；如果下游方程中的 (1 - *z*`)` 接近零，那么当前状态 *h* 对下一个状态的影响将被最小化。因此，*r*
    和 *z* 协同工作，使得 GRU 能够在适当条件下学习忘记历史或其一部分。例如，假设我们试图对电影评论进行正面或负面的分类。评论可能开始说“这部电影相当令人满意”，但评论过了一半后，又写到“然而，这部电影并不像其他基于类似观点的电影那么出色。”
    在这一点上，应该大部分地忘记关于初始赞美的记忆，因为应该更多地权衡评论后部分对该评论最终情感分析结果的影响。
- en: So, that’s a very rough and high-level outline of how GRU works. The important
    thing to remember is that the internal structure of GRU allows the RNN to learn
    when to carry over old state and when to update the state with information from
    the inputs. This learning is embodied by updates to the tunable weights, *W[z]*,
    *U[z]*, *W[r]*, *W[r]*, *W*, and *U* (in addition to the omitted bias terms).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是 GRU 如何工作的一个非常粗糙和高层次的概述。要记住的重要事情是，GRU 的内部结构允许 RNN 学习何时保留旧状态，何时使用来自输入的信息更新状态。这种学习通过可调权重
    *W[z]*、*U[z]*、*W[r]*、*W[r]*、*W* 和 *U* 的更新体现出来（除了省略的偏置项）。
- en: Don’t worry if you don’t follow all the details right away. At the end of the
    day, the intuitive explanation for GRU we wrote in the last couple of paragraphs
    doesn’t matter that much. It is not the human engineer’s job to understand how
    a GRU processes sequential data at a very detailed level, just like it is not
    the human engineer’s job to understand the fine-grained details of how a convnet
    converts an image input to output class probabilities. The details are found by
    the neural network in the hypothesis space delineated by the RNN’s structure data
    through the data-driven training process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一开始不明白所有细节，不要担心。归根结底，我们在最后几段中对 GRU 的直观解释并不那么重要。理解 GRU 如何以非常详细的层面处理序列数据并不是人类工程师的工作，就像理解卷积神经网络如何将图像输入转换为输出类别概率的细节并不是人类工程师的工作一样。细节是由神经网络在
    RNN 结构数据所描述的假设空间中通过数据驱动的训练过程找到的。
- en: To apply GRU on our temperature-prediction problem, we construct a TensorFlow.js
    model that contains a GRU layer. The code we use to do this (excerpted from jena-weather/train-rnn.js.)
    looks almost identical to what we used for the simpleRNN model ([listing 9.2](#ch09ex02)).
    The only difference is the type of the model’s first layer (GRU versus simpleRNN).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 GRU 应用于我们的温度预测问题，我们构建一个包含 GRU 层的 TensorFlow.js 模型。我们用于此的代码（摘自 jena-weather/train-rnn.js）几乎与我们用于简单
    RNN 模型的代码（[代码清单 9.2](#ch09ex02)）完全相同。唯一的区别是模型的第一层的类型（GRU 对比于简单 RNN）。
- en: Listing 9.4\. Creating a GRU model for the Jena-temperature-prediction problem
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 9.4\. 为 Jena 温度预测问题创建一个 GRU 模型
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1*** The hard-coded unit count is a number that works well, discovered through
    hand-tuning of the hyperparameter.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 硬编码的单元数是一个通过超参数手动调整而发现效果良好的数字。'
- en: '***2*** The first layer of the model is a GRU layer.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 模型的第一层是一个 GRU 层。'
- en: '***3*** The model ends with a dense layer with a single unit and the default
    linear activation for the regression problem.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 模型以具有单个单元和默认线性激活的密集层结束，用于回归问题。'
- en: To start training the GRU model on the Jena-weather dataset, use
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始在 Jena 天气数据集上训练 GRU 模型，请使用
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Figure 9.5](#ch09fig05) shows the training and validation loss curves obtained
    with the GRU model. It gets a best validation error of approximately 0.266, which
    beats the one we got from the simpleRNN model in the previous section (0.271).
    This reflects the greater capacity of GRU in learning sequential patterns compared
    to simpleRNN. There are indeed sequential patterns hidden in the weather-instrument
    readings that can help improve the accuracy of predicting the temperature; this
    information is picked up by GRU but not simpleRNN. This comes at the cost of greater
    training time. For example, on one of our machines, the GRU model trains at a
    speed of 3,000 ms/batch, as compared to the simpleRNN’s 950 ms/batch.^([[6](#ch09fn6)])
    But if the goal is to predict temperature as accurately as possible, this cost
    will most likely be worth it.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.5](#ch09fig05) 显示了使用 GRU 模型获得的训练和验证损失曲线。它获得了约为 0.266 的最佳验证错误，这超过了我们在上一节中从简单
    RNN 模型中获得的结果（0.271）。这反映了相较于简单 RNN，GRU 在学习序列模式方面具有更大的容量。在气象仪器读数中确实隐藏着一些序列模式，这些模式有助于提高温度的预测精度；这些信息被
    GRU 捕捉到，但简单 RNN 没有。但这是以更长的训练时间为代价的。例如，在我们的一台机器上，GRU 模型的训练速度为每批 3,000 毫秒，而简单 RNN
    的训练速度为每批 950 毫秒^([[6]](#ch09fn6)）。但如果目标是尽可能准确地预测温度，那么这个代价很可能是值得的。'
- en: ⁶
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These performance numbers are obtained from tfjs-node running on the CPU backend.
    If you use tfjs-node-gpu and the CUDA GPU backend, you’ll get approximately proportionate
    speedups for both model types.
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些性能数字是从在 CPU 后端运行的 tfjs-node 获得的。如果你使用 tfjs-node-gpu 和 CUDA GPU 后端，你将获得两种模型类型的近似比例的加速。
- en: Figure 9.5\. The loss curves from training a GRU model on the temperature-prediction
    problem. Compare this with the loss curves from the simpleRNN model ([figure 9.3](#ch09fig03)),
    and notice the small but real reduction in the best validation loss achieved by
    the GRU model.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.5\. 在温度预测问题上训练 GRU 模型的损失曲线。将其与简单 RNN 模型的损失曲线进行比较（[图 9.3](#ch09fig03)），注意
    GRU 模型取得的最佳验证损失的小幅但真实的降低。
- en: '![](09fig05_alt.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig05_alt.jpg)'
- en: 9.2\. Building deep-learning models for text
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2\. 为文本构建深度学习模型
- en: The weather-prediction problem we just studied dealt with sequential numerical
    data. But the most ubiquitous kinds of sequential data are probably text instead
    of numbers. In alphabet-based languages such as English, text can be viewed as
    either a sequence of characters or a sequence of words. The two approaches are
    suitable for different problems, and we will use both of them for different tasks
    in this section. The deep-learning models for text data we’ll introduce in the
    following sections can perform text-related tasks such as
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚研究的天气预测问题涉及顺序数值数据。但是最普遍的序列数据可能是文本而不是数字。在像英语这样以字母为基础的语言中，文本可以被视为字符序列或单词序列。这两种方法适用于不同的问题，并且在本节中我们将针对不同的任务使用它们。我们将在接下来的几节中介绍的文本数据的深度学习模型可以执行与文本相关的任务，例如
- en: Assigning a sentiment score to a body of text (for instance, whether a product
    review is positive or negative)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给一段文本分配情感分数（例如，一个产品评论是积极的还是消极的）
- en: Classifying a body of text by its topic (for example, whether a news article
    is about politics, finance, sports, health, weather, or miscellaneous)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一段文本按主题分类（例如，一篇新闻文章是关于政治、金融、体育、健康、天气还是其他）
- en: Converting a text input into a text output (for instance, for standardization
    of format or machine translation)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本输入转换为文本输出（例如，用于格式标准化或机器翻译）
- en: Predicting the upcoming parts of a text (for example, smart suggestion features
    of mobile input methods)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测文本的即将出现的部分（例如，移动输入方法的智能建议功能）
- en: This list is just a very small subset of interesting machine-learning problems
    that involve text, which are systematically studied in the field of natural language
    processing. Although we will only scratch the surface of neural-network-based
    natural language processing techniques in this chapter, the concepts and examples
    introduced here should give you a good starting point for further exploration
    (see the “[Materials for further reading](#ch09lev1sec4)” section at the end of
    this chapter).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表只是涉及文本的一小部分有趣的机器学习问题，这些问题在自然语言处理领域进行系统研究。尽管我们在本章中只是浅尝神经网络的自然语言处理技术，但这里介绍的概念和示例应该为你进一步探索提供了一个良好的起点（请参阅本章末尾的“[进一步阅读资料](#ch09lev1sec4)”部分）。
- en: Keep in mind that none of the deep neural networks in this chapter truly understand
    text or language in a human sense. Rather, these models can map the statistical
    structure of text to a certain target space, whether it is a continuous sentiment
    score, a multiclass-classification result, or a new sequence. This turns out to
    be sufficient for solving many practical, text-related tasks. Deep learning for
    natural language processing is nothing more than pattern recognition applied to
    characters and words, in much the same way that deep-learning-based computer vision
    ([chapter 4](kindle_split_015.html#ch04)) is pattern recognition applied to pixels.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，本章中的深度神经网络都不真正理解文本或语言的人类意义。相反，这些模型可以将文本的统计结构映射到特定的目标空间，无论是连续情感分数、多类别分类结果还是新序列。这证明对于解决许多实际的、与文本相关的任务来说，这是足够的。自然语言处理的深度学习只是对字符和单词进行的模式识别，方式与基于深度学习的计算机视觉（[第四章](kindle_split_015.html#ch04)）对像素进行的模式识别类似。
- en: Before we dive into the deep neural networks designed for text, we need to first
    understand how text is represented in machine learning.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨为文本设计的深度神经网络之前，我们首先需要了解机器学习中的文本是如何表示的。
- en: '9.2.1\. How text is represented in machine learning: One-hot and multi-hot
    encoding'
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1\. 机器学习中的文本表示：单热编码和多热编码
- en: Most of the input data we’ve encountered in this book so far is continuous.
    For example, the petal length of an iris flower varies continuously in a certain
    range; the weather-instrument readings in the Jena-weather dataset are all real
    numbers. These values are represented straightforwardly as float-type tensors
    (floating-point numbers). However, text is different. Text data comes in as a
    string of characters or words, not real numbers. Characters and words are discrete.
    For instance, there is no such thing as a letter between “j” and “k” in the same
    sense as there is a number between 0.13 and 0.14\. In this sense, characters and
    words are similar to classes in multiclass classification (such as the three iris-flower
    species or the 1,000 output classes of MobileNet). Text data needs to be turned
    into vectors (arrays of numbers) before it can be fed into deep-learning models.
    This conversion process is called *text vectorization*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中我们遇到的大部分输入数据都是连续的。例如，鸢尾花的花瓣长度在一定范围内连续变化；耶拿气象数据集中的天气仪读数都是实数。这些值可以直接表示为浮点型张量（浮点数）。但是，文本不同。文本数据以字符或单词的字符串形式出现，而不是实数。字符和单词是离散的。例如，在“j”和“k”之间没有类似于在
    0.13 和 0.14 之间存在数字的东西。在这个意义上，字符和单词类似于多类分类中的类别（例如三种鸢尾花物种或 MobileNet 的 1,000 个输出类别）。文本数据在被馈送到深度学习模型之前需要被转换为向量（数字数组）。这个转换过程称为*文本向量化*。
- en: There are multiple ways to vectorize text. *One-hot encoding* (as we’ve introduced
    in [chapter 3](kindle_split_014.html#ch03)) is one of the options. In English,
    depending on where you draw the line, there are about 10,000 most frequently used
    words. We can collect these 10,000 words and form a *vocabulary*. The unique words
    in the vocabulary may be sorted in a certain order (for example, descending order
    of frequency) so that any given word can be given an integer index.^([[7](#ch09fn7)])
    Then every English word can be represented as a length-10,000 vector, in which
    only the element that corresponds to the index is 1, and all remaining elements
    are 0\. This is the *one-hot vectorization* of the word. Panel A of [figure 9.6](#ch09fig06)
    presents this graphically.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种文本向量化的方式。*独热编码*（如我们在[第 3 章](kindle_split_014.html#ch03)中介绍的）是其中之一。在英语中，根据划分标准，大约有
    10,000 个最常用的单词。我们可以收集这 10,000 个单词并形成一个*词汇表*。词汇表中的唯一单词可以按照某种顺序排列（例如，按频率降序排列），以便为任何给定的单词分配一个整数索引。^([[7](#ch09fn7)])
    然后，每个英文单词都可以表示为一个长度为 10,000 的向量，其中只有对应索引的元素为 1，所有其余元素为 0。这就是该单词的*独热向量化*。[图 9.6](#ch09fig06)的
    A 面以图形方式展示了这一点。
- en: ⁷
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'An obvious question is: what if we get a rare word that falls out of the 10,000-word
    vocabulary? This is a practical problem that any text-oriented deep-learning algorithm
    is faced with. In practice, we solve this problem by adding a special item called
    *OOV* to the vocabulary. OOV stands for *out-of-vocabulary*. So, all rare words
    that do not belong to the vocabulary are lumped together in that special item
    and will have the same one-hot encoding or embedding vector. More sophisticated
    techniques have multiple OOV buckets and use a hash function to assign rare words
    to those buckets.'
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个显而易见的问题是：如果我们遇到一个落在这 10,000 词汇表之外的罕见单词怎么办？这是任何文本导向的深度学习算法所面临的实际问题。在实践中，我们通过向词汇表添加一个名为*OOV*的特殊项来解决这个问题。OOV
    代表*词汇表之外*。因此，所有不属于词汇表的罕见单词都被归类为该特殊项，并将具有相同独热编码或嵌入向量。更复杂的技术有多个 OOV 桶，并使用哈希函数将罕见单词分配到这些桶中。
- en: Figure 9.6\. One-hot encoding (vectorization) of a word (panel A) and of a sentence
    as a sequence of words (panel B). Panel C shows a simplified, multi-hot encoding
    of the same sentence as in panel B. It is a more succinct and scalable representation
    of the sequence, but it discards the order information. For the sake of visualization,
    we assume that the size of the vocabulary is only 14. In reality, the vocabulary
    size of English words used in deep learning is much larger (on the order of thousands
    or tens of thousands, for example, 10,000).
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.6. 一个单词的独热编码（向量化）（A 面）和一个句子作为一系列单词的独热编码（B 面）。C 面展示了与 B 面中相同句子的简化的多热编码。它是一种更简洁和可扩展的序列表示，但它丢弃了顺序信息。为了可视化，我们假设词汇表的大小只有
    14。实际上，在深度学习中使用的英语单词的词汇量要大得多（数量级为数千或数万，例如，10,000）。
- en: '![](09fig06_alt.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig06_alt.jpg)'
- en: What if we have a sentence instead of a single word? We can get the one-hot
    vectors for all the words that make up the sentence and put them together to form
    a 2D representation of the words of the sentence (see panel B of [figure 9.6](#ch09fig06)).
    This approach is simple and unambiguous. It perfectly preserves the information
    about what words appear in the sentence and in what order.^([[8](#ch09fn8)]) However,
    when text gets long, the size of the vector may get so big that it is no longer
    manageable. For instance, a sentence in English contains about 18 words on average.
    Given that our vocabulary has a size of 10,000, it takes 180,000 numbers to represent
    just a single sentence, which already takes a much larger space than the sentence
    itself. This is not to mention that some text-related problems deal with paragraphs
    or whole articles, which have many more words and will cause the size of the representation
    and the amount of computation to explode.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个句子而不是单个单词呢？我们可以为构成句子的所有单词获得独热向量，并将它们放在一起形成句子单词的二维表示（参见[图 9.6](#ch09fig06)
    的面板 B）。这种方法简单而明确。它完美地保留了句子中出现的单词及其顺序的信息。^([[8](#ch09fn8)]) 然而，当文本变得很长时，向量的大小可能会变得非常大，以至于无法管理。例如，英语句子平均包含约
    18 个单词。考虑到我们的词汇量为 10,000，仅表示一个句子就需要 180,000 个数字，这已经比句子本身占用的空间大得多了。更不用说一些与文本相关的问题涉及段落或整篇文章，其中包含更多的单词，会导致表示的大小和计算量急剧增加。
- en: ⁸
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This assumes there are no OOV words.
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这假设没有 OOV（Out of Vocabulary）词。
- en: 'One way to deal with this problem is to include all the words in a single vector
    so that each element in the vector represents whether the corresponding word has
    appeared in the text. Panel C of [figure 9.6](#ch09fig06) illustrates. In this
    representation, multiple elements of the vector can have the value 1\. This is
    why people sometimes refer to it as *multi-hot encoding*. Multi-hot encoding has
    a fixed length (the size of the vocabulary) regardless of how long the text is,
    so it solves the size-explosion problem. But this comes at the cost of losing
    the order information: we can’t tell from the multi-hot vector which words come
    first and which words come next. For some problems, this might be okay; for others,
    this is unacceptable. There are more sophisticated representations that take care
    of the size-explosion problem while preserving the order information, which we
    will explore later in this chapter. But first, let’s look at a concrete, text-related
    machine-learning problem that can be solved to a reasonable accuracy using the
    multi-hot approach.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是将所有单词都包含在一个单一向量中，以便向量中的每个元素表示对应的单词是否出现在文本中。[图 9.6](#ch09fig06) 的面板
    C 进行了说明。在这种表示中，向量的多个元素可以具有值 1。这就是为什么人们有时将其称为*多热编码*。多热编码具有固定长度（词汇量的大小），不管文本有多长，因此它解决了大小爆炸的问题。但这是以失去顺序信息为代价的：我们无法从多热向量中得知哪些单词先出现，哪些单词后出现。对于一些问题，这可能是可以接受的；对于其他问题，这是不可接受的。有更复杂的表示方法来解决大小爆炸问题，同时保留顺序信息，我们将在本章后面探讨。但首先，让我们看一个具体的与文本相关的机器学习问题，可以使用多热方法以合理的准确率解决。
- en: 9.2.2\. First attempt at the sentiment-analysis problem
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2\. 情感分析问题的首次尝试
- en: 'We will use the Internet Movie Database (IMDb) dataset in our first example
    of applying machine learning to text. The dataset is a collection of approximately
    25,000 textual movie reviews on [imdb.com](http://imdb.com), each of which has
    been labeled as positive or negative. The machine-learning task is binary classification:
    that is, whether a given movie review is positive or negative. The dataset is
    balanced (50% positive reviews and 50% negative ones). Just like what you expect
    from online reviews, the examples vary in word length. Some of them are as short
    as 10 words, while others can be as long as 2,000 words. The following is an example
    of what a typical review looks like. This example is labeled as negative. Punctuation
    is omitted in the dataset:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第一个例子中使用互联网电影数据库（IMDb）数据集来应用机器学习到文本上。该数据集是 [imdb.com](http://imdb.com) 上大约
    25,000 条电影评论的集合，每个评论都被标记为积极或消极。机器学习任务是二元分类：即给定的电影评论是积极的还是消极的。数据集是平衡的（50% 积极评论和
    50% 消极评论）。正如你从在线评论中所期望的那样，示例的单词长度各不相同。有些评论只有 10 个单词，而另一些则可以长达 2,000 个单词。以下是一个典型评论的例子。此示例被标记为消极。数据集中省略了标点符号。
- en: '*the mother in this movie is reckless with her children to the point of neglect
    i wish i wasn’t so angry about her and her actions because i would have otherwise
    enjoyed the flick what a number she was take my advise and fast forward through
    everything you see her do until the end also is anyone else getting sick of watching
    movies that are filmed so dark anymore one can hardly see what is being filmed
    as an audience we are impossibly involved with the actions on the screen so then
    why the hell can’t we have night vision*'
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这部电影中的母亲对她的孩子太粗心了，以至于忽视了，我希望我对她和她的行为不要那么生气，因为否则我会享受这部电影的，她太过分了，我建议你快进到你看到她做的事情结束，还有，有没有人厌倦看到拍得这么黑暗的电影了，观众几乎看不到正在拍摄的东西，所以我们为什么看不到夜视了呢*'
- en: The data is divided into a training set and an evaluation set, both of which
    are automatically downloaded from the web and written to your tmp directory when
    you issue a model-training command such as
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 数据被分为训练集和评估集，当您发出类似于模型训练命令时，它们会自动从网络下载并写入您的 tmp 目录
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you examine sentiment/data.js carefully, you can see that the data files
    it downloads and reads do not contain the actual words as character strings. Instead,
    the words are represented as 32-bit integers in those files. Although we won’t
    cover the data-loading code in that file in detail, it’s worthwhile to call out
    a part that performs the multi-hot vectorization of the sentences, shown in the
    next listing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细检查 sentiment/data.js，您会发现它下载和读取的数据文件不包含实际的单词作为字符字符串。相反，这些文件中的单词以 32 位整数表示。虽然我们不会详细介绍该文件中的数据加载代码，但值得一提的是它执行了句子的多热向量化的部分，如下一列表所示。
- en: Listing 9.5\. Multi-hot vectorization of sentences from the `loadFeatures()`
    function
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.5\. 从 `loadFeatures()` 函数对句子进行多热向量化
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***1*** Creates a TensorBuffer instead of a tensor because we will be setting
    its element values next. The buffer starts from all-zero.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 创建一个 TensorBuffer 而不是一个张量，因为我们将设置其元素值。缓冲区从全零开始。'
- en: '***2*** Iterates over all examples, each of which is a sentence'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 遍历所有例子，每个例子都是一个句子'
- en: '***3*** Each sequence (sentence) is an array of integers.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 每个序列（句子）都是一个整数数组。'
- en: '***4*** Skips out-of-vocabulary (OOV) words for multi-hot encoding'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 跳过多热编码中的词汇表外（OOV）单词'
- en: '***5*** Sets the corresponding index in the buffer to 1\. Note that every index
    i may have multiple wordIndex values set to 1, hence the multi-hot encoding.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将缓冲区中的相应索引设置为 1。请注意，每个索引 i 可能有多个 wordIndex 值设置为 1，因此是多热编码。'
- en: The multi-hot-encoded features are represented as a 2D tensor of shape `[numExamples,
    numWords]`, where `numWords` is the size of the vocabulary (10,000 in this case).
    This shape isn’t affected by the length of the individual sentences, which makes
    this a simple vectorization paradigm. The targets loaded from the data files have
    a shape of `[numExamples, 1]` and contain the negative and positive labels represented
    as 0s and 1s, respectively.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 多热编码的特征被表示为一个形状为 `[numExamples, numWords]` 的 2D 张量，其中 `numWords` 是词汇表的大小（在本例中为
    10,000）。这种形状不受各个句子长度的影响，这使得这成为一个简单的向量化范例。从数据文件加载的目标的形状为 `[numExamples, 1]`，包含负面和正面标签，分别表示为
    0 和 1。
- en: The model that we apply to the multi-hot data is an MLP. In fact, with the sequential
    information lost with the multi-hot encoding, there is no way to apply an RNN
    model to the data even if we wanted to. We will talk about RNN-based approaches
    in the next section. The code that creates the MLP model is from the `buildModel()`
    function in sentiment/train.js, with simplification, and looks like the following
    listing.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用于多热数据的模型是一个 MLP。实际上，即使我们想要，由于多热编码丢失了顺序信息，也无法对数据应用 RNN 模型。我们将在下一节讨论基于 RNN
    的方法。创建 MLP 模型的代码来自 sentiment/train.js 中的 `buildModel()` 函数，简化后的代码如下列表所示。
- en: Listing 9.6\. Building an MLP model for the multi-hot-encoded IMDb movie reviews
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.6\. 为多热编码的 IMDb 电影评论构建 MLP 模型
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1*** Adds two hidden dense layers with relu activation to enhance the representational
    power'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 添加两个带有 relu 激活的隐藏密集层以增强表示能力'
- en: '***2*** The input shape is the size of the vocabulary due to the multi-hot
    vectorization we are dealing with here.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 输入形状是词汇表的大小，因为我们在这里处理多热向量化。'
- en: '***3*** Uses sigmoid activation for the output layer to suit the binary-classification
    task'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 为输出层使用 sigmoid 激活以适应二元分类任务'
- en: By running the `yarn train multihot --maxLen 500` command, you can see that
    the model achieves a best validation accuracy of approximately 0.89\. This accuracy
    is okay, and is significantly higher than chance (0.5). This shows that it is
    possible to achieve a reasonable degree of accuracy in this sentiment-analysis
    problem by looking at just what words appear in the review. For example, words
    such as *enjoyable* and *sublime* are associated with positive reviews, and words
    such as *sucks* and *bland* are associated with negative ones with a relatively
    high degree of reliability. Of course, there are plenty of scenarios in which
    looking just at what words there are will be misleading. As a contrived example,
    understanding the true meaning of a sentence like “Don’t get me wrong, I hardly
    disagree this is an excellent film” requires taking into account sequential information—not
    only what the words are but also in what order they appear. In the next section,
    we will show that by using a text vectorization that doesn’t discard the sequential
    information and a model that can utilize the sequential information, we can beat
    this baseline accuracy. Let’s now look at how word embeddings and 1D convnets
    work.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行`yarn train multihot --maxLen 500`命令，可以看到模型达到大约0.89的最佳验证准确率。这个准确率还可以，明显高于机会的准确率（0.5）。这表明通过仅仅查看评论中出现的单词，可以在这个情感分析问题上获得一个相当合理的准确度。例如，像*令人愉快*和*崇高*这样的单词与积极的评论相关联，而像*糟糕*和*乏味*这样的单词与消极的评论相关联，并且具有相对较高的可靠性。当然，在许多情况下，仅仅看单词并不一定能得到正确的结论。举一个人为的例子，理解句子“别误会，我并不完全不同意这是一部优秀的电影”的真实含义需要考虑顺序信息——不仅是单词是什么，还有它们出现的顺序。在接下来的章节中，我们将展示通过使用一个不丢失顺序信息的文本向量化和一个能够利用顺序信息的模型，我们可以超越这个基准准确度。现在让我们看看词嵌入和一维卷积如何工作。
- en: '9.2.3\. A more efficient representation of text: Word embeddings'
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.3\. 文本的更高效表示：词嵌入
- en: What is *word embedding*? Just like one-hot encoding ([figure 9.6](#ch09fig06)),
    word embedding is a way to represent a word as a vector (a 1D tensor in TensorFlow.js).
    However, word embeddings allow the values of the vector’s elements to be trained,
    instead of hard-coded according to a rigid rule such as the word-to-index map
    in one-hot encoding. In other words, when a text-oriented neural network uses
    word embedding, the embedding vectors become trainable weight parameters of the
    model. They are updated through the same backpropagation rule as all other weight
    parameters of the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是*词嵌入*？就像一位热编码（[图 9.6](#ch09fig06)）一样，词嵌入是将一个单词表示为一个向量（在 TensorFlow.js 中是一个一维张量）的一种方式。然而，词嵌入允许向量的元素值被训练，而不是依据一个严格的规则进行硬编码，比如一热编码中的单词到索引映射。换句话说，当一个面向文本的神经网络使用词嵌入时，嵌入向量成为模型的可训练的权重参数。它们通过与模型的其他权重参数一样的反向传播规则进行更新。
- en: This situation is illustrated schematically in [figure 9.7](#ch09fig07). The
    layer type in TensorFlow.js that allows you to perform word embedding is `tf.layer.embedding()`.
    It contains a trainable weight matrix of shape `[vocabularySize, embeddingDims]`,
    where `vocabularySize` is the number of unique words in the vocabulary and `embeddingDims`
    is the user-selected dimensionality of the embedding vectors. Every time you are
    given a word, say *the*, you find the corresponding row in the embedding matrix
    using a word-to-index lookup table, and that row is the embedding vector for your
    word. Note that the word-to-index lookup table is not part of the embedding layer;
    it is maintained as a separate entity from the model (see [listing 9.9](#ch09ex09),
    for example).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在[图 9.7](#ch09fig07)中示意。在 TensorFlow.js 中，可以使用`tf.layer.embedding()`层类型来执行词嵌入。它包含一个可训练的形状为`[vocabularySize,
    embeddingDims]`的权重矩阵，其中`vocabularySize`是词汇表中唯一单词的数量，`embeddingDims`是用户选择的嵌入向量的维度。每当给出一个单词，比如*the*，你可以使用一个单词到索引的查找表在嵌入矩阵中找到对应的行，该行就是你的单词的嵌入向量。请注意，单词到索引的查找表不是嵌入层的一部分；它是模型以外的一个单独的实体（例如，参见[示例
    9.9](#ch09ex09)）
- en: Figure 9.7\. A schematic illustration of how an embedding matrix works. Each
    row of the embedding matrix corresponds to a word in the vocabulary, and each
    column is an embedding dimension. The values of the embedding matrix’s elements,
    represented as shades of gray in the diagram, are chosen at random.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.7\. 描述嵌入矩阵工作原理的示意图。嵌入矩阵的每一行对应词汇表中的一个单词，每一列是一个嵌入维度。嵌入矩阵的元素值在图中以灰度表示，并随机选择。
- en: '![](09fig07_alt.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig07_alt.jpg)'
- en: 'If you have a sequence of words, like a sentence as shown in [figure 9.7](#ch09fig07),
    you repeat this lookup process for all the words in the correct sequential order
    and stack the resulting embedding vectors into a 2D tensor of shape `[sequenceLength,`
    `embeddingDims]`, where `sequenceLength` is the number of words in the sentence.^([[9](#ch09fn9)])
    What if there are repeating words in the sentence (such as the word *the* in the
    example in [figure 9.7](#ch09fig07))? It doesn’t matter: just let the same embedding
    vector appear repeatedly in the resulting 2D tensor.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一系列单词，就像[图9.7](#ch09fig07)中显示的句子一样，你需要按照正确的顺序重复这个查找过程，并将得到的嵌入向量堆叠成一个形状为`[sequenceLength,`
    `embeddingDims]`的二维张量，其中`sequenceLength`是句子中的单词数量。^([[9](#ch09fn9)]) 如果句子中有重复的单词（比如在[图9.7](#ch09fig07)中的例子中的*the*），这并不重要：只需让相同的嵌入向量在结果的二维张量中重复出现。
- en: ⁹
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This multiword embedding lookup process can be done effectively using the `tf.gather()`
    method, which is how the embedding layer in TensorFlow.js is implemented under
    the hood.
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种多词嵌入查找过程可以有效地使用`tf.gather()`方法进行，这就是 TensorFlow.js 中嵌入层在底层实现的方式。
- en: 'Word embedding gives us the following benefits:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入为我们带来以下好处：
- en: It addresses the size problem with one-hot encodings. `embeddingDims` is usually
    much smaller than `vocabularySize`. For example, in the 1D convnet we are about
    to use on the IMDb dataset, `vocabularySize` is 10,000, and `embeddingDims` is
    128\. So, with a 500-word review from the IMDb dataset, representing the example
    requires 500 * 128 = 64k float numbers, instead of 500 * 10,000 = 5M numbers,
    as in one-hot encoding—a much more economical vectorization.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它解决了使用独热编码的大小问题。`embeddingDims`通常比`vocabularySize`要小得多。例如，在我们即将在 IMDb 数据集上使用的一维卷积网络中，`vocabularySize`为10,000，`embeddingDims`为128。因此，在来自
    IMDb 数据集的500字评论中，表示这个例子只需要500 * 128 = 64k个浮点数，而不是500 * 10,000 = 5M个数字，就像独热编码一样——这样的向量化更经济。
- en: 'By not being opinionated about how to order the words in the vocabulary and
    by allowing the embedding matrix to be trained via backpropagation just like all
    other neural network weights, word embeddings can learn semantic relations between
    words. Words with similar meanings should have embedding vectors that are closer
    in the embedding space. For example, words with similar meanings, such as *very*
    and *truly* should have vectors that are closer together than words that are more
    different in meaning, such as *very* and *barely*. Why should this be the case?
    An intuitive way to understand it is to realize the following: suppose you replace
    a number of words in a movie-review input with words with similar meaning; a well-trained
    network ought to output the same classification result. This could happen only
    if the embedding vectors for each pair of words, which are the input to the downstream
    part of the model, are close to each other.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过不在乎词汇中单词的排序方式，并允许嵌入矩阵通过反向传播来进行训练，就像所有其他神经网络权重一样，单词嵌入可以学习单词之间的语义关系。意思相近的单词应该在嵌入空间中距离更近。例如，意思相近的单词，比如*very*和*truly*，它们的向量应该比那些意思更不同的单词的向量更接近，比如*very*和*barely*。为什么会这样？一个直观理解它的方式是意识到以下：假设你用意思相近的单词替换电影评论输入中的一些单词；一个训练良好的网络应该输出相同的分类结果。这只有当每一对单词的嵌入向量，它们是模型后续部分的输入，彼此之间非常接近时才会发生。
- en: Also, the fact that the embedding space has multiple dimensions (for example,
    128) should allow the embedding vectors to capture different aspects of words.
    For example, there can be a dimension that represents part of speech, along which
    an adjective like *fast* is closer to another adjective (such as *warm*) than
    to a noun (such as *house*). There might be another dimension that encodes the
    gender aspect of a word, one along which a word like *actress* is closer to another
    feminine-meaning word (such as *queen*) than to a masculine-meaning one (such
    as *actor*). In the next section (see [info box 9.2](#ch09sb02)), we will show
    you a way to visualize the word embeddings and explore their interesting structures
    after they emerge from training an embedding-based neural network on the IMDb
    dataset.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也就是说，嵌入空间具有多个维度（例如，128）的事实应该允许嵌入向量捕获单词的不同方面。例如，可能会有一个表示词性的维度，其中形容词*fast*与另一个形容词（如*warm*）比与一个名词（如*house*）更接近。可能还有另一个维度编码单词的性别方面，其中像*actress*这样的词比一个男性意义的词（如*actor*）更接近另一个女性意义的词（如*queen*）。在下一节（见[info
    box 9.2](#ch09sb02)），我们将向您展示一种可视化单词嵌入并探索它们在对IMDb数据集进行嵌入式神经网络训练后出现的有趣结构的方法。
- en: '[Table 9.1](#ch09table01) gives a more succinct summary of the differences
    between one-/multi-hot encoding and word embedding, the two most frequently used
    paradigms for word vectorization.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 9.1](#ch09table01) 提供了一个更简洁的总结，概述了一热/多热编码和词嵌入这两种最常用的词向量化范式之间的差异。'
- en: 'Table 9.1\. Comparing two paradigms of word vectorization: one-hot/multi-hot
    encoding and word embedding'
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Table 9.1\. 比较两种词向量化范式：one-hot/multi-hot编码和词嵌入
- en: '|   | One-hot or multi-hot encoding | Word embedding |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|   | One-hot或multi-hot编码 | 词嵌入 |'
- en: '| --- | --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Hard-coded or learned? | Hard-coded. | Learned: the embedding matrix is a
    trainable weight parameter; the values often reflect the semantic structure of
    the vocabulary after training. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 硬编码还是学习？ | 硬编码。 | 学习：嵌入矩阵是一个可训练的权重参数；这些值通常在训练后反映出词汇的语义结构。 |'
- en: '| Sparse or dense? | Sparse: most elements are zero; some are one. | Dense:
    elements take continuously varying values. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏还是密集？ | 稀疏：大多数元素为零；一些为一。 | 密集：元素取连续变化的值。 |'
- en: '| Scalability | Not scalable to large vocabularies: the size of the vector
    is proportional to the size of the vocabulary. | Scalable to large vocabularies:
    the embedding size (number of embedding dimensions) doesn’t have to increase with
    the number of words in the vocabulary. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 不可扩展到大词汇量：向量的大小与词汇量的大小成正比。 | 可扩展到大词汇量：嵌入大小（嵌入维度数）不必随词汇量的增加而增加。 |'
- en: 9.2.4\. 1D convnets
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.4\. 1D卷积网络
- en: 'In [chapter 4](kindle_split_015.html#ch04), we showed the key role played by
    2D convolutional layers in deep neural networks for image inputs. conv2d layers
    learn to represent local features in small 2D patches in images. The idea of convolution
    can be extended to sequences. The resulting algorithm is called *1D convolution*
    and is available through the `tf.layers.conv1d()` function in TensorFlow.js. The
    ideas underlying conv1d and conv2d are the same: they are both trainable extractors
    of translationally invariant local features. For instance, a conv2d layer may
    become sensitive to corner patterns of a certain orientation and of a certain
    color change after training on an image task, while a conv1d layer may become
    sensitive to a pattern of “a negative verb followed by a praising adjective” after
    training on a text-related task.^([[10](#ch09fn10)])'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在[chapter 4](kindle_split_015.html#ch04)，我们展示了2D卷积层在深度神经网络中对图像输入的关键作用。conv2d层学习在图像中的小2D补丁中表示局部特征的方法。卷积的思想可以扩展到序列中。由此产生的算法称为*1D卷积*，在TensorFlow.js中通过`tf.layers.conv1d()`函数提供。conv1d和conv2d的基本思想是相同的：它们都是可训练的提取平移不变局部特征的工具。例如，一个conv2d层在图像任务训练后可能变得对某个方向的特定角落模式和颜色变化敏感，而一个conv1d层可能在文本相关任务训练后变得对“一个否定动词后跟一个赞美形容词”的模式敏感。^([[10](#ch09fn10)])
- en: ^(10)
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As you might have guessed, there is indeed 3D convolution, and it is useful
    for deep-learning tasks that involve 3D (volumetric) data, such as certain types
    of medical images and geological data.
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的那样，确实存在3D卷积，并且它对涉及3D（体积）数据的深度学习任务非常有用，例如某些类型的医学图像和地质数据。
- en: '[Figure 9.8](#ch09fig08) illustrates how a conv1d layer works in greater detail.
    Recall from [figure 4.3](kindle_split_015.html#ch04fig03) in [chapter 4](kindle_split_015.html#ch04)
    that a conv2d layer involves sliding a kernel over all possible locations in the
    input image. The 1D convolution algorithm also involves sliding a kernel, but
    is simpler because the sliding movement happens in only one dimension. At each
    sliding position, a slice of the input tensor is extracted. The slice has the
    length `kernelSize` (a configuration field for the conv1d layer), and in the case
    of this example, it has a second dimension equal to the number of embedding dimensions.
    Then a *dot* (multiply-and-add) operation is performed between the input slice
    and the kernel of the conv1d layer, which yields a single slice of the output
    sequence. This operation is repeated for all valid sliding positions until the
    full output is generated. Like the input tensor of the conv1d layer, the full
    output is a sequence, albeit with a different length (determined by the input
    sequence length, the `kernelSize`, and other configurations of the conv1d layer)
    and a different number of feature dimensions (determined by the `filters` configuration
    of the conv1d layer). This makes it possible to stack multiple conv1d layers to
    form a deep 1D convnet, just as stacking multiple conv2d layers is a frequently
    used trick in 2D convnets.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.8](#ch09fig08)详细说明了 conv1d 层的工作原理。回想一下，[第 4 章](kindle_split_015.html#ch04)中的
    [图 4.3](kindle_split_015.html#ch04fig03) 表明，conv2d 层涉及将一个核在输入图像的所有可能位置上滑动。1D 卷积算法也涉及滑动一个核，但更简单，因为滑动仅在一个维度上发生。在每个滑动位置，都会提取输入张量的一个片段。该片段的长度为
    `kernelSize`（conv1d 层的配置字段），在此示例中，它具有与嵌入维度数量相等的第二个维度。然后，在输入片段和 conv1d 层的核之间执行
    *点*（乘法和加法）操作，得到一个输出序列的单个片段。这个操作会在所有有效的滑动位置上重复，直到生成完整的输出。与 conv1d 层的输入张量一样，完整的输出是一个序列，尽管它具有不同的长度（由输入序列长度、`kernelSize`
    和 conv1d 层的其他配置确定）和不同数量的特征维度（由 conv1d 层的 `filters` 配置确定）。这使得可以堆叠多个 conv1d 层以形成深度的
    1D convnet，就像堆叠多个 conv2d 层一样，是 2D convnet 中经常使用的技巧之一。'
- en: Figure 9.8\. Schematic illustration of how 1D convolution (`tf.layers.conv1d()`)
    works. For the sake of simplicity, only one input example is shown (on the left
    side of the image). We suppose that the input sequence has a length of 12 and
    the conv1d layer has a kernel size of 5\. At each sliding window position, a length-5
    slice of the input sequence is extracted. The slice is dot-multiplied with the
    kernel of the conv1d layer, which generates one slide of the output sequence.
    This is repeated for all possible sliding-window positions, which gives rise to
    the output sequence (on the right side of the diagram).
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.8\. 示意图说明了 1D 卷积 (`tf.layers.conv1d()`) 的工作原理。为简单起见，仅显示一个输入示例（图像左侧）。假设输入序列的长度为
    12，conv1d 层的核大小为 5。在每个滑动窗口位置，都会提取输入序列的长度为 5 的片段。该片段与 conv1d 层的核进行点乘，生成一个输出序列的滑动。这一过程对所有可能的滑动窗口位置重复进行，从而产生输出序列（图像右侧）。
- en: '![](09fig08_alt.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig08_alt.jpg)'
- en: Sequence truncation and padding
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 序列截断和填充
- en: 'Now that we have conv1d in our arsenal for text-oriented machine learning,
    are we ready to train a 1D convnet on the IMDb data? Not quite yet. There is one
    more thing to explain: truncating and padding of sequences. Why do we need to
    do truncation and padding? TensorFlow.js models require the inputs to `fit()`
    to be a tensor, and a tensor must have a concrete shape. Therefore, although our
    movie reviews don’t have a fixed length (recall that they vary between 10 and
    2,400 words), we have to pick a specific length as the second dimension of the
    input feature tensor (`maxLen`), so that the full shape of the input tensor is
    `[numExamples, maxLen]`. No such problem existed when we used multi-hot encoding
    in the previous section because tensors from multi-hot encoding had a second tensor
    dimension unaffected by sequence length.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在文本导向的机器学习中使用 conv1d，准备好在 IMDb 数据上训练 1D convnet 了吗？还不太行。还有一件事要解释：序列的截断和填充。为什么我们需要截断和填充？TensorFlow.js
    模型要求 `fit()` 的输入是一个张量，而张量必须具有具体的形状。因此，尽管我们的电影评论长度不固定（回想一下，它们在 10 到 2,400 个单词之间变化），但我们必须选择一个特定的长度作为输入特征张量的第二个维度（`maxLen`），这样输入张量的完整形状就是
    `[numExamples, maxLen]`。在前一节使用多热编码时不存在这样的问题，因为来自多热编码的张量具有不受序列长度影响的第二个张量维度。
- en: 'The considerations that go into choosing the value of `maxLen` are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 `maxLen` 值的考虑如下：
- en: It should be long enough to capture the useful part of most of the reviews.
    If we choose `maxLen` to be 20, it will perhaps be so short that it will cut out
    the useful part for most reviews.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该足够长以捕获大多数评论的有用部分。如果我们选择 `maxLen` 为 20，可能会太短，以至于会剪掉大多数评论的有用部分。
- en: It should not be so large that a majority of the reviews are much shorter than
    that length, because that would lead to a waste of memory and computation time.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不应该太大，以至于大多数评论远远短于该长度，因为那将导致内存和计算时间的浪费。
- en: 'The trade-off of the two leads us to pick a value of 500 words per review (at
    maximum) for this example. This is specified in the flag `--maxLen` in the command
    for training the 1D convnet:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 两者的权衡使我们选择了每个评论的最大词数为 500（最大值）作为示例。这在用于训练 1D convnet 的命令中通过 `--maxLen` 标志指定：
- en: '[PRE13]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once the `maxLen` is chosen, all the review examples must be molded into this
    particular length. In particular, the ones that are longer are truncated; the
    ones that are shorter are padded. This is what the function `padSequences()` does
    ([listing 9.7](#ch09ex07)). There are two ways to truncate a long sequence: cut
    off the beginning part (the `''pre''` option in [listing 9.7](#ch09ex07)) or the
    ending part. Here, we use the former approach, based on the reasoning that the
    ending part of a movie review is more likely to contain information relevant to
    the sentiment than the beginning part. Similarly, there are two ways to pad a
    short sequence to the desired length: adding the padding character (`PAD_CHAR`)
    before (the `''pre''` option in [listing 9.7](#ch09ex07)) or after the sentence.
    Here, we arbitrarily choose the former option as well. The code in this listing
    is from sentiment/sequence_utils.js.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了 `maxLen`，所有的评论示例都必须被调整为这个特定的长度。特别是，比较长的评论被截断；比较短的评论被填充。这就是函数 `padSequences()`
    做的事情（[列表 9.7](#ch09ex07)）。截断长序列有两种方式：切掉开头部分（[列表 9.7](#ch09ex07) 中的 `'pre'` 选项）或结尾部分。这里，我们选择了前一种方法，理由是电影评论的结尾部分更有可能包含与情感相关的信息。类似地，填充短序列到期望的长度有两种方式：在句子之前添加填充字符（`PAD_CHAR`）（[列表
    9.7](#ch09ex07) 中的 `'pre'` 选项）或在句子之后添加。在这里，我们也是任意选择了前一种选项。此列表中的代码来自 sentiment/sequence_utils.js。
- en: Listing 9.7\. Truncating and padding a sequence as a step of loading text features
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.7\. 将文本特征加载的一步截断和填充序列
- en: '[PRE14]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1*** Loops over all the input sequences'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 遍历所有的输入序列'
- en: '***2*** This particular sequence is longer than the prescribed length (maxLen):
    truncate it to that length.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 这个特定序列比指定的长度（maxLen）长：将其截断为该长度。'
- en: '***3*** There are two ways to truncate a sequence: cut off the beginning (''pre'')
    or the end'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 有两种截断序列的方式：切掉开头 (''pre'') 或结尾'
- en: '***4*** The sequence is shorter than the prescribed length: it needs to be
    padded.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 序列比指定的长度短：需要填充。'
- en: '***5*** Generates the padding sequence'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 生成填充序列'
- en: '***6*** Like truncation, there are two ways to pad the sublength sequence:
    from the beginning (''pre'') or from behind.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 与截断类似，填充子长度序列有两种方式：从开头 (''pre'') 或从后面开始。'
- en: '***7*** Note: if the length of seq is exactly maxLen, it will be returned without
    change.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 注意：如果 seq 的长度恰好为 maxLen，则将原样返回。'
- en: Building and running a 1D convnet on the IMDb dataset
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 IMDb 数据集上构建并运行 1D convnet
- en: Now we have all the pieces ready for the 1D convnet; let’s put them together
    and see if we can get a higher accuracy on the IMDb sentiment-analysis task. The
    code in [listing 9.8](#ch09ex08) creates our 1D convnet (excerpted from sentiment/train.js,
    with simplification). The summary of the resulting `tf.Model` object is shown
    after that.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了 1D convnet 的所有组件；让我们把它们放在一起，看看我们是否可以在 IMDb 情感分析任务上获得更高的准确率。[列表 9.8](#ch09ex08)
    中的代码创建了我们的 1D convnet（从 sentiment/train.js 中摘录，简化了）。在此之后展示了生成的 `tf.Model` 对象的摘要。
- en: Listing 9.8\. Building a 1D convnet for the IMDb problem
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.8\. 构建 IMDb 问题的 1D convnet
- en: '[PRE15]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1*** The model begins with an embedding layer, which turns the input integer
    indices into the corresponding word vectors.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 模型以嵌入层开始，它将输入的整数索引转换为相应的词向量。'
- en: '***2*** The embedding layer needs to know the size of the vocabulary. Without
    this, it can’t determine the size of the embedding matrix.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 嵌入层需要知道词汇量的大小。否则，它无法确定嵌入矩阵的大小。'
- en: '***3*** Adds a dropout layer to combat overfitting'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 添加一个 dropout 层以防止过拟合'
- en: '***4*** Here comes the conv1D layer.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 接下来是 conv1D 层。'
- en: '***5*** The globalMaxPool1d layer collapses the time dimension by extracting
    the maximum element value in each filter. The output is ready for the upcoming
    dense layers (MLP).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** globalMaxPool1d层通过提取每个过滤器中的最大元素值来折叠时间维度。输出准备好供后续的密集层（MLP）使用。'
- en: '***6*** Adds a two-layer MLP at the top of the model'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 在模型顶部添加了一个两层的MLP'
- en: 'It is helpful to look at the JavaScript code and the text summary together.
    There are a few things worth calling out here:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 将JavaScript代码和文本摘要一起查看是有帮助的。这里有几个值得注意的地方：
- en: The model has a shape of `[null, 500]`, where `null` is the undetermined batch
    dimension (the number of examples) and 500 is the maximally allowed word length
    of each review (`maxLen`). The input tensor contains the truncated and padded
    sequences of integer word indices.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的形状为`[null, 500]`，其中`null`是未确定的批次维度（示例数量），500是每个评论的最大允许单词长度（`maxLen`）。输入张量包含截断和填充的整数单词索引序列。
- en: The first layer of the model is an embedding layer. It turns the word indices
    into their corresponding word vectors, which leads to a shape of `[null, 500,
    128]`. As you can see, the sequence length (500) is preserved, and the embedding
    dimension (128) is reflected as the last element of the shape.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的第一层是嵌入层。它将单词索引转换为它们对应的单词向量，导致形状为`[null, 500, 128]`。正如你所看到的，序列长度（500）得到保留，并且嵌入维度（128）反映在形状的最后一个元素上。
- en: The layer that follows the embedding layer is a conv1d layer—the core part of
    this model. It is configured to have a kernel size of 5, a default stride size
    of 1, and “valid” padding. As a result, there are 500 – 5 + 1 = 496 possible sliding
    positions along the sequence dimension. This leads to a value of 496 in the second
    element of the output shape (`[null, 496, 250]`). The last element of the shape
    (250) reflects the number of filters the conv1d layer is configured to have.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟在嵌入层后面的层是conv1d层——这个模型的核心部分。它配置为具有大小为5的卷积核，默认步幅大小为1，并且采用“valid”填充。因此，沿着序列维度有500-5+1=496个可能的滑动位置。这导致输出形状的第二个元素（`[null,
    496, 250]`）中有一个值为496。形状的最后一个元素（250）反映了conv1d层配置为具有的过滤器数量。
- en: The globalMaxPool1d layer that follows the conv1d layer is somewhat similar
    to the maxPooling2d layer we’ve seen in image convnets. However, it does a more
    dramatic pooling, one in which all elements along the sequence dimension are collapsed
    to a single maximum value. This leads to the output shape of `[null,` `250]`.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接在conv1d层后面的globalMaxPool1d层与我们在图像卷积网络中看到的maxPooling2d层有些相似。但它进行了更激烈的汇集，将沿着序列维度的所有元素折叠成一个单一的最大值。这导致输出形状为`[null,
    250]`。
- en: Now that the tensor has a 1D shape (ignoring the batch dimension), we can build
    two dense layers on top of it to form an MLP as the top of the entire model.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在张量具有1D形状（忽略批次维度），我们可以在其上构建两个密集层，形成MLP作为整个模型的顶部。
- en: Start training the 1D convnet with the command `yarn train --maxLen 500 cnn`.
    After two to three training epochs, you can see the model reach a best validation
    accuracy of about 0.903, which is a small but solid gain relative to the accuracy
    we got from the MLP based on the multi-hot vectorization (0.890). This reflects
    the sequential order information that our 1D convnet managed to learn but that
    was impossible to learn by the multi-hot MLP.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 用命令`yarn train --maxLen 500 cnn`开始训练1D卷积网络。经过两到三个训练周期后，你会看到模型达到了约0.903的最佳验证准确率，相对于基于多热编码的MLP得到的准确率（0.890），这是一个小但坚实的提升。这反映了我们的1D卷积网络设法学习到的顺序信息，而这是多热编码MLP无法学习到的。
- en: So how does a 1D convnet capture sequential order? It does this through its
    convolutional kernel. The dot product of the kernel is sensitive to the ordering
    of the elements. For example, if an input consists of five words, *I like it so
    much*, the 1D convolution will output one particular value; however, if the order
    of the words is altered to be *much so I like it*, it will cause a different output
    from the 1D convolution, even though the set of elements is exactly the same.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 那么1D卷积网络如何捕捉顺序信息呢？它通过其卷积核来实现。卷积核的点积对元素的顺序敏感。例如，如果输入由五个单词组成，*I like it so much*，1D卷积将输出一个特定的值；然而，如果单词的顺序改变为*much
    so I like it*，尽管元素集合完全相同，但1D卷积的输出将不同。
- en: However, it needs to be pointed out that a conv1d layer by itself is not able
    to learn sequential patterns beyond its kernel size. For instance, suppose the
    ordering of two far-apart words affects the meaning of the sentence; a conv1d
    layer with a kernel size smaller than the distance won’t be able to learn the
    long-range interaction. This is an aspect in which RNNs such as GRU and LSTM outshine
    1D convolution.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 但需要指出的是，一维卷积层本身无法学习超出其核大小的连续模式。 例如，假设两个远离的单词的顺序影响句子的含义； 具有小于距离的核大小的 conv1d 层将无法学习长距离交互。
    这是 RNN（如 GRU 和 LSTM）在一维卷积方面优于的方面之一。
- en: 'One way in which 1D convolution can ameliorate this shortcoming is to go deep—namely,
    stacking up multiple conv1d layers so that the “receptive field” of the higher-level
    conv1d layers is large enough to capture such long-range dependencies. However,
    in many text-related machine-learning problems, such long-range dependencies don’t
    play important roles, so that using a 1D convnet with a small number of conv1d
    layers suffices. In the IMDb sentiment example, you can try training an LSTM-based
    model based on the same `maxLen` value and embedding dimensions as the 1D convnet:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一种一维卷积可以改善这一缺点的方法是深入研究-即，堆叠多个 conv1d 层，以便较高级别的 conv1d 层的“接受场”足够大，以捕获这种长距离依赖关系。
    然而，在许多与文本相关的机器学习问题中，这种长距离依赖关系并不起重要作用，因此使用少量 conv1d 层的一维卷积网络就足够了。 在 IMDb 情感示例中，您可以尝试根据相同的
    `maxLen` 值和嵌入维度训练基于 LSTM 的模型：
- en: '[PRE16]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that the best validation accuracy from the LSTM (similar to but slightly
    more complex that GRU; see [figure 9.4](#ch09fig04)) is about the same as that
    from the 1D convnet. This is perhaps because long-range interactions between words
    and phrases don’t matter a lot for this body of movie reviews and the sentiment-classification
    task.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LSTM 的最佳验证准确度（类似于但略为复杂于 GRU；请参见 [figure 9.4](#ch09fig04)）与一维卷积网络的最佳验证准确度大致相同。
    这可能是因为长距离的单词和短语之间的相互作用对于这些电影评论和情感分类任务并不太重要。
- en: So, you can see that 1D convnets are an attractive alternative to RNNs for this
    type of text problem. This is especially true considering the much lower computational
    cost of 1D convnets compared to that of RNNs. From the `cnn` and `lstm` commands,
    you can see that training the 1D convnet is about six times as fast as training
    the LSTM model. The slower performance of LSTM and RNNs is related to their step-by-step
    internal operations, which cannot be parallelized; convolutions are amenable to
    parallelization by design.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到一维卷积网络是这种文本问题的一种有吸引力的替代选择，而不是 RNN。 这在考虑到一维卷积网络的计算成本远低于 RNN 的计算成本时尤为明显。
    从 `cnn` 和 `lstm` 命令中，您可以看到训练一维卷积网络的速度约为训练 LSTM 模型的六倍。 LSTM 和 RNN 的性能较慢与它们的逐步内部操作有关，这些操作无法并行化；
    卷积是可以通过设计进行并行化的。
- en: '|  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Using the Embedding Projector to visualize learned embedding vectors**'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用嵌入式投影仪可视化学习到的嵌入向量**'
- en: '![](f0318_01_alt.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](f0318_01_alt.jpg)'
- en: Visualizing the trained word embeddings from the 1D convnet using t-SNE dimension
    reduction in the Embedding Projector
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入式投影仪在嵌入式投影器中使用 t-SNE 维度约减可视化经过训练的一维卷积网络的词嵌入。
- en: 'Do any interesting structures emerge in the word embeddings of the 1D convnet
    after training? To find this out, you can use the optional flag `--embeddingFilesPrefix`
    of the `yarn train` command:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后，一维卷积网络的词嵌入中是否出现了任何有趣的结构？ 要找出，请使用 `yarn train` 命令的可选标志 `--embeddingFilesPrefix`：
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This command will generate two files:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将生成两个文件：
- en: /tmp/imdb_embed_vectors.tsv—A tab-separated-values file for the numeric values
    of the word embeddings. Each line contains the embedding vector from a word. In
    our case, there are 10,000 lines (our vocabulary size), and each line contains
    128 numbers (our embedding dimensions).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /tmp/imdb_embed_vectors.tsv-一个包含单词嵌入的数值的制表符分隔值文件。 每一行包含一个单词的嵌入向量。 在我们的情况下，有
    10,000 行（我们的词汇量大小），每行包含 128 个数字（我们的嵌入维度）。
- en: /tmp/imdb_embed_labels.tsv—A file consisting of the word labels that correspond
    to the vectors in the previous file. Each line is a word.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /tmp/imdb_embed_labels.tsv-一个由与前一个文件中的向量对应的单词标签组成的文件。 每一行是一个单词。
- en: 'These files can be uploaded to the Embedding Projector ([https://projector.tensorflow.org](https://projector.tensorflow.org))
    for visualization (see the previous figure). Because our embedding vectors reside
    in a high-dimensional (128D) space, it is necessary to reduce their dimensionality
    to three or fewer dimensions so that they can be understood by a human. The Embedding
    Projector tool provides two algorithms for dimension reduction: t-distributed
    stochastic neighbor embedding (t-SNE) and principal component analysis (PCA),
    which we won’t discuss in detail. But briefly, these methods map the high-dimensional
    embedding vectors to 3D while ensuring minimal loss in the relations between the
    vectors. t-SNE is the more sophisticated and computationally more intensive method
    between the two. The visualization it produces is shown in the figure.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件可以上传到嵌入投影仪（[https://projector.tensorflow.org](https://projector.tensorflow.org)）进行可视化（见前面的图）。因为我们的嵌入向量驻留在一个高维（128D）空间中，所以需要将它们的维度降低到三个或更少的维度，以便人类能够理解。嵌入投影仪工具提供了两种降维算法：t-分布随机邻域嵌入（t-SNE）和主成分分析（PCA），我们不会详细讨论。但简要地说，这些方法将高维嵌入向量映射到3D，同时确保向量之间的关系损失最小。t-SNE是两者中更复杂、计算更密集的方法。它产生的可视化效果如图所示。
- en: Each dot in the dot cloud corresponds to a word in our vocabulary. Move your
    mouse cursor around and hover it above the dots to see what words they correspond
    to. Our embedding vectors, trained on the smallish sentiment-analysis dataset,
    already show some interesting structure related to the semantics of the words.
    In particular, one end of the dot cloud contains a large proportion of words that
    appear frequently in positive movie reviews (such as *excellent*, *inspiring*,
    and *delightful*), while the opposite end contains many negative-sounding words
    (*sucks*, *gross*, and *pretentious*). More interesting structures may emerge
    from training larger models on larger text datasets, but this small example already
    gives you a hint of the power of the wordembedding method.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 每个点云中的点对应我们词汇表中的一个单词。将鼠标光标移动到点上方，悬停在点上以查看它们对应的单词。我们在较小的情感分析数据集上训练的嵌入向量已经显示出与单词语义相关的一些有趣结构。特别是，点云的一端包含许多在积极的电影评论中经常出现的词语（例如*优秀*、*鼓舞人心*和*令人愉快*），而另一端则包含许多听起来消极的词语（*糟糕*、*恶心*和*自命不凡*）。在更大的文本数据集上训练更大的模型可能会出现更有趣的结构，但是这个小例子已经给你一些关于词嵌入方法的威力的暗示。
- en: Because word embeddings are an important part of text-oriented deep neural networks,
    researchers have created pretrained word embeddings that machine-learning practitioners
    can use out-of-the-box, forgoing the need to train their own word embeddings as
    we did in our IMDb convnet example. One of the best known pretrained wordembedding
    sets is GloVe (for Global Vectors) by the Stanford Natural Language Processing
    Group (see [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 因为词嵌入是文本导向的深度神经网络的重要组成部分，研究人员创建了预训练词嵌入，机器学习从业者可以直接使用，无需像我们在IMDb卷积神经网络示例中那样训练自己的词嵌入。最著名的预训练词嵌入集之一是斯坦福自然语言处理组的GloVe（全局向量）（参见[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)）。
- en: The advantage of using pretrained word embeddings such as GloVe is two-fold.
    First, it reduces the amount of computation during training because the embedding
    layer doesn’t need to be trained further and hence can simply be frozen. Second,
    pretrained embeddings such as GloVe are trained from billions of words and hence
    are much higher-quality than what would be possible by training on a small dataset,
    such as the IMDb dataset here. In these senses, the role played by pretrained
    word embeddings in natural language processing problems is similar to the role
    of pretrained deep convnet bases (such as MobileNet, which we saw in [chapter
    5](kindle_split_016.html#ch05)) in computer vision.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的词嵌入（如GloVe）的优势是双重的。首先，它减少了训练过程中的计算量，因为嵌入层不需要进一步训练，因此可以直接冻结。其次，像GloVe这样的预训练嵌入是从数十亿个单词中训练出来的，因此质量比在小数据集上训练可能得到的要高得多，比如这里的IMDb数据集。从这些意义上讲，预训练词嵌入在自然语言处理问题中的作用类似于在计算机视觉中所见到的预训练深度卷积基（例如MobileNet，在[第5章](kindle_split_016.html#ch05)中见过）在计算机视觉中的作用。
- en: '|  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Using the 1D convnet for inference in a web page
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在网页中使用1D卷积神经网络进行推理
- en: In sentiment/index.js, you can find the code that deploys the model trained
    in Node.js to use at the client side. To see the client-side app in action, run
    the command `yarn watch` just like in most other examples in this book. The command
    will compile the code, start a web server, and automatically pop open a browser
    tab to display the index.html page. In the page, you can click a button to load
    the trained model via HTTP requests and use the loaded model to perform sentiment
    analysis on movie reviews in a text box. The movie review sample in the text box
    is editable, so you can make arbitrary edits to it and observe how that affects
    the binary prediction in real time. The page comes with two stock example reviews
    (a positive one and a negative one) that you may use as the starting point of
    your fiddling. The loaded 1D convnet runs fast enough that it can generate the
    sentiment score on the fly as you type in the text box.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 sentiment/index.js 中，你可以找到部署在 Node.js 中训练的模型以在客户端使用的代码。要查看客户端应用程序的运行情况，请运行命令
    `yarn watch`，就像本书中的大多数其他示例一样。该命令将编译代码，启动一个 web 服务器，并自动打开一个浏览器选项卡以显示 index.html
    页面。在页面中，你可以点击一个按钮通过 HTTP 请求加载训练好的模型，并在文本框中执行情感分析。文本框中的电影评论示例可编辑，因此你可以对其进行任意编辑，并观察实时观察到这如何影响二进制预测。页面带有两个示例评论（一个积极的评论和一个消极的评论），你可以将其用作你调试的起点。加载的
    1D convnet 运行速度足够快，可以在你在文本框中输入时实时生成情感分数。
- en: 'The core of the inference code is straightforward (see [listing 9.9](#ch09ex09),
    from sentiment/index.js), but there are several interesting things to point out:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 推断代码的核心很简单（参见 [列表 9.9](#ch09ex09)，来自 sentiment/index.js），但有几个有趣的地方值得指出：
- en: The code converts all the input text to lowercase, discards punctuation, and
    erases extra whitespace before converting the text to word indices. This is because
    the vocabulary we use contains only lowercase words.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该代码将所有输入文本转换为小写，丢弃标点符号，并在将文本转换为单词索引之前删除额外的空白。这是因为我们使用的词汇表只包含小写单词。
- en: Out-of-vocabulary words—words that fall outside the vocabulary—are represented
    with a special word index (`OOV_INDEX`). These include rare words and typos.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超出词汇表的词汇——即词汇表之外的词汇——用特殊的单词索引（`OOV_INDEX`）表示。这些词汇包括罕见的词汇和拼写错误。
- en: 'The same `padSequences()` function that we used for training (see [listing
    9.7](#ch09ex07)) is used here to make sure that the tensor input to the model
    has the correct length. This is achieved through truncation and padding, as we’ve
    seen previously. This is an example of a benefit of using TensorFlow.js for machine-learning
    tasks like this: you get to use the same data-preprocessing code for the backend
    training environment and the frontend serving environment, reducing the risk of
    data skew (see [chapter 6](kindle_split_018.html#ch06) for a more in-depth discussion
    of the risks of skew).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在训练中使用的相同 `padSequences()` 函数（参见 [列表 9.7](#ch09ex07)）在此处用于确保输入到模型的张量具有正确的长度。通过截断和填充来实现这一点，正如我们之前所见。这是使用
    TensorFlow.js 进行像这样的机器学习任务的一个好处的一个例子：你可以在后端训练环境和前端服务环境中使用相同的数据预处理代码，从而减少数据偏差的风险（有关数据偏差风险的更深入讨论，请参见
    [第 6 章](kindle_split_018.html#ch06)）。
- en: Listing 9.9\. Using the trained 1D convnet for inference in the frontend
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.9\. 在前端使用训练好的 1D convnet 进行推断
- en: '[PRE18]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1*** Converts to lowercase; removes punctuation and extra whitespace from
    the input text'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 转换为小写；从输入文本中删除标点符号和额外的空白'
- en: '***2*** Maps all the words to word indices. this.wordIndex has been loaded
    from a JSON file.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 将所有单词映射到单词索引。this.wordIndex 已从 JSON 文件加载。'
- en: '***3*** Words that fall out of the vocabulary are represented as a special
    word index: OOV_INDEX.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 超出词汇表的单词被表示为特殊的单词索引：OOV_INDEX。'
- en: '***4*** Truncates long reviews and pads short ones to the desired length'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 截断长评论，并填充短评论到所需长度'
- en: '***5*** Converts the data to a tensor representation, so it can be fed into
    the model'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将数据转换为张量表示，以便馈送到模型中'
- en: '***6*** Keeps track of how much time is spent on the model’s inference'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 跟踪模型推断所花费的时间'
- en: '***7*** The actual inference (forward pass on the model) happens here.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 实际推断（模型的前向传递）发生在这里。'
- en: 9.3\. Sequence-to-sequence tasks with attention mechanism
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 使用注意力机制的序列到序列任务
- en: 'In the Jena-weather and IMDb sentiment examples, we showed how to predict a
    single number or a class from an input sequence. However, some of the most interesting
    sequential problems involve generating an *output sequence* based on an input
    one. These types of tasks are aptly named *sequence-to-sequence* (or seq2seq,
    for short) tasks. There is a great variety of seq2seq tasks, of which the following
    list is just a small subset:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jena-weather和IMDb情感示例中，我们展示了如何从输入序列中预测单个数字或类别。然而，一些最有趣的序列问题涉及根据输入序列生成*输出序列*。这些类型的任务被恰当地称为*序列到序列*（或简称为seq2seq）任务。seq2seq任务有很多种，以下列表只是其中的一个小子集：
- en: '*Text summarization*—Given an article that may contain tens of thousands of
    words, generate a succinct summary of it (for example, in 100 or fewer words).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本摘要*—给定一篇可能包含数万字的文章，生成其简洁摘要（例如，100字或更少）。'
- en: '*Machine translation*—Given a paragraph in one language (such as English),
    generate a translation of it in another (such as Japanese).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器翻译*—给定一种语言（例如英语）中的一个段落，生成其在另一种语言（例如日语）中的翻译。'
- en: '*Word prediction for autocompletion*—Given a few first words in a sentence,
    predict what words will come after them. This is useful for autocompletion and
    suggestion in email apps and UIs for search engines.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动补全的单词预测*—给定句子中的前几个单词，预测它们之后会出现什么单词。这对电子邮件应用程序和搜索引擎UI中的自动补全和建议非常有用。'
- en: '*Music composition*—Given a leading sequence of musical notes, generate a melody
    that begins with those notes.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*音乐创作*—给定一系列音符的前导序列，生成以这些音符开头的旋律。'
- en: '*Chat bots*—Given a sentence entered by a user, generate a response that fulfills
    some conversational goal (for instance, a certain type of customer support or
    simply chatting for fun).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聊天机器人*—给定用户输入的一句话，生成一个满足某种对话目标的回应（例如，某种类型的客户支持或简单地用于娱乐聊天）。'
- en: The *attention mechanism*^([[11](#ch09fn11)]) is a powerful and popular method
    for seq2seq tasks. It is often used in conjunction with RNNs. In this section,
    we will show how we can use attention and LSTMs to solve a simple seq2seq task,
    namely, converting a myriad of calendar-date formats into a standard date format.
    Even though this is an intentionally simple example, the knowledge you’ll gain
    from it applies to more complex seq2seq tasks like the ones listed previously.
    Let’s first formulate the date-conversion problem.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意力机制*^([[11](#ch09fn11)])是一种强大且流行的用于seq2seq任务的方法。它通常与RNNs一起使用。在本节中，我们将展示如何使用注意力和LSTMs来解决一个简单的seq2seq任务，即将各种日历日期格式转换为标准日期格式。尽管这是一个有意简化的例子，但你从中获得的知识适用于像之前列出的更复杂的seq2seq任务。让我们首先制定日期转换问题。'
- en: ^(11)
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-278
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Alex Graves, “Generating Sequences with Recurrent Neural Networks, submitted
    4 Aug. 2013, [https://arxiv.org/abs/1308.0850](https://arxiv.org/abs/1308.0850);
    and Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural Machine Translation
    by Jointly Learning to Align and Translate,” submitted 1 Sept. 2014, [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Alex Graves，“Generating Sequences with Recurrent Neural Networks，”2013年8月4日提交，[https://arxiv.org/abs/1308.0850](https://arxiv.org/abs/1308.0850)；以及Dzmitry
    Bahdanau，Kyunghyun Cho和Yoshua Bengio，“Neural Machine Translation by Jointly Learning
    to Align and Translate，”2014年9月1日提交，[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)。
- en: 9.3.1\. Formulation of the sequence-to-sequence task
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1\. 序列到序列任务的制定
- en: If you are like us, you have been confused (or even mildly annoyed) by the large
    number of possible ways to write calendar dates, especially if you have traveled
    to different countries. Some people prefer to use the month-day-year order, some
    adopt the day-month-year order, and still others use the year-month-day order.
    Even within the same order, there are variations with regard to whether the month
    is written as a word (January), an abbreviation (Jan), a number (1), or a zero-padded
    two-digit number (01). The options for the day include whether you prepad it with
    a zero and whether you write it as an ordinal number (4th versus 4). As for the
    year, you can write the full four digits or only the last two. What’s more, the
    year, month, and day parts can be concatenated with spaces, commas, periods, or
    slashes, or they may be concatenated without any intervening characters at all!
    All these options come together in a combinatorial way, which gives rise to at
    least a few dozen ways to write the same date.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我们一样，你可能会因为写日历日期的可能方式太多而感到困惑（甚至可能有点恼火），特别是如果你去过不同的国家。有些人喜欢使用月-日-年的顺序，有些人采用日-月-年的顺序，还有些人使用年-月-日的顺序。即使在同一顺序中，对于月份是否写为单词（January）、缩写（Jan）、数字（1）或零填充的两位数字（01），也存在不同的选择。日期的选项包括是否在前面加零以及是否将其写为序数（4th
    与 4）。至于年份，你可以写全四位数或只写最后两位数。而且，年、月和日的部分可以用空格、逗号、句点或斜杠连接，或者它们可以在没有任何中间字符的情况下连接在一起！所有这些选项以组合的方式结合在一起，至少产生了几十种写相同日期的方式。
- en: So, it will be nice to have an algorithm that can take a calendar-date string
    in these formats as the input, and output the corresponding date string in the
    ISO-8601 format (for instance, 2019-02-05). We could solve this problem in a non-machine-learning
    way by writing a traditional program. But given the large number of possible formats,
    this is a somewhat cumbersome and time-consuming task, and the resulting code
    can easily reach hundreds of lines. Let’s try a deep-learning approach—in particular,
    with an LSTM-based attention encoder-decoder architecture.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，拥有一种算法可以将这些格式的日历日期字符串作为输入，并输出对应的 ISO-8601 格式的日期字符串（例如，2019-02-05）会很好。我们可以通过编写传统程序来非机器学习方式解决这个问题。但考虑到可能的格式数量庞大，这是一项有些繁琐且耗时的任务，结果代码很容易达到数百行。让我们尝试一种深度学习方法——特别是使用基于
    LSTM 的注意力编码器-解码器架构。
- en: 'To limit the scope of this example, we start from the 18 commonly seen date
    formats shown by the following examples. Note that all these are different ways
    to write the same date:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制本示例的范围，我们从以下示例展示的 18 种常见日期格式开始。请注意，所有这些都是写相同日期的不同方式：
- en: '[PRE19]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Of course, there are other date formats.^([[12](#ch09fn12)]) But adding support
    for additional formats will basically be a repetitive task once the foundation
    of the model training and inference has been laid. We leave the part of adding
    more input date formats as an exercise for you at the end of this chapter (exercise
    3).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有其他日期格式。[[12](#ch09fn12)] 但是一旦模型训练和推理的基础奠定，添加对其他格式的支持基本上将是一项重复性的任务。我们把添加更多输入日期格式的部分留给了本章末尾的练习（练习3）。
- en: ^(12)
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-287
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Another thing you might have noticed is that we use a set of date formats without
    any ambiguity. If we included both MM/DD/YYYY and DD/MM/YYYY in our set of formats,
    then there would be ambiguous date strings: that is, ones that can’t be interpreted
    with certainty. For instance, the string “01/02/2019” can be interpreted as either
    as January 2, 2019 or February 1, 2019.'
  id: totrans-288
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可能已经注意到的另一件事是，我们使用了一组没有任何歧义的日期格式。如果我们在我们的格式集中同时包含 MM/DD/YYYY 和 DD/MM/YYYY，那么就会有含糊不清的日期字符串：即，无法确定地解释的字符串。例如，字符串“01/02/2019”可以被解释为
    2019 年 1 月 2 日或 2019 年 2 月 1 日。
- en: 'First, let’s get the example running. Like the sentiment-analysis example earlier,
    this example consists of a training part and an inference part. The training part
    runs in the backend environment using tfjs-node or tfjs-node-gpu. To kick off
    the training, use the following commands:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们让示例运行起来。就像先前的情感分析示例一样，这个示例包括一个训练部分和一个推理部分。训练部分在后端环境中使用`tfjs-node`或`tfjs-node-gpu`运行。要启动训练，请使用以下命令：
- en: '[PRE20]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To perform the training using a CUDA GPU, use the `--gpu` flag with the `yarn
    train` command:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 CUDA GPU 执行训练，请在`yarn train`命令中使用`--gpu`标志：
- en: '[PRE21]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The training runs for two epochs by default, which should be sufficient to bring
    the loss value close to zero and the conversion accuracy close to perfect. In
    the sample inference results printed at the end of the training job, most, if
    not all, of the results should be correct. These inference samples are drawn from
    a test set that is nonoverlapping with the training set. The trained model is
    saved to the relative path dist/model and will be used during the browser-based
    inference stage. To bring up the inference UI, use
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练运行两个时期，这应该足以将损失值接近零并且转换精度接近完美。 在训练作业结束时打印的样本推断结果中，大多数，如果不是全部，结果应该是正确的。
    这些推断样本来自与训练集不重叠的测试集。 训练好的模型将保存到相对路径`dist/model`，并将在基于浏览器的推断阶段使用。 要启动推断 UI，请使用
- en: '[PRE22]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the web page that pops up, you can type dates into the Input Date String
    text box, hit Enter, and observe how the output date string changes accordingly.
    In addition, the heatmap with different shades displays the attention matrix used
    during the conversion (see [figure 9.9](#ch09fig09)). The attention matrix contains
    some interesting information and is central to this seq2seq model. It’s especially
    amenable to interpretation by humans. You should get yourself familiar with it
    by playing with it.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在弹出的网页中，您可以在输入日期字符串文本框中键入日期，然后按Enter键，观察输出日期字符串如何相应更改。 此外，具有不同色调的热图显示了转换期间使用的注意矩阵(请参见[图
    9.9](#ch09fig09))。 注意矩阵包含一些有趣的信息，并且是此 seq2seq 模型的核心。 它特别适合人类解释。 您应该通过与之互动来熟悉它。
- en: Figure 9.9\. The attention-based encoder-decoder for date conversion at work,
    with the attention matrix for the particular input-output pair displayed at the
    bottom-right
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.9\. 基于注意力的编码器-解码器在工作中进行日期转换，底部右侧显示了特定输入-输出对的注意力矩阵
- en: '![](09fig09_alt.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig09_alt.jpg)'
- en: 'Let’s take the result shown in [figure 9.9](#ch09fig09) as an example. The
    output of the model (`"2034-07-18"`) correctly translates the input date (`"JUL
    18, 2034"`). The rows of the attention matrix correspond to the input characters
    (`"J"`, `"U"`, `"L"`, `" "`, and so forth), while the columns correspond to the
    output characters (`"2"`, `"0"`, `"3"`, and so forth). So, each element of the
    attention matrix indicates how much attention is paid to the corresponding input
    character when the corresponding output character is generated. The higher the
    element’s value, the more attention is paid. For instance, look at the fourth
    column of the last row: that is, the one that corresponds to the last input character
    (`"4"`) and the fourth output character (`"4"`). It has a relatively high value,
    as indicated by the color scale. This makes sense because the last digit of the
    year part of the output should indeed depend primarily on the last digit of the
    year part in the input string. By contrast, other elements in that column have
    lower values, which indicates that the generation of the character `"4"` in the
    output string did not use much information from other characters of the input
    string. Similar patterns can be seen in the month and day parts of the output
    string. You are encouraged to experiment with other input date formats and see
    how the attention matrix changes.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们以[图 9.9](#ch09fig09)中显示的结果为例。 模型的输出(`"2034-07-18"`)正确地转换了输入日期(`"JUL 18,
    2034"`)。 注意矩阵的行对应于输入字符(`"J"`, `"U"`, `"L"`, `" "`, 等等)，而列对应于输出字符(`"2"`, `"0"`,
    `"3"`, 等等)。 因此，注意矩阵的每个元素指示了在生成相应输出字符时有多少关注力放在相应的输入字符上。 元素的值越高，关注度就越高。 例如，看看最后一行的第四列:
    也就是说，对应于最后一个输入字符(`"4"`)和第四个输出字符(`"4"`)的那个。 根据颜色刻度表，它具有相对较高的值。 这是有道理的，因为输出的年份部分的最后一位数字确实应该主要依赖于输入字符串的年份部分的最后一位数字。
    相比之下，该列中的其他元素具有较低的值，这表明输出字符串中字符`"4"`的生成并未使用来自输入字符串的其他字符的太多信息。 在输出字符串的月份和日期部分也可以看到类似的模式。
    鼓励您尝试使用其他输入日期格式，并查看注意矩阵如何变化。'
- en: 9.3.2\. The encoder-decoder architecture and the attention mechanism
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2\. 编码器-解码器架构和注意力机制
- en: This section helps you develop intuition for how the encoder-decoder architecture
    solves the seq2seq problem and what role the attention mechanism plays in it.
    An in-depth discussion of the mechanisms is presented alongside with the code
    in the following deep-dive section.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 本节帮助您了解编码器-解码器架构如何解决 seq2seq 问题以及注意力机制在其中起什么作用的直觉。 机制的深入讨论将与下面的深入研究部分中的代码一起呈现。
- en: 'Up to this point, all the neural networks we’ve seen output a single item.
    For a regression network, the output is just a single number; for a classification
    network, it’s a single probability distribution over a number of possible categories.
    But the date-conversion problem we are faced with is different: instead of predicting
    a single item, we need to predict a number of them. Specifically, we need to predict
    exactly 10 characters for the ISO-8601 date format. How should we achieve this
    using a neural network?'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们见过的所有神经网络都输出单个项目。对于回归网络，输出只是一个数字；对于分类网络，它是对可能类别的单个概率分布。但是我们面临的日期转换问题不同：我们不是要预测单个项目，而是需要预测多个项目。具体来说，我们需要准确预测ISO-8601日期格式的10个字符。我们应该如何使用神经网络实现这一点？
- en: 'The solution is to create a network that outputs a sequence of items. In particular,
    since the output sequence is made of discrete symbols from an “alphabet” with
    exactly 11 items (0 through 9, as well as the hyphen), we let the output tensor
    shape of the network have a 3D shape: `[numExamples, OUTPUT_LENGTH, OUTPUT_VOCAB_
    SIZE]`. The first dimension (`numExamples`) is the conventional example dimension
    that enables batch processing like all other networks we’ve seen in this book.
    `OUTPUT_ LENGTH` is 10—that is, the fixed length of the output date string in
    the ISO-8601 format. `OUTPUT_VOCAB_SIZE` is the size of the output vocabulary
    (or more accurately, “output alphabet”), which includes the digits 0 through 9
    and the hyphen (-), in addition to a couple of characters with special meanings
    that we’ll discuss later.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是创建一个输出序列的网络。特别是，由于输出序列由来自具有确切11个项目的“字母表”的离散符号组成，我们让网络的输出张量形状为3D形状：`[numExamples,
    OUTPUT_LENGTH, OUTPUT_VOCAB_SIZE]`。第一个维度（`numExamples`）是传统的示例维度，使得像本书中看到的所有其他网络一样可以进行批处理。`OUTPUT_LENGTH`为10，即ISO-8601格式输出日期字符串的固定长度。`OUTPUT_VOCAB_SIZE`是输出词汇表的大小（或更准确地说，“输出字母表”），其中包括数字0到9和连字符(-)，以及我们稍后将讨论的一些具有特殊含义的字符。
- en: So that covers the model’s output. How about the model’s inputs? It turns out
    the model takes *two* inputs instead of one. The model can be divided roughly
    into two parts, the encoder and the decoder, as is shown schematically in [figure
    9.10](#ch09fig10). The first input of the model goes into the encoder part. It
    is the input date string itself, represented as a sequence of character indices
    of shape `[numExamples, INPUT_LENGTH]`. `INPUT_LENGTH` is the maximum possible
    length among the supported input date formats (which turns out to be 12). Inputs
    shorter than that length are padded with zeros at the end. The second input goes
    into the decoder part of the model. It is the conversion result shifted to the
    right by one time step, and it has a shape of `[numExamples, OUTPUT_LENGTH]`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就涵盖了模型的输出。那么模型的输入呢？原来，模型不是一个输入，而是*两个*输入。模型可以大致分为两部分，编码器和解码器，如[图9.10](#ch09fig10)所示。模型的第一个输入进入编码器部分。它是输入日期字符串本身，表示为形状为`[numExamples,
    INPUT_LENGTH]`的字符索引序列。`INPUT_LENGTH`是支持的输入日期格式中最大可能的长度（结果为12）。比该长度短的输入在末尾用零填充。第二个输入进入模型的解码器部分。它是右移一个时间步长的转换结果，形状为`[numExamples,
    OUTPUT_LENGTH]`。
- en: Figure 9.10\. How the encoder-decoder architecture converts an input date string
    into an output one. `ST` is the special starting token for the decoder’s input
    and output. Panels A and B show the first two steps of the conversion, respectively.
    After the first conversion step, the first character in the output (`"2"`) is
    generated. After the second step, the second character (`"0"`) is generated. The
    remaining steps follow the same pattern and are hence omitted.
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.10\. 编码器-解码器架构如何将输入日期字符串转换为输出字符串。`ST`是解码器输入和输出的特殊起始标记。面板A和B分别显示了转换的前两个步骤。在第一个转换步骤之后，生成了输出的第一个字符（`"2"`）。在第二步之后，生成了第二个字符（`"0"`）。其余步骤遵循相同的模式，因此被省略。
- en: '![](09fig10_alt.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig10_alt.jpg)'
- en: Wait, the first input makes sense because it’s the input date string, but why
    does the model take the conversion result as an additional input? Isn’t that meant
    to be the *out**put* of the model? The key lies in the shifting of the conversion
    result. Note that the second input is *not* exactly the conversion result. Instead,
    it is a time-delayed version of the conversion result. The time delay is by exactly
    one step. For example, if during training, the desired conversion result is `"2034-07-18"`,
    then the second input to the model will be `"<ST>2034-07-1"`, where `<ST>` is
    a special start-of-sequence symbol. This shifted input lets the decoder be aware
    of the output sequence that has been generated so far. It makes it easier for
    the decoder to keep track of where it is in the conversion process.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，第一个输入是有意义的，因为它是输入日期字符串，但是为什么模型将转换结果作为额外的输入呢？这不是模型的输出吗？关键在于转换结果的偏移。请注意，第二个输入并不完全是转换结果。相反，它是转换结果的时延版本。时延为一步。例如，在训练期间，期望的转换结果是
    `"2034-07-18"`，那么模型的第二个输入将是 `"<ST>2034-07-1"`，其中 `<ST>` 是一个特殊的序列起始符号。这个偏移的输入使解码器能够意识到到目前为止已经生成的输出序列。它使解码器更容易跟踪转换过程中的位置。
- en: 'This is analogous to how humans speak. When you put a thought into words, your
    mental effort is spent on two things: the concept itself and what you’ve said
    so far. The latter part is important to ensure coherent, complete, and nonrepetitive
    speech. Our model works in a similar fashion: to generate every output character,
    it uses the information from both the input date string and the output characters
    that have been generated so far.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于人类说话的方式。当你将一个想法用语言表达出来时，你的心智努力分为两个部分：想法本身和你到目前为止所说的内容。后者对于确保连贯、完整和不重复的言论至关重要。我们的模型以类似的方式工作：为了生成每个输出字符，它使用来自输入日期字符串和到目前为止已生成的输出字符的信息。
- en: 'The time-delaying of the conversion result works during the training phase
    because we already know what the correct conversion result is. But how does it
    work during inference? The answer can be seen in the two panels of [figure 9.10](#ch09fig10):
    we generate the output characters one by one.^([[13](#ch09fn13)]) As panel A of
    the figure shows, we start by sticking an `ST` symbol at the beginning of the
    decoder’s input. Through one step of inference (one `Model.predict()` call), we
    obtain a new output item (the `"2"` in the panel). This new output item is then
    appended to the decoder input. Then the next step of conversion ensues. It sees
    the newly generated output character `"2"` in the decoder input (see panel B of
    [figure 9.10](#ch09fig10)). This step involves another `Model.predict()` call
    and generates a new output character (`"0"`), which is again appended to the decoder
    input. This process repeats until the desired length of the output (10 in this
    case) is reached. Notice that the output doesn’t include the `ST` item, so it
    can be used directly as the final output of the entire algorithm.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，转换结果的时延效果是有效的，因为我们已经知道正确的转换结果是什么。但是在推断过程中它是如何工作的呢？答案可以在 [图9.10](#ch09fig10)
    的两个面板中看到：我们逐个生成输出字符。如图的面板A所示，我们从将一个 `ST` 符号置于解码器输入的开头开始。通过一步推断（一个 `Model.predict()`
    调用），我们得到一个新的输出项（面板中的 `"2"`）。然后，这个新的输出项被附加到解码器输入中。然后进行转换的下一步。它在解码器输入中看到了新生成的输出字符
    `"2"`（请参阅 [图9.10](#ch09fig10) 的面板B）。这一步涉及另一个 `Model.predict()` 调用，并生成一个新的输出字符（`"0"`），然后再次附加到解码器输入中。这个过程重复，直到达到所需的输出长度（在本例中为10）。注意，输出不包括
    `ST` 项目，因此可以直接用作整个算法的最终输出。
- en: ^(13)
  id: totrans-309
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(13)
- en: ''
  id: totrans-310
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The code that implements the step-by-step conversion algorithm is the function
    `runSeq2SeqInference()` in date-conversion-attention/model.js.
  id: totrans-311
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实现逐步转换算法的代码是 `date-conversion-attention/model.js` 中的函数 `runSeq2SeqInference()`。
- en: The role of the attention mechanism
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意机制的作用
- en: The role of the attention mechanism is to enable each output character to “attend”
    to the correct characters in the input sequence. For example, the `"7"` part of
    the output string `"2034-07-18"` should attend to the `"JUL"` part of the input
    date string. This is again analogous to how humans generate language. For instance,
    when we translate a sentence from language A to language B, each word in the output
    sentence is usually determined by a small number of words from the input sentence.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制的作用是使每个输出字符能够“关注”输入序列中的正确字符。例如，输出字符串“2034-07-18”的“7”部分应关注输入日期字符串的“JUL”部分。这与人类生成语言的方式类似。例如，当我们将语言
    A 的句子翻译成语言 B 时，输出句子中的每个单词通常由输入句子中的少数单词确定。
- en: 'This may seem like a no-brainer: it’s hard to imagine what other approaches
    might work better. But the introduction of the attention mechanism introduced
    by deep-learning researchers around 2014–2015 was a major advancement in the field.
    To understand the historical reason behind this, look at the arrow that connects
    the Encoder box with the Decoder box in panel A of [figure 9.10](#ch09fig10).
    This arrow represents the last output of an LSTM in the encoder part of the model,
    which is passed to an LSTM in the decoder part of the model as its initial state.
    Recall that the initial state of RNNs is typically all-zero (for example, the
    simpleRNN we used in [section 9.1.2](#ch09lev2sec2)); however, TensorFlow.js allows
    you to set the initial state of an RNN to any given tensor value of the correct
    shape. This can be used as a way to pass upstream information to an LSTM. In this
    case, the encoder-to-decoder connection uses this mechanism to let the decoder
    LSTM access the encoded input sequence.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来显而易见：很难想象还有什么其他方法可能效果更好。但是，深度学习研究人员在2014年至2015年左右引入的注意机制的介绍是该领域的重大进展。要理解其历史原因，请查看[图
    9.10](#ch09fig10) A 面板中连接编码器框与解码器框的箭头。此箭头表示模型中编码器部分中 LSTM 的最后输出，该输出被传递到模型中解码器部分中的
    LSTM 作为其初始状态。回想一下 RNN 的初始状态通常是全零的（例如，我们在 [section 9.1.2](#ch09lev2sec2) 中使用的 simpleRNN）；但是，TensorFlow.js
    允许您将 RNN 的初始状态设置为任何给定形状的张量值。这可以用作向 LSTM 传递上游信息的一种方式。在这种情况下，编码器到解码器的连接使用此机制使解码器
    LSTM 能够访问编码的输入序列。
- en: However, the initial state is an entire input sequence packed into a single
    vector. It turns out that this representation is a little too condensed for the
    decoder to unpack, especially for longer and more complex sequences (such as the
    sentences seen in typical machine-translation problems). This is where the attention
    mechanism comes into play.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，初始状态是将整个输入序列打包成单个向量。事实证明，对于更长且更复杂的序列（例如典型的机器翻译问题中看到的句子），这种表示方式有点太简洁了，解码器无法解压缩。这就是注意机制发挥作用的地方。
- en: 'The attention mechanism expands the “field of view” available to the decoder.
    Instead of using just the encoder’s final output, the attention mechanism accesses
    the entire sequence of the encoder’s output. At each step of the conversion process,
    the mechanism attends to specific time steps in the encoder’s output sequence
    in order to decide what output character to generate. For example, the first conversion
    step may pay attention to the first two input characters, while the second conversion
    step pays attention to the second and third input characters, and so forth (see
    [figure 9.10](#ch09fig10) for a concrete example of such an attention matrix).
    Just like all weight parameters of the neural network, an attention model *learns*
    the way in which it allocated attention, instead of hard-coding a policy. This
    makes the model flexible and powerful: it can learn to attend to different parts
    of the input sequence depending on both the input sequence itself and what has
    been generated in the output sequence so far.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制扩展了解码器可用的“视野”。不再仅使用编码器的最终输出，注意机制访问整个编码器输出序列。在转换过程的每一步中，该机制会关注编码器输出序列中特定的时间步，以决定生成什么输出字符。例如，第一次转换步骤可能会关注前两个输入字符，而第二次转换步骤则关注第二个和第三个输入字符，依此类推（见[图
    9.10](#ch09fig10) ，其中提供了这种注意矩阵的具体示例）。就像神经网络的所有权重参数一样，注意模型 *学习* 分配注意力的方式，而不是硬编码策略。这使得模型灵活且强大：它可以根据输入序列本身以及迄今为止在输出序列中生成的内容学习关注输入序列的不同部分。
- en: This is as far as we can go in talking about the encoder-decoder mechanism without
    looking at the code or opening the black boxes that are the encoder, decoder,
    and attention mechanism. If this treatment sounds too high-level or too vague
    to you, read the next section, where we’ll dive a little deeper into the nuts
    and bolts of the model. This is worth the mental effort for those who wish to
    get a deeper understanding of the attention-based encoder-decoder architecture.
    To motivate you to read it, realize that the same architecture underlies systems
    such as state-of-the-art machine-translation models (Google Neural Machine Translation,
    or GNMT), even though these production models employ more layers of LSTMs and
    are trained on much larger amounts of data than the simple date-conversion model
    we are dealing with here.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在不看代码或打开编码器、解码器和注意力机制这些黑盒子的情况下，我们已经尽可能深入地讨论了编码器-解码器机制。如果你觉得这个处理过程对你来说太过高层或太模糊，请阅读下一节，我们将更深入地探讨模型的细节。这对于那些希望更深入了解基于注意力机制的编码器-解码器架构的人来说是值得付出的心智努力。要激励你去阅读它，要意识到相同的架构也支撑着一些系统，比如最先进的机器翻译模型（Google
    神经网络机器翻译，或 GNMT），尽管这些生产模型使用了更多层的 LSTM 并且在比我们处理的简单日期转换模型大得多的数据上进行了训练。
- en: 9.3.3\. Deep dive into the attention-based encoder-decoder model
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.3\. 深入理解基于注意力机制的编码器-解码器模型
- en: '[Figure 9.11](#ch09fig11) expands the boxes in [figure 9.10](#ch09fig10) and
    provides a more detailed view of their internal structures. It is most illustrative
    to view it in conjunction with the code that builds the model: `createModel()`
    function in date-conversion-attention/model.js. We’ll next walk through the important
    aspects of the code.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.11](#ch09fig11)扩展了[图 9.10](#ch09fig10)中的方框，并提供了它们内部结构的更详细视图。将它与构建模型的代码一起查看最具说明性：`date-conversion-attention/model.js`中的`createModel()`函数。接下来我们将逐步介绍代码的重要部分。'
- en: Figure 9.11\. Deep dive into the attention-based encoder-decoder model. You
    can think of this figure as an expanded view of the encoder-decoder architecture
    outlined in [figure 9.10](#ch09fig10), with finer-grained details depicted.
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.11\. 深入理解基于注意力机制的编码器-解码器模型。你可以把这个图像看作是对[图 9.10](#ch09fig10)中概述的编码器-解码器架构的扩展视图，显示了更细粒度的细节。
- en: '![](09fig11_alt.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig11_alt.jpg)'
- en: 'First, we define a couple of constants for the embedding and LSTM layers in
    the encoder and decoder:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为编码器和解码器中的嵌入和 LSTM 层定义了一些常量：
- en: '[PRE23]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The model we will construct takes two inputs, so we must use the functional
    model API instead of the sequential API. We start from the model’s symbolic inputs
    for the encoder input and the decoder input, respectively:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建的模型接受两个输入，因此我们必须使用功能模型 API 而不是顺序 API。我们从模型的符号输入开始，分别是编码器输入和解码器输入：
- en: '[PRE24]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The encoder and decoder both apply an embedding layer on their respective input
    sequences. The code for the encoder looks like
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器都对它们各自的输入序列应用了一个嵌入层。编码器的代码看起来像这样
- en: '[PRE25]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is similar to the embedding layers we used in the IMDb sentiment problem,
    but it embeds characters instead of words. This shows that the embedding method
    is not limited to words. In fact, it is flexible enough to be applied on any finite,
    discrete set, such as music genres, articles on a news website, airports in a
    country, and so forth. The `maskZero: true` configuration of the embedding layer
    instructs the downstream LSTM to skip steps with all-zero values. This saves unnecessary
    computation on sequences that have already ended.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '这类似于我们在 IMDb 情感问题中使用的嵌入层，但它是对字符而不是单词进行嵌入。这表明嵌入方法并不局限于单词。事实上，它足够灵活，可以应用于任何有限的、离散的集合，比如音乐类型、新闻网站上的文章、一个国家的机场等等。嵌入层的`maskZero:
    true`配置指示下游的 LSTM 跳过所有零值的步骤。这样就可以节省在已经结束的序列上的不必要计算。'
- en: 'LSTM is an RNN type we haven’t covered in detail yet. We won’t go into its
    internal structure here. It suffices to say that it is similar to GRU ([figure
    9.4](#ch09fig04)) in that it addresses the vanishing-gradient problem by making
    it easier to carry a state over multiple time steps. Chris Olah’s blog post “Understanding
    LSTM Networks,” for which a pointer is provided in “[Materials for further reading](#ch09lev1sec4)”
    at the end of the chapter, presents an excellent review and visualization of the
    structure and mechanisms of LSTMs. Our encoder LSTM is applied on the character-embedding
    vectors:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种我们尚未详细介绍的 RNN 类型。我们不会在这里讨论其内部结构。简而言之，它类似于 GRU（[图 9.4](#ch09fig04)）,
    通过使得在多个时间步中传递状态变得更容易来解决梯度消失的问题。Chris Olah 的博文“理解 LSTM 网络”，在本章末尾提供了指针在 “[进一步阅读资料](#ch09lev1sec4)”
    中，对 LSTM 结构和机制进行了出色的评述和可视化。我们的编码器 LSTM 应用在字符嵌入向量上：
- en: '[PRE26]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `returnSequences:` `true` configuration lets the output of the LSTM be a
    sequence of output vectors instead of the default output of a single vector that’s
    the final output (as we did in the temperature-prediction and sentiment-analysis
    models). This step is required by the downstream attention mechanism.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`returnSequences:` `true` 配置使得 LSTM 的输出是输出向量序列，而不是默认的单个向量输出（就像我们在温度预测和情感分析模型中所做的那样）。这一步是下游注意力机制所需的。'
- en: 'The `GetLastTimestepLayer` layer that follows the encoder LSTM is a custom-defined
    layer:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随编码器 LSTM 的 `GetLastTimestepLayer` 层是一个自定义定义的层：
- en: '[PRE27]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It simply slices the time-sequence tensor along the time dimension (the second
    dimension) and outputs the last time step. This allows us to send the final state
    of the encoder LSTM to the decoder LSTM as its initial state. This connection
    is one of the ways in which the decoder gets information about the input sequence.
    This is illustrated in [figure 9.11](#ch09fig11) with the arrow that connects
    *h*[12] in the green encoder block to the decoder LSTM layer in the blue decoder
    block.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 它简单地沿着时间维度（第二维度）切片时间序列张量并输出最后一个时间步。这使我们能够将编码器 LSTM 的最终状态发送到解码器 LSTM 作为其初始状态。这种连接是解码器获取有关输入序列信息的方式之一。这在
    [图 9.11](#ch09fig11) 中用将绿色编码器块中的 *h*[12] 与蓝色解码器块中的解码器 LSTM 层连接的箭头进行了说明。
- en: 'The decoder part of the code begins with an embedding layer and an LSTM layer
    reminiscent of the encoder’s topology:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的解码器部分以类似于编码器的拓扑结构的嵌入层和 LSTM 层开始：
- en: '[PRE28]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the last line of this code snippet, notice how the final state of the encoder
    is used as the initial state of the decoder. In case you wonder why the symbolic
    tensor `encoderLast` is repeated in the last line of code here, it is because
    an LSTM layer contains two states, unlike the one-state structure we’ve seen in
    simpleRNN and GRU.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段的最后一行，注意编码器的最终状态如何用作解码器的初始状态。如果你想知道为什么在这里的代码的最后一行中重复使用符号张量 `encoderLast`，那是因为
    LSTM 层包含两个状态，不像我们在 simpleRNN 和 GRU 中看到的单状态结构。
- en: 'The additional, and more powerful, way in which the decoder gets a view at
    the input sequences is, of course, the attention mechanism. The attention is a
    dot product (element-by-element product) between the encoder LSTM’s output and
    the decoder LSTM’s output, followed by a softmax activation:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器更强大的另一种方式是获得输入序列的视图，当然，这是通过注意力机制实现的。注意力是编码器 LSTM 输出和解码器 LSTM 输出的点积（逐元素相乘），然后是
    softmax 激活：
- en: '[PRE29]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The encoder LSTM’s output has a shape of `[null, 12, 64]`, where 12 is the input
    sequence’s length and 64 is the LSTM’s size. The decoder LSTM’s output has a shape
    of `[null, 10, 64]`, where 10 is the output sequence’s length and 64 is the LSTM’s
    size. A dot product between the two is performed along the last (LSTM features)
    dimension, which gives rise to a shape of `[null, 10, 12]` (that is, `[null, inputLength,
    outputLength]`). The softmax applied on the dot product turns the values into
    probability scores, which are guaranteed to be positive and sum to 1 along each
    column of the matrix. This is the attention matrix that’s central to our model.
    Its value is what’s visualized in the earlier [figure 9.9](#ch09fig09).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器 LSTM 的输出形状为 `[null, 12, 64]`，其中 12 是输入序列的长度，64 是 LSTM 的大小。解码器 LSTM 的输出形状为
    `[null, 10, 64]`，其中 10 是输出序列的长度，64 是 LSTM 的大小。在最后一个（LSTM 特征）维度上执行两者的点积，得到 `[null,
    10, 12]` 的形状（即 `[null, inputLength, outputLength]`）。对点积应用 softmax 将值转换为概率分数，保证它们在矩阵的每一列上都是正数且总和为
    1。这是我们模型中心的注意力矩阵。其值是早期 [图 9.9](#ch09fig09) 中可视化的。
- en: 'The attention matrix is then applied on the sequential output from the encoder
    LSTM. This is how the conversion process learns to pay attention to different
    elements of the input sequence (in its encoded form) at each step. The result
    of applying the attention on the encoder’s output is called the *context*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，注意力矩阵应用于编码器LSTM的序列输出。这是转换过程学习如何在每个步骤上关注输入序列（以其编码形式）中的不同元素的方式。将注意力应用于编码器输出的结果称为*上下文*：
- en: '[PRE30]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The context has a shape of `[null,` `10,` `64]` (that is, `[null,` `outputLength,`
    `lstmUnits]`). It is concatenated with the decoder’s output, which also has a
    shape of `[null, 10, 64]`. So, the result of the concatenation has a shape of
    `[null, 10, 128]`:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文的形状为`[null, 10, 64]`（即`[null, outputLength, lstmUnits]`）。它与解码器的输出连接在一起，解码器的输出形状也为`[null,
    10, 64]`。因此，连接的结果形状为`[null, 10, 128]`：
- en: '[PRE31]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`decoderCombinedContext` contains the feature vectors that go into the final
    stage of the model, namely, the stage that generates the output characters.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoderCombinedContext`包含进入模型最终阶段的特征向量，即生成输出字符的阶段。'
- en: 'The output characters are generated using an MLP that contains one hidden layer
    and a softmax output layer:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 输出字符使用包含一个隐藏层和一个softmax输出层的MLP生成：
- en: '[PRE32]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Thanks to the `timeDistributed` layer, all steps share the same MLP. The `timeDistributed`
    layer takes a layer and calls it repeatedly over all steps along the time dimension
    (that is, the second dimension) of its input. This converts the input feature
    shape of `[null, 10, 128]` to `[null, 10, 13]`, where 13 corresponds to the 11
    possible characters of the ISO-8601 date format, as well as the 2 special characters
    (padding and start-of-sequence).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了`timeDistributed`层，所有步骤共享同一个MLP。`timeDistributed`层接受一个层，并在其输入的时间维度（即第二维度）上重复调用它。这将输入特征形状从`[null,
    10, 128]`转换为`[null, 10, 13]`，其中13对应于ISO-8601日期格式的11个可能字符，以及2个特殊字符（填充和序列起始）。
- en: 'With all the pieces in place, we assemble them together into a `tf.Model` object
    with two inputs and one output:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 所有组件齐备后，我们将它们组装成一个具有两个输入和一个输出的`tf.Model`对象：
- en: '[PRE33]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To prepare for training, we call the `compile()` method with a categorical
    cross-entropy loss function. The choice of this loss function is based on the
    fact that the conversion problem is essentially a classification problem—at each
    time step, we choose a character from the set of all possible characters:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备训练，我们使用分类交叉熵损失函数调用`compile()`方法。选择这个损失函数是基于转换问题本质上是一个分类问题——在每个时间步，我们从所有可能字符的集合中选择一个字符：
- en: '[PRE34]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: At inference time, an `argMax()` operation is applied on the model’s output
    tensor to obtain the winning output character. At every step of the conversion,
    the winning output character is appended to the decoder’s input, so the next conversion
    step can use it (see the arrow on the right end of [figure 9.11](#ch09fig11)).
    As we mentioned before, this iterative process eventually yields the entire output
    sequence.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 推理时，对模型的输出张量应用`argMax()`操作以获取获胜的输出字符。在转换的每一步中，获胜的输出字符都会附加到解码器的输入中，因此下一转换步骤可以使用它（参见[图9.11](#ch09fig11)右端的箭头）。正如我们之前提到的，这个迭代过程最终产生整个输出序列。
- en: Materials for further reading
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步阅读的材料
- en: Chris Olah, “Understanding LSTM Networks,” blog, 27 Aug. 2015, [http://mng.bz/m4Wa](http://mng.bz/m4Wa).
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chris Olah，《理解LSTM网络》，博客，2015年8月27日，[http://mng.bz/m4Wa](http://mng.bz/m4Wa)。
- en: Chris Olah and Shan Carter, “Attention and Augmented Recurrent Neural Networks,”
    Distill, 8 Sept. 2016, [https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/).
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chris Olah和Shan Carter，《注意力和增强递归神经网络》，Distill，2016年9月8日，[https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/)。
- en: Andrej Karpathy, “The Unreasonable Effectiveness of Recurrent Neural Networks,”
    blog, 21 May 2015, [http://mng.bz/6wK6](http://mng.bz/6wK6).
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy，《递归神经网络的不合理有效性》，博客，2015年5月21日，[http://mng.bz/6wK6](http://mng.bz/6wK6)。
- en: Zafarali Ahmed, “How to Visualize Your Recurrent Neural Network with Attention
    in Keras,” Medium, 29 June 2017, [http://mng.bz/6w2e](http://mng.bz/6w2e).
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zafarali Ahmed，《如何使用Keras可视化您的递归神经网络和注意力》，Medium，2017年6月29日，[http://mng.bz/6w2e](http://mng.bz/6w2e)。
- en: In the date-conversion example, we described a decoding technique based on `argMax()`.
    This approach is often referred to as the *greedy decoding* technique because
    it extracts the output symbol of the highest probability at every step. A popular
    alternative to the greedy-decoding approach is *beam-search* decoding, which examines
    a larger range of possible output sequences in order to determine the best one.
    You can read more about it from Jason Brownlee, “How to Implement a Beam Search
    Decoder for Natural Language Processing,” 5 Jan. 2018, [https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/).
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在日期转换示例中，我们描述了一种基于`argMax()`的解码技术。这种方法通常被称为*贪婪解码*技术，因为它在每一步都提取具有最高概率的输出符号。贪婪解码方法的一个流行替代方案是*波束搜索*解码，它检查更大范围的可能输出序列，以确定最佳序列。你可以从Jason
    Brownlee的文章“如何为自然语言处理实现波束搜索解码器”中了解更多信息，2018年1月5日，[https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)。
- en: Stephan Raaijmakers, *Deep Learning for Natural Language Processing*, Manning
    Publications, in press, [www.manning.com/books/deep-learning-for-natural-language-processing](http://www.manning.com/books/deep-learning-for-natural-language-processing).
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stephan Raaijmakers，《自然语言处理的深度学习》，Manning Publications，在出版中，[www.manning.com/books/deep-learning-for-natural-language-processing](http://www.manning.com/books/deep-learning-for-natural-language-processing)。
- en: Exercises
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'Try rearranging the order of the data elements for various nonsequential data.
    Confirm that such reordering has no effect on the loss-metric values (for example,
    accuracy) of the modeling (beyond random fluctuation caused by random initialization
    of the weight parameters). You can do this for the following two problems:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试重新排列各种非连续数据的数据元素顺序。确认这种重新排序对建模的损失指标值（例如准确度）没有影响（超出由权重参数的随机初始化引起的随机波动）。你可以为以下两个问题进行此操作：
- en: In the iris-flower example (from [chapter 3](kindle_split_014.html#ch03)), rearrange
    the order of the four numeric features (petal length, petal width, sepal length,
    and sepal width) by making changes to the line
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在鸢尾花示例（来自[第3章](kindle_split_014.html#ch03)）中，通过修改行来重新排列四个数字特征（花瓣长度、花瓣宽度、萼片长度和萼片宽度）的顺序
- en: '[PRE35]'
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: in the iris/data.js file of the tfjs-examples repo. In particular, alter the
    order of the four elements in `data[indices[i]]`. This can be done through calls
    to the `slice()` and `concat()` methods of the JavaScript array. Note that the
    order rearrangement ought to be the same for all examples. You may write a JavaScript
    function to perform the reordering.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在tfjs-examples仓库的iris/data.js文件中。特别是，改变`data[indices[i]]`中四个元素的顺序。这可以通过JavaScript数组的`slice()`和`concat()`方法来完成。请注意，所有示例的重新排列顺序应该是相同的。你可以编写一个JavaScript函数来执行重新排序。
- en: In the linear regressor and MLP that we developed for the Jena-weather problem,
    try reordering the 240 time steps *and* the 14 numeric features (weather-instrument
    measurements). Specifically, you can achieve this by modifying the `nextBatchFn()`
    function in jena-weather/data.js. The line where it is the easiest to implement
    the reordering is
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们为Jena气象问题开发的线性回归器和MLP中，尝试重新排列240个时间步长*和*14个数字特征（气象仪器测量）。具体来说，你可以通过修改jena-weather/data.js中的`nextBatchFn()`函数来实现这一点。实现重新排序最容易的地方是
- en: '[PRE36]'
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: where you can map the index `exampleRow` to a new value using a function that
    performs a fixed permutation and map `exampleCol` in a similar manner.
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，你可以使用一个执行固定排列的函数将索引`exampleRow`映射到一个新值，并以类似的方式映射`exampleCol`。
- en: The 1D convnet we built for the IMDb sentiment analysis consisted of only one
    conv1d layer (see [listing 9.8](#ch09ex08)). As we discussed, stacking more conv1d
    layers on top of it may give us a deeper 1D convnet capable of capturing order
    information over a longer span of words. In this exercise, practice modifying
    the code in the `buildModel()` function of sentiment/train.js. The goal is to
    add another conv1d layer after the existing one, retrain the model, and observe
    if there is any improvement in its classification accuracy. The new conv1d layer
    may use the same number of filters and kernel size as the existing one. Also,
    read the output shapes in the summary of the modified model and make sure you
    understand how the `filters` and `kernelSize` parameters lead to the output shape
    of the new conv1d layer.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为 IMDb 情感分析构建的 1D 卷积神经网络仅包含一个 conv1d 层（参见[清单 9.8](#ch09ex08)）。正如我们讨论的那样，在其上叠加更多的
    conv1d 层可能会给我们一个更深的 1D 卷积神经网络，能够捕捉到更长一段单词的顺序信息。在这个练习中，尝试修改 sentiment/train.js
    中 `buildModel()` 函数中的代码。目标是在现有的层之后添加另一个 conv1d 层，重新训练模型，并观察其分类精度是否有所提高。新的 conv1d
    层可以使用与现有层相同数量的滤波器和内核大小。此外，请阅读修改后模型的摘要中的输出形状，并确保您理解 `filters` 和 `kernelSize` 参数如何影响新
    conv1d 层的输出形状。
- en: 'In the date-conversion-attention example, try adding a couple more input date
    formats. Following are the new formats you can choose from, sorted in order of
    increasing coding difficulty. You can also come up with your own date formats:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在日期转换注意事项示例中，尝试添加更多的输入日期格式。以下是您可以选择的新格式，按照编码难度递增的顺序排序。您也可以自己想出自己的日期格式：
- en: 'The YYYY-MMM-DD format: for example, “2012-MAR-08” or “2012-MAR-18.” Depending
    on whether single-digit day numbers are prepadded with a zero (as in 12/03/2015),
    this can actually be two different formats. However, regardless of the padding,
    the maximum length of this format is less than 12, and all the possible characters
    are already in the `INPUT_VOCAB` in date-conversion-attention/date_format.js.
    Therefore, all it takes is to add a function or two to the file, and those functions
    can be modeled after existing ones, such as `dateTupleToMMMSpaceDDSpaceYY()`.
    Make sure you add the new function(s) to the `INPUT_FNS` array in the file, so
    they can be included in the training. As a best practice, you should also add
    unit tests for your new date-format functions to date-conversion-attention/date_format_test.js.'
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: YYYY-MMM-DD 格式：例如，“2012年3月8日”或“2012年3月18日”。根据单个数字日期是否在前面补零（如 2015/03/12），这实际上可能是两种不同的格式。但是，无论如何填充，此格式的最大长度都小于12，并且所有可能的字符都已经在
    date-conversion-attention/date_format.js 中的 `INPUT_VOCAB` 中。因此，只需向文件添加一个或两个函数即可，这些函数可以模仿现有函数，例如
    `dateTupleToMMMSpaceDDSpaceYY()`。确保将新函数添加到文件中的 `INPUT_FNS` 数组中，以便它们可以包含在训练中。作为最佳实践，您还应该为新的日期格式函数添加单元测试到
    date-conversion-attention/date_format_test.js 中。
- en: A format with ordinal numbers as the day part, such as “Mar 8th, 2012.” Note
    that this is the same as the existing `dateTupleToMMMSpaceDDComma-SpaceYYYY()`
    format, except that the day number is suffixed with the ordinal suffices (`"st"`,
    `"nd"`, and `"th"`). Your new function should include the logic to determine the
    suffix based on the day value. In addition, you need to revise the `INPUT_LENGTH`
    constant in date_format_test.js to a larger value because the maximum possible
    length of the date string in this format exceeds the current value of 12\. Furthermore,
    the letters `"t"` and `"h"` need to be added to `INPUT_VOCAB`, as they do not
    appear in any of the three-letter month strings.
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个使用序数作为日期部分的格式，比如“3月8日，2012年”。请注意，这与现有的`dateTupleToMMMSpaceDDComma-SpaceYYYY()`格式相同，只是日期数字后缀了序数后缀（`"st"`，`"nd"`和`"th"`）。你的新函数应该包括根据日期值确定后缀的逻辑。此外，你需要将`date_format_test.js`中的`INPUT_LENGTH`常量修改为一个更大的值，因为此格式中日期字符串的最大可能长度超过了当前值12。此外，需要将字母`"t"`和`"h"`添加到`INPUT_VOCAB`中，因为它们不出现在任何三个字母月份字符串中。
- en: Now consider a format with the full English name of the month spelled out, such
    as “March 8th, 2012.” What is the maximum possible length of the input date string?
    How should you change `INPUT_VOCAB` in date_format.js accordingly?
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在考虑一个使用完整的英文月份名称拼写的格式，比如“2012年3月8日”。输入日期字符串的最大可能长度是多少？你应该如何相应地更改`date_format.js`中的`INPUT_VOCAB`？
- en: Summary
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: By virtue of being able to extract and learn information contained in the sequential
    order of things, RNNs can outperform feedforward models (for example, MLPs) in
    tasks that involve sequential input data. We see this through the example of applying
    simpleRNN and GRU to the temperature-prediction problem.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于能够提取和学习事物的顺序信息，循环神经网络（RNN）可以在涉及顺序输入数据的任务中胜过前馈模型（例如MLP）。我们通过将simpleRNN和GRU应用于温度预测问题的示例来看到这一点。
- en: 'There are three types of RNNs available from TensorFlow.js: simpleRNN, GRU,
    and LSTM. The latter two types are more sophisticated than simpleRNN in that they
    use a more complex internal structure to make it possible to carry memory state
    over many time steps, which mitigates the vanishing-gradient problem. GRU is computationally
    less intensive than LSTM. In most practical problems, you’ll probably want to
    use GRU and LSTM.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow.js提供了三种类型的RNN：simpleRNN、GRU和LSTM。后两种类型比simpleRNN更复杂，因为它们使用更复杂的内部结构来使得能够在许多时间步骤中保持内存状态，从而缓解了梯度消失问题。GRU的计算量比LSTM小。在大多数实际问题中，您可能希望使用GRU和LSTM。
- en: When building neural networks for text, the text inputs need to be represented
    as vectors of numbers first. This is called text vectorization. Most frequently
    used methods of text vectorization include one-hot and multi-hot encoding, as
    well as the more powerful embedding method.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建文本的神经网络时，文本输入首先需要表示为数字向量。这称为文本向量化。文本向量化的最常用方法包括one-hot和multi-hot编码，以及更强大的嵌入方法。
- en: In word embedding, each word is represented as a nonsparse vector, of which
    the element values are learned through backpropagation, just like all other weight
    parameters of the neural network. The function in TensorFlow.js that performs
    embedding is `tf.layers.embedding()`.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在词嵌入中，每个单词被表示为一个稀疏向量，其中元素值通过反向传播学习，就像神经网络的所有其他权重参数一样。在TensorFlow.js中执行嵌入的函数是`tf.layers.embedding()`。
- en: seq2seq problems are different from sequence-based regression and classification
    problems in that they involve generating a new sequence as the output. RNNs can
    be used (together with other layer types) to form an encoder-decoder architecture
    to solve seq2seq problems.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: seq2seq问题与基于序列的回归和分类问题不同，因为它们涉及生成一个新序列作为输出。循环神经网络（RNN）可以与其他类型的层一起用于形成编码器-解码器架构来解决seq2seq问题。
- en: In seq2seq problems, the attention mechanism enables different items of the
    output sequence to selectively depend on specific elements of the input sequence.
    We demonstrate how to train an attention-based encoder-decoder network to solve
    a simple date-conversion problem and visualize the attention matrix during inference.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在seq2seq问题中，注意机制使得输出序列的不同项能够选择性地依赖于输入序列的特定元素。我们演示了如何训练基于注意力的编码器-解码器网络来解决简单的日期转换问题，并在推断过程中可视化注意力矩阵。
