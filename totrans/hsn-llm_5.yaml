- en: Chapter 6\. Tokens & Token Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章\. 令牌与令牌嵌入
- en: Embeddings are a central concept to using large language models (LLMs), as you’ve
    seen over and over in part one of the book. They also are central to understanding
    how LLMs work, how they’re built, and where they’ll go in the future.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是使用大型语言模型（LLMs）的核心概念，正如你在本书第一部分中反复看到的那样。它们对理解LLMs的工作原理、构建方式以及未来发展方向至关重要。
- en: The majority of the embeddings we’ve looked at so far are *text embeddings*,
    vectors that represent an entire sentence, passage, or document. [Figure 6-1](#fig_1__the_difference_between_text_embeddings_one_vect)
    shows this distinction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所查看的大多数嵌入都是*文本嵌入*，这些向量表示整个句子、段落或文档。[图6-1](#fig_1__the_difference_between_text_embeddings_one_vect)展示了这一区别。
- en: '![  The difference between text embeddings  one vector for a sentence or paragraph  and
    token embeddings  one vector per word or token .](assets/tokens_token_embeddings_963889_01.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![文本嵌入（一个向量表示一个句子或段落）和令牌嵌入（每个单词或令牌一个向量）之间的区别。](assets/tokens_token_embeddings_963889_01.png)'
- en: Figure 6-1\. The difference between text embeddings (one vector for a sentence
    or paragraph) and token embeddings (one vector per word or token).
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 文本嵌入（一个向量表示一个句子或段落）和令牌嵌入（每个单词或令牌一个向量）之间的区别。
- en: In this chapter, we begin to discuss token embeddings in more detail. Chapter
    2 discussed tasks of token classification like Named Entity Recognition. In this
    chapter, we look more closely at what tokens are and the tokenization methods
    used to power LLMs. We will then go beyond the world of text and see how these
    concepts of token embeddings empower LLMs that can understand images and data
    modes (other than text, for example video, audio...etc). LLMs that can process
    modes of data in addition to text are called *multi-modal* models. We will then
    delve into the famous word2vec embedding method that preceded modern-day LLMs
    and see how it’s extending the concept of token embeddings to build commercial
    recommendation systems that power a lot of the apps you use.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始更详细地讨论令牌嵌入。第二章讨论了如命名实体识别的令牌分类任务。在本章中，我们将更仔细地研究什么是令牌以及用于支持LLMs的令牌化方法。随后我们将超越文本的世界，看看这些令牌嵌入的概念如何使LLMs能够理解图像和其他数据模式（例如视频、音频等）。能够处理除文本之外的数据模式的LLMs被称为*多模态*模型。然后我们将深入探讨著名的word2vec嵌入方法，它是现代LLMs的前身，并看看它如何扩展令牌嵌入的概念，以构建推动许多应用的商业推荐系统。
- en: LLM Tokenization
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM令牌化
- en: How tokenizers prepare the inputs to the language model
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 令牌化器如何准备语言模型的输入
- en: Viewed from the outside, generative LLMs take an input prompt and generate a
    response, as we can see in [Figure 6-2](#fig_2__high_level_view_of_a_language_model_and_its_inpu).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从外部看，生成式大型语言模型（LLMs）接受输入提示并生成响应，正如我们在[图6-2](#fig_2__high_level_view_of_a_language_model_and_its_inpu)中所见。
- en: '![  High level view of a language model and its input prompt.](assets/tokens_token_embeddings_963889_02.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![语言模型及其输入提示的高级视图。](assets/tokens_token_embeddings_963889_02.png)'
- en: Figure 6-2\. High-level view of a language model and its input prompt.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 语言模型及其输入提示的高级视图。
- en: 'As we’ve seen in Chapter 5, instruction-tuned LLMs produce better responses
    to prompts formulated as instructions or questions. At the most basic level of
    the code, let’s assume we have a generate method that hits a language model and
    generates text:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第五章中看到的，经过指令调优的LLMs对作为指令或问题表述的提示产生更好的响应。在代码的最基本层面上，假设我们有一个调用语言模型并生成文本的generate方法：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Generation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生成：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let us look closer into that generation process to examine more of the steps
    involved in text generation. Let’s start by loading our model and its tokenizer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地审视这一生成过程，以检查文本生成中涉及的更多步骤。让我们从加载模型及其令牌化器开始。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can then proceed to the actual generation. Notice that the generation code
    always includes a tokenization step prior to the generation step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以进入实际生成。请注意，生成代码总是包括一个令牌化步骤，位于生成步骤之前。
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Looking at this code, we can see that the model does not in fact receive the
    text prompt. Instead, the tokenizers processed the input prompt, and returned
    the information the model needed in the variable input_ids, which the model used
    as its input.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从这段代码来看，我们可以看到模型实际上并没有接收到文本提示。相反，令牌化器处理了输入提示，并返回模型所需的信息，存储在变量input_ids中，模型将其用作输入。
- en: 'Let’s print input_ids to see what it holds inside:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印input_ids以查看其内部内容：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This reveals the inputs that LLMs respond to. A series of integers as shown
    in [Figure 6-3](#fig_3__a_tokenizer_processes_the_input_prompt_and_prepa). Each
    one is the unique ID for a specific token (character, word or part of word). These
    IDs reference a table inside the tokenizer containing all the tokens it knows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这揭示了 LLMs 响应的输入。正如[图 6-3](#fig_3__a_tokenizer_processes_the_input_prompt_and_prepa)中所示，一系列整数。每个都是特定令牌（字符、单词或单词的一部分）的唯一
    ID。这些 ID 参考分词器内部的一个表，包含它所知道的所有令牌。
- en: '![  A tokenizer processes the input prompt and prepares the actual input into
    the language model  a list of token ids.](assets/tokens_token_embeddings_963889_03.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![分词器处理输入提示并准备实际输入到语言模型中的令牌 ID 列表。](assets/tokens_token_embeddings_963889_03.png)'
- en: 'Figure 6-3\. A tokenizer processes the input prompt and prepares the actual
    input into the language model: a list of token ids.'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 分词器处理输入提示并准备实际输入到语言模型中：一个令牌 ID 列表。
- en: 'If we want to inspect those IDs, we can use the tokenizer’s decode method to
    translate the IDs back into text that we can read:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想检查这些 ID，可以使用分词器的解码方法将 ID 转换回可读文本：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Which prints:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is how the tokenizer broke down our input prompt. Notice the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是分词器如何拆解我们的输入提示。注意以下几点：
- en: 'The first token is the token with ID #1, which is <s>, a special token indicating
    the beginning of the text'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '第一个令牌是 ID 为 #1 的令牌，它是 <s>，一个特殊令牌，表示文本的开始。'
- en: Some tokens are complete words (e.g., *Write*, *an*, *email*)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些令牌是完整的单词（例如，*写*，*一个*，*电子邮件*）。
- en: Some tokens are parts of words (e.g., *apolog*, *izing*, *trag*, *ic*)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些令牌是单词的一部分（例如，*道歉*，*izing*，*悲惨*，*ic*）。
- en: Punctuation characters are their own token
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号字符是它们自己的令牌。
- en: Notice how the space character does not have its own token. Instead, partial
    tokens (like ‘izing’ and ‘ic') have a special hidden character at their beginning
    that indicate that they’re connected with the token that precedes them in the
    text.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意空格字符没有自己的令牌。相反，部分令牌（如 ‘izing’ 和 ‘ic’）在开头有一个特殊的隐藏字符，指示它们与文本中前面的令牌相连。
- en: There are three major factors that dictate how a tokenizer breaks down an input
    prompt. First, at model design time, the creator of the model chooses a tokenization
    method. Popular methods include Byte-Pair Encoding (BPE for short, widely used
    by GPT models), WordPiece (used by BERT), and SentencePiece (used by LLAMA). These
    methods are similar in that they aim to optimize an efficient set of tokens to
    represent a text dataset, but they arrive at it in different ways.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个主要因素决定了分词器如何拆解输入提示。首先，在模型设计时，模型的创建者选择了一种分词方法。常用的方法包括字节对编码（BPE，广泛应用于 GPT 模型）、WordPiece（用于
    BERT）和 SentencePiece（用于 LLAMA）。这些方法的共同点在于，它们旨在优化一组有效的令牌来表示文本数据集，但它们的实现方式各不相同。
- en: Second, after choosing the method, we need to make a number of tokenizer design
    choices like vocabulary size, and what special tokens to use. More on this in
    the “Comparing Trained LLM Tokenizers” section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，选择方法后，我们需要进行一系列分词器设计选择，如词汇大小和使用哪些特殊令牌。关于这一点将在“比较训练后的 LLM 分词器”部分中详细介绍。
- en: Thirdly, the tokenizer needs to be trained on a specific dataset to establish
    the best vocabulary it can use to represent that dataset. Even if we set the same
    methods and parameters, a tokenizer trained on an English text dataset will be
    different from another trained on a code dataset or a multilingual text dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，分词器需要在特定数据集上进行训练，以建立最佳词汇表来表示该数据集。即使我们设置相同的方法和参数，在英语文本数据集上训练的分词器与在代码数据集或多语言文本数据集上训练的分词器也会有所不同。
- en: In addition to being used to process the input text into a language model, tokenizers
    are used on the output of the language model to turn the resulting token ID into
    the output word or token associated with it as [Figure 6-4](#fig_4__tokenizers_are_also_used_to_process_the_output_o)
    shows.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于将输入文本处理成语言模型外，分词器还用于语言模型的输出，将生成的令牌 ID 转换为与之关联的输出单词或令牌，如[图 6-4](#fig_4__tokenizers_are_also_used_to_process_the_output_o)所示。
- en: '![  Tokenizers are also used to process the output of the model by converting
    the output token ID into the word or token associated with that ID.](assets/tokens_token_embeddings_963889_04.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![分词器也用于处理模型的输出，通过将输出的令牌 ID 转换为与该 ID 关联的单词或令牌。](assets/tokens_token_embeddings_963889_04.png)'
- en: Figure 6-4\. Tokenizers are also used to process the output of the model by
    converting the output token ID into the word or token associated with that ID.
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 分词器也用于处理模型的输出，通过将输出的令牌 ID 转换为与该 ID 关联的单词或令牌。
- en: Word vs. Subword vs. Character vs. Byte Tokens
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词与子词、字符和字节令牌
- en: 'The tokenization scheme we’ve seen above is called subword tokenization. It’s
    the most commonly used tokenization scheme but not the only one. The four notable
    ways to tokenize are shown in [Figure 6-5](#fig_5__there_are_multiple_methods_of_tokenization_that).
    Let’s go over them:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的分词方案称为子词分词。它是最常用的分词方案，但不是唯一的。四种显著的分词方式如[图6-5](#fig_5__there_are_multiple_methods_of_tokenization_that)所示。让我们逐一了解它们：
- en: Word tokens
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 单词令牌
- en: This approach was common with earlier methods like Word2Vec but is being used
    less and less in NLP. Its usefulness, however, led it to be used outside of NLP
    for use cases such as recommendation systems, as we’ll see later in the chapter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在早期的Word2Vec等方法中很常见，但在NLP中的使用越来越少。然而，它的有效性使其被用于NLP以外的用例，例如推荐系统，正如我们将在本章后面看到的。
- en: '![  There are multiple methods of tokenization that break down the text to
    different sizes of components  words  subwords  characters  and bytes .](assets/tokens_token_embeddings_963889_05.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![有多种分词方法将文本拆分为不同大小的组件：单词、子词、字符和字节。](assets/tokens_token_embeddings_963889_05.png)'
- en: Figure 6-5\. There are multiple methods of tokenization that break down the
    text to different sizes of components (words, subwords, characters, and bytes).
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5. 有多种分词方法将文本拆分为不同大小的组件（单词、子词、字符和字节）。
- en: One challenge with word tokenization is that the tokenizer becomes unable to
    deal with new words that enter the dataset after the tokenizer was trained. It
    also results in a vocabulary that has a lot of tokens with minimal differences
    between them (e.g., apology, apologize, apologetic, apologist). This latter challenge
    is resolved by subword tokenization as we’ve seen as it has a token for '*apolog**',*
    and then suffix tokens (e.g., *'-y*', '*-**ize*', '*-etic*', '-*ist*') that are
    common with many other tokens, resulting in a more expressive vocabulary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 单词分词的一个挑战是，分词器无法处理训练后进入数据集的新单词。这也导致了一个词汇中存在许多差异微小的令牌（例如，apology, apologize,
    apologetic, apologist）。我们已经看到，这一后续挑战通过子词分词得到解决，因为它有一个令牌为'*apolog**'*，然后是许多其他令牌通用的后缀令牌（例如，*'-y*',
    '*-**ize*', '*-etic*', '-*ist*'），从而形成更具表现力的词汇。
- en: Subword Tokens
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 子词令牌
- en: This method contains full and partial words. In addition to the vocabulary expressivity
    mentioned earlier, another benefit of the approach is its ability to represent
    new words by breaking the new token down into smaller characters, which tend to
    be a part of the vocabulary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法包含完整和部分单词。除了之前提到的词汇表现力外，该方法的另一个好处是能够通过将新令牌分解为较小的字符来表示新单词，这些字符往往是词汇的一部分。
- en: When compared to character tokens, this method benefits from the ability to
    fit more text within the limited context length of a Transformer model. So with
    a model with a context length of 1024, you may be able to fit three times as much
    text using subword tokenization than using character tokens (sub word tokens often
    average three characters per token).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与字符令牌相比，该方法的好处在于能够在Transformer模型的有限上下文长度内容纳更多文本。因此，在一个上下文长度为1024的模型中，使用子词分词能够比使用字符令牌容纳三倍的文本（子词令牌平均每个令牌有三个字符）。
- en: Character Tokens
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 字符令牌
- en: This is another method that is able to deal successfully with new words because
    it has the raw letters to fall-back on. While that makes the representation easier
    to tokenize, it makes the modeling more difficult. Where a model with subword
    tokenization can represent “play” as one token, a model using character-level
    tokens needs to model the information to spell out “p-l-a-y” in addition to modeling
    the rest of the sequence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种能够成功处理新单词的方法，因为它有原始字母作为备用。虽然这使得表示更容易分词，但也使建模更具挑战性。使用子词分词的模型可以将“play”表示为一个令牌，而使用字符级令牌的模型需要建模信息以拼写出“p-l-a-y”，并建模其余序列。
- en: Byte Tokens
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 字节令牌
- en: 'One additional tokenization method breaks down tokens into the individual bytes
    that are used to represent unicode characters. Papers like [CANINE: Pre-training
    an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874)
    outline methods like this which are also called “tokenization free encoding”.
    Other works like [ByT5: Towards a token-free future with pre-trained byte-to-byte
    models](https://arxiv.org/abs/2105.13626) show that this can be a competitive
    method.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种分词方法将分词分解为用于表示 Unicode 字符的单个字节。像 [CANINE: Pre-training an Efficient Tokenization-Free
    Encoder for Language Representation](https://arxiv.org/abs/2103.06874) 这样的论文概述了这种方法，也被称为“无分词编码”。其他作品如
    [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
    显示这可以是一种具有竞争力的方法。'
- en: 'One distinction to highlight here: some subword tokenizers also include bytes
    as tokens in their vocabulary to be the final building block to fall back to when
    they encounter characters they can’t otherwise represent. The GPT2 and RoBERTa
    tokenizers do this, for example. This doesn’t make them tokenization-free byte-level
    tokenizers, because they don’t use these bytes to represent everything, only a
    subset as we’ll see in the next section.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要强调的一个区别是：一些子词分词器还将字节作为词汇中的分词，以便在遇到无法以其他方式表示的字符时回退到最终构建块。比如，GPT2 和 RoBERTa
    分词器就是这样做的。这并不意味着它们是无分词的字节级分词器，因为它们并不使用这些字节来表示所有内容，而只使用子集，正如我们将在下一节中看到的那样。
- en: Tokenizers are discussed in more detail in [Suhas’ book]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器在 [Suhas 的书中] 有更详细的讨论
- en: Comparing Trained LLM Tokenizers
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较经过训练的 LLM 分词器
- en: 'We’ve pointed out earlier three major factors that dictate the tokens that
    appear within a tokenizer: the tokenization method, the parameters and special
    tokens we use to initialize the tokenizer, and the dataset the tokenizer is trained
    on. Let’s compare and contrast a number of actual, trained tokenizers to see how
    these choices change their behavior.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到的三个主要因素决定了分词器中出现的分词：分词方法、初始化分词器所用的参数和特殊分词，以及分词器训练所用的数据集。让我们比较和对比多个实际的、经过训练的分词器，以观察这些选择如何改变它们的行为。
- en: 'We’ll use a number of tokenizers to encode the following text:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用多种分词器对以下文本进行编码：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will allow us to see how each tokenizer deals with a number of different
    kinds of tokens:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们能够看到每个分词器如何处理多种不同类型的分词：
- en: Capitalization
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大写化
- en: Languages other than English
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除英语外的其他语言
- en: Emojis
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表情符号
- en: Programming code with its keywords and whitespaces often used for indentation
    (in languages like python for example)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程代码及其关键字和空白通常用于缩进（例如在 Python 语言中）
- en: Numbers and digits
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字和数字符号
- en: Let’s go from older to newer tokenizers and see how they tokenize this text
    and what that might say about the language model. We’ll tokenize the text, and
    then print each token with a gray background color.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从较旧的到较新的分词器，看看它们如何对这段文本进行分词，以及这可能对语言模型意味着什么。我们将对文本进行分词，然后打印每个分词，背景颜色为灰色。
- en: bert-base-uncased
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bert-base-uncased
- en: 'Tokenization method: WordPiece, introduced in [Japanese and Korean voice search](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分词方法：WordPiece，介绍于 [日本和韩国语音搜索](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
- en: 'Vocabulary size: 30522'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇大小：30522
- en: 'Special tokens: ‘unk_token’: ''[UNK]'''
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '特殊分词：‘unk_token’: ''[UNK]'''
- en: '’sep_token’: ''[SEP]'''
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '’sep_token’: ''[SEP]'''
- en: '‘pad_token’: ''[PAD]'''
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '‘pad_token’: ''[PAD]'''
- en: '‘cls_token’: ''[CLS]'''
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '‘cls_token’: ''[CLS]'''
- en: '‘mask_token’: ''[MASK]'''
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '‘mask_token’: ''[MASK]'''
- en: 'Tokenized text:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分词文本：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the uncased (and more popular) version of the BERT tokenizer, we notice
    the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无大小写（更流行）的 BERT 分词器版本，我们注意到以下几点：
- en: The newline breaks are gone, which makes the model blind to information encoded
    in newlines (e.g., a chat log when each turn is in a new line)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 换行符被去掉，这使得模型无法识别编码在换行中的信息（例如，每次轮换在新行中的聊天记录）
- en: All the text is in lower case
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有文本均为小写字母
- en: 'The word “capitalization” is encoded as two subtokens capital ##ization . The
    ## characters are used to indicate this token is a partial token connected to
    the token the precedes it. This is also a method to indicate where the spaces
    are, it is assumed tokens without ## before them have a space before them.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“capitalization”这个词被编码为两个子分词 capital ##ization。## 字符用于指示该分词是与前一个分词相连的部分分词。这也是指示空格位置的一种方法，假定没有
    ## 的分词前有一个空格。'
- en: The emoji and Chinese characters are gone and replaced with the [UNK] special
    token indicating an “unknown token”.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表情符号和中文字符被替换为 [UNK] 特殊分词，表示“未知分词”。
- en: bert-base-cased
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bert-base-cased
- en: 'Tokenization method: WordPiece'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化方法：WordPiece
- en: 'Vocabulary size: 28,996'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表大小：28,996
- en: 'Special tokens: Same as the uncased version'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊标记：与无大小写版本相同
- en: 'Tokenized text:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化文本：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The cased version of the BERT tokenizer differs mainly in including upper-case
    tokens.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BERT标记器的大小写版本主要不同于包括大写标记。
- en: 'Notice how “CAPITALIZATION” is now represented as eight tokens: CA ##PI ##TA
    ##L ##I ##Z ##AT ##ION'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '注意“CAPITALIZATION”现在表示为八个标记：CA ##PI ##TA ##L ##I ##Z ##AT ##ION'
- en: Both BERT tokenizers wrap the input within a starting [CLS] token and a closing
    [SEP] token. [CLS] and [SEP] are utility tokens used to wrap the input text and
    they serve their own purposes. [CLS] stands for Classification as it’s a token
    used at times for sentence classification. [SEP] stands for Separator, as it’s
    used to separate sentences in some applications that require passing two sentences
    to a model (For example, in the rerankers in chapter 3, we would use a [SEP] token
    to separate the text of the query and a candidate result).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个BERT标记器在输入周围包裹一个起始的[CLS]标记和一个结束的[SEP]标记。[CLS]代表分类，因为它是有时用于句子分类的标记。[SEP]代表分隔符，因为它用于在一些需要将两个句子传递给模型的应用中分隔句子（例如，在第三章的重新排序器中，我们会使用[SEP]标记来分隔查询文本和候选结果）。
- en: gpt2
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gpt2
- en: 'Tokenization method: BPE, introduced in [Neural Machine Translation of Rare
    Words with Subword Units](https://arxiv.org/abs/1508.07909)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化方法：BPE，介绍于[神经机器翻译中的稀有词的子词单元](https://arxiv.org/abs/1508.07909)
- en: 'Vocabulary size: 50,257'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表大小：50,257
- en: 'Special tokens: <|endoftext|>'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊标记：<|endoftext|>
- en: 'Tokenized text:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化文本：
- en: English and CAP ITAL IZ ATION
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 英语和CAP ITAL IZ ATION
- en: � � � � � �
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: � � � � � �
- en: 'show _ t ok ens False None el if == >= else :'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'show _ t ok ens False None el if == >= else :'
- en: 'Four spaces : " " Two tabs : " "'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 四个空格： " " 两个制表符： " "
- en: 12 . 0 * 50 = 600
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 12 . 0 * 50 = 600
- en: 'With the GPT-2 tokenizer, we notice the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-2标记器，我们注意到以下内容：
- en: The newline breaks are represented in the tokenizer
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 换行符在标记器中表示
- en: Capitalization is preserved, and the word “CAPITALIZATION” is represented in
    four tokens
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 大写字母保持不变，单词“CAPITALIZATION”由四个标记表示。
- en: 'The 🎵 蟠characters are now represented into multiple tokens each. While we see
    these tokens printed as the � character, they actually stand for different tokens.
    For example, the 🎵 emoji is broken down into the tokens with token ids: 8582,
    236, and 113\. The tokenizer is successful in reconstructing the original character
    from these tokens. We can see that by printing tokenizer.decode([8582, 236, 113]),
    which prints out 🎵'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 🎵 蟠字符现在被表示为多个标记。尽管我们看到这些标记以�字符打印，但它们实际上代表不同的标记。例如，🎵 emoji被分解为标记，标记ID为：8582，236和113。标记器成功地从这些标记重构了原始字符。我们可以通过打印tokenizer.decode([8582,
    236, 113])来看到这一点，它打印出🎵
- en: The two tabs are represented as two tokens (token number 197 in that vocabulary)
    and the four spaces are represented as three tokens (number 220) with the final
    space being a part of the token for the closing quote character.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 两个制表符被表示为两个标记（该词汇表中的标记编号197），四个空格被表示为三个标记（编号220），最后一个空格是关闭引号字符的标记的一部分。
- en: Note
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: What is the significance of white space characters? These are important for
    models that understand or generate code. A model that uses a single token to represent
    four consecutive white space characters can be said to be more tuned to a python
    code dataset. While a model can live with representing it as four different tokens,
    it does make the modeling more difficult as the model needs to keep track of the
    indentation level. This is an example of where tokenization choices can help the
    model improve on a certain task.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 空白字符的意义是什么？这些对于理解或生成代码的模型很重要。使用单个标记表示四个连续空白字符的模型，可以说更适合Python代码数据集。尽管模型可以用四个不同的标记表示，但这确实使建模更加困难，因为模型需要跟踪缩进级别。这是标记化选择可以帮助模型在特定任务上改进的一个例子。
- en: google/flan-t5-xxl
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: google/flan-t5-xxl
- en: 'Tokenization method: SentencePiece, introduced in [SentencePiece: A simple
    and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226.pdf)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化方法：SentencePiece，介绍于[SentencePiece：一种简单的与语言无关的子词标记器和解码器，用于神经文本处理](https://arxiv.org/pdf/1808.06226.pdf)
- en: 'Vocabulary size: 32,100'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表大小：32,100
- en: 'Special tokens:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊标记：
- en: '- ‘unk_token’: ''<unk>'''
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '- ‘unk_token’: ''<unk>'''
- en: '- ‘pad_token’: ''<pad>'''
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '- ‘pad_token’: ''<pad>'''
- en: 'Tokenized text:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化文本：
- en: 'English and CA PI TAL IZ ATION <unk> <unk> show _ to ken s Fal s e None e l
    if = = > = else : Four spaces : " " Two tab s : " " 12\. 0 * 50 = 600 </s>'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '英语和 CA PI TAL IZ ATION <unk> <unk> 显示 _ to ken s Fal s e None e l if = = >
    = else : 四个空格 : " " 两个制表符 : " " 12\. 0 * 50 = 600 </s>'
- en: 'The FLAN-T5 family of models use the sentencepiece method. We notice the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN-T5模型家族使用sentencepiece方法。我们注意到以下几点：
- en: No newline or whitespace tokens, this would make it challenging for the model
    to work with code.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有换行或空白标记，这会使模型处理代码变得困难。
- en: The emoji and Chinese characters are both replaced by the <unk> token. Making
    the model completely blind to them.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表情符号和汉字都被替换为<unk>标记，使模型对此完全无感。
- en: GPT-4
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4
- en: 'Tokenization method: BPE'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 分词方法：BPE
- en: 'Vocabulary size: a little over 100,000'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表大小：略超过100,000
- en: 'Special tokens:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊标记：
- en: <|endoftext|>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <|endoftext|>
- en: 'Fill in the middle tokens. These three tokens enable the GPT-4 capability of
    generating a completion given not only the text before it but also considering
    the text after it. This method is explained in more detail in the paper [Efficient
    Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255).
    These special tokens are:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 填充中间标记。这三个标记使得GPT-4能够生成补全，不仅考虑之前的文本，还考虑之后的文本。该方法在论文[高效训练语言模型以填充中间部分](https://arxiv.org/abs/2207.14255)中有更详细的解释。这些特殊标记为：
- en: <|fim_prefix|>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <|fim_prefix|>
- en: <|fim_middle|>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <|fim_middle|>
- en: <|fim_suffix|>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <|fim_suffix|>
- en: 'Tokenized text:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 分词文本：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The GPT-4 tokenizer behaves similarly with its ancestor, the GPT-2 tokenizer.
    Some differences are:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的分词器行为与其前身GPT-2的分词器相似。有些差异是：
- en: The GPT-4 tokenizer represents the four spaces as a single token. In fact, it
    has a specific token to every sequence of white spaces up until a list of 83 white
    spaces.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4的分词器将四个空格表示为一个标记。实际上，它对每个空白序列都有特定的标记，直到一列83个空格。
- en: The python keyword elif has its own token in GPT-4\. Both this and the previous
    point stem from the model’s focus on code in addition to natural language.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: python关键字elif在GPT-4中有自己的标记。上述这一点和前一点均源于模型对代码的关注，除了自然语言。
- en: The GPT-4 tokenizer uses fewer tokens to represent most words. Example here
    include ‘CAPITALIZATION’ (two tokens, vs. four) and ‘tokens’ (one token vs. three).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4的分词器使用更少的标记来表示大多数单词。这里的例子包括‘CAPITALIZATION’（两个标记，而不是四个）和‘tokens’（一个标记而不是三个）。
- en: bigcode/starcoder
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: bigcode/starcoder
- en: 'Tokenization method:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 分词方法：
- en: 'Vocabulary size: about 50,000'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表大小：约50,000
- en: 'Special tokens:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊标记：
- en: '''<|endoftext|>'''
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '''<|endoftext|>'''
- en: 'FIll in the middle tokens:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 填充中间标记：
- en: '''<fim_prefix>'''
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_prefix>'''
- en: '''<fim_middle>'''
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_middle>'''
- en: '''<fim_suffix>'''
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_suffix>'''
- en: '''<fim_pad>'''
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '''<fim_pad>'''
- en: 'When representing code, managing the context is important. One file might make
    a function call to a function that is defined in a different file. So the model
    needs some way of being able to identify code that is in different files in the
    same code repository, while making a distinction between code in different repos.
    That’s why starcoder uses special tokens for the name of the repository and the
    filename:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在表示代码时，管理上下文很重要。一个文件可能会调用在另一个文件中定义的函数。因此，模型需要某种方式来识别在同一代码库中不同文件中的代码，同时区分不同代码库中的代码。这就是starcoder为库名称和文件名使用特殊标记的原因：
- en: '''<filename>'''
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '''<filename>'''
- en: '''<reponame>'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '''<reponame>'''
- en: '''<gh_stars>'''
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '''<gh_stars>'''
- en: 'The tokenizer also includes a bunch of the special tokens to perform better
    on code. These include:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该分词器还包含一系列特殊标记，以便在代码上表现得更好。这些包括：
- en: '''<issue_start>'''
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '''<issue_start>'''
- en: '''<jupyter_start>'''
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '''<jupyter_start>'''
- en: '''<jupyter_text>'''
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '''<jupyter_text>'''
- en: 'Paper: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 论文：[StarCoder：愿源代码与你同在！](https://arxiv.org/abs/2305.06161)
- en: 'Tokenized text:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 分词文本：
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is an encoder that focuses on code generation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个专注于代码生成的编码器。
- en: Similarly to GPT-4, it encodes the list of white spaces as a single token
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与GPT-4类似，它将空格列表编码为一个单一标记。
- en: A major difference here to everyone we’ve seen so far is that each digit is
    assigned its own token (so 600 becomes 6 0 0). The hypothesis here is that this
    would lead to better representation of numbers and mathematics. In GPT-2, for
    example, the number 870 is represented as a single token. But 871 is represented
    as two tokens (8 and 71). You can intuitively see how that might be confusing
    to the model and how it represents numbers.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里一个主要的不同之处是每个数字都被分配了自己的标记（因此600变为6 0 0）。假设这是为了更好地表示数字和数学。在GPT-2中，例如，数字870表示为一个标记。但871表示为两个标记（8和71）。你可以直观地理解，这可能会让模型在表示数字时感到困惑。
- en: facebook/galactica-1.3b
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: facebook/galactica-1.3b
- en: 'The galactica model described in [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
    is focused on scientific knowledge and is trained on many scientific papers, reference
    materials, and knowledge bases. It pays extra attention to tokenization that makes
    it more sensitive to the nuances of the dataset it’s representing. For example,
    it includes special tokens for citations, reasoning, mathematics, Amino Acid sequences,
    and DNA sequences.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '在[Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)中描述的
    Galactica 模型专注于科学知识，并在许多科学论文、参考材料和知识库上进行了训练。它特别关注分词，使其对所代表的数据集的细微差别更加敏感。例如，它包含用于引用、推理、数学、氨基酸序列和
    DNA 序列的特殊标记。'
- en: 'Tokenization method:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 分词方法：
- en: 'Vocabulary size: 50,000'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表大小：50,000
- en: 'Special tokens:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊标记：
- en: <s>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <s>
- en: <pad>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <pad>
- en: </s>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: </s>
- en: <unk>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <unk>
- en: 'References: Citations are wrapped within the two special tokens:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：引用被包裹在两个特殊标记内：
- en: '[START_REF]'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[开始引用]'
- en: '[END_REF]'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[结束引用]'
- en: 'One example of usage from the paper is:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的一个使用示例是：
- en: Recurrent neural networks, long short-term memory [START_REF]Long Short-Term
    Memory, Hochreiter[END_REF]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络，长短期记忆[开始引用]长短期记忆，霍希特[结束引用]
- en: Step-by-Step Reasoning -
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤推理 -
- en: <work> is an interesting token that the model uses for chain-of-thought reasoning.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <work> 是模型用于连锁推理的一个有趣标记。
- en: 'Tokenized text:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化文本：
- en: '[PRE12]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Galactica tokenizer behaves similar to star coder in that it has code in
    mind. It also encodes white spaces in the same way - assigning a single token
    to sequences of whitespace of different lengths. It differs in that it also does
    that for tabs, though. So from all the tokenizers we’ve seen so far, it’s the
    only one that’s assigned a single token to the string made up of two tabs ('\t\t')
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Galactica 标记器的行为类似于 Star Coder，因为它考虑了代码。它也以相同的方式编码空白 - 将不同长度的空白序列分配给一个单一的标记。它的不同之处在于它也会对制表符这样处理。因此，在我们迄今为止看到的所有标记器中，它是唯一一个将由两个制表符（'\t\t'）组成的字符串分配给单一标记的标记器。
- en: 'We can now recap our tour by looking at all these examples side by side:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过并排查看所有这些示例来回顾我们的导览：
- en: '| bert-base-uncased |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| bert-base-uncased |'
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| bert-base-cased |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| bert-base-cased |'
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| gpt2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| gpt2 |'
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| google/flan-t5-xxl |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| google/flan-t5-xxl |'
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| GPT-4 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 |'
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| bigcode/starcoder |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| bigcode/starcoder |'
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| facebook/galactica-1.3b |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| facebook/galactica-1.3b |'
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| meta-llama/Llama-2-70b-chat-hf |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| meta-llama/Llama-2-70b-chat-hf |'
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Notice how there’s a new tokenizer added in the bottom. By now, you should be
    able to understand many of its properties by just glancing at this output. This
    is the tokenizer for LLaMA2, the most recent of these models.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，底部添加了一个新的标记器。到现在为止，你应该能够通过快速浏览这个输出理解它的许多属性。这是 LLaMA2 的标记器，这些模型中最新的一个。
- en: Tokenizer Properties
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记器属性
- en: 'The preceding guided tour of trained tokenizers showed a number of ways in
    which actual tokenizers differ from each other. But what determines their tokenization
    behavior? There are three major groups of design choices that determine how the
    tokenizer will break down text: The tokenization method, the initialization parameters,
    and the dataset we train the tokenizer (but not the model) on.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的训练标记器导览展示了实际标记器之间的多种差异。但是什么决定了它们的分词行为呢？有三个主要的设计选择决定了标记器如何拆分文本：分词方法、初始化参数，以及我们训练标记器（但不是模型）所用的数据集。
- en: Tokenization methods
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词方法
- en: As we’ve seen, there are a number of tokenization methods with Byte-Pair Encoding
    (BPE), WordPiece, and SentencePiece being some of the more popular ones. Each
    of these methods outlines an algorithm for how to choose an appropriate set of
    tokens to represent a dataset. A great overview of all these methods can be found
    in the Hugging Face [Summary of the tokenizers page](https://huggingface.co/docs/transformers/tokenizer_summary).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，有多种分词方法，其中 Byte-Pair Encoding (BPE)、WordPiece 和 SentencePiece 是较为流行的几种。每种方法都概述了一种算法，用于选择适当的标记集来表示数据集。有关所有这些方法的优秀概述可以在
    Hugging Face 的[分词器页面摘要](https://huggingface.co/docs/transformers/tokenizer_summary)中找到。
- en: Tokenizer Parameters
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记器参数
- en: 'After choosing a tokenization method, an LLM designer needs to make some decisions
    about the parameters of the tokenizer. These include:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 选择分词方法后，LLM 设计师需要对标记器的参数做出一些决策。这些包括：
- en: Vocabulary size
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表大小
- en: How many tokens to keep in the tokenizer’s vocabulary? (30K, 50K are often used
    vocabulary size values, but more and more we’re seeing larger sizes like 100K)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词器的词汇中应保留多少个令牌？（常用的词汇大小值为30K、50K，但越来越多的情况是看到像100K这样更大的大小）
- en: Special tokens
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊令牌
- en: 'What special tokens do we want the model to keep track of. We can add as many
    of these as we want, especially if we want to build LLM for special use cases.
    Common choices include:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望模型跟踪哪些特殊令牌。我们可以根据需要添加任意数量，特别是如果我们想为特定用例构建LLM。常见的选择包括：
- en: Beginning of text token (e.g., <s>)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本开始令牌（例如，<s>）
- en: End of text token
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本结束令牌
- en: Padding token
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充令牌
- en: Unknown token
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未知令牌
- en: CLS token
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLS令牌
- en: Masking token
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩码令牌
- en: Aside from these, the LLM designer can add tokens that help better model the
    domain of the problem they’re trying to focus on, as we’ve seen with Galactica’s
    <work> and [START_REF] tokens.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，LLM设计者可以添加有助于更好地建模他们试图关注的问题领域的令牌，正如我们在Galactica的<work>和[START_REF]令牌中看到的那样。
- en: Capitalization
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 大写
- en: In languages such as English, how do we want to deal with capitalization? Should
    we convert everything to lower-case? (Name capitalization often carries useful
    information, but do we want to waste token vocabulary space on all caps versions
    of words?). This is why some models are released in both cased and uncased versions
    (like [Bert-base cased](https://huggingface.co/bert-base-cased) and the more popular
    [Bert-base uncased](https://huggingface.co/bert-base-uncased)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在像英语这样的语言中，我们希望如何处理大写？我们是否应该将所有内容转换为小写？（名称的大写通常携带有用的信息，但我们是否想在所有大写版本的单词上浪费令牌词汇空间？）这就是为什么一些模型以大写和小写版本发布（如[Bert-base
    cased](https://huggingface.co/bert-base-cased)和更流行的[Bert-base uncased](https://huggingface.co/bert-base-uncased)）。
- en: The Tokenizer Training Dataset
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词器训练数据集
- en: Even if we select the same method and parameters, tokenizer behavior will be
    different based on the dataset it was trained on (before we even start model training).
    The tokenization methods mentioned previously work by optimizing the vocabulary
    to represent a specific dataset. From our guided tour we’ve seen how that has
    an impact on datasets like code, and multilingual text.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们选择相同的方法和参数，分词器的行为也会因其训练的数据集而异（在我们甚至开始模型训练之前）。前面提到的分词方法通过优化词汇来代表特定的数据集。从我们的导览中，我们看到这对代码和多语言文本等数据集产生了影响。
- en: 'For code, for example, we’ve seen that a text-focused tokenizer may tokenize
    the indentation spaces like this (We’ll highlight some tokens in yellow and green):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于代码，我们看到一个以文本为中心的分词器可能会像这样分词缩进空格（我们将一些令牌用黄色和绿色突出显示）：
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Which may be suboptimal for a code-focused model. Code-focused models instead
    tend to make different tokenization choices:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以代码为中心的模型，这可能并不理想。以代码为中心的模型往往会做出不同的分词选择：
- en: '[PRE22]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These tokenization choices make the model’s job easier and thus its performance
    has a higher probability of improving.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分词选择使模型的工作变得更轻松，因此其性能更有可能提高。
- en: A more detailed tutorial on training tokenizers can be found in the [Tokenizers
    section of the Hugging Face course](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt).
    and in [Natural Language Processing with Transformers, Revised Edition](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有关训练分词器的更详细教程可以在[Hugging Face课程的分词器部分](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt)和[《使用变换器的自然语言处理，修订版》](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)中找到。
- en: A Language Model Holds Embeddings for the Vocabulary of its Tokenizer
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型为其分词器的词汇持有嵌入
- en: After a tokenizer is initialized, it is then used in the training process of
    its associated language model. This is why a pre-trained language model is linked
    with its tokenizer and can’t use a different tokenizer without training.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化一个分词器后，它将在其相关语言模型的训练过程中使用。这就是为什么预训练的语言模型与其分词器相连，并且在未经过训练的情况下不能使用不同的分词器。
- en: The language model holds an embedding vector for each token in the tokenizer’s
    vocabulary as we can see in [Figure 6-6](#fig_6__a_language_model_holds_an_embedding_vector_assoc).
    In the beginning, these vectors are randomly initialized like the rest of the
    model’s weights, but the training process assigns them the values that enable
    the useful behavior they’re trained to perform.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型为分词器词汇中的每个令牌持有一个嵌入向量，正如我们在[图 6-6](#fig_6__a_language_model_holds_an_embedding_vector_assoc)中看到的。一开始，这些向量像模型的其他权重一样随机初始化，但训练过程会赋予它们能够执行有用行为的值。
- en: '![  A language model holds an embedding vector associated with each token in
    its tokenizer.](assets/tokens_token_embeddings_963889_06.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![一个语言模型持有与其分词器中每个标记相关的嵌入向量。](assets/tokens_token_embeddings_963889_06.png)'
- en: Figure 6-6\. A language model holds an embedding vector associated with each
    token in its tokenizer.
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. 语言模型持有与其分词器中每个标记相关的嵌入向量。
- en: Creating Contextualized Word Embeddings with Language Models
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用语言模型创建上下文化单词嵌入。
- en: Now that we’ve covered token embeddings as the input to a language model, let’s
    look at how language models can *create* better token embeddings. This is one
    of the main ways of using language models for text representation that empowers
    applications like named-entity recognition or extractive text summarization (which
    summarizes a long text by highlighting to most important parts of it, instead
    of generating new text as a summary).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了作为语言模型输入的标记嵌入，让我们看看语言模型如何*创建*更好的标记嵌入。这是使用语言模型进行文本表示的主要方式之一，赋能应用程序如命名实体识别或提取式文本摘要（通过突出最重要的部分来总结长文本，而不是生成新的文本作为摘要）。
- en: '![  Language models produce contextualized token embeddings that improve on
    raw  static token embeddings](assets/tokens_token_embeddings_963889_07.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![语言模型生成的上下文化标嵌入比原始静态标嵌入更为优越。](assets/tokens_token_embeddings_963889_07.png)'
- en: Figure 6-7\. Language models produce contextualized token embeddings that improve
    on raw, static token embeddings
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7\. 语言模型生成的上下文化标嵌入比原始静态标嵌入更为优越。
- en: Instead of representing each token or word with a static vector, language models
    create contextualized word embeddings (shown in [Figure 6-7](#fig_7__language_models_produce_contextualized_token_emb))
    that represent a word with a different token based on its context. These vectors
    can then be used by other systems for a variety of tasks. In addition to the text
    applications we mentioned in the previous paragraph, these contextualized vectors,
    for example, are what powers AI image generation systems like Dall-E, Midjourney,
    and Stable Diffusion, for example.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型并不使用静态向量来表示每个标记或单词，而是生成上下文化单词嵌入（如[图6-7](#fig_7__language_models_produce_contextualized_token_emb)所示），根据上下文以不同的标记表示单词。这些向量可以被其他系统用于各种任务。除了我们在前一段中提到的文本应用外，这些上下文化向量，例如，正是驱动AI图像生成系统如Dall-E、Midjourney和Stable
    Diffusion的力量。
- en: 'Code Example: Contextualized Word Embeddings From a Language Model (Like BERT)'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码示例：来自语言模型（如BERT）的上下文化单词嵌入。
- en: 'Let’s look at how we can generate contextualized word embeddings, the majority
    of this code should be familiar to you by now:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何生成上下文化单词嵌入，这段代码的大部分现在应该对你很熟悉：
- en: '[PRE23]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code downloads a pre-trained tokenizer and model, then uses them to process
    the string “Hello world”. The output of the model is then saved in the output
    variable. Let’s inspect that variable by first printing its dimensions (we expect
    it to be a multi-dimensional array).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码下载一个预训练的分词器和模型，然后使用它们处理字符串“Hello world”。模型的输出结果随后保存在输出变量中。让我们通过首先打印其维度来检查该变量（我们预计它是一个多维数组）。
- en: 'The model we’re using here is called DeBERTA v3, which at the time of writing,
    is one of the best-performing language models for token embeddings while being
    small and highly efficient. It is described in the paper [DeBERTaV3: Improving
    DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding
    Sharing](https://openreview.net/forum?id=sE7-XhLxHA).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在这里使用的模型称为DeBERTA v3，在撰写时，它是表现最佳的标记嵌入语言模型之一，同时体积小且高效。其详细描述见论文[DeBERTaV3:
    Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled
    Embedding Sharing](https://openreview.net/forum?id=sE7-XhLxHA)。'
- en: '[PRE24]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This prints out:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can ignore the first dimension and read this as four tokens, each one embedded
    in 384 values.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以忽略第一维，将其视为四个标记，每个标记嵌入384个值。
- en: 'But what are these four vectors? Did the tokenizer break the two words into
    four tokens, or is something else happening here? We can use what we’ve learned
    about tokenizers to inspect them:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 但这四个向量是什么？分词器是否将两个单词分解成四个标记，还是发生了其他情况？我们可以利用所学的分词器知识来检查它们：
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Which prints out:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE27]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Which shows that this particular tokenizer and model operate by adding the [CLS]
    and [SEP] tokens to the beginning and end of a string.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明这个特定的分词器和模型通过在字符串的开头和结尾添加[CLS]和[SEP]标记来操作。
- en: 'Our language model has now processed the text input. The result of its output
    is the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语言模型现在已经处理了文本输入。其输出结果如下：
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This is the raw output of a language model. The applications of large language
    models build on top of outputs like this.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这是语言模型的原始输出。大型语言模型的应用基于这样的输出。
- en: We can recap the input tokenization and resulting outputs of a language model
    in [Figure 6-8](#fig_8__a_language_model_operates_on_raw_static_embeddi). Technically,
    the switch from token IDs into raw embeddings is the first step that happens inside
    a language model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图6-8](#fig_8__a_language_model_operates_on_raw_static_embeddi)中回顾输入标记化和语言模型的输出。技术上，从词元ID转换为原始嵌入是语言模型内部发生的第一步。
- en: '![  A language model operates on raw  static embeddings as its input and produces
    contextual text embeddings.](assets/tokens_token_embeddings_963889_08.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![一个语言模型在原始静态嵌入上操作，并生成上下文文本嵌入。](assets/tokens_token_embeddings_963889_08.png)'
- en: Figure 6-8\. A language model operates on raw, static embeddings as its input
    and produces contextual text embeddings.
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8\. 一个语言模型在原始静态嵌入上操作，并生成上下文文本嵌入。
- en: A visual like this is essential for the next chapter when we start to look at
    how Transformer-based LLMs work under the hood.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的可视化对于下一章我们开始研究基于Transformer的LLMs如何运作至关重要。
- en: Word Embeddings
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Token embeddings are useful even outside of large language models. Embeddings
    generated by pre-LLM methods like Word2Vec, Glove, and Fasttext still have uses
    in NLP and beyond NLP. In this section, we’ll look at how to use pre-trained Word2Vec
    embeddings and touch on how the method creates word embeddings. Seeing how Word2Vec
    is trained will prime you for the chapter on contrastive training. Then in the
    following section, we’ll see how those embeddings can be used for recommendation
    systems.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 词元嵌入即使在大型语言模型之外也很有用。通过如Word2Vec、Glove和Fasttext等预-LLM方法生成的嵌入在NLP及其之外仍然有用。在本节中，我们将探讨如何使用预训练的Word2Vec嵌入，并简要介绍该方法如何创建词嵌入。了解Word2Vec的训练过程将为后续关于对比训练的章节做好准备。接下来我们将看到这些嵌入如何用于推荐系统。
- en: Using Pre-trained Word Embeddings
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练词嵌入
- en: Let’s look at how we can download pre-trained word embeddings using the [Gensim](https://radimrehurek.com/gensim/)
    library
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用[Gensim](https://radimrehurek.com/gensim/)库下载预训练的词嵌入。
- en: '[PRE29]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, we’ve downloaded the embeddings of a large number of words trained on
    wikipedia. We can then explore the embedding space by seeing the nearest neighbors
    of a specific word, ‘king’ for example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们下载了在维基百科上训练的大量单词嵌入。我们可以通过查看特定单词的最近邻，来探索嵌入空间，例如‘king’：
- en: '[PRE30]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Which outputs:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出为：
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The Word2vec Algorithm and Contrastive Training
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2vec算法与对比训练
- en: The word2vec algorithm described in the paper [Efficient Estimation of Word
    Representations in Vector Space](https://arxiv.org/abs/1301.3781) is described
    in detail in [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/).
    The central ideas are condensed here as we build on them when discussing one method
    for creating embeddings for recommendation engines in the following section.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[高效估计向量空间中的词表示](https://arxiv.org/abs/1301.3781)中描述的word2vec算法在[图解Word2vec](https://jalammar.github.io/illustrated-word2vec/)中有详细说明。这里的中心思想在于我们在下一节讨论为推荐引擎创建嵌入的一种方法时进行扩展。
- en: Just like LLMs, word2vec is trained on examples generated from text. Let’s say
    for example, we have the text "*Thou shalt not make a machine in the likeness
    of a human mind*" from the *Dune* novels by Frank Herbert. The algorithm uses
    a sliding window to generate training examples. We can for example have a window
    size two, meaning that we consider two neighbors on each side of a central word.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 就像LLMs一样，word2vec是基于从文本生成的示例进行训练的。举个例子，我们有来自弗兰克·赫伯特的*沙丘*小说中的文本“*你不可制造与人类心智相似的机器*”。该算法使用滑动窗口生成训练示例。比如，我们可以设定窗口大小为2，这意味着我们考虑中心单词两侧的两个邻居。
- en: The embeddings are generated from a classification task. This task is used to
    train a neural network to predict if words appear in the same context or not.
    We can think of this as a neural network that takes two words and outputs 1 if
    they tend to appear in the same context, and 0 if they do not.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入是从分类任务中生成的。该任务用于训练神经网络以预测单词是否出现在同一上下文中。我们可以将其视为一个神经网络，它接受两个单词，并在它们倾向于出现在同一上下文时输出1，而在它们不出现在同一上下文时输出0。
- en: In the first position for the sliding window, we can generate four training
    examples as we can see in [Figure 6-9](#fig_9__a_sliding_window_is_used_to_generate_training_ex).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在滑动窗口的第一个位置，我们可以生成四个训练示例，如[图6-9](#fig_9__a_sliding_window_is_used_to_generate_training_ex)所示。
- en: '![  A sliding window is used to generate training examples for the word2vec
    algorithm to later predict if two words are neighbors or not.](assets/tokens_token_embeddings_963889_09.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![  使用滑动窗口生成训练样本，以便word2vec算法后续预测两个单词是否为邻居。](assets/tokens_token_embeddings_963889_09.png)'
- en: Figure 6-9\. A sliding window is used to generate training examples for the
    word2vec algorithm to later predict if two words are neighbors or not.
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9\. 使用滑动窗口生成训练样本，以便word2vec算法后续预测两个单词是否为邻居。
- en: In each of the produced training examples, the word in the center is used as
    one input, and each of its neighbors is a distinct second input in each training
    example. We expect the final trained model to be able to classify this neighbor
    relationship and output 1 if the two input words it receives are indeed neighbors.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个生成的训练样本中，中心的单词作为一个输入，每个邻居在每个训练样本中都是一个独特的第二输入。我们期望最终训练的模型能够分类这种邻居关系，并在收到确实是邻居的两个输入单词时输出1。
- en: These training examples are visualized in [Figure 6-10](#fig_10__each_generated_training_example_shows_a_pair_of).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这些训练样本在[图6-10](#fig_10__each_generated_training_example_shows_a_pair_of)中进行了可视化。
- en: '![  Each generated training example shows a pair of neighboring words.](assets/tokens_token_embeddings_963889_10.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![  每个生成的训练样本展示了一对邻居单词。](assets/tokens_token_embeddings_963889_10.png)'
- en: Figure 6-10\. Each generated training example shows a pair of neighboring words.
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10\. 每个生成的训练样本展示了一对邻居单词。
- en: If, however, we have a dataset of only a target value of 1, then a model can
    ace it by output 1 all the time. To get around this, we need to enrich our training
    dataset with examples of words that are not typically neighbors. These are called
    negative examples and are shown in [Figure 6-11](#fig_11__we_need_to_present_our_models_with_negative_exam).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们只有目标值为1的数据集，那么模型可以通过始终输出1来轻松应对。为了绕过这个问题，我们需要用通常不是邻居的单词例子丰富我们的训练数据集。这些被称为负例，如[图6-11](#fig_11__we_need_to_present_our_models_with_negative_exam)所示。
- en: '![  We need to present our models with negative examples  words that are not
    usually neighbors. A better model is able to better distinguish between the positive
    and negative examples.](assets/tokens_token_embeddings_963889_11.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![  我们需要向模型呈现负例：通常不是邻居的单词。更好的模型能够更好地区分正例和负例。](assets/tokens_token_embeddings_963889_11.png)'
- en: 'Figure 6-11\. We need to present our models with negative examples: words that
    are not usually neighbors. A better model is able to better distinguish between
    the positive and negative examples.'
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 我们需要向模型呈现负例：通常不是邻居的单词。更好的模型能够更好地区分正例和负例。
- en: 'It turns out that we don’t have to be too scientific in how we choose the negative
    examples. A lot of useful models are result from simple ability to detect positive
    examples from randomly generated examples (inspired by an important idea called
    Noise Contrastive Estimation and described in [Noise-contrastive estimation: A
    new estimation principle for unnormalized statistical models](https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)).
    So in this case, we get random words and add them to the dataset and indicate
    that they are not neighbors (and thus the model should output 0 when it sees them.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '事实证明，我们在选择负例时不必过于科学。许多有用的模型是通过简单地从随机生成的例子中检测正例的能力而得来的（灵感来自一个重要的思想，称为噪声对比估计，并在[Noise-contrastive
    estimation: A new estimation principle for unnormalized statistical models](https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)中描述）。因此，在这种情况下，我们获取随机单词并将其添加到数据集中，并表明它们不是邻居（因此模型在看到这些时应输出0）。'
- en: 'With this, we’ve seen two of the main concepts of word2vec ([Figure 6-12](#fig_12__skipgram_and_negative_sampling_are_two_of_the_ma)):
    Skipgram - the method of selecting neighboring words and negative sampling - adding
    negative examples by random sampling from the dataset.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们已经看到了word2vec的两个主要概念（[图6-12](#fig_12__skipgram_and_negative_sampling_are_two_of_the_ma)）：Skipgram
    - 选择邻居单词的方法，以及负采样 - 通过从数据集中随机抽样添加负例。
- en: '![  Skipgram and Negative Sampling are two of the main ideas behind the word2vec
    algorithm and are useful in many other problems that can be formulated as token
    sequence problems.](assets/tokens_token_embeddings_963889_12.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![  Skipgram和负采样是word2vec算法背后的两个主要思想，在许多其他可以表述为令牌序列问题的问题中也非常有用。](assets/tokens_token_embeddings_963889_12.png)'
- en: Figure 6-12\. Skipgram and Negative Sampling are two of the main ideas behind
    the word2vec algorithm and are useful in many other problems that can be formulated
    as token sequence problems.
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12\. Skipgram和负采样是word2vec算法背后的两个主要思想，并且在许多可以表述为标记序列问题的其他问题中也很有用。
- en: We can generate millions and even billions of training examples like this from
    running text. Before proceeding to train a neural network on this dataset, we
    need to make a couple of tokenization decisions, which, just like we’ve seen with
    LLM tokenizers, include how to deal with capitalization and punctuation and how
    many tokens we want in our vocabulary.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行文本生成数百万甚至数十亿个训练示例。在对该数据集进行神经网络训练之前，我们需要做出一些标记决策，正如我们在LLM标记器中看到的那样，包括如何处理大写和标点，以及我们想要在词汇表中有多少个标记。
- en: We then create an embedding vector for each token, and randomly initialize them,
    as can be seen in [Figure 6-13](#fig_13__a_vocabulary_of_words_and_their_starting_random).
    In practice, this is a matrix of dimensions vocab_size x embedding_dimensions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们为每个标记创建一个嵌入向量，并随机初始化它们，如[图6-13](#fig_13__a_vocabulary_of_words_and_their_starting_random)所示。实际上，这是一个维度为vocab_size
    x embedding_dimensions的矩阵。
- en: '![  A vocabulary of words and their starting  random  uninitialized embedding
    vectors.](assets/tokens_token_embeddings_963889_13.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![  词汇及其起始随机未初始化的嵌入向量。](assets/tokens_token_embeddings_963889_13.png)'
- en: Figure 6-13\. A vocabulary of words and their starting, random, uninitialized
    embedding vectors.
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13\. 词汇及其起始的随机未初始化的嵌入向量。
- en: 'A model is then trained on each example to take in two embedding vectors and
    predict if they’re related or not. We can see what this looks like in [Figure 6-14](#fig_14__a_neural_network_is_trained_to_predict_if_two_wo):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一个模型在每个示例上进行训练，接收两个嵌入向量并预测它们是否相关。我们可以在[图6-14](#fig_14__a_neural_network_is_trained_to_predict_if_two_wo)中看到这个过程的样子：
- en: '![  A neural network is trained to predict if two words are neighbors. It updates
    the embeddings in the training process to produce the final  trained embeddings.](assets/tokens_token_embeddings_963889_14.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![  一个神经网络被训练来预测两个词是否相邻。它在训练过程中更新嵌入，以生成最终的训练嵌入。](assets/tokens_token_embeddings_963889_14.png)'
- en: Figure 6-14\. A neural network is trained to predict if two words are neighbors.
    It updates the embeddings in the training process to produce the final, trained
    embeddings.
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14\. 一个神经网络被训练来预测两个词是否相邻。它在训练过程中更新嵌入，以生成最终的训练嵌入。
- en: Based on whether its prediction was correct or not, the typical machine learning
    training step updates the embeddings so that the next the model is presented with
    those two vectors, it has a better chance of being more correct. And by the end
    of the training process, we have better embeddings for all the tokens in our vocabulary.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预测是否正确，典型的机器学习训练步骤会更新嵌入，使得下次模型展示这两个向量时，有更大的机会变得更准确。到训练过程结束时，我们为词汇表中的所有标记拥有更好的嵌入。
- en: This idea of a model that takes two vectors and predicts if they have a certain
    relation is one of the most powerful ideas in machine learning, and time after
    time has proven to work very well with language models. This is why we’re dedicating
    chapter XXX to go over this concept and how it optimizes language models for specific
    tasks (like sentence embeddings and retrieval).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的想法是将两个向量结合并预测它们是否具有某种关系，这是机器学习中最强大的想法之一，并且一次又一次地证明在语言模型中非常有效。这就是为什么我们专门用章节XXX来详细讨论这一概念，以及它如何优化语言模型以应对特定任务（如句子嵌入和检索）。
- en: The same idea is also central to bridging modalities like text and images which
    is key to AI Image generation models. In that formulation, a model is presented
    with an image and a caption, and it should predict whether that caption describes
    this image or not.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的想法也在弥合文本和图像等模态之间至关重要，这对AI图像生成模型来说是关键。在该模型中，给定一张图像和一个标题，模型应该预测这个标题是否描述了这张图像。
- en: Embeddings for Recommendation Systems
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统的嵌入
- en: The concept of token embeddings is useful in so many other domains. In industry,
    it’s widely used for recommendation systems, for example.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 标记嵌入的概念在许多其他领域也非常有用。在行业中，例如，它广泛用于推荐系统。
- en: Recommending songs by embeddings
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过嵌入推荐歌曲
- en: In this section we’ll use the Word2vec algorithm to embed songs using human-made
    music playlists. Imagine if we treated each song as we would a word or token,
    and we treated each playlist like a sentence. These embeddings can then be used
    to recommend similar songs which often appear together in playlists.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Word2vec算法，通过人造音乐播放列表来嵌入歌曲。想象一下，如果我们将每首歌视为一个单词或标记，而将每个播放列表视为一个句子。这些嵌入可以用来推荐那些经常一起出现在播放列表中的相似歌曲。
- en: The [dataset](https://www.cs.cornell.edu/~shuochen/lme/data_page.html) we’ll
    use was collected by Shuo Chen from Cornell University. The dataset contains playlists
    from hundreds of radio stations around the US. [Figure 6-15](#fig_15__for_song_embeddings_that_capture_song_similarity)
    demonstrates this dataset.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的[数据集](https://www.cs.cornell.edu/~shuochen/lme/data_page.html)是由康奈尔大学的Shuo
    Chen收集的。该数据集包含来自美国数百个广播电台的播放列表。[图6-15](#fig_15__for_song_embeddings_that_capture_song_similarity)展示了该数据集。
- en: '![  For song embeddings that capture song similarity we ll use a dataset made
    up of a collection of playlists  each containing a list of songs.](assets/tokens_token_embeddings_963889_15.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![为了捕捉歌曲相似性的歌曲嵌入，我们将使用一个由包含歌曲列表的播放列表集合组成的数据集。](assets/tokens_token_embeddings_963889_15.png)'
- en: Figure 6-15\. For song embeddings that capture song similarity we’ll use a dataset
    made up of a collection of playlists, each containing a list of songs.
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15。为了捕捉歌曲相似性的歌曲嵌入，我们将使用一个由包含歌曲列表的播放列表集合组成的数据集。
- en: Let’s demonstrate the end product before we look at how it’s built. So let’s
    give it a few songs and see what it recommends in response.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在查看构建方式之前演示最终产品。所以，让我们给出几首歌曲，看看它推荐什么。
- en: 'Let’s start by giving it Michael Jackson’s *Billie Jean*, the song with ID
    #3822.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始，选择迈克尔·杰克逊的*比利·珍*，歌曲ID为#3822。
- en: '[PRE32]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '| id | title | artist |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| id | 标题 | 艺术家 |'
- en: '| --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 4181 | Kiss | Prince & The Revolution |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 4181 | 吻 | 王子与革命 |'
- en: '| 12749 | Wanna Be Startin’ Somethin’ | Michael Jackson |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 12749 | 想要开始一些事情 | 迈克尔·杰克逊 |'
- en: '| 1506 | The Way You Make Me Feel | Michael Jackson |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 1506 | 你让我感觉的方式 | 迈克尔·杰克逊 |'
- en: '| 3396 | Holiday | Madonna |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 3396 | 假期 | 麦当娜 |'
- en: '| 500 | Don’t Stop ‘Til You Get Enough | Michael Jackson |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 不停直到你满足 | 迈克尔·杰克逊 |'
- en: That looks reasonable. Madonna, Prince, and other Michael Jackson songs are
    the nearest neighbors.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来合理。麦当娜、王子和其他迈克尔·杰克逊的歌曲是最近的邻居。
- en: 'Let’s step away from Pop and into Rap, and see the neighbors of 2Pac’s California
    Love:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从流行音乐转向说唱，看看2Pac的《加州爱情》的邻居：
- en: '[PRE33]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '| id | title | artist |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| id | 标题 | 艺术家 |'
- en: '| --- | --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 413 | If I Ruled The World (Imagine That) (w\/ Lauryn Hill) | Nas |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 413 | 如果我统治世界（想象一下）（与劳伦·希尔） | 纳斯 |'
- en: '| 196 | I’ll Be Missing You | Puff Daddy & The Family |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 196 | 我会想念你 | Puff Daddy & The Family |'
- en: '| 330 | Hate It Or Love It (w\/ 50 Cent) | The Game |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 330 | 爱或恨（与50 Cent） | The Game |'
- en: '| 211 | Hypnotize | The Notorious B.I.G. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 211 | 催眠 | 知名B.I.G. |'
- en: '| 5788 | Drop It Like It’s Hot (w\/ Pharrell) | Snoop Dogg |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 5788 | 像热一样抛掉（与法瑞尔） | snoop dogg |'
- en: Another quite reasonable list!
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相当合理的列表！
- en: '[PRE34]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That takes a minute or two to train and results in embeddings being calculated
    for each song that we have. Now we can use those embeddings to find similar songs
    exactly as we did earlier with words.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 训练需要一两分钟，结果是为我们拥有的每首歌计算嵌入。现在，我们可以像之前处理单词一样使用这些嵌入来找到相似歌曲。
- en: '[PRE35]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Which outputs:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出为：
- en: '[PRE36]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: And that is the list of the songs whose embeddings are most similar to song
    2172\. See the jupyter notebook for the code that links song ids to their names
    and artist names.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与歌曲2172相似的歌曲嵌入列表。请查看jupyter笔记本中的代码，该代码将歌曲ID与其名称和艺术家名称链接起来。
- en: 'In this case, the song is:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，歌曲是：
- en: '[PRE37]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Resulting in recommendations that are all in the same heavy metal and hard
    rock genre:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 结果推荐都是在同一重金属和硬摇滚类型中：
- en: '| id | title | artist |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| id | 标题 | 艺术家 |'
- en: '| --- | --- | --- |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 11473 | Little Guitars | Van Halen |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 11473 | 小吉他 | 瓦恩·海伦 |'
- en: '| 3167 | Unchained | Van Halen |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 3167 | 不羁 | 瓦恩·海伦 |'
- en: '| 5586 | The Last In Line | Dio |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 5586 | 最后的行列 | 迪奥 |'
- en: '| 5634 | Mr. Brownstone | Guns N’ Roses |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 5634 | 布朗石先生 | 枪与玫瑰 |'
- en: '| 3094 | Breaking The Law | Judas Priest |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 3094 | 违反法律 | 猶達斯·普里斯特 |'
- en: Summary
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have covered LLM tokens, tokenizers, and useful approaches
    to use token embeddings beyond language models.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了LLM令牌、分词器以及使用令牌嵌入超越语言模型的有用方法。
- en: Tokenizers are the first step in processing the input to a LLM -- turning text
    into a list of token IDs.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器是处理输入到大型语言模型（LLM）的第一步——将文本转换为令牌ID列表。
- en: Some of the common tokenization schemes include breaking text down into words,
    subword tokens, characters, or bytes
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些常见的分词方案包括将文本拆分为单词、子词标记、字符或字节。
- en: A tour of real-world pre-trained tokenizers (from BERT to GPT2, GPT4, and other
    models) showed us areas where some tokenizers are better (e.g., preserving information
    like capitalization, new lines, or tokens in other languages) and other areas
    where tokenizers are just different from each other (e.g., how they break down
    certain words).
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界预训练分词器的巡览（从BERT到GPT2、GPT4及其他模型）向我们展示了某些分词器在保持信息（如大写、新行或其他语言的标记）方面表现更佳的领域，以及在某些方面分词器之间的不同之处（例如，它们如何拆分某些词）。
- en: Three of the major tokenizer design decisions are the tokenizer algorithm (e.g.,
    BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary
    size, special tokens, capitalization, treatment of capitalization and different
    languages), and the dataset the tokenizer is trained on.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个主要的分词器设计决策是分词器算法（如BPE、WordPiece、SentencePiece）、分词参数（包括词汇表大小、特殊标记、大写处理、对大写和不同语言的处理），以及分词器训练所用的数据集。
- en: Language models are also creators of high-quality contextualized token embeddings
    that improve on raw static embeddings. Those contextualized token embeddings are
    what’s used for tasks including NER, extractive text summarization, and span classification.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型也是高质量上下文化词嵌入的创造者，这些词嵌入在静态嵌入的基础上得到了改善。这些上下文化词嵌入用于包括命名实体识别（NER）、抽取式文本摘要和跨度分类等任务。
- en: Before LLMs, word embedding methods like word2vec, Glove and Fasttext were popular.
    They still have some use cases within and outside of language processing.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）出现之前，词嵌入方法如word2vec、Glove和Fasttext曾很流行。它们在语言处理内外仍然有一些应用场景。
- en: 'The Word2Vec algorithm relies on two main ideas: Skipgram and Negative Sampling.
    It also uses contrastive training similar to the one we’ll see in the contrastive
    training chapter.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec算法依赖于两个主要思想：Skipgram和负采样。它还使用了对比训练，类似于我们将在对比训练章节中看到的内容。
- en: Token embeddings are useful for creating and improving recommender systems as
    we’ve seen in the music recommender we’ve built from curated song playlists.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入对创建和改进推荐系统非常有用，正如我们在构建的基于精选歌曲播放列表的音乐推荐系统中所看到的那样。
