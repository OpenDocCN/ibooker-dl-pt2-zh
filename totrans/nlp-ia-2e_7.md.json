["```py\n>>> import pandas as pd\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_md')  # #1\n\n>>> text = 'right ones in the right order you can nudge the world'\n>>> doc = nlp(text)\n>>> df = pd.DataFrame([\n...    {k: getattr(t, k) for k in 'text pos_'.split()}\n...    for t in doc])\n```", "```py\ntext  pos_\n0   right   ADJ\n1    ones  NOUN\n2      in   ADP\n3     the   DET\n4   right   ADJ\n5   order  NOUN\n6     you  PRON\n7     can   AUX\n8   nudge  VERB\n9     the   DET\n10  world  NOUN\n```", "```py\n>>> pd.get_dummies(df, columns=['pos_'], prefix='', prefix_sep='')\n```", "```py\ntext  ADJ  ADP  AUX  DET  NOUN  PRON  VERB\n0   right    1    0    0    0     0     0     0\n1    ones    0    0    0    0     1     0     0\n2      in    0    1    0    0     0     0     0\n3     the    0    0    0    1     0     0     0\n4   right    1    0    0    0     0     0     0\n5   order    0    0    0    0     1     0     0\n6     you    0    0    0    0     0     1     0\n7     can    0    0    1    0     0     0     0\n8   nudge    0    0    0    0     0     0     1\n9     the    0    0    0    1     0     0     0\n10  world    0    0    0    0     1     0     0\n```", "```py\n0, 1   (right, ones)     (ADJ, NOUN)    _True_\n```", "```py\n1, 2   (ones, in)        (NOUN, ADP)    False\n```", "```py\n>>> def corr(a, b):\n...    \"\"\" Compute the Pearson correlation coefficient R \"\"\"\n...    a = a - np.mean(a)\n...    b = b - np.mean(b)\n...    return sum(a * b) / np.sqrt(sum(a*a) * sum(b*b))\n>>> a = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n>>> b = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n>>> corr(a, b)\n0.316...\n>>> corr(a, a)\n1.0\n```", "```py\n>>> nlp = spacy.load('en_core_web_md')\n>>> quote = \"The right word may be effective, but no word was ever\" \\\n...    \" as effective as a rightly timed pause.\"\n>>> tagged_words = {\n...    t.text: [t.pos_, int(t.pos_ == 'ADV')]  # #1\n...    for t in nlp(quote)}\n>>> df_quote = pd.DataFrame(tagged_words, index=['POS', 'ADV'])\n>>> print(df_quote)\n```", "```py\nThe right  word  may   be  ...    a rightly timed pause      .\nPOS  DET   ADJ  NOUN  AUX  AUX  ...  DET     ADV  VERB  NOUN  PUNCT\nADV    0     0     0    0    0  ...    0       1     0     0      0\n```", "```py\n>>> inpt = list(df_quote.loc['ADV'])\n>>> print(inpt)\n```", "```py\n[0, 0, 0, ... 0, 1, 1, 0, 0...]\n```", "```py\n>>> kernel = [.5, .5]  # #1\n>>>\n>>> output = []\n>>> for i in range(len(inpt) - 1):  # #2\n...    z = 0\n...    for k, weight in enumerate(kernel):  # #3\n...        z = z + weight * inpt[i + k]\n...    output.append(z)\n>>>\n>>> print(f'inpt:\\n{inpt}')\n>>> print(f'len(inpt): {len(inpt)}')\n>>> print(f'output:\\n{[int(o) if int(o)==o else o for o in output]}')\n>>> print(f'len(output): {len(output)}')\n```", "```py\ninpt:\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0., 1, 1., 0, 0, 0., 1., 0, 0, 0]\nlen(inpt): 20\noutput:\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .5, 1, .5, 0, 0, .5, .5, 0, 0]\nlen(output): 19\n```", "```py\n>>> import pandas as pd\n>>> from matplotlib import pyplot as plt\n>>> plt.rcParams['figure.dpi'] = 120  # #1\n\n>>> import seaborn as sns\n>>> sns.set_theme('paper')  # #2\n\n>>> df = pd.DataFrame([inpt, output], index=['inpt', 'output']).T\n>>> ax = df.plot(style=['+-', 'o:'], linewidth=3)\n```", "```py\n>>> def convolve(inpt, kernel):\n...    output = []\n...    for i in range(len(inpt) - len(kernel) + 1):  # #1\n...        output.append(\n...            sum(\n...                [\n...                    inpt[i + k] * kernel[k]\n...                    for k in range(len(kernel))  # #2\n...                ]\n...            )\n...        )\n...    return output\n```", "```py\n>>> tags = 'ADV ADJ VERB NOUN'.split()\n>>> tagged_words = [\n...    [tok.text] + [int(tok.pos_ == tag) for tag in tags]  # #1\n...    for tok in nlp(quote)]  # #2\n>>>\n>>> df = pd.DataFrame(tagged_words, columns=['token'] + tags).T\n>>> print(df)\n```", "```py\nThe  right  word  may  be  ...  a  rightly  timed  pause  .\nADV     0      0     0    0   0  ...  0        1      0      0  0\nADJ     0      1     0    0   0  ...  0        0      0      0  0\nVERB    0      0     0    0   0  ...  0        0      1      0  0\nNOUN    0      0     1    0   0  ...  0        0      0      1  0\n```", "```py\n>>> import torch\n>>> x = torch.tensor(\n...     df.iloc[1:].astype(float).values,\n...     dtype=torch.float32)  # #1\n>>> x = x.unsqueeze(0) # #2\n```", "```py\n>>> kernel = pd.DataFrame(\n...           [[1, 0, 0.],\n...            [0, 0, 0.],\n...            [0, 1, 0.],\n...            [0, 0, 1.]], index=tags)\n>>> print(kernel)\n```", "```py\n>>> kernel = torch.tensor(kernel.values, dtype=torch.float32)\n>>> kernel = kernel.unsqueeze(0)  # #1\n>>> conv = torch.nn.Conv1d(in_channels=4,\n...                     out_channels=1,\n...                     kernel_size=3,\n...                     bias=False)\n>>> conv.load_state_dict({'weight': kernel})\n>>> print(conv.weight)\n\ntensor([[[1., 0., 0.],\n         [0., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]]])\n```", "```py\n>>> y = np.array(conv.forward(x).detach()).squeeze()\n>>> df.loc['y'] = pd.Series(y)\n>>> df\n        0      1     2    3    4   ...   15       16     17     18   19\ntoken  The  right  word  may   be  ...    a  rightly  timed  pause    .\nADV      0      0     0    0    0  ...    0        1      0      0    0\nADJ      0      1     0    0    0  ...    0        0      0      0    0\nVERB     0      0     0    1    0  ...    0        0      1      0    0\nNOUN     0      0     1    0    0  ...    0        0      0      1    0\ny      1.0    0.0   1.0  0.0  0.0  ...  0.0      3.0    0.0    NaN  NaN\n```", "```py\n>>> from nlpia2.init import maybe_download\n\n>>> url = 'https://upload.wikimedia.org/wikipedia/' \\\n      'commons/7/78/1210secretmorzecode.wav'\n>>> filepath = maybe_download(url)  # #1\n>>> filepath\n'/home/hobs/.nlpia2-data/1210secretmorzecode.wav'\n```", "```py\n>>> from scipy.io import wavfile\n\n>>> sample_rate, audio = wavfile.read(filepath)\n>>> print(f'sample_rate: {sample_rate}')\n>>> print(f'audio:\\n{audio}')\n```", "```py\nsample_rate: 4000\naudio:\n[255   0 255 ...   0 255   0]\n```", "```py\n>>> pd.options.display.max_rows = 7\n\n>>> audio = audio[:sample_rate * 2]  # #1\n>>> audio = np.abs(audio - audio.max() / 2) - .5  # #2\n>>> audio = audio / audio.max()  # #3\n>>> audio = audio[::sample_rate // 400]  # #4\n>>> audio = pd.Series(audio, name='audio')\n>>> audio.index = 1000 * audio.index / sample_rate  # #5\n>>> audio.index.name = 'time (ms)'\n>>> print(f'audio:\\n{audio}')\n```", "```py\n>>> kernel = [-1] * 24 + [1] * 24 + [-1] * 24  # #1\n>>> kernel = pd.Series(kernel, index=2.5 * np.arange(len(kernel)))\n>>> kernel.index.name = 'Time (ms)'\n>>> ax = kernel.plot(linewidth=3, ylabel='Kernel weight')\n```", "```py\n>>> kernel = np.array(kernel) / sum(np.abs(kernel))  # #1\n>>> pad = [0] * (len(kernel) // 2)  # #2\n>>> isdot = convolve(audio.values, kernel)\n>>> isdot =  np.array(pad[:-1] + list(isdot) + pad)  # #3\n>>> df = pd.DataFrame()\n>>> df['audio'] = audio\n>>> df['isdot'] = isdot - isdot.min()\n>>> ax = df.plot()\n```", "```py\n>>> isdot = np.convolve(audio.values, kernel, mode='same')  # #1\n>>> df['isdot'] = isdot - isdot.min()\n>>> ax = df.plot()\n```", "```py\n>>> df = pd.read_csv(HOME_DATA_DIR / 'news.csv')\n>>> df = df[['text', 'target']]  # #1\n>>> print(df)\n```", "```py\ntext  target\n0     Our Deeds are the Reason of this #earthquake M...       1\n1                Forest fire near La Ronge Sask. Canada       1\n2     All residents asked to 'shelter in place' are ...       1\n...                                                 ...     ...\n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n7611  Police investigating after an e-bike collided ...       1\n7612  The Latest: More Homes Razed by Northern Calif...       1\n[7613 rows x 2 columns]\n```", "```py\nimport re\nfrom collections import Counter\nfrom itertools import chain\nHOME_DATA_DIR = Path.home() / '.nlpia2-data'\n\ncounts = Counter(chain(*[\n    re.findall(r'\\w+', t.lower()) for t in df['text']]))  # #1\nvocab = [tok for tok, count in counts.most_common(4000)[3:]]  # #2\n\nprint(counts.most_common(10))\n```", "```py\n[('t', 5199), ('co', 4740), ('http', 4309), ('the', 3277), ('a', 2200),\n    ('in', 1986)]\n```", "```py\ndef pad(sequence, pad_value, seq_len):\n    padded = list(sequence)[:seq_len]\n    padded = padded + [pad_value] * (seq_len - len(padded))\n    return padded\n```", "```py\nfrom torch import nn\n\nembedding = nn.Embedding(\n    num_embeddings=2000,  # #1\n    embedding_dim=64,  # #2\n    padding_idx=0)\n```", "```py\nfrom nlpia2.ch07.cnn.train79 import Pipeline  # #1\n\npipeline = Pipeline(\n    vocab_size=2000,\n    embeddings=(2000, 64),\n    epochs=7,\n    torch_random_state=433994,  # #2\n    split_random_state=1460940,\n)\n\npipeline = pipeline.train()\n```", "```py\nEpoch: 1, loss: 0.66147, Train accuracy: 0.61392, Test accuracy: 0.63648\nEpoch: 2, loss: 0.64491, Train accuracy: 0.69712, Test accuracy: 0.70735\nEpoch: 3, loss: 0.55865, Train accuracy: 0.73391, Test accuracy: 0.74278\nEpoch: 4, loss: 0.38538, Train accuracy: 0.76558, Test accuracy: 0.77165\nEpoch: 5, loss: 0.27227, Train accuracy: 0.79288, Test accuracy: 0.77690\nEpoch: 6, loss: 0.29682, Train accuracy: 0.82119, Test accuracy: 0.78609\nEpoch: 7, loss: 0.23429, Train accuracy: 0.82951, Test accuracy: 0.79003\n```", "```py\npipeline.epochs = 13  # #1\npipeline = pipeline.train()\n```", "```py\nEpoch: 1, loss: 0.24797, Train accuracy: 0.84528, Test accuracy: 0.78740\nEpoch: 2, loss: 0.16067, Train accuracy: 0.86528, Test accuracy: 0.78871\n...\nEpoch: 12, loss: 0.04796, Train accuracy: 0.93578, Test accuracy: 0.77690\nEpoch: 13, loss: 0.13394, Train accuracy: 0.94132, Test accuracy: 0.77690\n```", "```py\npipeline.indexes_to_texts(pipeline.x_test[:4])\n```", "```py\n['getting in the poor girl <PAD> <PAD> ...',\n 'Spot Flood Combo Cree LED Work Light Bar Offroad Lamp Full ...',\n 'ice the meltdown <PAD> <PAD> <PAD> <PAD> ...',\n 'and burn for bush fires in St http t co <PAD> <PAD> ...']\n```", "```py\n>>> def describe_model(model):  # #1\n...     state = model.state_dict()\n...     names = state.keys()\n...     weights = state.values()\n...     params = model.parameters()\n>>>     df = pd.DataFrame()\n>>>     df['name'] = list(state.keys())\n>>>     df['all'] = p.numel(),\n...     df['learned'] = [\n...         p.requires_grad  # #2\n...         for p in params],  # #3\n...     size=p.size(),\n...     )\n        for name, w, p in zip(names, weights, params)\n    ]\n    )\n    df = df.set_index('name')\n    return df\n\ndescribe_model(pipeline.model)  # #4\n```", "```py\nlearned_params  all_params        size\nname\nembedding.weight             128064      128064  (2001, 64)  # #1\nlinear_layer.weight            1856        1856   (1, 1856)\nlinear_layer.bias                 1           1        (1,)\n```", "```py\n>>> from torch import nn\n>>> embedding = nn.Embedding(\n...     num_embeddings=2000,  # #1\n...     embedding_dim=50,  # #2\n...     padding_idx=0)\n```", "```py\n>>> from nessvec.files import load_vecs_df\n>>> glove = load_vecs_df(HOME_DATA_DIR / 'glove.6B.50d.txt')\n>>> zeroes = [0.] * 50\n>>> embed = []\n>>> for tok in vocab:  # #1\n...     if tok in glove.index:\n...         embed.append(glove.loc[tok])\n...     else:\n...         embed.append(zeros.copy())  # #2\n>>> embed = np.array(embed)\n>>> embed.shape\n(4000, 50)\n```", "```py\n>>> pd.Series(vocab)\n0               a\n1              in\n2              to\n          ...\n3831         43rd\n3832    beginners\n3833        lover\nLength: 3834, dtype: object\n```", "```py\nembed = torch.Tensor(embed)  # #1\nprint(f'embed.size(): {embed.size()}')\nembed = nn.Embedding.from_pretrained(embed, freeze=False)  # #2\nprint(embed)\n```", "```py\nclass CNNTextClassifier(nn.Module):\n\n    def __init__(self, embeddings):\n        super().__init__()\n\n        self.seq_len = 40  # #1\n        self.vocab_size = 10000  # #2\n        self.embedding_size = 50  # #3\n        self.out_channels = 5  # #4\n        self.kernel_lengths = [2, 3, 4, 5, 6]  # #5\n        self.stride = 1  # #6\n        self.dropout = nn.Dropout(0)  # #7\n        self.pool_stride = self.stride  # #8\n        self.conv_out_seq_len = calc_out_seq_len(  # #9\n            seq_len=self.seq_len,\n            kernel_lengths=self.kernel_lengths,\n            stride=self.stride,\n            )\n```", "```py\ndef calc_conv_out_seq_len(seq_len, kernel_len,\n                          stride=1, dilation=1, padding=0):\n    \"\"\"\n    L_out =     (L_in + 2 * padding - dilation * (kernel_size - 1) - 1)\n            1 + _______________________________________________________\n                                        stride\n    \"\"\"\n    return (\n        1 + (seq_len +\n             2 * padding - dilation * (kernel_len - 1) - 1\n            ) //\n        stride\n        )\n```", "```py\nself.embed = nn.Embedding(\n    self.vocab_size,  # #1\n    self.embedding_size,  # #2\n    padding_idx=0)\nstate = self.embed.state_dict()\nstate['weight'] = embeddings  # #3\nself.embed.load_state_dict(state)\n```", "```py\nself.convolvers = []\nself.poolers = []\ntotal_out_len = 0\nfor i, kernel_len in enumerate(self.kernel_lengths):\n    self.convolvers.append(\n        nn.Conv1d(in_channels=self.embedding_size,\n                  out_channels=self.out_channels,\n                  kernel_size=kernel_len,\n                  stride=self.stride))\n    print(f'conv[{i}].weight.shape: {self.convolvers[-1].weight.shape}')\n    conv_output_len = calc_conv_out_seq_len(\n        seq_len=self.seq_len, kernel_len=kernel_len, stride=self.stride)\n    print(f'conv_output_len: {conv_output_len}')\n    self.poolers.append(\n        nn.MaxPool1d(kernel_size=conv_output_len, stride=self.stride))\n    total_out_len += calc_conv_out_seq_len(\n        seq_len=conv_output_len, kernel_len=conv_output_len,\n        stride=self.stride)\n    print(f'total_out_len: {total_out_len}')\n    print(f'poolers[{i}]: {self.poolers[-1]}')\nprint(f'total_out_len: {total_out_len}')\nself.linear_layer = nn.Linear(self.out_channels * total_out_len, 1)\nprint(f'linear_layer: {self.linear_layer}')\n```", "```py\nconv[0].weight.shape: torch.Size([5, 50, 2])\nconv_output_len: 39\ntotal_pool_out_len: 1\npoolers[0]: MaxPool1d(kernel_size=39, stride=1, padding=0, dilation=1,\n    ceil_mode=False)\nconv[1].weight.shape: torch.Size([5, 50, 3])\nconv_output_len: 38\ntotal_pool_out_len: 2\npoolers[1]: MaxPool1d(kernel_size=38, stride=1, padding=0, dilation=1,\n    ceil_mode=False)\nconv[2].weight.shape: torch.Size([5, 50, 4])\nconv_output_len: 37\ntotal_pool_out_len: 3\npoolers[2]: MaxPool1d(kernel_size=37, stride=1, padding=0, dilation=1,\n    ceil_mode=False)\nconv[3].weight.shape: torch.Size([5, 50, 5])\nconv_output_len: 36\ntotal_pool_out_len: 4\npoolers[3]: MaxPool1d(kernel_size=36, stride=1, padding=0, dilation=1,\n    ceil_mode=False)\nconv[4].weight.shape: torch.Size([5, 50, 6])\nconv_output_len: 35\ntotal_pool_out_len: 5\npoolers[4]: MaxPool1d(kernel_size=35, stride=1, padding=0, dilation=1,\n     ceil_mode=False)\ntotal_out_len: 5\nlinear_layer: Linear(in_features=25, out_features=1, bias=True)\n```", "```py\nEpoch:  1, loss: 0.76782, Train accuracy: 0.59028, Test accuracy: 0.64961\nEpoch:  2, loss: 0.64052, Train accuracy: 0.65947, Test accuracy: 0.67060\nEpoch:  3, loss: 0.51934, Train accuracy: 0.68632, Test accuracy: 0.68766\n...\nEpoch: 55, loss: 0.04995, Train accuracy: 0.80558, Test accuracy: 0.72966\nEpoch: 65, loss: 0.05682, Train accuracy: 0.80835, Test accuracy: 0.72178\nEpoch: 75, loss: 0.04491, Train accuracy: 0.81287, Test accuracy: 0.71522\n```", "```py\nEpoch:  1, loss: 0.61644, Train accuracy: 0.57773, Test accuracy: 0.58005\nEpoch:  2, loss: 0.52941, Train accuracy: 0.63232, Test accuracy: 0.64567\nEpoch:  3, loss: 0.45162, Train accuracy: 0.67202, Test accuracy: 0.65486\n...\nEpoch: 55, loss: 0.21011, Train accuracy: 0.79200, Test accuracy: 0.69816\nEpoch: 65, loss: 0.21707, Train accuracy: 0.79434, Test accuracy: 0.69423\nEpoch: 75, loss: 0.20077, Train accuracy: 0.79784, Test accuracy: 0.70079\n```", "```py\nout_pool_total = 0\nfor kernel_len, stride in zip(kernel_lengths, strides):\n    out_conv = (\n        (in_seq_len - dilation * (kernel_len - 1) - 1) // stride) + 1\n    out_pool = (\n        (out_conv - dilation * (kernel_len - 1) - 1) // stride) + 1\n    out_pool_total += out_pool\n```", "```py\npython train.py --dropout_portion=.35 --epochs=16 --batch_size=8 --win=True\n```", "```py\nEpoch:  1, loss: 0.44480, Train accuracy: 0.58152, Test accuracy: 0.64829\nEpoch:  2, loss: 0.27265, Train accuracy: 0.63640, Test accuracy: 0.69029\n...\nEpoch: 15, loss: 0.03373, Train accuracy: 0.83871, Test accuracy: 0.79396\nEpoch: 16, loss: 0.09545, Train accuracy: 0.84718, Test accuracy: 0.79134\n```", "```py\nlearning  seq  case vocab           training      test\n kernel_sizes    rate  len  sens  size dropout  accuracy  accuracy\n          [2]  0.0010   32 False  2000     NaN    0.5790    0.5459\n[1 2 3 4 5 6]  0.0010   40 False  2000     NaN    0.7919    0.7100\n    [2 3 4 5]  0.0015   40 False  2000     NaN    0.8038    0.7152\n[1 2 3 4 5 6]  0.0010   40  True  2000     NaN    0.7685    0.7520\n          [2]  0.0010   32  True  2000     0.2    0.8472    0.7533\n    [2 3 4 5]  0.0010   32  True  2000     0.2    0.8727    0.7900\n```"]