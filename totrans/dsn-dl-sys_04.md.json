["```py\ndef setup(rank, world_size):\n  os.environ['MASTER_ADDR'] = 'xxxx'\n  os.environ['MASTER_PORT'] = 'xxx'\n\n  # initialize the process group, \"gloo\" is one of the communication \n  # backends Pytorch supports, it also supports MPI and NCCL. \n  # rank is the process’s rank, it's a globally unique id \n  # for this process. rank=0 means  master process.\n  # world_size is the total number of processes in this training group.\n  dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n  dist.destroy_process_group()\n```", "```py\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# create model and move it to GPU\nmodel = DpModel().to(device)\n\n# wrap the model with DDP\nddp_model = DDP(model, device_ids=[rank])\noutputs = ddp_model(data)                    ❶\n\n# compute the loss and sync gradient with other workers.\n# when 'backward' function returns, the param.grad already \n# contains synchronized gradient tensor\nloss_fn(outputs, labels).backward()\n```", "```py\n# Step 1: define 'TF_CONFIG' environment variable to describe\n# the training group and the role for the process.\n# The worker array defines the IP addresses and ports of \n# all the TensorFlow servers used in this training.  \ntf_config = {\n  'cluster': {\n    'worker': ['192.168.4.53:12345', '192.168.4.55:23456']\n  },\n\n  # A 'task' provides information of the current task and is \n  # different for each worker. It specifies the 'type' and\n  # 'index' of that worker.\n  'task': {'type': 'worker', 'index': 0}\n}\n\nos.environ['TF_CONFIG'] = json.dumps(tf_config)\n\n# Step 2: define distributed training strategy,\n# the MultiWorkerMirroredStrategy takes \n# care of the synchronous data parallel distributed training.\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\nglobal_batch_size = per_worker_batch_size * num_workers\nmulti_worker_dataset = mnist.mnist_dataset(global_batch_size)\n\n# Step 3: start the distributed training.\nwith strategy.scope():\n  # Model building/compiling need to be within 'strategy.scope()'.\n  multi_worker_model = mnist.build_and_compile_cnn_model()\n\nmulti_worker_model.fit(multi_worker_dataset, \n  epochs=3, steps_per_epoch=70)\n```", "```py\nhvd.init()                                       ❶\n.. .. ..\n\n@tf.function\ndef training_step(images, labels, first_batch):\n   with tf.GradientTape() as tape:\n       probs = mnist_model(images, training=True)\n       loss_value = loss(labels, probs)\n\n   # Wrap tape with Horovod Distributed GradientTape. \n   # This gradient tape averages gradients from all \n   # workers by using allreduce or allgather, and then \n   # applies those averaged gradients back to the local model.\n   tape = hvd.DistributedGradientTape(tape)\n\n   grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n   opt.apply_gradients(zip(grads, mnist_model.trainable_variables))\n\n   # Broadcast initial variable states \n   # from rank 0 to all other processes.\n   if first_batch:\n       hvd.broadcast_variables(mnist_model.variables, root_rank=0)\n       hvd.broadcast_variables(opt.variables(), root_rank=0)\n\n   return loss_value\n\nfor batch, (images, labels) in \\                ❷\n  enumerate(dataset.take(10000 / hvd.size())): \n   loss_value = training_step(images, labels, batch == 0)\n   .. .. ..\n\n# save checkpoints only on worker 0 to \n# prevent other workers from corrupting it.\nif hvd.rank() == 0:\n   checkpoint.save(checkpoint_dir)\n```", "```py\n# Horovod: initialize Horovod.\nimport torch\nimport horovod.torch as hvd\n\n# Initialize Horovod\nhvd.init()\n.. .. .. \n\n# Build model...\nmodel = ...\noptimizer = optim.SGD(model.parameters())\n\n# Add Horovod Distributed Optimizer, this is equal\n# to hvd.DistributedGradientTape(tape) \n# for Tensorflow2 \noptimizer = hvd.DistributedOptimizer(optimizer,\n  named_parameters=model.named_parameters())\n\n# Broadcast parameters from rank 0 to \n#all other processes.\nhvd.broadcast_parameters(model.state_dict(),\n  root_rank=0)\n\nfor epoch in range(100):\n   for batch_idx, (data, target) in enumerate(train_loader):\n       optimizer.zero_grad()\n       output = model(data)\n       loss = F.nll_loss(output, target)\n       loss.backward()\n       optimizer.step()\n   .. .. ..\n```", "```py\ntf_config = {\n  'cluster': {\n    'worker': ['192.168.4.53:12345', '192.168.4.55:23456']\n  },\n\n  # A 'task' provides information of the current task \n  # and is different for each worker. It specifies \n  # the 'type' and 'index' of that worker.\n  'task': {'type': 'worker', 'index': 0}\n}\n```", "```py\n$ docker build -t orca3/services:latest -f services.dockerfile .\n$ docker run --name training-service -v \\\n    $HOME/.kube/config:/.kube/config --env \\     ❶\n    APP_CONFIG=config-kube.properties \\\n    --network orca3 --rm -d -p \n  \"${TS_PORT}\":51001 \n  orca3/services:latest training-service.jar\n```", "```py\n# submit a distributed training request\n$ grpcurl -plaintext -d \"{ \"metadata\": \n  { \"algorithm\":\"intent-classification\",\n    \"dataset_id\":\"1\",\n    \"Name\":\"test1\",\n    \"train_data_version_hash\":\"hashBA==\",\n    \"Parameters\":{\n      \"LR\":\"4\",\"EPOCHS\":\"15\",\n      \"PARALLEL_INSTANCES\":\"3\",         ❶\n    \"BATCH_SIZE\":\"64\",\"FC_SIZE\":\"128\"}}\n  }\" \n ${TS_SERVER}:${TS_PORT} \ntraining.TrainingService/Train \n```", "```py\ngrpcurl -plaintext -d \"{\"job_id\": \"$1\"}\"    ❶\n ${TS_SERVER}:\"${TS_PORT}\" \ntraining.TrainingService/GetTrainingStatus \n```", "```py\n# check Kubernetes resources status. \n# We could see a distributed training group contains \n# with three pods and one service are created in Kubernetes\n$ kubectl get all -n orca3\nNAME                                   READY   STATUS\npod/intent-classification-1-1-worker   0/1     Completed          ❶\npod/intent-classification-1-2-worker   0/1     Completed\npod/intent-classification-1-master     0/1     Completed          ❷\n\nNAME                                             TYPE       .. ..   \nservice/intent-classification-1-master-service   ClusterIP        ❸\n```", "```py\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'xxx xxx xxx xxx'\n    os.environ['MASTER_PORT'] = '12356'\n    dist.init_process_group(\"gloo\",       ❶\n      rank=rank, world_size=world_size)   ❶\n```", "```py\nMaster Pod: \n  WORLD_SIZE:3; RANK:0, \n  MASTER_ADDR: intent-classification-1-master-service, \n  MASTER_PORT: 12356\nWorker Pod 1: \n  WORLD_SIZE:3; RANK:1, \n  MASTER_ADDR: intent-classification-1-master-service, \n  MASTER_PORT: 12356\nWorker Pod 2: \n  WORLD_SIZE:3; RANK:2, \n  MASTER_ADDR: intent-classification-1-master-service, \n  MASTER_PORT: 12356\n```", "```py\nprotected List<String> launchTrainingPods(\n  int jobId, int worldSize, TrainingJobMetadata metadata, .. ..) {\n  .. .. ..\n\n  // It's a distributed training if the worldSize is greater than 1.\n  if (worldSize > 1) {                                              ❶\n    // .. .. .. \n    api.createNamespacedService(                                    ❷\n      config.kubeNamespace, serviceBody,                            ❷\n      null, null, null);                                            ❷\n\n    serviceTracker.add(masterServiceName);\n    logger.info(String.format(\"Launched master service %s\", masterServiceName));\n       .. .. ..\n  }\n\n  // create training pods definition\n  for (int rank = 0; rank < worldSize; rank++) {\n    envs.put(\"WORLD_SIZE\", Integer.toString(worldSize));            ❸\n    // RANK 0 is master\n    envs.put(\"RANK\", Integer.toString(rank));                       ❸\n    envs.put(\"MASTER_ADDR\", masterPodDnsName);                      ❸\n    envs.put(\"MASTER_PORT\", Integer.toString(masterPort));          ❸\n\n    V1PodSpec podSpec = new V1PodSpec()                             ❹\n      .restartPolicy(\"Never\")                                       ❹\n      .addContainersItem(new V1Container()                          ❹\n        .image(algorithmToImage(                                    ❹\n           metadata.getAlgorithm()))                                ❹\n        .env(envVarsToList(envs)) .. .. ..\n\n    String workerPodName = rank == 0 ? masterPodName :\n      String.format(\"job-%d-%d-%s-worker-%d\", jobId, \n        now, metadata.getName(), rank);\n    V1Pod workerPodBody = new V1Pod();\n    workerPodBody.apiVersion(\"v1\");\n       .. .. ..\n\n    // (3) \n    api.createNamespacedPod(config.kubeNamespace,                   ❺\n      workerPodBody, null, null, null);                             ❺\n       .. .. ..\n  }\n  return podNames;\n}\n```", "```py\ndef should_distribute():\n   return dist.is_available() and config.WORLD_SIZE > 1\n\ndef is_distributed():\n   return dist.is_available() and dist.is_initialized()\n\nif should_distribute():\n   # initialize the distributed process group, \n   # wait until all works are ready. \n   dist.init_process_group(\"gloo\", \n     rank=config.RANK, world_size=config.WORLD_SIZE)\n\nif is_distributed():\n   # wrap the model with DistributedDataParallel (DDP) \n   # package to enable data parallel training. \n   model = DDP(model)\n\nif is_distributed():\n   # restricts data loading to a subset of the dataset \n   # exclusive to the current process\n   train_sampler = DistributedSampler(\n     dataset=split_train_, num_replicas=config.WORLD_SIZE,\n     rank=config.RANK)\n```", "```py\nif config.RANK == 0:                       ❶\n   accu_test = evaluate(test_dataloader)\n   .. .. ..\n   # upload model to metadata store. \n   artifact = orca3_utils.create_artifact(\n     config.MODEL_BUCKET, config.MODEL_OBJECT_NAME)\n   .. .. .. \n```", "```py\ngpu1 = 1\ngpu2 = 2\n\nclass a_large_model(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n    # initialize the network as two subnetworks.\n    self.subnet1 = ...\n    self.subnet2 = ...\n\n    # put subnetwork 1 and 2 to two different GPUs \n    self.subnet1.cuda(gpu1)\n    self.subnet2.cuda(gpu2)\n\n  def forward(x):\n    # load data to GPU 1 and calculate output for \n    # subnet 1, GPU 2 is idle at the moment.\n    x = x.cuda(gpu1)\n    x = self.subnet1(x)\n\n    # move the output of subnet 1 to GPU 2 and calculate\n    # output for subnet 2\\. GPU 1 is idle\n    x = x.cuda(gpu2)\n    x = self.sub_network2(x)\n    return x\n```", "```py\n## Part One: initialize remote communication \n# for multiple machines \nrpc.init_rpc(\n  name=\"worker\",\n  # set rank number to this node, rank is the global \n  # unique id of a node, 0 is the master, \n  # other ranks are observers\n  rank=0,\n\n  # set the number of workers in the group\n  world_size=1,\n    .. .. ..\n)\n\n.. .. ..\n\n## Part Two: split model to 2 subnetworks, load \n# to different GPUs and initialize the pipeline.\n\nnum_gpus = 2\npartition_len = ((nlayers - 1) // num_gpus) + 1\n\n# Add all the necessary transformer blocks.\nfor i in range(nlayers):\n  transformer_block = TransformerEncoderLayer(emsize, \n    nhead, nhid, dropout)\n    .. .. ..\n\n  # Load first half encoder layers to GPU 0 and second hard encoder layers to GPU 1.\n  device = i // (partition_len)\n  tmp_list.append(transformer_block.to(device))\n\n# Load decoder to GPU 1.\ntmp_list.append(Decoder(ntokens, emsize).cuda(num_gpus - 1))\nmodule_list.append(nn.Sequential(*tmp_list))\n\n## Part Three: Build up the pipeline.\nchunks = 8 # Set micro-batches number to 8.\nmodel = Pipe(torch.nn.Sequential(*module_list), chunks = chunks)\n\n.. .. ..\n\n## Part 4: Train with pipeline\ndef train():\n  model.train() # Turn on the train mode\n    .. .. ..\n\n  for batch, i in enumerate(range(0, nbatches, bptt)):\n    data, targets = get_batch(train_data, i)\n    optimizer.zero_grad()\n\n    # Compute pipeline output,by following the pipeline setup,\n    # the Pytorch framework will coordinate the network computation \n    # between GPU 0 and GPU 1.\n    # Since the Pipe is only within a single host and process the \"RRef\"\n    # returned by forward method is local to this node and can simply\n    # retrieved via \"RRef.local_value()\".\n    output = model(data).local_value()\n\n    # Compute the loss on GPU 1.\n    # Need to move targets to the device where the output of the\n    # pipeline resides.\n    loss = criterion(output.view(-1, ntokens), targets.cuda(1))\n\n    # Backprop and model parameters update are the same as single GPU training.\n    # The Pytorch framework hides all the details of micro-batches \n    # computation and model parameters update. \n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    optimizer.step()\n\n.. .. ..     \n```"]