- en: 2 Managing Generative AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 管理生成型 AI
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Understanding how to access and interact with AI models in the most productive
    ways possible
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握如何以最高效的方式访问和使用 AI 模型进行交互
- en: Configuring models to provide the best fit possible for your specific needs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置模型以尽可能地满足你的具体需求
- en: Leveraging the OpenAI Playground to better understand key tools for controlling
    AI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 OpenAI Playground 来更好地了解控制 AI 的关键工具
- en: Throwing clever prompts at an AI chat interface can definitely produce impressive
    results. But by ignoring the finer points of model configuration, you’ll be missing
    out on most of your AI’s potential value. So with this chapter, we’ll begin the
    process of figuring out which dials to turn and how far to turn them. (And which
    big red button you should absolutely avoid!)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 把巧妙的提示投给一个 AI 聊天界面肯定会产生令人印象深刻的结果。但是如果忽视模型配置中的细节，你就会错过大部分 AI 的潜在价值。所以在本章中，我们将开始找出应该转动哪些旋钮以及转动到什么程度的过程。（以及你绝对应该避免的哪个大红按钮！）
- en: Of course for all I know, by the time you get around to reading this, the whole
    process might be automated. That blue light gently pulsing on your keyboard that
    you can’t resist staring at? That would be the brainwave scanner GPT now uses
    to directly download your innermost goals and desires.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我不知道，等你读到这里，整个过程可能已经自动化了。你键盘上轻轻闪烁的蓝色光？那将是 GPT 现在用来直接下载你最内心的目标和欲望的脑波扫描仪。
- en: Your results are available now.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果现在可以使用了。
- en: 2.1 Accessing GPT models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 访问 GPT 模型
- en: In case you haven’t yet had the pleasure, most popular interactive AI platforms
    require you to create an account before trying them out. For OpenAI’s ChatGPT,
    [that’ll happen at chat.openai.com/auth/login](auth.html). Even if you’re asked
    to provide credit card information, you’ll be given plenty of warning before you’re
    actually billed. Just don’t ignore those warnings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这个荣幸，大多数流行的交互式 AI 平台在你尝试它们之前都需要你创建一个帐户。对于 OpenAI 的 ChatGPT，[你可以在 chat.openai.com/auth/login](auth.html)
    进行注册。即使你被要求提供信用卡信息，你在实际上被收费之前会有足够的警告。只要别忽略了这些警告。
- en: Once you’re signed up, [the ChatGPT interface is here](chat.openai.com.html).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你注册成功，[ChatGPT 的界面在这里](chat.openai.com.html)。
- en: Now what?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: Besides ChatGPT, it doesn’t require a PhD in electrical engineering to realize
    that Microsoft’s Bing search engine gives you access to GPT-4 from within the
    Edge browser. I’d mention Google’s GPT-competitor, Bard here, too, but I’m in
    Canada. Bard doesn’t happen here, yet. In fact, you can expect to come across
    all kinds of geographic, commercial, or even technological restrictions on how
    and when you can access various AI services as they evolve. Be patient and flexible
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 ChatGPT，你不需要拥有电气工程博士学位就可以意识到，微软的必应搜索引擎让你可以在 Edge 浏览器内访问 GPT-4。我想在这里也提一下谷歌的
    GPT 竞争对手 Bard，但是我在加拿大。Bard 在这里还没有出现。事实上，你会发现在不同的地理、商业或者技术上，对于访问不同的 AI 服务会有各种各样的限制。要耐心和灵活。
- en: Besides those web services that are hosted directly by their creators, there
    are also plenty of third-party web projects like [ChatPDF (for analyzing PDF documents)](www.chatpdf.com.html)
    and [Rytr (for generating written content)](rytr.me.html) that offer nice managed
    applications for specialized use-cases. There’s nothing particularly complicated
    about putting those to use either. We will talk more about those kinds of services
    later.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了那些由它们的创建者直接提供的网络服务，还有很多第三方网络项目，比如[ChatPDF（用于分析 PDF 文档）](www.chatpdf.com.html)和[Rytr（用于生成写作内容）](rytr.me.html)，它们提供了优秀的托管应用程序，适用于各种特殊用例。对于使用这些工具来说，并没有什么特别复杂的。我们以后会更详细地讨论这些服务。
- en: But all that’s consumer-quality stuff. It’s OK. But the serious work, you might
    say, is happening "off-campus". Meaning, high-productivity interactions like carefully
    configuring your model, letting your AI loose on large stores of your *own* data
    or automating multiple rounds of prompts and completions, and then incorporating
    the responses into your code-driven workflow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但是那些都是消费级的东西。没关系。但是你可能会说，严肃的工作是在“校外”发生的。也就是说，高生产力的交互，比如仔细配置你的模型，让你的 AI 在大量的*自己的*数据中自由运行，或者自动化多轮的提示和完成，并将回答融入到你的代码驱动工作流程中。
- en: No matter which models you’re using, *this* kind of access will happen through
    an application programming interface (API). As illustrated in the figure below,
    an API serves as a bridge between software applications, allowing them to communicate
    and interact. It defines a set of rules and protocols that allow one application
    to request services or data from another. APIs provide developers who have appropriate
    authorization to access specific functionalities of a service. They enable seamless
    integration by specifying how requests should be structured and how responses
    will be formatted.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用的是哪种模型，*这*种访问都将通过应用程序编程接口（API）进行。如下图所示，API 充当软件应用程序之间的桥梁，允许它们进行通信和交互。它定义了一组规则和协议，允许一个应用程序向另一个应用程序请求服务或数据。API
    提供给具有适当授权的开发人员访问服务的特定功能。它们通过指定请求应如何结构化以及响应将如何格式化来实现无缝集成。
- en: Figure 2.1 A typical API architecture
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.1 典型的 API 架构
- en: '![gai 2 1](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![gai 2 1](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-1.png)'
- en: A lot of the technical examples you’ll see later in the book will happen through
    APIs. For practical reasons, those will mostly use OpenAI models and infrastructure.
    But the broad underlying methodologies should mostly apply to other services,
    too (once they become widely available).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书后面你将看到的很多技术示例将通过 API 实现。出于实际原因，这些示例大多将使用 OpenAI 的模型和基础设施。但是广义的基础方法应该也大多适用于其他服务（一旦它们变得广泛可用）。
- en: So the road to coding your AI leads through APIs. But don’t worry if you’ve
    never done this kind of thing before. I’ll give you all the background and technical
    details you’ll need to make everything work just fine.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以编写自己的 AI 的道路通过 API。但是如果你以前从未做过这种事情，不用担心。我会为你提供所有背景和技术细节，以确保一切都能正常运行。
- en: Before we go *there*, though, we should check out [OpenAI’s Playground](platform.openai.com.html).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们*进入*那里之前，我们应该先查看 [OpenAI 的 Playground](platform.openai.com.html)。
- en: 2.2 Learning by "playing"
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 通过“游戏”学习
- en: Playground, shown in the figure below, existed even before ChatGPT, and it was
    where I got my first interactions with GPT. Although do keep in mind that, along
    with everything else in the AI world, the interface will probably have changed
    at least twice by the time you get to it. We’re going to use the playground throughout
    this chapter to learn how to interact with GPT.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Playground，如下图所示，甚至在 ChatGPT 之前就已经存在了，那是我第一次与 GPT 进行互动的地方。尽管要记住，与 AI 世界中的其他一切一样，界面可能在你使用之前至少会变化两次。我们将在本章中使用
    Playground 来学习如何与 GPT 互动。
- en: Figure 2.2 OpenAI’s Playground interface
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.2 OpenAI 的 Playground 界面
- en: '![gai 2 2](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![gai 2 2](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-2.png)'
- en: You get to Playground from your OpenAI login account. Rather than enjoying a
    sustained conversation where subsequent exchanges are informed by earlier prompts
    and completions, the default text field in Playground offers only one exchange
    at a time. The models it’s based on might also be a bit older and less refined
    than the ChatGPT version.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过你的 OpenAI 登录账户进入 Playground。与之前的持续对话不同，后续交互不会受到先前提示和完成的影响，Playground 中的默认文本字段一次只提供一个交换。其基于的模型可能也比
    ChatGPT 版本要旧一些，也不太精细。
- en: But there are two things that set Playground apart from ChatGPT. One is the
    configuration controls displayed down the right side of the screen in the image
    above, and the second is the *View code* feature at the top-right. It’s those
    features that make Playground primarily an educational tool rather than just another
    GPT interface.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有两个方面使得 Playground 与 ChatGPT 不同。一个是屏幕右侧显示的配置控件，如上图所示，第二个是右上角的*查看代码*功能。正是这些功能使
    Playground 主要成为一个教育工具，而不仅仅是另一个 GPT 接口。
- en: 2.3 Accessing Python code samples
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 获取 Python 代码示例
- en: We’re going to look at those features one at a time in the next section of this
    chapter. But, accessing the GPT API using code is what will probably give you
    the greatest value over the long term, I really want to show you what *View code*
    is all about right away. The image below shows a typical Playground session where
    I’ve typed in a prompt and then hit the "View code" button with the "Python" option
    selected. I’m shown working code that, assuming you’ll add a valid OpenAI API
    key on line 4, can be copied and run from any internet-connected computer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的下一部分逐个查看这些功能。但是，使用代码访问 GPT API 可能会在长期内为您提供最大的价值，我真的想立即向您展示*查看代码*的功能是什么。下面的图像显示了一个典型的
    Playground 会话，我在其中键入了提示，然后点击了“查看代码”按钮，选择了“Python”选项。我看到的是可工作的代码，假设您在第 4 行添加了有效的
    OpenAI API 密钥，可以从任何连接到互联网的计算机上复制并运行。
- en: Figure 2.3 Playground’s View code tool with Python code
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.3 Playground 的查看代码工具显示的 Python 代码
- en: '![gai 2 3](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![gai 2 3](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-3.png)'
- en: Don’t worry about the details right now, but take a moment to look through the
    arguments that are included in the `openai.Completion.create()` method. The model
    that’s currently selected in the Model field on the right side of the Playground
    is there (`text-davinci-003`), as is my actual prompt (`Explain the purpose of…​`).
    In fact, each configuration option I’ve selected is there. In other words, I can
    experiment with any combination of configurations here in the Playground, and
    then copy the code and run it - or variations of it - anywhere.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在不要担心细节，但请花点时间浏览包含在`openai.Completion.create()`方法中的参数。右侧游乐场中当前选择的模型在“模型”字段中（`text-davinci-003`），“Explain
    the purpose of…​”实际提示也在其中。事实上，我选择的每个配置选项都在其中。换句话说，我可以在游乐场中尝试任何组合的配置，然后复制代码并在任何地方运行它
    - 或其变体。
- en: This, in fact, is where you learn how to use the API.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这正是您学习如何使用 API 的地方。
- en: 2.4 Accessing CURL code samples
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 访问 CURL 代码示例
- en: The next image shows us how that exact same prompt would work if I decided to
    use the command line tool, curl, instead of Python. Besides Python and curl, you
    can also display code in node.js and JSON.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图像向我们展示了如果我决定使用命令行工具 curl 而不是 Python，那么完全相同的提示会如何工作。除了 Python 和 curl，您还可以在
    node.js 和 JSON 中显示代码。
- en: Figure 2.4 Playground’s View code tool with curl code
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.4 Playground 的查看代码工具显示的 curl 代码
- en: '![gai 2 4](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-4.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![gai 2 4](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-4.png)'
- en: '`curl` is a venerable open source command line tool that’s often available
    by default. To confirm it’s available on your system, simply type `curl` at any
    command line prompt. You should see some kind of help message with suggestions
    for proper usage.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl`是一个备受尊敬的开源命令行工具，通常默认可用。要确认它在您的系统上可用，请简单地在任何命令行提示符下键入`curl`。您应该会看到一些帮助消息，其中包含了正确使用的建议。'
- en: One more thing. The table below shows each available OpenAI model along with
    their associated API endpoints. An endpoint is an address that can be used within
    your code to access a resource. Besides the value of having that information,
    this is also important because it shows us the *kinds* of prompts you can send.
    Besides the `completions` operations that you’d expect, there’s also `edits`,
    `transcriptions`, `translations`, `fine-tunes`, `embeddings`, and `moderations`.
    We’ll talk more about using those later in the book. But do keep them all in mind.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事。下表显示了每个可用的 OpenAI 模型以及它们关联的 API 终端点。端点是您在代码中可以使用的地址，用于访问资源。除了具有这些信息的价值之外，这也很重要，因为它向我们展示了您可以发送的*种类*提示。除了您预期的
    `completions` 操作之外，还有 `edits`、`transcriptions`、`translations`、`fine-tunes`、`embeddings`
    和 `moderations`。我们将在本书后面更多地讨论如何使用它们。但请记住所有这些。
- en: '| ENDPOINT | MODEL NAME |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 终端点 | 模型名称 |'
- en: '| /v1/chat/completions | gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo,
    gpt-3.5-turbo-0301 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| /v1/chat/completions | gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo,
    gpt-3.5-turbo-0301 |'
- en: '| /v1/completions | text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001,
    text-ada-001 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| /v1/completions | text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001,
    text-ada-001 |'
- en: '| /v1/edits | text-davinci-edit-001, code-davinci-edit-001 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| /v1/edits | text-davinci-edit-001, code-davinci-edit-001 |'
- en: '| /v1/audio/transcriptions | whisper-1 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| /v1/audio/transcriptions | whisper-1 |'
- en: '| /v1/audio/translations | whisper-1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| /v1/audio/translations | whisper-1 |'
- en: '| /v1/fine-tunes | davinci, curie, babbage, ada |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| /v1/fine-tunes | davinci, curie, babbage, ada |'
- en: '| /v1/embeddings | text-embedding-ada-002, text-search-ada-doc-001 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| /v1/embeddings | text-embedding-ada-002, text-search-ada-doc-001 |'
- en: '| /v1/moderations | text-moderation-stable, text-moderation-latest |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: Takeaway
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: No matter which AI you’re using, make sure you understand all available options
    so you can optimize your environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Completion configurations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You could think of "completion configurations" as a kind of fine tuning, and
    you wouldn’t be wrong. However, in the context of AI, the term "fine tuning" can
    have far more specific meanings. We’ll spend more time discussing that topic in
    chapters 6 and 9.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Before I start explaining the way each of these configurations works, here’s
    an image that should help you visualize what an AI model might do to your prompt
    before spitting out a response.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 How parameters are applied to an AI prompt
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 5](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: As you can see, a typical language model might immediately produce a tentative,
    draft response ("Generate Output" in the diagram). But, before sharing it with
    you ("Return Output"), it’ll first test it for compliance with any preferences
    (i.e,. temperature, frequency, etc.) you might have set. Those preferences - which
    we’ll soon see in action - can control a prompt’s tone, creativity, focus, verbosity,
    and even cost.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Now here’s what those controls look like in the Playground.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 Upper selection of tools in the Playground UI
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 6](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s see what those are really all about. Just to keep you oriented, we’ll
    cover each of these configuration categories:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top P value
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop sequences
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency penalty
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presence penalty
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best of
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inject start text
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to give you some context, imagine that you’re building a web application
    that provides on-demand responses to user questions about your hotel. You might
    initially ask your users to select from a list of categories ("dining and entertainment",
    "trip planning", and so on). Based on the category they choose, you want to fine
    tune your chat tool so that the responses are a good match for the users' expectations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: These configurations can help you create just such an customization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Setting the mode
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Depending on the model you’re using, the **Mode** drop down might have three
    options: Chat, Complete, and Edit. Complete and Chat provide the familiar completion
    and chat experiences that we’re already used to. If you have some text you’d like
    to rework in one way or another, you can select the Edit mode, enter your text
    in the Input section, and what you’d like changed in the text in the Instructions
    field below. The Playground suggests "Fix the grammar" as a possible instruction,
    but you’re free to use your own. We’ll learn more about this kind of operation
    in the next chapter.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The pre-built list in the **Model** drop-down displays all the OpenAI models
    that are available for the currently-selected Mode. This list will change dynamically
    based on the Mode you’ve selected. You’ll obviously want to select a model that’s
    the best fit for your needs and budget (as described above).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**下拉菜单中预置的列表显示了当前选定模式下可用的所有OpenAI模型。此列表将根据您选择的模式动态变化。显然，您会想要选择一个最适合您的需求和预算的模型（如上所述）。'
- en: 2.5.2 Setting the temperature
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 设置温度
- en: '**Temperature** controls the amount of randomness used in the generation of
    output. It allows the user to adjust the level of creativity and unpredictability
    in the output. By setting the temperature to a value between `0` and `2` (the
    default is `1`), the AI can generate different types of output, ranging from highly
    creative and unpredictable (i.e., 2) to highly structured and conservative (i.e,.
    `0`).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**温度**控制生成输出时所使用的随机性程度。它允许用户调整输出中的创造性和不可预测性水平。通过将温度设置为介于`0`和`2`之间的值（默认为`1`），AI可以生成不同类型的输出，从高度创造性和不可预测性（即2）到高度结构化和保守性（即`0`）。'
- en: 'Let’s consider an example using a chatbot scenario where a user asks for a
    restaurant recommendation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个使用聊天机器人场景的例子，其中用户询问餐厅推荐：
- en: Prompt
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: 'User: "Can you suggest a good restaurant nearby?"'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：“你能推荐附近的好餐厅吗？”
- en: 'We can use different temperature settings to generate responses with varying
    degrees of randomness. A higher temperature (e.g., 1.3) encourages the LM to produce
    more diverse and creative responses, even if they might be less focused or accurate:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的温度设置来生成具有不同随机性程度的响应。更高的温度（例如1.3）鼓励语言模型生成更多样化和创造性的响应，即使它们可能不太集中或准确：
- en: Response
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 响应
- en: 'Temperature 1.3: Assistant: "Sure! How about trying the cozy little café around
    the corner? They serve delicious food, from gourmet burgers to exotic sushi. You’ll
    definitely have an adventurous dining experience there!"'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 温度1.3：助手：“当然！试试角落里那家舒适的小咖啡馆怎么样？他们供应美味的食物，从美味的汉堡到异国情调的寿司。那里的用餐体验绝对让你大开眼界！”
- en: In this case, the high temperature setting led to a more imaginative response,
    suggesting a variety of food options and emphasizing an "adventurous dining experience."
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，高温设置导致了一个更有想象力的回应，提出了各种食品选择，并强调了“冒险的用餐体验”。
- en: 'On the other hand, a lower temperature (e.g., 0.2) encourages the LM to produce
    more deterministic and focused responses, often sticking to safe and conventional
    choices:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，较低的温度（例如0.2）鼓励语言模型生成更确定性和集中的回应，通常固守安全和传统的选择：
- en: Response
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 响应
- en: 'Temperature 0.2: Assistant: "Sure! I recommend the Italian restaurant on Main
    Street. They have authentic pasta dishes and mouthwatering pizzas. You won’t be
    disappointed!"'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 温度0.2：助手：“当然！我建议你去主街上的意大利餐厅。那里有正宗的意大利面和令人垂涎的披萨。你绝对不会失望！”
- en: With the lower temperature, the response is more straightforward and focuses
    on a specific cuisine and establishment, providing a safe and reliable recommendation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过降低温度，回应更为直接，并专注于特定的烹饪风格和场所，提供了一个安全可靠的建议。
- en: 2.5.3 Setting the Top P value
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 设置Top P值
- en: '**Top P** (also known as nucleus sampling or "penalty-free sampling") is a
    parameter that helps control the diversity of the generated text. It involves
    selecting the most likely words for the next token in a sequence, considering
    a cumulative probability threshold (P). Instead of generating from its entire
    vocabulary, it narrows down choices to a subset of the most probable words whose
    cumulative probabilities exceed P. This helps in controlling the output’s length
    and diversity while maintaining coherence.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**Top P**（也称为核采样或“无惩罚采样”）是帮助控制生成文本多样性的参数。它涉及在序列中选择下一个标记的最有可能的单词，考虑累积概率阈值（P）。它不是从整个词汇表中生成，而是将选择范围缩小到那些累积概率超过P的最有可能的单词的子集。这有助于控制输出的长度和多样性，同时保持连贯性。'
- en: When using the Top P sampling approach, the model selects the most likely next
    word based on a probability threshold (p), which allows for more diverse and controlled
    output. Words with cumulative probabilities that exceed the threshold are considered,
    ensuring a balance between predictability and creativity in generated text. In
    simpler terms, "top p" allows the model to think beyond the obvious choices while
    still staying coherent. It’s like giving the AI a bit of freedom to surprise you
    while ensuring the generated text remains meaningful. This technique is particularly
    useful when you want creative responses that don’t completely deviate from the
    topic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Top P抽样方法时，模型根据概率阈值（p）选择最有可能的下一个词，这允许更多样化和受控制的输出。累积概率超过阈值的单词将被考虑，确保在生成的文本中平衡可预测性和创造性。简单来说，“top
    p”允许模型超越明显的选择，同时保持连贯性。这就像给AI一点自由来给你带来惊喜，同时确保生成的文本仍然有意义。当您需要创意回复而不完全偏离主题时，这种技术特别有用。
- en: By adjusting the Top P value, users can influence the amount of randomness or
    diversity in the model’s responses. A higher value (e.g., `0.9`) includes a larger
    set of potential tokens, making the output more diverse, while a lower value (e.g.,
    `0.1`) narrows down the choices, resulting in more focused and deterministic outputs.
    The default Top P value is `1`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整Top P值，用户可以影响模型响应中的随机性或多样性。较高的值（例如，`0.9`）包括更大的潜在标记集，使输出更加多样化，而较低的值（例如，`0.1`）限制了选择，导致更加聚焦和确定性的输出。默认的Top
    P值为`1`。
- en: 'Here’s an example where a user asks for information about the weather:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用户询问天气信息的例子：
- en: Prompt
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: 'User: "What’s the weather like today?"'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：“今天的天气怎么样？”
- en: We can explore the effect of different Top P values on the response generation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以探索不同的Top P值对响应生成的影响。
- en: Response
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 响应
- en: 'Top P 0.2 (Highly Focused): Assistant: "The weather today is sunny with a high
    of 25 degrees Celsius. It’s a perfect day to enjoy outdoor activities."'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Top P 0.2（高度聚焦）：助手：“今天的天气是晴朗的，最高气温为摄氏25度。是享受户外活动的完美天气。”
- en: With a low Top P value, the model selects only the most likely next words, resulting
    in focused and deterministic responses that are generally more reliable but less
    varied.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较低的Top P值，模型仅选择最可能的下一个词，导致聚焦和确定性的响应，通常更可靠但变化较小。
- en: Response
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 响应
- en: 'Top P 0.8 (More Diverse): Assistant: "Today’s weather is sunny, warm, and delightful.
    You might want to grab your sunglasses and head out for a fun-filled day!"'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Top P 0.8（更多样化）：助手：“今天的天气晴朗、温暖、令人愉快。你可能想戴上太阳镜，出去享受充满乐趣的一天！”
- en: With a higher Top P value, the model considers a broader range of likely next
    words, resulting in more diverse responses. This can introduce variations in the
    generated text, offering different ways of expressing the same information.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较高的Top P值，模型会考虑更广泛范围的可能的下一个词，从而产生更多样化的响应。这可以在生成的文本中引入变化，提供表达相同信息的不同方式。
- en: 'While Top P and temperature seem similar, Temperature controls the randomness
    in language generation: higher values (e.g., 1.0) increase diversity, lower values
    (e.g., 0.2) produce more predictable output. By contrast, Top-p sampling sets
    a probability threshold (p) to select from a subset of most probable words, adjusting
    output diversity based on the threshold (e.g., p=0.9 for more diversity, p=0.2
    for less).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Top P和Temperature看起来相似，但Temperature控制语言生成中的随机性：较高的值（例如，1.0）增加多样性，较低的值（例如，0.2）产生更可预测的输出。相反，Top-p抽样设置概率阈值（p）以从最有可能的词组中进行选择，根据阈值（例如，p=0.9用于更多多样性，p=0.2用于较少多样性）调整输出的多样性。
- en: As a rule, one can alter Temperature *or* Top P, but not both.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一般情况下，可以更改Temperature *或* Top P，但不能同时更改两者。
- en: 2.5.4 Working with stop sequences
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 使用停止序列
- en: '**Stop sequences** are special tokens used to indicate the end of a generated
    response or to prompt the model to stop generating further text. These tokens
    are typically added to the input prompt to control the length of the generated
    output.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**停止序列**是用于指示生成的响应结束或提示模型停止生成进一步文本的特殊标记。这些标记通常添加到输入提示中以控制生成的输出长度。'
- en: In English, a common example of an end-of-sentence token is the period (.),
    followed by a space. In programming languages, a common stop sequence might be
    a pair of curly braces ("{}") to indicate the end of a code block. In HTML or
    XML, a closing tag like "</tag>" signals the end of an element.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that modern language models like GPT do not necessarily
    rely on explicit stop sequences to generate text. Instead, they often use techniques
    like tokenization and context window to understand where to naturally conclude
    sentences or paragraphs. Additionally, they may use special tokens like "<eos>"
    (end of sequence) internally, but these details are typically abstracted away
    from the user.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the middle of explaining a lesson, a virtual instructor could
    be configured to say, “Let’s stop here for today. Repeat back to me the three
    main points of this lesson.” After the student responds with the three main points
    (matching the required stop sequence), the instructor could say, “Yes, excellent!
    Alright, that concludes our session today.”
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.5 Setting the frequency penalty
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **frequency penalty** is used to discourage repetitive or redundant output.
    It helps address a common issue where models tend to produce repetitive or excessively
    long phrases, leading to less coherent and less diverse responses. By applying
    a frequency penalty, users can control the trade-off between generating diverse
    responses and avoiding repetitive patterns.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The penalty is calculated based on the number of times a generated token or
    sequence of tokens has already appeared in the output. When the penalty is applied,
    the model’s probability distribution over tokens is adjusted to decrease the likelihood
    of generating tokens that have already been used. The purpose of the frequency
    penalty is to encourage the model to explore alternative options and generate
    more varied and coherent responses.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The penalty value (which can lie between `-2.0` and `2.0`) can be adjusted based
    on the desired level of repetition avoidance. Higher penalty values result in
    stronger discouragement of repetition, while lower values allow for more flexibility
    in the generated output. Finding the right balance is important to ensure the
    generated text remains coherent and contextually relevant while avoiding excessive
    repetition.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example where a user asks for some ideas for a new smartphone:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and response
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'User: Generate some ideas for a new smartphone.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: Sure! Here are a few ideas: a high-resolution display, multiple camera
    lenses, fast processing speed, long battery life, and a sleek design.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'That language model generated ideas that are already common - it’s hardly ground
    breaking. However, if you add a frequency penalty:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and response
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'User: Generate some ideas for a new smartphone while using a frequency penalty
    to encourage unique and rare features.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: Certainly! How about a smartphone with a holographic display that projects
    3D images, a graphene-based battery for instant charging, a customizable modular
    design allowing users to swap hardware components, a built-in AI language translator
    for real-time conversations, and a biometric authentication system that reads
    unique vein patterns in the user’s palm?'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 模型：当然！如何把智能手机和全息显示器结合起来，以投射3D图像，采用石墨烯电池实现即时充电，设计可定制的模块化结构，使用户可以更换硬件组件，内置AI语言翻译器实现实时对话，以及读取用户手掌上独特静脉纹理的生物识别系统？
- en: In this case, by using a frequency penalty, the model is more likely to generate
    ideas that involve less common words or concepts. This results in suggestions
    that are more innovative and imaginative, aligning with your goal of coming up
    with unique product features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，通过使用频率惩罚，模型更有可能生成涉及不太常见的单词或概念的想法。这导致了更具创新性和想象力的建议，符合您提出独特产品特性的目标。
- en: 2.5.6 Setting the presence penalty
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.6 设置存在惩罚
- en: The **presence penalty** is a parameter in GPT models that helps control the
    repetition of phrases and words in the generated text. It’s designed to prevent
    the model from repeating the same phrases or words too often in the generated
    output. The presence penalty modifies the probability distribution to make less
    likely words that were present in the input prompt or seed text. This encourages
    the model to generate words and cover topics that were not explicit in the input.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**存在惩罚** 是GPT模型中的一个参数，有助于控制生成文本中短语和单词的重复。它旨在防止模型在生成的输出中过于频繁地重复相同的短语或单词。存在惩罚修改概率分布，以减少输入提示或种子文本中存在的单词。这鼓励模型生成未明确存在于输入中的单词和涉及的主题。'
- en: Although the `presence` and `frequency` penalties do seem to overlap, they’re
    not identical. The frequency penalty is a contribution that is proportional to
    how often a specific token has already been sampled. It discourages the model
    from repeating the same words or phrases too frequently within the generated text.
    On the other hand, the presence penalty is a one-time, additive contribution that
    applies to all tokens that have been sampled at least once. It modifies the probability
    distribution to make less likely words that were present in the input prompt or
    seed text. This encourages the model to generate words that were not in the input.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`存在`和`频率`惩罚似乎有重叠之处，但它们并不相同。频率惩罚是与特定标记已被采样的频率成比例的贡献。它阻止模型在生成文本中过于频繁地重复相同的单词或短语。另一方面，存在惩罚是一次性的、增加的贡献，适用于至少被采样一次的所有标记。它修改概率分布，以减少输入提示或种子文本中存在的单词。这鼓励模型生成未在输入中的单词。
- en: 'We can explore the effect of different presence penalty values on the response
    generation while comparing it with frequency penalty. Imagine you prompted the
    model with: "Tell me about the recent performance of my favorite sports team."'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以探索不同存在惩罚值对响应生成的影响，同时与频率惩罚进行比较。想象一下，您提示模型：“告诉我关于我最喜欢的体育队最近的表现。”
- en: Response
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回应
- en: 'Presence Penalty 0.2 (Less Avoidance):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 存在惩罚 0.2（减少规避）：
- en: 'Assistant: "Your favorite sports team has been performing exceptionally well
    in recent matches. They have displayed remarkable teamwork and strategic play,
    leading to several victories."'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：“您最喜欢的体育队在最近的比赛中表现异常出色。他们展示了卓越的团队合作和战略性的比赛，取得了几场胜利。”
- en: 'With a lower presence penalty, the model may generate responses that contain
    the desired keywords or concepts more frequently, as it is less inclined to avoid
    using them. Doing the same thing with the frequency penalty, on the other hand,
    would produce:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 采用较低的存在惩罚，模型可能更频繁地生成包含所需关键词或概念的响应，因为它不太倾向于避免使用它们。而使用频率惩罚进行相同操作，结果会是：
- en: Response
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回应
- en: 'Frequency Penalty 0.2 (Less Repetition):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚 0.2（减少重复）：
- en: 'Assistant: "Your favorite sports team has had an impressive run in recent games.
    They showcased great skill, coordination, and game-changing strategies, resulting
    in a string of successes."'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：“您最喜欢的体育队最近表现出色。他们展示了出色的技巧、协调能力和改变比赛局势的策略，取得了一系列的成功。”
- en: In comparison to presence penalty, frequency penalty focuses on reducing the
    repetition of specific phrases or responses, irrespective of their relevance to
    the user’s input.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与存在惩罚相比，频率惩罚侧重于减少特定短语或响应的重复，而不考虑它们与用户输入的相关性。
- en: By adjusting the presence penalty, you can control how much the AI adheres to
    specific keywords or concepts in its generated text. Lower values may result in
    the model mentioning the desired topics more frequently, while higher values encourage
    the model to avoid excessive repetition of those topics.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整存在惩罚，您可以控制AI在生成的文本中遵循特定关键字或概念的程度。较低的值可能导致模型更频繁地提到所需的主题，而较高的值则鼓励模型避免过多地重复这些主题。
- en: Here’s an example of how you might use a high presence penalty in a language
    model prompt. Imagine that you’re using a language model to generate a story or
    conversation, and you want to ensure that the generated text avoids any mention
    of violence or graphic content. You want to apply a high presence penalty to ensure
    that the model strictly avoids using words or phrases related to violence.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在语言模型提示中使用高存在惩罚的示例。想象一下，您正在使用语言模型来生成一个故事或对话，并且希望确保生成的文本避免提及任何暴力或图形内容。您希望应用高存在惩罚来确保模型严格避免使用与暴力相关的单词或短语。
- en: Prompt
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: 'User: Create a story about two adventurers on a quest to save their kingdom
    from a great threat. Apply a high presence penalty to avoid any description of
    violence or graphic scenes.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：创建一个关于两位冒险家为了拯救他们的王国免受巨大威胁的故事。应用高存在惩罚，避免描述任何暴力或图形场景。
- en: By using a high presence penalty, you can guide the language model to generate
    responses that adhere to specific content guidelines, making it suitable for various
    contexts where certain topics or language need to be avoided.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用高存在惩罚，您可以引导语言模型生成符合特定内容指南的响应，使其适用于各种需要避免某些主题或语言的情境。
- en: 'Besides the configuration controls that you can see on the Playground page,
    there are some other controls that are both fairly common and useful:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了您在游乐场页面上可以看到的配置控件之外，还有一些其他控件既常见又有用：
- en: 2.5.7 Working with "best of"
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.7 使用“最佳选择”
- en: When generating responses from a generative AI model, you may sometimes receive
    multiple candidate outputs. The **Best of** approach involves selecting the most
    suitable or highest-quality response from these candidates based on certain criteria.
    The default setting (`1`) will stream all outputs without any selection or filtering.
    Higher values (up to `20`) will increase the ratio of possibility generations
    to outputs that you’re shown.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当从生成式AI模型生成响应时，有时可能会收到多个候选输出。**最佳选择**方法涉及根据某些标准从这些候选项中选择最合适或最高质量的响应。默认设置（`1`）将流式传输所有输出而不进行任何选择或过滤。较高的值（最高可达`20`）将增加您所看到的可能性生成与输出之间的比率。
- en: 'The purpose of the "Best of" approach is to curate and refine the output by
    hand picking the most favorable response among several options. It allows you
    to have more control over the final output, ensuring it meets your desired standards
    or aligns with the intended purpose of the generative AI model. But keep in mind:
    the higher the Best of value, the more you pay for each output.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳选择”方法的目的是通过手工选择最有利的响应来策划和完善输出中的内容。它使您能够更多地控制最终输出，确保它符合您的期望标准或与生成式AI模型的预期目的相一致。但请记住：最佳选择值越高，您为每个输出支付的费用就越多。
- en: For example, in a text summarization task, you may want to identify the most
    important phrases or sentences that capture the essence of a document or article.
    You could use "best of" to extract the top n phrases or sentences based on their
    importance or relevance, and then use these phrases to generate a summary of the
    original text.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在文本摘要任务中，您可能希望识别出捕捉文档或文章精华的最重要的短语或句子。您可以使用“最佳选择”来提取基于其重要性或相关性的前n个短语或句子，然后使用这些短语来生成原始文本的摘要。
- en: 2.5.8 Working with the inject start text setting
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.8 使用注入起始文本设置
- en: The **Inject start text** or "input prefix" is to guide or condition the model’s
    output based on specific initial text provided by the user. It involves pre-pending
    or inserting a prompt, question, or context at the beginning of the input sequence
    to influence the generated response. By injecting start text, you can provide
    the model with additional context or information that helps steer its output in
    a desired direction. Unlike the other prompt tools we’ve seen, injected start
    text becomes an integral part of the input prompt itself and serves as the beginning
    of the generated response. This can be useful in scenarios where you want the
    generated text to be more focused, specific, or tailored to a particular context.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: “注入开始文本”或“输入前缀”是指根据用户提供的特定初始文本来指导或约束模型的输出。它涉及在输入序列开头处事先添加或插入提示、问题或上下文，以影响生成的回复。通过注入开始文本，您可以为模型提供额外的上下文或信息，以帮助其将输出引导到所需的方向。与我们之前看到的其他提示工具不同，注入的开始文本成为输入提示本身的一部分，并用作生成的回复的开头。这在您想要生成的文本更加专注、具体或适用于特定上下文的情况下非常有用。
- en: 'For example, if you are using a language model to generate responses in a customer
    support chatbot, you can inject start text such as "User: What is the return policy
    for your products?" before the model generates a response. This helps frame the
    conversation and ensures the model understands the context of the user’s query.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您正在使用语言模型在客户支持聊天机器人中生成回复，可以在模型生成回复之前插入“用户：您的产品的退货政策是什么？”等开始文本。这有助于给对话框架并确保模型理解用户查询的上下文。
- en: Rather than specifying text to prefix a completion, **Inject restart text**
    allows users to *continue* a patterned conversation structure by inserting text
    within a completion.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与指定文本前缀完成不同，“注入重新启动文本”允许用户在一个完成中插入文本，以“继续”某种模式化的对话结构。
- en: There are, as you might imagine, many more cool and wonderful things you can
    do with GPT via the API. We’ll certainly be touching on many of them through the
    rest of this book. But you can (and should) visit [the API reference page](api-reference.html)
    early and often.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 想必你可以想象，在API的帮助下，GPT还有很多酷炫和精彩的用途。在本书的剩余部分中，我们肯定会触及其中的许多方面。但是你可以（而且应该）经常去[API参考页面](api-reference.html)查看。
- en: Takeaway
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 要点
- en: Any generative AI operation that’s more complicated than a simple request for
    information - and certainly any automated prompt that’s delivered programmatically
    - can probably be executed more effectively by tweaking your model’s parameters.
    So consider moving beyond the default settings for things like frequency and temperature.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单的信息获取请求之外，任何比较复杂的生成型AI操作（特别是以程序化方式交付的自动提示）都可以通过调整模型参数来更有效地执行。所以考虑超越默认设置，比如频率和温度等内容。
- en: 2.6 Summary
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 小结
- en: There are multiple classes of generative AI model, including software frameworks
    like GPT and PaLM-2 and more specific task-based modules like GPT’s davinci and
    ada. Each has it’s own best-use scenarios.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成型AI模型有多个类别，包括像GPT和PaLM-2这样的软件框架，以及像GPT的davinci和ada这样更具体的基于任务的模块。每种模型都有其最佳应用场景。
- en: OpenAI’s Playground is a valuable tool for learning about the configuration
    options GPT offers and generating code for running prompts programmatically. You
    should use the Playground as a source for custom-built code for executing prompts
    through the OpenAI API.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI的Playground是一个有价值的工具，可用于了解GPT提供的配置选项并生成用于通过OpenAI API运行提示的代码。您应该将Playground作为自定义构建代码的来源，以执行通过OpenAI
    API的提示。
- en: Configuration controls, like Temperature, presence penalties, and Best of can
    be used to fine-tune your model prompts. There are generally interactive tools
    for applying these controls no matter which AI model you’re using.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度、存在惩罚和最优选择等配置控制变量可以用于微调模型提示。无论使用哪个AI模型，通常都有互动工具可用于应用这些控制变量。
- en: The OpenAI API reference guide is an important resource. Make it your best friend.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI API参考指南是一个重要的资源，把它当做你最好的朋友。
- en: We explored fine tuning in its larger context, giving us a quick glimpse into
    some of the flexibility that’s possible with LLMs.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探索了微调在更大上下文中的应用，从中快速了解到了LLM所具备的一些灵活性。
