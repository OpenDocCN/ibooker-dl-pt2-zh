- en: 2 Managing Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding how to access and interact with AI models in the most productive
    ways possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring models to provide the best fit possible for your specific needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the OpenAI Playground to better understand key tools for controlling
    AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throwing clever prompts at an AI chat interface can definitely produce impressive
    results. But by ignoring the finer points of model configuration, you’ll be missing
    out on most of your AI’s potential value. So with this chapter, we’ll begin the
    process of figuring out which dials to turn and how far to turn them. (And which
    big red button you should absolutely avoid!)
  prefs: []
  type: TYPE_NORMAL
- en: Of course for all I know, by the time you get around to reading this, the whole
    process might be automated. That blue light gently pulsing on your keyboard that
    you can’t resist staring at? That would be the brainwave scanner GPT now uses
    to directly download your innermost goals and desires.
  prefs: []
  type: TYPE_NORMAL
- en: Your results are available now.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Accessing GPT models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In case you haven’t yet had the pleasure, most popular interactive AI platforms
    require you to create an account before trying them out. For OpenAI’s ChatGPT,
    [that’ll happen at chat.openai.com/auth/login](auth.html). Even if you’re asked
    to provide credit card information, you’ll be given plenty of warning before you’re
    actually billed. Just don’t ignore those warnings.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’re signed up, [the ChatGPT interface is here](chat.openai.com.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now what?
  prefs: []
  type: TYPE_NORMAL
- en: Besides ChatGPT, it doesn’t require a PhD in electrical engineering to realize
    that Microsoft’s Bing search engine gives you access to GPT-4 from within the
    Edge browser. I’d mention Google’s GPT-competitor, Bard here, too, but I’m in
    Canada. Bard doesn’t happen here, yet. In fact, you can expect to come across
    all kinds of geographic, commercial, or even technological restrictions on how
    and when you can access various AI services as they evolve. Be patient and flexible
  prefs: []
  type: TYPE_NORMAL
- en: Besides those web services that are hosted directly by their creators, there
    are also plenty of third-party web projects like [ChatPDF (for analyzing PDF documents)](www.chatpdf.com.html)
    and [Rytr (for generating written content)](rytr.me.html) that offer nice managed
    applications for specialized use-cases. There’s nothing particularly complicated
    about putting those to use either. We will talk more about those kinds of services
    later.
  prefs: []
  type: TYPE_NORMAL
- en: But all that’s consumer-quality stuff. It’s OK. But the serious work, you might
    say, is happening "off-campus". Meaning, high-productivity interactions like carefully
    configuring your model, letting your AI loose on large stores of your *own* data
    or automating multiple rounds of prompts and completions, and then incorporating
    the responses into your code-driven workflow.
  prefs: []
  type: TYPE_NORMAL
- en: No matter which models you’re using, *this* kind of access will happen through
    an application programming interface (API). As illustrated in the figure below,
    an API serves as a bridge between software applications, allowing them to communicate
    and interact. It defines a set of rules and protocols that allow one application
    to request services or data from another. APIs provide developers who have appropriate
    authorization to access specific functionalities of a service. They enable seamless
    integration by specifying how requests should be structured and how responses
    will be formatted.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 A typical API architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 1](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-1.png)'
  prefs: []
  type: TYPE_IMG
- en: A lot of the technical examples you’ll see later in the book will happen through
    APIs. For practical reasons, those will mostly use OpenAI models and infrastructure.
    But the broad underlying methodologies should mostly apply to other services,
    too (once they become widely available).
  prefs: []
  type: TYPE_NORMAL
- en: So the road to coding your AI leads through APIs. But don’t worry if you’ve
    never done this kind of thing before. I’ll give you all the background and technical
    details you’ll need to make everything work just fine.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go *there*, though, we should check out [OpenAI’s Playground](platform.openai.com.html).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Learning by "playing"
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Playground, shown in the figure below, existed even before ChatGPT, and it was
    where I got my first interactions with GPT. Although do keep in mind that, along
    with everything else in the AI world, the interface will probably have changed
    at least twice by the time you get to it. We’re going to use the playground throughout
    this chapter to learn how to interact with GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 OpenAI’s Playground interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 2](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-2.png)'
  prefs: []
  type: TYPE_IMG
- en: You get to Playground from your OpenAI login account. Rather than enjoying a
    sustained conversation where subsequent exchanges are informed by earlier prompts
    and completions, the default text field in Playground offers only one exchange
    at a time. The models it’s based on might also be a bit older and less refined
    than the ChatGPT version.
  prefs: []
  type: TYPE_NORMAL
- en: But there are two things that set Playground apart from ChatGPT. One is the
    configuration controls displayed down the right side of the screen in the image
    above, and the second is the *View code* feature at the top-right. It’s those
    features that make Playground primarily an educational tool rather than just another
    GPT interface.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Accessing Python code samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to look at those features one at a time in the next section of this
    chapter. But, accessing the GPT API using code is what will probably give you
    the greatest value over the long term, I really want to show you what *View code*
    is all about right away. The image below shows a typical Playground session where
    I’ve typed in a prompt and then hit the "View code" button with the "Python" option
    selected. I’m shown working code that, assuming you’ll add a valid OpenAI API
    key on line 4, can be copied and run from any internet-connected computer.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 Playground’s View code tool with Python code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 3](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Don’t worry about the details right now, but take a moment to look through the
    arguments that are included in the `openai.Completion.create()` method. The model
    that’s currently selected in the Model field on the right side of the Playground
    is there (`text-davinci-003`), as is my actual prompt (`Explain the purpose of…​`).
    In fact, each configuration option I’ve selected is there. In other words, I can
    experiment with any combination of configurations here in the Playground, and
    then copy the code and run it - or variations of it - anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: This, in fact, is where you learn how to use the API.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Accessing CURL code samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next image shows us how that exact same prompt would work if I decided to
    use the command line tool, curl, instead of Python. Besides Python and curl, you
    can also display code in node.js and JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 Playground’s View code tool with curl code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 4](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-4.png)'
  prefs: []
  type: TYPE_IMG
- en: '`curl` is a venerable open source command line tool that’s often available
    by default. To confirm it’s available on your system, simply type `curl` at any
    command line prompt. You should see some kind of help message with suggestions
    for proper usage.'
  prefs: []
  type: TYPE_NORMAL
- en: One more thing. The table below shows each available OpenAI model along with
    their associated API endpoints. An endpoint is an address that can be used within
    your code to access a resource. Besides the value of having that information,
    this is also important because it shows us the *kinds* of prompts you can send.
    Besides the `completions` operations that you’d expect, there’s also `edits`,
    `transcriptions`, `translations`, `fine-tunes`, `embeddings`, and `moderations`.
    We’ll talk more about using those later in the book. But do keep them all in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '| ENDPOINT | MODEL NAME |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/chat/completions | gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo,
    gpt-3.5-turbo-0301 |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/completions | text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001,
    text-ada-001 |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/edits | text-davinci-edit-001, code-davinci-edit-001 |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/audio/transcriptions | whisper-1 |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/audio/translations | whisper-1 |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/fine-tunes | davinci, curie, babbage, ada |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/embeddings | text-embedding-ada-002, text-search-ada-doc-001 |'
  prefs: []
  type: TYPE_TB
- en: '| /v1/moderations | text-moderation-stable, text-moderation-latest |'
  prefs: []
  type: TYPE_TB
- en: Takeaway
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: No matter which AI you’re using, make sure you understand all available options
    so you can optimize your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Completion configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You could think of "completion configurations" as a kind of fine tuning, and
    you wouldn’t be wrong. However, in the context of AI, the term "fine tuning" can
    have far more specific meanings. We’ll spend more time discussing that topic in
    chapters 6 and 9.
  prefs: []
  type: TYPE_NORMAL
- en: Before I start explaining the way each of these configurations works, here’s
    an image that should help you visualize what an AI model might do to your prompt
    before spitting out a response.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 How parameters are applied to an AI prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 5](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-5.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, a typical language model might immediately produce a tentative,
    draft response ("Generate Output" in the diagram). But, before sharing it with
    you ("Return Output"), it’ll first test it for compliance with any preferences
    (i.e,. temperature, frequency, etc.) you might have set. Those preferences - which
    we’ll soon see in action - can control a prompt’s tone, creativity, focus, verbosity,
    and even cost.
  prefs: []
  type: TYPE_NORMAL
- en: Now here’s what those controls look like in the Playground.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 Upper selection of tools in the Playground UI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![gai 2 6](https://drek4537l1klr.cloudfront.net/clinton6/v-3/Figures/gai-2-6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s see what those are really all about. Just to keep you oriented, we’ll
    cover each of these configuration categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top P value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency penalty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presence penalty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best of
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inject start text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to give you some context, imagine that you’re building a web application
    that provides on-demand responses to user questions about your hotel. You might
    initially ask your users to select from a list of categories ("dining and entertainment",
    "trip planning", and so on). Based on the category they choose, you want to fine
    tune your chat tool so that the responses are a good match for the users' expectations.
  prefs: []
  type: TYPE_NORMAL
- en: These configurations can help you create just such an customization.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Setting the mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Depending on the model you’re using, the **Mode** drop down might have three
    options: Chat, Complete, and Edit. Complete and Chat provide the familiar completion
    and chat experiences that we’re already used to. If you have some text you’d like
    to rework in one way or another, you can select the Edit mode, enter your text
    in the Input section, and what you’d like changed in the text in the Instructions
    field below. The Playground suggests "Fix the grammar" as a possible instruction,
    but you’re free to use your own. We’ll learn more about this kind of operation
    in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The pre-built list in the **Model** drop-down displays all the OpenAI models
    that are available for the currently-selected Mode. This list will change dynamically
    based on the Mode you’ve selected. You’ll obviously want to select a model that’s
    the best fit for your needs and budget (as described above).
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 Setting the temperature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Temperature** controls the amount of randomness used in the generation of
    output. It allows the user to adjust the level of creativity and unpredictability
    in the output. By setting the temperature to a value between `0` and `2` (the
    default is `1`), the AI can generate different types of output, ranging from highly
    creative and unpredictable (i.e., 2) to highly structured and conservative (i.e,.
    `0`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an example using a chatbot scenario where a user asks for a
    restaurant recommendation:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'User: "Can you suggest a good restaurant nearby?"'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use different temperature settings to generate responses with varying
    degrees of randomness. A higher temperature (e.g., 1.3) encourages the LM to produce
    more diverse and creative responses, even if they might be less focused or accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Temperature 1.3: Assistant: "Sure! How about trying the cozy little café around
    the corner? They serve delicious food, from gourmet burgers to exotic sushi. You’ll
    definitely have an adventurous dining experience there!"'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the high temperature setting led to a more imaginative response,
    suggesting a variety of food options and emphasizing an "adventurous dining experience."
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, a lower temperature (e.g., 0.2) encourages the LM to produce
    more deterministic and focused responses, often sticking to safe and conventional
    choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Temperature 0.2: Assistant: "Sure! I recommend the Italian restaurant on Main
    Street. They have authentic pasta dishes and mouthwatering pizzas. You won’t be
    disappointed!"'
  prefs: []
  type: TYPE_NORMAL
- en: With the lower temperature, the response is more straightforward and focuses
    on a specific cuisine and establishment, providing a safe and reliable recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.3 Setting the Top P value
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Top P** (also known as nucleus sampling or "penalty-free sampling") is a
    parameter that helps control the diversity of the generated text. It involves
    selecting the most likely words for the next token in a sequence, considering
    a cumulative probability threshold (P). Instead of generating from its entire
    vocabulary, it narrows down choices to a subset of the most probable words whose
    cumulative probabilities exceed P. This helps in controlling the output’s length
    and diversity while maintaining coherence.'
  prefs: []
  type: TYPE_NORMAL
- en: When using the Top P sampling approach, the model selects the most likely next
    word based on a probability threshold (p), which allows for more diverse and controlled
    output. Words with cumulative probabilities that exceed the threshold are considered,
    ensuring a balance between predictability and creativity in generated text. In
    simpler terms, "top p" allows the model to think beyond the obvious choices while
    still staying coherent. It’s like giving the AI a bit of freedom to surprise you
    while ensuring the generated text remains meaningful. This technique is particularly
    useful when you want creative responses that don’t completely deviate from the
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: By adjusting the Top P value, users can influence the amount of randomness or
    diversity in the model’s responses. A higher value (e.g., `0.9`) includes a larger
    set of potential tokens, making the output more diverse, while a lower value (e.g.,
    `0.1`) narrows down the choices, resulting in more focused and deterministic outputs.
    The default Top P value is `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example where a user asks for information about the weather:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'User: "What’s the weather like today?"'
  prefs: []
  type: TYPE_NORMAL
- en: We can explore the effect of different Top P values on the response generation.
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Top P 0.2 (Highly Focused): Assistant: "The weather today is sunny with a high
    of 25 degrees Celsius. It’s a perfect day to enjoy outdoor activities."'
  prefs: []
  type: TYPE_NORMAL
- en: With a low Top P value, the model selects only the most likely next words, resulting
    in focused and deterministic responses that are generally more reliable but less
    varied.
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Top P 0.8 (More Diverse): Assistant: "Today’s weather is sunny, warm, and delightful.
    You might want to grab your sunglasses and head out for a fun-filled day!"'
  prefs: []
  type: TYPE_NORMAL
- en: With a higher Top P value, the model considers a broader range of likely next
    words, resulting in more diverse responses. This can introduce variations in the
    generated text, offering different ways of expressing the same information.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Top P and temperature seem similar, Temperature controls the randomness
    in language generation: higher values (e.g., 1.0) increase diversity, lower values
    (e.g., 0.2) produce more predictable output. By contrast, Top-p sampling sets
    a probability threshold (p) to select from a subset of most probable words, adjusting
    output diversity based on the threshold (e.g., p=0.9 for more diversity, p=0.2
    for less).'
  prefs: []
  type: TYPE_NORMAL
- en: As a rule, one can alter Temperature *or* Top P, but not both.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.4 Working with stop sequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Stop sequences** are special tokens used to indicate the end of a generated
    response or to prompt the model to stop generating further text. These tokens
    are typically added to the input prompt to control the length of the generated
    output.'
  prefs: []
  type: TYPE_NORMAL
- en: In English, a common example of an end-of-sentence token is the period (.),
    followed by a space. In programming languages, a common stop sequence might be
    a pair of curly braces ("{}") to indicate the end of a code block. In HTML or
    XML, a closing tag like "</tag>" signals the end of an element.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that modern language models like GPT do not necessarily
    rely on explicit stop sequences to generate text. Instead, they often use techniques
    like tokenization and context window to understand where to naturally conclude
    sentences or paragraphs. Additionally, they may use special tokens like "<eos>"
    (end of sequence) internally, but these details are typically abstracted away
    from the user.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the middle of explaining a lesson, a virtual instructor could
    be configured to say, “Let’s stop here for today. Repeat back to me the three
    main points of this lesson.” After the student responds with the three main points
    (matching the required stop sequence), the instructor could say, “Yes, excellent!
    Alright, that concludes our session today.”
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.5 Setting the frequency penalty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **frequency penalty** is used to discourage repetitive or redundant output.
    It helps address a common issue where models tend to produce repetitive or excessively
    long phrases, leading to less coherent and less diverse responses. By applying
    a frequency penalty, users can control the trade-off between generating diverse
    responses and avoiding repetitive patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The penalty is calculated based on the number of times a generated token or
    sequence of tokens has already appeared in the output. When the penalty is applied,
    the model’s probability distribution over tokens is adjusted to decrease the likelihood
    of generating tokens that have already been used. The purpose of the frequency
    penalty is to encourage the model to explore alternative options and generate
    more varied and coherent responses.
  prefs: []
  type: TYPE_NORMAL
- en: The penalty value (which can lie between `-2.0` and `2.0`) can be adjusted based
    on the desired level of repetition avoidance. Higher penalty values result in
    stronger discouragement of repetition, while lower values allow for more flexibility
    in the generated output. Finding the right balance is important to ensure the
    generated text remains coherent and contextually relevant while avoiding excessive
    repetition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example where a user asks for some ideas for a new smartphone:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'User: Generate some ideas for a new smartphone.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: Sure! Here are a few ideas: a high-resolution display, multiple camera
    lenses, fast processing speed, long battery life, and a sleek design.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That language model generated ideas that are already common - it’s hardly ground
    breaking. However, if you add a frequency penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'User: Generate some ideas for a new smartphone while using a frequency penalty
    to encourage unique and rare features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model: Certainly! How about a smartphone with a holographic display that projects
    3D images, a graphene-based battery for instant charging, a customizable modular
    design allowing users to swap hardware components, a built-in AI language translator
    for real-time conversations, and a biometric authentication system that reads
    unique vein patterns in the user’s palm?'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, by using a frequency penalty, the model is more likely to generate
    ideas that involve less common words or concepts. This results in suggestions
    that are more innovative and imaginative, aligning with your goal of coming up
    with unique product features.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.6 Setting the presence penalty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **presence penalty** is a parameter in GPT models that helps control the
    repetition of phrases and words in the generated text. It’s designed to prevent
    the model from repeating the same phrases or words too often in the generated
    output. The presence penalty modifies the probability distribution to make less
    likely words that were present in the input prompt or seed text. This encourages
    the model to generate words and cover topics that were not explicit in the input.
  prefs: []
  type: TYPE_NORMAL
- en: Although the `presence` and `frequency` penalties do seem to overlap, they’re
    not identical. The frequency penalty is a contribution that is proportional to
    how often a specific token has already been sampled. It discourages the model
    from repeating the same words or phrases too frequently within the generated text.
    On the other hand, the presence penalty is a one-time, additive contribution that
    applies to all tokens that have been sampled at least once. It modifies the probability
    distribution to make less likely words that were present in the input prompt or
    seed text. This encourages the model to generate words that were not in the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can explore the effect of different presence penalty values on the response
    generation while comparing it with frequency penalty. Imagine you prompted the
    model with: "Tell me about the recent performance of my favorite sports team."'
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Presence Penalty 0.2 (Less Avoidance):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: "Your favorite sports team has been performing exceptionally well
    in recent matches. They have displayed remarkable teamwork and strategic play,
    leading to several victories."'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a lower presence penalty, the model may generate responses that contain
    the desired keywords or concepts more frequently, as it is less inclined to avoid
    using them. Doing the same thing with the frequency penalty, on the other hand,
    would produce:'
  prefs: []
  type: TYPE_NORMAL
- en: Response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Frequency Penalty 0.2 (Less Repetition):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assistant: "Your favorite sports team has had an impressive run in recent games.
    They showcased great skill, coordination, and game-changing strategies, resulting
    in a string of successes."'
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to presence penalty, frequency penalty focuses on reducing the
    repetition of specific phrases or responses, irrespective of their relevance to
    the user’s input.
  prefs: []
  type: TYPE_NORMAL
- en: By adjusting the presence penalty, you can control how much the AI adheres to
    specific keywords or concepts in its generated text. Lower values may result in
    the model mentioning the desired topics more frequently, while higher values encourage
    the model to avoid excessive repetition of those topics.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of how you might use a high presence penalty in a language
    model prompt. Imagine that you’re using a language model to generate a story or
    conversation, and you want to ensure that the generated text avoids any mention
    of violence or graphic content. You want to apply a high presence penalty to ensure
    that the model strictly avoids using words or phrases related to violence.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'User: Create a story about two adventurers on a quest to save their kingdom
    from a great threat. Apply a high presence penalty to avoid any description of
    violence or graphic scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: By using a high presence penalty, you can guide the language model to generate
    responses that adhere to specific content guidelines, making it suitable for various
    contexts where certain topics or language need to be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the configuration controls that you can see on the Playground page,
    there are some other controls that are both fairly common and useful:'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.7 Working with "best of"
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When generating responses from a generative AI model, you may sometimes receive
    multiple candidate outputs. The **Best of** approach involves selecting the most
    suitable or highest-quality response from these candidates based on certain criteria.
    The default setting (`1`) will stream all outputs without any selection or filtering.
    Higher values (up to `20`) will increase the ratio of possibility generations
    to outputs that you’re shown.
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of the "Best of" approach is to curate and refine the output by
    hand picking the most favorable response among several options. It allows you
    to have more control over the final output, ensuring it meets your desired standards
    or aligns with the intended purpose of the generative AI model. But keep in mind:
    the higher the Best of value, the more you pay for each output.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a text summarization task, you may want to identify the most
    important phrases or sentences that capture the essence of a document or article.
    You could use "best of" to extract the top n phrases or sentences based on their
    importance or relevance, and then use these phrases to generate a summary of the
    original text.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.8 Working with the inject start text setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Inject start text** or "input prefix" is to guide or condition the model’s
    output based on specific initial text provided by the user. It involves pre-pending
    or inserting a prompt, question, or context at the beginning of the input sequence
    to influence the generated response. By injecting start text, you can provide
    the model with additional context or information that helps steer its output in
    a desired direction. Unlike the other prompt tools we’ve seen, injected start
    text becomes an integral part of the input prompt itself and serves as the beginning
    of the generated response. This can be useful in scenarios where you want the
    generated text to be more focused, specific, or tailored to a particular context.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you are using a language model to generate responses in a customer
    support chatbot, you can inject start text such as "User: What is the return policy
    for your products?" before the model generates a response. This helps frame the
    conversation and ensures the model understands the context of the user’s query.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than specifying text to prefix a completion, **Inject restart text**
    allows users to *continue* a patterned conversation structure by inserting text
    within a completion.
  prefs: []
  type: TYPE_NORMAL
- en: There are, as you might imagine, many more cool and wonderful things you can
    do with GPT via the API. We’ll certainly be touching on many of them through the
    rest of this book. But you can (and should) visit [the API reference page](api-reference.html)
    early and often.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaway
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Any generative AI operation that’s more complicated than a simple request for
    information - and certainly any automated prompt that’s delivered programmatically
    - can probably be executed more effectively by tweaking your model’s parameters.
    So consider moving beyond the default settings for things like frequency and temperature.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are multiple classes of generative AI model, including software frameworks
    like GPT and PaLM-2 and more specific task-based modules like GPT’s davinci and
    ada. Each has it’s own best-use scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s Playground is a valuable tool for learning about the configuration
    options GPT offers and generating code for running prompts programmatically. You
    should use the Playground as a source for custom-built code for executing prompts
    through the OpenAI API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration controls, like Temperature, presence penalties, and Best of can
    be used to fine-tune your model prompts. There are generally interactive tools
    for applying these controls no matter which AI model you’re using.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenAI API reference guide is an important resource. Make it your best friend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explored fine tuning in its larger context, giving us a quick glimpse into
    some of the flexibility that’s possible with LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
