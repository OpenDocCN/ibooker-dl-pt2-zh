["```py\nimport os\nimport requests\nimport tarfile\n\nimport shutil\n# Retrieve the data\nif not os.path.exists(os.path.join('data', 'lm','CBTest.tgz')):      ❶\n    url = \"http:/ /www.thespermwhale.com/jaseweston/babi/CBTest.tgz\"\n    # Get the file from web\n    r = requests.get(url)\n\n    if not os.path.exists(os.path.join('data','lm')):\n        os.mkdir(os.path.join('data','lm'))\n\n    # Write to a file\n    with open(os.path.join('data', 'lm', 'CBTest.tgz'), 'wb') as f:  ❷\n        f.write(r.content)\n\nelse:\n    print(\"The tar file already exists.\")\n\nif not os.path.exists(os.path.join('data', 'lm', 'CBTest')):         ❸\n    # Write to a file\n    tarf = tarfile.open(os.path.join(\"data\",\"lm\",\"CBTest.tgz\"))\n    tarf.extractall(os.path.join(\"data\",\"lm\"))  \nelse:\n    print(\"The extracted data already exists\")\n```", "```py\ndef read_data(path):\n    stories = []                                           ❶\n\n    with open(path, 'r') as f:    \n        s = []                                             ❷\n        for row in f:\n\n            if row.startswith(\"_BOOK_TITLE_\"):             ❸\n                if len(s)>0:\n                    stories.append(' '.join(s).lower())    ❹\n                s = []                                     ❺\n\n            s.append(row)                                  ❻\n\n    if len(s)>0:\n        stories.append(' '.join(s).lower())                ❼\n\n    return stories\n```", "```py\nstories = read_data(os.path.join('data','lm','CBTest','data','cbt_train.txt'))\nval_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_valid.txt'))\ntest_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_test.txt'))\n```", "```py\nprint(\"Collected {} stories (train)\".format(len(stories)))\nprint(\"Collected {} stories (valid)\".format(len(val_stories)))\nprint(\"Collected {} stories (test)\".format(len(test_stories)))\nprint(stories[0][:100])\nprint('\\n', stories[10][:100])\n```", "```py\nCollected 98 stories (train)\nCollected 5 stories (valid)\nCollected 5 stories (test)\n\nchapter i. -lcb- chapter heading picture : p1.jpg -rcb- how the fairies \n➥ were not invited to court .\n\n a tale of the tontlawald long , long ago there stood in the midst of a \n➥ country covered with lakes a \n```", "```py\nfrom collections import Counter\n# Create a large list which contains all the words in all the reviews\ndata_list = [w for doc in stories for w in doc.split(' ')]\n\n# Create a Counter object from that list\n# Counter returns a dictionary, where key is a word and the value is the frequency\ncnt = Counter(data_list)\n\n# Convert the result to a pd.Series \nfreq_df = pd.Series(\n    list(cnt.values()), index=list(cnt.keys())\n).sort_values(ascending=False)\n\n# Count of words >= n frequent\nn=10\nprint(\"Vocabulary size (>={} frequent): {}\".format(n, (freq_df>=n).sum()))\n```", "```py\n,      348650\nthe    242890\n.\\n    192549\nand    179205\nto     120821\na      101990\nof      96748\ni       79780\nhe      78129\nwas     66593\ndtype: int64\n\nVocabulary size (>=10 frequent): 14473\n```", "```py\nI went to the bookshop\n```", "```py\n\"I \", \" w\", \"we\", \"en\", \"nt\", \"t \", \" t\", \"to\", \"o \", \"th\", \"he\", \"e \", \" b\", \"bo\", \"oo\", \"ok\", \"ks\", \"sh\", \"ho\", \"op\"\n```", "```py\n\"I w\", \" we\", \"wen\", \"ent\", \"nt \", \"t t\", \" to\", \"to \", \"o t\", \" th\", \"the\", \"he \", \"e b\", \" bo\", \"boo\", \"ook\", \"oks\", \"ksh\", \"sho\", \"hop\"\n```", "```py\n\"I \", \"we\", \"nt\", \" t\", \"o \", \"th\", \"e \", \"bo\", \"ok\", \"sh\", \"op\"\n```", "```py\ndef get_ngrams(text, n):\n    return [text[i:i+n] for i in range(0,len(text),n)]\n```", "```py\ntest_string = \"I like chocolates\"\nprint(\"Original: {}\".format(test_string))\nfor i in list(range(3)):\n    print(\"\\t{}-grams: {}\".format(i+1, get_ngrams(test_string, i+1)))\n```", "```py\nOriginal: I like chocolates\n    1-grams: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 'c', 'h', 'o', 'c', \n➥ 'o', 'l', 'a', 't', 'e', 's']\n    2-grams: ['I ', 'li', 'ke', ' c', 'ho', 'co', 'la', 'te', 's']\n    3-grams: ['I l', 'ike', ' ch', 'oco', 'lat', 'es']\n```", "```py\nfrom itertools import chain\nfrom collections import Counter\n\n# Create a counter with the bi-grams\nngrams = 2\n\ntext = chain(*[get_ngrams(s, ngrams) for s in stories])\ncnt = Counter(text)\n\n# Create a pandas series with the counter results\nfreq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n```", "```py\nn_vocab = (freq_df>=10).sum()\nprint(\"Size of vocabulary: {}\".format(n_vocab))\n```", "```py\nSize of vocabulary: 735\n```", "```py\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n```", "```py\ntokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n```", "```py\ntrain_ngram_stories = [get_ngrams(s,ngrams) for s in stories]\n```", "```py\ntokenizer.fit_on_texts(train_ngram_stories)\n```", "```py\ntrain_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n\nval_ngram_stories = [get_ngrams(s,ngrams) for s in val_stories]\nval_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n\ntest_ngram_stories = [get_ngrams(s,ngrams) for s in test_stories]\ntest_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n```", "```py\nOriginal: the yellow fairy book the cat and the mouse in par\nn-grams: ['th', 'e ', 'ye', 'll', 'ow', ' f', 'ai', 'ry', ' b', 'oo', 'k ', \n➥ 'th', 'e ', 'ca', 't ', 'an', 'd ', 'th', 'e ', 'mo', 'us', 'e ', 'in', \n➥ ' p', 'ar']\nWord ID sequence: [6, 2, 215, 54, 84, 35, 95, 146, 26, 97, 123, 6, 2, 128, \n➥ 8, 15, 5, 6, 2, 147, 114, 2, 17, 65, 52]\n\nOriginal: chapter i. down the rabbit-hole alice was beginnin\nn-grams: ['ch', 'ap', 'te', 'r ', 'i.', ' d', 'ow', 'n ', 'th', 'e ', 'ra', \n➥ 'bb', 'it', '-h', 'ol', 'e ', 'al', 'ic', 'e ', 'wa', 's ', 'be', 'gi', \n➥ 'nn', 'in']\nWord ID sequence: [93, 207, 57, 19, 545, 47, 84, 18, 6, 2, 126, 344, \n➥ 38, 400, 136, 2, 70, 142, 2, 66, 9, 71, 218, 251, 17]\n\nOriginal: a patent medicine testimonial `` you might as well\nn-grams: ['a ', 'pa', 'te', 'nt', ' m', 'ed', 'ic', 'in', 'e ', 'te', 'st', \n➥ 'im', 'on', 'ia', 'l ', '``', ' y', 'ou', ' m', 'ig', 'ht', ' a', 's ', \n➥ 'we', 'll']\nWord ID sequence: [60, 179, 57, 78, 33, 31, 142, 17, 2, 57, 50, 125, 43, \n➥ 266, 56, 122, 92, 29, 33, 152, 149, 7, 9, 103, 54]\n```", "```py\ntext_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))     \n```", "```py\nif shuffle:\n    text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)    \n```", "```py\ntf.data.Dataset(              # <- From the original dataset\n  tf.data.Dataset(    # <- From inner dataset containing word IDs of story S only\n    tf.data.WindowDataset(...)  # <- Dataset returned by the window() function\n  )\n)\n```", "```py\n    text_ds = text_ds.flat_map(\n        lambda x: tf.data.Dataset.from_tensor_slices(\n            x\n        ).window(\n            n_seq+1,shift=shift\n        ).flat_map(\n            lambda window: window.batch(n_seq+1, drop_remainder=True)\n        )\n    )\n    )\n```", "```py\nds = tf.data.Dataset.from_tensor_slices([[1,2,3], [5,6,7]])\n```", "```py\nds = ds.map(lambda x: x**2) \n```", "```py\n[[1, 4, 9], [25, 36, 49]]\n```", "```py\nds = ds.flat_map(lambda x: x**2)\n```", "```py\n[1,4,9,25,36,49]\n```", "```py\n    # Shuffle the data (shuffle the order of n_seq+1 long sequences)\n    if shuffle:\n        text_ds = text_ds.shuffle(buffer_size=10*batch_size)    \n```", "```py\n    # Batch the data\n    text_ds = text_ds.batch(batch_size)    \n```", "```py\n    # Split each sequence to an input and a target\n    text_ds = tf.data.Dataset.zip(\n        text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))\n    ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)    \n```", "```py\ndef get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n    \"\"\" Define a tf.data pipeline that takes a set of sequences of text and \n    convert them to fixed length sequences for the model \"\"\"\n\n    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))❶\n\n    if shuffle:\n        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)               ❷\n\n    text_ds = text_ds.flat_map(                                               ❸\n        lambda x: tf.data.Dataset.from_tensor_slices(\n            x\n        ).window(\n            n_seq+1, shift=shift\n        ).flat_map(\n            lambda window: window.batch(n_seq+1, drop_remainder=True)\n        )\n    ) \n\n    if shuffle:\n        text_ds = text_ds.shuffle(buffer_size=10*batch_size)                  ❹\n\n    text_ds = text_ds.batch(batch_size)                                       ❺\n\n    text_ds = tf.data.Dataset.zip(\n        text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))\n    ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)                     ❻\n\n    return text_ds   \n```", "```py\nds = get_tf_pipeline(train_data_seq, 5, batch_size=6)\n\nfor a in ds.take(1):\n    print(a)\n```", "```py\n(\n<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\narray([[161,  12,  69, 396,  17],\n       [  2,  72,  77,  84,  24],\n       [ 87,   6,   2,  72,  77],\n       [276, 484,  57,   5,  15],\n       [ 75, 150,   3,   4,  11],\n       [ 11,  73, 211,  35, 141]])>, \n<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\narray([[ 12,  69, 396,  17,  44],\n       [ 72,  77,  84,  24,  51],\n       [  6,   2,  72,  77,  84],\n       [484,  57,   5,  15,  67],\n       [150,   3,   4,  11,  73],\n       [ 73, 211,  35, 141,  98]])>\n)\n```", "```py\nprint(\"n_grams uses n={}\".format(ngrams))\nprint(\"Vocabulary size: {}\".format(n_vocab))\n\nn_seq=100\nprint(\"Sequence length for model: {}\".format(n_seq))\n\nwith open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:\n    pickle.dump({'n_vocab': n_vocab, 'ngrams':ngrams, 'n_seq': n_seq}, f)\n```", "```py\ntf.keras.layers.GRU(units=1024, return_state=False, return_sequences=True)\n```", "```py\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=n_vocab+1, output_dim=512,input_shape=(None,)           ❶\n    ),\n\n    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True), ❷\n\n    tf.keras.layers.Dense(512, activation='relu'),                        ❸\n\n    tf.keras.layers.Dense(n_vocab, name='final_out'),                     ❹\n    tf.keras.layers.Activation(activation='softmax')                      ❹\n])\n```", "```py\n.Dense(n_vocab, activation=’softmax’, name=’final_out’)\n```", "```py\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=n_vocab+1, output_dim=512,input_shape=(None,)   \n    ),\n    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),   \n    tf.keras.layers.Dense(n_vocab, activation=’softmax’, name='final_out'), ])\n```", "```py\nimport tensorflow.keras.backend as K\n\nclass PerplexityMetric(tf.keras.metrics.Mean):\n\n    def __init__(self, name='perplexity', **kwargs):\n      super().__init__(name=name, **kwargs)\n      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n         from_logits=False, reduction='none'\n       )\n\n    def _calculate_perplexity(self, real, pred):   ❶\n\n      loss_ = self.cross_entropy(real, pred)       ❷\n\n      mean_loss = K.mean(loss_, axis=-1)           ❸\n      perplexity = K.exp(mean_loss)                ❹\n\n      return perplexity \n\n    def update_state(self, y_true, y_pred, sample_weight=None):            \n      perplexity = self._calculate_perplexity(y_true, y_pred)\n      super().update_state(perplexity)\n```", "```py\nmodel.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy', PerplexityMetric()]\n)\n```", "```py\nn_seq = 100\ntrain_ds = get_tf_pipeline(\n    train_data_seq[:50], n_seq, stride=25, batch_size=128\n)\nvalid_ds = get_tf_pipeline(\n    val_data_seq, n_seq, stride=n_seq, batch_size=128\n)\n```", "```py\nos.makedirs('eval', exist_ok=True)\n\ncsv_logger = \n➥ tf.keras.callbacks.CSVLogger(os.path.join('eval','1_language_modelling.\n➥ log'))\n\nmonitor_metric = 'val_perplexity'\nmode = 'min' \nprint(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=monitor_metric, factor=0.1, patience=2, mode=mode, min_lr=1e-8\n)\n\nes_callback = tf.keras.callbacks.EarlyStopping(\n    monitor=monitor_metric, patience=5, mode=mode, \n➥ restore_best_weights=False\n)\n```", "```py\nmodel.fit(train_ds, epochs=50,  validation_data = valid_ds, \n➥ callbacks=[es_callback, lr_callback, csv_logger])\n```", "```py\nbatch_size = 128\ntest_ds = get_tf_pipeline(\n    test_data_seq, n_seq, shift=n_seq, batch_size=batch_size\n)\nmodel.evaluate(test_ds)\n```", "```py\n61/61 [==============================] - 2s 39ms/step - loss: 2.2620 - \n➥ accuracy: 0.4574 - perplexity: 10.5495\n```", "```py\nos.makedirs('models', exist_ok=True)\ntf.keras.models.save_model(model, os.path.join('models', '2_gram_lm.h5'))\n```", "```py\nes_callback = tf.keras.callbacks.EarlyStopping(\n    monitor=’val_perlexity’, patience=5, mode=’min’, \n➥ restore_best_weights=False\n)\n```", "```py\ninp = tf.keras.layers.Input(shape=(None,))                                 ❶\ninp_state = tf.keras.layers.Input(shape=(1024,))                           ❷\n\nemb_layer = tf.keras.layers.Embedding(\n    input_dim=n_vocab+1, output_dim=512, input_shape=(None,)\n)                                                                          ❸\nemb_out = emb_layer(inp)                                                   ❹\n\ngru_layer = tf.keras.layers.GRU(\n    1024, return_state=True, return_sequences=True\n)\ngru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)         ❺❻\n\ndense_layer = tf.keras.layers.Dense(512, activation='relu')                ❼\ndense_out = dense_layer(gru_out)                                           ❼\n\nfinal_layer = tf.keras.layers.Dense(n_vocab, name='final_out')             ❽\nfinal_out = final_layer(dense_out)                                         ❽\nsoftmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)  ❽\n\ninfer_model = tf.keras.models.Model(\n    inputs=[inp, inp_state], outputs=[softmax_out, gru_state]\n)                                                                          ❾\n```", "```py\n# Copy the weights from the original model\nemb_layer.set_weights(model.get_layer('embedding').get_weights())\ngru_layer.set_weights(model.get_layer('gru').get_weights())\ndense_layer.set_weights(model.get_layer('dense').get_weights())\nfinal_layer.set_weights(model.get_layer('final_out').get_weights())\n```", "```py\nmodel.get_layer(<layer name>).get_weights()\n```", "```py\nlayer.set_weights(<weight matrix>)\n```", "```py\ntext = get_ngrams(\n    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired \n➥ of sitting by her sister on the bank ,\".lower(), \n    ngrams\n)\n\nseq = tokenizer.texts_to_sequences([text])\n```", "```py\n# Reset the state of the model initially\nmodel.reset_states()\n# Defining the initial state as all zeros\nstate = np.zeros(shape=(1,1024))\n```", "```py\n# Recursively update the model by assining new state to state\nfor c in seq[0]:    \n    out, state = infer_model.predict([np.array([[c]]), state])\n\n# Get final prediction after feeding the input string\nwid = int(np.argmax(out[0],axis=-1).ravel())\nword = tokenizer.index_word[wid]\ntext.append(word)\n```", "```py\n# Define first input to generate text recursively from\nx = np.array([[wid]])\n```", "```py\nfor _ in range(500):\n\n    out, state = infer_model.predict([x, state])                   ❶\n\n    out_argsort = np.argsort(out[0], axis=-1).ravel()              ❷\n    wid = int(out_argsort[-1])                                     ❷\n    word = tokenizer.index_word[wid]                               ❷\n\n    if word.endswith(' '):                                         ❸\n        if np.random.normal()>0.5:\n            width = 3                                              ❹\n            i = np.random.choice(                                  ❹\n                list(range(-width,0)), \n                p=out_argsort[-width:]/out_argsort[-width:].sum()\n            )    \n            wid = int(out_argsort[i])                              ❹\n            word = tokenizer.index_word[wid]                       ❹\n    text.append(word)                                              ❺\n\n    x = np.array([[wid]])                                          ❻\n```", "```py\n# Print the final output    \nprint('\\n')\nprint('='*60)\nprint(\"Final text: \")\nprint(''.join(text))\n```", "```py\nFinal text: \nchapter i. down the rabbit-hole alice was beginning to get very tired of \n➥ sitting by her sister on the bank , and then they went to the shore , \n➥ and then the princess was so stilling that he was a little girl , \n\n...\n\n it 's all right , and i 'll tell you how young things would n't be able to \n➥ do it .\n i 'm not goin ' to think of itself , and i 'm going to be sure to see you .\n i 'm sure i can notice them .\n i 'm going to see you again , and i 'll tell you what i 've got , '\n```", "```py\nfor _ in range(500):\n\n    out, new_s = infer_model.predict([x, s])                                    \n\n    out_argsort = np.argsort(out[0], axis=-1).ravel()                               \n    wid = int(out_argsort[-1])                                                      \n    word = tokenizer.index_word[wid]\n\n    text.append(word)                                                               \n\n    x = np.array([[wid]])       \n```", "```py\ndef beam_one_step(model, input_, state):    \n    \"\"\" Perform the model update and output for one step\"\"\"\n    output, new_state = model.predict([input_, state])\n    return output, new_state\n```", "```py\ndef beam_search(\n    model, input_, state, beam_depth=5, beam_width=3, ignore_blank=True\n):                                                                           ❶\n    \"\"\" Defines an outer wrapper for the computational function of beam \n➥ search \"\"\"\n\n    def recursive_fn(input_, state, sequence, log_prob, i):                  ❷\n        \"\"\" This function performs actual recursive computation of the long \n➥ string\"\"\"\n\n        if i == beam_depth:                                                  ❸\n            \"\"\" Base case: Terminate the beam search \"\"\"\n            results.append((list(sequence), state, np.exp(log_prob)))        ❹\n            return sequence, log_prob, state                                 ❹\n        else:\n            \"\"\" Recursive case: Keep computing the output using the \n➥ previous outputs\"\"\"\n            output, new_state = beam_one_step(model, input_, state)          ❺\n\n            # Get the top beam_widht candidates for the given depth\n            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)           ❻\n            top_probs, top_ids = top_probs.numpy().ravel(), \n➥ top_ids.numpy().ravel()                                                   ❻\n\n            # For each candidate compute the next prediction\n            for p, wid in zip(top_probs, top_ids):                           ❼\n                new_log_prob = log_prob + np.log(p)                          ❼\n                if len(sequence)>0 and wid == sequence[-1]:                  ❽\n                    new_log_prob = new_log_prob + np.log(1e-1)               ❽\n\n                sequence.append(wid)                                         ❾\n                _ = recursive_fn(\n                    np.array([[wid]]), new_state, sequence, new_log_prob, i+1❿\n                )                                         \n                sequence.pop()\n\n    results = []\n    sequence = []\n    log_prob = 0.0\n    recursive_fn(input_, state, sequence, log_prob, 0)                       ⓫\n\n    results = sorted(results, key=lambda x: x[2], reverse=True)              ⓬\n\n    return results\n```", "```py\ntext = get_ngrams(\n    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired \n➥ of sitting by her sister on the bank ,\".lower(),     \n    ngrams\n)                                                                ❶\n\nseq = tokenizer.texts_to_sequences([text])                       ❷\n\nstate = np.zeros(shape=(1,1024))\nfor c in seq[0]:    \n    out, state = infer_model.predict([np.array([[c]]), state     ❸\n\nwid = int(np.argmax(out[0],axis=-1).ravel())                     ❹\nword = tokenizer.index_word[wid]                                 ❹\ntext.append(word)                                                ❹\n\nx = np.array([[wid]])\n\nfor i in range(100):                                             ❺\n\n    result = beam_search(infer_model, x, state, 7, 2)            ❻\n\n    n_probs = np.array([p for _,_,p in result[:10                ❼\n    p_j = np.random.choice(list(range(\n       n_probs.size)), p=n_probs/n_probs.sum())                  ❼\n\n    best_beam_ids, state, _ = result[p_j]                        ❽\n    x = np.array([[best_beam_ids[-1]]])                          ❽\n\n    text.extend([tokenizer.index_word[w] for w in best_beam_ids])\n\nprint('\\n')\nprint('='*60)\nprint(\"Final text: \")\nprint(''.join(text))\n```", "```py\nFinal text: \n\nchapter i. down the rabbit-hole alice was beginning to get very tired of \n➥ sitting by her sister on the bank , and there was no reason that her \n➥ father had brought him the story girl 's face .\n `` i 'm going to bed , '' said the prince , `` and you can not be able \n➥ to do it . ''\n `` i 'm sure i shall have to go to bed , '' he answered , with a smile \n➥ .\n `` i 'm so happy , '' she said .\n `` i do n't know how to bring you into the world , and i 'll be sure \n➥ that you would have thought that it would have been a long time .\n there was no time to be able to do it , and it would have been a \n➥ little thing . ''\n `` i do n't know , '' she said .\n\n...\n\n `` what is the matter ? ''\n `` no , '' said anne , with a smile .\n `` i do n't know what to do , '' said mary .\n `` i 'm so glad you come back , '' said mrs. march , with\n```", "```py\nds = tf.data.Dataset.from_tensor_slices(x)\nds = ds.window(5,shift=5).flat_map(\n    lambda window: window.batch(5, drop_remainder=True)\n)\nds = ds.map(lambda xx: (xx[:-2], xx[2:]))\n```", "```py\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(\n        input_dim=n_vocab+1, output_dim=512,input_shape=(None,)   \n    ),\n\n    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True), \n    tf.keras.layers.GRU(512, return_state=False, return_sequences=True), \n    tf.keras.layers.Dense(n_vocab, activation=’softmax’, name='final_out'),\n])\n```", "```py\nes_callback = tf.keras.callbacks.EarlyStopping(\n    monitor=’val_accuracy’, patience=5, mode=’max’, restore_best_weights=False\n)\n```", "```py\nresult = beam_search(infer_model, x, state, 3, 5)\n```"]