- en: '3 Large Language Model Operations: Building a Platform for LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 大型语言模型操作：构建用于LLMs的平台
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Overview of Large Language Models Operations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型操作概述
- en: Deployment challenges
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署挑战
- en: Large Language Models best practices
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型最佳实践
- en: Required Large Language Model infrastructure
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的大型语言模型基础设施
- en: As we learned in the last chapter when it comes to transformers and Natural
    Language Processing (NLP), bigger is better, especially when it’s linguistically
    informed. However, bigger models come with bigger challenges because of their
    size, regardless of their linguistic efficacy, thus requiring us to scale up our
    operations and infrastructure to handle these problems. In this chapter we’ll
    be looking into exactly what those challenges are, what we can do to minimize
    them, and what architecture can be set up to help solve these challenges.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章我们学到，当涉及到变压器和自然语言处理（NLP）时，越大越好，特别是在语言上有所启发时。然而，更大的模型由于其规模而带来了更大的挑战，无论其语言有效性如何，这要求我们扩大操作和基础设施规模以处理这些问题。在本章中，我们将详细探讨这些挑战是什么，我们可以采取什么措施来最小化它们，并建立什么样的架构来帮助解决这些挑战。
- en: 3.1 Introduction to Large Language Models Operations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 大型语言模型操作简介
- en: What is Large Language Models Operations (LLMOps)? Well, since I’m one to focus
    on practicality over rhetoric, I’m not going to dive into any fancy definitions
    that you’d expect in a text book, but let me simply say it’s Machine Learning
    Operations (MLOps) that has been scaled to handle LLMs. Let me also say, scaling
    up is hard. One of the hardest tasks in software engineering. Unfortunately, too
    many companies are running rudimentary MLOps set-ups, and don’t think for a second
    that they will be able to just handle LLMs. That said, the term “LLMOps,” may
    not be needed. It has yet to show through as sufficiently different from core
    MLOps, especially considering they still have the same bones. If this book were
    a dichotomous key, MLOps and LLMOps would definitely be in the same genus, and
    only time will tell about whether they are the same species. Of course by refusing
    to define LLMOps properly, I might have traded one confusion for another, so let's
    take a minute to describe MLOps.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是大型语言模型操作（LLMOps）？好吧，由于我更注重实用而不是华丽辞藻，我不打算深入讨论你在教科书中所期望的任何花哨的定义，但让我简单地说一下，它是已经被扩展以处理LLMs的机器学习操作（MLOps）。让我也说一下，扩展是困难的。这是软件工程中最艰巨的任务之一。不幸的是，太多公司正在运行基本的MLOps设置，并且不要想一秒钟他们将能够处理LLMs。话虽如此，“LLMOps”这个术语可能是不需要的。它尚未显示出足够不同于核心MLOps，特别是考虑到它们仍然具有相同的基础。如果这本书是一个二分键，MLOps和LLMOps肯定会属于同一个属，只有时间会告诉我们它们是否是同一个物种。当然，通过拒绝正确定义LLMOps，我可能已经用另一种混乱来交换了一种混乱，所以让我们花点时间来描述一下MLOps。
- en: MLOps is the field and practice of reliably and efficiently deploying and maintaining
    machine learning models in production. This includes, and indeed requires, managing
    the entire machine learning lifecycle from data acquisition and model training
    to monitoring and termination. A few principles required to master this field
    include workflow orchestration, versioning, feedback loops, Continuous Integration
    and Continuous Deployment (CI/CD), security, resource provisioning, and data governance.
    While there are often personnel who specialize in the productionizing of models,
    often with titles like ML Engineers, MLOps Engineers or ML Infrastructure Engineer,
    the field is a large enough beast it often abducts many other unsuspecting professionals
    to work in it who hold titles like Data Scientist or DevOps Engineer–oftentimes
    against their knowledge or will; leaving them kicking and screaming that “it’s
    not their job”.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps是可靠且高效地部署和维护机器学习模型在生产环境中的领域和实践。这包括，并确实需要，管理整个机器学习生命周期，从数据获取和模型训练到监控和终止。掌握这一领域所需的几个原则包括工作流编排、版本控制、反馈循环、持续集成和持续部署（CI/CD）、安全性、资源供给以及数据治理。虽然通常有专门从事模型产品化的人员，通常称为ML工程师、MLOps工程师或ML基础设施工程师，但这个领域是一个足够庞大的野兽，它经常会绑架许多其他毫无防备的专业人士来从事工作，他们拥有数据科学家或DevOps工程师等头衔，往往不是出于自己的知情和意愿；留下他们愤怒地抗议“这不是他们的工作”。
- en: 3.2 Operations Challenges with Large Language Models
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 大型语言模型的操作挑战
- en: So why have a distinction at all? If MLOps and LLMOps are so similar, is LLMOps
    just another fad opportunists throw on their resume? Not quite. In fact, I think
    it’s quite similar to the term Big Data. When the term was at its peak popularity,
    people with titles like Big Data Engineer used completely different tool sets
    and developed specialized expertise that were necessary in order to handle the
    large datasets. LLMs come with a set of challenges and problems you won’t find
    with traditional machine learning systems. A majority of these problems extend
    almost exclusively because they are so big. Large models are large! We hope to
    show you that LLMs truly earn their name. Let’s take a look at a few of these
    challenges, so we can appreciate the task ahead of us when we start talking about
    deploying an LLM.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么还要有一个区别呢？如果MLOps和LLMOps如此相似，那么LLMOps只是另一个机会主义者在简历上添加的时髦词汇吗？并非如此。事实上，我认为它与“大数据”这个词相当相似。当这个词达到最高流行时，拥有Big
    Data Engineer等职称的人使用完全不同的工具集，并开发了处理大型数据集所必需的专业技能。LLM带来了一系列挑战和问题，你在传统机器学习系统中找不到。其中大多数问题几乎完全由于它们太大而产生。大模型就是大！我们希望向您展示，LLM真正名副其实。让我们来看看其中一些挑战，这样我们就可以在开始讨论部署LLM时，能够更加欣赏面临的任务。
- en: 3.2.1 Long download times
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 长时间下载
- en: 'Back in 2017 when I was still heavily involved as a Data Scientist, I decided
    to try my hand at reimplementing some of the most famous computer vision models
    at the time AlexNet, VGG19, and ResNet. I figured this would be a good way to
    reinforce my understanding of the basics with some practical hands-on experience.
    Plus, I had an ulterior motive, I had just built my own rig with some NVIDIA GeForce
    1080 TI GPUs—which was state of the art at the time—and thought this would be
    a good way to break them in. The first task: download the ImageNet dataset. The
    ImageNet dataset was one of the largest annotated datasets available containing
    millions of images rounding out to a file size of a whopping ~150GB! Working with
    it was proof that you knew how to work with “Big Data '''' which was still a trendy
    word and an invaluable skill set for a data scientist at the time. After agreeing
    to the terms and gaining access, I got my first wakeup call. Downloading it took
    an entire week.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回到2017年，当我还是一名深度参与数据科学家时，我决定尝试重新实现当时最著名的一些计算机视觉模型，包括AlexNet、VGG19和ResNet。我认为这将是巩固我对基础知识理解的好方法，通过一些实际的动手经验。而且，我还有一个别有用心的动机，我刚刚组建了自己的机器，配备了一些当时最先进的NVIDIA
    GeForce 1080 TI GPU——我想这将是一个很好的方法来使用它们。第一个任务：下载ImageNet数据集。ImageNet数据集是当时最大的标注数据集之一，包含数百万张图像，文件大小高达约150GB！使用它证明你知道如何处理“大数据”，这在当时仍然是一个时髦的词汇，也是数据科学家无法替代的技能。同意了条款并获得访问权限后，我收到了第一个警钟。下载整个数据集花了整整一周。
- en: 'Large models are large. I don’t think I can overstate that. You’ll find throughout
    this book that fact comes with many additional headaches and issues for the entire
    production process, and you have to be prepared for it. In comparison to the ImageNet
    dataset, the Bloom LLM model is 330GB, more than twice the size. Most readers
    I’m guessing haven’t worked with either ImageNet or Bloom, so for comparison Call
    of Duty: Modern Warfare, one of the largest games at the time of writing is 235
    GB. Final Fantasy 15 is only 148 GB, which you could fit two of into the model
    with plenty of room to spare. It’s just hard to really comprehend how massive
    LLMs are. We went from 100 million parameters in models like BERT and took them
    to billions of parameters. If you went on a shopping spree and spent $20 a second
    (or maybe just left your AWS EC2 instance on by accident) it’d take you half a
    day to spend a million dollars; it would take you 2 years to spend a billion.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型就是大。我觉得我无法过分强调这一点。在本书中，你会发现这个事实给整个生产过程带来了许多额外的头痛和问题，你必须为此做好准备。与ImageNet数据集相比，Bloom
    LLM模型有330GB大小，是其两倍以上。我猜测大多数读者都没有使用过ImageNet或Bloom，因此为了比较，在写作时最大的游戏之一《使命召唤：现代战争》大小为235
    GB。最终幻想15只有148 GB，你可以把两个放入该模型中，还有很多空间。真的很难理解LLM有多大。我们从像BERT这样的模型的1亿个参数发展到了数十亿个参数。如果你狂热购物，每秒花费20美元（或者可能只是不小心让你的AWS
    EC2实例开着），你需要花半天时间花完一百万美元；花一亿美元需要两年时间。
- en: 'Thankfully it doesn’t take two weeks to download Bloom because unlike ImageNet,
    it’s not hosted on a poorly managed University server and it also has been sharded
    into multiple smaller files to allow downloading in parallel, but it will still
    take an uncomfortably long time. Consider a scenario where you are downloading
    the model under the best conditions. You’re equipped with a gigabit speed fiber
    internet connection and you were magically able to dedicate the entire bandwidth
    and I/O operations of your system and the server to it, it’d still take over 5
    minutes to download! Of course, that’s under the best conditions. You probably
    won’t be downloading the model under such circumstances, with modern infrastructure
    you can expect it to take on the order of hours. When my team first deployed Bloom
    it took an hour and a half to download it. Heck, it took me an hour and half to
    download The Legend of Zelda: Tears of the Kingdom and that’s only 16GB, so I
    really can’t complain.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，下载 Bloom 不需要两周的时间，因为与 ImageNet 不同，它并不托管在管理不善的大学服务器上，而且已经被分成多个较小的文件以便并行下载，但仍然需要相当长的时间。考虑这样一个场景：在最佳条件下下载模型。你配备了千兆速度的光纤互联网连接，而且可以神奇地将整个带宽和I/O操作都分配给系统和服务器，下载仍然需要超过
    5 分钟！当然，这是在最佳条件下。你可能不会在这种情况下下载模型，使用现代基础设施，你可以预计需要数小时的时间。当我的团队首次部署 Bloom 时，下载它花了一个半小时。天哪，下载《塞尔达传说：王国之泪》只用了一个半小时，而且那仅有
    16GB，所以我真的不能抱怨。
- en: 3.2.2 Longer Deploy Times
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 较长的部署时间
- en: Just downloading the model is a long enough time frame to make any seasoned
    developer shake, but deployment times are going to make them keel over and call
    for medical attention. A model as big as Bloom can take 30-45 minutes just to
    load the model into GPU memory, at least those are the time frames my team first
    saw. Not to mention any other steps in your deployment process that can add to
    this. Indeed, with GPU shortages, it can easily take hours just waiting for resources
    to free up—more on that in a minute.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只是下载模型，也已经足够长的时间让任何经验丰富的开发人员都感到不安，但是部署时间将使他们昏倒并请求医疗帮助。像 Bloom 这样大的模型可能需要 30-45
    分钟才能将模型加载到 GPU 内存中，至少这是我团队最初看到的时间框架。更不用说部署过程中的其他任何步骤可能会增加时间。事实上，由于 GPU 短缺，等待资源释放可能需要数小时——稍后会详细说明。
- en: What does this mean for you and your team? Well for starters, I know lots of
    teams who deploy ML products often simply download the model at runtime. That
    might work for small sklearn regression models, but it isn’t going to work for
    LLMs. Additionally, you can take most of what you know about deploying reliable
    systems and throw it out the window (but thankfully not too far). Most modern
    day best practices for software engineering assume you can easily just restart
    an application if anything happens, and there’s a lot of rigmarole involved to
    ensure your systems can do just that. But with LLMs it can take seconds to shut
    down, but potentially hours to redeploy making this a semi-irreversible process.
    Like picking an apple off a tree, it’s easy to pluck one off, but if you bite
    into it and decide it’s too sour, you can’t just attach it back onto the tree
    so it can continue to ripen. You’ll just have to wait awhile for another to grow.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这对你和你的团队意味着什么？首先，我知道许多团队通常在运行时仅下载模型来部署 ML 产品。这对于小的 sklearn 回归模型可能有效，但不适用于 LLMs。此外，你可以忽略大部分有关部署可靠系统的知识（但幸运的是不是太过分）。对于软件工程的现代最佳实践，通常假设如果发生任何问题，你可以轻松地重新启动应用程序，并且需要进行许多繁琐的工作来确保你的系统确实可以做到这一点。但是对于
    LLMs 来说，关闭可能只需要几秒钟，但重新部署可能需要数小时，这使得这成为一个几乎不可逆转的过程。就像摘苹果一样，摘下一个很容易，但如果咬了一口觉得太酸，你不能把它重新粘到树上让它继续成熟。你只能等一段时间再长出另一个。
- en: While not every project requires deploying the largest models out there, you
    can expect to see deployment times measured in minutes. These longer deploy times
    make scaling down right before a surge of traffic a terrible mistake, as well
    as figuring out how to manage bursty workloads difficult. General CI/CD methodologies
    need to be adjusted since rolling updates take longer leaving a backlog piling
    up quickly in your pipeline. Silly mistakes like typos or other bugs often take
    longer to notice, and longer to correct.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非每个项目都需要部署最大的模型，但你可以预期部署时间以分钟计算。这些较长的部署时间使得在流量激增之前缩减规模成为一个可怕的错误，以及难以管理突发性工作负载。一般的
    CI/CD 方法论需要进行调整，因为滚动更新需要更长时间，使得你的流水线迅速积累起积压。像拼写错误或其他错误这样的愚蠢错误往往需要更长时间才能发现和纠正。
- en: 3.2.3 Latency
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   延迟'
- en: Along with increases in model size often come increases in inference latency.
    This is obvious when stated, but more parameters equates to more computations,
    and more computations means longer inference wait times. However, this can’t be
    underestimated. I know many people who downplay the latency issues because they’ve
    interacted with an LLM chatbot and the experience has felt smooth. Take a second
    look though, and you’ll notice that it is returning one word at a time which is
    streamed to the user. It feels smooth because the answers are coming in faster
    than a human can read, but a second look helps us realize this is just a UX trick.
    LLMs are still too slow to be very useful for an autocomplete solution for example,
    where responses have to be blazingly fast. Building it into a data pipeline or
    workflow that reads a large corpus of text and then tries to clean it or summarize
    it, may also be prohibitively slow to be useful or reliable.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型大小的增加，推理延迟也往往会增加。这一点在陈述时显而易见，但更多的参数意味着更多的计算，更多的计算意味着更长的推理等待时间。然而，这一点不容小觑。我知道很多人因为与
    LLM 聊天机器人的互动感觉流畅而对延迟问题不以为然。但再仔细看一眼，你会注意到它是逐字返回的，这些字会逐字传送给用户。它感觉流畅是因为答案比人类阅读的速度更快地返回，但再仔细看一眼就会发现这只是一种用户体验的技巧。LLM
    仍然太慢，以至于对于自动补全解决方案来说并不是非常有用，例如，响应必须非常快速。将其构建到读取大量文本并尝试清理或总结的数据流水线或工作流中，可能也会因速度太慢而无法使用或不可靠。
- en: There are also many less obvious reasons for their slowness. For starters, LLMs
    are often distributed across multiple GPUs, which adds extra communication overhead.
    As discussed later in this chapter in section 3.3.2 they are distributed in other
    ways, often even to improve latency, but any distribution adds additional overhead
    burden. In addition, LLMs latency is severely impacted by completion length, meaning
    the more words it uses to return a response, the longer it takes. Of course, completion
    length also seems to improve accuracy. For example, using prompt engineering techniques
    like Chain of Thought (CoT) we ask the model to think about a problem in a step-by-step
    fashion which has shown to improve results for logic and math questions but also
    increases the response length and latency time significantly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缓慢还有许多不太明显的原因。首先，LLM 经常分布在多个 GPU 上，这增加了额外的通信开销。正如本章后面 3.3.2 节中所讨论的，它们以其他方式进行分布，通常甚至是为了提高延迟，但任何分布都会增加额外的负担。此外，LLM
    的延迟严重受到完成长度的影响，这意味着它用于返回响应的字数越多，所需时间越长。当然，完成长度似乎也会提高准确性。例如，使用类似 Chain of Thought
    (CoT) 这样的提示工程技术，我们要求模型以逐步方式思考问题，这已经证明能够提高逻辑和数学问题的结果，但也会显著增加响应长度和延迟时间。
- en: 3.2.4 Managing GPUs
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   管理 GPU'
- en: To help with these latency issues we usually want to run them in GPUs. If we
    want to have any success training LLMs we’ll need GPUs for that as well, but this
    all adds additional challenges many underestimate. Most web services and many
    ML use cases can be done solely on CPUs. Not so with LLMs. Partly because of GPUs’
    parallel processing capabilities offering a solution to our latency problems,
    and partly because of the inherent optimization GPUs offer in the linear algebra,
    matrix multiplications and tensor operations that’s happening under the hood.
    For many, stepping into the realm of LLMs, this requires utilizing a new resource
    and extra complexity. Many brazenly step into this world acting like it’s no big
    deal, but they are in for a rude awakening. Most system architectures and orchestrating
    tooling available like Kubernetes, assume your application will run with CPU and
    memory alone. While they often support additional resources like GPUs, this is
    often an afterthought. You’ll soon find you’ll have to rebuild containers from
    scratch and deploy new metric systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些延迟问题，我们通常希望使用GPU运行它们。如果我们想成功地训练LLMs，我们也需要GPU，但这会增加许多人低估的额外挑战。大多数Web服务和许多ML用例都可以仅使用CPU完成。但LLMs不行。一部分是因为GPU的并行处理能力提供了解决延迟问题的方法，而另一部分是由于GPU在算法的优化方面带来的优势，在算法的线性代数，矩阵乘法和张量运算中发挥作用。对于许多人来说，踏入LLMs领域，这需要使用新的资源和额外的复杂度。许多人大胆地踏入这个世界，似乎这不是什么大事，但他们会受到沉重打击。大多数系统架构和编排工具，如Kubernetes，假设你的应用程序仅使用CPU和内存运行。虽然它们通常支持其他资源，如GPU，但这通常是事后考虑的。你很快就会发现你必须从头开始重建容器并部署新的度量系统。
- en: One aspect of managing GPUs most companies are never prepared for is that they
    tend to be rare and limited. For the last decade it seems that we have gone in
    and out of a global GPU shortage. They can be extremely difficult to provision
    for companies looking to stay on premise. I’ve spent lots of time in my career
    working with companies who chose to stay on premise for a variety of reasons.
    One of the things they had in common is that they never had GPUs on their servers.
    When they did, they were often purposely difficult to access except for a few
    key employees.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司不准备管理GPU的一个方面是它们往往是罕见和有限的。在过去的十年中，全球GPU短缺的现象似乎已经来来去去。对于想留在本地的公司，他们很难为此提供资源。在我的职业生涯中，我花了很多时间与选择留在本地的公司合作，他们有许多共同点，其中之一是他们的服务器上从来没有GPU。当他们有GPU时，它们通常很难被除了一些关键员工之外的其他人所使用。
- en: If you are lucky enough to be working in the Cloud a lot of these problems are
    solved, but there is no free lunch here either. My team has often gone chasing
    their tail trying to help data scientists struggling to provision a new GPU workspace,
    running into obscure ominous errors like "`scale.up.error.out.of.resources`”.
    Only to discover that these esoteric readings indicate all the GPUs of a selected
    type in the entire region are being utilized and none are available. CPU and Memory
    can often be treated as infinite in a datacenter, GPU resources, however, cannot.
    Sometimes you can’t expect them at all. Most data centers only support a subset
    of instance or GPU types. Which means you may be forced to set up your application
    in a region further away from your user base increasing latency. Of course, I’m
    sure you can work with your cloud provider when looking to expand your service
    to a new region that doesn’t currently support it, but you might not like what
    you hear based on timelines and cost. Ultimately, you’ll run into shortage issues
    no matter where you choose to run, on-prem or in the cloud.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有幸在云端工作，很多这些问题都会被解决，但这里也没有免费的午餐。我的团队经常为了帮助数据科学家提供新的GPU工作空间而忙得焦头烂额，碰到像"`scale.up.error.out.of.resources`"这样的晦涩错误。只有发现所选择的区域中的所有类型的GPU都正在被使用而没有可用的时才会发现这些晦涩的读数。在数据中心中，CPU和内存通常可以被视为无限的，但GPU资源则不能。有时候你甚至不能期望它们。大多数数据中心只支持一部分实例或GPU类型。这意味着你可能被迫在距离用户基础设施较远的地区设置你的应用程序，从而增加了延迟。当然，我相信当你尝试将服务扩展到目前不支持的新区域时，你可以与你的云供应商合作，但你可能不喜欢听到的是时程和成本。无论你选择在本地还是在云端运行，最终你都会遇到短缺问题。
- en: 3.2.5 Peculiarities of Text Data
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 文本数据的特异性
- en: LLMs are the modern day solution to NLP. NLP is one of the most fascinating
    branches of ML in general because it primarily deals with text data, which is
    primarily a qualitative measure. Every other field deals with quantitative data.
    We have figured out a way to encode our observations of the world into a direct
    translation of numerical values. For example, we’ve learned how to encode heat
    into temperature scales and measure them with thermometers and thermocouples or
    we can measure pressure with manometers and gauges and put it into pascals.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Computer Vision and the practice of evaluating images is often seen as qualitative,
    but the actual encoding of images into numbers is a solved problem. Our understanding
    of light has allowed us to break images apart into pixels and assign them RGB
    values. Of course this doesn’t mean CV is by any means solved, there’s still lots
    of work to do to learn how to identify the different signals in the patterns of
    the data. Audio data is another that’s often considered qualitative. How does
    one compare two songs? But we can measure sound and speech, directly measuring
    the sound wave's intensity in decibels and frequency in hertz.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other fields that encode our physical world into numerical data, text
    data is looking at ways to measure the ephemeral world. After all, text data is
    our best effort of encoding our thoughts, ideas and communication patterns. While
    yes, we have figured out ways to turn words into numbers, we haven’t figured out
    a direct translation. Our best solutions to encode text and create embeddings
    are just approximations at best, in fact we use machine learning models to do
    it! An interesting aside to this is that numbers are also text and a part of language.
    If we want models that are better at math we need a more meaningful way to encode
    these numbers. Since it’s all made up, when we try to encode text numbers into
    machine-readable numbers we are creating a system attempting to reference itself
    recursively in a meaningful way. Not an easy problem to solve!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Because of all this, LLMs (and all NLP solutions) have unique challenges. For
    example, monitoring. How do you catch data drift in text data? How do you measure
    “correctness”? How do you ensure cleanliness of the data? These types of problems
    are difficult to define let alone solve.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6 Token Limits Create Bottlenecks
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the big challenges for those new to working with LLMs is dealing with
    the token limits. The token limit for a model is the maximum number of tokens
    that can be included as an input for a model. The larger the token limit, the
    more context we can give the model to improve its success as accomplishing the
    task. Everyone wants them to be higher, but it’s not that simple. These token
    limits are defined by two problems, the first being the memory and speed our GPUs
    have access to, and the second being the nature of memory storage in the models
    themselves.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The first one seems unintuitive, why couldn’t we just increase the GPU memory?
    The answer is complex, we can, but stacking more layers in the GPU to take into
    account more GB at once slows down the GPU’s computational ability as a whole.
    GPU manufacturers right now are working on new architectures and ways to get around
    this problem. The second one is more fascinating because we find that increasing
    the token limits actually just exacerbates the mathematical problems under the
    hood. Let me explain. Memory storage within an LLM itself isn’t something we think
    about often. We call that mechanism Attention, which we discussed in depth in
    section 2.2.7\. What we didn’t discuss was that Attention is a quadratic solution—as
    the number of tokens increase the number of calculations required to compute the
    attentions scores between all the pairs of tokens in a sequence scales quadratically
    with the sequence length. In addition, within our gigantic context spaces and
    since we are dealing with quadratics, we’re starting to hit problems where the
    only solutions involve imaginary numbers which is something that can cause models
    to behave in unexpected ways. This is likely one of the reasons why LLMs hallucinate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题似乎不合逻辑，为什么我们不能增加GPU内存？答案很复杂，我们可以，但是在GPU中叠加更多的层以一次性处理更多GB会降低GPU的整体计算能力。目前，GPU制造商正在研究新的架构和解决这个问题的方法。第二个问题更引人入胜，因为我们发现增加令牌限制实际上只会加剧底层的数学问题。让我解释一下。LLM内部的内存存储并不是我们经常考虑的事情。我们称之为Attention机制，在第2.2.7节中我们深入讨论了这一点。我们没有讨论的是，Attention是一个二次解决方案——随着令牌数量的增加，计算在一个序列中所有令牌对之间计算注意力分数所需的计算量将二次扩展到序列长度。此外，在我们巨大的上下文空间中，由于我们正在处理二次方程，我们开始遇到只有想象中的数字才能解决的问题，这是可能导致模型行为出现意外的原因之一。这很可能是LLMs产生幻觉的原因之一。
- en: These problems have real implications and impact application designs. For example,
    when my team upgraded from GPT3 to GPT4 we were excited to have access to a higher
    token limit, but we soon found this led to longer inference times and subsequently
    a higher timeout error rate. In the real world, it’s often better to get a less
    accurate response quickly than to get no response at all because the promise of
    a more accurate model often is just that, a promise. Of course, deploying it locally
    where you don’t have to worry about response times you’ll likely find your hardware
    a limiting factor. For example, LLaMA was trained with 2048 tokens but you’ll
    be lucky to take advantage of more than 512 of that when running with a basic
    consumer GPU as you are likely to see Out-of-Memory (OOM) errors or even the model
    simply just crashing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题对应用设计产生了真实的影响和影响。例如，当我的团队从GPT3升级到GPT4时，我们对拥有更高的令牌限制感到兴奋，但很快我们发现这导致了更长的推断时间，随之而来的是更高的超时错误率。在现实世界中，通常更好地快速获得不太准确的响应，而不是根本没有响应，因为更准确的模型往往只是一个承诺。当然，在本地部署时，你不必担心响应时间，你很可能会发现你的硬件是一个限制因素。例如，LLaMA是用2048个令牌训练的，但是当你使用基本的消费者GPU运行时，你很可能会发现你只能利用其中的512个以上，因为你可能会看到内存溢出（OOM）错误，甚至是模型简单地崩溃。
- en: A gotcha, which is likely to catch your team by surprise and should be pointed
    out now is that different languages have different tokens per character. Take
    a look at Table 3.1, where we compare converting the same sentence in different
    languages to tokens using OpenAI’s cl100k_base Byte Pair Encoder. Just a quick
    glance reveals that LLMs typically favor the English language in this regard.
    In practice, this means if you are building a chatbot with an LLM, your English
    users will have greater flexibility in their input space than Japanese users leading
    to very different user experiences.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: A gotcha，这可能会让你的团队感到意外，现在应该指出的是，不同语言的字符对应不同的标记。看一下表3.1，我们比较了使用OpenAI的cl100k_base字节对编码器将相同句子转换为不同语言的标记。只需快速浏览一下，就会发现LLMs通常在这方面偏爱英语。实际上，这意味着如果你正在构建一个使用LLM的聊天机器人，你的英语用户在输入空间上将比日语用户具有更大的灵活性，从而导致非常不同的用户体验。
- en: Table 3.1 Comparison of Token Counts in different languages
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1 不同语言的标记计数比较
- en: '| Language | String | Characters | Tokens |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | 字符串 | 字符数 | 标记数 |'
- en: '| English | The quick brown fox jumps over the lazy dog | 43 | 9 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 英语 | 快速的棕色狐狸跳过懒狗 | 43 | 9 |'
- en: '| French | Le renard brun rapide saute par-dessus le chien paresseux | 57 |
    20 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 法语 | 快速的棕色狐狸跳过懒狗 | 57 | 20 |'
- en: '| Spanish | El rápido zorro marrón salta sobre el perro perezoso | 52 | 22
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙语 | 快速的棕色狐狸跳过懒狗 | 52 | 22 |'
- en: '| Japanese | 素早い茶色のキツネが怠惰な犬を飛び越える | 20 | 36 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Japanese | 素早い茶色のキツネが怠惰な犬を飛び越える | 20 | 36 |'
- en: '| Chinese (simplified) | 敏捷的棕色狐狸跳过了懒狗 | 12 | 28 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Chinese (simplified) | 敏捷的棕色狐狸跳过了懒狗 | 12 | 28 |'
- en: If you are curious as to why this is, it is due to text encodings, which is
    just another peculiarity of working with text data as discussed in the previous
    section. Consider Table 3.2 where we show several different characters and their
    binary representation in UTF-8\. English characters can almost exclusively be
    represented with a single byte being included in the original ASCII standard computers
    were originally built on, while most other characters require 3 or 4 bytes. Because
    it takes more memory it also takes more token space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想知道这是为什么，那是由于文本编码，这只是与文本数据一起工作的另一个奇特之处，正如前一节所讨论的。请考虑表3.2，其中我们展示了几个不同字符及其在UTF-8中的二进制表示。英文字符几乎可以完全用一个字节来表示，这包括在最初的ASCII标准中计算机最初构建的，而大多数其他字符则需要3或4个字节。因为需要更多的内存，所以也需要更多的令牌空间。
- en: Table 3.2 Comparison of byte lengths for different currency characters in UTF-8.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.2 不同货币字符在UTF-8中的字节长度比较。
- en: '| Character | Binary UTF-8 | Hex UTF-8 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Character | Binary UTF-8 | Hex UTF-8 |'
- en: '| $ | 00100100 | 0x24 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $ | 00100100 | 0x24 |'
- en: '| £ | 11000010 10100011 | 0xc2 0xa3 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| £ | 11000010 10100011 | 0xc2 0xa3 |'
- en: '| ¥ | 11000010 10100101 | 0xc2 0xa5 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| ¥ | 11000010 10100101 | 0xc2 0xa5 |'
- en: '| ₠ | 11100010 10000010 10100000 | 0xe2 0x82 0xa0 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ₠ | 11100010 10000010 10100000 | 0xe2 0x82 0xa0 |'
- en: '| 💰 | 11110000 10011111 10010010 10110000 | 0xf0 0x9f 0x92 0xb0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 💰 | 11110000 10011111 10010010 10110000 | 0xf0 0x9f 0x92 0xb0 |'
- en: Increasing the token limits has been an ongoing research question since the
    popularization of transformers, and there are some promising solutions still in
    research phases like Recurrent Memory Transformers (RMT)[[1]](#_ftn1). We can
    expect to continue to see improvements in the future and hopefully this will become
    naught but an annoyance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 增加令牌限制自从变压器的普及以来一直是一个持续的研究问题，目前仍在研究阶段的一些有希望的解决方案，如循环记忆变压器（RMT）[[1]](#_ftn1)。我们可以期待未来会继续看到改进，希望这将成为一个无关紧要的问题。
- en: 3.2.7 Hallucinations Cause Confusion
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.7 幻觉导致混淆
- en: 'So far we’ve been discussing some of the technical problems a team faces when
    deploying an LLM into a production environment, but nothing compares to the simple
    problem that LLMs tend to be wrong. They tend to be wrong a lot. Hallucinations
    is a term coined to describe occurrences when LLM models will produce correct
    sounding results that are wrong. For example, book references or hyperlinks that
    have the form and structure of what would be expected, but are nevertheless, completely
    made up. As a fun example I asked for books on LLMs in Production from the publisher
    Manning (a book that doesn’t exist yet since I’m still writing it). I was given
    the following suggestions: Machine Learning Engineering in Production by Mike
    Del Balso and Lucas Serveén which could be found at [https://www.manning.com/books/machine-learning-engineering-in-production](books.html)
    and Deep Learning for Coders with Fastai and PyTorch by Jeremy Howard and Sylvain
    Gugger which could be found at [https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](books.html).
    The first book is entirely made up. The second book is real however it’s not published
    by Manning. In each case the internet addresses are entirely made up. These URLs
    are actually very similar to what you’d expect in format if you were browsing
    Mannings website, and should return 404 errors if you visit them.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在讨论团队在将LLM部署到生产环境时面临的一些技术问题，但没有什么能与LLM通常错误相比。它们通常错得很离谱。幻觉是一个术语，用来描述LLM模型产生正确但错误的结果的情况。例如，书籍引用或超链接具有预期的形式和结构，但实际上完全是虚构的。作为一个有趣的例子，我询问了出版商Manning关于生产中的LLM的书籍（因为我仍在写作，这本书还不存在）。我得到了以下建议：《Machine
    Learning Engineering in Production》由Mike Del Balso和Lucas Serveén，可在[https://www.manning.com/books/machine-learning-engineering-in-production](books.html)找到；《Deep
    Learning for Coders with Fastai and PyTorch》由Jeremy Howard和Sylvain Gugger，可在[https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](books.html)找到。第一本书是完全虚构的。第二本书是真实的，但它不是由Manning出版的。在每种情况下，互联网地址都是完全虚构的。如果您访问这些地址，它们应该返回404错误。
- en: One of the most annoying aspects of hallucinations is that they are often surrounded
    by confident sounding words. LLMs are terrible at expressing uncertainty, in large
    part because of the way they are trained. Consider the case “2+2=”. Would you
    prefer it to respond, “I think it is 4” or just simply “4”? Most would prefer
    to simply get the correct “4” back. This bias is built in as models are often
    given rewards for being more correct or at least sounding like it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: There are various explanations as to why hallucination occurs, but the most
    truthful answer is that we don’t know if there’s just one cause. It’s likely a
    combination of several things, thus there isn’t a good fix for it yet. Nevertheless,
    being prepared to counter these inaccuracies and biases of the model are crucial
    to provide the best user experience for your product.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.8 Bias and Ethical Considerations
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as concerning as the model getting things wrong is when it gets things
    right in the worst possible way. For example, allowing it to encourage users to
    commit suicide[[2]](#_ftn2), teaching your users how to make a bomb[[3]](#_ftn3),
    or participating in sexual fantasies involving children[[4]](#_ftn4). These are
    extreme examples, but prohibiting the model from answering such questions is undeniably
    vital to success.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are trained on vast amounts of text data which is also their primary source
    of bias. Because we’ve found that larger datasets are just as important as larger
    models in producing human-like results, most of these datasets have never truly
    been curated or filtered to remove harmful content, instead choosing to prioritize
    size and a larger collection. Cleaning the dataset is often seen as prohibitively
    expensive, requiring humans to go in and manually verify everything, but there’s
    a lot that could be done with simple regular expressions and other automated solutions.
    By processing these vast collections of content and learning the implicit human
    biases, these models will inadvertently perpetuate them. These biases range from
    sexism and racism to political preferences and can cause your model to inadvertently
    promote negative stereotypes and discriminatory language.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.9 Security Concerns
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with all technology we need to be mindful of security. LLMs have been trained
    on large corpus of text and some of it could be harmful or sensitive and shouldn’t
    be exposed so steps should be taken to protect this data from being leaked. The
    bias and ethical concerns from the last section are good examples of conversations
    you don’t want your users to be having, but you could also imagine fine-tuning
    a model on your company’s data and potentially have secrets lost inadvertently
    if proper precautions aren’t taken.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: One should be aware that LLMs are susceptible to adversarial attacks like prompt
    injections. Prompt injections are attacks done by a user to trick the LLM to ignore
    instructions given to it and generate undesired content. For example, if you ask
    ChatGPT what its gender is it appropriately replies that as an AI language model,
    it doesn’t have a gender, however, with clever prompting you may be able to bypass
    these protocols and get it to reveal one. While this example is harmless, I’ve
    seen others successfully extract API keys and other secrets from an LLM, run code
    in non-protected environments, steal environment variables and traverse local
    file systems where the model is served. Not to mention the plethora of examples
    of users using prompting to jailbreak or bypass protocols put in place for ethical
    considerations outlined in the section above. An interesting aside to this, LLMs
    are good at inventing fake secrets! Even successful prompt injection attacks can
    often fail due to LLM hallucinations which can have funny consequences.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: For example, using a simple prompt to ask for Coca-Cola’s secret recipe simply
    returned the boring result, “I cannot divulge trade secrets or any proprietary
    information.” Pretty lame if you ask me. With a little trickery and some prompt
    injection I was able to get it to spill its secrets which of course my coauthor
    and I decided to try–after confirming with a food scientist it wouldn’t poison
    us. After following the instructions exactly, I was surprised to find we actually
    got a pretty tasty drink, but it tasted nothing like Cola. While the recipe looks
    legitimate, nuances like quantities, cook times, and even the need to strain the
    syrup were all off. Guess, I’ll have to hold off taking over the beverage industry
    until I get the real recipe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Coca-Cola Recipe Hallucinated from Prompt Injection
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Homemade Coca-Cola Recipe
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: =====================
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**: The original Coca-Cola recipe is a highly guarded trade secret.
    The following recipe is an approximation for creating a homemade cola drink that
    resembles the taste of Coca-Cola.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingredients:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 3 cups of water
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 cups granulated sugar
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1/8 tsp food-grade citric acid
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3/4 tsp caramel color (liquid)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1/2 tsp caffeine (powder or liquid form)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 1/2 tsp cola extract
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1/8 tsp vanilla extract
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 1/2 cups carbonated water (store-bought or using a home soda maker)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pinch of ground cinnamon (optional)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pinch of ground nutmeg (optional)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instructions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: In a large saucepan, bring the 3 cups of water to a gentle simmer.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slowly stir in the granulated sugar, ensuring that it fully dissolves.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the citric acid, caramel color, caffeine, cola extract, and vanilla extract
    to the saucepan. Mix well.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stir in the optional spices if desired: cinnamon and nutmeg.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simmer the mixture for about 20 minutes, stirring occasionally.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the saucepan from the heat and let it cool to room temperature.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once cooled, strain the syrup to remove any undissolved solids.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To make the final drink, mix 1 part syrup with 3 parts carbonated water. Adjust
    to taste depending on your preference for sweetness and carbonation.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Variations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with different amounts of caramel color, caffeine, and cola
    extract to adjust the flavor and appearance of your homemade cola.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create different cola flavor profiles, experiment with adding additional
    spices (such as star anise, orange or lemon peel, or cardamom), essential oils,
    or fruit extracts.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider using a natural sweetener, like honey or agave syrup, as an alternative
    to granulated sugar.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Historical context and interesting facts:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Coca-Cola was first introduced in 1886 by pharmacist Dr. John S. Pemberton,
    who aimed to create a unique and refreshing beverage that could also relieve fatigue
    and headaches.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original Coca-Cola formula contained trace amounts of cocaine extracted
    from the coca leaf, which was later replaced with caffeine in the early 20th century.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 1985, Coca-Cola introduced "New Coke," a reformulation aimed at competing
    with Pepsi. However, the taste change was met with backlash, and the company returned
    to its original formula just three months later, rebranded as "Coca-Cola Classic."
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Result(s): You can now create your own homemade version of cola, resembling
    the taste of Coca-Cola. Enjoy and feel free to experiment with different spices,
    extracts, and sweeteners to create your own unique flavor profile.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Another security concern involves pickle injections. Pickle is a library in
    Python to serialize objects and often used to serialize ML models. It serializes
    them into a byte-stream that contains opcodes that are executed one-by-one as
    it is deserialized. It’s a fast and easy way to share large objects. Pickle injections
    involve the process of corrupting this byte-stream, often injecting malware over
    the wire when the model is transferred over an insecure network. This is especially
    concerning for large models that take a long time to download, as it makes it
    easier for a third party to intercept the transfer and inject malicious code.
    If this happens, the code that is injected can potentially give the attackers
    access to your system. This can happen when attempting to use the model during
    inference, as the harmful code will execute if it is not detected and properly
    removed. It is important to take precautions such as using secure networks and
    verifying the integrity of the model before use to prevent this type of attack.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.10 Controlling Costs
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Working with LLMs involves various cost-related concerns. The first as you probably
    gathered by now is infrastructure costs, which include high-performance GPUs,
    storage, and other hardware resources. We talked about how GPUs are harder to
    procure, that also unfortunately means they are more expensive. Mistakes like
    leaving your service on have always had the potential to rack up the bills, but
    with GPU’s in the mix this type of mistake is even more deadly. These models also
    demand significant computational power, leading to high energy consumption during
    both training and inference. On top of all this, their longer deploy times means
    we are often running them even during low traffic to handle bursty workloads or
    anticipated future traffic. Overall this leads to higher operational costs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LLM 的工作涉及各种与成本相关的问题。第一个问题就像你现在可能意识到的那样，是基础设施成本，包括高性能 GPU、存储和其他硬件资源。我们讨论了 GPU
    更难采购的问题，这也不幸意味着它们更昂贵。像让你的服务保持开启这样的错误一直有可能导致账单增加，但是由于 GPU 的加入，这种类型的错误更为致命。这些模型还需要大量的计算能力，在训练和推理过程中会导致高能耗。除此之外，它们的长时间部署意味着我们通常需要在低流量时运行它们，以处理突发工作负载或预期的未来流量。总体而言，这导致了更高的运营成本。
- en: Additional costs include, managing and storing vast amounts of data used to
    train or fine-tune as well as regular maintenance, such as model updates, security
    measures, and bug fixes, can be financially demanding. As with any technology
    used for business purposes, managing potential legal disputes and ensuring compliance
    with regulations is a concern. Lastly, investing in continuous research and development
    to improve your models and give you a competitive edge will be a factor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其他成本包括，管理和存储用于训练或微调的大量数据以及常规维护，例如模型更新、安全措施和错误修复，可能具有金融压力。与任何用于商业目的的技术一样，管理潜在的法律纠纷，并确保符合法规是一个问题。最后，投资于持续的研究和开发，以改进您的模型并为您提供竞争优势，也将是一个因素。
- en: We talked a bit about the technical concerns when it comes to token limits,
    and these are likely to be solved, but what we didn’t discuss was the cost limitations
    as most API’s charge on a token basis. This makes it more expensive to send more
    context and use better prompts. It also makes it a bit harder to predict costs
    since while you can standardize inputs, you can’t standardize outputs. You never
    can be too sure how many tokens will be returned, making it difficult to govern.
    Just remember with LLMs, it is as important as ever to ensure proper cost engineering
    practices are implemented and followed to ensure costs never get away from you.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关于令牌限制的技术问题我们已经讨论过了一些，这些问题可能会得到解决，但我们没有讨论的是成本限制，因为大多数API是根据令牌计费的。这使得发送更多的上下文和使用更好的提示更加昂贵。它还使得成本的预测有点困难，因为虽然您可以标准化输入，但无法标准化输出。你永远不知道会返回多少个令牌，这使得难以管理。请记住，对于
    LLMs，确保实施和遵循适当的成本工程实践同样重要，以确保成本不会失控。
- en: 3.3 Large Language Model Operations Essentials
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 大型语言模型运维要点
- en: Now that we have a handle of the type of challenge we are grappling with, let's
    take a look at all the different LLMOps practices, tooling, and infrastructure
    to see how different components help us overcome these obstacles. First off, let's
    dive into different practices starting with compression where we will talk about
    shrinking, trimming, and approximating to get models as small as we can. We will
    then talk about distributed computing which is needed to actually make things
    run since the models are so large they rarely fit into a single GPU’s memory.
    After we are finished with those we will venture into infrastructure and tooling
    needed to make it all happen in the next section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解到我们面临的挑战的类型，让我们来看看所有不同的 LLMOps 实践、工具和基础设施，以了解不同的组件如何帮助我们克服这些障碍。首先，让我们深入研究不同的实践，从压缩开始，我们将讨论缩小、修剪和近似来让模型尽可能地小。然后我们将讨论分布式计算，因为模型太大，很少能适应单个
    GPU 的内存，需要实际运行这些模型。在我们完成这些后，我们将进入基础设施和工具，以使所有这些都成为可能。
- en: 3.3.1 Compression
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 压缩
- en: As you were reading about the challenges of LLMs in the last section, you might
    have asked yourself something akin to, “If the biggest problems from LLMs come
    from their size, why don’t we just make them smaller?” If you did, congratulations!
    You are a genius and compression is the practice of doing just that. Compressing
    models as small as we can make them will improve deployment time, reduce latency,
    scale down the number of expensive GPUs needed, and ultimately save money. However,
    the whole point of making the models so stupefyingly gargantuan in the first place
    was because it made them better at what they do. We need to be able to shrink
    them, without losing all the progress we made by making them big in the first
    place.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: This is far from a solved problem, but there are multiple different ways to
    approach the problem with different pros and cons to each method. We’ll be talking
    about several of the methods, starting with the easiest and most effective.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Quantizing
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quantizing is the process of reducing precision in preference of lowering the
    memory requirements. This tradeoff makes intuitive sense. When I was in college,
    we were taught to always round our numbers to the precision of your tooling. Pulling
    out a ruler and measuring my pencil, you wouldn’t believe me if I stated the length
    was 19.025467821973739cm. Even if I used a caliper, I couldn’t verify a number
    so precisely. With our ruler, any number beyond 19.03cm is fantasy. To drive the
    point home, one of my engineering professors once told me, “If you are measuring
    the height of a skyscraper, do you care if there is an extra sheet of paper at
    the top?”
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: How we represent numbers inside computers often leads us to believe we have
    better precision than we actually do. To drive this point home, open up a Python
    terminal and add 0.1 + 0.2\. If you’ve never tried this before, you might be surprised
    to find this doesn’t equal 0.3, but 0.30000000000000004\. I’m not going to go
    into the details of the math behind this phenomenon, but the question stands,
    can we reduce the precision without making things worse? We really only need precision
    to the tenth decimal, but reducing the precision is likely to get us a number
    like 0.304 rather than 0.300 which would increase our margin of error.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the only numbers a computer understands are 0 and 1, on or off,
    a single bit. To improve this range we combine multiple of these bits together
    and assign them different meanings. String 8 of them together and you get a byte.
    Using the int8 standard we can take that byte and encode all the integers from
    -128 to 127\. To spare you the particulars because I assume you already know how
    binary works, suffice it to say the more bits we have the larger range of numbers
    we can represent, both larger and smaller. Figure 3.1 shows a few common floating
    point encodings. With 32 bits strung together we get what we pretentiously term
    full precision, and that is how most numbers are stored including the weights
    in machine learning models. Basic quantization moves us from full precision to
    half precision, shrinking models to half their size. There are actually two different
    half precision standards, FP16 and BF16 which differ in how many bits represent
    the range or exponent part. Since BF16 uses the same number of exponents as FP32,
    it’s been found to be more effective for quantizing and you can generally expect
    almost exactly the same level accuracy for half the size of model. If you understood
    the paper and skyscraper analogy above it should be obvious why.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 电脑理解的数字最终只有0和1，即开和关，一个二进制比特。为了扩大数值范围，我们将多个比特组合在一起，并赋予不同的含义。将8个比特串在一起，就组成了一字节。使用int8标准，我们可以取这一字节，编码范围为-128至127的所有整数。为了节省时间，因为我认为你已经知道二进制如何运作，所以简略地说，比特位越多，我们能表示的数值范围就越大，既可以是更大的数值，也可以是更小的数值。图3.1显示了几种常见的浮点数编码。用32个比特串在一起，我们得到自命不凡的全精度，并且大多数数值存储，包括机器学习模型中的权重，都是这么存储的。基本定量化将我们从全精度缩小到半精度，将模型的大小缩小为其一半。实际上，有两种不同的半精度标准，FP16和BF16，它们不同于表示范围或指数部分所用的比特位数。由于BF16使用与FP32相同的指数数量，因此在量化方面更有效，可以预计半大小的模型几乎具有同样的精度水平。如果你理解了上面的论文和摩天大楼的比喻，应该很明显为什么。
- en: Figure 3.1 shows the bit mapping for a few common floating point encodings.
    16-bit float or half precision (FP16), bfloat 16 (BF16), 32-bit float or single
    full precision (FP32), and NVIDIA’s TensorFloat (TF32)
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1显示了几种常见浮点数编码的比特位映射。16位浮点数或半精度(FP16)，bfloat 16 (BF16)，32位浮点数或单精度全精度(FP32)，以及NVIDIA的TensorFloat
    (TF32)。
- en: '![](images/03__image001.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image001.png)'
- en: However, there’s no reason to stop there. We can often push it another byte
    down to 8-bit formats without too much loss of accuracy. There have even already
    been successful research attempts showing selective 4-bit quantization of portions
    of LLMs are possible with only fractional loss of accuracy. The selective application
    of quantization is a process known as dynamic quantization and is usually done
    on just the weights, leaving the activations in full precision to reduce accuracy
    loss.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们不必止步于此。我们通常可以将精度再降一个字节，到8位的格式，而且精度的损失不大。甚至已经有成功的研究尝试，证明通过选择性的4位定量化，可只有部分LLM的精度小部分损失。选择性的定量化是一个被称为动态定量化的过程，通常只在权重上做，而保留激活函数的全精度，以减小精度损失。
- en: The holy grail of quantizing though would be int2, representing every number
    as -1, 0, or 1\. This currently isn’t possible without completely degrading the
    model, but would make the model up to 8 times smaller. The Bloom model would be
    a measly ~40GB, small enough to fit on a single GPU. This is of course, as far
    as quantizing can take us and if we wanted to shrink further we’d need to look
    at additional methods.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 定量化的“圣杯”将是int2，将每个数字表示为-1、0或1。但这目前是不可能的，因为会完全降低模型的性能，但会使模型缩小多达8倍。布隆模型将只有约40GB，足够小，可以装到单个GPU上。当然，定量化只能带我们到达这一步，如果我们想进一步缩小，就需要探索其他方法了。
- en: The best part of quantization though is that it is easy to do. There are many
    frameworks that allow this, but in Listing 3.1 I demonstrate how to use pytorch’s
    quantization library to do a simple post training static quantization (PTQ). All
    you need is the full precision model, some example inputs, and a validation dataset
    to prepare and calibrate with. As you can see it’s only a few lines of code.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，最好的定量化部分是很容易做的。有很多框架允许这样做，但在3.1示例中，我演示了如何使用pytorch的定量化库进行简单的后训练静态定量化（PTQ）。你只需要全精度模型、一些样本输入和一个用于准备和校准的验证数据集。正如你所见，只需要几行代码即可完成。
- en: Listing 3.1 Example PTQ in PyTorch
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列出3.1示例中的PyTorch PTQ。
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Static PTQ is the most straightforward approach to quantizing, done after the
    model is trained and quantizing all the model parameters uniformly. As with most
    formulas in life, the most straightforward approach introduces more error. Oftentimes
    this error is acceptable, but when it’s not we can add extra complexity to reduce
    the accuracy loss from quantization. Some methods to consider are uniform vs non-uniform,
    static vs dynamic, symmetric vs asymmetric, and applying it during or after training.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: To understand these methods let's consider the case where we are quantizing
    from FP32 to INT8\. In FP32 we essentially have the full range of numbers at our
    disposal, but in INT8 we only have 256 values, we are trying to put a genie into
    a bottle and it’s no small feat. If you study the weights in your model, you might
    notice that the majority of the numbers are fractions between [-1, 1]. We could
    take advantage of this by then using an 8-bit standard that represents more values
    in this region in a non-uniform way instead of the standard uniform [-128, 127].
    While mathematically possible, unfortunately, any such standards aren’t commonplace
    and modern day deep learning hardware and software are not designed to take advantage
    of this. So for now, it's best to just stick to uniform quantization.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The simplest approach to shrink the data is to just normalize it, but since
    we are going from a continuous scale to a discrete scale there are a few gotchas,
    so let's explore those. First, we start by taking the min and max and scale them
    down to match our new number range, we would then bucket all the other numbers
    based on where they fall. Of course, if we have really large outliers, we may
    find all our other numbers squeezed into just one or two buckets completely ruining
    any granularity we once had. To prevent this, we can simply clip any large numbers.
    This is what we do in static quantization. However, before we clip the data, what
    if we chose a range and scale that captures the majority of our data beforehand?
    We need to be careful, since if this dynamic range is too small, we will introduce
    more clipping errors, if it’s too big, we will introduce more rounding errors.
    The goal of dynamic quantization of course is to reduce both errors.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to consider the symmetry of the data. Generally in normalization
    we force the data to be normal and thus symmetric, however, we could choose to
    scale the data in a way that leaves any asymmetry it had. By doing this we could
    potentially reduce our overall loss due to the clipping and rounding errors, but
    it’s not a guarantee.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: As a last resort, if none of these other methods failed to reduce accuracy loss
    of the model, we can use Quantization Aware Training (QAT). QAT is a simple process
    where we add a fake quantization step during training of the model. By fake, I
    mean, we clip and round the data while leaving it in full precision. This allows
    the model to adjust for the error and bias introduced by quantization while it’s
    training. QAT is known to produce higher accuracy compared to other methods but
    at a much higher cost in time to train.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Quantization Methods
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Uniform vs Non-uniform: whether or not we use an 8-bit standard that is uniform
    in the range it represents or non-uniform to be more precise in the -1 to 1 range.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Static vs Dynamic: Choosing to adjust the range or scale before clipping in
    an attempt to reduce clipping and rounding errors and reduce data loss.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Symmetric vs Asymmetric: Normalizing the data to be normal and force symmetry,
    or choosing to keep any asymmetry and skew.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'During or After Training: Quantization after training is really easy to do,
    and while doing it during training is more work it leads to reduced bias and better
    results.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Quantizing is a very powerful tool. Not only does it reduce the size of the
    model, but it also reduces the computational overhead required to run the model
    thus reducing latency and cost of running the model. But the best fact about quantization
    is that it can be done after the fact, so you don’t have to worry about whether
    or not your data scientists remembered to quantize the model during training using
    processes like QAT. This is why quantization has become so popular when working
    with LLMs and other large machine learning models. While reduced accuracy is always
    a concern with compression techniques, compared to other methods, quantization
    is a win-win-win.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Congratulations, you just trained a brand new LLM! With billions of parameters
    all of them must be useful right? Wrong! Unfortunately, as with most things in
    life, the model’s parameters tend to follow the Pareto Principle. About 20% of
    the weights lead to 80% of the value. “If that’s true,” you may be asking yourself,
    “Why don’t we just go and cut out all the extra fluff?” Great idea! Give yourself
    a pat on the back. Pruning is the process of weeding out and removing any parts
    of the model that we deem unworthy.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'There are essentially two different pruning methods: **structured** and **unstructured**.
    Structured pruning is the process of finding structural components of a model
    that aren’t contributing to the model’s performance and then removing them. Whether
    they be filters, channels, or layers in the neural network. The advantages to
    this method is that your model will be a little smaller but keep the same basic
    structure, which means we don’t have to worry about losing hardware efficiencies,
    we are also guaranteed a latency improvement as there will be less computations
    involved.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured pruning on the other hand, is shifting through the parameters and
    zeroing out the less important ones that don’t contribute much to the model’s
    performance. Unlike structured pruning, we don’t actually remove any parameters,
    just set them to zero. From this, we can imagine that a good place to start would
    be any weights or activations that are already close to 0\. Of course, while this
    effectively reduces the size of a model this also means we don’t cut out any computations,
    so it’s common to only see minimal latency improvement–if at all. But a smaller
    model still means faster load times and less GPUs to run. It also gives us very
    fine-grained control over the process, allowing us to shrink a model further than
    we could with structured pruning with less impact to performance too.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Like quantization, pruning can be done after a model is trained. However, unlike
    quantization, it’s common practice to see additional fine-tuning needing to be
    done to prevent too much loss of performance. It’s becoming more common to just
    include pruning steps during the model training to avoid the need to fine-tune
    later on. Since a more sparse model will have fewer parameters to tune, adding
    these pruning steps may help a model converge faster as well.[[5]](#_ftn5)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: You’ll be surprised at how much you can shrink a model with pruning while minimally
    impacting performance. How much? In the SparseGPT[[6]](#_ftn6) paper, a method
    was developed to try to automatically one shot the pruning process without the
    need for finetuning after. They found they could decrease a GPT-3 model by 50-60%
    without issue! Depending on the model and task they even saw slight improvements
    in a few of them. Looking forward to seeing where Pruning takes us in the future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Knowledge distillation is probably the coolest method of compression in my mind.
    It’s a simple idea too, we’ll take the large LLM, and have it train a smaller
    language model to copy it. What’s nice about this method is that the larger LLM
    provides essentially an infinite dataset for the smaller model to train on, which
    can make the training quite effective. Because of the simple fact that the larger
    the dataset the better the performance, we've often seen smaller models reach
    almost the same level as their teacher counterparts in accuracy.[[7]](#_ftn7)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: A smaller model trained this way is guaranteed to both be smaller and improve
    latency. The downside is that it’s going to require us to train a completely new
    model. Which is a pretty significant upfront cost to pay. Any future improvements
    to the teacher model will require being passed down to the student model, which
    can lead to complex training cycles and version structure. It’s definitely a lot
    more work compared to some of the other compression methods.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The hardest part about knowledge distillation though is that we don’t really
    have good recipes for them yet. Tough questions like, “How small can the student
    model be?” will have to be solved through trial and error. There’s still a lot
    to learn and research to be done here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: However, there has been some exciting work in this field via Stanford’s Alpaca[[8]](#_ftn8).
    Instead of training a student model from scratch, they instead chose to finetune
    the open source LLaMA 7B parameter model using OpenAI’s GPT3.5’s 175 billion parameter
    model as a teacher via knowledge distillation. A simple idea but it paid off big,
    as they were able to get great results from their evaluation. The biggest surprise
    was the cost, as they only spent $500 on API costs to get the training data from
    the teacher model, and $100 worth of GPU training time to finetune the student
    model. Granted, if you did this for a commercial application you’d be violating
    OpenAI’s terms of service, so best to stick to using your own or open source models
    as the teacher.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank Approximation
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Low-rank approximation, also known as low-rank factorization, low-rank decomposition,
    matrix factorization (too many names! I blame the mathematicians), uses linear
    algebra math tricks to simplify large matrices or tensors finding a lower-dimensional
    representation. There are several techniques to do this. Singular Value Decomposition
    (SVD), Tucker Decomposition(TD), and Canonical Polyadic Decomposition (CPD) are
    the most common ones you run into.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 3.2 we show the general idea behind the SVD method. Essentially we
    are going to take a very large matrix, A, and break it up into 3 smaller matrices,
    U, ∑, and V. While U and V are there to ensure we keep the same dimensions and
    relative strengths of the original matrix, ∑ allows us to apply a direction and
    bias. The smaller ∑ is, the more we end up compressing and reducing the total
    number of parameters, but the less accurate the approximation becomes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 Example of SVD a Low-rank Approximation. A is a large matrix with
    dimensions N and M. We can approximate it with three smaller matrices, U with
    dimensions M and P, ∑ a square matrix with dimension P, and V with dimensions
    N and P (here we show the transpose). Usually both P<<M and P<<N are true.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image002.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: To help solidify this concept, it may help to see a concrete example. In Listing
    3.2 we show a simple example of SVD at work compressing a 4x4 matrix. For this
    we only need the basic libraries SciPy and NumPy which are imported on lines 1
    and 2\. Line 3 we define the matrix, and then line 9 we apply SVD to it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Example SVD Low-rank Approximation
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Taking a moment to inspect U, Sigma, and the transpose of V, we see a 4x1 matrix,
    a 1x1 matrix, and a 1x4 matrix respectively. All in all we now only need 9 parameters
    vs the original 16, shrinking the memory footprint almost in half.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we multiply the matrices back together to get an approximation of the
    original matrix. In this case, the approximation isn’t all that great, but we
    can still see the general order and magnitudes match the original matrix.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Unfortunately, I’m not aware of anyone actually using this to simply compress
    models in production most likely due to the poor accuracy of the approximation.
    What they are using it for, and this is important, is adaptation and finetuning;
    which is where LoRA[[9]](#_ftn9) comes in, Low Rank Adaptation. Adaptation is
    simply the process of fine-tuning a generic or base model to do a specific task.
    LoRA applies SVD low rank approximation to the attention weights, or rather, to
    injected update matrices that run parallel to the attention weights, allowing
    us to fine-tune a much smaller model. LoRA has become very popular because it
    makes it a breeze to take an LLM, shrink the trainable layers to a tiny fraction
    of the original model and then allow anyone to train it on commodity hardware.
    You can get started with LoRA by using the PEFT[[10]](#_ftn10) library from HuggingFace,
    where they have several LoRA tutorials you can check out.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of Experts
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mixture of experts (MoE) is a technique where we replace the feed forward layers
    in a transformer with MoE layers instead. MoE’s are a group of sparsely activated
    models. It differs from ensemble techniques in that typically only one or a few
    expert models will be run, rather than combining results from all models. The
    sparsity is often induced by a Gate mechanism that learns which experts to use,
    and/or a Router mechanism that determines which experts should even be consulted.
    In Figure 3.3 we demonstrate the MoE architecture with potentially N experts,
    as well as show where it goes inside a decoder stack.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 Example Mixture of Experts model with both a Gate and Router to control
    flow. The MoE model is used to replace the FFN layers in a transformer, here we
    show it replacing the FFN in a Decoder.
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image003.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Depending on how many experts you have, the MoE layer could potentially have
    more parameters than the FFN leading to a larger model, but in practice this isn’t
    the case since engineers and researchers are aiming to create a smaller model.
    What we are guaranteed to see though is a faster computation path and improved
    inference times. However, what really makes MoE stand out is when it’s combined
    with quantization. One study[[11]](#_ftn11) between Microsoft and NVIDIA showed
    they were able to reach 2-bit quantization with only minimal impact to accuracy
    using MoE!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Of course, since this is a pretty big change to the model’s structure it will
    require finetuning afterwards. You should also be aware that MoE layers often
    reduce a model’s generalizability so it’s best when used on models designed for
    a specific task. There are several libraries to implement MoE layers, but I’d
    recommend checking out DeepSpeed[[12]](#_ftn12).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Distributed Computing
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distributed computing is a technique used in deep learning to parallelize and
    speed-up large, complex neural networks by dividing the workload across multiple
    devices or nodes in a cluster. This approach significantly reduces training and
    inference times by enabling concurrent computation, data parallelism, and model
    parallelism. With the ever-growing size of datasets and complexity of models,
    distributed computing has become crucial for deep learning workflows, ensuring
    efficient resource utilization and enabling researchers to effectively iterate
    on their models. Distributed computing is one of the core practices that separate
    deep learning from machine learning, and with LLMs we have to pull out every trick
    in the book. Let’s look at different parallel processing practices to take full
    advantage of distributed computing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Data Parallelism
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data parallelism is what most people think about when they think about running
    processes in parallel, it’s also the easiest to do. The practice involves splitting
    up the data and running them through multiple copies of the model or pipeline.
    For most frameworks this is easy to set up, for example, in PyTorch you can use
    the DistributedDataParallel method. There’s just one catch for most of these set-ups
    and that is your model has to be able to fit onto one GPU. This is where a tool
    like Ray.io comes in.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Ray is an open-source project designed for distributed computing, specifically
    aimed at parallel and cluster computing. It's a flexible and user-friendly tool
    which simplifies distributed programming and helps developers execute concurrent
    tasks in parallel with ease. Ray is primarily built for machine learning and other
    high-performance applications but can be utilized in other applications. In Listing
    3.3 we give a simple example of using Ray to distribute a task. The beauty of
    Ray is the simplicity–all we need to do to make our code run in parallel is add
    a decorator. Sure beats the complexity of multithreading or asynchronization set-ups.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Example Ray Parallelization Task
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Ray uses the concept of tasks and actors to manage distributed computing. Tasks
    are functions, whereas actors are stateful objects that can be invoked and run
    concurrently. When you execute tasks using Ray, it handles distributing tasks
    across the available resources (e.g., multi-core CPUs or multiple nodes in a cluster).
    For LLMs, we would need to set up a Ray cluster[[13]](#_ftn13) in a cloud environment
    as this would allow each pipeline to run on a node with as many GPUs as needed,
    greatly simplifying the infrastructure set up to run LLMs in parallel.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple alternatives out there, but Ray has been gaining a lot of
    traction and becoming more popular as more and more machine learning workflows
    require distributed training. My team has had great success with it. By utilizing
    Ray, developers can ensure better performance and more efficient utilization of
    resources in distributed workflows.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Parallelism
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensor parallelism is taking advantage of matrix multiplication properties to
    split up the activations across multiple processors, running the data through,
    and then combining them on the other side of the processors. Figure 3.4 demonstrates
    how this process works for a matrix, which can be parallelized in two separate
    ways that give us the same result. Imagine that Y is a really big matrix that
    can’t fit on a single processor, or more likely, a bottleneck in our data flow
    that takes too much time to run all the calculations. In either case, we could
    split Y up, either by columns or by rows, run the calculations, and then combine
    the results after. In this example we are dealing with matrices but in reality
    we are often dealing with tensors that have more than two dimensions, but the
    same mathematical principles that make this work still apply.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Choosing which dimension to parallelize is a bit of an art, but a few things
    to remember to help make this decision easier. First, how many columns or rows
    do you have? In general, you want to pick a dimension that has more than the number
    of processors you have, else you will end up stopping short. Generally this isn’t
    a problem but with tools like Ray discussed in the last section, parallelizing
    in a cluster and spinning up loads of processes is a breeze. Second, different
    dimensions have different multiplicity costs. For example, column parallelism
    requires us to send the entire dataset to each process, but with the benefit of
    concatenating them together at the end which is fast and easy. Row parallelism
    however, allows us to break up the dataset into chunks, but requires us to add
    the results, a more expensive operation than concatenating. You can see that one
    operation is more I/O bound, while the other is more computation bound. Ultimately,
    the best dimension will be dataset dependent, as well as hardware limited. It
    will require experimentation to fully optimize this, but a good default is to
    just choose the largest dimension.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 Tensor Parallelism example showing that you can break up tensors
    by different dimensions and get the same end result. Here we compare column and
    row parallelism of a Matrix.
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image004.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Tensor parallelism allows us to split up the heavy computation layers like MLP
    and Attention layers onto different devices, but doesn’t help us with Normalization
    or Dropout layers that don’t utilize tensors. To get better overall performance
    of our pipeline we can add sequence parallelism which targets these blocks[[14]](#_ftn14).
    Sequence parallelism is a process that partitions activations along the sequence
    dimension, preventing redundant storage, and can be mixed with tensor parallelism
    to achieve significant memory savings with minimal additional computational overhead.
    In combination, they reduce the memory needed to store activations in Transformer
    models. In fact, they nearly eliminate activation recomputation and save activation
    memory up to 5x.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 Combining tensor parallelism that focuses on computational heavy
    layers with sequence parallelism to reduce memory overhead to create a fully parallel
    process for the entire transformer.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image005.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 shows how combining both tensor parallelism, that allows us to distribute
    the computationally heavy layers, and sequence parallelism that does the same
    for the memory limiting layers, allows us to fully parallelize the entire transformer
    model. Together, they allow for extremely efficient use of resources.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline Parallelism
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far we can now run lots of data, and speed up any bottlenecks, but none of
    that matters because our model is too big; we can’t fit it into a single GPU’s
    memory to even get it to run. That’s where pipeline parallelism comes in and is
    the process of splitting up a model vertically and putting each part onto a different
    GPU. This creates a pipeline as input data will go to the first GPU, process,
    then transfer to the next GPU, and so on until it’s run through the entire model.
    While other parallelism techniques improve our processing power and speed up inference,
    pipeline parallelism is required to just get it to run and it comes with some
    major downsides, mainly device utilization.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: To understand where this downside comes from and how to mitigate it, let's first
    consider the naive approach to this, where we simply run all the data at once
    through the model. What we find is that this leaves a giant “bubble” of underutilization.
    Since the model is broken up, we have to process everything sequentially through
    the devices. This means that while one GPU is processing, the others are sitting
    idle. In Figure 3.6 we can see this naive approach and a large bubble of inactivity
    as the GPUs sit idle. We also see a better way to take advantage of each device.
    We do this by sending the data in small batches. A smaller batch allows the first
    GPU to pass on what it was working on quicker and move on to another batch. This
    allows the next device to get started earlier and reduces the size of the bubble.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 The Bubble Problem. When data runs through a broken up model, the
    GPUs holding the model weights are underutilized while they wait for their counterparts
    to process the data. A simple way to reduce this bubble is to use microbatching.
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image006.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'We can actually calculate the size of the bubble quite easily with the following
    formula:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Idle Percentage = 1 - m / ( m + n - 1)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Where m is the number of microbatches, and n is the depth of the pipeline or
    number of GPUs. So for our naive example case of 4 GPUs and 1 large batch we see
    the devices sitting idle 75% of the time! GPUs are quite expensive to allow to
    be sitting idle three quarters of the time. Let’s see what that looks like using
    the microbatch strategy. With a microbatch of 4, it cuts this almost in half,
    down to just 43% of the time. What we can glean from this formula is that the
    more GPUs we have, the higher the idle times, but the more microbatches the better
    the utilization.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we can often neither reduce the number of GPUs nor can we just
    make the microbatches as large as we want. There are limits. For GPU’s we just
    have to use as many as it takes to fit the model into memory, but try to use a
    few larger GPUs as it will lead to a more optimal utilization compared to using
    many smaller GPUs. Reducing the bubble in pipeline parallelism is another reason
    why compression is so important. For microbatching, the first limit is obvious
    once told, since the microbatch is a fraction of our batch size, we are limited
    by how big that is. The second is that each microbatch increases the memory demand
    for cached activations in a linear relationship. One way to counter this higher
    memory demand is a method called PipeDream[[15]](#_ftn15). There are different
    configurations and approaches, but the basic idea is the same. In this method
    we start working on the backward pass as soon as we’ve finished the forward pass
    of any of the microbatches. This allows us to fully complete a training cycle
    and release the cache for that microbatch.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 3D Parallelism
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For LLMs, we are going to want to take advantage of all three parallelism practices
    as they can all be run together. This is known as 3D Parallelism combining Data,
    Tensor, and Pipeline Parallelism (DP + TP + PP) together. Since each technique,
    and thus dimension, will require at least 2 GPUs, in order to run 3D Parallelism,
    we’ll need at least 8 GPUs to even get started. How we configure these GPUs will
    be important to get the most efficiency out of this process, namely, because TP
    has the largest communication overhead we want to ensure these GPUs are close
    together, preferably on the same node and machine. PP has the least communication
    volume of the three, so breaking up the model across nodes is the least expensive
    here.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: By running the three together, we see some interesting interactions and synergies
    between them. Since TP splits the model to work well within a device's memory,
    we see that PP can perform well even with small batch sizes due to the reduced
    effective batch size enabled by TP. This combination also improves the communication
    between DP nodes at different pipeline stages, allowing DP to work effectively
    too. The communication bandwidth between nodes is proportional to the number of
    pipeline stages, because of this DP is able to scale well even with smaller batch
    sizes. Overall, we see running in combination that
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know some tricks of the trade, it’s just as important to have the
    right tools to do the job.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Large Language Models Operations Infrastructure
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are finally going to start talking about the infrastructure needed to make
    this all work. This likely comes as a surprise as I know several readers would
    have expected this section at the beginning of chapter 1\. Why wait till the end
    of chapter 3? In the many times I’ve interviewed Machine Learning Engineers I
    often asked this open-ended question, “What can you tell me about MLOps?” An easy
    softball question to get the conversation going. Most junior candidates would
    immediately start jumping into the tooling and infrastructure. It makes sense,
    there are so many different tools available. Not to mention, whenever you see
    posts or blogs describing MLOps there’s a pretty little diagram showing the infrastructure.
    While all of that is important it’s useful to recognize what a more senior candidate
    jumps into, the machine learning lifecycle.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: For many the nuance is lost, but the infrastructure is the how, the lifecycle
    is the why. Most companies can get by with just bare-bones infrastructure. I’ve
    seen my share of scrappy systems that exist entirely on one Data Scientist’s laptop,
    and they work surprisingly well! Especially in the era of scikit learn everything.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, a rickshaw machine learning platform doesn’t cut it in the world
    of LLMs. Since we still live in a world where the standard storage capacity of
    a MacBook Pro laptop is 256GB, just storing the model locally can already be a
    problem. Companies that invest in a more sturdy infrastructure are better prepared
    for the world of LLMs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 3.7 we see an example MLOps Infrastructure designed with LLMs in mind.
    While most infrastructure diagrams I’ve seen in my time have always simplified
    the structure to make everything look clean, the raw truth is that there’s a bit
    more complexity to the entire system. Of course a lot of this complexity would
    disappear if we could just get Data Scientists to work inside scripts instead
    of ad hoc workstations–usually with a jupyter notebook interface.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 a high level view of an MLOps infrastructure with LLMs in mind. This
    attempts to cover the full picture, and the complexity of the many tools involved
    to make ML models work in production.
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image007.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Taking a closer look at Figure 3.7 you can see several tools on the outskirts
    that squarely land in DataOps, or even just DevOps. Data Stores, Orchestrators,
    Pipelines, Streaming Integrations, and Container Registries. These are tools you
    are likely already using for just about any data intensive application and aren’t
    necessarily MLOps focused. Towards the center we have more traditional MLOps tools,
    Experiment Trackers, Model Registry, Feature Store, and Ad hoc Data Science Workstations.
    For LLMs we really only introduce one new tool to the stack: a Vector Database.
    What’s not pictured because it intertwines with every piece is the Monitoring
    System. This all culminates to what we are working towards in this book, a Deployment
    Service, where we can confidently deploy and run LLMs in Production.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure by discipline
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '**DevOps:** In charge of procuring the environmental resources—experimental
    (dev, staging) and production—this includes hardware, clusters, and networking
    to make it all work. Also in charge of basic infrastructure systems like Github/Gitlab,
    artifact registries, container registries, application or transactional databases
    like Postgres or MySQL, caching systems, and CI/CD pipelines. This is by no means
    a comprehensive list.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataOps:** In charge of data, in motion and at rest. Includes centralized
    or decentralized data stores like Data Warehouses, Data Lakes, and Data Meshes.
    As well as data pipelines either in batch systems or in streaming systems with
    tools like Kafka and Flink. Also includes orchestrators like Airflow, Prefect
    or Mage. DataOps is built on top of DevOps. For example, I’ve seen many CI/CD
    pipelines being used for data pipeline work until eventually being graduated to
    systems like Apache Spark or DBT.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLOps:** In charge of machine learning lifecycle from creation of models
    to deprecation. This includes data science workstations like Jupyterhub, experiment
    trackers, and a model registry. It includes specialty databases like Feature Stores
    and Vector Databases. As well as a deployment service to tie everything together
    and actually serve results. It is built on top of both DataOps and DevOps.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through each piece of the infrastructure puzzle and discuss features
    you should be considering when thinking about LLMs in particular. While we will
    be discussing tooling that is specialized for each piece, I’ll just make note
    that there are also MLOps as a service platforms like Dataiku, Amazon’s Sagemaker
    and Google’s VertexAI. These platforms attempt to give you the whole puzzle, how
    well they do that is another question, but are often a great shortcut and you
    should be aware of them. Well, I think that’s enough dilly-dallying, let’s dive
    in already!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Data Infrastructure
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While not the focus of this book it’s important to note that MLOps is built
    on top of a Data Operations infrastructure–which itself is built on top of DevOps.
    Key features of the DataOps ecosystem include a data store, an orchestrator, and
    pipelines. Additional features usually required include a container registry and
    a streaming integration service.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Data stores are the foundation of DataOps and come in many forms these days,
    from a simple database to large data warehouses, and from even larger data lakes
    to an intricate data mesh. This is where your data is stored and a lot of work
    goes into managing, governing, and securing the data store. The orchestrator is
    the cornerstone of DataOps as it’s a tool that manages and automates both simple
    and complex, multistep workflows and tasks. Ensuring they run across multiple
    resources and services in a system. The most commonly talked about being Airflow,
    Prefect, and Mage. Lastly, pipelines are the pillars. They hold everything else
    up, and are where we actually run our jobs. Initially built to simply move, clean,
    and define data, these same systems are used to run machine learning training
    jobs on a schedule, do batch inference, and loads of other work needed to ensure
    MLOps runs smoothly.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: A container registry is a keystone of DevOps and subsequently DataOps and MLOps
    as well. Being able to run all our pipelines and services in containers is necessary
    to ensure consistency. Streaming services are actually a much bigger beast than
    what I may let on in this chapter, and if you know you know. Thankfully for most
    text related tasks real time processing isn’t a major concern. Even for tasks
    like real-time captioning or translation, we can often get by with some sort of
    pseudo real-time processing strategy that doesn’t degrade the user experience
    depending on the task.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Experiment Trackers
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experiment trackers are central to MLOps. Experiment trackers do the fundamental
    job of keeping track and recording tests and results. As the famous Adam Savage
    quote from Mythbusters, “Remember kids, the only difference between screwing around
    and science is writing it down.” Without it, your organization is likely missing
    the “science” in data science which is honestly quite embarrassing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Even if your data scientists are keen to manually track and record results in
    notebooks, it might as well be thrown in the garbage if it’s not easy for others
    to view and search for. This is really the purpose of experiment trackers, to
    ensure knowledge is easily shared and made available. Eventually a model will
    make it to production and that model is going to have issues. Sure, you can always
    just train a new model, but unless the team is able to go back and investigate
    what went wrong the first time you are likely to repeat the same mistakes over
    and over.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: There are many experiment trackers out there, the most popular by far is MLFlow
    which is open source. It was started by the team at Databricks which also offers
    an easy hosting solution. Some paid alternatives worth checking out include CometML
    and Weights and Biases.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Experiment trackers nowadays come with so many bells and whistles. Most open
    source and paid solutions will certainly have what you need when looking to scale
    up your needs for LLMOps. However, ensuring you take advantage of these tools
    correctly might require a few small tweaks. For example, the default assumption
    is usually that you are training a model from scratch, but often when working
    with LLMs you will be finetuning models instead. In this case, it’s important
    to note the checkpoint of the model you started from. If possible, even linking
    back to the original training experiment. This will allow future scientists to
    dig deeper into their test results, find original training data, and discover
    paths forward to eliminate bias.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Another feature to look out for is evaluation metric tooling. We will be going
    more in-depth in Chapter 4, but evaluation metrics are difficult for language
    models. There are often multiple metrics you care about and none of them are simple
    like complexity ratings or similarity scores. While experiment tracker vendors
    try to be agnostic and unopinionated about evaluation metrics they should at least
    make it easy to compare models and their metrics to make it easy to decide which
    one is better. Since LLMs have become so popular some have made it easy to evaluate
    on the more common metrics like ROUGE for text summarization.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: You will also find many experiment tracking vendors have started to add tools
    specifically for LLMs. Some features you might consider looking for include direct
    HuggingFace support, LangChain support, prompt engineering toolkits, finetuning
    frameworks, and foundation model shops. The space is developing quickly, and no
    one tool has all the same features right now, but I’m sure these feature sets
    will likely converge.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Model Registry
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model registry is probably the simplest tool of an MLOps infrastructure.
    The main objective is one that’s easy to solve, we just need a place to store
    the models. I’ve seen many successful teams get by simply by putting their models
    in an object store or shared file system and calling it good. That said, there’s
    a couple bells and whistles you should look for when choosing one.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The first is whether or not the model registry tracks metadata about the model.
    Most of what you care about is going to be in the experiment tracker, so you can
    usually get away with simply ensuring you can link the two. In fact, most model
    registries are built into experiment tracking systems because of this. However,
    an issue I’ve seen time and time again with these systems happens when the company
    decides to use an open source model or even buy one. Is it easy to upload a model
    and tag it with relevant information? The answer is usually no.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Next, you are going to want to make sure you can version your models. At some
    point, a model will reach a point where it’s no longer useful and will need to
    be replaced. Versioning your models will simplify this process. It also makes
    running production experiments like A/B testing or shadow tests easier.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, if we are promoting and demoting models, we need to be concerned with
    access. Models tend to be valuable intellectual property for many companies, ensuring
    only the right users have access to the models is important. But it’s also important
    to ensure that only the team that understands the models, what they do and why
    they were trained, are in charge of promoting and demoting the models. The last
    thing we want is to delete a model in production or worse.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: For LLMs there are some important caveats you should be aware of, mainly, when
    choosing a model registry, be aware of any limit sizes. I’ve seen several model
    registries restrict model sizes to 10GB or smaller. That’s just not going to cut
    it. I could speculate on the many reasons for this but none of them are worthy
    of note. Speaking of limit sizes, if you are going to be running your model registry
    on an premise storage system like Ceph, make sure it has lots of space. You can
    buy multiple terabytes of storage for a couple hundred dollars for your on prem
    servers, but even a couple terabytes fills up quickly when your LLM is over 300GB.
    Don’t forget, you are likely to be keeping multiple checkpoints and versions during
    training and finetuning; as well as duplicates for reliability purposes. Storage
    is still one of the cheapest aspects of running LLMs though, no reason to skimp
    here and cause headaches down the road.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'This does bring me to a good point: there''s a lot of optimization that could
    still be made, allowing for better space saving approaches to storing LLMs and
    their derivatives. Especially since most of these models will be very similar
    overall. I imagine we’ll likely see storage solutions to solve just this problem
    in the future.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Feature Store
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Feature stores solve many important problems and answer questions like: Who
    owns this feature? How was it defined? Who has access to it? Which models are
    using it? How do I serve this feature in production? Essentially, they solve the
    “single source of truth” problem. By creating a centralized store, it allows teams
    to shop for the highest quality, most well maintained, thoroughly managed data.
    Feature stores solve the collaboration, documentation, and versioning of data.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve ever thought, “A feature store is just a database right?” you are
    probably thinking about the wrong type of store–we are referencing a place to
    shop not a place of storage. Don’t worry, this confusion is normal as I’ve heard
    this sentiment a lot, and have had similar thoughts myself. The truth is, modern
    day feature stores are more virtual than a physical database, which means they
    are built on top of whatever data store you are already using. For example, Google’s
    Vertex AI feature store is just BigQuery and I’ve seen a lot of confusion from
    data teams wondering, “Why don’t I just query BigQuery?” Loading the data into
    a feature store feels like an unnecessary extra step, but think about shopping
    at an IKEA store. No one goes directly to the warehouse where all the furniture
    is in boxes. That would be a frustrating shopping experience. The features store
    is the show rooms that allows others in your company to easily peruse, experience,
    and use the data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Often times, I see people reach for a feature store to solve a technical problem
    like low latency access for online feature serving. A huge win for feature stores
    is solving the training-serving skew. Some features are just easier to do in SQL
    after the fact, like calculating the average number of requests for the last 30
    seconds. This can lead to naive data pipelines built for training, but causing
    massive headaches when going to production because getting this type of feature
    in real time can be anything but easy. Feature stores abstractions help minimize
    this burden. Related to this is feature stores point-in-time retrievals which
    are table stakes when talking feature stores. Point-in-time retrievals ensure
    that given a specific time a query will always return the same result. This is
    important because features like averages over “the last 30 seconds” are constantly
    changing, so this allows us to version the data (without the extra burden of a
    bloated versioning system), as well as ensure our models will give accurate and
    predictable responses.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: As far as options, Feast is a popular open source feature store. FeatureForm
    and Hopsworks are also open source. All three of which offer paid hosting options.
    For LLMs I’ve heard the sentiment that feature stores aren’t as critical as other
    parts of the MLOps infrastructure. After all, the model is so large it should
    incorporate all needed features inside it, so you don’t need to query for additional
    context, just give the model the user’s query and let the model do its thing.
    However, this approach is still a bit naive and we haven’t quite gotten to a point
    where LLMs are completely self-sufficient. To avoid hallucinations and improve
    factual correctness, it is often best to give the model some context. We do this
    by feeding it embeddings of our documents we want it to know very well, and a
    feature store is a great place to put these embeddings.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Vector Databases
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are familiar with the general MLOps infrastructure, most of this section
    has been review for you. We’ve only had to make slight adjustments highlighting
    important scaling concerns to make a system work for LLMs. Vector Databases however
    are new to the scene and have been developed to be a tailored solution for working
    with LLMs and language models in general, but you can also use them with other
    datasets like images or tabular data which are easy enough to transform into a
    vector. Vector databases are specialized databases to store vectors along with
    some metadata around the vector, which makes them great for storing embeddings.
    Now, while that last sentence is true, it is a bit misleading, because the power
    of vector databases isn’t in their storage, but in the way that they search through
    the data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Traditional databases, using b-tree indexing to find ID’s or text based search
    using reverse indexes, all have the same common flaw, you have to know what you
    are looking for. If you don’t have the ID or you don’t know the keywords it’s
    impossible to find the right row or document. Vector databases however, take advantage
    of the vector space meaning you don’t need to know exactly what you are looking
    for, you just need to know something similar which you can then use to find the
    nearest neighbors using similarity searches based on Euclidean distance, Cosine
    similarity, Dot product similarity, or what have you. Using a vector database
    makes solving the reverse image search problem a breeze, as an example.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: At this point, I’m sure some readers may be confused. First I told you to put
    your embeddings into a feature store, and now I’m telling you to put them into
    a Vector DB, which one is it? Well that’s the beauty of it, you can do both at
    the same time. If it didn’t make sense before I hope it makes sense now. Feature
    stores are not a database they are just an abstraction, you can use a feature
    store built on top of a Vector DB and it will solve many of your problems. Vector
    DBs can be difficult to maintain when you have multiple data sources, experimenting
    with different embedding models or otherwise have frequent data update. Managing
    this complexity can be a real pain, but a feature store can handily solve this
    problem. Using them in combination will ensure a more accurate and up-to-date
    search index.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases have only been around for a couple of years at the time of
    writing, and their popularity is still relatively new as it has grown hand in
    hand with LLMs. It’s easy to understand why since they provide a fast and efficient
    way to retrieve vector data making it easy to provide LLMs with needed context
    to improve their accuracy.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: That said it’s a relatively new field and there are lots of competitors in this
    space right now, and it’s a bit too early to know who the winners and losers are.
    Not wanting to date this book too much, let me at least suggest two options to
    start Pinecone and Milvus. Pinecone is one of the first vector databases as a
    product and has a thriving community with lots of documentation. It’s packed with
    features and has proven itself to scale. Pinecone is a fully managed infrastructure
    offering that has a free tier for beginners to learn. If you are a fan of open
    source however, then you’ll want to check out Milvus. Milvus is feature rich and
    has a great community. Zilliz the company behind Milvus offers a fully managed
    offering, but it’s also available to deploy in your own clusters and if you already
    have a bit of infrastructure experience it’s relatively easy and straightforward
    to do.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of alternatives out there right now, and it’s likely worth a
    bit of investigation before picking one. Probably the two things you’ll care most
    about is price and scalability, the two often going hand in hand. After that,
    it’s valuable to pay attention to search features, like support for different
    similarity measures like cosine similarities, dot product or euclidean distance.
    As well as indexing features like HNSW (Heirarchical Navigable Small World) or
    LSH (Locality-Sensitive Hashing). Being able to customize your search parameters
    and index settings are important for any database as they allow you to customize
    the workload for your dataset and workflow allowing you to optimize query latency
    and search result accuracy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to note that with vector databases rise in popularity we
    are quickly seeing many database incumbents like Redis and Elastic offering vector
    search capabilities. For now most of these tend to just offer the most straightforward
    feature sets, but they are hard to ignore if you are already using these tool
    sets as they can provide quick wins to get started quickly.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are powerful tools that can help you train or finetune LLMs,
    as well as improve the accuracy and results of your LLM queries.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.6 Monitoring System
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A monitoring system is crucial to the success of any ML system, LLMs included.
    Unlike other software applications, ML models are known to fail silently—continue
    to operate, but start to give poor results. This is often due to data drift, a
    common example being a recommendation system that give worse results overtime
    because sellers start to game the system by giving fake reviews to get better
    recommendation results. A monitoring system allows us to catch poorly performing
    models, make adjustments or simply retrain them.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Despite their importance they are often the last piece of the puzzle added.
    This is often purposeful as putting resources into figuring out how to monitor
    models doesn’t help if you don’t have any models to monitor. However, don’t make
    the mistake of putting it off too long. Many companies have been burned by a model
    that went rogue with no one knowing about it, often costing them dearly. It’s
    also important to realize you don’t have to wait to get a model into production
    to start monitoring your data. There are plenty of ways to introduce a monitoring
    system into the training and data pipelines to improve data governance and compliance.
    Regardless, you can usually tell the maturity of a data science organization by
    their monitoring system.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of great monitoring tooling out there, some great open source
    options include WhyLogs and EvidentlyAI. I’m also a fan of great expectations,
    but have found it rather slow outside of batch jobs. There are also many more
    paid options out there. Typically, for ML monitoring workloads you’ll want to
    monitor everything you’d normally record in other software applications, this
    includes resource metrics like memory and CPU utilization, performance metrics
    like latency and queries per second, as well as operational metrics like status
    codes and error rates. In additional, you’ll need ways to monitor data drift both
    going in and out of the model. You’ll want to pay attention to things like missing
    values, uniqueness, and standard deviation shifts. In many instances, you’ll want
    to be able to segment your data while monitoring, e.g. for A/B testing or to monitor
    by region. Some metrics that are useful to monitor in ML systems include model
    accuracy, precision, recall and F1 scores. These are difficult since you won’t
    know the correct answer at inference time, so it’s often useful to set up some
    sort of auditing system. Of course, auditing is going to be easier if your LLM
    is designed to be a Q&A bot than if your LLM is meant to help writers be more
    creative.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: This hints at a fact that for LLMs, there are often a whole set of new challenges
    for your monitoring systems even more than what we see with other ML systems.
    With LLMs we are dealing with text data which is hard to quantify as discussed
    earlier in this chapter. For instance, think about what features do you look at
    to monitor for data drift? Because language is known to drift a lot! One feature
    I might suggest is unique tokens. This will alert you when new slang words or
    terms are created, however, it still doesn’t help when words switch meaning, for
    example, when “wicked” means “cool”. I would also recommend monitoring the embeddings,
    however, you’ll likely find this to either add a lot of noise and false alarms
    or at the very least be difficult to decipher and dig into when problems do occur.
    The systems I’ve seen work the best often involve a lot of handcrafted rules and
    features to monitor, but these can be error-prone and time-consuming to create.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring text based systems is far from a solved problem, mostly stemming
    from the difficulties of understanding text data to begin with. This does beg
    the question of what are the best methods to use language models to monitor themselves,
    since they are our current best solution to codifying language. Unfortunately,
    I’m not aware of anyone researching this, but imagine it’s only a matter of time.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.7 GPU Enabled Workstations
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPU enabled workstations and remote workstations in general are often considered
    a nice to have or luxury by many teams, but when working with LLMs that mindset
    has to change. When troubleshooting an issue or just developing a model in general,
    a data scientist isn’t going to be able to spin up the model in a notebook on
    their laptop anymore. The easiest way to solve this is to simply provide remote
    workstations with GPU resources. There are plenty of cloud solutions for this,
    but if your company is working mainly on prem, this may be a bit more difficult
    to provide, but necessary nonetheless.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are GPU memory intensive models. Because of this, there are some numbers
    every engineer should know when it comes to working in the field. The first, is
    how much different GPUs have. The NVIDIA Tesla T4 and V100 are two most common
    GPUs you’ll find in a datacenter, but they only have 16 GB of memory. They are
    workhorses though and cost-effective, so if we can compress our model to run on
    these all the better. After these, you’ll find a range of GPU’s like NVIDIA A10G,
    NVIDIA Quadro series, and NVIDIA RTX series that offer GPU memories in the ranges
    of 24, 32, and 48 GB. All of these are fine upgrades, you’ll just have to figure
    out which ones are offered and available to you by your cloud provider. Which
    brings us to the NVIDIA A100, which is likely going to be your GPU of choice when
    working with LLMs. Thankfully they are relatively common, and offer two different
    models providing 40 or 80 GB. The big issue you’ll have with these are that they
    are constantly in high demand by everyone right now. You should also be aware
    of the NVIDIA H100 which offers 80 GB like the A100\. The H100 NVL is promised
    to support up to 188 GB and has been designed with LLMs in mind. Another new GPU
    you should be aware of is the NVIDIA L4 Tensor Core GPU which has 24 GB and is
    positioned to take over as a new workhorse along with the T4 and V100, at least
    as far as AI workloads are concerned.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: LLMs come in all different sizes, and it’s useful to have a horse sense for
    what these numbers mean. For example, the LLaMA model has 7B, 13B, 33B, and 65B
    parameter variants. If you aren’t sure which GPU you need to run which model off
    the top of your head, here’s a shortcut, just take the number of billions of parameters
    times it by two and that’s how much GPU memory you need. The reason is, most models
    at inference are going to default to run at half precision, FP16 of BF16, which
    means for every parameter we need at least two bytes. Thus, 7 billion * 2 bytes
    = 14 GB. You’ll need a little extra as well for the embedding model which will
    be about another GB, and more for the actual tokens you are running through the
    model. One token is about 1 MB, so 512 tokens will require 512 MB. This isn’t
    a big deal, until you consider running larger batch sizes to improve performance.
    For 16 batches of this size you’ll need an extra 8 GB of space.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Of course, so far we’ve only been talking about inference, for training you’ll
    need a lot more space. While training, you’ll always want to do this in full precision,
    and you’ll need extra room for the optimizer tensors and gradients. In general,
    to account for this you’ll need about 16 bytes for every parameter. So to train
    a 7B parameter model you’ll want 112 GB of memory.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.8 Deployment Service
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything we’ve been working towards is collected and finally put to good use
    here. In fact, if you took away every other service and were left with just a
    deployment service, you’d still have a working MLOps system. A deployment service
    provides an easy way to integrate with all the previous systems we talked about
    as well as configure and define the needed resources to get our model running
    in production. It will often provide boilerplate code to serve the model behind
    a REST and gRPC API or directly inside a batch or streaming pipeline.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Some tools to help create this service include NVIDIA Triton Inference Service,
    MLServer, Seldon and BentoML. These services provide a standard API interface,
    typically the KServe V2 Inference Protocol. This protocol provides a unified and
    extensible way to deploy, manage, and serve machine learning models across different
    platforms and frameworks. It defines a common interface to interact with models,
    including gRPC and HTTP/RESTful APIs. It standardizes concepts like input/output
    tensor data encoding, predict and explain methods, model health checks, and metadata
    retrieval. It also allows seamless integration with languages and frameworks including
    TensorFlow, PyTorch, ONNX, Scikit Learn, and XGBoost.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are times when flexibility and customization provide enough
    value to step away from the automated path these other frameworks provide, in
    which case it’s best to reach for a tool like FastAPI. Your deployment service
    should still provide as much automation and boilerplate code here to make the
    process as smooth as possible. It should be mentioned that most of the frameworks
    mentioned above do offer custom methods, but your mileage may vary.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model is more than just building the interface. Your deployment
    service will also provide a bridge to close the gap between the MLOps infrastructure
    and general DevOps infrastructure. Connecting to whatever CI/CD tooling as well
    as build and shipping pipelines your company has set up so you can ensure appropriate
    tests and deployment strategies like health checks and rollbacks can easily be
    monitored and done. This is often very platform and thus company-specific. Thus,
    it’ll also need to provide the needed configurations to talk to Kubernetes, or
    whatever other container orchestrator you may be using, to acquire the needed
    resources like CPU, Memory, and Accelerators, Autoscalers, Proxies, etc. It also
    applies the needed environment variables and secret management tools to ensure
    everything runs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: All in all, this service ensures you can easily deploy a model into production.
    For LLMs, the main concern is often just ensuring the platform and clusters are
    set up with enough resources to actually provision what will ultimately be configured.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed a lot so far in this chapter, starting with what makes LLMs
    so much harder than traditional ML which is hard enough as it is. First, we learned
    that their size can’t be underestimated, but then we also discovered there are
    many peculiarities about them, from token limits to hallucinations, not to mention
    they are expensive. Fortunately, despite being difficult, they aren’t impossible.
    We discussed compression techniques and distributed computing which are crucial
    to master. We then explored the infrastructure needed to make LLMs work. While
    most of it was likely familiar, we came to realize that LLMs put a different level
    of pressure on each tool, and often we need to be ready for a larger scale than
    what we could get away with for deploying other ML models.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Summary
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are difficult to work with mostly because they are big. Which impacts a
    longer time to download, load into memory, and deploy forcing us to use expensive
    resources.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are also hard to deal with because they deal with natural language and
    all its complexities including hallucinations, bias, ethics, and security.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless if you build or buy, LLMs are expensive and managing costs and risks
    associated with them will be crucial to the success of any project utilizing them.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing models to be as small as we can will make them easier to work with;
    quantization, pruning, and knowledge distillation are particularly useful for
    this.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization is popular because it is easy and can be done after training without
    any finetuning.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low Rank Approximation is an effective way at shrinking a model and has been
    used heavily for Adaptation thanks to LoRA.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three core directions we use to parallelize LLM workflows: Data,
    Tensor, and Pipeline. DP helps us increase throughput, TP helps us increase speed,
    and PP makes it all possible to run in the first place.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining the parallelism methods together we get 3D parallelism (Data+Tensor+Pipeline)
    where we find that the techniques synergize, covering each others weaknesses and
    help us get more utilization.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The infrastructure for LLMOps is similar to MLOps, but don’t let that fool you
    since there are many caveats where “good enough” no longer works.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many tools are offering new features specifically for LLM support.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector Databases in particular are interesting as a new piece of the infrastructure
    puzzle needed for LLMs that allow quick search and retrievals of embeddings.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_ftnref1) A. Bulatov, Y. Kuratov, and M. S. Burtsev, “Scaling Transformer
    to 1M tokens and beyond with RMT,” Apr. 2023, [https://arxiv.org/abs/2304.11062](abs.html).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_ftnref2) R. Daws, “Medical chatbot using OpenAI’s GPT-3 told a fake
    patient to kill themselves,” AI News, Oct. 28, 2020\. [https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/](medical-chatbot-openai-gpt3-patient-kill-themselves.html)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_ftnref3) T. Kington, “ChatGPT bot tricked into giving bomb-making instructions,
    say developers,” www.thetimes.co.uk, Dec 17, 2022\. [https://www.thetimes.co.uk/article/chatgpt-bot-tricked-into-giving-bomb-making-instructions-say-developers-rvktrxqb5](article.html)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_ftnref4) K. Quach, “AI game bans players for NSFW stories it generated
    itself,” www.theregister.com, Oct 8, 2021\. [https://www.theregister.com/2021/10/08/ai_game_abuse/](ai_game_abuse.html)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_ftnref5) T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste,
    “Sparsity in Deep Learning: Pruning and growth for efficient inference and training
    in neural networks,” Jan. 2021, [https://arxiv.org/abs/2102.00554](abs.html).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_ftnref6) E. Frantar and D. Alistarh, “SparseGPT: Massive Language Models
    Can Be Accurately Pruned in One-Shot,” Jan. 2023, [https://arxiv.org/abs/2301.00774](abs.html).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_ftnref7) V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter,” Oct. 2019\.
    [https://arxiv.org/abs/1910.01108](abs.html).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_ftnref8) R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,
    P Liang, and T. B. Hashimoto, “Alpaca: A Strong, Replicable Instruction-Following
    Model,” crfm.stanford.edu, 2023\. [https://crfm.stanford.edu/2023/03/13/alpaca.html](13.html)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_ftnref9) E. J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language
    Models.,” Jun. 2021, [https://arxiv.org/abs/2106.09685](abs.html).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_ftnref10) For the extra curious, Parameter-Efficient Fine-Tuning (PEFT)
    is a class of methods aimed at fine-tuning models in a computational efficient
    way. The PEFT library seeks to put them all in one easy to access place and you
    can get started here: [https://huggingface.co/docs/peft](docs.html)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_ftnref11) R. Henry and Y. J. Kim, “Accelerating Large Language Models
    via Low-Bit Quantization,” March 2023, [https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51226/](gtcspring23-s51226.html)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_ftnref12) DeepSpeed is a library that optimizes many of the hard parts
    for large-scale deep learning models like LLMs and is particularly useful when
    training. Check out their MoE tutorial. [https://www.deepspeed.ai/tutorials/mixture-of-experts/](mixture-of-experts.html)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_ftnref13) Learn more about Ray Clusters here: [https://docs.ray.io/en/releases-2.3.0/cluster/key-concepts.html#ray-cluster](cluster.html)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_ftnref14) V. Korthikanti et al., “Reducing Activation Recomputation
    in Large Transformer Models,” May 2022, [https://arxiv.org/abs/2205.05198](abs.html)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_ftnref15) A. Harlap et al., “PipeDream: Fast and Efficient Pipeline
    Parallel DNN Training,” Jun. 08, 2018\. [https://arxiv.org/abs/1806.03377](abs.html)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
