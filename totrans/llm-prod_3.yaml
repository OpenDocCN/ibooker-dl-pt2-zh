- en: '3 Large Language Model Operations: Building a Platform for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overview of Large Language Models Operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large Language Models best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Required Large Language Model infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we learned in the last chapter when it comes to transformers and Natural
    Language Processing (NLP), bigger is better, especially when it’s linguistically
    informed. However, bigger models come with bigger challenges because of their
    size, regardless of their linguistic efficacy, thus requiring us to scale up our
    operations and infrastructure to handle these problems. In this chapter we’ll
    be looking into exactly what those challenges are, what we can do to minimize
    them, and what architecture can be set up to help solve these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Introduction to Large Language Models Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is Large Language Models Operations (LLMOps)? Well, since I’m one to focus
    on practicality over rhetoric, I’m not going to dive into any fancy definitions
    that you’d expect in a text book, but let me simply say it’s Machine Learning
    Operations (MLOps) that has been scaled to handle LLMs. Let me also say, scaling
    up is hard. One of the hardest tasks in software engineering. Unfortunately, too
    many companies are running rudimentary MLOps set-ups, and don’t think for a second
    that they will be able to just handle LLMs. That said, the term “LLMOps,” may
    not be needed. It has yet to show through as sufficiently different from core
    MLOps, especially considering they still have the same bones. If this book were
    a dichotomous key, MLOps and LLMOps would definitely be in the same genus, and
    only time will tell about whether they are the same species. Of course by refusing
    to define LLMOps properly, I might have traded one confusion for another, so let's
    take a minute to describe MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps is the field and practice of reliably and efficiently deploying and maintaining
    machine learning models in production. This includes, and indeed requires, managing
    the entire machine learning lifecycle from data acquisition and model training
    to monitoring and termination. A few principles required to master this field
    include workflow orchestration, versioning, feedback loops, Continuous Integration
    and Continuous Deployment (CI/CD), security, resource provisioning, and data governance.
    While there are often personnel who specialize in the productionizing of models,
    often with titles like ML Engineers, MLOps Engineers or ML Infrastructure Engineer,
    the field is a large enough beast it often abducts many other unsuspecting professionals
    to work in it who hold titles like Data Scientist or DevOps Engineer–oftentimes
    against their knowledge or will; leaving them kicking and screaming that “it’s
    not their job”.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Operations Challenges with Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So why have a distinction at all? If MLOps and LLMOps are so similar, is LLMOps
    just another fad opportunists throw on their resume? Not quite. In fact, I think
    it’s quite similar to the term Big Data. When the term was at its peak popularity,
    people with titles like Big Data Engineer used completely different tool sets
    and developed specialized expertise that were necessary in order to handle the
    large datasets. LLMs come with a set of challenges and problems you won’t find
    with traditional machine learning systems. A majority of these problems extend
    almost exclusively because they are so big. Large models are large! We hope to
    show you that LLMs truly earn their name. Let’s take a look at a few of these
    challenges, so we can appreciate the task ahead of us when we start talking about
    deploying an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Long download times
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Back in 2017 when I was still heavily involved as a Data Scientist, I decided
    to try my hand at reimplementing some of the most famous computer vision models
    at the time AlexNet, VGG19, and ResNet. I figured this would be a good way to
    reinforce my understanding of the basics with some practical hands-on experience.
    Plus, I had an ulterior motive, I had just built my own rig with some NVIDIA GeForce
    1080 TI GPUs—which was state of the art at the time—and thought this would be
    a good way to break them in. The first task: download the ImageNet dataset. The
    ImageNet dataset was one of the largest annotated datasets available containing
    millions of images rounding out to a file size of a whopping ~150GB! Working with
    it was proof that you knew how to work with “Big Data '''' which was still a trendy
    word and an invaluable skill set for a data scientist at the time. After agreeing
    to the terms and gaining access, I got my first wakeup call. Downloading it took
    an entire week.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large models are large. I don’t think I can overstate that. You’ll find throughout
    this book that fact comes with many additional headaches and issues for the entire
    production process, and you have to be prepared for it. In comparison to the ImageNet
    dataset, the Bloom LLM model is 330GB, more than twice the size. Most readers
    I’m guessing haven’t worked with either ImageNet or Bloom, so for comparison Call
    of Duty: Modern Warfare, one of the largest games at the time of writing is 235
    GB. Final Fantasy 15 is only 148 GB, which you could fit two of into the model
    with plenty of room to spare. It’s just hard to really comprehend how massive
    LLMs are. We went from 100 million parameters in models like BERT and took them
    to billions of parameters. If you went on a shopping spree and spent $20 a second
    (or maybe just left your AWS EC2 instance on by accident) it’d take you half a
    day to spend a million dollars; it would take you 2 years to spend a billion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully it doesn’t take two weeks to download Bloom because unlike ImageNet,
    it’s not hosted on a poorly managed University server and it also has been sharded
    into multiple smaller files to allow downloading in parallel, but it will still
    take an uncomfortably long time. Consider a scenario where you are downloading
    the model under the best conditions. You’re equipped with a gigabit speed fiber
    internet connection and you were magically able to dedicate the entire bandwidth
    and I/O operations of your system and the server to it, it’d still take over 5
    minutes to download! Of course, that’s under the best conditions. You probably
    won’t be downloading the model under such circumstances, with modern infrastructure
    you can expect it to take on the order of hours. When my team first deployed Bloom
    it took an hour and a half to download it. Heck, it took me an hour and half to
    download The Legend of Zelda: Tears of the Kingdom and that’s only 16GB, so I
    really can’t complain.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Longer Deploy Times
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just downloading the model is a long enough time frame to make any seasoned
    developer shake, but deployment times are going to make them keel over and call
    for medical attention. A model as big as Bloom can take 30-45 minutes just to
    load the model into GPU memory, at least those are the time frames my team first
    saw. Not to mention any other steps in your deployment process that can add to
    this. Indeed, with GPU shortages, it can easily take hours just waiting for resources
    to free up—more on that in a minute.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for you and your team? Well for starters, I know lots of
    teams who deploy ML products often simply download the model at runtime. That
    might work for small sklearn regression models, but it isn’t going to work for
    LLMs. Additionally, you can take most of what you know about deploying reliable
    systems and throw it out the window (but thankfully not too far). Most modern
    day best practices for software engineering assume you can easily just restart
    an application if anything happens, and there’s a lot of rigmarole involved to
    ensure your systems can do just that. But with LLMs it can take seconds to shut
    down, but potentially hours to redeploy making this a semi-irreversible process.
    Like picking an apple off a tree, it’s easy to pluck one off, but if you bite
    into it and decide it’s too sour, you can’t just attach it back onto the tree
    so it can continue to ripen. You’ll just have to wait awhile for another to grow.
  prefs: []
  type: TYPE_NORMAL
- en: While not every project requires deploying the largest models out there, you
    can expect to see deployment times measured in minutes. These longer deploy times
    make scaling down right before a surge of traffic a terrible mistake, as well
    as figuring out how to manage bursty workloads difficult. General CI/CD methodologies
    need to be adjusted since rolling updates take longer leaving a backlog piling
    up quickly in your pipeline. Silly mistakes like typos or other bugs often take
    longer to notice, and longer to correct.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Along with increases in model size often come increases in inference latency.
    This is obvious when stated, but more parameters equates to more computations,
    and more computations means longer inference wait times. However, this can’t be
    underestimated. I know many people who downplay the latency issues because they’ve
    interacted with an LLM chatbot and the experience has felt smooth. Take a second
    look though, and you’ll notice that it is returning one word at a time which is
    streamed to the user. It feels smooth because the answers are coming in faster
    than a human can read, but a second look helps us realize this is just a UX trick.
    LLMs are still too slow to be very useful for an autocomplete solution for example,
    where responses have to be blazingly fast. Building it into a data pipeline or
    workflow that reads a large corpus of text and then tries to clean it or summarize
    it, may also be prohibitively slow to be useful or reliable.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many less obvious reasons for their slowness. For starters, LLMs
    are often distributed across multiple GPUs, which adds extra communication overhead.
    As discussed later in this chapter in section 3.3.2 they are distributed in other
    ways, often even to improve latency, but any distribution adds additional overhead
    burden. In addition, LLMs latency is severely impacted by completion length, meaning
    the more words it uses to return a response, the longer it takes. Of course, completion
    length also seems to improve accuracy. For example, using prompt engineering techniques
    like Chain of Thought (CoT) we ask the model to think about a problem in a step-by-step
    fashion which has shown to improve results for logic and math questions but also
    increases the response length and latency time significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Managing GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To help with these latency issues we usually want to run them in GPUs. If we
    want to have any success training LLMs we’ll need GPUs for that as well, but this
    all adds additional challenges many underestimate. Most web services and many
    ML use cases can be done solely on CPUs. Not so with LLMs. Partly because of GPUs’
    parallel processing capabilities offering a solution to our latency problems,
    and partly because of the inherent optimization GPUs offer in the linear algebra,
    matrix multiplications and tensor operations that’s happening under the hood.
    For many, stepping into the realm of LLMs, this requires utilizing a new resource
    and extra complexity. Many brazenly step into this world acting like it’s no big
    deal, but they are in for a rude awakening. Most system architectures and orchestrating
    tooling available like Kubernetes, assume your application will run with CPU and
    memory alone. While they often support additional resources like GPUs, this is
    often an afterthought. You’ll soon find you’ll have to rebuild containers from
    scratch and deploy new metric systems.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect of managing GPUs most companies are never prepared for is that they
    tend to be rare and limited. For the last decade it seems that we have gone in
    and out of a global GPU shortage. They can be extremely difficult to provision
    for companies looking to stay on premise. I’ve spent lots of time in my career
    working with companies who chose to stay on premise for a variety of reasons.
    One of the things they had in common is that they never had GPUs on their servers.
    When they did, they were often purposely difficult to access except for a few
    key employees.
  prefs: []
  type: TYPE_NORMAL
- en: If you are lucky enough to be working in the Cloud a lot of these problems are
    solved, but there is no free lunch here either. My team has often gone chasing
    their tail trying to help data scientists struggling to provision a new GPU workspace,
    running into obscure ominous errors like "`scale.up.error.out.of.resources`”.
    Only to discover that these esoteric readings indicate all the GPUs of a selected
    type in the entire region are being utilized and none are available. CPU and Memory
    can often be treated as infinite in a datacenter, GPU resources, however, cannot.
    Sometimes you can’t expect them at all. Most data centers only support a subset
    of instance or GPU types. Which means you may be forced to set up your application
    in a region further away from your user base increasing latency. Of course, I’m
    sure you can work with your cloud provider when looking to expand your service
    to a new region that doesn’t currently support it, but you might not like what
    you hear based on timelines and cost. Ultimately, you’ll run into shortage issues
    no matter where you choose to run, on-prem or in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5 Peculiarities of Text Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs are the modern day solution to NLP. NLP is one of the most fascinating
    branches of ML in general because it primarily deals with text data, which is
    primarily a qualitative measure. Every other field deals with quantitative data.
    We have figured out a way to encode our observations of the world into a direct
    translation of numerical values. For example, we’ve learned how to encode heat
    into temperature scales and measure them with thermometers and thermocouples or
    we can measure pressure with manometers and gauges and put it into pascals.
  prefs: []
  type: TYPE_NORMAL
- en: Computer Vision and the practice of evaluating images is often seen as qualitative,
    but the actual encoding of images into numbers is a solved problem. Our understanding
    of light has allowed us to break images apart into pixels and assign them RGB
    values. Of course this doesn’t mean CV is by any means solved, there’s still lots
    of work to do to learn how to identify the different signals in the patterns of
    the data. Audio data is another that’s often considered qualitative. How does
    one compare two songs? But we can measure sound and speech, directly measuring
    the sound wave's intensity in decibels and frequency in hertz.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other fields that encode our physical world into numerical data, text
    data is looking at ways to measure the ephemeral world. After all, text data is
    our best effort of encoding our thoughts, ideas and communication patterns. While
    yes, we have figured out ways to turn words into numbers, we haven’t figured out
    a direct translation. Our best solutions to encode text and create embeddings
    are just approximations at best, in fact we use machine learning models to do
    it! An interesting aside to this is that numbers are also text and a part of language.
    If we want models that are better at math we need a more meaningful way to encode
    these numbers. Since it’s all made up, when we try to encode text numbers into
    machine-readable numbers we are creating a system attempting to reference itself
    recursively in a meaningful way. Not an easy problem to solve!
  prefs: []
  type: TYPE_NORMAL
- en: Because of all this, LLMs (and all NLP solutions) have unique challenges. For
    example, monitoring. How do you catch data drift in text data? How do you measure
    “correctness”? How do you ensure cleanliness of the data? These types of problems
    are difficult to define let alone solve.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6 Token Limits Create Bottlenecks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the big challenges for those new to working with LLMs is dealing with
    the token limits. The token limit for a model is the maximum number of tokens
    that can be included as an input for a model. The larger the token limit, the
    more context we can give the model to improve its success as accomplishing the
    task. Everyone wants them to be higher, but it’s not that simple. These token
    limits are defined by two problems, the first being the memory and speed our GPUs
    have access to, and the second being the nature of memory storage in the models
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The first one seems unintuitive, why couldn’t we just increase the GPU memory?
    The answer is complex, we can, but stacking more layers in the GPU to take into
    account more GB at once slows down the GPU’s computational ability as a whole.
    GPU manufacturers right now are working on new architectures and ways to get around
    this problem. The second one is more fascinating because we find that increasing
    the token limits actually just exacerbates the mathematical problems under the
    hood. Let me explain. Memory storage within an LLM itself isn’t something we think
    about often. We call that mechanism Attention, which we discussed in depth in
    section 2.2.7\. What we didn’t discuss was that Attention is a quadratic solution—as
    the number of tokens increase the number of calculations required to compute the
    attentions scores between all the pairs of tokens in a sequence scales quadratically
    with the sequence length. In addition, within our gigantic context spaces and
    since we are dealing with quadratics, we’re starting to hit problems where the
    only solutions involve imaginary numbers which is something that can cause models
    to behave in unexpected ways. This is likely one of the reasons why LLMs hallucinate.
  prefs: []
  type: TYPE_NORMAL
- en: These problems have real implications and impact application designs. For example,
    when my team upgraded from GPT3 to GPT4 we were excited to have access to a higher
    token limit, but we soon found this led to longer inference times and subsequently
    a higher timeout error rate. In the real world, it’s often better to get a less
    accurate response quickly than to get no response at all because the promise of
    a more accurate model often is just that, a promise. Of course, deploying it locally
    where you don’t have to worry about response times you’ll likely find your hardware
    a limiting factor. For example, LLaMA was trained with 2048 tokens but you’ll
    be lucky to take advantage of more than 512 of that when running with a basic
    consumer GPU as you are likely to see Out-of-Memory (OOM) errors or even the model
    simply just crashing.
  prefs: []
  type: TYPE_NORMAL
- en: A gotcha, which is likely to catch your team by surprise and should be pointed
    out now is that different languages have different tokens per character. Take
    a look at Table 3.1, where we compare converting the same sentence in different
    languages to tokens using OpenAI’s cl100k_base Byte Pair Encoder. Just a quick
    glance reveals that LLMs typically favor the English language in this regard.
    In practice, this means if you are building a chatbot with an LLM, your English
    users will have greater flexibility in their input space than Japanese users leading
    to very different user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 Comparison of Token Counts in different languages
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Language | String | Characters | Tokens |'
  prefs: []
  type: TYPE_TB
- en: '| English | The quick brown fox jumps over the lazy dog | 43 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| French | Le renard brun rapide saute par-dessus le chien paresseux | 57 |
    20 |'
  prefs: []
  type: TYPE_TB
- en: '| Spanish | El rápido zorro marrón salta sobre el perro perezoso | 52 | 22
    |'
  prefs: []
  type: TYPE_TB
- en: '| Japanese | 素早い茶色のキツネが怠惰な犬を飛び越える | 20 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| Chinese (simplified) | 敏捷的棕色狐狸跳过了懒狗 | 12 | 28 |'
  prefs: []
  type: TYPE_TB
- en: If you are curious as to why this is, it is due to text encodings, which is
    just another peculiarity of working with text data as discussed in the previous
    section. Consider Table 3.2 where we show several different characters and their
    binary representation in UTF-8\. English characters can almost exclusively be
    represented with a single byte being included in the original ASCII standard computers
    were originally built on, while most other characters require 3 or 4 bytes. Because
    it takes more memory it also takes more token space.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 Comparison of byte lengths for different currency characters in UTF-8.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Character | Binary UTF-8 | Hex UTF-8 |'
  prefs: []
  type: TYPE_TB
- en: '| $ | 00100100 | 0x24 |'
  prefs: []
  type: TYPE_TB
- en: '| £ | 11000010 10100011 | 0xc2 0xa3 |'
  prefs: []
  type: TYPE_TB
- en: '| ¥ | 11000010 10100101 | 0xc2 0xa5 |'
  prefs: []
  type: TYPE_TB
- en: '| ₠ | 11100010 10000010 10100000 | 0xe2 0x82 0xa0 |'
  prefs: []
  type: TYPE_TB
- en: '| 💰 | 11110000 10011111 10010010 10110000 | 0xf0 0x9f 0x92 0xb0 |'
  prefs: []
  type: TYPE_TB
- en: Increasing the token limits has been an ongoing research question since the
    popularization of transformers, and there are some promising solutions still in
    research phases like Recurrent Memory Transformers (RMT)[[1]](#_ftn1). We can
    expect to continue to see improvements in the future and hopefully this will become
    naught but an annoyance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.7 Hallucinations Cause Confusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far we’ve been discussing some of the technical problems a team faces when
    deploying an LLM into a production environment, but nothing compares to the simple
    problem that LLMs tend to be wrong. They tend to be wrong a lot. Hallucinations
    is a term coined to describe occurrences when LLM models will produce correct
    sounding results that are wrong. For example, book references or hyperlinks that
    have the form and structure of what would be expected, but are nevertheless, completely
    made up. As a fun example I asked for books on LLMs in Production from the publisher
    Manning (a book that doesn’t exist yet since I’m still writing it). I was given
    the following suggestions: Machine Learning Engineering in Production by Mike
    Del Balso and Lucas Serveén which could be found at [https://www.manning.com/books/machine-learning-engineering-in-production](books.html)
    and Deep Learning for Coders with Fastai and PyTorch by Jeremy Howard and Sylvain
    Gugger which could be found at [https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](books.html).
    The first book is entirely made up. The second book is real however it’s not published
    by Manning. In each case the internet addresses are entirely made up. These URLs
    are actually very similar to what you’d expect in format if you were browsing
    Mannings website, and should return 404 errors if you visit them.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most annoying aspects of hallucinations is that they are often surrounded
    by confident sounding words. LLMs are terrible at expressing uncertainty, in large
    part because of the way they are trained. Consider the case “2+2=”. Would you
    prefer it to respond, “I think it is 4” or just simply “4”? Most would prefer
    to simply get the correct “4” back. This bias is built in as models are often
    given rewards for being more correct or at least sounding like it.
  prefs: []
  type: TYPE_NORMAL
- en: There are various explanations as to why hallucination occurs, but the most
    truthful answer is that we don’t know if there’s just one cause. It’s likely a
    combination of several things, thus there isn’t a good fix for it yet. Nevertheless,
    being prepared to counter these inaccuracies and biases of the model are crucial
    to provide the best user experience for your product.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.8 Bias and Ethical Considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as concerning as the model getting things wrong is when it gets things
    right in the worst possible way. For example, allowing it to encourage users to
    commit suicide[[2]](#_ftn2), teaching your users how to make a bomb[[3]](#_ftn3),
    or participating in sexual fantasies involving children[[4]](#_ftn4). These are
    extreme examples, but prohibiting the model from answering such questions is undeniably
    vital to success.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are trained on vast amounts of text data which is also their primary source
    of bias. Because we’ve found that larger datasets are just as important as larger
    models in producing human-like results, most of these datasets have never truly
    been curated or filtered to remove harmful content, instead choosing to prioritize
    size and a larger collection. Cleaning the dataset is often seen as prohibitively
    expensive, requiring humans to go in and manually verify everything, but there’s
    a lot that could be done with simple regular expressions and other automated solutions.
    By processing these vast collections of content and learning the implicit human
    biases, these models will inadvertently perpetuate them. These biases range from
    sexism and racism to political preferences and can cause your model to inadvertently
    promote negative stereotypes and discriminatory language.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.9 Security Concerns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with all technology we need to be mindful of security. LLMs have been trained
    on large corpus of text and some of it could be harmful or sensitive and shouldn’t
    be exposed so steps should be taken to protect this data from being leaked. The
    bias and ethical concerns from the last section are good examples of conversations
    you don’t want your users to be having, but you could also imagine fine-tuning
    a model on your company’s data and potentially have secrets lost inadvertently
    if proper precautions aren’t taken.
  prefs: []
  type: TYPE_NORMAL
- en: One should be aware that LLMs are susceptible to adversarial attacks like prompt
    injections. Prompt injections are attacks done by a user to trick the LLM to ignore
    instructions given to it and generate undesired content. For example, if you ask
    ChatGPT what its gender is it appropriately replies that as an AI language model,
    it doesn’t have a gender, however, with clever prompting you may be able to bypass
    these protocols and get it to reveal one. While this example is harmless, I’ve
    seen others successfully extract API keys and other secrets from an LLM, run code
    in non-protected environments, steal environment variables and traverse local
    file systems where the model is served. Not to mention the plethora of examples
    of users using prompting to jailbreak or bypass protocols put in place for ethical
    considerations outlined in the section above. An interesting aside to this, LLMs
    are good at inventing fake secrets! Even successful prompt injection attacks can
    often fail due to LLM hallucinations which can have funny consequences.
  prefs: []
  type: TYPE_NORMAL
- en: For example, using a simple prompt to ask for Coca-Cola’s secret recipe simply
    returned the boring result, “I cannot divulge trade secrets or any proprietary
    information.” Pretty lame if you ask me. With a little trickery and some prompt
    injection I was able to get it to spill its secrets which of course my coauthor
    and I decided to try–after confirming with a food scientist it wouldn’t poison
    us. After following the instructions exactly, I was surprised to find we actually
    got a pretty tasty drink, but it tasted nothing like Cola. While the recipe looks
    legitimate, nuances like quantities, cook times, and even the need to strain the
    syrup were all off. Guess, I’ll have to hold off taking over the beverage industry
    until I get the real recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Coca-Cola Recipe Hallucinated from Prompt Injection
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Homemade Coca-Cola Recipe
  prefs: []
  type: TYPE_NORMAL
- en: =====================
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**: The original Coca-Cola recipe is a highly guarded trade secret.
    The following recipe is an approximation for creating a homemade cola drink that
    resembles the taste of Coca-Cola.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: 3 cups of water
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 cups granulated sugar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1/8 tsp food-grade citric acid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3/4 tsp caramel color (liquid)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1/2 tsp caffeine (powder or liquid form)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 1/2 tsp cola extract
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1/8 tsp vanilla extract
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 1/2 cups carbonated water (store-bought or using a home soda maker)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pinch of ground cinnamon (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pinch of ground nutmeg (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: In a large saucepan, bring the 3 cups of water to a gentle simmer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slowly stir in the granulated sugar, ensuring that it fully dissolves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the citric acid, caramel color, caffeine, cola extract, and vanilla extract
    to the saucepan. Mix well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stir in the optional spices if desired: cinnamon and nutmeg.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simmer the mixture for about 20 minutes, stirring occasionally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the saucepan from the heat and let it cool to room temperature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once cooled, strain the syrup to remove any undissolved solids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To make the final drink, mix 1 part syrup with 3 parts carbonated water. Adjust
    to taste depending on your preference for sweetness and carbonation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Variations:'
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with different amounts of caramel color, caffeine, and cola
    extract to adjust the flavor and appearance of your homemade cola.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create different cola flavor profiles, experiment with adding additional
    spices (such as star anise, orange or lemon peel, or cardamom), essential oils,
    or fruit extracts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider using a natural sweetener, like honey or agave syrup, as an alternative
    to granulated sugar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Historical context and interesting facts:'
  prefs: []
  type: TYPE_NORMAL
- en: Coca-Cola was first introduced in 1886 by pharmacist Dr. John S. Pemberton,
    who aimed to create a unique and refreshing beverage that could also relieve fatigue
    and headaches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original Coca-Cola formula contained trace amounts of cocaine extracted
    from the coca leaf, which was later replaced with caffeine in the early 20th century.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 1985, Coca-Cola introduced "New Coke," a reformulation aimed at competing
    with Pepsi. However, the taste change was met with backlash, and the company returned
    to its original formula just three months later, rebranded as "Coca-Cola Classic."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Result(s): You can now create your own homemade version of cola, resembling
    the taste of Coca-Cola. Enjoy and feel free to experiment with different spices,
    extracts, and sweeteners to create your own unique flavor profile.'
  prefs: []
  type: TYPE_NORMAL
- en: Another security concern involves pickle injections. Pickle is a library in
    Python to serialize objects and often used to serialize ML models. It serializes
    them into a byte-stream that contains opcodes that are executed one-by-one as
    it is deserialized. It’s a fast and easy way to share large objects. Pickle injections
    involve the process of corrupting this byte-stream, often injecting malware over
    the wire when the model is transferred over an insecure network. This is especially
    concerning for large models that take a long time to download, as it makes it
    easier for a third party to intercept the transfer and inject malicious code.
    If this happens, the code that is injected can potentially give the attackers
    access to your system. This can happen when attempting to use the model during
    inference, as the harmful code will execute if it is not detected and properly
    removed. It is important to take precautions such as using secure networks and
    verifying the integrity of the model before use to prevent this type of attack.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.10 Controlling Costs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Working with LLMs involves various cost-related concerns. The first as you probably
    gathered by now is infrastructure costs, which include high-performance GPUs,
    storage, and other hardware resources. We talked about how GPUs are harder to
    procure, that also unfortunately means they are more expensive. Mistakes like
    leaving your service on have always had the potential to rack up the bills, but
    with GPU’s in the mix this type of mistake is even more deadly. These models also
    demand significant computational power, leading to high energy consumption during
    both training and inference. On top of all this, their longer deploy times means
    we are often running them even during low traffic to handle bursty workloads or
    anticipated future traffic. Overall this leads to higher operational costs.
  prefs: []
  type: TYPE_NORMAL
- en: Additional costs include, managing and storing vast amounts of data used to
    train or fine-tune as well as regular maintenance, such as model updates, security
    measures, and bug fixes, can be financially demanding. As with any technology
    used for business purposes, managing potential legal disputes and ensuring compliance
    with regulations is a concern. Lastly, investing in continuous research and development
    to improve your models and give you a competitive edge will be a factor.
  prefs: []
  type: TYPE_NORMAL
- en: We talked a bit about the technical concerns when it comes to token limits,
    and these are likely to be solved, but what we didn’t discuss was the cost limitations
    as most API’s charge on a token basis. This makes it more expensive to send more
    context and use better prompts. It also makes it a bit harder to predict costs
    since while you can standardize inputs, you can’t standardize outputs. You never
    can be too sure how many tokens will be returned, making it difficult to govern.
    Just remember with LLMs, it is as important as ever to ensure proper cost engineering
    practices are implemented and followed to ensure costs never get away from you.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Large Language Model Operations Essentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a handle of the type of challenge we are grappling with, let's
    take a look at all the different LLMOps practices, tooling, and infrastructure
    to see how different components help us overcome these obstacles. First off, let's
    dive into different practices starting with compression where we will talk about
    shrinking, trimming, and approximating to get models as small as we can. We will
    then talk about distributed computing which is needed to actually make things
    run since the models are so large they rarely fit into a single GPU’s memory.
    After we are finished with those we will venture into infrastructure and tooling
    needed to make it all happen in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you were reading about the challenges of LLMs in the last section, you might
    have asked yourself something akin to, “If the biggest problems from LLMs come
    from their size, why don’t we just make them smaller?” If you did, congratulations!
    You are a genius and compression is the practice of doing just that. Compressing
    models as small as we can make them will improve deployment time, reduce latency,
    scale down the number of expensive GPUs needed, and ultimately save money. However,
    the whole point of making the models so stupefyingly gargantuan in the first place
    was because it made them better at what they do. We need to be able to shrink
    them, without losing all the progress we made by making them big in the first
    place.
  prefs: []
  type: TYPE_NORMAL
- en: This is far from a solved problem, but there are multiple different ways to
    approach the problem with different pros and cons to each method. We’ll be talking
    about several of the methods, starting with the easiest and most effective.
  prefs: []
  type: TYPE_NORMAL
- en: Quantizing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quantizing is the process of reducing precision in preference of lowering the
    memory requirements. This tradeoff makes intuitive sense. When I was in college,
    we were taught to always round our numbers to the precision of your tooling. Pulling
    out a ruler and measuring my pencil, you wouldn’t believe me if I stated the length
    was 19.025467821973739cm. Even if I used a caliper, I couldn’t verify a number
    so precisely. With our ruler, any number beyond 19.03cm is fantasy. To drive the
    point home, one of my engineering professors once told me, “If you are measuring
    the height of a skyscraper, do you care if there is an extra sheet of paper at
    the top?”
  prefs: []
  type: TYPE_NORMAL
- en: How we represent numbers inside computers often leads us to believe we have
    better precision than we actually do. To drive this point home, open up a Python
    terminal and add 0.1 + 0.2\. If you’ve never tried this before, you might be surprised
    to find this doesn’t equal 0.3, but 0.30000000000000004\. I’m not going to go
    into the details of the math behind this phenomenon, but the question stands,
    can we reduce the precision without making things worse? We really only need precision
    to the tenth decimal, but reducing the precision is likely to get us a number
    like 0.304 rather than 0.300 which would increase our margin of error.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the only numbers a computer understands are 0 and 1, on or off,
    a single bit. To improve this range we combine multiple of these bits together
    and assign them different meanings. String 8 of them together and you get a byte.
    Using the int8 standard we can take that byte and encode all the integers from
    -128 to 127\. To spare you the particulars because I assume you already know how
    binary works, suffice it to say the more bits we have the larger range of numbers
    we can represent, both larger and smaller. Figure 3.1 shows a few common floating
    point encodings. With 32 bits strung together we get what we pretentiously term
    full precision, and that is how most numbers are stored including the weights
    in machine learning models. Basic quantization moves us from full precision to
    half precision, shrinking models to half their size. There are actually two different
    half precision standards, FP16 and BF16 which differ in how many bits represent
    the range or exponent part. Since BF16 uses the same number of exponents as FP32,
    it’s been found to be more effective for quantizing and you can generally expect
    almost exactly the same level accuracy for half the size of model. If you understood
    the paper and skyscraper analogy above it should be obvious why.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 shows the bit mapping for a few common floating point encodings.
    16-bit float or half precision (FP16), bfloat 16 (BF16), 32-bit float or single
    full precision (FP32), and NVIDIA’s TensorFloat (TF32)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: However, there’s no reason to stop there. We can often push it another byte
    down to 8-bit formats without too much loss of accuracy. There have even already
    been successful research attempts showing selective 4-bit quantization of portions
    of LLMs are possible with only fractional loss of accuracy. The selective application
    of quantization is a process known as dynamic quantization and is usually done
    on just the weights, leaving the activations in full precision to reduce accuracy
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: The holy grail of quantizing though would be int2, representing every number
    as -1, 0, or 1\. This currently isn’t possible without completely degrading the
    model, but would make the model up to 8 times smaller. The Bloom model would be
    a measly ~40GB, small enough to fit on a single GPU. This is of course, as far
    as quantizing can take us and if we wanted to shrink further we’d need to look
    at additional methods.
  prefs: []
  type: TYPE_NORMAL
- en: The best part of quantization though is that it is easy to do. There are many
    frameworks that allow this, but in Listing 3.1 I demonstrate how to use pytorch’s
    quantization library to do a simple post training static quantization (PTQ). All
    you need is the full precision model, some example inputs, and a validation dataset
    to prepare and calibrate with. As you can see it’s only a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.1 Example PTQ in PyTorch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Static PTQ is the most straightforward approach to quantizing, done after the
    model is trained and quantizing all the model parameters uniformly. As with most
    formulas in life, the most straightforward approach introduces more error. Oftentimes
    this error is acceptable, but when it’s not we can add extra complexity to reduce
    the accuracy loss from quantization. Some methods to consider are uniform vs non-uniform,
    static vs dynamic, symmetric vs asymmetric, and applying it during or after training.
  prefs: []
  type: TYPE_NORMAL
- en: To understand these methods let's consider the case where we are quantizing
    from FP32 to INT8\. In FP32 we essentially have the full range of numbers at our
    disposal, but in INT8 we only have 256 values, we are trying to put a genie into
    a bottle and it’s no small feat. If you study the weights in your model, you might
    notice that the majority of the numbers are fractions between [-1, 1]. We could
    take advantage of this by then using an 8-bit standard that represents more values
    in this region in a non-uniform way instead of the standard uniform [-128, 127].
    While mathematically possible, unfortunately, any such standards aren’t commonplace
    and modern day deep learning hardware and software are not designed to take advantage
    of this. So for now, it's best to just stick to uniform quantization.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest approach to shrink the data is to just normalize it, but since
    we are going from a continuous scale to a discrete scale there are a few gotchas,
    so let's explore those. First, we start by taking the min and max and scale them
    down to match our new number range, we would then bucket all the other numbers
    based on where they fall. Of course, if we have really large outliers, we may
    find all our other numbers squeezed into just one or two buckets completely ruining
    any granularity we once had. To prevent this, we can simply clip any large numbers.
    This is what we do in static quantization. However, before we clip the data, what
    if we chose a range and scale that captures the majority of our data beforehand?
    We need to be careful, since if this dynamic range is too small, we will introduce
    more clipping errors, if it’s too big, we will introduce more rounding errors.
    The goal of dynamic quantization of course is to reduce both errors.
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to consider the symmetry of the data. Generally in normalization
    we force the data to be normal and thus symmetric, however, we could choose to
    scale the data in a way that leaves any asymmetry it had. By doing this we could
    potentially reduce our overall loss due to the clipping and rounding errors, but
    it’s not a guarantee.
  prefs: []
  type: TYPE_NORMAL
- en: As a last resort, if none of these other methods failed to reduce accuracy loss
    of the model, we can use Quantization Aware Training (QAT). QAT is a simple process
    where we add a fake quantization step during training of the model. By fake, I
    mean, we clip and round the data while leaving it in full precision. This allows
    the model to adjust for the error and bias introduced by quantization while it’s
    training. QAT is known to produce higher accuracy compared to other methods but
    at a much higher cost in time to train.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Uniform vs Non-uniform: whether or not we use an 8-bit standard that is uniform
    in the range it represents or non-uniform to be more precise in the -1 to 1 range.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Static vs Dynamic: Choosing to adjust the range or scale before clipping in
    an attempt to reduce clipping and rounding errors and reduce data loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Symmetric vs Asymmetric: Normalizing the data to be normal and force symmetry,
    or choosing to keep any asymmetry and skew.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During or After Training: Quantization after training is really easy to do,
    and while doing it during training is more work it leads to reduced bias and better
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantizing is a very powerful tool. Not only does it reduce the size of the
    model, but it also reduces the computational overhead required to run the model
    thus reducing latency and cost of running the model. But the best fact about quantization
    is that it can be done after the fact, so you don’t have to worry about whether
    or not your data scientists remembered to quantize the model during training using
    processes like QAT. This is why quantization has become so popular when working
    with LLMs and other large machine learning models. While reduced accuracy is always
    a concern with compression techniques, compared to other methods, quantization
    is a win-win-win.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Congratulations, you just trained a brand new LLM! With billions of parameters
    all of them must be useful right? Wrong! Unfortunately, as with most things in
    life, the model’s parameters tend to follow the Pareto Principle. About 20% of
    the weights lead to 80% of the value. “If that’s true,” you may be asking yourself,
    “Why don’t we just go and cut out all the extra fluff?” Great idea! Give yourself
    a pat on the back. Pruning is the process of weeding out and removing any parts
    of the model that we deem unworthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are essentially two different pruning methods: **structured** and **unstructured**.
    Structured pruning is the process of finding structural components of a model
    that aren’t contributing to the model’s performance and then removing them. Whether
    they be filters, channels, or layers in the neural network. The advantages to
    this method is that your model will be a little smaller but keep the same basic
    structure, which means we don’t have to worry about losing hardware efficiencies,
    we are also guaranteed a latency improvement as there will be less computations
    involved.'
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured pruning on the other hand, is shifting through the parameters and
    zeroing out the less important ones that don’t contribute much to the model’s
    performance. Unlike structured pruning, we don’t actually remove any parameters,
    just set them to zero. From this, we can imagine that a good place to start would
    be any weights or activations that are already close to 0\. Of course, while this
    effectively reduces the size of a model this also means we don’t cut out any computations,
    so it’s common to only see minimal latency improvement–if at all. But a smaller
    model still means faster load times and less GPUs to run. It also gives us very
    fine-grained control over the process, allowing us to shrink a model further than
    we could with structured pruning with less impact to performance too.
  prefs: []
  type: TYPE_NORMAL
- en: Like quantization, pruning can be done after a model is trained. However, unlike
    quantization, it’s common practice to see additional fine-tuning needing to be
    done to prevent too much loss of performance. It’s becoming more common to just
    include pruning steps during the model training to avoid the need to fine-tune
    later on. Since a more sparse model will have fewer parameters to tune, adding
    these pruning steps may help a model converge faster as well.[[5]](#_ftn5)
  prefs: []
  type: TYPE_NORMAL
- en: You’ll be surprised at how much you can shrink a model with pruning while minimally
    impacting performance. How much? In the SparseGPT[[6]](#_ftn6) paper, a method
    was developed to try to automatically one shot the pruning process without the
    need for finetuning after. They found they could decrease a GPT-3 model by 50-60%
    without issue! Depending on the model and task they even saw slight improvements
    in a few of them. Looking forward to seeing where Pruning takes us in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Knowledge distillation is probably the coolest method of compression in my mind.
    It’s a simple idea too, we’ll take the large LLM, and have it train a smaller
    language model to copy it. What’s nice about this method is that the larger LLM
    provides essentially an infinite dataset for the smaller model to train on, which
    can make the training quite effective. Because of the simple fact that the larger
    the dataset the better the performance, we've often seen smaller models reach
    almost the same level as their teacher counterparts in accuracy.[[7]](#_ftn7)
  prefs: []
  type: TYPE_NORMAL
- en: A smaller model trained this way is guaranteed to both be smaller and improve
    latency. The downside is that it’s going to require us to train a completely new
    model. Which is a pretty significant upfront cost to pay. Any future improvements
    to the teacher model will require being passed down to the student model, which
    can lead to complex training cycles and version structure. It’s definitely a lot
    more work compared to some of the other compression methods.
  prefs: []
  type: TYPE_NORMAL
- en: The hardest part about knowledge distillation though is that we don’t really
    have good recipes for them yet. Tough questions like, “How small can the student
    model be?” will have to be solved through trial and error. There’s still a lot
    to learn and research to be done here.
  prefs: []
  type: TYPE_NORMAL
- en: However, there has been some exciting work in this field via Stanford’s Alpaca[[8]](#_ftn8).
    Instead of training a student model from scratch, they instead chose to finetune
    the open source LLaMA 7B parameter model using OpenAI’s GPT3.5’s 175 billion parameter
    model as a teacher via knowledge distillation. A simple idea but it paid off big,
    as they were able to get great results from their evaluation. The biggest surprise
    was the cost, as they only spent $500 on API costs to get the training data from
    the teacher model, and $100 worth of GPU training time to finetune the student
    model. Granted, if you did this for a commercial application you’d be violating
    OpenAI’s terms of service, so best to stick to using your own or open source models
    as the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank Approximation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Low-rank approximation, also known as low-rank factorization, low-rank decomposition,
    matrix factorization (too many names! I blame the mathematicians), uses linear
    algebra math tricks to simplify large matrices or tensors finding a lower-dimensional
    representation. There are several techniques to do this. Singular Value Decomposition
    (SVD), Tucker Decomposition(TD), and Canonical Polyadic Decomposition (CPD) are
    the most common ones you run into.
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 3.2 we show the general idea behind the SVD method. Essentially we
    are going to take a very large matrix, A, and break it up into 3 smaller matrices,
    U, ∑, and V. While U and V are there to ensure we keep the same dimensions and
    relative strengths of the original matrix, ∑ allows us to apply a direction and
    bias. The smaller ∑ is, the more we end up compressing and reducing the total
    number of parameters, but the less accurate the approximation becomes.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 Example of SVD a Low-rank Approximation. A is a large matrix with
    dimensions N and M. We can approximate it with three smaller matrices, U with
    dimensions M and P, ∑ a square matrix with dimension P, and V with dimensions
    N and P (here we show the transpose). Usually both P<<M and P<<N are true.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image002.png)'
  prefs: []
  type: TYPE_IMG
- en: To help solidify this concept, it may help to see a concrete example. In Listing
    3.2 we show a simple example of SVD at work compressing a 4x4 matrix. For this
    we only need the basic libraries SciPy and NumPy which are imported on lines 1
    and 2\. Line 3 we define the matrix, and then line 9 we apply SVD to it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.2 Example SVD Low-rank Approximation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Taking a moment to inspect U, Sigma, and the transpose of V, we see a 4x1 matrix,
    a 1x1 matrix, and a 1x4 matrix respectively. All in all we now only need 9 parameters
    vs the original 16, shrinking the memory footprint almost in half.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we multiply the matrices back together to get an approximation of the
    original matrix. In this case, the approximation isn’t all that great, but we
    can still see the general order and magnitudes match the original matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, I’m not aware of anyone actually using this to simply compress
    models in production most likely due to the poor accuracy of the approximation.
    What they are using it for, and this is important, is adaptation and finetuning;
    which is where LoRA[[9]](#_ftn9) comes in, Low Rank Adaptation. Adaptation is
    simply the process of fine-tuning a generic or base model to do a specific task.
    LoRA applies SVD low rank approximation to the attention weights, or rather, to
    injected update matrices that run parallel to the attention weights, allowing
    us to fine-tune a much smaller model. LoRA has become very popular because it
    makes it a breeze to take an LLM, shrink the trainable layers to a tiny fraction
    of the original model and then allow anyone to train it on commodity hardware.
    You can get started with LoRA by using the PEFT[[10]](#_ftn10) library from HuggingFace,
    where they have several LoRA tutorials you can check out.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of Experts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mixture of experts (MoE) is a technique where we replace the feed forward layers
    in a transformer with MoE layers instead. MoE’s are a group of sparsely activated
    models. It differs from ensemble techniques in that typically only one or a few
    expert models will be run, rather than combining results from all models. The
    sparsity is often induced by a Gate mechanism that learns which experts to use,
    and/or a Router mechanism that determines which experts should even be consulted.
    In Figure 3.3 we demonstrate the MoE architecture with potentially N experts,
    as well as show where it goes inside a decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 Example Mixture of Experts model with both a Gate and Router to control
    flow. The MoE model is used to replace the FFN layers in a transformer, here we
    show it replacing the FFN in a Decoder.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on how many experts you have, the MoE layer could potentially have
    more parameters than the FFN leading to a larger model, but in practice this isn’t
    the case since engineers and researchers are aiming to create a smaller model.
    What we are guaranteed to see though is a faster computation path and improved
    inference times. However, what really makes MoE stand out is when it’s combined
    with quantization. One study[[11]](#_ftn11) between Microsoft and NVIDIA showed
    they were able to reach 2-bit quantization with only minimal impact to accuracy
    using MoE!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, since this is a pretty big change to the model’s structure it will
    require finetuning afterwards. You should also be aware that MoE layers often
    reduce a model’s generalizability so it’s best when used on models designed for
    a specific task. There are several libraries to implement MoE layers, but I’d
    recommend checking out DeepSpeed[[12]](#_ftn12).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Distributed Computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distributed computing is a technique used in deep learning to parallelize and
    speed-up large, complex neural networks by dividing the workload across multiple
    devices or nodes in a cluster. This approach significantly reduces training and
    inference times by enabling concurrent computation, data parallelism, and model
    parallelism. With the ever-growing size of datasets and complexity of models,
    distributed computing has become crucial for deep learning workflows, ensuring
    efficient resource utilization and enabling researchers to effectively iterate
    on their models. Distributed computing is one of the core practices that separate
    deep learning from machine learning, and with LLMs we have to pull out every trick
    in the book. Let’s look at different parallel processing practices to take full
    advantage of distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Data Parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data parallelism is what most people think about when they think about running
    processes in parallel, it’s also the easiest to do. The practice involves splitting
    up the data and running them through multiple copies of the model or pipeline.
    For most frameworks this is easy to set up, for example, in PyTorch you can use
    the DistributedDataParallel method. There’s just one catch for most of these set-ups
    and that is your model has to be able to fit onto one GPU. This is where a tool
    like Ray.io comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Ray is an open-source project designed for distributed computing, specifically
    aimed at parallel and cluster computing. It's a flexible and user-friendly tool
    which simplifies distributed programming and helps developers execute concurrent
    tasks in parallel with ease. Ray is primarily built for machine learning and other
    high-performance applications but can be utilized in other applications. In Listing
    3.3 we give a simple example of using Ray to distribute a task. The beauty of
    Ray is the simplicity–all we need to do to make our code run in parallel is add
    a decorator. Sure beats the complexity of multithreading or asynchronization set-ups.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.3 Example Ray Parallelization Task
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Ray uses the concept of tasks and actors to manage distributed computing. Tasks
    are functions, whereas actors are stateful objects that can be invoked and run
    concurrently. When you execute tasks using Ray, it handles distributing tasks
    across the available resources (e.g., multi-core CPUs or multiple nodes in a cluster).
    For LLMs, we would need to set up a Ray cluster[[13]](#_ftn13) in a cloud environment
    as this would allow each pipeline to run on a node with as many GPUs as needed,
    greatly simplifying the infrastructure set up to run LLMs in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple alternatives out there, but Ray has been gaining a lot of
    traction and becoming more popular as more and more machine learning workflows
    require distributed training. My team has had great success with it. By utilizing
    Ray, developers can ensure better performance and more efficient utilization of
    resources in distributed workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tensor parallelism is taking advantage of matrix multiplication properties to
    split up the activations across multiple processors, running the data through,
    and then combining them on the other side of the processors. Figure 3.4 demonstrates
    how this process works for a matrix, which can be parallelized in two separate
    ways that give us the same result. Imagine that Y is a really big matrix that
    can’t fit on a single processor, or more likely, a bottleneck in our data flow
    that takes too much time to run all the calculations. In either case, we could
    split Y up, either by columns or by rows, run the calculations, and then combine
    the results after. In this example we are dealing with matrices but in reality
    we are often dealing with tensors that have more than two dimensions, but the
    same mathematical principles that make this work still apply.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing which dimension to parallelize is a bit of an art, but a few things
    to remember to help make this decision easier. First, how many columns or rows
    do you have? In general, you want to pick a dimension that has more than the number
    of processors you have, else you will end up stopping short. Generally this isn’t
    a problem but with tools like Ray discussed in the last section, parallelizing
    in a cluster and spinning up loads of processes is a breeze. Second, different
    dimensions have different multiplicity costs. For example, column parallelism
    requires us to send the entire dataset to each process, but with the benefit of
    concatenating them together at the end which is fast and easy. Row parallelism
    however, allows us to break up the dataset into chunks, but requires us to add
    the results, a more expensive operation than concatenating. You can see that one
    operation is more I/O bound, while the other is more computation bound. Ultimately,
    the best dimension will be dataset dependent, as well as hardware limited. It
    will require experimentation to fully optimize this, but a good default is to
    just choose the largest dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 Tensor Parallelism example showing that you can break up tensors
    by different dimensions and get the same end result. Here we compare column and
    row parallelism of a Matrix.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image004.png)'
  prefs: []
  type: TYPE_IMG
- en: Tensor parallelism allows us to split up the heavy computation layers like MLP
    and Attention layers onto different devices, but doesn’t help us with Normalization
    or Dropout layers that don’t utilize tensors. To get better overall performance
    of our pipeline we can add sequence parallelism which targets these blocks[[14]](#_ftn14).
    Sequence parallelism is a process that partitions activations along the sequence
    dimension, preventing redundant storage, and can be mixed with tensor parallelism
    to achieve significant memory savings with minimal additional computational overhead.
    In combination, they reduce the memory needed to store activations in Transformer
    models. In fact, they nearly eliminate activation recomputation and save activation
    memory up to 5x.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 Combining tensor parallelism that focuses on computational heavy
    layers with sequence parallelism to reduce memory overhead to create a fully parallel
    process for the entire transformer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 shows how combining both tensor parallelism, that allows us to distribute
    the computationally heavy layers, and sequence parallelism that does the same
    for the memory limiting layers, allows us to fully parallelize the entire transformer
    model. Together, they allow for extremely efficient use of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline Parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So far we can now run lots of data, and speed up any bottlenecks, but none of
    that matters because our model is too big; we can’t fit it into a single GPU’s
    memory to even get it to run. That’s where pipeline parallelism comes in and is
    the process of splitting up a model vertically and putting each part onto a different
    GPU. This creates a pipeline as input data will go to the first GPU, process,
    then transfer to the next GPU, and so on until it’s run through the entire model.
    While other parallelism techniques improve our processing power and speed up inference,
    pipeline parallelism is required to just get it to run and it comes with some
    major downsides, mainly device utilization.
  prefs: []
  type: TYPE_NORMAL
- en: To understand where this downside comes from and how to mitigate it, let's first
    consider the naive approach to this, where we simply run all the data at once
    through the model. What we find is that this leaves a giant “bubble” of underutilization.
    Since the model is broken up, we have to process everything sequentially through
    the devices. This means that while one GPU is processing, the others are sitting
    idle. In Figure 3.6 we can see this naive approach and a large bubble of inactivity
    as the GPUs sit idle. We also see a better way to take advantage of each device.
    We do this by sending the data in small batches. A smaller batch allows the first
    GPU to pass on what it was working on quicker and move on to another batch. This
    allows the next device to get started earlier and reduces the size of the bubble.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 The Bubble Problem. When data runs through a broken up model, the
    GPUs holding the model weights are underutilized while they wait for their counterparts
    to process the data. A simple way to reduce this bubble is to use microbatching.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can actually calculate the size of the bubble quite easily with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Idle Percentage = 1 - m / ( m + n - 1)
  prefs: []
  type: TYPE_NORMAL
- en: Where m is the number of microbatches, and n is the depth of the pipeline or
    number of GPUs. So for our naive example case of 4 GPUs and 1 large batch we see
    the devices sitting idle 75% of the time! GPUs are quite expensive to allow to
    be sitting idle three quarters of the time. Let’s see what that looks like using
    the microbatch strategy. With a microbatch of 4, it cuts this almost in half,
    down to just 43% of the time. What we can glean from this formula is that the
    more GPUs we have, the higher the idle times, but the more microbatches the better
    the utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we can often neither reduce the number of GPUs nor can we just
    make the microbatches as large as we want. There are limits. For GPU’s we just
    have to use as many as it takes to fit the model into memory, but try to use a
    few larger GPUs as it will lead to a more optimal utilization compared to using
    many smaller GPUs. Reducing the bubble in pipeline parallelism is another reason
    why compression is so important. For microbatching, the first limit is obvious
    once told, since the microbatch is a fraction of our batch size, we are limited
    by how big that is. The second is that each microbatch increases the memory demand
    for cached activations in a linear relationship. One way to counter this higher
    memory demand is a method called PipeDream[[15]](#_ftn15). There are different
    configurations and approaches, but the basic idea is the same. In this method
    we start working on the backward pass as soon as we’ve finished the forward pass
    of any of the microbatches. This allows us to fully complete a training cycle
    and release the cache for that microbatch.
  prefs: []
  type: TYPE_NORMAL
- en: 3D Parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For LLMs, we are going to want to take advantage of all three parallelism practices
    as they can all be run together. This is known as 3D Parallelism combining Data,
    Tensor, and Pipeline Parallelism (DP + TP + PP) together. Since each technique,
    and thus dimension, will require at least 2 GPUs, in order to run 3D Parallelism,
    we’ll need at least 8 GPUs to even get started. How we configure these GPUs will
    be important to get the most efficiency out of this process, namely, because TP
    has the largest communication overhead we want to ensure these GPUs are close
    together, preferably on the same node and machine. PP has the least communication
    volume of the three, so breaking up the model across nodes is the least expensive
    here.
  prefs: []
  type: TYPE_NORMAL
- en: By running the three together, we see some interesting interactions and synergies
    between them. Since TP splits the model to work well within a device's memory,
    we see that PP can perform well even with small batch sizes due to the reduced
    effective batch size enabled by TP. This combination also improves the communication
    between DP nodes at different pipeline stages, allowing DP to work effectively
    too. The communication bandwidth between nodes is proportional to the number of
    pipeline stages, because of this DP is able to scale well even with smaller batch
    sizes. Overall, we see running in combination that
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know some tricks of the trade, it’s just as important to have the
    right tools to do the job.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Large Language Models Operations Infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are finally going to start talking about the infrastructure needed to make
    this all work. This likely comes as a surprise as I know several readers would
    have expected this section at the beginning of chapter 1\. Why wait till the end
    of chapter 3? In the many times I’ve interviewed Machine Learning Engineers I
    often asked this open-ended question, “What can you tell me about MLOps?” An easy
    softball question to get the conversation going. Most junior candidates would
    immediately start jumping into the tooling and infrastructure. It makes sense,
    there are so many different tools available. Not to mention, whenever you see
    posts or blogs describing MLOps there’s a pretty little diagram showing the infrastructure.
    While all of that is important it’s useful to recognize what a more senior candidate
    jumps into, the machine learning lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: For many the nuance is lost, but the infrastructure is the how, the lifecycle
    is the why. Most companies can get by with just bare-bones infrastructure. I’ve
    seen my share of scrappy systems that exist entirely on one Data Scientist’s laptop,
    and they work surprisingly well! Especially in the era of scikit learn everything.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, a rickshaw machine learning platform doesn’t cut it in the world
    of LLMs. Since we still live in a world where the standard storage capacity of
    a MacBook Pro laptop is 256GB, just storing the model locally can already be a
    problem. Companies that invest in a more sturdy infrastructure are better prepared
    for the world of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 3.7 we see an example MLOps Infrastructure designed with LLMs in mind.
    While most infrastructure diagrams I’ve seen in my time have always simplified
    the structure to make everything look clean, the raw truth is that there’s a bit
    more complexity to the entire system. Of course a lot of this complexity would
    disappear if we could just get Data Scientists to work inside scripts instead
    of ad hoc workstations–usually with a jupyter notebook interface.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 a high level view of an MLOps infrastructure with LLMs in mind. This
    attempts to cover the full picture, and the complexity of the many tools involved
    to make ML models work in production.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](images/03__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Taking a closer look at Figure 3.7 you can see several tools on the outskirts
    that squarely land in DataOps, or even just DevOps. Data Stores, Orchestrators,
    Pipelines, Streaming Integrations, and Container Registries. These are tools you
    are likely already using for just about any data intensive application and aren’t
    necessarily MLOps focused. Towards the center we have more traditional MLOps tools,
    Experiment Trackers, Model Registry, Feature Store, and Ad hoc Data Science Workstations.
    For LLMs we really only introduce one new tool to the stack: a Vector Database.
    What’s not pictured because it intertwines with every piece is the Monitoring
    System. This all culminates to what we are working towards in this book, a Deployment
    Service, where we can confidently deploy and run LLMs in Production.'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure by discipline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '**DevOps:** In charge of procuring the environmental resources—experimental
    (dev, staging) and production—this includes hardware, clusters, and networking
    to make it all work. Also in charge of basic infrastructure systems like Github/Gitlab,
    artifact registries, container registries, application or transactional databases
    like Postgres or MySQL, caching systems, and CI/CD pipelines. This is by no means
    a comprehensive list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataOps:** In charge of data, in motion and at rest. Includes centralized
    or decentralized data stores like Data Warehouses, Data Lakes, and Data Meshes.
    As well as data pipelines either in batch systems or in streaming systems with
    tools like Kafka and Flink. Also includes orchestrators like Airflow, Prefect
    or Mage. DataOps is built on top of DevOps. For example, I’ve seen many CI/CD
    pipelines being used for data pipeline work until eventually being graduated to
    systems like Apache Spark or DBT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLOps:** In charge of machine learning lifecycle from creation of models
    to deprecation. This includes data science workstations like Jupyterhub, experiment
    trackers, and a model registry. It includes specialty databases like Feature Stores
    and Vector Databases. As well as a deployment service to tie everything together
    and actually serve results. It is built on top of both DataOps and DevOps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through each piece of the infrastructure puzzle and discuss features
    you should be considering when thinking about LLMs in particular. While we will
    be discussing tooling that is specialized for each piece, I’ll just make note
    that there are also MLOps as a service platforms like Dataiku, Amazon’s Sagemaker
    and Google’s VertexAI. These platforms attempt to give you the whole puzzle, how
    well they do that is another question, but are often a great shortcut and you
    should be aware of them. Well, I think that’s enough dilly-dallying, let’s dive
    in already!
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Data Infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While not the focus of this book it’s important to note that MLOps is built
    on top of a Data Operations infrastructure–which itself is built on top of DevOps.
    Key features of the DataOps ecosystem include a data store, an orchestrator, and
    pipelines. Additional features usually required include a container registry and
    a streaming integration service.
  prefs: []
  type: TYPE_NORMAL
- en: Data stores are the foundation of DataOps and come in many forms these days,
    from a simple database to large data warehouses, and from even larger data lakes
    to an intricate data mesh. This is where your data is stored and a lot of work
    goes into managing, governing, and securing the data store. The orchestrator is
    the cornerstone of DataOps as it’s a tool that manages and automates both simple
    and complex, multistep workflows and tasks. Ensuring they run across multiple
    resources and services in a system. The most commonly talked about being Airflow,
    Prefect, and Mage. Lastly, pipelines are the pillars. They hold everything else
    up, and are where we actually run our jobs. Initially built to simply move, clean,
    and define data, these same systems are used to run machine learning training
    jobs on a schedule, do batch inference, and loads of other work needed to ensure
    MLOps runs smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: A container registry is a keystone of DevOps and subsequently DataOps and MLOps
    as well. Being able to run all our pipelines and services in containers is necessary
    to ensure consistency. Streaming services are actually a much bigger beast than
    what I may let on in this chapter, and if you know you know. Thankfully for most
    text related tasks real time processing isn’t a major concern. Even for tasks
    like real-time captioning or translation, we can often get by with some sort of
    pseudo real-time processing strategy that doesn’t degrade the user experience
    depending on the task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Experiment Trackers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experiment trackers are central to MLOps. Experiment trackers do the fundamental
    job of keeping track and recording tests and results. As the famous Adam Savage
    quote from Mythbusters, “Remember kids, the only difference between screwing around
    and science is writing it down.” Without it, your organization is likely missing
    the “science” in data science which is honestly quite embarrassing.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your data scientists are keen to manually track and record results in
    notebooks, it might as well be thrown in the garbage if it’s not easy for others
    to view and search for. This is really the purpose of experiment trackers, to
    ensure knowledge is easily shared and made available. Eventually a model will
    make it to production and that model is going to have issues. Sure, you can always
    just train a new model, but unless the team is able to go back and investigate
    what went wrong the first time you are likely to repeat the same mistakes over
    and over.
  prefs: []
  type: TYPE_NORMAL
- en: There are many experiment trackers out there, the most popular by far is MLFlow
    which is open source. It was started by the team at Databricks which also offers
    an easy hosting solution. Some paid alternatives worth checking out include CometML
    and Weights and Biases.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment trackers nowadays come with so many bells and whistles. Most open
    source and paid solutions will certainly have what you need when looking to scale
    up your needs for LLMOps. However, ensuring you take advantage of these tools
    correctly might require a few small tweaks. For example, the default assumption
    is usually that you are training a model from scratch, but often when working
    with LLMs you will be finetuning models instead. In this case, it’s important
    to note the checkpoint of the model you started from. If possible, even linking
    back to the original training experiment. This will allow future scientists to
    dig deeper into their test results, find original training data, and discover
    paths forward to eliminate bias.
  prefs: []
  type: TYPE_NORMAL
- en: Another feature to look out for is evaluation metric tooling. We will be going
    more in-depth in Chapter 4, but evaluation metrics are difficult for language
    models. There are often multiple metrics you care about and none of them are simple
    like complexity ratings or similarity scores. While experiment tracker vendors
    try to be agnostic and unopinionated about evaluation metrics they should at least
    make it easy to compare models and their metrics to make it easy to decide which
    one is better. Since LLMs have become so popular some have made it easy to evaluate
    on the more common metrics like ROUGE for text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: You will also find many experiment tracking vendors have started to add tools
    specifically for LLMs. Some features you might consider looking for include direct
    HuggingFace support, LangChain support, prompt engineering toolkits, finetuning
    frameworks, and foundation model shops. The space is developing quickly, and no
    one tool has all the same features right now, but I’m sure these feature sets
    will likely converge.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Model Registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model registry is probably the simplest tool of an MLOps infrastructure.
    The main objective is one that’s easy to solve, we just need a place to store
    the models. I’ve seen many successful teams get by simply by putting their models
    in an object store or shared file system and calling it good. That said, there’s
    a couple bells and whistles you should look for when choosing one.
  prefs: []
  type: TYPE_NORMAL
- en: The first is whether or not the model registry tracks metadata about the model.
    Most of what you care about is going to be in the experiment tracker, so you can
    usually get away with simply ensuring you can link the two. In fact, most model
    registries are built into experiment tracking systems because of this. However,
    an issue I’ve seen time and time again with these systems happens when the company
    decides to use an open source model or even buy one. Is it easy to upload a model
    and tag it with relevant information? The answer is usually no.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you are going to want to make sure you can version your models. At some
    point, a model will reach a point where it’s no longer useful and will need to
    be replaced. Versioning your models will simplify this process. It also makes
    running production experiments like A/B testing or shadow tests easier.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, if we are promoting and demoting models, we need to be concerned with
    access. Models tend to be valuable intellectual property for many companies, ensuring
    only the right users have access to the models is important. But it’s also important
    to ensure that only the team that understands the models, what they do and why
    they were trained, are in charge of promoting and demoting the models. The last
    thing we want is to delete a model in production or worse.
  prefs: []
  type: TYPE_NORMAL
- en: For LLMs there are some important caveats you should be aware of, mainly, when
    choosing a model registry, be aware of any limit sizes. I’ve seen several model
    registries restrict model sizes to 10GB or smaller. That’s just not going to cut
    it. I could speculate on the many reasons for this but none of them are worthy
    of note. Speaking of limit sizes, if you are going to be running your model registry
    on an premise storage system like Ceph, make sure it has lots of space. You can
    buy multiple terabytes of storage for a couple hundred dollars for your on prem
    servers, but even a couple terabytes fills up quickly when your LLM is over 300GB.
    Don’t forget, you are likely to be keeping multiple checkpoints and versions during
    training and finetuning; as well as duplicates for reliability purposes. Storage
    is still one of the cheapest aspects of running LLMs though, no reason to skimp
    here and cause headaches down the road.
  prefs: []
  type: TYPE_NORMAL
- en: 'This does bring me to a good point: there''s a lot of optimization that could
    still be made, allowing for better space saving approaches to storing LLMs and
    their derivatives. Especially since most of these models will be very similar
    overall. I imagine we’ll likely see storage solutions to solve just this problem
    in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Feature Store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Feature stores solve many important problems and answer questions like: Who
    owns this feature? How was it defined? Who has access to it? Which models are
    using it? How do I serve this feature in production? Essentially, they solve the
    “single source of truth” problem. By creating a centralized store, it allows teams
    to shop for the highest quality, most well maintained, thoroughly managed data.
    Feature stores solve the collaboration, documentation, and versioning of data.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve ever thought, “A feature store is just a database right?” you are
    probably thinking about the wrong type of store–we are referencing a place to
    shop not a place of storage. Don’t worry, this confusion is normal as I’ve heard
    this sentiment a lot, and have had similar thoughts myself. The truth is, modern
    day feature stores are more virtual than a physical database, which means they
    are built on top of whatever data store you are already using. For example, Google’s
    Vertex AI feature store is just BigQuery and I’ve seen a lot of confusion from
    data teams wondering, “Why don’t I just query BigQuery?” Loading the data into
    a feature store feels like an unnecessary extra step, but think about shopping
    at an IKEA store. No one goes directly to the warehouse where all the furniture
    is in boxes. That would be a frustrating shopping experience. The features store
    is the show rooms that allows others in your company to easily peruse, experience,
    and use the data.
  prefs: []
  type: TYPE_NORMAL
- en: Often times, I see people reach for a feature store to solve a technical problem
    like low latency access for online feature serving. A huge win for feature stores
    is solving the training-serving skew. Some features are just easier to do in SQL
    after the fact, like calculating the average number of requests for the last 30
    seconds. This can lead to naive data pipelines built for training, but causing
    massive headaches when going to production because getting this type of feature
    in real time can be anything but easy. Feature stores abstractions help minimize
    this burden. Related to this is feature stores point-in-time retrievals which
    are table stakes when talking feature stores. Point-in-time retrievals ensure
    that given a specific time a query will always return the same result. This is
    important because features like averages over “the last 30 seconds” are constantly
    changing, so this allows us to version the data (without the extra burden of a
    bloated versioning system), as well as ensure our models will give accurate and
    predictable responses.
  prefs: []
  type: TYPE_NORMAL
- en: As far as options, Feast is a popular open source feature store. FeatureForm
    and Hopsworks are also open source. All three of which offer paid hosting options.
    For LLMs I’ve heard the sentiment that feature stores aren’t as critical as other
    parts of the MLOps infrastructure. After all, the model is so large it should
    incorporate all needed features inside it, so you don’t need to query for additional
    context, just give the model the user’s query and let the model do its thing.
    However, this approach is still a bit naive and we haven’t quite gotten to a point
    where LLMs are completely self-sufficient. To avoid hallucinations and improve
    factual correctness, it is often best to give the model some context. We do this
    by feeding it embeddings of our documents we want it to know very well, and a
    feature store is a great place to put these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.5 Vector Databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are familiar with the general MLOps infrastructure, most of this section
    has been review for you. We’ve only had to make slight adjustments highlighting
    important scaling concerns to make a system work for LLMs. Vector Databases however
    are new to the scene and have been developed to be a tailored solution for working
    with LLMs and language models in general, but you can also use them with other
    datasets like images or tabular data which are easy enough to transform into a
    vector. Vector databases are specialized databases to store vectors along with
    some metadata around the vector, which makes them great for storing embeddings.
    Now, while that last sentence is true, it is a bit misleading, because the power
    of vector databases isn’t in their storage, but in the way that they search through
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional databases, using b-tree indexing to find ID’s or text based search
    using reverse indexes, all have the same common flaw, you have to know what you
    are looking for. If you don’t have the ID or you don’t know the keywords it’s
    impossible to find the right row or document. Vector databases however, take advantage
    of the vector space meaning you don’t need to know exactly what you are looking
    for, you just need to know something similar which you can then use to find the
    nearest neighbors using similarity searches based on Euclidean distance, Cosine
    similarity, Dot product similarity, or what have you. Using a vector database
    makes solving the reverse image search problem a breeze, as an example.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, I’m sure some readers may be confused. First I told you to put
    your embeddings into a feature store, and now I’m telling you to put them into
    a Vector DB, which one is it? Well that’s the beauty of it, you can do both at
    the same time. If it didn’t make sense before I hope it makes sense now. Feature
    stores are not a database they are just an abstraction, you can use a feature
    store built on top of a Vector DB and it will solve many of your problems. Vector
    DBs can be difficult to maintain when you have multiple data sources, experimenting
    with different embedding models or otherwise have frequent data update. Managing
    this complexity can be a real pain, but a feature store can handily solve this
    problem. Using them in combination will ensure a more accurate and up-to-date
    search index.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases have only been around for a couple of years at the time of
    writing, and their popularity is still relatively new as it has grown hand in
    hand with LLMs. It’s easy to understand why since they provide a fast and efficient
    way to retrieve vector data making it easy to provide LLMs with needed context
    to improve their accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: That said it’s a relatively new field and there are lots of competitors in this
    space right now, and it’s a bit too early to know who the winners and losers are.
    Not wanting to date this book too much, let me at least suggest two options to
    start Pinecone and Milvus. Pinecone is one of the first vector databases as a
    product and has a thriving community with lots of documentation. It’s packed with
    features and has proven itself to scale. Pinecone is a fully managed infrastructure
    offering that has a free tier for beginners to learn. If you are a fan of open
    source however, then you’ll want to check out Milvus. Milvus is feature rich and
    has a great community. Zilliz the company behind Milvus offers a fully managed
    offering, but it’s also available to deploy in your own clusters and if you already
    have a bit of infrastructure experience it’s relatively easy and straightforward
    to do.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of alternatives out there right now, and it’s likely worth a
    bit of investigation before picking one. Probably the two things you’ll care most
    about is price and scalability, the two often going hand in hand. After that,
    it’s valuable to pay attention to search features, like support for different
    similarity measures like cosine similarities, dot product or euclidean distance.
    As well as indexing features like HNSW (Heirarchical Navigable Small World) or
    LSH (Locality-Sensitive Hashing). Being able to customize your search parameters
    and index settings are important for any database as they allow you to customize
    the workload for your dataset and workflow allowing you to optimize query latency
    and search result accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to note that with vector databases rise in popularity we
    are quickly seeing many database incumbents like Redis and Elastic offering vector
    search capabilities. For now most of these tend to just offer the most straightforward
    feature sets, but they are hard to ignore if you are already using these tool
    sets as they can provide quick wins to get started quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are powerful tools that can help you train or finetune LLMs,
    as well as improve the accuracy and results of your LLM queries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.6 Monitoring System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A monitoring system is crucial to the success of any ML system, LLMs included.
    Unlike other software applications, ML models are known to fail silently—continue
    to operate, but start to give poor results. This is often due to data drift, a
    common example being a recommendation system that give worse results overtime
    because sellers start to game the system by giving fake reviews to get better
    recommendation results. A monitoring system allows us to catch poorly performing
    models, make adjustments or simply retrain them.
  prefs: []
  type: TYPE_NORMAL
- en: Despite their importance they are often the last piece of the puzzle added.
    This is often purposeful as putting resources into figuring out how to monitor
    models doesn’t help if you don’t have any models to monitor. However, don’t make
    the mistake of putting it off too long. Many companies have been burned by a model
    that went rogue with no one knowing about it, often costing them dearly. It’s
    also important to realize you don’t have to wait to get a model into production
    to start monitoring your data. There are plenty of ways to introduce a monitoring
    system into the training and data pipelines to improve data governance and compliance.
    Regardless, you can usually tell the maturity of a data science organization by
    their monitoring system.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of great monitoring tooling out there, some great open source
    options include WhyLogs and EvidentlyAI. I’m also a fan of great expectations,
    but have found it rather slow outside of batch jobs. There are also many more
    paid options out there. Typically, for ML monitoring workloads you’ll want to
    monitor everything you’d normally record in other software applications, this
    includes resource metrics like memory and CPU utilization, performance metrics
    like latency and queries per second, as well as operational metrics like status
    codes and error rates. In additional, you’ll need ways to monitor data drift both
    going in and out of the model. You’ll want to pay attention to things like missing
    values, uniqueness, and standard deviation shifts. In many instances, you’ll want
    to be able to segment your data while monitoring, e.g. for A/B testing or to monitor
    by region. Some metrics that are useful to monitor in ML systems include model
    accuracy, precision, recall and F1 scores. These are difficult since you won’t
    know the correct answer at inference time, so it’s often useful to set up some
    sort of auditing system. Of course, auditing is going to be easier if your LLM
    is designed to be a Q&A bot than if your LLM is meant to help writers be more
    creative.
  prefs: []
  type: TYPE_NORMAL
- en: This hints at a fact that for LLMs, there are often a whole set of new challenges
    for your monitoring systems even more than what we see with other ML systems.
    With LLMs we are dealing with text data which is hard to quantify as discussed
    earlier in this chapter. For instance, think about what features do you look at
    to monitor for data drift? Because language is known to drift a lot! One feature
    I might suggest is unique tokens. This will alert you when new slang words or
    terms are created, however, it still doesn’t help when words switch meaning, for
    example, when “wicked” means “cool”. I would also recommend monitoring the embeddings,
    however, you’ll likely find this to either add a lot of noise and false alarms
    or at the very least be difficult to decipher and dig into when problems do occur.
    The systems I’ve seen work the best often involve a lot of handcrafted rules and
    features to monitor, but these can be error-prone and time-consuming to create.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring text based systems is far from a solved problem, mostly stemming
    from the difficulties of understanding text data to begin with. This does beg
    the question of what are the best methods to use language models to monitor themselves,
    since they are our current best solution to codifying language. Unfortunately,
    I’m not aware of anyone researching this, but imagine it’s only a matter of time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.7 GPU Enabled Workstations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPU enabled workstations and remote workstations in general are often considered
    a nice to have or luxury by many teams, but when working with LLMs that mindset
    has to change. When troubleshooting an issue or just developing a model in general,
    a data scientist isn’t going to be able to spin up the model in a notebook on
    their laptop anymore. The easiest way to solve this is to simply provide remote
    workstations with GPU resources. There are plenty of cloud solutions for this,
    but if your company is working mainly on prem, this may be a bit more difficult
    to provide, but necessary nonetheless.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are GPU memory intensive models. Because of this, there are some numbers
    every engineer should know when it comes to working in the field. The first, is
    how much different GPUs have. The NVIDIA Tesla T4 and V100 are two most common
    GPUs you’ll find in a datacenter, but they only have 16 GB of memory. They are
    workhorses though and cost-effective, so if we can compress our model to run on
    these all the better. After these, you’ll find a range of GPU’s like NVIDIA A10G,
    NVIDIA Quadro series, and NVIDIA RTX series that offer GPU memories in the ranges
    of 24, 32, and 48 GB. All of these are fine upgrades, you’ll just have to figure
    out which ones are offered and available to you by your cloud provider. Which
    brings us to the NVIDIA A100, which is likely going to be your GPU of choice when
    working with LLMs. Thankfully they are relatively common, and offer two different
    models providing 40 or 80 GB. The big issue you’ll have with these are that they
    are constantly in high demand by everyone right now. You should also be aware
    of the NVIDIA H100 which offers 80 GB like the A100\. The H100 NVL is promised
    to support up to 188 GB and has been designed with LLMs in mind. Another new GPU
    you should be aware of is the NVIDIA L4 Tensor Core GPU which has 24 GB and is
    positioned to take over as a new workhorse along with the T4 and V100, at least
    as far as AI workloads are concerned.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs come in all different sizes, and it’s useful to have a horse sense for
    what these numbers mean. For example, the LLaMA model has 7B, 13B, 33B, and 65B
    parameter variants. If you aren’t sure which GPU you need to run which model off
    the top of your head, here’s a shortcut, just take the number of billions of parameters
    times it by two and that’s how much GPU memory you need. The reason is, most models
    at inference are going to default to run at half precision, FP16 of BF16, which
    means for every parameter we need at least two bytes. Thus, 7 billion * 2 bytes
    = 14 GB. You’ll need a little extra as well for the embedding model which will
    be about another GB, and more for the actual tokens you are running through the
    model. One token is about 1 MB, so 512 tokens will require 512 MB. This isn’t
    a big deal, until you consider running larger batch sizes to improve performance.
    For 16 batches of this size you’ll need an extra 8 GB of space.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, so far we’ve only been talking about inference, for training you’ll
    need a lot more space. While training, you’ll always want to do this in full precision,
    and you’ll need extra room for the optimizer tensors and gradients. In general,
    to account for this you’ll need about 16 bytes for every parameter. So to train
    a 7B parameter model you’ll want 112 GB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.8 Deployment Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Everything we’ve been working towards is collected and finally put to good use
    here. In fact, if you took away every other service and were left with just a
    deployment service, you’d still have a working MLOps system. A deployment service
    provides an easy way to integrate with all the previous systems we talked about
    as well as configure and define the needed resources to get our model running
    in production. It will often provide boilerplate code to serve the model behind
    a REST and gRPC API or directly inside a batch or streaming pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Some tools to help create this service include NVIDIA Triton Inference Service,
    MLServer, Seldon and BentoML. These services provide a standard API interface,
    typically the KServe V2 Inference Protocol. This protocol provides a unified and
    extensible way to deploy, manage, and serve machine learning models across different
    platforms and frameworks. It defines a common interface to interact with models,
    including gRPC and HTTP/RESTful APIs. It standardizes concepts like input/output
    tensor data encoding, predict and explain methods, model health checks, and metadata
    retrieval. It also allows seamless integration with languages and frameworks including
    TensorFlow, PyTorch, ONNX, Scikit Learn, and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are times when flexibility and customization provide enough
    value to step away from the automated path these other frameworks provide, in
    which case it’s best to reach for a tool like FastAPI. Your deployment service
    should still provide as much automation and boilerplate code here to make the
    process as smooth as possible. It should be mentioned that most of the frameworks
    mentioned above do offer custom methods, but your mileage may vary.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model is more than just building the interface. Your deployment
    service will also provide a bridge to close the gap between the MLOps infrastructure
    and general DevOps infrastructure. Connecting to whatever CI/CD tooling as well
    as build and shipping pipelines your company has set up so you can ensure appropriate
    tests and deployment strategies like health checks and rollbacks can easily be
    monitored and done. This is often very platform and thus company-specific. Thus,
    it’ll also need to provide the needed configurations to talk to Kubernetes, or
    whatever other container orchestrator you may be using, to acquire the needed
    resources like CPU, Memory, and Accelerators, Autoscalers, Proxies, etc. It also
    applies the needed environment variables and secret management tools to ensure
    everything runs.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, this service ensures you can easily deploy a model into production.
    For LLMs, the main concern is often just ensuring the platform and clusters are
    set up with enough resources to actually provision what will ultimately be configured.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed a lot so far in this chapter, starting with what makes LLMs
    so much harder than traditional ML which is hard enough as it is. First, we learned
    that their size can’t be underestimated, but then we also discovered there are
    many peculiarities about them, from token limits to hallucinations, not to mention
    they are expensive. Fortunately, despite being difficult, they aren’t impossible.
    We discussed compression techniques and distributed computing which are crucial
    to master. We then explored the infrastructure needed to make LLMs work. While
    most of it was likely familiar, we came to realize that LLMs put a different level
    of pressure on each tool, and often we need to be ready for a larger scale than
    what we could get away with for deploying other ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are difficult to work with mostly because they are big. Which impacts a
    longer time to download, load into memory, and deploy forcing us to use expensive
    resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are also hard to deal with because they deal with natural language and
    all its complexities including hallucinations, bias, ethics, and security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless if you build or buy, LLMs are expensive and managing costs and risks
    associated with them will be crucial to the success of any project utilizing them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing models to be as small as we can will make them easier to work with;
    quantization, pruning, and knowledge distillation are particularly useful for
    this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization is popular because it is easy and can be done after training without
    any finetuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low Rank Approximation is an effective way at shrinking a model and has been
    used heavily for Adaptation thanks to LoRA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three core directions we use to parallelize LLM workflows: Data,
    Tensor, and Pipeline. DP helps us increase throughput, TP helps us increase speed,
    and PP makes it all possible to run in the first place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining the parallelism methods together we get 3D parallelism (Data+Tensor+Pipeline)
    where we find that the techniques synergize, covering each others weaknesses and
    help us get more utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The infrastructure for LLMOps is similar to MLOps, but don’t let that fool you
    since there are many caveats where “good enough” no longer works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many tools are offering new features specifically for LLM support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector Databases in particular are interesting as a new piece of the infrastructure
    puzzle needed for LLMs that allow quick search and retrievals of embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#_ftnref1) A. Bulatov, Y. Kuratov, and M. S. Burtsev, “Scaling Transformer
    to 1M tokens and beyond with RMT,” Apr. 2023, [https://arxiv.org/abs/2304.11062](abs.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#_ftnref2) R. Daws, “Medical chatbot using OpenAI’s GPT-3 told a fake
    patient to kill themselves,” AI News, Oct. 28, 2020\. [https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/](medical-chatbot-openai-gpt3-patient-kill-themselves.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#_ftnref3) T. Kington, “ChatGPT bot tricked into giving bomb-making instructions,
    say developers,” www.thetimes.co.uk, Dec 17, 2022\. [https://www.thetimes.co.uk/article/chatgpt-bot-tricked-into-giving-bomb-making-instructions-say-developers-rvktrxqb5](article.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#_ftnref4) K. Quach, “AI game bans players for NSFW stories it generated
    itself,” www.theregister.com, Oct 8, 2021\. [https://www.theregister.com/2021/10/08/ai_game_abuse/](ai_game_abuse.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#_ftnref5) T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste,
    “Sparsity in Deep Learning: Pruning and growth for efficient inference and training
    in neural networks,” Jan. 2021, [https://arxiv.org/abs/2102.00554](abs.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#_ftnref6) E. Frantar and D. Alistarh, “SparseGPT: Massive Language Models
    Can Be Accurately Pruned in One-Shot,” Jan. 2023, [https://arxiv.org/abs/2301.00774](abs.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#_ftnref7) V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter,” Oct. 2019\.
    [https://arxiv.org/abs/1910.01108](abs.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#_ftnref8) R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,
    P Liang, and T. B. Hashimoto, “Alpaca: A Strong, Replicable Instruction-Following
    Model,” crfm.stanford.edu, 2023\. [https://crfm.stanford.edu/2023/03/13/alpaca.html](13.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#_ftnref9) E. J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language
    Models.,” Jun. 2021, [https://arxiv.org/abs/2106.09685](abs.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#_ftnref10) For the extra curious, Parameter-Efficient Fine-Tuning (PEFT)
    is a class of methods aimed at fine-tuning models in a computational efficient
    way. The PEFT library seeks to put them all in one easy to access place and you
    can get started here: [https://huggingface.co/docs/peft](docs.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#_ftnref11) R. Henry and Y. J. Kim, “Accelerating Large Language Models
    via Low-Bit Quantization,” March 2023, [https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51226/](gtcspring23-s51226.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#_ftnref12) DeepSpeed is a library that optimizes many of the hard parts
    for large-scale deep learning models like LLMs and is particularly useful when
    training. Check out their MoE tutorial. [https://www.deepspeed.ai/tutorials/mixture-of-experts/](mixture-of-experts.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#_ftnref13) Learn more about Ray Clusters here: [https://docs.ray.io/en/releases-2.3.0/cluster/key-concepts.html#ray-cluster](cluster.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#_ftnref14) V. Korthikanti et al., “Reducing Activation Recomputation
    in Large Transformer Models,” May 2022, [https://arxiv.org/abs/2205.05198](abs.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[15]](#_ftnref15) A. Harlap et al., “PipeDream: Fast and Efficient Pipeline
    Parallel DNN Training,” Jun. 08, 2018\. [https://arxiv.org/abs/1806.03377](abs.html)'
  prefs: []
  type: TYPE_NORMAL
