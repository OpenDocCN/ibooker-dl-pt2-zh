- en: 'Chapter 3\. Adding nonlinearity: Beyond weighted sums'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章。添加非线性：超越加权和
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容*'
- en: What nonlinearity is and how nonlinearity in hidden layers of a neural network
    enhances the network’s capacity and leads to better prediction accuracies
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是非线性，神经网络隐藏层中的非线性如何增强网络的容量并导致更好的预测准确性
- en: What hyperparameters are and methods for tuning them
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数是什么，以及调整它们的方法
- en: Binary classification through nonlinearity at the output layer, introduced with
    the phishing-website-detection example
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在输出层引入非线性进行二分类，以钓鱼网站检测示例为例介绍
- en: Multiclass classification and how it differs from binary classification, introduced
    with the iris-flower example
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类以及它与二分类的区别，以鸢尾花示例介绍
- en: In this chapter, you’ll build on the groundwork laid in [chapter 2](kindle_split_013.html#ch02)
    to allow your neural networks to learn more complicated mappings, from features
    to labels. The primary enhancement we will introduce is *nonlinearity*—a mapping
    between input and output that isn’t a simple weighted sum of the input’s elements.
    Nonlinearity enhances the representational power of neural networks and, when
    used correctly, improves the prediction accuracy in many problems. We will illustrate
    this point by continuing to use the Boston-housing dataset. In addition, this
    chapter will introduce a deeper look at *over-* and *underfitting* to help you
    train models that not only perform well on the training data but also achieve
    good accuracy on data that the models haven’t seen during training, which is what
    ultimately counts in terms of models’ quality.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将在[第2章](kindle_split_013.html#ch02)中奠定的基础上，允许您的神经网络学习更复杂的映射，从特征到标签。我们将介绍的主要增强是*非线性*——一种输入和输出之间的映射，它不是输入元素的简单加权和。非线性增强了神经网络的表征能力，并且当正确使用时，在许多问题上提高了预测准确性。我们将继续使用波士顿房屋数据集来说明这一点。此外，本章还将更深入地研究*过拟合*和*欠拟合*，以帮助您训练模型，这些模型不仅在训练数据上表现良好，而且在模型训练过程中没有见过的数据上达到良好的准确性，这才是模型质量的最终标准。
- en: '3.1\. Nonlinearity: What it is and what it is good for'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 非线性：它是什么，它有什么用处
- en: Let’s pick up where we left off with the Boston-housing example from the last
    chapter. Using a single dense layer, you saw trained models leading to MSEs corresponding
    to misestimates of roughly US$5,000\. Can we do better? The answer is yes. To
    make a better model for the Boston-housing data, we add one more dense layer to
    it, as shown by the following code listing (from index.js of the Boston-housing
    example).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从上一章的波士顿房屋示例中继续进行。使用一个密集层，您看到训练模型导致的MSE对应于大约5000美元的误差估计。我们能做得更好吗？答案是肯定的。为了创建一个更好的波士顿房屋数据模型，我们为其添加了一个更多的密集层，如以下代码列表所示（来自波士顿房屋示例的index.js）。
- en: Listing 3.1\. Defining a two-layer neural network for the Boston-housing problem
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 定义波士顿房屋问题的两层神经网络
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1*** Specifies how the kernel values should be initialized; see [section
    3.1.2](#ch03lev2sec2) for a discussion of how this is chosen through hyperparameter
    optimization.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 指定了如何初始化内核值；参见[3.1.2节](#ch03lev2sec2)讨论通过超参数优化选择的方式。'
- en: '***2*** Adds a hidden layer'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 添加一个隐藏层'
- en: '***3*** Prints a text summary of the model’s topology'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 打印模型拓扑结构的文本摘要'
- en: To see this model in action, first run the `yarn && yarn watch` command as mentioned
    in [chapter 2](kindle_split_013.html#ch02). Once the web page is open, click the
    Train Neural Network Regressor (1 Hidden Layer) button in the UI in order to start
    the model’s training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此模型的运行情况，请首先运行`yarn && yarn watch`命令，如[第2章](kindle_split_013.html#ch02)中所述。一旦网页打开，请点击UI中的Train
    Neural Network Regressor (1 Hidden Layer)按钮，以开始模型的训练。
- en: The model is a two-layer network. The first layer is a dense layer with 50 units.
    It is also configured to have custom activation and a kernel initializer, which
    we will discuss in [section 3.1.2](#ch03lev2sec2). This layer is a *hidden* layer
    because its output is not directly seen from outside the model. The second layer
    is a dense layer with the default activation (the linear activation) and is structurally
    the same layer we used in the pure linear model from [chapter 2](kindle_split_013.html#ch02).
    This layer is an *output* layer because its output is the model’s final output
    and is what’s returned by the model’s `predict()` method. You may have noticed
    that the function name in the code refers to the model as a *multilayer perceptron*
    (MLP). This is an oft-used term that describes neural networks that 1) have a
    simple topology without loops (what’s referred to as *feedforward neural networks*)
    and 2) have at least one hidden layer. All the models you will see in this chapter
    meet this definition.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是一个双层网络。第一层是一个具有50个单元的稠密层。它也配置了自定义激活函数和内核初始化程序，我们将在[第3.1.2节](#ch03lev2sec2)讨论。这一层是一个*隐藏*层，因为其输出不是直接从模型外部看到的。第二层是一个具有默认激活函数（线性激活）的稠密层，结构上与我们在[第2章](kindle_split_013.html#ch02)使用的纯线性模型中使用的同一层一样。这一层是一个*输出*层，因为其输出是模型的最终输出，并且是模型的`predict()`方法返回的内容。您可能已经注意到代码中的函数名称将模型称为*多层感知器*（MLP）。这是一个经常使用的术语，用来描述神经网络，其1）拥有没有回路的简单拓扑结构（所谓*前馈神经网络*）和2）至少有一层隐藏层。本章中您将看到的所有模型都符合这一定义。
- en: 'The `model.summary()` call in [listing 3.1](#ch03ex01) is new. It is a diagnostic/reporting
    tool that prints the topology of TensorFlow.js models to the console (either in
    the browser’s developer tool or to the standard output in Node.js). Here’s what
    the two-layer model generated:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单3.1](#ch03ex01)中的`model.summary()`调用是新的。这是一个诊断/报告工具，将TensorFlow.js模型的拓扑结构打印到控制台（在浏览器的开发者工具中或在Node.js的标准输出中）。以下是双层模型生成的结果：'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The key information in the summary includes
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要中的关键信息包括：
- en: The names and types of the layers (first column).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层的名称和类型（第一列）。
- en: The output shape for each layer (second column). These shapes almost always
    contain a null dimension as the first (batch) dimension, representing undetermined
    and variable batch size.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层的输出形状（第二列）。这些形状几乎总是包含一个空维度作为第一（批处理）维度，代表着不确定和可变大小的批处理。
- en: 'The number of weight parameters for each layer (third column). This is a count
    of all the individual numbers that make up the layer’s weights. For layers with
    more than one weight, this is a sum across all the weights. For instance, the
    first dense layer in this example contains two weights: a kernel of shape `[12,
    50]` and a bias of shape `[50]`, leading to `12 * 50 + 50 = 650` parameters.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层的权重参数数量（第三列）。这是一个计算各层权重的所有个别数量的计数。对于具有多个权重的层，这是跨所有权重求和。例如，本例中的第一个稠密层包含两个权重：形状为`[12,
    50]`的内核和形状为`[50]`的偏置，导致`12 * 50 + 50 = 650`个参数。
- en: The total number of the model’s weight parameters (at the bottom of the summary),
    followed by a breakdown of how many of the parameters are trainable and how many
    are nontrainable. The models we’ve seen so far contain only trainable parameters,
    which belong to the model weights that are updated when `tf.Model.fit()` is called.
    We will discuss nontrainable weights when we talk about transfer learning and
    model fine-tuning in [chapter 5](kindle_split_016.html#ch05).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的总权重参数数量（摘要底部），以及参数中可训练和不可训练的数量。到目前为止，我们看到的模型仅包含可训练参数，这些参数属于模型权重，在调用`tf.Model.fit()`时更新。在[第5章](kindle_split_016.html#ch05)讨论迁移学习和模型微调时，我们将讨论不可训练权重。
- en: 'The `model.summary()` output of the purely linear model from [chapter 2](kindle_split_013.html#ch02)
    is as follows. Compared with the linear model, our two-layer model contains about
    54 times as many weight parameters. Most of the additional weights come from the
    added hidden layer:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[第2章](kindle_split_013.html#ch02)纯线性模型的`model.summary()`输出如下。与线性模型相比，我们的双层模型包含大约54倍的权重参数。大部分额外权重来自于添加的隐藏层：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Because the two-layer model contains more layers and weight parameters, its
    training and inference consumes more computation resources and time. Is this added
    cost worth the gain in accuracy? When we train this model for 200 epochs, we end
    up with final MSEs on the test set that fall into the range of 14–15 (variability
    due to randomness of initialization), as compared to a test-set loss of approximately
    25 from the linear model. Our new model ends up with a misestimate of US$3,700–$3,900
    versus the approximately $5,000 misestimates we saw with the purely linear attempts.
    This is a significant improvement.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为两层模型包含更多层和权重参数，其训练和推断消耗更多的计算资源和时间。增加的成本是否值得准确度的提高？当我们为这个模型训练200个epochs时，我们得到的最终MSE在测试集上落在14-15的范围内（由于初始化的随机性而产生的变异性），相比之下，线性模型的测试集损失约为25。我们的新模型最终的误差为美元3,700-3,900，而纯线性尝试的误差约为5,000美元。这是一个显著的改进。
- en: 3.1.1\. Building the intuition for nonlinearity in neural networks
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 建立神经网络非线性的直觉
- en: 'Why does the accuracy improve? The key is the model’s enhanced complexity,
    as [figure 3.1](#ch03fig01) shows. First, there is an additional layer of neurons,
    which is the hidden layer. Second, the hidden layer contains a nonlinear *activation
    function* (as specified by `activation: ''sigmoid''` in the code), which is represented
    by the square boxes in panel B of [figure 3.1](#ch03fig01). An activation function^([[1](#ch03fn1)])
    is an element-by-element transform. The sigmoid function is a “squashing” nonlinearity,
    in the sense that it “squashes” all real values from –infinity to +infinity into
    a much smaller range (0 to +1, in this case). Its mathematical equation and plot
    are shown in [figure 3.2](#ch03fig02). Let’s take the hidden dense layer as an
    example. Suppose the result of the matrix multiplication and addition with the
    bias is a 2D tensor consisting of the following array of random values:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么准确度会提高呢？关键在于模型的增强复杂性，正如[图3.1](#ch03fig01)所示。首先，有一个额外的神经元层，即隐藏层。其次，隐藏层包含一个非线性的*激活函数*（在代码中指定为`activation:
    ''sigmoid''`），在[图3.1](#ch03fig01)的面板B中用方框表示。激活函数^([[1](#ch03fn1)])是逐元素的转换。sigmoid函数是一种“压缩”非线性，它“压缩”了所有从负无穷到正无穷的实数值到一个更小的范围（在本例中是0到+1）。它的数学方程和图表如[图3.2](#ch03fig02)所示。让我们以隐藏的稠密层为例。假设矩阵乘法和加法的结果与偏差的结果是一个由以下随机值数组组成的2D张量：'
- en: ¹
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The term *activation function* originated from the study of biological neurons,
    which communicate with each other through *action potentials* (voltage spikes
    on their cell membranes). A typical biological neuron receives inputs from a number
    of upstream neurons via contact points called *synapses*. The upstream neurons
    fire action potentials at different rates, which leads to the release of neurotransmitters
    and opening or closing of ion channels at the synapses. This in turn leads to
    variation in the voltage on the recipient neuron’s membrane. This is not unlike
    the kind of weighted sum seen for a unit in the dense layer. Only when the potential
    exceeds a certain threshold will the recipient neuron actually produce action
    potentials (that is, be “activated”) and thereby affect the state of downstream
    neurons. In this sense, the activation function of a typical biological neuron
    is somewhat similar to the relu function ([figure 3.2](#ch03fig02), right panel),
    which consists of a “dead zone” below a certain threshold of the input and increases
    linearly with the input above the threshold (at least up to a certain saturation
    level, which is not captured by the relu function).
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*激活函数*这个术语来源于对生物神经元的研究，它们通过*动作电位*（细胞膜上的电压尖峰）相互通信。一个典型的生物神经元从多个上游神经元接收输入，通过称为*突触*的接触点。上游神经元以不同的速率发出动作电位，这导致神经递质的释放和突触上离子通道的开闭。这反过来导致了接收神经元膜上的电压变化。这与稠密层中的单位所见到的加权和有些相似。只有当电位超过一定的阈值时，接收神经元才会实际产生动作电位（即被“激活”），从而影响下游神经元的状态。在这个意义上，典型生物神经元的激活函数与relu函数（[图3.2](#ch03fig02)，右面板）有些相似，它在输入的某个阈值以下有一个“死区”，并且随着输入在阈值以上的增加而线性增加（至少到达某个饱和水平，这并不被relu函数所捕捉）。'
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Figure 3.1\. The linear-regression model (panel A) and two-layer neural network
    (panel B) created for the Boston-housing dataset. For the sake of clarity, we
    reduced the number of input features from 12 to 3 and the number of the hidden
    layer’s units from 50 to 5 in panel B. Each model has only a single output unit
    because the models solve a univariate (one-targetnumber) regression problem. Panel
    B illustrates the nonlinear (sigmoid) activation of the model’s hidden layer.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1。为波士顿住房数据集创建的线性回归模型（面板A）和两层神经网络（面板B）。为了清晰起见，在面板B中，我们将输入特征的数量从12个减少到3个，并将隐藏层的单元数量从50个减少到5个。每个模型只有一个输出单元，因为这些模型解决单变量（单目标数值）回归问题。面板B描绘了模型隐藏层的非线性（sigmoid）激活。
- en: '![](03fig01_alt.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig01_alt.jpg)'
- en: The final output of the dense layer is then obtained by calling the sigmoid
    (`S`) function on each of the 50 elements individually, giving
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过将sigmoid（`S`）函数应用于每个元素的50个元素中的每一个，得到密集层的最终输出，如下所示：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Why is this function called *nonlinear*? Intuitively, the plot of the activation
    function is not a straight line. For example, sigmoid is a curve ([figure 3.2](#ch03fig02),
    left panel), and relu is a concatenation of two line segments ([figure 3.2](#ch03fig02),
    right panel). Even though sigmoid and relu are nonlinear, one of their properties
    is that they are smooth and differentiable at every point, which makes it possible
    to perform backpropagation^([[2](#ch03fn2)]) through them. Without this property,
    it wouldn’t be possible to train a model with layers that contain this activation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个函数被称为*非线性*？直观地说，激活函数的图形不是一条直线。例如，sigmoid是一条曲线（图3.2，左侧面板），而relu是两条线段的拼接（图3.2，右侧面板）。尽管sigmoid和relu是非线性的，但它们的一个特性是它们在每个点上都是平滑且可微的，这使得可以通过它们进行反向传播^([[2](#ch03fn2)])。如果没有这个特性，就不可能训练包含这种激活函数的层的模型。
- en: ²
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [section 2.2.2](kindle_split_013.html#ch02lev2sec9) if you need a refresher
    on backpropagation.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果需要回顾反向传播，请参阅[第2.2.2节](kindle_split_013.html#ch02lev2sec9)。
- en: 'Figure 3.2\. Two frequently used nonlinear activation functions for deep neural
    networks. Left: the sigmoid function ``S(x) = 1 / (1 + e ^ -x)`. Right: the rectified
    linear unit (relu) function `relu(x) = {0:x < 0, x:x >= 0}``'
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2。用于深度神经网络的两个常用非线性激活函数。左：sigmoid函数 `S(x) = 1 / (1 + e ^ -x)`。右：修正线性单元（relu）函数
    `relu(x) = {0:x < 0, x:x >= 0}`
- en: '![](03fig02_alt.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig02_alt.jpg)'
- en: Apart from the sigmoid function, a few other types of differentiable nonlinear
    functions are used frequently in deep learning. These include relu and hyperbolic
    tangent (or tanh). We will describe them in detail when we encounter them in subsequent
    examples.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了sigmoid函数之外，在深度学习中还经常使用一些其他类型的可微非线性函数。其中包括relu和双曲正切函数（tanh）。在后续的例子中遇到它们时，我们将对它们进行详细描述。
- en: Nonlinearity and model capacity
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非线性和模型容量
- en: Why does nonlinearity improve the accuracy of our model? Nonlinear functions
    allow us to represent a more diverse family of input-output relations. Many relations
    in the real world are approximately linear, such as the download-time problem
    we saw in the last chapter. But many others are not. It is easy to conceive examples
    of nonlinear relations. Consider the relation between a person’s height and their
    age. Height varies roughly linearly with age only up to a certain point, where
    it bends and plateaus. As another totally reasonable scenario, house prices can
    vary in a negative fashion with the neighborhood crime rate only if the crime
    rate is within a certain range. A purely linear model, like the one we developed
    in the last chapter, cannot accurately model this type of relation, while sigmoid
    nonlinearity is much better suited to model this relation. Of course, the crime-rate-house-price
    relation is more like an inverted (decreasing) sigmoid function than the original,
    increasing one in the left panel of [figure 3.2](#ch03fig02). But our neural network
    has no issue modeling this relation because the sigmoid activation is preceded
    and followed by linear functions with tunable weights.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么非线性能够提高我们模型的准确性？非线性函数使我们能够表示更多样化的输入-输出关系。现实世界中的许多关系大致是线性的，比如我们在上一章中看到的下载时间问题。但是，还有许多其他关系不是线性的。很容易构想出非线性关系的例子。考虑一个人的身高与年龄之间的关系。身高仅在某一点之前大致与年龄线性变化，之后会弯曲并趋于稳定。另一个完全合理的情景是，房价可以与社区犯罪率呈负相关，但前提是犯罪率在某一范围内。一个纯线性模型，就像我们在上一章中开发的模型一样，无法准确地建模这种类型的关系，而sigmoid非线性则更适合于建模这种关系。当然，犯罪率-房价关系更像是一个倒置的（下降的）sigmoid函数，而不是左侧面板中原始的增长函数。但是我们的神经网络可以毫无问题地建模这种关系，因为sigmoid激活前后都是由可调节权重的线性函数。
- en: But by replacing the linear activation with a nonlinear one like sigmoid, do
    we lose the ability to learn any linear relations that might be present in the
    data? Luckily, the answer is no. This is because part of the sigmoid function
    (the part close to the center) is fairly close to being a straight line. Other
    frequently used nonlinear activations, such as tanh and relu, also contain linear
    or close-to-linear parts. If the relations between certain elements of the input
    and those of the output are approximately linear, it is entirely possible for
    a dense layer with a nonlinear activation to learn the proper weights and biases
    to utilize the near-linear parts of the activation function. Hence, adding nonlinear
    activation to a dense layer leads to a net gain in the breadth of input-output
    relations it can learn.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，通过将线性激活替换为非线性激活（比如sigmoid），我们会失去学习数据中可能存在的任何线性关系的能力吗？幸运的是，答案是否定的。这是因为sigmoid函数的一部分（靠近中心的部分）非常接近一条直线。其他经常使用的非线性激活函数，比如tanh和relu，也包含线性或接近线性的部分。如果输入的某些元素与输出的某些元素之间的关系大致是线性的，那么一个带有非线性激活函数的密集层完全可以学习到使用激活函数的接近线性部分的正确权重和偏差。因此，向密集层添加非线性激活会导致它能够学习的输入-输出关系的广度增加。
- en: Furthermore, nonlinear functions are different from linear ones in that cascading
    nonlinear functions lead to richer sets of nonlinear functions. Here, *cascading*
    refers to passing the output of one function as the input to another. Suppose
    there are two linear functions,
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，非线性函数与线性函数不同之处在于级联非线性函数会导致更丰富的非线性函数集合。这里，“级联”是指将一个函数的输出作为另一个函数的输入。假设有两个线性函数，
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: and
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Cascading the two functions amounts to defining a new function `h`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 级联两个函数等同于定义一个新函数`h`：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, `h` is still a linear function. It just has a different kernel
    (slope) and a different bias (intercept) from those of `f1` and `f2`. The slope
    is now `(k2 * k1)`, and the bias is now `(k2 * b1 + b2)`. Cascading any number
    of linear functions always results in a linear function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`h`仍然是一个线性函数。它的核（斜率）和偏差（截距）与`f1`和`f2`的不同。斜率现在是`(k2 * k1)`，偏差现在是`(k2 * b1
    + b2)`。级联任意数量的线性函数始终会产生一个线性函数。
- en: 'However, consider a frequently used nonlinear activation function: relu. In
    the bottom part of [figure 3.3](#ch03fig03), we illustrate what happens when you
    cascade two relu functions with linear scaling. By cascading two scaled relu functions,
    we get a function that doesn’t look like relu at all. It has a new shape (something
    of a downward slope flanked by two flat sections in this case). Further cascading
    the step function with other relu functions will give an even more diverse set
    of functions, such as a “window” function, a function consisting of multiple windows,
    functions with windows stacked on top of wider windows, and so on (not shown in
    [figure 3.3](#ch03fig03)). There is a remarkably rich range of function shapes
    that you can create by cascading nonlinearities such as relu (one of the most
    commonly used activation functions). But what does this have to do with neural
    networks? In essence, neural networks are cascaded functions. Each layer of a
    neural network can be viewed as a function, and the stacking of layers amounts
    to cascading these functions to form a more complex function that is the neural
    network itself. This should make it clear why including nonlinear activation functions
    increases the range of input-output relations the model is capable of learning.
    This also gives you an intuitive understanding behind the oft-used trick of “adding
    more layers to a deep neural network” and why it often (but not always!) leads
    to models that can fit the dataset better.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请考虑一个经常使用的非线性激活函数：relu。在[图3.3](#ch03fig03)的底部，我们说明了当您级联两个具有线性缩放的relu函数时会发生什么。通过级联两个缩放的relu函数，我们得到一个看起来根本不像relu的函数。它具有一个新形状（在这种情况下，是由两个平坦部分包围的向下倾斜的部分）。进一步级联阶跃函数与其他relu函数将得到一组更多样化的函数，例如“窗口”函数，由多个窗口组成的函数，窗口叠加在更宽的窗口上的函数等（未显示在[图3.3](#ch03fig03)中）。通过级联relu等非线性函数，您可以创建出非常丰富的一系列函数形状。但这与神经网络有什么关系呢？实质上，神经网络是级联函数。神经网络的每一层都可以看作是一个函数，而将这些层堆叠起来就相当于级联这些函数，形成更复杂的函数，即神经网络本身。这应该清楚地说明为什么包含非线性激活函数会增加模型能够学习的输入-输出关系范围。这也让你直观地理解了常用技巧“向深度神经网络添加更多层”以及为什么它通常（但并非总是！）会导致更能拟合数据集的模型。
- en: Figure 3.3\. Cascading linear functions (top) and nonlinear functions (bottom).
    Cascading linear functions always leads to linear functions, albeit with new slopes
    and intercepts. Cascading nonlinear functions (such as relu in this example) leads
    to nonlinear functions with novel shapes, such as the “downward step” function
    in this example. This exemplifies why nonlinear activations and the cascading
    of them in neural networks leads to enhanced representational power (that is,
    capacity).
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3。级联线性函数（顶部）和非线性函数（底部）。级联线性函数总是导致线性函数，尽管具有新的斜率和截距。级联非线性函数（例如relu在本例中）会导致具有新形状的非线性函数，例如本例中的“向下阶跃”函数。这说明了为什么在神经网络中使用非线性激活函数以及级联它们会导致增强的表示能力（即容量）。
- en: '![](03fig03_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig03_alt.jpg)'
- en: The range of input-output relations a machine-learning model is capable of learning
    is often referred to as the model’s *capacity*. From the prior discussion about
    nonlinearity, we can see that a neural network with hidden layers and nonlinear
    activation functions has a greater capacity compared to a linear regressor. This
    explains why our two-layer network achieves a superior test-set accuracy compared
    to the linear-regression model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型能够学习的输入-输出关系范围通常被称为模型的*容量*。从先前关于非线性的讨论中，我们可以看出，具有隐藏层和非线性激活函数的神经网络与线性回归器相比具有更大的容量。这就解释了为什么我们的两层网络在测试集准确度方面比线性回归模型表现出更好的效果。
- en: You might ask, since cascading nonlinear activation functions leads to greater
    capacity (as in the bottom part of [figure 3.3](#ch03fig03)), can we get a better
    model for the Boston-housing problem by adding more hidden layers to the neural
    network? The `multiLayerPerceptronRegressionModel2Hidden()` function in index.js,
    which is wired to the button titled Train Neural Network Regressor (2 Hidden Layers),
    does exactly that. See the following code excerpt (from index.js of the Boston-housing
    example).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，由于级联非线性激活函数会导致更大的容量（如[图3.3](#ch03fig03)的底部所示），我们是否可以通过向神经网络添加更多的隐藏层来获得更好的波士顿房价问题模型？`multiLayerPerceptronRegressionModel2Hidden()`函数位于index.js中，它连接到标题为训练神经网络回归器（2个隐藏层）的按钮。该函数确实执行了这样的操作。请参阅以下代码摘录（来自波士顿房价示例的index.js）。
- en: Listing 3.2\. Defining a three-layer neural network for the Boston-housing problem
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.2\. 为波士顿房屋问题定义一个三层神经网络
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1*** Adds the first hidden layer'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 添加第一个隐藏层'
- en: '***2*** Adds another hidden layer'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 添加另一个隐藏层'
- en: '***3*** Prints a text summary of the model’s topology'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 展示模型拓扑的文本摘要'
- en: 'In the `summary()` printout (not shown), you can see that the model contains
    three layers—that is, one more than the model in [listing 3.1](#ch03ex01). It
    also has a significantly larger number of parameters: 3,251 as compared to 701
    in the two-layer model. The extra 2,550 weight parameters are due to the inclusion
    of the second hidden layer, which consists of a kernel of shape `[50, 50]` and
    a bias of shape `[50]`.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在`summary()`打印输出中（未显示），你可以看到该模型包含三层——比 [列表 3.1](#ch03ex01) 中的模型多一层。它也具有显著更多的参数：3,251个，相比两层模型中的701个。额外的
    2,550 个权重参数是由于包括了第二个隐藏层造成的，它由形状为`[50, 50]`的内核和形状为`[50]`的偏差组成。
- en: 'Repeating the model training a number of times, we can get a sense of the range
    of the final test-set (that is, evaluation) MSE of the three-layer networks: roughly
    10.8–13.4\. This corresponds to a misestimate of $3,280–$3,660, which beats that
    of the two-layer network ($3,700–$3,900). So, we have again improved the prediction
    accuracy of our model by adding nonlinear hidden layers and thereby enhancing
    its capacity.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 重复训练模型多次，我们可以对三层网络最终测试集（即评估）MSE 的范围有所了解：大致为10.8–13.4。这相当于对$3,280–$3,660的误估，超过了两层网络的$3,700–$3,900。因此，我们通过添加非线性隐藏层再次提高了模型的预测准确性，增强了其容量。
- en: Avoiding the fallacy of stacking layers without nonlinearity
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 避免将层堆叠而没有非线性的谬误
- en: Another way to see the importance of the nonlinear activation for the improved
    Boston-housing model is to remove it from the model. [Listing 3.3](#ch03ex03)
    is the same as [listing 3.1](#ch03ex01), except that the line that specifies the
    sigmoid activation function is commented out. Removing the custom activation causes
    the layer to have the default linear activation. Other aspects of the model, including
    the number of layers and weight parameters, don’t change.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看到非线性激活对改进波士顿房屋模型的重要性的方式是将其从模型中移除。[列表 3.3](#ch03ex03) 与 [列表 3.1](#ch03ex01)
    相同，只是注释掉了指定 S 型激活函数的一行。移除自定义激活会导致该层具有默认的线性激活。模型的其他方面，包括层数和权重参数数量，都不会改变。
- en: Listing 3.3\. A two-layer neural network without nonlinear activation
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.3\. 没有非线性激活的两层神经网络
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1*** Disables the nonlinear activation function'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 禁用非线性激活函数'
- en: How does this change affect the model’s learning? As you can find out by clicking
    the Train Neural Network Regressor (1 Hidden Layer) button again in the UI, the
    MSE on the test goes up to about 25, as compared with the 14–15 range when the
    sigmoid activation was included. In other words, the two-layer model without the
    sigmoid activation performs about the same as the one-layer linear regressor!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种改变如何影响模型的学习？通过再次点击 UI 中的 Train Neural Network Regressor（1 Hidden Layer）按钮，你可以得知测试集上的
    MSE 上升到约 25，而当 S 型激活包含时大约为 14–15 的范围。换句话说，没有 S 型激活的两层模型表现与一层线性回归器大致相同！
- en: 'This confirms our reasoning about cascading linear functions. By removing the
    nonlinear activation from the first layer, we end up with a model that is a cascade
    of two linear functions. As we have demonstrated before, the result is another
    linear function without any increase in the model’s capacity. Thus, it is no surprise
    that we end up with about the same accuracy as the linear model. This brings up
    a common “gotcha” in building multilayer neural networks: *be sure to include
    nonlinear activations in the hidden layers*. Failing to do so results in wasted
    computation resources and time, with potential increases in numerical instability
    (observe the wigglier loss curves in panel B of [figure 3.4](#ch03fig04)). Later,
    we will see that this applies not only to dense but also to other layer types,
    such as convolutional layers.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了我们关于级联线性函数的推理。通过从第一层中移除非线性激活，我们最终得到了一个两个线性函数级联的模型。正如我们之前展示的，结果是另一个线性函数，而模型的容量没有增加。因此，我们最终的准确性与线性模型大致相同并不奇怪。这提出了构建多层神经网络的常见“陷阱”：*一定要在隐藏层中包括非线性激活*。没有这样做会导致计算资源和时间的浪费，并有潜在的增加数值不稳定性（观察
    [图 3.4](#ch03fig04) 的面板 B 中更加不稳定的损失曲线）。稍后，我们将看到这不仅适用于密集层，还适用于其他层类型，如卷积层。
- en: Figure 3.4\. Comparing the training results with (panel A) and without (panel
    B) the sigmoid activation. Notice that removing the sigmoid activation leads to
    higher final loss values on the training, validation, and evaluation sets (a level
    comparable to the purely linear model from before) and to less smooth loss curves.
    Note that the y-axis scales are different between the two plots.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.4\. 比较使用（面板 A）和不使用（面板 B）Sigmoid 激活的训练结果。请注意，去除 Sigmoid 激活会导致训练、验证和评估集上的最终损失值更高（与之前的纯线性模型相当）且损失曲线不够平滑。请注意，两个图之间的
    y 轴刻度是不同的。
- en: '![](03fig04_alt.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig04_alt.jpg)'
- en: Nonlinearity and model interpretability
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非线性和模型可解释性
- en: 'In [chapter 2](kindle_split_013.html#ch02), we showed that once a linear model
    was trained on the Boston-housing dataset, we could examine its weights and interpret
    its individual parameters in a reasonably meaningful way. For example, the weight
    that corresponds to the “average number of rooms per dwelling” feature had a positive
    value, and the weight that corresponds to the “crime rate” feature had a negative
    value. The signs of such weights reflect the expected positive or negative relation
    between house price and the respective features. Their magnitudes also hint at
    the relative importance assigned to the various features by the model. Given what
    you just learned in this chapter, a natural question is: with a nonlinear model
    containing one or more hidden layers, is it still possible to come up with an
    understandable and intuitive interpretation of its weight values?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](kindle_split_013.html#ch02)中，我们展示了一旦在波士顿房屋数据集上训练了一个线性模型，我们就可以检查其权重并以相当有意义的方式解释其各个参数。例如，与“每个住宅的平均房间数”特征相对应的权重具有正值，而与“犯罪率”特征相对应的权重具有负值。这些权重的符号反映了房价与相应特征之间的预期正相关或负相关关系。它们的大小也暗示了模型对各种特征的相对重要性。鉴于您刚刚在本章学到的内容，一个自然的问题是：使用一个或多个隐藏层的非线性模型，是否仍然可能提出可理解和直观的权重值解释？
- en: 'The API for accessing weight values is exactly the same between a nonlinear
    model and a linear model: you just use the `getWeights()` method on the model
    object or its constituent layer objects. Take the MLP in [listing 3.1](#ch03ex01),
    for example—you can insert the following line after the model training is done
    (right after the `model.fit()` call):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 访问权重值的 API 在非线性模型和线性模型之间完全相同：您只需在模型对象或其组成层对象上使用`getWeights()`方法。以[清单 3.1](#ch03ex01)中的
    MLP 为例——您可以在模型训练完成后（`model.fit()`调用之后）插入以下行：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This line prints the value of the kernel of the first layer (that is, the hidden
    layer). This is one of the four weight tensors in the model, the other three being
    the hidden layer’s bias and the output layer’s kernel and bias. One thing to notice
    about the printout is that it has a larger size than the kernel we saw when printing
    the kernel of the linear model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这行打印了第一层（即隐藏层）的核心值。这是模型中的四个权重张量之一，另外三个是隐藏层的偏置和输出层的核心和偏置。关于打印输出的一件事值得注意的是，它的大小比我们打印线性模型的核心时要大：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is because the hidden layer consists of 50 units, which leads to a weight
    size of `[18, 50]`. This kernel has 900 individual weight parameters, as compared
    to the `12 + 1 = 13` parameters in the linear model’s kernel. Can we assign a
    meaning to each of the individual weight parameters? In general, the answer is
    no. This is because there is no easily identifiable meaning to any of the 50 outputs
    from the hidden layer. These are the dimensions of high-dimensional space created
    so that the model can learn (automatically discover) nonlinear relations in it.
    The human mind is not very good at keeping track of nonlinear relations in such
    high-dimensional spaces. In general, it is very difficult to write down a few
    sentences in layman’s terms to describe what each of the hidden layer’s units
    does or to explain how it contributes to the final prediction of the deep neural
    network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为隐藏层由 50 个单元组成，导致权重大小为`[18, 50]`。与线性模型的核心中的`12 + 1 = 13`个参数相比，该核心有 900 个单独的权重参数。我们能赋予每个单独的权重参数一定的含义吗？一般来说，答案是否定的。这是因为从隐藏层的
    50 个输出中很难找到任何一个的明显含义。这些是高维空间的维度，使模型能够学习（自动发现）其中的非线性关系。人类大脑在跟踪这种高维空间中的非线性关系方面并不擅长。一般来说，很难用通俗易懂的几句话来描述隐藏层每个单元的作用，或者解释它如何对深度神经网络的最终预测做出贡献。
- en: Also, realize that the model here has only one hidden layer. The relations become
    even more obscure and harder to describe when there are multiple hidden layers
    stacked on top of each other (as is the case in the model defined in [listing
    3.2](#ch03ex02)). Even though there are research efforts to find better ways to
    interpret the meaning of deep neural networks’ hidden layers,^([[3](#ch03fn3)])
    and progress is being made for some classes of models,^([[4](#ch03fn4)]) it is
    fair to say that deep neural networks are harder to interpret compared to shallow
    neural networks and certain types of nonneural network machine-learning models
    (such as decision trees). By choosing a deep model over a shallow one, we are
    essentially trading some interpretability for greater model capacity.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的模型只有一个隐藏层。当有多个隐藏层堆叠在一起时（就像在[清单 3.2](#ch03ex02)中定义的模型中一样），关系变得更加模糊和更难描述。尽管有研究努力寻找解释深度神经网络隐藏层含义的更好方法，^([[3](#ch03fn3)])并且针对某些类别的模型正在取得进展，^([[4](#ch03fn4)])但可以说，深度神经网络比浅层神经网络和某些类型的非神经网络机器学习模型（如决策树）更难解释。通过选择深度模型而不是浅层模型，我们基本上是在为更大的模型容量交换一些可解释性。
- en: ³
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin, “Local Interpretable
    Model-Agnostic Explanations (LIME): An Introduction,” O’Reilly, 12 Aug. 2016,
    [http://mng.bz/j5vP](http://mng.bz/j5vP).'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Marco Tulio Ribeiro，Sameer Singh 和 Carlos Guestrin，“局部可解释的模型无关解释（LIME）：简介”，O’Reilly，2016
    年 8 月 12 日，[http://mng.bz/j5vP](http://mng.bz/j5vP)。
- en: ⁴
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chris Olah et al., “The Building Blocks of Interpretability,” Distill, 6 Mar.
    2018, [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/).
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Chris Olah 等，“可解释性的基本构建块”，Distill，2018 年 3 月 6 日，[https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)。
- en: 3.1.2\. Hyperparameters and hyperparameter optimization
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 超参数和超参数优化
- en: 'Our discussion of the hidden layers in [listings 3.1](#ch03ex01) and [3.2](#ch03ex02)
    has been focusing on the nonlinear activation (sigmoid). However, other configuration
    parameters for this layer are also important for ensuring a good training result
    from this model. These include the number of units (50) and the kernel’s `''leCunNormal''`
    initialization. The latter is a special way to generate the random numbers that
    go into the kernel’s initial value based on the size of the input. It is distinct
    from the default kernel initializer (`''glorotNormal''`), which uses the sizes
    of both the input and output. Natural questions to ask are: Why use this particular
    custom kernel initializer instead of the default one? Why use 50 units (instead
    of, say, 30)? These choices are made to ensure a best-possible or close-to-best-possible
    good model quality through trying out various combinations of parameters repeatedly.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[清单 3.1](#ch03ex01)和[3.2](#ch03ex02)中对隐藏层的讨论一直侧重于非线性激活（sigmoid）。然而，该层的其他配置参数对于确保模型的良好训练结果也很重要。这些包括单位数量（50）和内核的
    `'leCunNormal'` 初始化。后者是根据输入的大小生成进入内核初始值的随机数的特殊方式。它与默认的内核初始化器（`'glorotNormal'`）不同，后者使用输入和输出的大小。自然的问题是：为什么使用这个特定的自定义内核初始化器而不是默认的？为什么使用
    50 个单位（而不是，比如，30 个）？通过反复尝试各种参数组合，这些选择是为了确保通过尽可能多地尝试各种参数组合获得最佳或接近最佳的模型质量。
- en: Parameters such as number of units, kernel initializers, and activation are
    *hyperparameters* of the model. The name “hyperparameters” signifies the fact
    that these parameters are distinct from the model’s weight parameters, which are
    updated automatically through backpropagation during training (that is, `Model.fit()`
    calls). Once the hyperparameters have been selected for a model, they do not change
    during the training process. They often determine the number and size of the weight
    parameters (for instance, consider the `units` field for a dense layer), the initial
    values of the weight parameters (consider the `kernelInitializer` field), and
    how they are updated during training (consider the `optimizer` field passed to
    `Model.compile()`). Therefore, they are on a level higher than the weight parameters.
    Hence the name “hyperparameter.”
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 参数，如单位数量、内核初始化器和激活函数，是模型的*超参数*。名称“超参数”表明这些参数与模型的权重参数不同，后者在训练期间通过反向传播自动更新（即，`Model.fit()`
    调用）。一旦为模型选择了超参数，它们在训练过程中不会改变。它们通常确定权重参数的数量和大小（例如，考虑密集层的 `units` 字段）、权重参数的初始值（考虑
    `kernelInitializer` 字段）以及它们在训练期间如何更新（考虑传递给 `Model.compile()` 的 `optimizer` 字段）。因此，它们位于高于权重参数的层次上。因此得名“超参数”。
- en: Apart from the sizes of the layers and the type of weight initializers, there
    are many other types of hyperparameters for a model and its training, such as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了层的大小和权重初始化器的类型之外，模型及其训练还有许多其他类型的超参数，例如
- en: The number of dense layers in a model, like the ones in [listings 3.1](#ch03ex01)
    and [3.2](#ch03ex02)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型中的密集层数量，比如[listings 3.1](#ch03ex01)和[3.2](#ch03ex02)中的那些
- en: What type of initializer to use for the kernel of a dense layer
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于密集层核的初始化器的类型
- en: Whether to use any weight regularization (see [section 8.1](kindle_split_020.html#ch08lev1sec1))
    and, if so, the regularization factor
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否使用任何的权重正则化（参见[第8.1节](kindle_split_020.html#ch08lev1sec1)），如果是，则是正则化因子
- en: Whether to include any dropout layers (see [section 4.3.2](kindle_split_015.html#ch04lev2sec9),
    for example) and, if so, the dropout rate
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否包括任何的 dropout 层（例如，参见[第4.3.2节](kindle_split_015.html#ch04lev2sec9)），如果是，则是多少的
    dropout 率
- en: The type of optimizer used for training (such as `'sgd'` versus `'adam'`; see
    [info box 3.1](#ch03sb01))
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的优化器的类型（例如，`'sgd'`与`'adam'`之间的区别；参见[info box 3.1](#ch03sb01)）
- en: How many epochs to train the model for
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型的时期数是多少
- en: The learning rate of the optimizer
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器的学习率
- en: Whether the learning rate of the optimizer should be decreased gradually as
    training progresses and, if so, at what rate
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否应该随着训练的进行逐渐减小优化器的学习率，如果是，以什么速度
- en: The batch size for training
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的批次大小
- en: The last five examples listed are somewhat special in that they are not related
    to the architecture of the model per se; instead, they are configurations of the
    model’s training process. Nonetheless, they affect the outcome of the training
    and hence are treated as hyperparameters. For models consisting of more diverse
    types of layers (such as convolutional and recurrent layers, discussed in [chapters
    4](kindle_split_015.html#ch04), [5](kindle_split_016.html#ch05), and [9](kindle_split_021.html#ch09)),
    there are even more potentially tunable hyperparameters. Therefore, it is clear
    why even a simple deep-learning model may have dozens of tunable hyperparameters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列出的最后五个例子有些特殊，因为它们与模型本身的架构无关；相反，它们是模型训练过程的配置。然而，它们会影响训练的结果，因此被视为超参数。对于包含更多不同类型层的模型（例如，在[第4章](kindle_split_015.html#ch04)、[第5章](kindle_split_016.html#ch05)和[第9章](kindle_split_021.html#ch09)中讨论的卷积和循环层），还有更多可能可调整的超参数。因此，即使是一个简单的深度学习模型可能也有几十个可调整的超参数是很清楚的。
- en: The process of selecting good hyperparameter values is referred to as *hyperparameter
    optimization* or *hyperparameter tuning*. The goal of hyperparameter optimization
    is to find a set of parameters that leads to the lowest validation loss after
    training. Unfortunately, there is currently no definitive algorithm that can determine
    the best hyperparameters given a dataset and the machine-learning task involved.
    The difficulty lies in the fact that many of the hyperparameters are discrete,
    so the validation loss value is not differentiable with respect to them. For example,
    the number of units in a dense layer and the number of dense layers in a model
    are integers; the type of optimizer is a categorical parameter. Even for the hyperparameters
    that are continuous and against which the validation loss is differentiable (for
    example, regularization factors), it is usually too computationally expensive
    to keep track of the gradients with respect to those hyperparameters during training,
    so it is not really feasible to perform gradient descent in the space of such
    hyperparameters. Hyperparameter optimization remains an active area of research,
    one which deep-learning practitioners should pay attention to.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '选择良好的超参数值的过程称为*超参数优化*或*超参数调整*。超参数优化的目标是找到一组参数，使训练后验证损失最低。不幸的是，目前没有一种确定的算法可以确定给定数据集和涉及的机器学习任务的最佳超参数。困难在于许多超参数是离散的，因此验证损失值对它们不是可微的。例如，密集层中的单元数和模型中的密集层数是整数；优化器的类型是一个分类参数。即使对于那些是连续的超参数（例如，正则化因子），对它们进行训练期间的梯度跟踪通常也是计算上过于昂贵的，因此在这些超参数空间中执行梯度下降实际上并不可行。超参数优化仍然是一个活跃的研究领域，深度学习从业者应该注意。  '
- en: Given the lack of a standard, out-of-the-box methodology or tool for hyperparameter
    optimization, deep-learning practitioners often use the following three approaches.
    First, if the problem at hand is similar to a well-studied problem (say, any of
    the examples you can find in this book), you can start with applying a similar
    model on your problem and “inherit” the hyperparameters. Later, you can search
    in a relatively small hyperparameter space around that starting point.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于缺乏一种标准的、开箱即用的超参数优化方法或工具，深度学习从业者通常采用以下三种方法。首先，如果手头的问题类似于一个经过深入研究的问题（比如，你可以在本书中找到的任何示例），你可以开始应用类似的模型来解决你的问题，并“继承”超参数。稍后，你可以在以该起点为中心的相对较小的超参数空间中进行搜索。
- en: Second, practitioners with sufficient experience might have intuition and educated
    guesses about what may be reasonably good hyperparameters for a given problem.
    Even such subjective choices are almost never optimal—they form good starting
    points and can facilitate subsequent fine-tuning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，有足够经验的从业者可能对于给定问题的合理良好的超参数有直觉和教育性的猜测。即使是这样主观的选择几乎从来都不是最佳的——它们形成了良好的起点，并且可以促进后续的微调。
- en: Third, for cases in which there are only a small number of hyperparameters to
    optimize (for example, fewer than four), we can use grid search—that is, exhaustively
    iterating over a number of hyperparameter combinations, training a model to completion
    for each of them, recording the validation loss, and taking the hyperparameter
    combination that yields the lowest validation loss. For example, suppose the only
    two hyperparameters to tune are 1) the number of units in a dense layer and 2)
    the learning rate; you might select a set of units (`{10, 20, 50, 100, 200}`)
    and a set of learning rates (`{1e-5, 1e-4, 1e-3, 1e-2}`) and perform a cross of
    the two sets, which leads to a total of `5 * 4 = 20` hyperparameter combinations
    to search over. If you were to implement the grid search yourself, the pseudo-code
    might look something like the following listing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，对于只有少量需要优化的超参数的情况（例如少于四个），我们可以使用格点搜索——即，穷举地迭代一些超参数组合，对每一个组合训练一个模型至完成，记录验证损失，并取得验证损失最低的超参数组合。例如，假设唯一需要调整的两个超参数是1）密集层中的单元数和2）学习率；你可以选择一组单元（`{10,
    20, 50, 100, 200}`）和一组学习率（`{1e-5, 1e-4, 1e-3, 1e-2}`），并对两组进行交叉，从而得到一共`5 * 4 =
    20`个要搜索的超参数组合。如果你要自己实现格点搜索，伪代码可能看起来像以下清单。
- en: Listing 3.4\. Pseudo-code for a simple hyperparameter grid search
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单3.4\. 用于简单超参数格点搜索的伪代码
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How are the ranges of these hyperparameters selected? Well, there is another
    place deep learning cannot provide a formal answer. These ranges are usually based
    on the experience and intuition of the deep-learning practitioner. They may also
    be constrained by computation resources. For example, a dense layer with too many
    units may cause the model to be too slow to train or to run during inference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数的范围是如何选择的？嗯，深度学习无法提供正式答案的另一个地方。这些范围通常基于深度学习从业者的经验和直觉。它们也可能受到计算资源的限制。例如，一个单位过多的密集层可能导致模型训练过程太慢或推断时运行太慢。
- en: Oftentimes, there are a larger number of hyperparameters to optimize over, to
    the extent that it becomes computationally too expensive to search over the exponentially
    increasing number of hyperparameter combinations. In such cases, you should use
    more sophisticated methods than grid search, such as random search^([[5](#ch03fn5)])
    and Bayesian^([[6](#ch03fn6)]) methods.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，需要优化的超参数数量较多，以至于在指数增长的超参数组合数量上进行搜索变得计算上过于昂贵。在这种情况下，应该使用比格点搜索更复杂的方法，如随机搜索^([[5](#ch03fn5)])和贝叶斯^([[6](#ch03fn6)])方法。
- en: ⁵
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: James Bergstra and Yoshua Bengio, “Random Search for Hyper-Parameter Optimization,”
    *Journal of Machine Learning Research*, vol. 13, 2012, pp. 281–305, [http://mng.bz/WOg1](http://mng.bz/WOg1).
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: James Bergstra和Yoshua Bengio，“超参数优化的随机搜索”，*机器学习研究杂志*，2012年，第13卷，第281–305页，[http://mng.bz/WOg1](http://mng.bz/WOg1)。
- en: ⁶
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Will Koehrsen, “A Conceptual Explanation of Bayesian Hyperparameter Optimization
    for Machine Learning, *Towards Data Science*, 24 June 2018, [http://mng.bz/8zQw](http://mng.bz/8zQw).
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Will Koehrsen，“贝叶斯超参数优化的概念解释”，*Towards Data Science*，2018年6月24日，[http://mng.bz/8zQw](http://mng.bz/8zQw)。
- en: '3.2\. Nonlinearity at output: Models for classification'
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 输出的非线性：用于分类的模型
- en: The two examples we’ve seen so far have both been regression tasks in which
    we try to predict a numeric value (such as the download time or the average house
    price). However, another common task in machine learning is classification. Some
    classification tasks are *binary classification*, wherein the target is the answer
    to a yes/no question. The tech world is full of this type of problem, including
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止看到的两个例子都是回归任务，我们试图预测一个数值（如下载时间或平均房价）。然而，机器学习中另一个常见的任务是分类。一些分类任务是*二元分类*，其中目标是对一个是/否问题的答案。技术世界充满了这种类型的问题，包括
- en: Whether a given email is or isn’t spam
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否给定的电子邮件是垃圾邮件
- en: Whether a given credit-card transaction is legitimate or fraudulent
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否给定的信用卡交易是合法的还是欺诈的
- en: Whether a given one-second-long audio sample contains a specific spoken word
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否给定的一秒钟音频样本包含特定的口语单词
- en: Whether two fingerprint images match each other (come from the same person’s
    same finger)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个指纹图像是否匹配（来自同一个人的同一个手指）
- en: 'Another type of classification problem is a *multiclass-classification* task,
    for which examples also abound:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分类问题是*多类别分类*任务，对此类任务也有很多例子：
- en: Whether a news article is about sports, weather, gaming, politics, or other
    general topics
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一篇新闻文章是关于体育、天气、游戏、政治还是其他一般话题
- en: Whether a picture is a cat, dog, shovel, and so on
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一幅图片是猫、狗、铲子等等
- en: Given stroke data from an electronic stylus, determining what a handwritten
    character is
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定电子笔的笔触数据，确定手写字符是什么
- en: In the scenario of using machine learning to play a simple Atari-like video
    game, determining in which of the four possible directions (up, down, left, and
    right) the game character should go next, given the current state of the game
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用机器学习玩一个类似Atari的简单视频游戏的场景中，确定游戏角色应该向四个可能的方向之一（上、下、左、右）前进，给定游戏的当前状态
- en: 3.2.1\. What is binary classification?
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 什么是二元分类？
- en: We’ll start with a simple case of binary classification. Given some data, we
    want a yes/no decision. For our motivating example, we’ll talk about the Phishing
    Website dataset.^([[7](#ch03fn7)]) The task is, given a collection of features
    about a web page and its URL, predicting whether the web page is used for *phishing*
    (masquerading as another site with the aim to steal users’ sensitive information).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的二元分类案例开始。给定一些数据，我们想要一个是/否的决定。对于我们的激励示例，我们将谈论钓鱼网站数据集。任务是，给定关于网页和其URL的一组特征，预测该网页是否用于*钓鱼*（伪装成另一个站点，目的是窃取用户的敏感信息）。
- en: ⁷
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Rami M. Mohammad, Fadi Thabtah, and Lee McCluskey, “Phishing Websites Features,”
    [http://mng.bz/E1KO](http://mng.bz/E1KO).
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Rami M. Mohammad, Fadi Thabtah, 和 Lee McCluskey，“Phishing Websites Features,”
    [http://mng.bz/E1KO](http://mng.bz/E1KO)。
- en: 'The dataset contains 30 features, all of which are binary (represented as the
    values –1 and 1) or ternary (represented as –1, 0, and 1). Rather than listing
    all the individual features like we did for the Boston-housing dataset, here we
    present a few representative features:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含30个特征，所有特征都是二元的（表示值为-1和1）或三元的（表示为-1、0和1）。与我们为波士顿房屋数据集列出所有单个特征不同，这里我们提供一些代表性的特征：
- en: '`HAVING_IP_ADDRESS`—Whether an IP address is used as an alternative to a domain
    name (binary value: `{-1, 1}`)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HAVING_IP_ADDRESS`—是否使用IP地址作为域名的替代（二进制值：`{-1, 1}`）'
- en: '`SHORTENING_SERVICE`—Whether it is using a URL shortening service or not (binary
    value: `{1, -1}`)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SHORTENING_SERVICE`—是否使用URL缩短服务（二进制值：`{1, -1}`）'
- en: '`SSLFINAL_STATE`—Whether 1) the URL uses HTTPS and the issuer is trusted, 2)
    it uses HTTPS but the issuer is not trusted, or 3) no HTTPS is used (ternary value:
    `{-1, 0, 1}`)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SSLFINAL_STATE`—URL是否使用HTTPS并且发行者是受信任的，它是否使用HTTPS但发行者不受信任，或者没有使用HTTPS（三元值：`{-1,
    0, 1}`）'
- en: The dataset consists of approximately 5,500 training examples and an equal number
    of test examples. In the training set, approximately 45% of the examples are positive
    (truly phishing web pages). The percentage of positive examples is about the same
    in the test set.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由大约5500个训练示例和相同数量的测试示例组成。在训练集中，大约有45%的示例是正面的（真正的钓鱼网页）。在测试集中，正面示例的百分比大约是相同的。
- en: This is just about the easiest type of dataset to work with—the features in
    the data are already in a consistent range, so there is no need to normalize their
    means and standard deviations as we did for the Boston-housing dataset. Additionally,
    we have a large number of training examples relative to both the number of features
    and the number of possible predictions (two—yes or no). Taken as a whole, this
    is a good sanity check that it’s a dataset we can work with. If we wanted to spend
    more time investigating our data, we might do pairwise feature-correlation checks
    to know if we have redundant information; however, this is something our model
    can tolerate.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是最容易处理的数据集类型——数据中的特征已经在一致的范围内，因此无需对其均值和标准偏差进行归一化，就像我们为波士顿房屋数据集所做的那样。此外，相对于特征数量和可能预测数量（两个——是或否），我们有大量的训练示例。总的来说，这是一个很好的健全性检查，表明这是一个我们可以处理的数据集。如果我们想要花更多时间研究我们的数据，我们可能会进行成对特征相关性检查，以了解是否有冗余信息；但是，这是我们的模型可以容忍的。
- en: 'Since our data looks similar to what we used (post-normalization) for Boston-housing,
    our starting model is based on the same structure. The example code for this problem
    is available in the website-phishing folder of the tfjs-examples repo. You can
    check out and run the example as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据与我们用于波士顿房屋（后归一化）的数据相似，我们的起始模型基于相同的结构。此问题的示例代码可在tfjs-examples存储库的website-phishing文件夹中找到。您可以按照以下方式查看和运行示例：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Listing 3.5\. Defining a binary-classification model for phishing detection
    (from index.js)
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5. 为钓鱼检测定义二分类模型（来自index.js）
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This model has a lot of similarities to the multilayer network we built for
    the Boston-housing problem. It starts with two hidden layers, and both of them
    use the sigmoid activation. The last (output) has exactly 1 unit, which means
    the model outputs a single number for each input example. However, a key difference
    here is that the last layer of our model for phishing detection has a sigmoid
    activation instead of the default linear activation as in the model for Boston-housing.
    This means that our model is constrained to output numbers between only 0 and
    1, which is unlike the Boston-housing model, which might output any float number.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型与我们为波士顿房屋问题构建的多层网络有很多相似之处。它以两个隐藏层开始，两者都使用sigmoid激活。最后（输出）有确切的1个单元，这意味着模型为每个输入示例输出一个数字。然而，这里的一个关键区别是，我们用于钓鱼检测的模型的最后一层具有sigmoid激活，而不是波士顿房屋模型中的默认线性激活。这意味着我们的模型受限于只能输出介于0和1之间的数字，这与波士顿房屋模型不同，后者可能输出任何浮点数。
- en: 'Previously, we have seen sigmoid activations for hidden layers help increase
    model capacity. But why do we use sigmoid activation at the output of this new
    model? This has to do with the binary-classification nature of the problem we
    have at hand. For binary classification, we generally want the model to produce
    a guess of the probability for the positive class—that is, how likely it is that
    the model “thinks” a given example belongs to the positive class. As you may recall
    from high school math, a probability is always a number between 0 and 1\. By having
    the model always output an estimated probability value, we get two benefits:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们已经看到sigmoid激活对隐藏层有助于增加模型容量。但是为什么在这个新模型的输出处使用sigmoid激活？这与我们手头问题的二分类特性有关。对于二分类，我们通常希望模型产生正类别的概率猜测——也就是说，模型“认为”给定示例属于正类别的可能性有多大。您可能还记得高中数学中的知识，概率始终是介于0和1之间的数字。通过让模型始终输出估计的概率值，我们获得了两个好处：
- en: It captures the degree of support for the assigned classification. A sigmoid
    value of 0.5 indicates complete uncertainty, wherein either classification is
    equally supported. A value of 0.6 indicates that while the system predicts the
    positive classification, it’s only weakly supported. A value of 0.99 means the
    model is quite certain that the example belongs to the positive class, and so
    forth. Hence, we make it easy and straightforward to convert the model’s output
    into a final answer (for instance, just threshold the output at a given value,
    say 0.5). Now imagine how hard it would be to find such a threshold if the range
    of the model’s output may vary widely.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它捕获了对分配的分类的支持程度。`sigmoid`值为`0.5`表示完全不确定性，其中每个分类都得到了同等的支持。值为`0.6`表示虽然系统预测了正分类，但支持程度很低。值为`0.99`表示模型非常确定该示例属于正类，依此类推。因此，我们使得将模型的输出转换为最终答案变得简单而直观（例如，只需在给定值处对输出进行阈值处理，例如`0.5`）。现在想象一下，如果模型的输出范围可能变化很大，那么找到这样的阈值将会有多难。
- en: We also make it easier to come up with a differentiable loss function, which,
    given the model’s output and the true binary target labels, produces a number
    that is a measure of how much the model missed the mark. For the latter point,
    we will elaborate more when we examine the actual binary cross entropy used by
    this model.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还使得更容易构造一个可微的损失函数，它根据模型的输出和真实的二进制目标标签产生一个衡量模型错失程度的数字。至于后者，当我们检查该模型使用的实际二元交叉熵时，我们将会更详细地阐述。
- en: However, the question is how to force the output of the neural network into
    the range of `[0, 1]`. The last layer of a neural network, which is often a dense
    layer, performs matrix multiplication (`matMul`) and bias addition (`biasAdd`)
    operations with its input. There are no intrinsic constraints in either the `matMul`
    or the `biasAdd` operation that guarantee a `[0, 1]` range in the result. Adding
    a squashing nonlinearity like sigmoid to the result of `matMul` and `biasAdd`
    is a natural way to achieve the `[0, 1]` range.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，问题是如何将神经网络的输出强制限制在`[0, 1]`范围内。神经网络的最后一层通常是一个密集层，它对其输入执行矩阵乘法(`matMul`)和偏置加法(`biasAdd`)操作。在`matMul`或`biasAdd`操作中都没有固有的约束，以保证结果在`[0,
    1]`范围内。将`sigmoid`等压缩非线性添加到`matMul`和`biasAdd`的结果中是实现`[0, 1]`范围的一种自然方法。
- en: 'Another aspect of the code in [listing 3.5](#ch03ex05) that’s new to you is
    the type of optimizer: `''adam''`, which is different from the `''sgd''` optimizer
    used in previous examples. How is `adam` different from `sgd`? As you may recall
    from [section 2.2.2](kindle_split_013.html#ch02lev2sec9) in the last chapter,
    the `sgd` optimizer always multiplies the gradients obtained through backpropagation
    with a fixed number (its learning rate times –1) in order to calculate the updates
    to the model’s weights. This approach has some drawbacks, including slow convergence
    toward the loss minimum when a small learning rate is chosen and “zigzag” paths
    in the weight space when the shape of the loss (hyper)surface has certain special
    properties. The `adam` optimizer aims at addressing these shortcomings of `sgd`
    by using a multiplication factor that varies with the history of the gradients
    (from earlier training iterations) in a smart way. Moreover, it uses different
    multiplication factors for different model weight parameters. As a result, `adam`
    usually leads to better convergence and less dependence on the choice of learning
    rate compared to `sgd` over a range of deep-learning model types; hence it is
    a popular choice of optimizer. The TensorFlow.js library provides a number of
    other optimizer types, some of which are also popular (such as `rmsprop`). The
    table in [info box 3.1](#ch03sb01) gives a brief overview of them.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 3.5](#ch03ex05) 中代码的另一个新方面是优化器的类型：`''adam''`，它与之前示例中使用的`''sgd''`优化器不同。`adam`与`sgd`有何不同？正如你可能还记得上一章[第
    2.2.2 节](kindle_split_013.html#ch02lev2sec9)所述，`sgd`优化器总是将通过反向传播获得的梯度乘以一个固定数字（学习率乘以-1）以计算模型权重的更新。这种方法有一些缺点，包括当选择较小的学习率时，收敛速度较慢，并且当损失（超）表面的形状具有某些特殊属性时，在权重空间中出现“之”形路径。`adam`优化器旨在通过以一种智能方式使用梯度的历史（来自先前的训练迭代）的乘法因子来解决这些`sgd`的缺点。此外，它对不同的模型权重参数使用不同的乘法因子。因此，与一系列深度学习模型类型相比，`adam`通常导致更好的收敛性和对学习率选择的依赖性较小；因此，它是优化器的流行选择。TensorFlow.js库提供了许多其他优化器类型，其中一些也很受欢迎（如`rmsprop`）。[信息框
    3.1](#ch03sb01) 中的表格提供了它们的简要概述。'
- en: '|  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Optimizers supported by TensorFlow.js**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow.js 支持的优化器**'
- en: The following table summarizes the APIs of the most frequently used types of
    optimizers in TensorFlow.js, along with a simple, intuitive explanation for each
    of them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了 TensorFlow.js 中最常用类型的优化器的 API，以及对每个优化器的简单直观解释。
- en: '**Commonly used optimizers and their APIs in TensorFlow.js**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow.js 中常用的优化器及其 API**'
- en: '| Name | API (string) | API (function) | Description |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | API（字符串） | API（函数） | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Stochastic gradient descent (SGD) | ''sgd'' | tf.train.sgd | The simplest
    optimizer, always using the learning rate as the multiplier for gradients |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 随机梯度下降（SGD） | ''sgd'' | tf.train.sgd | 最简单的优化器，始终使用学习率作为梯度的乘子 |'
- en: '| Momentum | ''momentum'' | tf.train.momentum | Accumulates past gradients
    in a way such that the update to a weight parameter gets faster when past gradients
    for the parameter line up more in the same direction and gets slower when they
    change a lot in direction |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Momentum | ''momentum'' | tf.train.momentum | 以一种方式累积过去的梯度，使得对于某个权重参数的更新在过去的梯度更多地朝着同一方向时变得更快，并且当它们在方向上发生大变化时变得更慢
    |'
- en: '| RMSProp | ''rmsprop'' | tf.train.rmsprop | Scales the multiplication factor
    differently for different weight parameters of the model by keeping track of a
    recent history of each weight gradient’s root-meansquare (RMS) value; hence its
    name |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| RMSProp | ''rmsprop'' | tf.train.rmsprop | 通过跟踪模型不同权重参数的最近梯度的均方根（RMS）值的历史记录，为不同的权重参数设置不同的乘法因子；因此得名
    |'
- en: '| AdaDelta | ''adadelta'' | tf.train.adadelta | Scales the learning rate for
    each individual weight parameter in a way similar to RMSProp |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| AdaDelta | ''adadelta'' | tf.train.adadelta | 类似于 RMSProp，以一种类似的方式为每个单独的权重参数调整学习率
    |'
- en: '| ADAM | ''adam'' | tf.train.adam | Can be understood as a combination of the
    adaptive learning rate approach of AdaDelta and the momentum method |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| ADAM | ''adam'' | tf.train.adam | 可以理解为 AdaDelta 的自适应学习率方法和动量方法的结合 |'
- en: '| AdaMax | ''adamax'' | tf.train.adamax | Similar to ADAM, but keeps track
    of the magnitudes of gradients using a slightly different algorithm |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| AdaMax | ''adamax'' | tf.train.adamax | 类似于 ADAM，但使用稍微不同的算法跟踪梯度的幅度 |'
- en: An obvious question is which optimizer you should use given the machine-learning
    problem and model you are working on. Unfortunately, there is no consensus in
    the field of deep learning yet (which is why TensorFlow.js provides all the optimizers
    listed in the previous table!). In practice, you should start with the popular
    ones, including `adam` and `rmsprop`. Given sufficient time and computation resources,
    you can also treat the optimizer as a hyperparameter and find the choice that
    gives you the best training result through hyperparameter tuning (see [section
    3.1.2](#ch03lev2sec2)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个明显的问题是，针对你正在处理的机器学习问题和模型，应该使用哪种优化器。不幸的是，在深度学习领域尚无共识（这就是为什么 TensorFlow.js 提供了上表中列出的所有优化器！）。在实践中，你应该从流行的优化器开始，包括
    `adam` 和 `rmsprop`。在有足够的时间和计算资源的情况下，你还可以将优化器视为超参数，并通过超参数调整找到为你提供最佳训练结果的选择（参见 [section
    3.1.2](#ch03lev2sec2)）。
- en: '|  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '3.2.2\. Measuring the quality of binary classifiers: Precision, recall, a-
    accuracy, and ROC curves'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 衡量二元分类器的质量：准确率、召回率、准确度和 ROC 曲线
- en: In a binary-classification problem, we emit one of two values—0/1, yes/no, and
    so on. In a more abstract sense, we’ll talk about the positives and negatives.
    When our network makes a guess, it is either right or wrong, so we have four possible
    scenarios for the actual label of the input example and the output of the network,
    as [table 3.1](#ch03table01) shows.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类问题中，我们发出两个值之一——0/1、是/否等等。在更抽象的意义上，我们将讨论正例和负例。当我们的网络进行猜测时，它要么正确要么错误，所以我们有四种可能的情况，即输入示例的实际标签和网络输出，如[table
    3.1](#ch03table01)所示。
- en: Table 3.1\. The four types of classification results in a binary classification
    problem
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.1\. 二元分类问题中的四种分类结果类型
- en: '|   |   | Prediction |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 预测 |'
- en: '| --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|   |   | Positive | Negative |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 正类 | 负类 |'
- en: '|   | Positive | True positive (TP) | False negative (FN) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|   | 正类 | 真正例（TP） | 假反例（FN） |'
- en: '|   | Negative | False positive (FP) | True negative (TN) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|   | 负类 | 假正例（FP） | 真负例（TN） |'
- en: The true positives (TPs) and true negatives (TNs) are where the model predicted
    the correct answer; the false positives (FPs) and false negatives (FNs) are where
    the model got it wrong. If we fill in the four cells with counts, we get a *confusion
    matrix*; [table 3.2](#ch03table02) shows a hypothetical one for our phishing-detection
    problem.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的正例（TP）和真正的负例（TN）是模型预测出正确答案的地方；假正例（FP）和假负例（FN）是模型出错的地方。如果我们用计数填充这四个单元格，我们就得到了一个*混淆矩阵*；[表
    3.2](#ch03table02)显示了我们钓鱼检测问题的一个假设性混淆矩阵。
- en: Table 3.2\. The confusion matrix from a hypothetical binary classification problem
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.2\. 一个假设的二元分类问题的混淆矩阵
- en: '|   |   | Prediction |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 预测 |'
- en: '| --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|   |   | Positive | Negative |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 正例 | 负例 |'
- en: '|   | Positive | 4 | 2 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|   | 正例 | 4 | 2 |'
- en: '|   | Negative | 1 | 93 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|   | 负例 | 1 | 93 |'
- en: In our hypothetical results from our phishing examples, we see that we correctly
    identified four phishing web pages, missed two, and had one false alarm. Let’s
    now look at the different common metrics for expressing this performance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们假设的钓鱼示例结果中，我们看到我们正确识别了四个钓鱼网页，漏掉了两个，而且有一个误报。现在让我们来看看用于表达这种性能的不同常见指标。
- en: '*Accuracy* is the simplest metric. It quantifies what percentage of the examples
    are classified correctly:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*准确率*是最简单的度量标准。它量化了多少百分比的示例被正确分类：'
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In our particular example,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们特定的例子中，
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Accuracy is an easy-to-communicate and easy-to-understand concept. However,
    it can be misleading—often in a binary-classification task, we don’t have equal
    distributions of positive and negative examples. We’re often in a situation where
    there are considerably fewer positive examples than there are negative ones (for
    example, most links aren’t phishing, most parts aren’t defective, and so on).
    If only 5 in 100 links are phishing, our network could always predict false and
    get 95% accuracy! Put that way, accuracy seems like a very bad measure for our
    system. High accuracy always sounds good but is often misleading. It’s a good
    thing to monitor but would be a very bad thing to use as a loss function.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是一个易于沟通和易于理解的概念。然而，它可能会具有误导性——在二元分类任务中，我们通常没有相等分布的正负例。我们通常处于这样的情况：正例要远远少于负例（例如，大多数链接不是钓鱼网站，大多数零件不是有缺陷的，等等）。如果
    100 个链接中只有 5 个是钓鱼的，我们的网络可以总是预测为假，并获得 95% 的准确率！这样看来，准确率似乎是我们系统的一个非常糟糕的度量。高准确率听起来总是很好，但通常会误导人。监视准确率是件好事，但作为损失函数使用则是一件非常糟糕的事情。
- en: The next pair of metrics attempts to capture the subtlety missing in accuracy—*precision*
    and *recall*. In the discussion that follows, we’re also typically thinking about
    problems in which a positive implies further action is required—a link is highlighted,
    a post is flagged for manual review—while a negative indicates the status quo.
    These metrics focus on the different types of “wrong” that our prediction could
    be.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下一对指标试图捕捉准确率中缺失的微妙之处——*精确率*和*召回率*。在接下来的讨论中，我们通常考虑的是一个正例意味着需要进一步的行动——一个链接被标记，一篇帖子被标记为需要手动审查——而负例表示现状不变。这些指标专注于我们的预测可能出现的不同类型的“错误”。
- en: '*Precision* is the ratio of positive predictions made by the model that are
    actually positive:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确率*是模型预测的正例中实际为正例的比率：'
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: With our numbers from the confusion matrix, we’d calculate
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们混淆矩阵的数字，我们将计算
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Like accuracy, it is usually possible to game precision. You can make your model
    very conservative in emitting positive predictions, for example, by labeling only
    the input examples with very high sigmoid output (say >0.95, instead of the default
    >0.5) as positive. This will usually cause the precision to go up, but doing so
    will likely cause the model to miss many actual positive examples (labeling them
    as negative). The last cost is captured by the metric that often goes with and
    complements precision, namely recall.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与准确率类似，通常可以操纵精确率。例如，您可以通过仅将具有非常高 S 型输出（例如 >0.95，而不是默认的 >0.5）的输入示例标记为正例，从而使您的模型非常保守地发出正面预测。这通常会导致精确率提高，但这样做可能会导致模型错过许多实际的正例（将它们标记为负例）。这最后一个成本被常与精确率配合使用并补充的度量所捕获，即召回率。
- en: '*Recall* is the ratio of actual positive examples that are classified by the
    model as positive:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*是模型将实际正例分类为正例的比率：'
- en: '[PRE19]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With the example data, we get a result of
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 根据示例数据，我们得到了一个结果
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Of all the positives in the sample set, how many did the model find? It will
    normally be a conscious decision to accept a higher false alarm rate to lower
    the chance of missing something. To game this metric, you’d simply declare all
    examples as positives; because false positives don’t enter into the equation,
    you can score 100% recall at the cost of decreased precision.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本集中所有阳性样本中，模型发现了多少个？通常会有一个有意识的决定，即接受较高的误报率以降低遗漏的可能性。为了优化这一指标，你可以简单地声明所有样本为阳性；由于假阳性不进入计算，因此你可以在降低精确度的代价下获得100%的召回率。
- en: As we can see, it’s fairly easy to craft a system that scores very well on accuracy,
    recall, or precision. In real-world binary-classification problems, it’s often
    difficult to get both good precision and recall at the same time. (If it were
    easy to do so, you’d have a simple problem and probably wouldn’t need to use machine
    learning in the first place.) Precision and recall are about tuning the model
    in the tricky places where there is a fundamental uncertainty about what the correct
    answer should be. You’ll see more nuanced and combined metrics, such as *Precision
    at X% Recall*, X being something like 90%—what is the precision if we’re tuned
    to find at least X% of the positives? For example, in [figure 3.5](#ch03fig05),
    we see that after 400 epochs of training, our phishing-detection model is able
    to achieve a precision of 96.8% and a recall of 92.9% when the model’s probability
    output is thresholded at 0.5.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，制作一个在准确度、召回率或精确度上表现出色的系统相当容易。在现实世界中的二元分类问题中，同时获得良好的精确度和召回率通常很困难。（如果这样做很容易，你就会面临一个简单的问题，可能根本不需要使用机器学习。）精确度和召回率涉及在对正确答案存在根本不确定的复杂区域调整模型。你会看到更多细致和组合的指标，如*在X%召回率下的精确度*，其中X通常为90%——如果我们调整到至少发现X%的阳性样本，精确度是多少？例如，在[图
    3.5](#ch03fig05)中，我们看到经过400个轮次的训练后，当模型的概率输出门槛设为0.5时，我们的钓鱼检测模型能够达到96.8%的精确度和92.9%的召回率。
- en: 'Figure 3.5\. An example result from a round of training the model for phishing
    web page detection. Pay attention to the various metrics at the bottom: precision,
    recall, and FPR. The area under the curve (AUC) is discussed in [section 3.2.3](#ch03lev2sec5).'
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.5。训练模型用于钓鱼网页检测的一轮结果示例。注意底部的各种指标：精确度、召回率和 FPR。曲线下面积（AUC）在[3.2.3节](#ch03lev2sec5)中讨论。
- en: '![](03fig05_alt.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig05_alt.jpg)'
- en: As we have briefly alluded to, an important realization is that the threshold
    applied on the sigmoid output to pick out positive predictions doesn’t have to
    be exactly 0.5\. In fact, depending on the circumstances, it might be better to
    set it to a value above 0.5 (but below 1) or to one below 0.5 (but above 0). Lowering
    the threshold makes the model more liberal in labeling inputs as positive, which
    leads to higher recall but likely lower precision. On the other hand, raising
    the threshold causes the model to be more cautious in labeling inputs as positive,
    which usually leads to higher precision but likely lower recall. So, we can see
    that there is a trade-off between precision and recall, and this trade-off is
    hard to quantify with any one of the metrics we’ve talked about so far. Luckily,
    the rich history of research into binary classification has given us better ways
    to quantify and visualize this trade-off relation. The ROC curve that we will
    discuss next is a frequently used tool of this sort.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们已略有提及的，一个重要的认识是，对正预测的选择，不需要在 sigmoid 输出上设置恰好为0.5的门槛。事实上，根据情况，它可能最好设定为0.5以上（但小于1）或0.5以下（但大于0）。降低门槛使模型在将输入标记为阳性时更加自由，这会导致更高的召回率但可能降低精确度。另一方面，提高门槛使模型在将输入标记为阳性时更加谨慎，通常会导致更高的精确度但可能降低召回率。因此，我们可以看到精确度和召回率之间存在权衡，这种权衡很难用我们迄今讨论过的任何一种指标来量化。幸运的是，二元分类研究的丰富历史为我们提供了更好的方式来量化和可视化这种权衡关系。我们接下来将讨论的
    ROC 曲线是这种常用的工具之一。
- en: '3.2.3\. The ROC curve: Showing trade-offs in binary classification'
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3。ROC 曲线：展示二元分类中的权衡
- en: ROC curves are used in a wide range of engineering problems that involve binary
    classification or the detection of certain types of events. The full name, *receiver
    operating characteristic*, is a term from the early age of radar. Nowadays, you’ll
    almost never see the expanded name. [Figure 3.6](#ch03fig06) is a sample ROC curve
    for our application.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线被用于广泛的工程问题，其中包括二分类或特定类型事件的检测。全名“接收者操作特性”是一个来自雷达早期的术语。现在，你几乎看不到这个扩展名了。[图3.6](#ch03fig06)是我们应用程序的一个样本ROC曲线。
- en: Figure 3.6\. A set of sample ROCs plotted during the training of the phishing-detection
    model. Each curve is for a different epoch number. The curves show gradual improvement
    in the quality of the binary-classification model as the training progresses.
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6. 在钓鱼检测模型训练期间绘制的一组样本ROC曲线。每条曲线对应不同的周期数。这些曲线显示了二分类模型随着训练的进展而逐渐改进的质量。
- en: '![](03fig06_alt.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig06_alt.jpg)'
- en: As you may have noticed in the axis labels in [figure 3.6](#ch03fig06), ROC
    curves are not exactly made by plotting the precision and recall metrics against
    each other. Instead, they are based on two slightly different metrics. The horizontal
    axis of an ROC curve is a *false positive rate* (FPR), defined as
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经在[图3.6](#ch03fig06)的坐标轴标签中注意到的，ROC曲线并不是通过将精确度和召回率指标相互绘制得到的。相反，它们是基于两个稍微不同的指标。ROC曲线的横轴是*假阳性率*（FPR），定义为
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The vertical axis of an ROC curve is the *true positive rate* (TPR), defined
    as
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线的纵轴是*真阳性率*（TPR），定义为
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: TPR has exactly the same definition as recall; it is just a different name for
    the same metric. However, FPR is something new. Its denominator is a count of
    all the cases in which the actual class of the example is negative; its numerator
    is a count of all false positive cases. In other words, FPR is the ratio of actually
    negative examples that are erroneously classified as positive, which is the probability
    of something commonly referred to as a *false alarm*. [Table 3.3](#ch03table03)
    summarizes the most common metrics you will encounter in a binary-classification
    problem.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: TPR与召回率具有完全相同的定义，只是使用了不同的名称。然而，FPR是一些新的东西。分母是实际类别为负的案例数量；分子是所有误报的数量。换句话说，FPR是将实际上是负的案例错误分类为正的比例，这是一个常常被称为*虚警（false
    alarm）*的概率。[表3.3](#ch03table03)总结了在二分类问题中遇到的最常见的指标。
- en: Table 3.3\. Commonly seen metrics for a binary-classification problem
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.3. 二分类问题中常见的指标
- en: '| Name of metric | Definition | How it is used in ROCs or precision/recall
    curves |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 定义 | ROC曲线或精确度/召回率曲线中的使用方式 |'
- en: '| --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Accuracy | (#TP + #TN) / (#TP + #TN + # FP + #FN) | (Not used by ROCs) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 准确度（Accuracy） | (#TP + #TN) / (#TP + #TN + # FP + #FN) |（ROC曲线中不使用） |'
- en: '| Precision | #TP / (#TP + #FP) | The vertical axis of a precision/recall curve
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 精确度（Precision） | #TP / (#TP + #FP) | 精确度/召回率曲线的纵轴 |'
- en: '| Recall/sensitivity/true positive rate (TPR) | #TP / (#TP + #FN) | The vertical
    axis of an ROC curve (as in [figure 3.6](#ch03fig06)), or the horizontal axis
    of a precision/recall curve |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 召回率/灵敏度/真阳性率（TPR） | #TP / (#TP + #FN) | ROC曲线的纵轴（如[图3.6](#ch03fig06)）或精确度/召回率曲线的横轴
    |'
- en: '| False positive rate (FPR) | #FP / (#FP + #TN) | The horizontal axis of an
    ROC curve (see [figure 3.6](#ch03fig06)) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 假阳性率（False positive rate，FPR） | #FP / (#FP + #TN) | ROC曲线的横轴（见[图3.6](#ch03fig06)）
    |'
- en: '| Area under the curve (AUC) | Calculated through numerical integration under
    the ROC curve; see [listing 3.7](#ch03ex07) for an example | (Not used by ROCs
    but is instead calculated from ROCs) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 曲线下面积（Area under the curve，AUC） | 将ROC曲线的数值积分计算得出；查看[代码示例3.7](#ch03ex07)以获取示例
    |（ROC曲线不使用，而是从ROC曲线计算得到） |'
- en: The seven ROC curves in [figure 3.6](#ch03fig06) are made at the beginning of
    seven different training epochs, from the first epoch (epoch 001) to the last
    (epoch 400). Each one of them is created based on the model’s predictions on the
    test data (not the training data). [Listing 3.6](#ch03ex06) shows the details
    of how this is done with the `onEpochBegin` callback of the `Model.fit()` API.
    This approach allows you to perform interesting analysis and visualization on
    the model in the midst of a training call without needing to write a `for` loop
    or use multiple `Model.fit()` calls.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.6](#ch03fig06) 中的七条 ROC 曲线分别绘制于七个不同的训练周期的开头，从第一个周期 (周期 001) 到最后一个周期 (周期
    400)。每条曲线都是基于模型在测试数据上的预测结果（而不是训练数据）创建的。[代码清单 3.6](#ch03ex06) 显示了如何利用 `Model.fit()`
    API 中的 `onEpochBegin` 回调函数详细实现此过程。这种方法使您可以在训练过程中执行有趣的分析和可视化，而不需要编写 `for` 循环或使用多个
    `Model.fit()` 调用。'
- en: Listing 3.6\. Using callback to render ROC curves in the middle of model training
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 3.6 使用回调函数在模型训练中间绘制 ROC 曲线
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1*** Draws ROC every few epochs'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 每隔几个周期绘制 ROC 曲线。'
- en: 'The body of the function `drawROC()` contains the details of how an ROC is
    made (see [listing 3.7](#ch03ex07)). It does the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `drawROC()` 的主体包含了如何创建 ROC 曲线的细节（参见[代码清单 3.7](#ch03ex07)）。它执行以下操作：
- en: Varies the threshold on the sigmoid output (probabilities) of the neural network
    to get different sets of classification results
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据神经网络的 S 型输出（概率）的阈值，可获取不同分类结果的集合。
- en: For each classification result, uses it in conjunction with the actual labels
    (targets) to calculate the TPR and FPR
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 TPR 绘制在 FPR 上以形成 ROC 曲线。
- en: Plots the TPRs against the FPRs to form the ROC curve
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ⁸
- en: As [figure 3.6](#ch03fig06) shows, in the beginning of the training (epoch 001),
    as the model’s weights are initialized randomly, the ROC curve is very close to
    a diagonal line connecting the point (0, 0) with the point (1, 1). This is what
    random guessing looks like. As the training progresses, the ROC curves are pushed
    up more and more toward the top-left corner—a place where the FPR is close to
    0, and the TPR is close to 1\. If we focus on any given level of FPR, such as
    0.1, we see a monotonic increase in the corresponding TPR value as we move further
    along in the training. In plain words, this means that as the training goes on,
    we can achieve a higher and higher level of recall (TPR) if we are pinned to a
    fixed level of false alarm (FPR).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 3.6](#ch03fig06) 所示，在训练开始时（周期 001），由于模型的权重是随机初始化的，ROC 曲线非常接近连接点 (0, 0)
    和点 (1, 1) 的对角线。这就是随机猜测的样子。随着训练的进行，ROC 曲线越来越向左上角推进——那里的 FPR 接近 0，TPR 接近 1。如果我们专注于任何一个给定的
    FPR 级别，例如 0.1，我们可以看到在训练过程中，相应的 TPR 值随着训练的进展而单调递增。简而言之，这意味着随着训练的进行，如果我们将假报警率（FPR）保持不变，就可以实现越来越高的召回率（TPR）。
- en: The “ideal” ROC is a curve bent so much toward the top-left corner that it becomes
    a γ^([[8](#ch03fn8)]) shape. In this scenario, you can get 100% TPR and 0% FPR,
    which is the “Holy Grail” for any binary classifier. However, with real problems,
    we can only improve the model to push the ROC curve ever closer to the top-left
    corner—the theoretical ideal at the top-left can never be achieved.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: “理想”的 ROC 曲线向左上角弯曲得越多，就会变成一个类似 γ^([[8](#ch03fn8)]) 形状的曲线。在这种情况下，您可以获得 100% 的
    TPR 和 0% 的 FPR，这是任何二元分类器的“圣杯”。然而，在实际问题中，我们只能改进模型，将 ROC 曲线推向左上角，但理论上的左上角理想状态是无法实现的。
- en: ⁸
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注释：γ 字母
- en: ''
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Greek letter gamma.
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于每个分类结果，将其与实际标签（目标）结合使用，计算 TPR 和 FPR。
- en: Based on this discussion of the shape of the ROC curve and its implications,
    we can see that it is possible to quantify how good an ROC curve is by looking
    at the area under it—that is, how much of the space in the unit square is enclosed
    by the ROC curve and the x-axis. This is called the *area under the curve* (AUC)
    and is computed by the code in [listing 3.7](#ch03ex07) as well. This metric is
    better than precision, recall, and accuracy in the sense that it takes into account
    the trade-off between false positives and false negatives. The ROC for random
    guessing (the diagonal line) has an AUC of 0.5, while the γ-shaped ideal ROC has
    an AUC of 1.0\. Our phishing-detection model reaches an AUC of 0.981 after training.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对ROC曲线形状及其含义的讨论，我们可以看到通过查看其下方的区域（即ROC曲线和x轴之间的单位正方形的空间）来量化ROC曲线的好坏是可能的。这被称为*曲线下面积*（AUC），并且也在[listing
    3.7](#ch03ex07)的代码中计算。这个指标比精确率、召回率和准确率更好，因为它考虑了假阳性和假阴性之间的权衡。随机猜测的ROC曲线（对角线）的AUC为0.5，而γ形状的理想ROC曲线的AUC为1.0。我们的钓鱼检测模型在训练后达到了0.981的AUC。
- en: Listing 3.7\. The code for calculating and rendering an ROC curve and the AUC
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[listing 3.7](#ch03ex07)的代码用于计算和绘制ROC曲线和AUC'
- en: '[PRE24]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1*** A manually selected set of probability thresholds'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 一组手动选择的概率阈值'
- en: '***2*** Converts the probability into predictions through thresholding'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 通过阈值将概率转换为预测'
- en: '***3*** falsePositiveRate() calculates the false positive rate by comparing
    the predictions and actual targets. It is defined in the same file.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** falsePositiveRate()函数通过比较预测和实际目标来计算假阳性率。该函数在同一文件中定义。'
- en: '***4*** Accumulates to area for AUC calculation'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 用于AUC计算的面积累积'
- en: Apart from visualizing the characteristics of a binary classifier, the ROC also
    helps us make sensible decisions about how to select the probability threshold
    in real-world situations. For example, imagine that we are a commercial company
    developing the phishing detector as a service. Do we want to do one of the following?
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可视化二元分类器的特性外，ROC还帮助我们在实际情况下做出明智的选择，比如如何选择概率阈值。例如，想象一下，我们是一家商业公司，正在开发钓鱼检测器作为一项服务。我们想要做以下哪项？
- en: Make the threshold relatively low because missing a real phishing website will
    cost us a lot in terms of liability or lost contracts.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于错过了真实的网络钓鱼网站将会在责任或失去合同方面给我们造成巨大的损失，因此将阈值设定相对较低。
- en: Make the threshold relatively high because we are more averse to the complaints
    filed by users whose normal websites are blocked because the model classifies
    them as phishy.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们更不愿意接受将正常网站误分类为可疑而导致用户提交投诉，因此将阈值设定相对较高。
- en: Each threshold value corresponds to a point on the ROC curve. When we increase
    the threshold gradually from 0 to 1, we move from the top-right corner of the
    plot (where FPR and TPR are both 1) to the bottom-left corner of the plot (where
    FPR and TPR are both 0). In real engineering problems, the decision of which point
    to pick on the ROC curve is always based on weighing opposing real-life costs
    of this sort, and it may vary for different clients and at different stages of
    business development.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阈值对应于ROC曲线上的一个点。当我们将阈值从0逐渐增加到1时，我们从图的右上角（其中FPR和TPR都为1）移动到图的左下角（其中FPR和TPR都为0）。在实际的工程问题中，选择ROC曲线上的哪个点的决定总是基于权衡这种相反的现实生活成本，并且在不同的客户和不同的业务发展阶段可能会有所不同。
- en: Apart from the ROC curve, another commonly used visualization of binary classification
    is the *precision-recall curve* (sometimes called a P/R curve, mentioned briefly
    in [table 3.3](#ch03table03)). Unlike the ROC curve, a precision-recall plots
    precision against recall. Since precision-recall curves are conceptually similar
    to ROC curves, we won’t delve into them here.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ROC曲线之外，二元分类的另一个常用可视化方法是*精确率-召回率曲线*（有时称为P/R曲线，在[table 3.3](#ch03table03)中简要提到）。与ROC曲线不同，精确率-召回率曲线将精确率绘制为召回率的函数。由于精确率-召回率曲线在概念上与ROC曲线相似，我们在这里不会深入讨论它们。
- en: One thing worth pointing out in [listing 3.7](#ch03ex07) is the use of `tf.tidy()`.
    This function ensures that the tensors created within the anonymous function passed
    to it as arguments are disposed of properly, so they won’t continue to occupy
    WebGL memory. In the browser, TensorFlow.js can’t manage the memory of tensors
    created by the user, primarily due to a lack of object finalization in JavaScript
    and a lack of garbage collection for the WebGL textures that underlie TensorFlow.js
    tensors. If such intermediate tensors are not cleaned up properly, a WebGL memory
    leak will happen. If such memory leaks are allowed to continue long enough, they
    will eventually result in WebGL out-of-memory errors. [Section 1.3](kindle_split_011.html#ch01lev1sec3)
    of [appendix B](kindle_split_030.html#app02) contains a detailed tutorial on memory
    management in TensorFlow.js. There are also exercises on this topic in section
    1.5 of [appendix B](kindle_split_030.html#app02). If you plan to define custom
    functions by composing TensorFlow.js functions, you should study these sections
    carefully.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [代码清单 3.7](#ch03ex07) 中值得指出的一点是使用了 `tf.tidy()`。这个函数确保了在作为参数传递给它的匿名函数内创建的张量被正确地处理，这样它们就不会继续占用
    WebGL 内存。在浏览器中，TensorFlow.js 无法管理用户创建的张量的内存，主要是因为 JavaScript 中缺乏对象终结和底层 TensorFlow.js
    张量下层的 WebGL 纹理缺乏垃圾回收。如果这样的中间张量没有被正确清理，就会发生 WebGL 内存泄漏。如果允许这样的内存泄漏持续足够长的时间，最终会导致
    WebGL 内存不足错误。[附录 B](kindle_split_030.html#app02) 的 [章节 1.3](kindle_split_011.html#ch01lev1sec3)
    包含了有关 TensorFlow.js 内存管理的详细教程。此外，[附录 B](kindle_split_030.html#app02) 的 [章节 1.5](kindle_split_030.html#app02)
    中还有关于这个主题的练习题。如果您计划通过组合 TensorFlow.js 函数来定义自定义函数，您应该仔细研究这些章节。
- en: '3.2.4\. Binary cross entropy: The loss function for binary classification'
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 二元交叉熵：二元分类的损失函数
- en: 'So far, we have talked about a few different metrics that quantify different
    aspects of how well a binary classifier is performing, such as accuracy, precision,
    and recall ([table 3.3](#ch03table03)). But we haven’t talked about an important
    metric, one that is differentiable and can generate gradients that support the
    model’s gradient-descent training. This is the `binaryCrossentropy` that we saw
    briefly in [listing 3.5](#ch03ex05) and haven’t explained yet:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了几种不同的度量标准，用于量化二元分类器的不同表现方面，比如准确率、精确率和召回率（[表 3.3](#ch03table03)）。但我们还没有讨论一个重要的度量标准，一个可以微分并生成梯度来支持模型梯度下降训练的度量标准。这就是我们在
    [代码清单 3.5](#ch03ex05) 中简要看到的 `binaryCrossentropy`，但我们还没有解释过：
- en: '[PRE25]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'First off, you might ask, why can’t we simply take accuracy, precision, recall,
    or perhaps even AUC and use it as the loss function? After all, these metrics
    are understandable. Also, in the regression problems we’ve seen previously, we
    used MSE, a fairly understandable metric, as the loss function for training directly.
    The answer is that none of these binary classification metrics can produce the
    gradients we need for training. Take the accuracy metric, for example: to see
    why it is not gradient-friendly, realize the fact that calculating accuracy requires
    determining which of the model’s predictions are positive and which are negative
    (see the first row in [table 3.3](#ch03table03)). In order to do that, it is necessary
    to apply a *thresholding function*, which converts the model’s sigmoid output
    into binary predictions. Here is the crux of the problem: although the thresholding
    function (or *step function* in more technical terms) is differentiable almost
    everywhere (“almost” because it is not differentiable at the “jumping point” at
    0.5), the derivative is always exactly zero (see [figure 3.7](#ch03fig07))! What
    happens if you try to do backpropagation through this thresholding function? Your
    gradients will end up being all zeros because at some point, upstream gradient
    values need to be multiplied with these all-zero derivatives from the step function.
    Put more simply, if accuracy (or precision, recall, AUC, and so on) is chosen
    as the loss, the flat sections of the underlying step function make it impossible
    for the training procedure to know where to move in the weight space to decrease
    the loss value.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可能会问，为什么不能直接以精确度、准确度、召回率，或者甚至 AUC 作为损失函数？毕竟这些指标容易理解。此外，在之前我们见过的回归问题中，我们使用了
    MSE 作为训练的损失函数，这是一个相当容易理解的指标。答案是，这些二分类度量指标都无法产生我们需要训练的梯度。以精确度指标为例：要了解为什么它不友好的梯度，请认识到计算精确度需要确定模型的预测哪些是正样本，哪些是负样本（参见
    [表3.3](#ch03table03) 的第一行）。为了做到这一点，必须应用一个 *阈值函数*，将模型的 sigmoid 输出转换为二进制预测。这里就是问题的关键：虽然阈值函数（在更技术的术语中称为*step
    function*）几乎在任何地方都是可微分的（“几乎”是因为它在0.5的“跳跃点”处不可微分），但其导数始终恰好为零（参见[图3.7](#ch03fig07)）！如果您试图通过该阈值函数进行反向传播会发生什么呢？因为上游梯度值在某些地方需要与该阈值函数的所有零导数相乘，所以您的梯度最终将全是零。更简单地说，如果将精确度（或准确度、召回率、AUC等）选为损失，底层阶跃函数的平坦部分使得训练过程无法知道在权重空间中向哪个方向移动可以降低损失值。
- en: Figure 3.7\. The step function used to convert the probability output of a binary-classification
    model is differentiable almost everywhere. Unfortunately, the gradient (derivative)
    at every differentiable point is exactly zero.
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7 用于转换二分类模型的概率输出的阶跃函数，几乎在每个可微点都是可微分的。不幸的是，每个可微分点的梯度（导数）恰好为零。
- en: '![](03fig07_alt.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig07_alt.jpg)'
- en: Therefore, using accuracy as the loss function doesn’t allow us to calculate
    useful gradients and hence prevents us from getting meaningful updates to the
    weights of the model. The same limitation applies to metrics including precision,
    recall, FPR, and AUC. While these metrics are useful for humans to understand
    the behavior of a binary classifier, they are useless for these models’ training
    process.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果使用精确度作为损失函数，便无法计算有用的梯度，从而阻止了在模型的权重上获得有意义的更新。此限制同样适用于包括准确度、召回率、FPR 和 AUC
    在内的度量。虽然这些指标对人类理解二分类器的行为很有用，但对于这些模型的训练过程来说是无用的。
- en: The loss function that we use for a binary classification task is *binary cross
    entropy*, which corresponds to the `'binaryCrossentropy'` configuration in our
    phishing-detection model code ([listings 3.5](#ch03ex05) and [3.6](#ch03ex06)).
    Algorithmically, we can define binary cross entropy with the following pseudo-code.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们针对二分类任务使用的损失函数是*二进制交叉熵*，它对应于我们的钓鱼检测模型代码中的 `'binaryCrossentropy'` 配置（见[列表3.5](#ch03ex05)和[3.6](#ch03ex06)）。算法上，我们可以用以下伪代码来定义二进制交叉熵。
- en: Listing 3.8\. The pseudo-code for the binary cross-entropy loss function^([[9](#ch03fn9)])
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8 二进制交叉熵损失函数的伪代码^([[9](#ch03fn9)])
- en: ⁹
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The actual code for `binaryCrossentropy` needs to guard against cases in which
    `prob` or `1 – prob` is exactly zero, which would lead to infinity if the value
    is passed directly to the `log` function. This is done by adding a very small
    positive number (such as `1e-6`, commonly referred to as “epsilon” or a “fudge
    factor”) to `prob` and `1 - prob` before passing them to the log function.
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`binaryCrossentropy` 的实际代码需要防范 `prob` 或 `1 - prob` 等恰好为零的情况，否则如果将这些值直接传递给 `log`
    函数，会导致无穷大。这是通过在将它们传递给对数函数之前添加一个非常小的正数（例如 `1e-6`，通常称为“epsilon”或“修正因子”）来实现的。'
- en: '[PRE26]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this pseudo-code, `truthLabel` is a number that takes the 0–1 values and
    indicates whether the input example has a negative (0) or positive (1) label in
    reality. `prob` is the probability of the example belonging to the positive class,
    as predicted by the model. Note that unlike `truthLabel`, `prob` is expected to
    be a real number that can take any value between 0 and 1\. `log` is the natural
    logarithm, with *e* (2.718) as the base, which you may recall from high school
    math. The body of the `binaryCrossentropy` function contains an if-else logical
    branching, which performs different calculations depending on whether `truthLabel`
    is 0 or 1\. [Figure 3.8](#ch03fig08) plots the two cases in the same plot.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在此伪代码中，`truthLabel` 是一个数字，取 0 到 1 的值，指示输入样本在现实中是否具有负（0）或正（1）标签。`prob` 是模型预测的样本属于正类的概率。请注意，与
    `truthLabel` 不同，`prob` 应为实数，可以取 0 到 1 之间的任何值。`log` 是自然对数，以 *e*（2.718）为底，您可能还记得它来自高中数学。`binaryCrossentropy`
    函数的主体包含一个 if-else 逻辑分支，根据 `truthLabel` 是 0 还是 1 执行不同的计算。[图 3.8](#ch03fig08) 在同一图中绘制了这两种情况。
- en: Figure 3.8\. The binary cross-entropy loss function. The two cases (`truthLabel
    = 1` and `truthLabel = 0`) are plotted separately, reflecting the if-else logical
    branching in [listing 3.8](#ch03ex08).
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.8。二元交叉熵损失函数。两种情况（`truthLabel = 1` 和 `truthLabel = 0`）分别绘制在一起，反映了 [代码清单 3.8](#ch03ex08)
    中的 if-else 逻辑分支。
- en: '![](03fig08_alt.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig08_alt.jpg)'
- en: 'When looking at the plots in [figure 3.8](#ch03fig08), remember that lower
    values are better because this is a loss function. The important things to note
    about the loss function are as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看 [图 3.8](#ch03fig08) 中的图表时，请记住较低的值更好，因为这是一个损失函数。关于损失函数需要注意的重要事项如下：
- en: 'If `truthLabel` is 1, a value of `prob` closer to 1.0 leads to a lower loss-function
    value. This makes sense because when the example is actually positive, we want
    the model to output a probability as close to 1.0 as possible. And vice versa:
    if the `truthLabel` is 0, the loss value is lower when the probability value is
    closer to 0\. This also makes sense because in that case, we want the model to
    output a probability as close to 0 as possible.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `truthLabel` 为 1，`prob` 值接近 1.0 会导致较低的损失函数值。这是有道理的，因为当样本实际上是正例时，我们希望模型输出的概率尽可能接近
    1.0。反之亦然：如果 `truthLabel` 为 0，则当概率值接近 0 时，损失值较低。这也是有道理的，因为在这种情况下，我们希望模型输出的概率尽可能接近
    0。
- en: Unlike the binary-thresholding function shown in [figure 3.7](#ch03fig07), these
    curves have nonzero slopes at every point, leading to nonzero gradients. This
    is why it is suitable for backpropagation-based model training.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 [图 3.7](#ch03fig07) 中显示的二进制阈值函数不同，这些曲线在每个点都有非零斜率，导致非零梯度。这就是为什么它适用于基于反向传播的模型训练。
- en: One question you might ask is, why not repeat what we did for the regression
    model—just pretend that the 0–1 values are regression targets and use MSE as the
    loss function? After all, MSE is differentiable, and calculating the MSE between
    the truth label and the probability would yield nonzero derivatives just like
    `binaryCrossentropy`. The answer has to do with the fact that MSE has “diminishing
    returns” at the boundaries. For example, in [table 3.4](#ch03table04), we list
    the `binaryCrossentropy` and MSE loss values for a number of `prob` values when
    `truthLabel` is 1\. As `prob` gets closer to 1 (the desired value), the MSE decreases
    more and more slowly compared to `binaryCrossentropy`. As a result, it is not
    as good at “encouraging” the model to produce a higher (closer to 1) `prob` value
    when `prob` is already fairly close to 1 (for instance, 0.9). Likewise, when `truthLabel`
    is 0, MSE is not as good as `binaryCrossentropy` in generating gradients that
    push the model’s `prob` output toward 0 either.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问的一个问题是，为什么不重复我们为回归模型所做的事情——只是假装0-1值是回归目标，并使用MSE作为损失函数？毕竟，MSE是可微分的，并且计算真实标签和概率之间的MSE会产生与`binaryCrossentropy`一样的非零导数。答案与MSE在边界处具有“递减收益”有关。例如，在
    [表3.4](#ch03table04) 中，我们列出了当 `truthLabel` 为1时一些 `prob` 值的 `binaryCrossentropy`
    和 MSE 损失值。当 `prob` 接近1（期望值）时，MSE相对于`binaryCrossentropy`的减小速度会越来越慢。因此，当 `prob`
    已经接近1（例如，0.9）时，它不太好地“鼓励”模型产生较高（接近1）的 `prob` 值。同样，当 `truthLabel` 为0时，MSE也不如 `binaryCrossentropy`
    那样好，不能生成推动模型的 `prob` 输出向0靠近的梯度。
- en: Table 3.4\. Comparing values of binary cross entropy and MSE for hypothetical
    binary classification results
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.4\. 比较假想的二分类结果的二元交叉熵和MSE值
- en: '| truthLabel | prob | Binary cross entropy | MSE |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 真实标签 | 概率 | 二元交叉熵 | MSE |'
- en: '| --- | --- | --- | --- |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 0.1 | 2.302 | 0.81 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.1 | 2.302 | 0.81 |'
- en: '| 1 | 0.5 | 0.693 | 0.25 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.5 | 0.693 | 0.25 |'
- en: '| 1 | 0.9 | 0.100 | 0.01 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.9 | 0.100 | 0.01 |'
- en: '| 1 | 0.99 | 0.010 | 0.0001 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.99 | 0.010 | 0.0001 |'
- en: '| 1 | 0.999 | 0.001 | 0.000001 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.999 | 0.001 | 0.000001 |'
- en: '| 1 | 1 | 0 | 0 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 0 |'
- en: 'This shows another aspect in which binary-classification problems are different
    from regression problems: for a binary-classification problem, the loss (`binaryCrossentropy`)
    and metrics (accuracy, precision, and so on) are different, while they are usually
    the same for a regression problem (for example, `meanSquaredError`). As we will
    see in the next section, multiclass-classification problems also involve different
    loss functions and metrics.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了二分类问题与回归问题不同的另一个方面：对于二分类问题，损失（`binaryCrossentropy`）和指标（准确率、精确率等）是不同的，而对于回归问题通常是相同的（例如，`meanSquaredError`）。正如我们将在下一节看到的那样，多类别分类问题也涉及不同的损失函数和指标。
- en: 3.3\. Multiclass classification
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 多类别分类
- en: 'In [section 3.2](#ch03lev1sec2), we explored how to structure a binary-classification
    problem; now we’ll do a quick aside into how to handle *nonbinary classification*—that
    is, classification tasks involving three or more classes.^([[10](#ch03fn10)])
    The dataset we will use to illustrate multiclass classification is the *iris-flower
    dataset*, a famous dataset with its origin in the field of statistics (see [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)).
    This dataset focuses on three species of the iris flower, called *iris setosa*,
    *iris versicolor*, and *iris* *virginica*. These three species can be distinguished
    from one another on the basis of their shapes and sizes. In the early 20th century,
    Ronald Fisher, a British statistician, measured the length and width of the petals
    and sepals (different parts of the flower) of 150 samples of iris. This dataset
    is balanced: there are exactly 50 samples for each target label.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第3.2节](#ch03lev1sec2) 中，我们探讨了如何构建二分类问题的结构；现在我们将快速进入 *非二分类* 的处理方式——即，涉及三个或更多类别的分类任务。^([[10](#ch03fn10)])
    我们将使用用于说明多类别分类的数据集是 *鸢尾花数据集*，这是一个有着统计学根源的著名数据集（参见 [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)）。这个数据集关注于三种鸢尾花的品种，分别为
    *山鸢尾*、*变色鸢尾* 和 *维吉尼亚鸢尾*。这三种鸢尾花可以根据它们的形状和大小来区分。在20世纪初，英国统计学家罗纳德·费舍尔测量了150个鸢尾花样本的花瓣和萼片（花的不同部位）的长度和宽度。这个数据集是平衡的：每个目标标签都有确切的50个样本。
- en: ^(10)
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-274
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is important not to confuse *multiclass* classification with *multilabel*
    classification. In multilabel classification, an individual input example may
    correspond to multiple output classes. An example is detecting the presence of
    various types of objects in an input image. One image may include only a person;
    another image may include a person, a car, and an animal. A multilabel classifier
    is required to generate an output that represents all the classes that are applicable
    to the input example, no matter whether there is one or more than one such class.
    This section is not concerned with multilabel classification. Instead, we focus
    on the simpler single-label, multiclass classification, in which every input example
    corresponds to exactly one output class among >2 possible classes.
  id: totrans-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不要混淆 *多类别* 分类和 *多标签* 分类。在多标签分类中，单个输入示例可能对应于多个输出类别。一个例子是检测输入图像中各种类型物体的存在。一个图像可能只包括一个人；另一个图像可能包括一个人、一辆车和一个动物。多标签分类器需要生成一个表示适用于输入示例的所有类别的输出，无论该类别是一个还是多个。本节不涉及多标签分类。相反，我们专注于更简单的单标签、多类别分类，其中每个输入示例都对应于>2个可能类别中的一个输出类别。
- en: 'In this problem, our model takes as input four numeric features—petal length,
    petal width, sepal length, and sepal width—and tries to predict a target label
    (one of the three species). The example is available in the iris folder of tfjs-examples,
    which you can check out and run with these commands:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们的模型以四个数值特征（花瓣长度、花瓣宽度、萼片长度和萼片宽度）作为输入，并尝试预测一个目标标签（三种物种之一）。该示例位于 tfjs-examples
    的 iris 文件夹中，您可以使用以下命令查看并运行：
- en: '[PRE27]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 3.3.1\. One-hot encoding of categorical data
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 对分类数据进行 one-hot 编码
- en: 'Before studying the model that solves the iris-classification problem, we need
    to highlight the way in which the categorical target (species) is represented
    in this multiclass-classification task. All the machine-learning examples we’ve
    seen in this book so far involve simpler representation of targets, such as the
    single number in the download-time prediction problem and that in the Boston-housing
    problem, as well as the 0–1 representation of binary targets in the phishing-detection
    problem. However, in the iris problem, the three species of flowers are represented
    in a slightly less familiar way called *one-hot encoding*. Open data.js, and you
    will notice this line:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究解决鸢尾花分类问题的模型之前，我们需要强调这个多类别分类任务中分类目标（物种）的表示方式。到目前为止，在本书中我们看到的所有机器学习示例都涉及更简单的目标表示，例如下载时间预测问题中的单个数字以及波士顿房屋问题中的数字，以及钓鱼检测问题中的二进制目标的
    0-1 表示。然而，在鸢尾问题中，三种花的物种以稍微不那么熟悉的方式称为 *one-hot 编码* 进行表示。打开 data.js，您将注意到这一行：
- en: '[PRE28]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, `shuffledTargets` is a plain JavaScript array consisting of the integer
    labels for the examples in a shuffled order. Its elements all have values 0, 1,
    and 2, reflecting the three iris species in the dataset. It is converted into
    a int32-type 1D tensor through the `tf.tensor1d(shuffledTargets).toInt()` call.
    The resultant 1D tensor is then passed into the `tf.oneHot()` function, which
    returns a 2D tensor of the shape `[numExamples, IRIS_NUM_CLASSES]`. `numExamples`
    is the number of examples that `targets` contains, and `IRIS_NUM_CLASSES` is simply
    the constant 3\. You can examine the actual values of `targets` and `ys` by adding
    some printing lines right below the previously cited line—that is, something like
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`shuffledTargets` 是一个普通的 JavaScript 数组，其中包含按随机顺序排列的示例的整数标签。其元素的值均为 0、1 和
    2，反映了数据集中的三种鸢尾花品种。通过调用 `tf.tensor1d(shuffledTargets).toInt()`，它被转换为 int32 类型的
    1D 张量。然后将结果的 1D 张量传递到 `tf.oneHot()` 函数中，该函数返回形状为 `[numExamples, IRIS_NUM_CLASSES]`
    的 2D 张量。`numExamples` 是 `targets` 包含的示例数，而 `IRIS_NUM_CLASSES` 简单地是常量 3。您可以通过在先前引用的行下面添加一些打印行来查看
    `targets` 和 `ys` 的实际值，例如：
- en: '[PRE29]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ^(11)
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlike `target`, `ys` is not a plain JavaScript array. Instead, it is a tensor
    object backed by GPU memory. Therefore, the regular console.log won’t show its
    value. The `print()` method is specifically for retrieving the values from the
    GPU, formatting them in a shape-aware and human-friendly way, and logging them
    to the console.
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与 `targets` 不同，`ys` 不是一个普通的 JavaScript 数组。相反，它是由 GPU 内存支持的张量对象。因此，常规的 console.log
    不会显示其值。`print()` 方法是专门用于从 GPU 中检索值，以形状感知和人性化的方式进行格式化，并将其记录到控制台的方法。
- en: 'Once you have made these changes, the parcel bundler process that has been
    started by the Yarn `watch` command in your terminal will automatically rebuild
    the web files. Then you can open the devtool in the browser tab being used to
    watch this demo and refresh the page. The printed messages from the `console.log()`
    and `print()` calls will be logged into the console of the devtool. The printed
    messages you will see will look something like this:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您进行了这些更改，Yarn `watch` 命令在终端启动的包捆绑器进程将自动重建 Web 文件。然后，您可以打开用于观看此演示的浏览器选项卡中的开发工具，并刷新页面。`console.log()`
    和 `print()` 调用的打印消息将记录在开发工具的控制台中。您将看到的打印消息将类似于这样：
- en: '[PRE30]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: or
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE31]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'and so forth. To describe this in words, for an example with the integer label
    0, you get a row of values `[1, 0, 0]`; for an example with integer label 1, you
    get a row of values `[0, 1, 0]`, and so forth. This is a simple and clear example
    of one-hot encoding: it turns an integer label into a vector consisting of all-zero
    values except at the index that corresponds to the label, where the value is 1\.
    The length of the vector equals the number of all possible categories. The fact
    that there is a single 1 value in the vector is precisely the reason why this
    encoding scheme is called “one-hot.”'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。用言语来描述，以整数标签 0 为例，您会得到一个值为 `[1, 0, 0]` 的值行；对于整数标签为 1 的示例，您会得到一个值为 `[0, 1,
    0]` 的行，依此类推。这是独热编码的一个简单明了的例子：它将一个整数标签转换为一个向量，该向量除了在对应标签的索引处的值为 1 之外，其余都为零。向量的长度等于所有可能类别的数量。向量中只有一个
    1 值的事实正是这种编码方案被称为“独热”的原因。
- en: This encoding may look unnecessarily complicated to you. Why use three numbers
    to represent a category when a single number could do the job? Why do we choose
    this over the simpler and more economical single-integer-index encoding? This
    can be understood from two different angles.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您来说，这种编码可能看起来过于复杂了。在一个类别中使用三个数字来表示，为什么不使用一个单一的数字就能完成任务呢？为什么我们选择这种复杂的编码而不是更简单和更经济的单整数索引编码呢？这可以从两个不同的角度来理解。
- en: 'First, it is much easier for a neural network to output a continuous, float-type
    value than an integer one. It is not elegant to apply rounding on float-type output,
    either. A much more elegant and natural approach is for the last layer of the
    neural network to output a few separate float-type numbers, each constrained to
    be in the `[0, 1]` interval through a carefully chosen activation function similar
    to the sigmoid activation function we used for binary classification. In this
    approach, each number is the model’s estimate of the probability of the input
    example belonging to the corresponding class. This is exactly what one-hot encoding
    is for: it is the “correct answer” for the probability scores, which the model
    should aim to fit through its training process.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于神经网络来说，输出连续的浮点型值要比整数值容易得多。在浮点型输出上应用舍入也不够优雅。一个更加优雅和自然的方法是，神经网络的最后一层输出几个单独的浮点型数值，每个数值通过一个类似于我们用于二元分类的
    S 型激活函数的精心选择的激活函数被限制在 `[0, 1]` 区间内。在这种方法中，每个数字都是模型对输入示例属于相应类别的概率的估计。这正是独热编码的用途：它是概率分数的“正确答案”，模型应该通过其训练过程来拟合。
- en: Second, by encoding a category as an integer, we implicitly create an ordering
    among the classes. For example, we may label *iris setosa* as 0, *iris versicolor*
    as 1, and *iris virginica* as 2\. But ordering schemes like this are often artificial
    and unjustified. For example, this numbering scheme implies that *setosa* is “closer”
    to *versicolor* than to *virginica*, which may not be true. Neural networks operate
    on real numbers and are based on mathematical operations such as multiplication
    and addition. Hence, they are sensitive to the magnitude of numbers and their
    ordering. If the categories are encoded as a single number, it becomes an extra,
    nonlinear relation that the neural network must learn. By contrast, one-hot-encoded
    categories don’t involve any implied ordering and hence don’t tax the learning
    capability of a neural network in this fashion.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，通过将类别编码为整数，我们隐含地为类别创建了一个顺序。例如，我们可以将 *鸢尾花 setosa* 标记为 0，*鸢尾花 versicolor* 标记为
    1，*鸢尾花 virginica* 标记为 2。但是，这样的编号方案通常是人为的和不合理的。例如，这种编号方案暗示 *setosa* 比 *versicolor*
    更“接近” *virginica*，这可能并不正确。神经网络基于实数进行操作，并且基于诸如乘法和加法之类的数学运算。因此，它们对数字的数量和顺序敏感。如果将类别编码为单一数字，则成为神经网络必须学习的额外非线性关系。相比之下，独热编码的类别不涉及任何隐含的排序，因此不会以这种方式限制神经网络的学习能力。
- en: As we will see in [chapter 9](kindle_split_021.html#ch09), one-hot encoding
    not only is used for output targets of neural networks but also is applicable
    when categorical data form the inputs to neural networks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们将在[第9章](kindle_split_021.html#ch09)中看到的那样，独热编码不仅用于神经网络的输出目标，而且还适用于分类数据形成神经网络的输入。
- en: 3.3.2\. Softmax activation
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. Softmax激活函数
- en: With an understanding of how the input features and output target are represented,
    we are now ready to look at the code that defines our model (from iris/index.js).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 了解了输入特征和输出目标的表示方式后，我们现在可以查看定义模型的代码（来自iris/index.js）。
- en: Listing 3.9\. The multilayer neural network for iris-flower classification
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.9\. 用于鸢尾花分类的多层神经网络
- en: '[PRE32]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The model defined in [listing 3.9](#ch03ex09) leads to the following summary:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 3.9](#ch03ex09)中定义的模型导致了以下摘要：
- en: '[PRE33]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As can be seen from the printed summary, this is a fairly simple model with
    a relatively small (83) number of weight parameters. The output shape `[null,
    3]` corresponds to the one-hot encoding of the categorical target. The activation
    used for the last layer, namely *softmax*, is designed specifically for the multiclass
    classification problem. The mathematical definition of softmax can be written
    as the following pseudo-code:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看打印的概述，我们可以看出这是一个相当简单的模型，具有相对较少的（83个）权重参数。输出形状`[null, 3]`对应于分类目标的独热编码。最后一层使用的激活函数，即*softmax*，专门设计用于多分类问题。softmax的数学定义可以写成以下伪代码：
- en: '[PRE34]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Unlike the sigmoid activation function we’ve seen, the softmax activation function
    is not element-by-element because each element of the input vector is transformed
    in a way that depends on all other elements. Specifically, each element of the
    input is converted to its natural exponential (the `exp` function, with *e* =
    2.718 as the base). Then the exponential is divided by the sum of all elements’
    exponentials. What does this do? First, it ensures that every number is in the
    interval between 0 and 1\. Second, it is guaranteed that all the elements of the
    output vector sum to 1\. This is a desirable property because 1) the outputs can
    be interpreted as probability scores assigned to the classes, and 2) in order
    to be compatible with the categorical cross-entropy loss function, the outputs
    must satisfy this property. Third, the definition ensures that a larger element
    in the input vector maps to a larger element in the output vector. To give a concrete
    example, suppose the matrix multiplication and bias addition in the last dense
    layer produces a vector of
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前见过的sigmoid激活函数不同，softmax激活函数不是逐元素的，因为输入向量的每个元素都以依赖于所有其他元素的方式进行转换。具体来说，输入的每个元素被转换为其指数（以*
    e*=2.718为底）的自然指数。然后指数被除以所有元素的指数的和。这样做有什么作用？首先，它确保了每个数字都在0到1的区间内。其次，保证了输出向量的所有元素之和为1。这是一个理想的属性，因为1）输出可以被解释为分配给各个类别的概率得分，2）为了与分类交叉熵损失函数兼容，输出必须满足此属性。第三，该定义确保输入向量中的较大元素映射到输出向量中的较大元素。举个具体的例子，假设最后一个密集层的矩阵乘法和偏置相加生成了一个向量
- en: '[PRE35]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Its length is 3 because the dense layer is configured to have 3 units. Note
    that the elements are float numbers unconstrained to any particular range. The
    softmax activation will convert the vector into
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 它的长度为3，因为密集层被配置为具有3个单元。请注意，这些元素是浮点数，不受特定范围的约束。softmax激活函数将向量转换为
- en: '[PRE36]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can verify this yourself by running the following TensorFlow.js code (for
    example, in the devtool console when the page is pointing at [js.tensorflow.org](http://js.tensorflow.org)):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下TensorFlow.js代码（例如，在页面指向[js.tensorflow.org](http://js.tensorflow.org)时，在开发工具控制台中）来自行验证这一点：
- en: '[PRE37]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The three elements of the softmax function’s output 1) are all in the `[0, 1]`
    interval, 2) sum to 1, and 3) are ordered in a way that matches the ordering in
    the input vector. As a result of these properties, the output can be interpreted
    as the probability values assigned (by the model) to all the possible classes.
    In the previous code snippet, the second category is assigned the highest probability
    while the first is assigned the lowest.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数的输出有三个元素。1）它们都在`[0, 1]`区间内，2）它们的和为1，3）它们的顺序与输入向量中的顺序相匹配。由于这些属性的存在，输出可以被解释为被模型分配的（概率）值，表示所有可能的类别。在前面的代码片段中，第二个类别被分配了最高的概率，而第一个类别被分配了最低的概率。
- en: 'As a consequence, when using an output from a multiclass classifier of this
    sort, you can choose the index of the highest softmax element as the final decision—that
    is, a decision on what class the input belongs to. This can be achieved by using
    the method `argMax()`. For example, this is an excerpt from index.js:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当使用这种多类别分类器的输出时，你可以选择最高 softmax 元素的索引作为最终决策——也就是输入属于哪个类别的决策。这可以通过使用方法 `argMax()`
    来实现。例如，这是 index.js 的摘录：
- en: '[PRE38]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`predictOut` is a 2D tensor of shape `[numExamples, 3]`. Calling its `argMax0`
    method causes the shape to be reduced to `[numExample]`. The argument value –1
    indicates that `argMax()` should look for maximum values along the last dimension
    and return their indices. For instance, suppose `predictOut` has the following
    value:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`predictOut` 是形状为 `[numExamples, 3]` 的二维张量。调用它的 `argMax0` 方法会导致形状被减少为 `[numExample]`。参数值
    -1 表示 `argMax()` 应该在最后一个维度上查找最大值并返回它们的索引。例如，假设 `predictOut` 有以下值：'
- en: '[PRE39]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then, `argMax(-1)` will return a tensor that indicates the maximum values along
    the last (second) dimension are found at indices 1 and 0 for the first and second
    examples, respectively:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，`argMax(-1)` 将返回一个张量，指示沿着最后（第二个）维度找到的最大值分别在第一个和第二个示例的索引为 1 和 0：
- en: '[PRE40]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '3.3.3\. Categorical cross entropy: The loss function for multiclass classification'
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 分类交叉熵：多类别分类的损失函数
- en: 'In the binary classification example, we saw how binary cross entropy was used
    as the loss function and why other, more human-interpretable metrics such as accuracy
    and recall couldn’t be used as the loss function. The situation for multiclass
    classification is quite analogous. There exists a straightforward metric—accuracy—that
    is the fraction of examples that are classified correctly by the model. This metric
    is important for humans to understand how well the model is performing and is
    used in this code snippet in [listing 3.9](#ch03ex09):'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类示例中，我们看到了如何使用二元交叉熵作为损失函数，以及为什么其他更易于人类理解的指标，如准确率和召回率，不能用作损失函数。多类别分类的情况相当类似。存在一个直观的度量标准——准确率——它是模型正确分类的例子的比例。这个指标对于人们理解模型的性能有重要意义，并且在
    [列表 3.9](#ch03ex09) 中的这段代码片段中使用：
- en: '[PRE41]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'However, accuracy is a bad choice for loss function because it suffers from
    the same zero-gradient issue as the accuracy in binary classification. Therefore,
    people have devised a special loss function for multiclass classification: *categorical
    cross entropy*. It is simply a generalization of binary cross entropy into the
    cases where there are more than two categories.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，准确率对于损失函数来说是一个糟糕的选择，因为它遇到了与二元分类中的准确率相同的零梯度问题。因此，人们为多类别分类设计了一个特殊的损失函数：*分类交叉熵*。它只是将二元交叉熵推广到存在两个以上类别的情况。
- en: Listing 3.10\. Pseudo-code for categorical cross-entropy loss
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.10\. 用于分类交叉熵损失的伪代码
- en: '[PRE42]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In the pseudo-code in the previous listing, `oneHotTruth` is the one-hot encoding
    of the input example’s actual class. `probs` is the softmax probability output
    from the model. The key takeaway from this pseudo-code is that as far as categorical
    cross entropy is concerned, only one element of `probs` matters, and that is the
    element whose indices correspond to the actual class. The other elements of `probs`
    may vary all they like, but as long as they don’t change the element for the actual
    class, it won’t affect the categorical cross entropy. For that particular element
    of `probs`, the closer it gets to 1, the lower the value of the cross entropy
    will be. Like binary cross entropy, categorical cross entropy is directly available
    as a function under the `tf.metrics` namespace, and you can use it to calculate
    the categorical cross entropy of simple but illustrating examples. For example,
    with the following code, you can create a hypothetical, one-hot-encoded truth
    label and a hypothetical `probs` vector and compute the corresponding categorical
    cross-entropy value:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的伪代码中，`oneHotTruth` 是输入示例的实际类别的独热编码。`probs` 是模型的 softmax 概率输出。从这段伪代码中可以得出的关键信息是，就分类交叉熵而言，`probs`
    中只有一个元素是重要的，那就是与实际类别对应的索引的元素。`probs` 的其他元素可以随意变化，但只要它们不改变实际类别的元素，就不会影响分类交叉熵。对于
    `probs` 的特定元素，它越接近 1，交叉熵的值就越低。与二元交叉熵类似，分类交叉熵直接作为 `tf.metrics` 命名空间下的一个函数可用，你可以用它来计算简单但说明性的示例的分类交叉熵。例如，使用以下代码，你可以创建一个假设的独热编码的真实标签和一个假设的
    `probs` 向量，并计算相应的分类交叉熵值：
- en: '[PRE43]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This gives you an answer of approximately 0.693\. This means that when the probability
    assigned by the model to the actual class is 0.5, `categoricalCrossentropy` has
    a value of 0.693\. You can verify it against the pseudo-code in [listing 3.10](#ch03ex10).
    You may also try raising or lowering the value from 0.5 to see how `categoricalCrossentropy`
    changes (for instance, see [table 3.5](#ch03table05)). The table also includes
    a column that shows the MSE between the one-hot truth label and the `probs` vector.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了一个约为0.693的答案。这意味着当模型对实际类别分配的概率为0.5时，`categoricalCrossentropy`的值为0.693。你可以根据[pseudo-code(伪代码)](#ch03ex10)进行验证。你也可以尝试将值从0.5提高或降低，看看`categoricalCrossentropy`如何变化（例如，参见[table
    3.5](#ch03table05)）。表中还包括一列显示了单热真实标签和`probs`向量之间的MSE。
- en: Table 3.5\. The values of categorical cross entropy under different probability
    outputs. Without loss of generality, all the examples (row) are based on a case
    in which there are three classes (as is the case in the iris-flower dataset),
    and the actual class is the second one.
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.5. 不同概率输出下的分类交叉熵值。不失一般性，所有示例（行）都是基于有三个类别的情况（如鸢尾花数据集），并且实际类别是第二个类别。
- en: '| One-hot truth label | probs (softmax output) | Categorical cross entropy
    | MSE |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| One-hot truth label | probs (softmax output) | Categorical cross entropy
    | MSE |'
- en: '| --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| [0, 1, 0] | [0.2, 0.5, 0.3] | 0.693 | 0.127 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| [0, 1, 0] | [0.2, 0.5, 0.3] | 0.693 | 0.127 |'
- en: '| [0, 1, 0] | [0.0, 0.5, 0.5] | 0.693 | 0.167 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| [0, 1, 0] | [0.0, 0.5, 0.5] | 0.693 | 0.167 |'
- en: '| [0, 1, 0] | [0.0, 0.9, 0.1] | 0.105 | 0.006 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| [0, 1, 0] | [0.0, 0.9, 0.1] | 0.105 | 0.006 |'
- en: '| [0, 1, 0] | [0.1, 0.9, 0.0] | 0.105 | 0.006 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| [0, 1, 0] | [0.1, 0.9, 0.0] | 0.105 | 0.006 |'
- en: '| [0, 1, 0] | [0.0, 0.99, 0.01] | 0.010 | 0.00006 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| [0, 1, 0] | [0.0, 0.99, 0.01] | 0.010 | 0.00006 |'
- en: By comparing rows 1 and 2 or comparing rows 3 and 4 in this table, it should
    be clear that changing the elements of `probs` that don’t correspond to the actual
    class doesn’t alter the binary cross entropy, even though it may alter the MSE
    between the one-hot truth label and `probs`. Also, like in binary cross entropy,
    MSE shows diminished return when the `probs` value for the actual class approaches
    1, and hence is not good at encouraging the probability value of the correct class
    to go up as categorical entropy in this regime. These are the reasons why categorical
    cross entropy is more suitable as the loss function than MSE for multiclass-classification
    problems.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '通过比较表中的第1行和第2行，或比较第3行和第4行，可以明显看出更改`probs`中与实际类别不对应的元素不会改变二元交叉熵的值，尽管这可能会改变单热真实标签和`probs`之间的MSE。同样，就像在二元交叉熵中一样，当`probs`值接近1时，MSE显示出递减的回报，并且在这个区间内，MSE不适合鼓励正确类别的概率值上升，而分类熵则更适合作为多类别分类问题的损失函数。 '
- en: '3.3.4\. Confusion matrix: Fine-grained analysis of multiclass classification'
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. 混淆矩阵：多类别分类的细致分析
- en: By clicking the Train Model from Scratch button on the example’s web page, you
    can get a trained model in a few seconds. As [figure 3.9](#ch03fig09) shows, the
    model reaches nearly perfect accuracy after 40 epochs of training. This reflects
    the fact that the iris dataset is a small one with relatively well-defined boundaries
    between the classes in the feature space.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 点击示例网页上的从头开始训练模型按钮，你可以在几秒钟内得到一个经过训练的模型。正如[图3.9](#ch03fig09)所示，模型经过40个训练周期后几乎达到了完美的准确度。这反映了鸢尾花数据集是一个相对较小且在特征空间中类别边界相对明确的数据集的事实。
- en: 'Figure 3.9\. A typical result from training the iris model for 40 epochs. Top
    left: the loss function plotted against epochs of training. Top right: the accuracy
    plotted against epochs of training. Bottom: the confusion matrix.'
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9. 40个训练周期后鸢尾花模型的典型结果。左上方：损失函数随训练周期变化的图表。右上方：准确度随训练周期变化的图表。底部：混淆矩阵。
- en: '![](03fig09_alt.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig09_alt.jpg)'
- en: The bottom part of [figure 3.9](#ch03fig09) shows an additional way of characterizing
    the behavior of a multiclass classifier, called a *confusion matrix*. A confusion
    matrix breaks down the results of a multiclass classifier according to their actual
    classes and the model’s predicted classes. It is a square matrix of shape `[numClasses,
    numClasses]`. The element at indices `[i, j]` (row i and column j) is the number
    of examples that belong to class `i` and are predicted as class `j` by the model.
    Therefore, the diagonal elements of a confusion matrix correspond to correctly
    classified examples. A perfect multiclass classifier should produce a confusion
    matrix with no nonzero elements outside the diagonal. This is exactly the case
    for the confusion matrix in [figure 3.9](#ch03fig09).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.9](#ch03fig09)的底部显示了描述多类分类器行为的另一种方式，称为*混淆矩阵*。混淆矩阵根据其实际类别和模型预测类别将多类分类器的结果进行了细分。它是一个形状为`[numClasses,
    numClasses]`的方阵。索引`[i, j]`（第i行和第j列）处的元素是属于类别`i`并由模型预测为类别`j`的示例数量。因此，混淆矩阵的对角线元素对应于正确分类的示例。一个完美的多类分类器应该产生一个没有对角线之外的非零元素的混淆矩阵。这正是[图3.9](#ch03fig09)中的混淆矩阵的情况。'
- en: 'In addition to showing the final confusion matrix, the iris example also draws
    the confusion matrix at the end of every training epoch, using the `onTrainEnd()`
    callback. In early epochs, you may see a less perfect confusion matrix than the
    one in [figure 3.9](#ch03fig09). The confusion matrix in [figure 3.10](#ch03fig10)
    shows that 8 out of the 24 input examples were misclassified, which corresponds
    to an accuracy of 66.7%. However, the confusion matrix tells us about more than
    just a single number: it shows which classes involve the most mistakes and which
    involve fewer. In this particular example, all flowers from the second class are
    misclassified (either as the first or the third class), while the flowers from
    the first and third classes are always classified correctly. Therefore, you can
    see that in multiclass classification, a confusion matrix is a more informative
    measurement than simply the accuracy, just like precision and recall together
    form a more comprehensive measurement than accuracy in binary classification.
    Confusion matrices can provide information that aids decision-making related to
    the model and the training process. For example, making some types of mistakes
    may be more costly than confusing other pairs of classes. Perhaps mistaking a
    sports site for a gaming site is less of a problem than confusing a sports site
    for a phishing scam. In those cases, you can adjust the model’s hyperparameters
    to minimize the costliest mistakes.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 除了展示最终的混淆矩阵外，鸢尾花示例还在每个训练周期结束时使用`onTrainEnd()`回调绘制混淆矩阵。在早期周期中，您可能会看到一个不太完美的混淆矩阵，与[图3.9](#ch03fig09)中的混淆矩阵不同。[图3.10](#ch03fig10)中的混淆矩阵显示，24个输入示例中有8个被错误分类，对应的准确率为66.7%。然而，混淆矩阵告诉我们不仅仅是一个数字：它显示了哪些类别涉及最多的错误，哪些涉及较少。在这个特定的示例中，所有来自第二类的花都被错误分类（要么作为第一类，要么作为第三类），而来自第一类和第三类的花总是被正确分类。因此，您可以看到，在多类分类中，混淆矩阵比简单的准确率更具信息量，就像精确率和召回率一起形成了比二分类准确率更全面的衡量标准一样。混淆矩阵可以提供有助于与模型和训练过程相关的决策的信息。例如，某些类型的错误可能比混淆其他类别对更为昂贵。也许将一个体育网站误认为游戏网站不如将体育网站误认为钓鱼网站那么严重。在这些情况下，您可以调整模型的超参数以最小化最昂贵的错误。
- en: Figure 3.10\. An example of an “imperfect” confusion matrix, in which there
    are nonzero elements off the diagonal. This confusion matrix is generated after
    only 2 epochs, before the training converged.
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10\. 一个“不完美”混淆矩阵的示例，在对角线之外存在非零元素。该混淆矩阵是在训练收敛之前的仅2个周期后生成的。
- en: '![](03fig10_alt.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig10_alt.jpg)'
- en: The models we’ve seen so far all take an array of numbers as inputs. In other
    words, each input example is represented as a simple list of numbers, of which
    the length is fixed, and the ordering of the elements doesn’t matter as long as
    they are consistent for all examples fed to the model. While this type of model
    covers a large subset of important and practical machine-learning problems, it
    is far from the only kind. In the coming chapters, we will look at more complex
    input data types, including images and sequences. In [chapter 4](kindle_split_015.html#ch04),
    we’ll start from images, a ubiquitous and widely useful type of input data for
    which powerful neural network structures have been developed to push the accuracy
    of machine-learning models to superhuman levels.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所见的模型都将一组数字作为输入。换句话说，每个输入示例都表示为一组简单的数字列表，其中长度固定，元素的排序不重要，只要它们对馈送到模型的所有示例都一致即可。虽然这种类型的模型涵盖了重要和实用的机器学习问题的大量子集，但它远非唯一的类型。在接下来的章节中，我们将研究更复杂的输入数据类型，包括图像和序列。在
    [第 4 章](kindle_split_015.html#ch04) 中，我们将从图像开始，这是一种无处不在且广泛有用的输入数据类型，为此已经开发了强大的神经网络结构，以将机器学习模型的准确性推向超人级别。
- en: Exercises
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: When creating neural networks for the Boston-housing problem, we stopped at
    a model with two hidden layers. Given what we said about cascading nonlinear functions
    leading to enhanced capacity of models, will adding more hidden layers to the
    model lead to improved evaluation accuracy? Try this out by modifying index.js
    and rerunning the training and evaluation.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当创建用于波士顿房屋问题的神经网络时，我们停留在一个具有两个隐藏层的模型上。鉴于我们所说的级联非线性函数会增强模型的容量，那么将更多的隐藏层添加到模型中会导致评估准确性提高吗？通过修改
    index.js 并重新运行训练和评估来尝试一下。
- en: What is the factor that prevents more hidden layers from improving the evaluation
    accuracy?
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是什么因素阻止了更多的隐藏层提高评估准确性？
- en: 'What makes you reach this conclusion? (Hint: look at the error on the training
    set.)'
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是什么让您得出这个结论？（提示：看一下训练集上的误差。）
- en: Look at how the code in [listing 3.6](#ch03ex06) uses the `onEpochBegin` callback
    to calculate and draw an ROC curve at the beginning of every training epoch. Can
    you follow this pattern and make some modifications to the body of the callback
    function so that you can print the precision and recall values (calculated on
    the test set) at the beginning of every epoch? Describe how these values change
    as the training progresses.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看 [清单 3.6](#ch03ex06) 中的代码如何使用 `onEpochBegin` 回调在每个训练时期的开始计算并绘制 ROC 曲线。您能按照这种模式并对回调函数的主体进行一些修改，以便您可以在每个时期的开始打印精度和召回率值（在测试集上计算）吗？描述这些值随着训练的进行而如何变化。
- en: Study the code in [listing 3.7](#ch03ex07) and understand how it computes the
    ROC curve. Can you follow this example and write a new function, called `drawPrecisionRecallCurve()`,
    which, as its name indicates, computes and renders a precision-recall curve? Once
    you are done writing the function, call it from the `onEpochBegin` callback so
    that a precision-recall curve can be plotted alongside the ROC curve at the beginning
    of every training epoch. You may need to make some changes or additions to ui.js.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究 [清单 3.7](#ch03ex07) 中的代码，并理解它是如何计算 ROC 曲线的。您能按照这个示例并编写一个新的函数，名为 `drawPrecisionRecallCurve()`，它根据名称显示一个精度-召回率曲线吗？写完函数后，从
    `onEpochBegin` 回调中调用它，以便在每个训练时期的开始绘制一个精度-召回率曲线。您可能需要对 ui.js 进行一些修改或添加。
- en: Suppose you are told the FPR and TPR of a binary classifier’s results. With
    those two numbers, is it possible for you to calculate the overall accuracy? If
    not, what extra piece(s) of information do you require?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设您得知二元分类器结果的 FPR 和 TPR。凭借这两个数字，您能计算出整体准确性吗？如果不能，您需要什么额外信息？
- en: The definitions of binary cross entropy ([section 3.2.4](#ch03lev2sec6)) and
    categorical cross entropy ([section 3.3.3](#ch03lev2sec9)) are both based on the
    natural logarithm (the log of base *e*). What if we change the definition so that
    they use the log of base 10? How would that affect the training and inference
    of binary and multiclass classifiers?
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 二元交叉熵（[3.2.4 节](#ch03lev2sec6)）和分类交叉熵（[3.3.3 节](#ch03lev2sec9)）的定义都基于自然对数（以
    *e* 为底的对数）。如果我们改变定义，让它们使用以 10 为底的对数会怎样？这会如何影响二元和多类分类器的训练和推断？
- en: Turn the pseudo-code for the hyperparameter grid search in [listing 3.4](#ch03ex04)
    into actual JavaScript code, and use the code to perform hyperparameter optimization
    for the two-layer Boston-housing model in [listing 3.1](#ch03ex01). Specifically,
    tune the number of units of the hidden layer and the learning rate. Feel free
    to decide on the ranges of units and learning rate to search over. Note that machine-learning
    engineers generally use approximately geometric sequences (that is, logarithmic)
    spacing for these searches (for example, units = 2, 5, 10, 20, 50, 100, 200, .
    . .).
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将超参数网格搜索的伪代码转换为实际的JavaScript代码，并使用该代码对[列表3.1](#ch03ex01)中的两层波士顿房屋模型进行超参数优化。具体来说，调整隐藏层的单位数和学习率。可以自行决定要搜索的单位和学习率的范围。注意，机器学习工程师通常使用近似几何序列（即对数）间隔进行这些搜索（例如，单位=
    2、5、10、20、50、100、200，...）。
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Classification tasks are different from regression tasks in that they involve
    making discrete predictions.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类任务与回归任务不同，因为它们涉及进行离散预测。
- en: 'There are two types of classification: binary and multiclass. In binary classification,
    there are two possible classes for a given input, whereas in multiclass classification,
    there are three or more.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类有两种类型：二元和多类。在二元分类中，对于给定的输入，有两种可能的类别，而在多类分类中，有三个或更多。
- en: Binary classification can usually be viewed as detecting a certain type of event
    or object of significance, called positives, among all the input examples. When
    viewed this way, we can use metrics such as precision, recall, and FPR, in addition
    to accuracy, to quantify various aspects of a binary classifier’s behavior.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类通常可以被看作是在所有输入示例中检测一种称为正例的特定类型事件或对象。从这个角度来看，我们可以使用精确率、召回率和FPR等指标，除了准确度，来量化二元分类器行为的各个方面。
- en: The trade-off between the need to catch all positive examples and the need to
    minimize false positives (false alarms) is common in binary-classification tasks.
    The ROC curve, along with the associated AUC metric, is a technique that helps
    us quantify and visualize this relation.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二元分类任务中，需要在捕获所有正例和最小化假阳性（误报警）之间进行权衡是很常见的。ROC曲线与相关的AUC指标是一种帮助我们量化和可视化这种关系的技术。
- en: A neural network created for binary classification should use the sigmoid activation
    in its last (output) layer and use binary cross entropy as the loss function during
    training.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了进行二元分类而创建的神经网络应该在其最后（输出）层使用sigmoid激活，并在训练过程中使用二元交叉熵作为损失函数。
- en: To create a neural network for multiclass classification, the output target
    is usually represented by one-hot encoding. The neural network ought to use softmax
    activation in its output layer and be trained using the categorical cross-entropy
    loss function.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了创建一个用于多类分类的神经网络，输出目标通常由独热编码表示。神经网络应该在其输出层使用softmax激活，并使用分类交叉熵损失函数进行训练。
- en: For multiclass classification, confusion matrices can provide more fine-grained
    information regarding the mistakes made by the model than accuracy can.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多类分类，混淆矩阵可以提供比准确度更细粒度的信息，关于模型所犯错误的信息。
- en: '[Table 3.6](#ch03table06) summarizes recommended methodologies for the most
    common types of machine-learning problems we have seen so far (regression, binary
    classification, and multiclass classification).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[表3.6](#ch03table06)总结了迄今为止我们见过的最常见的机器学习问题类型（回归、二元分类和多类分类）的推荐方法。'
- en: Hyperparameters are configurations concerning a machine-learning model’s structure,
    properties of its layer, and its training process. They are distinct from the
    model’s weight parameters in that 1) they do not change during the model’s training
    process, and 2) they are often discrete. Hyperparameter optimization is the process
    in which values of the hyperparameters are sought in order to minimize a loss
    on the validation dataset. Hyperparameter optimization is still an active area
    of research. Currently, the most frequently used methods include grid search,
    random search, and Bayesian methods.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数是关于机器学习模型结构、其层属性以及其训练过程的配置。它们与模型的权重参数不同，因为1）它们在模型的训练过程中不变化，2）它们通常是离散的。超参数优化是一种寻找超参数值以在验证数据集上最小化损失的过程。超参数优化仍然是一个活跃的研究领域。目前，最常用的方法包括网格搜索、随机搜索和贝叶斯方法。
- en: Table 3.6\. An overview of the most common types of machine-learning tasks,
    their suitable last-layer activation function and loss function, as well as the
    metrics that help quantify the model quality
  id: totrans-362
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表格 3.6. 最常见的机器学习任务类型，它们适用的最后一层激活函数和损失函数，以及有助于量化模型质量的指标的概述
- en: '| Type of task | Activation of output layer | Loss function | Suitable metrics
    supported during Model.fit() calls | Additional metrics |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 任务类型 | 输出层的激活函数 | 损失函数 | 在 Model.fit() 调用中支持的适用指标 | 额外的指标 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Regression | ''linear'' (default) | ''meanSquaredError'' or ''meanAbsoluteError''
    | (same as loss) |   |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 回归 | ''linear'' (默认) | ''meanSquaredError'' 或 ''meanAbsoluteError'' | (与损失函数相同)
    |   |'
- en: '| Binary classification | ''sigmoid'' | ''binaryCrossentropy'' | ''accuracy''
    | Precision, recall, precision-recall curve, ROC curve, AUC |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 二分类 | ''sigmoid'' | ''binaryCrossentropy'' | ''accuracy'' | 精确率，召回率，精确-召回曲线，ROC曲线，AUC值
    |'
- en: '| Single-label, multiclass classification | ''softmax'' | ''categoricalCrossentropy''
    | ''accuracy'' | Confusion matrix |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 单标签，多类别分类 | ''softmax'' | ''categoricalCrossentropy'' | ''accuracy'' | 混淆矩阵
    |'
