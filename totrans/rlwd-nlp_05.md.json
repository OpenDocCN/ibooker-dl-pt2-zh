["```py\n>>> import nltk\n>>> from nltk.tokenize import word_tokenize, sent_tokenize\n```", "```py\n>>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n\n>>> word_tokenize(s)\n['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please',\n 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n\n>>> sent_tokenize(s)\n['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.',\n 'Thanks.']\n```", "```py\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_sm')\n\n>>> doc = nlp(s)\n\n>>> [token.text for token in doc]\n['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'in', 'New', 'York', '.', ' ',\n 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.']\n\n>>> [sent.string.strip() for sent in doc.sents]\n['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.', 'Thanks.']\n```", "```py\n>>> from nltk.stem.porter import PorterStemmer\n>>> stemmer = PorterStemmer()\n\n>>> words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n...          'died', 'agreed', 'owned', 'humbled', 'sized',\n...          'meetings', 'stating', 'siezing', 'itemization',\n...          'sensational', 'traditional', 'reference', 'colonizer',\n...          'plotted']\n>>> [stemmer.stem(word) for word in words]\n['caress', 'fli', 'die', 'mule', 'deni',\n 'die', 'agre', 'own', 'humbl', 'size',\n 'meet', 'state', 'siez', 'item',\n 'sensat', 'tradit', 'refer', 'colon',\n 'plot']\n```", "```py\n>>> from nltk.stem import WordNetLemmatizer\n>>> lemmatizer = WordNetLemmatizer()\n\n>>> [lemmatizer.lemmatize(word) for word in words]\n['caress', 'fly', 'dy', 'mule', 'denied',\n 'died', 'agreed', 'owned', 'humbled', 'sized',\n 'meeting', 'stating', 'siezing', 'itemization',\n 'sensational', 'traditional', 'reference', 'colonizer',\n 'plotted']\n```", "```py\n>>> doc = nlp(' '.join(words))\n>>> [token.lemma_ for token in doc]\n['caress', 'fly', 'die', 'mule', 'deny',\n 'die', 'agree', 'own', 'humble', 'sized',\n 'meeting', 'state', 'siezing', 'itemization',\n 'sensational', 'traditional', 'reference', 'colonizer',\n 'plot']\n```", "```py\n{\"bark\": 1.4,\n \"chocolate\": 0.1,\n ...,\n \"pet\": 1.2,\n ...,\n \"smell\": 0.6,\n ...}\n```", "```py\n[1.4, 0.1, ..., 1.2, ..., 0.6, ...]\n```", "```py\ndef linear(x):\n    return w * x + b\n```", "```py\ndef linear2(x1, x2):\n    return w1 * x1 + w2 * x2 + b\n```", "```py\ndef linear3(x1, x2):\n    y1 = w11 * x1 + w12 * x2 + b1\n    y2 = w21 * x1 + w22 * x2 + b2\n    return [y1, y2]\n```", "```py\nfrom examples.embeddings.word2vec import SkipGramReader\n```", "```py\nreader = SkipGramReader()\ntext8 = reader.read('https:/./realworldnlpbook.s3.amazonaws.com/data/text8/text8')\n```", "```py\nfrom collections import Counter\n\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import SimpleDataLoader\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training.trainer import GradientDescentTrainer\nfrom torch.nn import CosineSimilarity\nfrom torch.nn import functional\n\nEMBEDDING_DIM = 256\nBATCH_SIZE = 256\n```", "```py\n>>> for inst in text8:\n>>>     print(inst)\n...\nInstance with fields:\n     token_in: LabelField with label: ideas in namespace: 'token_in'.'\n     token_out: LabelField with label: us in namespace: 'token_out'.'\n\nInstance with fields:\n     token_in: LabelField with label: ideas in namespace: 'token_in'.'\n     token_out: LabelField with label: published in namespace: 'token_out'.'\n\nInstance with fields:\n     token_in: LabelField with label: ideas in namespace: 'token_in'.'\n     token_out: LabelField with label: journal in namespace: 'token_out'.'\n\nInstance with fields:\n     token_in: LabelField with label: in in namespace: 'token_in'.'\n     token_out: LabelField with label: nature in namespace: 'token_out'.'\n\nInstance with fields:\n     token_in: LabelField with label: in in namespace: 'token_in'.'\n     token_out: LabelField with label: he in namespace: 'token_out'.'\n\nInstance with fields:\n     token_in: LabelField with label: in in namespace: 'token_in'.'\n     token_out: LabelField with label: announced in namespace: 'token_out'.'\n...\n```", "```py\nvocab = Vocabulary.from_instances(\n    text8, min_count={'token_in': 5, 'token_out': 5})\n```", "```py\ndata_loader = SimpleDataLoader(text8, batch_size=BATCH_SIZE)\ndata_loader.index_with(vocab)\n```", "```py\nembedding_in = Embedding(num_embeddings=vocab.get_vocab_size('token_in'),\n                         embedding_dim=EMBEDDING_DIM)\n```", "```py\nclass SkipGramModel(Model):                                   ❶\n\n    def __init__(self, vocab, embedding_in):\n        super().__init__(vocab)\n\n        self.embedding_in = embedding_in                      ❷\n\n        self.linear = torch.nn.Linear(\n            in_features=EMBEDDING_DIM,\n            out_features=vocab.get_vocab_size('token_out'),\n            bias=False)                                       ❸\n\n    def forward(self, token_in, token_out):                   ❹\n\n        embedded_in = self.embedding_in(token_in)             ❺\n\n        logits = self.linear(embedded_in)                     ❻\n\n        loss = functional.cross_entropy(logits, token_out)    ❼\n\n        return {'loss': loss}\n```", "```py\nreader = SkipGramReader()\ntext8 = reader.read(' https:/./realworldnlpbook.s3.amazonaws.com/data/text8/text8')\n\nvocab = Vocabulary.from_instances(\n    text8, min_count={'token_in': 5, 'token_out': 5})\n\ndata_loader = SimpleDataLoader(text8, batch_size=BATCH_SIZE)\ndata_loader.index_with(vocab)\n\nembedding_in = Embedding(num_embeddings=vocab.get_vocab_size('token_in'),\n                         embedding_dim=EMBEDDING_DIM)\n\nmodel = SkipGramModel(vocab=vocab,\n                      embedding_in=embedding_in)\noptimizer = optim.Adam(model.parameters())\n\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=data_loader,\n    num_epochs=5,\n    cuda_device=CUDA_DEVICE)\ntrainer.train()\n```", "```py\ndef get_related(token: str, embedding: Model, vocab: Vocabulary, \n                num_synonyms: int = 10):\n    token_id = vocab.get_token_index(token, 'token_in')\n    token_vec = embedding.weight[token_id]\n    cosine = CosineSimilarity(dim=0)\n    sims = Counter()\n\n    for index, token in vocab.get_index_to_token_vocabulary('token_in').items():\n        sim = cosine(token_vec, embedding.weight[index]).item()\n        sims[token] = sim\n\n    return sims.most_common(num_synonyms)\n```", "```py\n...\nif 0.15778 0.17928 -0.45811 -0.12817 0.367 0.18817 -4.5745 0.73647 ...\none 0.38661 0.33503 -0.25923 -0.19389 -0.037111 0.21012 -4.0948 0.68349 ...\nhas 0.08088 0.32472 0.12472 0.18509 0.49814 -0.27633 -3.6442 1.0011 ...\n...\n```", "```py\nfrom annoy import AnnoyIndex\nimport pickle\n\nEMBEDDING_DIM = 300\nGLOVE_FILE_PREFIX = 'data/glove/glove.42B.300d{}'\n\ndef build_index():\n    num_trees = 10\n\n    idx = AnnoyIndex(EMBEDDING_DIM)\n\n    index_to_word = {}\n    with open(GLOVE_FILE_PREFIX.format('.txt')) as f:\n        for i, line in enumerate(f):\n            fields = line.rstrip().split(' ')\n            vec = [float(x) for x in fields[1:]]\n            idx.add_item(i, vec)\n            index_to_word[i] = fields[0]\n\n    idx.build(num_trees)\n    idx.save(GLOVE_FILE_PREFIX.format('.idx'))\n    pickle.dump(index_to_word,\n                open(GLOVE_FILE_PREFIX.format('.i2w'), mode='wb'))\n```", "```py\ndef search(query, top_n=10):\n    idx = AnnoyIndex(EMBEDDING_DIM)\n    idx.load(GLOVE_FILE_PREFIX.format('.idx'))\n    index_to_word = pickle.load(open(GLOVE_FILE_PREFIX.format('.i2w'),\n                                     mode='rb'))\n    word_to_index = {word: index for index, word in index_to_word.items()}\n\n    query_id = word_to_index[query]\n    word_ids = idx.get_nns_by_item(query_id, top_n)\n    for word_id in word_ids:\n        print(index_to_word[word_id])\n```", "```py\n$ ./fasttext skipgram -input ../data/text8 -output model\n```", "```py\n$ echo \"supercalifragilisticexpialidocious\" \\\n| ./fasttext print-word-vectors model.bin\nsupercalifragilisticexpialidocious 0.032049 0.20626 -0.21628 -0.040391 -0.038995 0.088793 -0.0023854 0.41535 -0.17251 0.13115 ...\n```", "```py\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.doc2vec import TaggedDocument\n\ndef read_corpus(file_path):\n    with open(file_path) as f:\n        for i, line in enumerate(f):\n            yield TaggedDocument(simple_preprocess(line), [i])\n```", "```py\n    from gensim.models.doc2vec import Doc2Vec\n\n    train_set = list(read_corpus('data/mt/sentences.eng.200k.txt'))\n    model = Doc2Vec(vector_size=256, min_count=3, epochs=30)\n    model.build_vocab(train_set)\n    model.train(train_set,\n                total_examples=model.corpus_count,\n                epochs=model.epochs)\n\n    query_vec = model.infer_vector(\n        ['i', 'heard', 'a', 'dog', 'barking', 'in', 'the', 'distance'])\n    sims = model.docvecs.most_similar([query_vec], topn=10)\n    for doc_id, sim in sims:\n        print('{:3.2f} {}'.format(sim, train_set[doc_id].words)) \n```", "```py\n0.67 ['she', 'was', 'heard', 'playing', 'the', 'violin']\n0.65 ['heard', 'the', 'front', 'door', 'slam']\n0.61 ['we', 'heard', 'tigers', 'roaring', 'in', 'the', 'distance']\n0.61 ['heard', 'dog', 'barking', 'in', 'the', 'distance']\n0.60 ['heard', 'the', 'door', 'open']\n0.60 ['tom', 'heard', 'the', 'door', 'open']\n0.60 ['she', 'heard', 'dog', 'barking', 'in', 'the', 'distance']\n0.59 ['heard', 'the', 'door', 'close']\n0.59 ['when', 'he', 'heard', 'the', 'whistle', 'he', 'crossed', 'the', 'street']\n0.58 ['heard', 'the', 'telephone', 'ringing']\n```", "```py\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ndef read_glove(file_path):\n    with open(file_path) as f:\n        for i, line in enumerate(f):\n            fields = line.rstrip().split(' ')\n            vec = [float(x) for x in fields[1:]]\n            word = fields[0]\n            yield (word, vec)\n\nwords = []\nvectors = []\nfor word, vec in read_glove('data/glove/glove.42B.300d.txt'):\n    words.append(word)\n    vectors.append(vec)\n\nmodel = TSNE(n_components=2, init='pca', random_state=0)\ncoordinates = model.fit_transform(vectors)\n\nplt.figure(figsize=(8, 8))\n\nfor word, xy in zip(words, coordinates):\n    plt.scatter(xy[0], xy[1])\n    plt.annotate(word,\n                 xy=(xy[0], xy[1]),\n                 xytext=(2, 2),\n                 textcoords='offset points')\n\nplt.xlim(25, 55)\nplt.ylim(-15, 15)\nplt.show()\n```"]