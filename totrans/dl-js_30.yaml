- en: B.4\. Calculating gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section is for readers who are interested in performing derivative and
    gradient calculation in TensorFlow.js. For most deep-learning models in this book,
    the calculation of derivatives and gradients is taken care of under the hood by
    `model.fit()` and `model.fitDataset()`. However, for certain problem types, such
    as finding maximally activating images for convolution filters in [chapter 7](kindle_split_019.html#ch07)
    and RL in [chapter 11](kindle_split_023.html#ch11), it is necessary to calculate
    derivatives and gradients explicitly. TensorFlow.js provides APIs to support such
    use cases. Let’s start from the simplest scenario—namely, a function that takes
    a single input tensor and returns a single output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to calculate the derivative of the function (`f`) with respect to
    the input (`x`), we use the `tf.grad()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that `tf.grad()` doesn’t give you the derivative’s value right away. Instead,
    it gives you a *function* that is the derivative of the original function (`f`).
    You can invoke that function (`df`) with a concrete value of `x`, and that’s when
    you get the value of `df/dx`. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'which gives you an output that correctly reflects the derivative of the `atan()`
    function at x-values of –4, –2, 0, 2, and 4 (see [figure B.5](#app02fig05)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Figure B.5\. A plot of the function `atan(x)`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![](btab05_alt.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`tf.grad()` is limited to a function with only one input tensor. What if you
    have a function with multiple inputs? Let’s consider an example of `h(x, y)`,
    which is simply the product of two tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.grads()` (with the “s” in the name) generates a function that returns the
    partial derivative of the input function with respect to all the arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: which gives the results
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These results are correct because the partial derivative of *x* `*` *y* with
    respect to *x* is *y* and that with respect to *y* is *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The functions generated by `tf.grad()` and `tf.grads()` give you only the derivatives,
    not the return value of the original function. In the example of *h*(*x*, *y*),
    what if we want to get not only the derivatives but also the value of *h*? For
    that, you can use the `tf.valueAndGrads()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output (`out`) is an object with two fields: `value`, which is the value
    of *h* given the input values, and `grads`, which has the same format as the return
    value of the function generated by `tf.grads()`—namely, an array of partial-derivative
    tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The APIs discussed are all about calculating the derivatives of functions with
    respect to their explicit arguments. However, a common scenario in deep learning
    involves functions that use weights in their calculation. Those weights are represented
    as `tf.Variable` objects and are *not* explicitly passed to the functions as arguments.
    For such functions, we often need to calculate their derivatives with respect
    to the weights during training. This workflow is served by the `tf.variableGrads()`
    function, which keeps track of what trainable variables are accessed by the function
    being differentiated and automatically calculates the derivatives with respect
    to them. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '***1*** *f*(*a*, *b*) = *a* * *x* ^ 2 + *b* * *x*. The sum() method is called
    because tf.variableGrads() requires the function being differentiated to return
    a scalar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `value` field of `tf.variableGrads()`’s output is the return value of `f`
    given the current values of `a`, `b`, and `x`. The `grads` field is a JavaScript
    object that carries the derivatives with respect to the two variables (`a` and
    `b`) under the corresponding key names. For example, the derivative of `f(a, b)`
    with respect to `a` is `x ^ 2`, and the derivative of `f(a, b)` with respect to
    `b` is `x`,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: which correctly gives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
