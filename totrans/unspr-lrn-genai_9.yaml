- en: 9 Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Out of intense complexities, intense simplicities emerge – Winston Churchill”
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python code using tensorflow and keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the last chapter of the book, we explored the concepts of deep learning.
    Those are the foundational concepts enabling you to grasp unsupervised deep learning.
    So, let’s start the first topic on unsupervised deep learning. We are starting
    with Autoencoders as the very first topic. We will be first covering the basics
    of autoencoders, what are they and how do we train the autoencoders. We will then
    get into the different types of autoencoders followed by a Python code on the
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the ninth chapter and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at this location.
  prefs: []
  type: TYPE_NORMAL
- en: You would need to install a few Python libraries in this chapter which are –
    tensorflow and keras.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with Chapter 9 of the book!
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Feature Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Predictive modelling is quite an interesting topic. Across various domains and
    business functions, predictive modelling is used for various purposes like predicting
    the sales for a business next year, predicting the amount of rainfall expected,
    predicting whether the incoming credit card transaction is fraud or not, predicting
    whether the customer will make a purchase or not and so on. The use cases are
    many and all of the above-mentioned use cases fall under supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The data sets that we use have variable or attributes. They are also called
    characteristics or *features*.
  prefs: []
  type: TYPE_NORMAL
- en: Whilst we wish to create these predictive models, we are also interested to
    understand the variables which are useful for making the prediction. Let’s consider
    a case where a bank wants to predict if an incoming transaction is fraudulent
    or not. In such a scenario, the bank will wish know which factors are significant
    in order to identify an incoming transaction as fraud. Factors that might be considered
    include the amount of the transaction, the time of the transaction, origin/source
    of the transaction etc. The variables which are important for making a prediction
    are called as *significant variables*.
  prefs: []
  type: TYPE_NORMAL
- en: For us to create a machine learning based predictive model, *feature engineering*
    is used. Feature engineering, otherwise known as feature extraction, is the process
    of extracting features from the raw data to improve the overall quality of the
    model and enhance the accuracy as compared to a model where only raw data was
    fed to the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering can be done using the domain understanding, using various
    manual methods and a few automated methods too. One of such methods is known as
    Feature Learning. Feature learning is the set of techniques which help a solution
    to automatically discover the representations required for feature detection.
    With the help of feature learning, manual feature engineering is not required.
    The impact of feature learning is much more relevant for datasets where images,
    text, audio and videos are being used.
  prefs: []
  type: TYPE_NORMAL
- en: Feature learning can be both supervised and unsupervised. For supervised feature
    learning, we have neural networks as the best example. For unsupervised feature
    learning we have examples like matrix factorization, clustering algorithms and
    autoencoders. We have already covered clustering and matrix factorization in the
    last chapters of the book. We are discussing autoencoders in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with introductions to autoencoders in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Introducing Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we start with any data science problem, data plays the most significant
    role. A data set which has a lot of noise is one of the biggest challenges in
    data science and machine learning. There are quite a few solutions available now
    and autoencoders are one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, an autoencoder is a type of artificial neural network and they are
    used to learn the data encodings. They are typically used for dimensionality reduction
    methods. They can also be used as generative models which can create synthetic
    data for us which is like the old data. For example, if we do not have good amount
    of data to train a machine learning, we can use generated synthetic data to train
    the models.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are feed forward neural networks and they compress the input into
    a lower-dimensional code and then try to reconstruct the output from this representation.
    The objective for an autoencoder is to learn the lower-dimensional representation
    (also sometimes known as encoding) for a high-dimensional dataset. Recall from
    the previous chapters about Principal Component Analysis (PCA). Autoencoders can
    be thought as a generalization for PCA. PCA is a linear method whereas autoencoders
    can learn non-linear relationships as well. Hence, autoencoders are required for
    dimensionality reduction solutions wherein they capture the most significant attributes
    from the input data.
  prefs: []
  type: TYPE_NORMAL
- en: We will now study the various components of autoencoders in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Components of autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The architecture for an autoencoder is quite simple to understand. An autoencoder
    consists of three parts: an encoder, a bottleneck or a code, and a decoder as
    shown in (Figure 9.1). In simple terms, an encoder compresses the input data,
    a bottleneck or code contains this compressed information and the decoder decompresses
    the knowledge and hence reconstructs this data back to its original form. Once
    the decompression has been done and the data has been reconstructed to its encoded
    form, the input and output can be compared.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s study these components in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: the input data passes through the encoder. Encoder is nothing
    but a fully connected artificial neural network. It compresses the input data
    into an encoded representation and hence, in the process the output generated
    is much reduced in size. Encoder compresses the input data into a compressed module
    known as the bottleneck.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bottleneck**: The bottleneck can be called as the brain of the encoder. It
    contains the compressed information representations and it is the job of the bottleneck
    to allow only the most important information to pass through it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decoder**: The information received from the bottleneck is decompressed by
    decoder. It recreates the data back to its original or encoded form. Once the
    job of decoder is done, the actual values are compared with the decompressed values
    created by the decoder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.1 Structure of an autoencoder with input layer, hidden layer and output
    layer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![09_01](images/09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For autoencoders, a few important points should be noticed:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a loss of information in autoencoders when the decompression is done
    as compared to the original inputs. So, when the compressed data is decompressed,
    then there is a loss as compared to the original data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Autoencoders are specific to datasets. This means that an algorithm which is
    trained on images of flowers will not work on the images of traffic signals and
    vice versa. This is due to the fact that the features the autoencoder would have
    learned will be specific to flowers only. So, we can say that auto-encoders are
    only able to compress the data similar to the one used for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is relatively easier to train specialized instances of algorithms to perform
    good on specific type of inputs. We just need representative training datasets
    to tarin the autoencoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ve now covered the major components of autoencoders. Next let’s go into the
    process of training an autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Training of autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to note that if there is no correlation between the variables
    in the data, then it is really difficult to compress and subsequently decompress
    the input data. For us to create a meaningful solution, there ought to be some
    level of relationships or correlations between the variables in the input data.
    To create an autoencoder we require an encoding method, a decoding method and
    a loss function to compare the actual vs decompressed values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First the input data passes through the encoder module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder compresses the input to a model into a compact bottleneck.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is the job of the bottleneck to restrict the flow of information and allows
    only important information to pass through and hence bottleneck is sometimes referred
    to as *knowledge-representation*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the bottleneck is the decoder which decompresses the information and
    recreates the data back to its original or encoded form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This encoder-decoder architecture is quite efficient in getting the most significant
    attributes from the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective of the solution is generating an output as identical to the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, it is observed that decoder architecture is mirror image of the coder
    architecture. It is not mandatory but is generally followed. We ensure that the
    dimensionality of the input and outputs are same.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to define four hyperparameters for training an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code size: It is perhaps the most significant hyperparameter. It represents
    the number of nodes in the middle layer. This decides the compression of the data
    and can also act as a regularization term. The lesser the value of code size,
    the compression of the data is more.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of layers is a parameter which denotes the depth of the autoencoder.
    A model which has more depth is obviously more complex and will take higher processing
    time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of nodes per layer is weight used per layer. It generally decreases with
    every subsequent layer as the input becomes smaller across the layers. And it
    increases back in the decoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final hyperparameter is the loss function used. If the input values are
    in [0,1] range binary cross-entropy is preferred, else mean squared error is used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have covered the hyperparameters used in training autoencoders. The training
    process is similar to the backpropagation which we have already covered in the
    last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to a few important applications of autoencoders in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6 Application of autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Autoencoders are capable of solving a number of problems inherent to unsupervised
    learning. Major applications for autoencoders include:'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is one of the major applications of autoencoders. It
    has been observed that sometimes autoencoders can learn more complex data projections
    than Principal Component analysis and other techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Anomaly detection is also an application of autoencoders. The error or the reconstruction
    error (error between the actual data and the reconstructed data) can be used to
    detect the anomalies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data compression is also thought as one of the major applications of autoencoders.
    But it has been observed that it is so difficult to beat the basic solutions like
    JPEG by training the algorithm. Moreover, since autoencoders are data specific,
    they can be used only the types of datasets they have been trained upon. If we
    wish to enhance the capacity to include more data types and make it more general,
    they the amount of the training data required will be too high, and obviously
    the time required will be high too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are other applications too like drug discovery, machine translation, image
    denoising etc. But there are still not a lot of practical implementations of autoencoders
    in the real world. This is because of multitude of reasons like the non-availability
    of data sets, infrastructure, readiness of various systems etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now proceed to the types of autoencoders in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 9.7 Types of autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are five main types of autoencoders. A brief description of the different
    types of encoders is given below. We have kept the section mathematically light
    and skipped the mathematics behind the scenes as it is quite complex to understand.
    For the curious readers, the link to understand the mathematics behind the scene
    is shared in the Further Reading section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Undercomplete autoencoders**: Undercomplete autoencoder is the simplest form
    of an autoencoder. It simply takes an input dataset and then reconstructs the
    same dataset again from the compressed bottleneck region. By penalizing the neural
    network as per the reconstruction error, the model will learn the most significant
    attributes of the data. By learning the most important attributes, the model will
    be able to reconstruct the original data from the compressed state. As we know
    that there is a loss when the compressed data is reconstructed, and this loss
    is called as *reconstruction* loss.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Undercomplete autoencoders are really unsupervised in nature as they do not
    have any target label to train. Such types of autoencoders are used for dimensionality
    reduction. Recall in chapter 2, we discussed the dimensionality reduction (PCA)
    and in chapter 6 we discussed the advanced dimensionality reduction algorithms
    (t-SNE and MDS).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.2 The performance starts to improve with more dimensions but decreases
    after sometime. Curse of dimensionality is a real problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![09_02](images/09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dimensionality reduction is possible using undercomplete autoencoders as the
    bottleneck is created which is the compressed form of the input data. This compressed
    data can be decompressed back with the aid of the network. Recall in chapter 3,
    we discussed that PCA provides a linear combination of the input variables. For
    more details and to refresh PCA, please refer to Chapter 3\. We know that PCA
    tries to get a low dimensional hyperplane to describe the original dataset, undercomplete
    autoencoders can also learn non-linear relationships. We have shown the difference
    in the Figure 9.3 below:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 PCA is a linear in nature while autoencoders are non-linear in nature.
    It is the core difference between the two algorithms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![09_03](images/09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, if all the non-linear activation functions are removed from the
    undercomplete autoencoder and only linear layers are used, it is equivalent to
    a PCA only. To make the autoencoder generalize and not memorize the training data,
    an undercomplete autoencoder is regulated and is fine-tuned by the size of the
    bottleneck. It allows the solution to not memorize the training data and generalize
    very well.
  prefs: []
  type: TYPE_NORMAL
- en: If a machine learning model works very well on the training data but does not
    work on the unseen test data, it is called as overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse autoencoders**: Sparse autoencoders are similar to undercomplete autoencoders
    except they use a different methodology to tackle overfitting. Conceptually, a
    sparse autoencoder changes the number of nodes at each of the hidden layer and
    keep it flexible. Now since it is not possible to have a neural network with such
    a capability of flexible number of neurons, the loss function is customized for
    it. In the loss function, a term is introduced which captures the number of activated
    neurons. There is one more term as the penalty term which is proportional to the
    number of activated neurons. The higher the number of activated neurons, the higher
    is the penalty. This penalty is called *sparsity function*. Using the penalty,
    it is possible to reduce the number of activated neurons, hence the penalty is
    lower and the network is able to tackle the issue of overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Contractive autoencoders**: Contractive autoencoders work on the similar
    concept as other autoencoders. They consider that the inputs which are quite same,
    should be encoded same. And hence, they should have same latent space representation.
    It means that there should not be much difference between the input data and the
    latent space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Denoising autoencoders**: Denoising means removing the noise and that is
    the precise task of denoising autoencoders. They do not take an image as an input,
    instead they take a noisy version of an image as an input as shown in the (Figure
    9.4) below.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.4 Original image, noisy output and the outputs from the autoencoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![09_04](images/09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: The process in denoising autoencoder is depicted below. The original image is
    changed by adding noise to it. This noisy image is fed to the encoder-decoder
    architecture and the output received is compared to the original image. The autoencoder
    learns the representation of the image which is used to remove the noise, and
    it is achieved by mapping the input image into a lower-dimensional manifold.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 The process of denoising in an autoencoder. It starts with original
    image, noise is added which results in a noisy image and then it is fed to the
    autoencoder.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![09_05](images/09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use denoising autoencoders for non-linear dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variational autoencoders**: In a standard autoencoder model, represent the
    input in a compressed form using bottleneck. They are probabilistic generative
    models and only need neural networks as a part of their overall structure. They
    are trained using expectation-maximization meta algorithms. The technical details
    are beyond the scope of this book.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now move to creating an autoencoder using Python in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 9.8 Python implementation of Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are creating two versions of autoencoder here. The code has been taken from
    the official source at Keras website ([https://blog.keras.io/building-autoencoders-in-keras.html](blog.keras.io.html))
    and has been modified for our usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: First we will import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: We are creating our network architecture here'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Add more details to the model'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: load the datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Create the train and test datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 6: fit the model now'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![09_05a](images/09_05a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 7: test it on the test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 8: and plot the results. You can see the original image and the final
    output.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![09_05b](images/09_05b.png)'
  prefs: []
  type: TYPE_IMG
- en: 9.9 Closing Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is a powerful tool. Having a sound business problem and a quality
    dataset, we can create a lot of innovative solutions. Autoencoders are one of
    the types of such solutions only.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we started with feature engineering which allows us to extract
    the most significant features from a dataset. Then we moved to autoencoders. Autoencoders
    are type of neural networks only used to learning efficient codings of unlabeled
    datasets. Autoencoders can be applied to many business problems like facial recognition,
    anamoly detection, image recognition, drug discovery, machine translation and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we covered autoencoders. In the next chapter, we are going to
    talk about Generative AI or GenAI which is a hot topic. That will be the tenth
    and penultimate chapter of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 9.10 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We studied feature engineering in this chapter. Feature engineering or feature
    extraction is the process of extracting features from the raw data to improve
    the overall quality of the model and enhance the accuracy as compared to a model
    where only raw data was fed to the machine learning model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then covered autoencoders in the next sections. We studied autoencoders in
    this chapter. Autoencoders are feed forward neural networks and they compress
    the input into a lower-dimensional code and then try to reconstruct the output
    from this representation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A typical architecture of autoencoder is an encoder, bottleneck and a decoder.
    Anamoly detection, data compression and dimensionality reductions are major usages
    of autoencoders.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We also covered different types of autoencoders like undercomplete, sparse,
    denoisining, contractive and variational autoencoders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical next steps and suggested readings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is a great blog here [https://blog.keras.io/building-autoencoders-in-keras.html](blog.keras.io.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the paper Transforming Auto-encoders by G.E.Hinton, A. Krizhevsky, S.D.Wang
    at [https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/capsules/transforming-autoencoders,-hinton,-icann-2011.pdf](capsules.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the paper Autoencoders by Dor Bank, Naom Koenigstein, Raja Giryes at [https://arxiv.org/abs/2003.05991](abs.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the paper An introduction to Autoencoders by Umberto Michelucci at [https://arxiv.org/abs/2201.03898](abs.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a good code and data set available at tensorflow official page at [https://www.tensorflow.org/tutorials/generative/autoencoder](generative.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
