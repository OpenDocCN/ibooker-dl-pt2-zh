- en: 12 Generative deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*This chapter covers*'
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepDream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural style transfer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The potential of artificial intelligence to emulate human thought processes
    goes beyond passive tasks such as object recognition and mostly reactive tasks
    such as driving a car. It extends well into creative activities. When I first
    made the claim that in the not-so-distant future, most of the cultural content
    that we consume will be created with substantial help from AIs, I was met with
    utter disbelief, even from long-time machine learning practitioners. That was
    in 2014\. Fast-forward a few years, and the disbelief had receded at an incredible
    speed. In the summer of 2015, we were entertained by Google’s DeepDream algorithm
    turning an image into a psychedelic mess of dog eyes and pareidolic artifacts;
    in 2016, we started using smartphone applications to turn photos into paintings
    of various styles. In the summer of 2016, an experimental short movie, *Sunspring*,
    was directed using a script written by a long short-term memory (LSTM). Maybe
    you’ve recently listened to music that was generated by a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Granted, the artistic productions we’ve seen from AI so far have been fairly
    low quality. AI isn’t anywhere close to rivaling human screenwriters, painters,
    and composers. But replacing humans was always beside the point: artificial intelligence
    isn’t about replacing our own intelligence with something else; it’s about bringing
    into our lives and work *more* intelligence—intelligence of a different kind.
    In many fields, but especially in creative ones, AI will be used by humans as
    a tool to augment their own capabilities: more *augmented intelligence* than *artificial
    intelligence*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A large part of artistic creation consists of simple pattern recognition and
    technical skill. And that’s precisely the part of the process that many find less
    attractive or even dispensable. That’s where AI comes in. Our perceptual modalities,
    our language, and our artwork all have statistical structure. Learning this structure
    is what deep learning algorithms excel at. Machine learning models can learn the
    statistical *latent space* of images, music, and stories, and they can then *sample*
    from this space, creating new artworks with characteristics similar to those the
    model has seen in its training data. Naturally, such sampling is hardly an act
    of artistic creation in itself. It’s a mere mathematical operation: the algorithm
    has no grounding in human life, human emotions, or our experience of the world;
    instead, it learns from an experience that has little in common with ours. It’s
    only our interpretation, as human spectators, that will give meaning to what the
    model generates. But in the hands of a skilled artist, algorithmic generation
    can be steered to become meaningful—and beautiful. Latent space sampling can become
    a brush that empowers the artist, augments our creative affordances, and expands
    the space of what we can imagine. What’s more, it can make artistic creation more
    accessible by eliminating the need for technical skill and practice, setting up
    a new medium of pure expression, factoring art apart from craft.'
  prefs: []
  type: TYPE_NORMAL
- en: Iannis Xenakis, a visionary pioneer of electronic and algorithmic music, beautifully
    expressed this same idea in the 1960s, in the context of the application of automation
    technology to music composition:^([1](#Rendnote1))
  prefs: []
  type: TYPE_NORMAL
- en: '*Freed from tedious calculations, the composer is able to devote himself to
    the general problems that the new musical form poses and to explore the nooks
    and crannies of this form while modifying the values of the input data. For example,
    he may test all instrumental combinations from soloists, to chamber orchestras,
    to large orchestras. With the aid of electronic computers the composer becomes
    a sort of pilot: he presses the buttons, introduces coordinates, and supervises
    the controls of a cosmic vessel sailing in the space of sound, across sonic constellations
    and galaxies that he could formerly glimpse only as a distant dream.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore from various angles the potential of deep learning
    to augment artistic creation. We’ll review sequence data generation (which can
    be used to generate text or music), DeepDream, and image generation using both
    variational autoencoders and generative adversarial networks. We’ll get your computer
    to dream up content never seen before; and maybe we’ll get you to dream, too,
    about the fantastic possibilities that lie at the intersection of technology and
    art. Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore how recurrent neural networks can be used to
    generate sequence data. We’ll use text generation as an example, but the exact
    same techniques can be generalized to any kind of sequence data: you could apply
    it to sequences of musical notes to generate new music, to time series of brushstroke
    data (perhaps recorded while an artist paints on an iPad) to generate paintings
    stroke by stroke, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence data generation is in no way limited to artistic content generation.
    It has been successfully applied to speech synthesis and to dialogue generation
    for chatbots. The Smart Reply feature that Google released in 2016, capable of
    automatically generating a selection of quick replies to emails or text messages,
    is powered by similar techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 A brief history of generative deep learning for sequence generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In late 2014, few people had ever seen the initials LSTM, even in the machine
    learning community. Successful applications of sequence data generation with recurrent
    networks began to appear in the mainstream only in 2016\. But these techniques
    have a fairly long history, starting with the development of the LSTM algorithm
    in 1997 (discussed in chapter 10). This new algorithm was used early on to generate
    text character by character.
  prefs: []
  type: TYPE_NORMAL
- en: In 2002, Douglas Eck, then at Schmidhuber’s lab in Switzerland, applied LSTM
    to music generation for the first time, with promising results. Eck is now a researcher
    at Google Brain, and in 2016, he started a new research group there, called Magenta,
    focused on applying modern deep learning techniques to produce engaging music.
    Sometimes good ideas take 15 years to get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the late 2000s and early 2010s, Alex Graves did important pioneering work
    using recurrent networks for sequence data generation. In particular, his 2013
    work on applying recurrent mixture density networks to generate human-like handwriting
    using time series of pen positions is seen by some as a turning point.^([2](#Rendnote2))
    This specific application of neural networks at that specific moment in time captured
    for me the notion of *machines that dream* and was a significant inspiration around
    the time I started developing Keras. Graves left a similar commented-out remark
    hidden in a 2013 LaTeX file uploaded to the preprint server arXiv: “Generating
    sequential data is the closest computers get to dreaming.” Several years later,
    we take a lot of these developments for granted, but at the time it was difficult
    to watch Graves’s demonstrations and not walk away awe-inspired by the possibilities.
    Between 2015 and 2017, recurrent neural networks were successfully used for text
    and dialogue generation, music generation, and speech synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: Then around 2017–2018, the Transformer architecture started taking over recurrent
    neural networks, not just for supervised natural language processing tasks but
    also for generative sequence models—in particular *language modeling* (word-level
    text generation). The best-known example of a generative Transformer would be
    GPT-3, a 175 billion parameter text-generation model trained by the startup OpenAI
    on an astound-ingly large text corpus, including most digitally available books,
    Wikipedia, and a large fraction of a crawl of the entire internet. GPT-3 made
    headlines in 2020 due to its capability to generate plausible-sounding text paragraphs
    on virtually any topic, a prowess that has fed a short-lived hype wave worthy
    of the most torrid AI summer.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 How do you generate sequence data?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The universal way to generate sequence data in deep learning is to train a
    model (usually a Transformer or an RNN) to predict the next token or next few
    tokens in a sequence, using the previous tokens as input. For instance, given
    the input “the cat is on the,” the model is trained to predict the target “mat,”
    the next word. As usual when working with text data, tokens are typically words
    or characters, and any network that can model the probability of the next token
    given the previous ones is called a *language model*. A language model captures
    the *latent space* of language: its statistical structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have such a trained language model, you can *sample* from it (generate
    new sequences): you feed it an initial string of text (called *conditioning data*),
    ask it to generate the next character or the next word (you can even generate
    several tokens at once), add the generated output back to the input data, and
    repeat the process many times (see [figure 12.1](#fig12-1)). This loop allows
    you to generate sequences of arbitrary length that reflect the structure of the
    data on which the model was trained: sequences that look *almost* like human-written
    sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0402-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.1 The process of word-by-word text generation using a language
    model**'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.3 The importance of the sampling strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When generating text, the way you choose the next token is crucially important.
    A naive approach is *greedy sampling*, consisting of always choosing the most
    likely next character. But such an approach results in repetitive, predictable
    strings that don’t look like coherent language. A more interesting approach makes
    slightly more surprising choices: it introduces randomness in the sampling process
    by sampling from the probability distribution for the next character. This is
    called *stochastic sampling* (recall that *stochasticity* is what we call *randomness*
    in this field). In such a setup, if a word has a 0.3 probability of being next
    in the sentence according to the model, you’ll choose it 30% of the time. Note
    that greedy sampling can also be cast as sampling from a probability distribution:
    one where a certain word has probability 1 and all others have probability 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sampling probabilistically from the softmax output of the model is neat: it
    allows even unlikely words to be sampled some of the time, generating more interesting-looking
    sentences and sometimes showing creativity by coming up with new, realistic-sounding
    sentences that didn’t occur in the training data. But there’s one issue with this
    strategy: it doesn’t offer a way to *control the amount of randomness* in the
    sampling process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Why would you want more or less randomness? Consider an extreme case: pure
    random sampling, where you draw the next word from a uniform probability distribution,
    and every word is equally likely. This scheme has maximum randomness; in other
    words, this probability distribution has maximum entropy. Naturally, it won’t
    produce anything interesting. At the other extreme, greedy sampling doesn’t produce
    anything interesting, either, and has no randomness: the corresponding probability
    distribution has minimum entropy. Sampling from the “real” probability distribution—
    the distribution that is output by the model’s softmax function—constitutes an
    intermediate point between these two extremes. But there are many other intermediate
    points of higher or lower entropy that you may want to explore. Less entropy will
    give the generated sequences a more predictable structure (and, thus, they will
    potentially be more realistic looking), whereas more entropy will result in more
    surprising and creative sequences. When sampling from generative models, it’s
    always good to explore different amounts of randomness in the generation process.
    Because we—humans— are the ultimate judges of how interesting the generated data
    is, interestingness is highly subjective, and there’s no telling in advance where
    the point of optimal entropy lies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To control the amount of stochasticity in the sampling process, we’ll introduce
    a parameter called the *softmax temperature*, which characterizes the entropy
    of the probability distribution used for sampling: it characterizes how surprising
    or predictable the choice of the next word will be. Given a temperature value,
    a new probability distribution is computed from the original one (the softmax
    output of the model) by reweighting it in the following way.'
  prefs: []
  type: TYPE_NORMAL
- en: Higher temperatures result in sampling distributions of higher entropy that
    will generate more surprising and unstructured generated data, whereas a lower
    temperature will result in less randomness and much more predictable generated
    data (see [figure 12.2](#fig12-2)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Reweighting a probability distribution to a different temperature
  prefs: []
  type: TYPE_NORMAL
- en: reweight_distribution <-
  prefs: []
  type: TYPE_NORMAL
- en: function(original_distribution, temperature = 0.5) {
  prefs: []
  type: TYPE_NORMAL
- en: original_distribution %>% .➊
  prefs: []
  type: TYPE_NORMAL
- en: '{ exp(log(.) / temperature) } %>%'
  prefs: []
  type: TYPE_NORMAL
- en: '{ . / sum(.) } ➋'
  prefs: []
  type: TYPE_NORMAL
- en: '} ➌'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **original_distribution is a 1D array of probability values that must sum
    to 1. temperature is a factor quantifying the entropy of the output distribution.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Return a reweighted version of the original distribution. The sum of the
    distribution may no longer be 1, so you divide it by its sum to obtain the new
    distribution.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Note that reweight_distribution() will work for both 1D R vectors and 1D
    Tensorflow tensors, because exp, log, /, and sum are all R generics.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0404-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.2 Different reweightings of one probability distribution: Low temperature
    = more deterministic; high temperature = more random**'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.4 Implementing text generation with Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s put these ideas into practice in a Keras implementation. The first thing
    you need is a lot of text data that you can use to learn a language model. You
    can use any sufficiently large text file or set of text files—Wikipedia, *The
    Lord of the Rings*, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’ll keep working with the IMDB movie review dataset from
    the last chapter, and we’ll learn to generate never-read-before movie reviews.
    As such, our language model will be a model of the style and topics of these movie
    reviews specifically, rather than a general model of the English language.
  prefs: []
  type: TYPE_NORMAL
- en: PREPARING THE DATA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like in the previous chapter, let’s download and uncompress the IMDB movie
    reviews dataset. (This is the same dataset we downloaded in chapter 11.)
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Downloading and uncompressing the IMDB movie reviews dataset
  prefs: []
  type: TYPE_NORMAL
- en: url <— "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
  prefs: []
  type: TYPE_NORMAL
- en: filename <— basename(url)
  prefs: []
  type: TYPE_NORMAL
- en: options(timeout = 60 * 10)➊
  prefs: []
  type: TYPE_NORMAL
- en: download.file(url, destfile = filename)
  prefs: []
  type: TYPE_NORMAL
- en: untar(filename)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **10-minute timeout**
  prefs: []
  type: TYPE_NORMAL
- en: 'You’re already familiar with the structure of the data: we get a folder named
    aclImdb containing two subfolders, one for negative-sentiment movie reviews and
    one for positive-sentiment reviews. There’s one text file per review. We’ll call
    text_dataset_ from_directory() with label_mode = NULL to create a TF Dataset that
    reads from these files and yields the text content of each file.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.3 Creating a TF Dataset from text files (one file = one sample)**'
  prefs: []
  type: TYPE_NORMAL
- en: library(tensorflow)
  prefs: []
  type: TYPE_NORMAL
- en: library(tfdatasets)
  prefs: []
  type: TYPE_NORMAL
- en: library(keras)
  prefs: []
  type: TYPE_NORMAL
- en: dataset <— text_dataset_from_directory(
  prefs: []
  type: TYPE_NORMAL
- en: directory = "aclImdb",
  prefs: []
  type: TYPE_NORMAL
- en: label_mode = NULL,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 256)
  prefs: []
  type: TYPE_NORMAL
- en: dataset <— dataset %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map( ~ tf$strings$regex_replace(.x, "<br />", " "))➊
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Strip the "<br />" HTML tag that occurs in many of the reviews. This did
    not matter much for text classification, but we wouldn't want to generate "<br
    />" tags in this example!**
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s use a layer_text_vectorization() to compute the vocabulary we’ll
    be working with. We’ll use only the first sequence_length words of each review:
    our layer_ text_vectorization() will cut off anything beyond that when vectorizing
    a text.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Preparing a layer_text_vectorization()
  prefs: []
  type: TYPE_NORMAL
- en: sequence_length <— 100
  prefs: []
  type: TYPE_NORMAL
- en: vocab_size <— 15000➊
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization <— layer_text_vectorization(
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens = vocab_size,
  prefs: []
  type: TYPE_NORMAL
- en: output_mode = "int",➋
  prefs: []
  type: TYPE_NORMAL
- en: output_sequence_length = sequence_length➌
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: adapt(text_vectorization, dataset)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We'll consider only the top 15,000 most common words—anything else will
    be treated as the out-of-vocabulary token, "[UNK]".**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We want to return integer word index sequences.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **We'll work with inputs and targets of length 100 (but because we'll offset
    the targets by 1, the model will actually see sequences of length 99).**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the layer to create a language-modeling dataset where input samples
    are vectorized texts and corresponding targets are the same texts offset by one
    word.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.5 Setting up a language-modeling dataset
  prefs: []
  type: TYPE_NORMAL
- en: prepare_lm_dataset <— function(text_batch) {
  prefs: []
  type: TYPE_NORMAL
- en: vectorized_sequences <— text_vectorization(text_batch)➊
  prefs: []
  type: TYPE_NORMAL
- en: x <— vectorized_sequences[, NA:-2]➋
  prefs: []
  type: TYPE_NORMAL
- en: y <— vectorized_sequences[, 2:NA]➌
  prefs: []
  type: TYPE_NORMAL
- en: list(x, y)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: lm_dataset <— dataset %>%
  prefs: []
  type: TYPE_NORMAL
- en: dataset_map(prepare_lm_dataset, num_parallel_calls = 4)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Convert a batch of texts (strings) to a batch of integer sequences.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Create inputs by cutting off the last word of the sequences (drop last column).**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Create targets by offsetting the sequences by 1 (drop first column).**
  prefs: []
  type: TYPE_NORMAL
- en: A TRANSFORMER-BASED SEQUENCE-TO-SEQUENCE MODEL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll train a model to predict a probability distribution over the next word
    in a sentence, given a number of initial words. When the model is trained, we’ll
    feed it with a prompt, sample the next word, add that word back to the prompt,
    and repeat, until we’ve generated a short paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Like we did for temperature forecasting in chapter 10, we could train a model
    that takes as input a sequence of *N* words and simply predicts word *N* +1\.
    However, several issues exist with this setup in the context of sequence generation.
  prefs: []
  type: TYPE_NORMAL
- en: First, the model would learn to produce predictions only when *N* words were
    available, but it would be useful to be able to start predicting with fewer than
    *N* words. Otherwise, we’d be constrained to using only relatively long prompts
    (in our implementation, *N* = 100 words). We didn’t have this need in chapter
    10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, many of our training sequences will be mostly overlapping. Consider
    N = 4. The text “A complete sentence must have, at minimum, three things: a subject,
    verb, and an object” would be used to generate the following training sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: “A complete sentence must”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “complete sentence must have”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “sentence must have at”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “and so on, until “verb and an object”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model that treats each such sequence as an independent sample would have to
    do a lot of redundant work, re-encoding multiple times subsequences that it has
    largely seen before. In chapter 10, this wasn’t much of a problem, because we
    didn’t have that many training samples in the first place, and we needed to benchmark
    dense and convolutional models, for which redoing the work every time is the only
    option. We could try to alleviate this redundancy problem by using *strides* to
    sample our sequences— skipping a few words between two consecutive samples. But
    that would reduce our number of training samples while providing only a partial
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these two issues, we’ll use a *sequence-to-sequence model*: we’ll
    feed sequences of *N* words (indexed from *1* to *N*) into our model, and we’ll
    predict the sequence offset by one (from *2* to *N+1*). We’ll use causal masking
    to make sure that, for any *i*, the model will use only words from *1* to *i*
    to predict the word *i + 1*. This means that we’re simultaneously training the
    model to solve *N* mostly overlapping but different problems: predicting the next
    words given a sequence of 1 <= i <= N prior words (see [figure 12.3](#fig12-3)).
    At generation time, even if you prompt the model with only a single word, it will
    be able to give you a probability distribution for the next possible words.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0407-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.3 Compared to plain next-word prediction, sequence-to-sequence
    modeling simultaneously optimizes for multiple prediction problems.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we could have used a similar sequence-to-sequence setup on our temperature-forecasting
    problem in chapter 10: given a sequence of 120 hourly data points, learn to generate
    a sequence of 120 temperatures offset by 24 hours in the future. You’d be solving
    not only the initial problem but also the 119 related problems of forecasting
    temperature in 24 hours, given 1 <= i < 120 prior hourly data points. If you try
    to retrain the RNNs from chapter 10 in a sequence-to-sequence setup, you’ll find
    that you get similar but incrementally worse results, because the constraint of
    solving these additional 119 related problems with the same model interferes slightly
    with the task we actually do care about.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, you learned about the setup you can use for sequence-to-sequence
    learning in the general case: feed the source sequence into an encoder, and then
    feed both the encoded sequence and the target sequence into a decoder that tries
    to predict the same target sequence, offset by one step. When you’re doing text
    generation, there is no source sequence: you’re just trying to predict the next
    tokens in the target sequence given past tokens, which we can do using only the
    decoder. And thanks to causal padding, the decoder will look at only words *1…N*
    to predict the word *N+1*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement our model—we’re going to reuse the building blocks we created
    in chapter 11: layer_positional_embedding() and layer_transformer_decoder().'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.6 A simple Transformer-based language model
  prefs: []
  type: TYPE_NORMAL
- en: embed_dim <- 256
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <- 2048
  prefs: []
  type: TYPE_NORMAL
- en: num_heads <- 2
  prefs: []
  type: TYPE_NORMAL
- en: transformer_decoder <-
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_decoder(NULL, embed_dim, latent_dim, num_heads)
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  prefs: []
  type: TYPE_NORMAL
- en: transformer_decoder(., .) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(vocab_size, activation = "softmax")➊
  prefs: []
  type: TYPE_NORMAL
- en: model <—
  prefs: []
  type: TYPE_NORMAL
- en: keras_model(inputs, outputs) %>%
  prefs: []
  type: TYPE_NORMAL
- en: compile(loss = "sparse_categorical_crossentropy",
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = "rmsprop")
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Softmax over possible vocabulary words, computed for each output sequence
    time step.**
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.5 A text-generation callback with variable-temperature sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll use a callback to generate text using a range of different temperatures
    after every epoch. This allows you to see how the generated text evolves as the
    model begins to converge, as well as the impact of temperature in the sampling
    strategy. To seed text generation, we’ll use the prompt “this movie”: all of our
    generated texts will start with this.'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s define some functions to generate sentences. Later, we’ll use these
    functions in a callback.
  prefs: []
  type: TYPE_NORMAL
- en: vocab <— get_vocabulary(text_vectorization) ➊
  prefs: []
  type: TYPE_NORMAL
- en: sample_next <— function(predictions, temperature = 1.0) {
  prefs: []
  type: TYPE_NORMAL
- en: predictions %>%
  prefs: []
  type: TYPE_NORMAL
- en: reweight_distribution(temperature) %>% ➋
  prefs: []
  type: TYPE_NORMAL
- en: sample.int(length(.), 1, prob = .)➌
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: generate_sentence <—
  prefs: []
  type: TYPE_NORMAL
- en: function(model, prompt, generate_length, temperature) {
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— prompt➍
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(generate_length)) {➎
  prefs: []
  type: TYPE_NORMAL
- en: model_preds <— sentence %>%
  prefs: []
  type: TYPE_NORMAL
- en: array(dim = c(1, 1)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization() %>%
  prefs: []
  type: TYPE_NORMAL
- en: predict(model, .) ➏
  prefs: []
  type: TYPE_NORMAL
- en: sampled_word <— model_preds %>%
  prefs: []
  type: TYPE_NORMAL
- en: .[1, i, ] %>%➐
  prefs: []
  type: TYPE_NORMAL
- en: sample_next(temperature) %>%➑
  prefs: []
  type: TYPE_NORMAL
- en: vocab[.]➒
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— paste(sentence, sampled_word)➓
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sentence
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The vector we will use to convert word indices (integers) back to strings,
    to be used for text decoding**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The temperature to use for sampling**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Implement variable-temperature sampling from a probability distribution.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Prompt that we use to seed text generation**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Iterate for how many words to generate.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Feed the current sequence into our model.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Retrieve the predictions for the last time step…**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **…and use them to sample a new token…**
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **…and convert the token integer to a string.**
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Append the new word to the current sequence and repeat.**
  prefs: []
  type: TYPE_NORMAL
- en: sample_next() and generate_sentence() do the work of generating sentences from
    a model. They work eagerly; they call predict() to produce predictions as R arrays,
    call sample.int() to pick the next token, and build up the sentence as an R string
    with paste().
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we may want to generate many sentences, it makes sense to optimize
    it a little. We can speed up generate_sentence considerably (~25x) by rewriting
    it as a tf_function(). To do this, we just need to replace a few R functions with
    TensorFlow equivalents. Instead of for(i in seq()), we can write for(i in tf$range()).
    We can also substitute sample.int() with tf$random$categorical(), paste() with
    tf$strings$join(), and predict(model, .) with model(.). Here is what sample_ next()
    and generate_sentence() look like as tf_function()s:'
  prefs: []
  type: TYPE_NORMAL
- en: tf_sample_next <— function(predictions, temperature = 1.0) {
  prefs: []
  type: TYPE_NORMAL
- en: predictions %>%
  prefs: []
  type: TYPE_NORMAL
- en: reweight_distribution(temperature) %>%
  prefs: []
  type: TYPE_NORMAL
- en: '{ log(.[tf$newaxis, ]) } %>% ➊'
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$categorical(1L) %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$reshape(shape())➋
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: library(tfautograph)➌
  prefs: []
  type: TYPE_NORMAL
- en: tf_generate_sentence <— tf_function(
  prefs: []
  type: TYPE_NORMAL
- en: function(model, prompt, generate_length, temperature) {
  prefs: []
  type: TYPE_NORMAL
- en: withr::local_options(tensorflow.extract.style = "python")
  prefs: []
  type: TYPE_NORMAL
- en: vocab <— as_tensor(vocab)
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— prompt %>% as_tensor(shape = c(1, 1))
  prefs: []
  type: TYPE_NORMAL
- en: ag_loop_vars(sentence)➍
  prefs: []
  type: TYPE_NORMAL
- en: for (i in tf$range(generate_length)) {
  prefs: []
  type: TYPE_NORMAL
- en: model_preds <— sentence %>%
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization() %>%
  prefs: []
  type: TYPE_NORMAL
- en: model()
  prefs: []
  type: TYPE_NORMAL
- en: sampled_word <— model_preds %>%
  prefs: []
  type: TYPE_NORMAL
- en: .[0, i, ] %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf_sample_next(temperature) %>%
  prefs: []
  type: TYPE_NORMAL
- en: vocab[.]
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— sampled_word %>%
  prefs: []
  type: TYPE_NORMAL
- en: '{ tf$strings$join(c(sentence, .), " ") }'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sentence %>% tf$reshape(shape())➎
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **tf$random$catagorical() expects a batch of log probabilities.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **tf$random$catagorical() returns a scalar integer with shape (1, 1). Reshape
    to shape ().**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **For ag_loop_vars() (more on this soon)**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Provide a hint to the compiler that `sentence` is the only variable we want
    after iteration.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Reshape from (1, 1) to (). Note that tf$strings$join() preserves sentence's
    (1, 1) shape throughout iteration.**
  prefs: []
  type: TYPE_NORMAL
- en: On my machine, generating a sentence of 50 words takes approx 2.5 seconds with
    the eager generate_sentence(), and .1 seconds with tf_generate_sentence(), a 25×
    improvement! Remember, it always makes sense to prototype your code first by running
    it eagerly, and only move to using tf_function() once you have it working how
    you want.
  prefs: []
  type: TYPE_NORMAL
- en: '**for loops and autograph**'
  prefs: []
  type: TYPE_NORMAL
- en: 'One wrinkle with evaluating R functions eagerly before wrapping them with tf_
    function(fn, autograph = TRUE) (the default) is that autograph = TRUE gives capabilities
    that base R doesn’t have, like the ability for for to iterate over tensors. You
    can still evaluate expressions like for(i in tf$range()) or for(batch in tf_ dataset)
    eagerly by calling tfautograph::autograph() directly, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: library(tfautograph)
  prefs: []
  type: TYPE_NORMAL
- en: autograph({
  prefs: []
  type: TYPE_NORMAL
- en: for(i in tf$range(3L))
  prefs: []
  type: TYPE_NORMAL
- en: print(i)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(0, shape=(), dtype=int32)
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(1, shape=(), dtype=int32)
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(2, shape=(), dtype=int32)
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: fn <— function(x) {
  prefs: []
  type: TYPE_NORMAL
- en: for(i in x) print(i)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ag_fn <— autograph(fn)
  prefs: []
  type: TYPE_NORMAL
- en: ag_fn(tf$range(3))
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(0.0, shape=(), dtype=float32)
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(1.0, shape=(), dtype=float32)
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(2.0, shape=(), dtype=float32)
  prefs: []
  type: TYPE_NORMAL
- en: In interactive sessions you can temporarily globally enable if, while, and for
    to accept tensors by calling tfautograph:::attach_ag_mask().
  prefs: []
  type: TYPE_NORMAL
- en: A for() loop that iterates over a tensor in a tf_function() builds a tf$while_loop(),
    and it inherits all same restrictions. Every tensor tracked by the loop must have
    a stable shape and dtype throughout iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The call ag_loop_vars(sentence) gives the tf_function() compiler a hint that
    the only variable we’re interested in after the for loop is sentence. This informs
    the compiler that other tensors, like sampled_word, i, and model_preds, are loop-local
    variables and can be safely optimized away after the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Note that iterating over a regular R object like for(i in seq(0, 49)) in a tf_
    function() would not build a tf$while_loop(), but would instead evaluate with
    regular R semantics and would result in the tf_function() tracing an unrolled
    loop (which is sometimes preferable, for short loops with a fixed number of iterations).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the callback where we’ll call tf_generate_sentence() to generate text
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.7 The text-generation callback
  prefs: []
  type: TYPE_NORMAL
- en: callback_text_generator <— new_callback_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "TextGenerator",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(prompt, generate_length,
  prefs: []
  type: TYPE_NORMAL
- en: temperatures = 1,
  prefs: []
  type: TYPE_NORMAL
- en: print_freq = 1L) {
  prefs: []
  type: TYPE_NORMAL
- en: private$prompt <— as_tensor(prompt, "string")
  prefs: []
  type: TYPE_NORMAL
- en: private$generate_length <— as_tensor(generate_length, "int32")
  prefs: []
  type: TYPE_NORMAL
- en: private$temperatures <— as.numeric(temperatures)➊
  prefs: []
  type: TYPE_NORMAL
- en: private$print_freq <— as.integer(print_freq)
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_end = function(epoch, logs = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: if ((epoch %% private$print_freq) != 0)
  prefs: []
  type: TYPE_NORMAL
- en: return()
  prefs: []
  type: TYPE_NORMAL
- en: for (temperature in private$temperatures) {➋
  prefs: []
  type: TYPE_NORMAL
- en: cat("== Generating with temperature", temperature, "\n")
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— tf_generate_sentence(➌
  prefs: []
  type: TYPE_NORMAL
- en: self$model,
  prefs: []
  type: TYPE_NORMAL
- en: private$prompt,
  prefs: []
  type: TYPE_NORMAL
- en: private$generate_length,➍
  prefs: []
  type: TYPE_NORMAL
- en: as_tensor(temperature, "float32")
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: cat(as.character(sentence), "\n")
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: text_gen_callback <— callback_text_generator(
  prefs: []
  type: TYPE_NORMAL
- en: prompt = "This movie",
  prefs: []
  type: TYPE_NORMAL
- en: generate_length = 50,
  prefs: []
  type: TYPE_NORMAL
- en: temperatures = c(0.2, 0.5, 0.7, 1., 1.5) ➎
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We'll use a diverse range of temperatures to sample text, to demonstrate
    the effect of temperature on text generation.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **This is a regular R for loop iterating eagerly over an R vector.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Note we call this function with only tensors and the model, not R numeric
    or character vectors.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **These were already cast to Tensors in initialize().**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The set of temperatures we generate text with**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s fit() this thing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.8 Fitting the language model**'
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  prefs: []
  type: TYPE_NORMAL
- en: fit(lm_dataset,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 200,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = list(text_gen_callback))
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some cherry-picked examples of what we’re able to generate after 200
    epochs of training. Note that punctuation isn’t part of our vocabulary, so none
    of our generated text has any punctuation:'
  prefs: []
  type: TYPE_NORMAL
- en: With temperature=0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a [UNK] of the original movie and the first half hour of the
    movie is pretty good but it is a very good movie it is a good movie for the time
    period”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a [UNK] of the movie it is a movie that is so bad that it is
    a [UNK] movie it is a movie that is so bad that it makes you laugh and cry at
    the same time it is not a movie i dont think ive ever seen”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a [UNK] of the best genre movies of all time and it is not a
    good movie it is the only good thing about this movie i have seen it for the first
    time and i still remember it being a [UNK] movie i saw a lot of years”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a waste of time and money i have to say that this movie was a
    complete waste of time i was surprised to see that the movie was made up of a
    good movie and the movie was not very good but it was a waste of time and”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=0.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is fun to watch and it is really funny to watch all the characters
    are extremely hilarious also the cat is a bit like a [UNK] [UNK] and a hat [UNK]
    the rules of the movie can be told in another scene saves it from being in the
    back of”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is about [UNK] and a couple of young people up on a small boat in
    the middle of nowhere one might find themselves being exposed to a [UNK] dentist
    they are killed by [UNK] i was a huge fan of the book and i havent seen the original
    so it”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie was entertaining i felt the plot line was loud and touching but
    on a whole watch a stark contrast to the artistic of the original we watched the
    original version of england however whereas arc was a bit of a little too ordinary
    the [UNK] were the present parent [UNK]”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie was a masterpiece away from the storyline but this movie was simply
    exciting and frustrating it really entertains friends like this the actors in
    this movie try to go straight from the sub thats image and they make it a really
    good tv show”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=1.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie was possibly the worst film about that 80 women its as weird insightful
    actors like barker movies but in great buddies yes no decorated shield even [UNK]
    land dinosaur ralph ian was must make a play happened falls after miscast [UNK]
    bach not really not wrestlemania seriously sam didnt exist”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie could be so unbelievably lucas himself bringing our country wildly
    funny things has is for the garish serious and strong performances colin writing
    more detailed dominated but before and that images gears burning the plate patriotism
    we you expected dyan bosses devotion to must do your own duty and another”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, a low temperature value results in very boring and repetitive
    text and can sometimes cause the generation process to get stuck in a loop. With
    higher temperatures, the generated text becomes more interesting, surprising,
    and even creative. With a very high temperature, the local structure starts to
    break down, and the output looks largely random. Here, a good generation temperature
    would seem to be about 0.7\. Always experiment with multiple sampling strategies!
    A clever balance between learned structure and randomness is what makes generation
    interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that by training a bigger model, longer, on more data, you can achieve
    generated samples that look far more coherent and realistic than this one—the
    output of a model like GPT-3 is a good example of what can be done with language
    models (GPT-3 is effectively the same thing as what we trained in this example,
    but with a deep stack of Transformer decoders, and a much bigger training corpus).
    But don’t expect to ever generate any meaningful text, other than through random
    chance and the magic of your own interpretation: all you’re doing is sampling
    data from a statistical model of which words come after which words. Language
    models are all form and no substance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Natural language is many things: a communication channel; a way to act on the
    world; a social lubricant; a way to formulate, store, and retrieve your own thoughts.
    These uses of languages are where its meaning originates. A deep learning “language
    model,” despite its name, captures effectively none of these fundamental aspects
    of language. It cannot communicate (it has nothing to communicate about and no
    one to communicate with), it cannot act on the world (it has no agency and no
    intent), it cannot be social, and it doesn’t have any thoughts to process with
    the help of words. Language is the operating system of the mind, and so, for language
    to be meaningful, it needs a mind to leverage it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What a language model does is capture the statistical structure of the observable
    artifacts—books, online movie reviews, tweets—that we generate as we use language
    to live our lives. The fact that these artifacts have a statistical structure
    at all is a side effect of how humans implement language. Here’s a thought experiment:
    what if our languages did a better job of compressing communications, much like
    computers do with most digital communications? Language would be no less meaningful
    and could still fulfill its many purposes, but it would lack any intrinsic statistical
    structure, thus making it impossible to model as you just did.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.6 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can generate discrete sequence data by training a model to predict the next
    token(s), given previous tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of text, such a model is called a *language model*. It can be based
    on either words or characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling the next token requires a balance between adhering to what the model
    judges likely, and introducing randomness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to handle this is the notion of softmax temperature. Always experiment
    with different temperatures to find the right one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.2 DeepDream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*DeepDream* is an artistic image-modification technique that uses the representations
    learned by convolutional neural networks. It was first released by Google in the
    summer of 2015 as an implementation written using the Caffe deep learning library
    (this was several months before the first public release of TensorFlow).^([3](#Rendnote3))
    It quickly became an internet sensation thanks to the trippy pictures it could
    generate (see, for example, [figure 12.4](#fig12-4)), full of algorithmic pareidolia
    artifacts, bird feathers, and dog eyes—a byproduct of the fact that the DeepDream
    convnet was trained on ImageNet, where dog breeds and bird species are vastly
    overrepresented.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0414-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.4 Example of a DeepDream output image**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The DeepDream algorithm is almost identical to the convnet filter-visualization
    technique introduced in chapter 9, consisting of running a convnet in reverse:
    doing gradient ascent on the input to the convnet to maximize the activation of
    a specific filter in an upper layer of the convnet. DeepDream uses this same idea,
    with a few simple differences:'
  prefs: []
  type: TYPE_NORMAL
- en: With DeepDream, you try to maximize the activation of entire layers rather than
    that of a specific filter, thus mixing together visualizations of large numbers
    of features at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You start not from blank, slightly noisy input, but rather from an existing
    image—thus the resulting effects latch on to preexisting visual patterns, distorting
    elements of the image in a somewhat artistic fashion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input images are processed at different scales (called *octaves*), which
    improves the quality of the visualizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s make some DeepDreams.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Implementing DeepDream in Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start by retrieving a test image to dream with. We’ll use a view of the
    rugged Northern California coast in the winter ([figure 12.5](#fig12-5)).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.9 Fetching the test image
  prefs: []
  type: TYPE_NORMAL
- en: base_image_path <— get_file(
  prefs: []
  type: TYPE_NORMAL
- en: '"coast.jpg", origin = "https://img-datasets.s3.amazonaws.com/coast.jpg")'
  prefs: []
  type: TYPE_NORMAL
- en: plot(as.raster(jpeg::readJPEG(base_image_path)))
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0415-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.15 Our test image**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need a pretrained convnet. In Keras, many such convnets are available—
    VGG16, VGG19, Xception, ResNet50, and so on—all with weights pretrained on ImageNet.
    You can implement DeepDream with any of them, but your base model of choice will
    naturally affect your visualizations, because different architectures result in
    different learned features. The convnet used in the original DeepDream release
    was an Inception model, and in practice, Inception is known to produce nice-looking
    DeepDreams, so we’ll use the Inception V3 model that comes with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.10 Instantiating a pretrained InceptionV3 model**'
  prefs: []
  type: TYPE_NORMAL
- en: model <— application_inception_v3(weights = "imagenet", include_top = FALSE)
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use our pretrained convnet to create a feature extractor model that returns
    the activations of the various intermediate layers, listed in the following code.
    For each layer, we pick a scalar score that weights the contribution of the layer
    to the loss we will seek to maximize during the gradient-ascent process. If you
    want a complete list of layer names that you can use to pick new layers to play
    with, just use print(model).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.11 Configuring the contribution of each layer to the DeepDream loss
  prefs: []
  type: TYPE_NORMAL
- en: layer_settings <— c( ➊
  prefs: []
  type: TYPE_NORMAL
- en: mixed4 = 1,
  prefs: []
  type: TYPE_NORMAL
- en: mixed5 = 1.5,
  prefs: []
  type: TYPE_NORMAL
- en: mixed6 = 2,
  prefs: []
  type: TYPE_NORMAL
- en: mixed7 = 2.5
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: outputs <— list()
  prefs: []
  type: TYPE_NORMAL
- en: for(layer_name in names(layer_settings))
  prefs: []
  type: TYPE_NORMAL
- en: outputs[[layer_name]] <—➋
  prefs: []
  type: TYPE_NORMAL
- en: get_layer(model, layer_name)$output➋
  prefs: []
  type: TYPE_NORMAL
- en: feature_extractor <— keras_model(inputs = model$inputs,➌
  prefs: []
  type: TYPE_NORMAL
- en: outputs = outputs)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Layers for which we try to maximize activation, as well as their weight
    in the total loss. You can tweak these setting to obtain new visual effects.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Collect in a named list the output symbolic tensor from each layer.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **A model that returns the activation values for every target layer (as a
    named list)**
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll compute the *loss*: the quantity we’ll seek to maximize during
    the gradient-ascent process at each processing scale. In chapter 9, for filter
    visualization, we tried to maximize the value of a specific filter in a specific
    layer. Here, we’ll simultaneously maximize the activation of all filters in a
    number of layers. Specifically, we’ll maximize a weighted mean of the L2 norm
    of the activations of a set of high-level layers. The exact set of layers we choose
    (as well as their contribution to the final loss) has a major influence on the
    visuals we’ll be able to produce, so we want to make these parameters easily configurable.
    Lower layers result in geometric patterns, whereas higher layers result in visuals
    in which you can recognize some classes from ImageNet (e.g., birds or dogs). We’ll
    start from a somewhat arbitrary configuration involving four layers, but you’ll
    definitely want to explore many different configurations later.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.12 The DeepDream loss
  prefs: []
  type: TYPE_NORMAL
- en: compute_loss <— function(input_image) {
  prefs: []
  type: TYPE_NORMAL
- en: features <— feature_extractor(input_image)
  prefs: []
  type: TYPE_NORMAL
- en: feature_losses <— names(features) %>%
  prefs: []
  type: TYPE_NORMAL
- en: lapply(function(name) {
  prefs: []
  type: TYPE_NORMAL
- en: coeff <— layer_settings[[name]]
  prefs: []
  type: TYPE_NORMAL
- en: activation <— features[[name]]➊
  prefs: []
  type: TYPE_NORMAL
- en: coeff * mean(activation[, 3:-3, 3:-3, ] ^ 2)➋
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce(`+`, feature_losses)➌
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Extract activations.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We avoid border artifacts by involving only nonborder pixels in the loss.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **feature_losses is a list of scalar tensors. Sum up the loss from each feature.**
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s set up the gradient-ascent process that we will run at each octave.
    You’ll recognize that it’s the same thing as the filter-visualization technique
    from chapter 9! The DeepDream algorithm is simply a multiscale form of filter
    visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.13 The DeepDream gradient-ascent process**'
  prefs: []
  type: TYPE_NORMAL
- en: gradient_ascent_step <— tf_function(➊
  prefs: []
  type: TYPE_NORMAL
- en: function(image, learning_rate) {
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: tape$watch(image)
  prefs: []
  type: TYPE_NORMAL
- en: loss <— compute_loss(image)➋
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(loss, image) %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$math$l2_normalize()➌
  prefs: []
  type: TYPE_NORMAL
- en: image %<>% `+`(learning_rate * grads)➍
  prefs: []
  type: TYPE_NORMAL
- en: list(loss, image)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: gradient_ascent_loop <—➎
  prefs: []
  type: TYPE_NORMAL
- en: function(image, iterations, learning_rate, max_loss = -Inf) {
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate %<>% as_tensor()
  prefs: []
  type: TYPE_NORMAL
- en: for(i in seq(iterations)) {➏
  prefs: []
  type: TYPE_NORMAL
- en: c(loss, image) %<—% gradient_ascent_step(image, learning_rate)
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% as.numeric()
  prefs: []
  type: TYPE_NORMAL
- en: if(loss > max_loss)➐
  prefs: []
  type: TYPE_NORMAL
- en: break
  prefs: []
  type: TYPE_NORMAL
- en: writeLines(sprintf(
  prefs: []
  type: TYPE_NORMAL
- en: '"… Loss value at step %i: %.2f", i, loss))'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: image
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We make the training step fast by compiling it as a tf_function().**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Compute gradients of DeepDream loss with respect to the current image.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Normalize gradients (the same trick we used in chapter 9).**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Repeatedly update the image in a way that increases the DeepDream loss.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **This runs gradient ascent for a given image scale (octave).**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **This is a regular eager R for loop.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Break out if the loss crosses a certain threshold (overoptimizing would
    create unwanted image artifacts).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the outer loop of the DeepDream algorithm. First, we’ll define a list
    of *scales* (also called *octaves*) at which to process the images. We’ll process
    our image over three different such octaves. For each successive octave, from
    the smallest to the largest, we’ll run 20 gradient ascent steps via gradient_ascent_loop()
    to maximize the loss we previously defined. Between each octave, we’ll upscale
    the image by 40% (1.4×): we’ll start by processing a small image and then increasingly
    scale it up (see [figure 12.6](#fig12-6)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0418-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.6 The DeepDream process: Successive scales of spatial processing
    (octaves) and detail reinjection upon upscaling**'
  prefs: []
  type: TYPE_NORMAL
- en: We define the parameters of this process in the following code. Tweaking these
    parameters will allow you to achieve new effects!
  prefs: []
  type: TYPE_NORMAL
- en: step <— 20➊
  prefs: []
  type: TYPE_NORMAL
- en: num_octaves <— 3➋
  prefs: []
  type: TYPE_NORMAL
- en: octave_scale <— 1.4➌
  prefs: []
  type: TYPE_NORMAL
- en: iterations <— 30➍
  prefs: []
  type: TYPE_NORMAL
- en: max_loss <— 15➎
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Gradient ascent step size**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Number of scales at which to run gradient ascent**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Size ratio between successive scales**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Number of gradient ascent steps per scale**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **We’ll stop the gradient-ascent process for a scale if the loss gets higher
    than this.**
  prefs: []
  type: TYPE_NORMAL
- en: We’re also going to need a couple of utility functions to load and save images.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.14 Image processing utilities**'
  prefs: []
  type: TYPE_NORMAL
- en: preprocess_image <— tf_function(function(image_path) {➊
  prefs: []
  type: TYPE_NORMAL
- en: image_path %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$read_file() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$decode_image() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$expand_dims(axis = 0L) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: tf$cast("float32") %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: inception_v3_preprocess_input()
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: deprocess_image <— tf_function(function(img) {➍
  prefs: []
  type: TYPE_NORMAL
- en: img %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$squeeze(axis = 0L) %>%➎
  prefs: []
  type: TYPE_NORMAL
- en: '{ (. * 127.5) + 127.5 } %>%➏'
  prefs: []
  type: TYPE_NORMAL
- en: tf$saturate_cast("uint8")➐
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor <— function(x, …, max = 255,
  prefs: []
  type: TYPE_NORMAL
- en: plot_margins = c(0, 0, 0, 0)) {
  prefs: []
  type: TYPE_NORMAL
- en: if (!is.null(plot_margins))
  prefs: []
  type: TYPE_NORMAL
- en: withr::local_par(mar = plot_margins)➑
  prefs: []
  type: TYPE_NORMAL
- en: x %>%
  prefs: []
  type: TYPE_NORMAL
- en: as.array() %>%➒
  prefs: []
  type: TYPE_NORMAL
- en: drop() %>%
  prefs: []
  type: TYPE_NORMAL
- en: as.raster(max = max) %>%➓
  prefs: []
  type: TYPE_NORMAL
- en: plot(…, interpolate = FALSE)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Util function to load, resize, and format pictures into appropriate arrays**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Add batch axis, equivalent to .[tf$newaxis, all_dims()]. axis arg is 0-based.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Cast from 'uint8'.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Util function to convert a tensor array into a valid image and undo preprocessing**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Drop first dim-the batch axis (must be size 1), the inverse of tf$expand_dims().**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Rescale so values in [-1, 1] are remapped to [0, 255].**
  prefs: []
  type: TYPE_NORMAL
- en: '➐ **saturate_case() clips values to the dtype range: [0, 255].**'
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Default to no margins when plotting images.**
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Convert tensors to R arrays.**
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Convert to R native raster format.**
  prefs: []
  type: TYPE_NORMAL
- en: '**withr::local_***'
  prefs: []
  type: TYPE_NORMAL
- en: Here we use withr::local_par() to set par() before calling plot(). local_ par()
    acts just like par(), except that it restores the previous par() settings when
    the function exits. Using funcions like local_par() or local_options() helps ensure
    that functions you write don’t permanently modify global state, which makes them
    more predictable and usable in more contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can replace local_par() and do the equivalent with a separate on.exit()
    call like this:'
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor <— function()
  prefs: []
  type: TYPE_NORMAL
- en: <…>
  prefs: []
  type: TYPE_NORMAL
- en: opar <— par(mar = plot_margins)
  prefs: []
  type: TYPE_NORMAL
- en: on.exit(par(opar))
  prefs: []
  type: TYPE_NORMAL
- en: <…>
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the outer loop. To avoid losing a lot of image detail after each successive
    scale-up (resulting in increasingly blurry or pixelated images), we can use a
    simple trick: after each scale-up, we’ll reinject the lost details back into the
    image, which is possible because we know what the original image should look like
    at the larger scale. Given a small image size *S* and a larger image size *L*,
    we can compute the difference between the original image resized to size *L* and
    the original resized to size *S*—this difference quantifies the details lost when
    going from *S* to *L*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.15 Running gradient ascent over multiple successive octaves**'
  prefs: []
  type: TYPE_NORMAL
- en: original_img <— preprocess_image(base_image_path)➊
  prefs: []
  type: TYPE_NORMAL
- en: original_HxW <— dim(original_img)[2:3]
  prefs: []
  type: TYPE_NORMAL
- en: calc_octave_HxW <— function(octave) {
  prefs: []
  type: TYPE_NORMAL
- en: as.integer(round(original_HxW / (octave_scale ^ octave)))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: octaves <— seq(num_octaves - 1, 0) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: '{ zip_lists(num = .,'
  prefs: []
  type: TYPE_NORMAL
- en: HxW = lapply(., calc_octave_HxW)) }
  prefs: []
  type: TYPE_NORMAL
- en: str(octaves)
  prefs: []
  type: TYPE_NORMAL
- en: List of 3
  prefs: []
  type: TYPE_NORMAL
- en: $ :List of 2
  prefs: []
  type: TYPE_NORMAL
- en: '..$ num: int 2'
  prefs: []
  type: TYPE_NORMAL
- en: '..$ HxW: int [1:2] 459 612'
  prefs: []
  type: TYPE_NORMAL
- en: $ :List of 2
  prefs: []
  type: TYPE_NORMAL
- en: '..$ num: int 1'
  prefs: []
  type: TYPE_NORMAL
- en: '..$ HxW: int [1:2] 643 857'
  prefs: []
  type: TYPE_NORMAL
- en: $ :List of 2
  prefs: []
  type: TYPE_NORMAL
- en: '..$ num: int 0'
  prefs: []
  type: TYPE_NORMAL
- en: '..$ HxW: int [1:2] 900 1200'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Load and preprocess the test image.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Compute the target shape of the image at different octaves.**
  prefs: []
  type: TYPE_NORMAL
- en: shrunk_original_img <— original_img %>% tf$image$resize(octaves[[1]]$HxW)
  prefs: []
  type: TYPE_NORMAL
- en: img <— original_img ➊
  prefs: []
  type: TYPE_NORMAL
- en: for (octave in octaves) {➋
  prefs: []
  type: TYPE_NORMAL
- en: cat(sprintf("Processing octave %i with shape (%s)\n",
  prefs: []
  type: TYPE_NORMAL
- en: octave$num, paste(octave$HxW, collapse = ", ")))
  prefs: []
  type: TYPE_NORMAL
- en: img <— img %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$image$resize(octave$HxW) %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: gradient_ascent_loop(iterations = iterations, ➍
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate = step,
  prefs: []
  type: TYPE_NORMAL
- en: max_loss = max_loss)
  prefs: []
  type: TYPE_NORMAL
- en: upscaled_shrunk_original_img <— ➎
  prefs: []
  type: TYPE_NORMAL
- en: shrunk_original_img %>% tf$image$resize(octave$HxW)
  prefs: []
  type: TYPE_NORMAL
- en: same_size_original <—
  prefs: []
  type: TYPE_NORMAL
- en: original_img %>% tf$image$resize(octave$HxW)➏
  prefs: []
  type: TYPE_NORMAL
- en: lost_detail <—➐
  prefs: []
  type: TYPE_NORMAL
- en: same_size_original - upscaled_shrunk_original_img
  prefs: []
  type: TYPE_NORMAL
- en: img %<>% `+`(lost_detail)➑
  prefs: []
  type: TYPE_NORMAL
- en: shrunk_original_img <—
  prefs: []
  type: TYPE_NORMAL
- en: original_img %>% tf$image$resize(octave$HxW)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: img <— deprocess_image(img)
  prefs: []
  type: TYPE_NORMAL
- en: img %>% display_image_tensor()
  prefs: []
  type: TYPE_NORMAL
- en: img %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$encode_png() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$write_file("dream.png", .)➒
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Save a reference to the original image (we need to keep the original around).**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Iterate over the different octaves.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Scale up the dream image.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Run gradient ascent, altering the dream.**
  prefs: []
  type: TYPE_NORMAL
- en: '➎ **Scale up the smaller version of the original image: it will be pixelated.**'
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Compute the high-quality version of the original image at this size.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **The difference between the two is the detail that was lost when detail up.**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Reinject the lost detail into the dream.**
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Save the final result.**
  prefs: []
  type: TYPE_NORMAL
- en: Because the original Inception V3 network was trained to recognize concepts
    in images of size 299 × 299, and given that the process involves scaling the images
    down by a reasonable factor, the DeepDream implementation produces much better
    results on images that are somewhere between 300 × 300 and 400 × 400\. Regardless,
    you can run the same code on images of any size and any ratio.
  prefs: []
  type: TYPE_NORMAL
- en: On a GPU, it takes only a few seconds to run the whole thing. [Figure 12.7](#fig12-7)
    shows the result of our dream configuration on the test image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0421-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.7 Running the DeepDream code on the test image**'
  prefs: []
  type: TYPE_NORMAL
- en: I strongly suggest that you explore what you can do by adjusting which layers
    you use in your loss. Layers that are lower in the network contain more-local,
    less-abstract representations and lead to dream patterns that look more geometric.
    Layers that are higher up lead to more-recognizable visual patterns based on the
    most common objects found in ImageNet, such as dog eyes, bird feathers, and so
    on. You can use random generation of the parameters in the layer_settings vector
    to quickly explore many different layer combinations. [Figure 12.8](#fig12-8)
    shows a range of results obtained on an image of a delicious homemade pastry using
    different layer configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DeepDream consists of running a convnet in reverse to generate inputs based
    on the representations learned by the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results produced are fun and somewhat similar to the visual artifacts induced
    in humans by the disruption of the visual cortex via psychedelics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the process isn’t specific to image models or even to convnets. It
    can be done for speech, music, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0422-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.8 Trying a range of DeepDream configurations on an example image**'
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Neural style transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to DeepDream, another major development in deep-learning-driven
    image modification is *neural style transfer*, introduced by Leon Gatys et al.
    in the summer of 2015.^([4](#Rendnote4)) The neural style transfer algorithm has
    undergone many refinements and spawned many variations since its original introduction,
    and it has made its way into many smartphone photo apps. For simplicity, this
    section focuses on the formulation described in the original paper.
  prefs: []
  type: TYPE_NORMAL
- en: Neural style transfer consists of applying the style of a reference image to
    a target image while conserving the content of the target image. [Figure 12.9](#fig12-9)
    shows an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0422-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.9 A style transfer example**'
  prefs: []
  type: TYPE_NORMAL
- en: In this context, *style* essentially means textures, colors, and visual patterns
    in the image, at various spatial scales, and the content is the higher-level macrostructure
    of the image. For instance, blue-and-yellow circular brushstrokes are considered
    to be the style in [figure 12.9](#fig12-9) (using *Starry Night* by Vincent van
    Gogh), and the buildings in the Tübingen photograph are considered to be the content.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of style transfer, which is tightly related to that of texture generation,
    has had a long history in the image-processing community prior to the development
    of neural style transfer in 2015\. But as it turns out, the deep-learning-based
    implementations of style transfer offer results unparalleled by what had been
    previously achieved with classical computer vision techniques, and they triggered
    an amazing renaissance in creative applications of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key notion behind implementing style transfer is the same idea that’s central
    to all deep learning algorithms: you define a loss function to specify what you
    want to achieve, and you minimize this loss. We know what we want to achieve:
    conserving the content of the original image while adopting the style of the reference
    image. If we were able to mathematically define *content* and *style*, then an
    appropriate loss function to minimize would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: loss <— distance(style(reference_image) - style(combination_image)) +
  prefs: []
  type: TYPE_NORMAL
- en: distance(content(original_image) - content(combination_image))
  prefs: []
  type: TYPE_NORMAL
- en: Here, distance() is a norm function such as the L2 norm, content() is a function
    that takes an image and computes a representation of its content, and style()
    is a function that takes an image and computes a representation of its style.
    Minimizing this loss causes style(combination_image) to be close to style(reference_image),
    and content(combination_image) is close to content(original_image), thus achieving
    style transfer as we defined it.
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental observation made by Gatys et al. was that deep convolutional neural
    networks offer a way to mathematically define the style and content functions.
    Let’s see how.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 The content loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you already know, activations from earlier layers in a network contain *local*
    information about the image, whereas activations from higher layers contain increasingly
    global, abstract information. Formulated in a different way, the activations of
    the different layers of a convnet provide a decomposition of the contents of an
    image over different spatial scales. Therefore, you’d expect the content of an
    image, which is more global and abstract, to be captured by the representations
    of the upper layers in a convnet.
  prefs: []
  type: TYPE_NORMAL
- en: A good candidate for content loss is thus the L2 norm between the activations
    of an upper layer in a pretrained convnet, computed over the target image, and
    the activations of the same layer computed over the generated image. This guarantees
    that, as seen from the upper layer, the generated image will look similar to the
    original target image. Assuming that what the upper layers of a convnet see is
    really the content of their input images, this works as a way to preserve image
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 The style loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The content loss uses only a single upper layer, but the style loss as defined
    by Gatys et al. uses multiple layers of a convnet: you try to capture the appearance
    of the style-reference image at all spatial scales extracted by the convnet, not
    just a single scale. For the style loss, Gatys et al. use the *Gram matrix* of
    a layer’s activations: the inner product of the feature maps of a given layer.
    This inner product can be understood as representing a map of the correlations
    between the layer’s features. These feature correlations capture the statistics
    of the patterns of a particular spatial scale, which empirically correspond to
    the appearance of the textures found at this scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the style loss aims to preserve similar internal correlations within
    the activations of different layers, across the style-reference image and the
    generated image. In turn, this guarantees that the textures found at different
    spatial scales look similar across the style-reference image and the generated
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, you can use a pretrained convnet to define a loss that will do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Preserve content by maintaining similar high-level layer activations between
    the original image and the generated image. The convnet should “see” both the
    original image and the generated image as containing the same things.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Preserve style by maintaining similar *correlations* within activations for
    both low-level layers and high-level layers. Feature correlations capture *textures*:
    the generated image and the style-reference image should share the same textures
    at different spatial scales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at a Keras implementation of the original 2015 neural style transfer
    algorithm. As you’ll see, it shares many similarities with the DeepDream implementation
    we developed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.3 Neural style transfer in Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural style transfer can be implemented using any pretrained convnet. Here,
    we’ll use the VGG19 network used by Gatys et al. VGG19 is a simple variant of
    the VGG16 network introduced in chapter 5, with three more convolutional layers.
    Here’s the general process:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up a network that computes VGG19 layer activations for the style-reference
    image, the base image, and the generated image at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the layer activations computed over these three images to define the loss
    function described earlier, which we’ll minimize to achieve style transfer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a gradient-descent process to minimize this loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by defining the paths to the style-reference image and the base
    image. To make sure that the processed images are a similar size (widely different
    sizes make style transfer more difficult), we’ll later resize them all to a shared
    height of 400 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.16 Getting the style and content images
  prefs: []
  type: TYPE_NORMAL
- en: base_image_path <— get_file(➊
  prefs: []
  type: TYPE_NORMAL
- en: '"sf.jpg", origin = "https://img-datasets.s3.amazonaws.com/sf.jpg")'
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image_path <— get_file(➋
  prefs: []
  type: TYPE_NORMAL
- en: '"starry_night.jpg",'
  prefs: []
  type: TYPE_NORMAL
- en: origin = "https://img-datasets.s3.amazonaws.com/starry_night.jpg")
  prefs: []
  type: TYPE_NORMAL
- en: c(original_height, original_width) %<—% {
  prefs: []
  type: TYPE_NORMAL
- en: base_image_path %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$read_file() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$decode_image() %>%
  prefs: []
  type: TYPE_NORMAL
- en: dim() %>% .[1:2]
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: img_height <— 400➌
  prefs: []
  type: TYPE_NORMAL
- en: img_width <— round(img_height * (original_width / original_height))➌
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Path to the image we want to transform**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Path to the style image**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Dimensions of the generated picture**
  prefs: []
  type: TYPE_NORMAL
- en: Our content image is shown in [figure 12.10](#fig12-10), and [figure 12.11](#fig12-11)
    shows our style image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0425-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.10 Content image: San Francisco from Nob Hill**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0426-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.11 Style image: *Starry Night* by van Gogh**'
  prefs: []
  type: TYPE_NORMAL
- en: We also need some auxiliary functions for loading, preprocessing, and postprocessing
    the images that go in and out of the VGG19 convnet.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.17 Auxiliary functions**'
  prefs: []
  type: TYPE_NORMAL
- en: preprocess_image <— function(image_path) {➊
  prefs: []
  type: TYPE_NORMAL
- en: image_path %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$read_file() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$decode_image() %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$image$resize(as.integer(c(img_height, img_width))) %>%
  prefs: []
  type: TYPE_NORMAL
- en: k_expand_dims(axis = 1) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: imagenet_preprocess_input()
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: deprocess_image <— tf_function(function(img) {➌
  prefs: []
  type: TYPE_NORMAL
- en: if (length(dim(img)) == 4)
  prefs: []
  type: TYPE_NORMAL
- en: img <— k_squeeze(img, axis = 1)➍
  prefs: []
  type: TYPE_NORMAL
- en: c(b, g, r) %<—% {
  prefs: []
  type: TYPE_NORMAL
- en: img %>%
  prefs: []
  type: TYPE_NORMAL
- en: k_reshape(c(img_height, img_width, 3)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: k_unstack(axis = 3)➎
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: r %<>% `+`(123.68)➏
  prefs: []
  type: TYPE_NORMAL
- en: g %<>% `+`(103.939)➏
  prefs: []
  type: TYPE_NORMAL
- en: b %<>% `+`(116.779)➏
  prefs: []
  type: TYPE_NORMAL
- en: k_stack(c(r, g, b), axis = 3) %>%➐
  prefs: []
  type: TYPE_NORMAL
- en: k_clip(0, 255) %>%
  prefs: []
  type: TYPE_NORMAL
- en: k_cast("uint8")
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Util function to open, resize, and format pictures into appropriate arrays**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Add a batch dimension.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Util function to convert a tensor into a valid image**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Also accept an image with a batch-dim of size 1. (This will throw an error
    if the first axis is not size 1.)**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Unstack along the third axis, and return a list of length 3.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Zero-center by removing the mean pixel value from ImageNet. This reverses
    a transformation done by imagenet_preprocess_input().**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Note that we're reversing the order of the channels, BGR to RGB. This is
    also part of the reversal of imagenet_preprocess_input().**
  prefs: []
  type: TYPE_NORMAL
- en: Keras backend functions (k_*)
  prefs: []
  type: TYPE_NORMAL
- en: In this version of preprocess_image() and deprocess_image(), we used Keras backend
    functions, like k_expand_dims() but in earlier versions, we used functions from
    the tf module, like tf$expand_dims(). What’s the difference?
  prefs: []
  type: TYPE_NORMAL
- en: The Keras package contains an extensive suite of backend functions, all starting
    with the k_ prefix. They are a vestige from a time when the Keras library was
    designed to work with multiple backends. Today it’s more common to call directly
    to functions in tf module, where the functions typically expose more features
    and capabilities. One nicety of the keras::k_ backend functions, however, is that
    they all are 1-based and will often do automatic coercion of function arguments
    to integer as necessary. For example, k_expand_dims(axis = 1) is equivalent to
    tf$expand_dims(axis = 0L).
  prefs: []
  type: TYPE_NORMAL
- en: The backend functions are no longer actively developed, but they are covered
    by the TensorFlow stability promise, are maintained, and will not be going away
    anytime soon. You can safely use functions like k_expand_dims(), k_squeeze(),
    and k_stack() to do common tensor operations, especially when it’s easier to reason
    with consistent 1-based counting conventions. However, when you find the capabilities
    of the backend functions limiting, don’t hesitate to switch over to using the
    tf module functions directly. You can find additional documentation about backend
    functions at [https://keras.rstudio.com/articles/backend.html](https://www.keras.rstudio.com/articles/backend.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up the VGG19 network. Like in the DeepDream example, we’ll use the
    pre-trained convnet to create a feature exactor model that returns the activations
    of intermediate layers—all layers in the model this time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.18 Using a pretrained VGG19 model to create a feature extractor
  prefs: []
  type: TYPE_NORMAL
- en: model <— application_vgg19(weights = "imagenet",
  prefs: []
  type: TYPE_NORMAL
- en: include_top = FALSE)➊
  prefs: []
  type: TYPE_NORMAL
- en: outputs <— list()
  prefs: []
  type: TYPE_NORMAL
- en: for (layer in model$layers)
  prefs: []
  type: TYPE_NORMAL
- en: outputs[[layer$name]] <— layer$output
  prefs: []
  type: TYPE_NORMAL
- en: feature_extractor <— keras_model(inputs = model$inputs,➋
  prefs: []
  type: TYPE_NORMAL
- en: outputs = outputs)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Build a VGG19 model loaded with pretrained ImageNet weights.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **A model that returns the activation values for every target layer (as a
    named list)**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define the content loss, which will make sure the top layer of the VGG19
    convnet has a similar view of the style image and the combination image.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.19 Content loss
  prefs: []
  type: TYPE_NORMAL
- en: content_loss <— function(base_img, combination_img)
  prefs: []
  type: TYPE_NORMAL
- en: sum((combination_img - base_img) ^ 2)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the style loss. It uses an auxiliary function to compute the Gram matrix
    of an input matrix: a map of the correlations found in the original feature matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.20 Style loss
  prefs: []
  type: TYPE_NORMAL
- en: gram_matrix <— function(x) {➊
  prefs: []
  type: TYPE_NORMAL
- en: n_features <— tf$shape(x)[3]
  prefs: []
  type: TYPE_NORMAL
- en: x %>%
  prefs: []
  type: TYPE_NORMAL
- en: tf$reshape(c(-1L, n_features)) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: tf$matmul(t(.), .)➌
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: style_loss <— function(style_img, combination_img) {
  prefs: []
  type: TYPE_NORMAL
- en: S <— gram_matrix(style_img)
  prefs: []
  type: TYPE_NORMAL
- en: C <— gram_matrix(combination_img)
  prefs: []
  type: TYPE_NORMAL
- en: channels <— 3
  prefs: []
  type: TYPE_NORMAL
- en: size <— img_height * img_width
  prefs: []
  type: TYPE_NORMAL
- en: sum((S - C) ^ 2) /
  prefs: []
  type: TYPE_NORMAL
- en: (4 * (channels ^ 2) * (size ^ 2))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **x has the shape (height, width, features).**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Flatten the first two spatial axes, and preserve the features axis.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The output will have the shape (n_features, n_features).**
  prefs: []
  type: TYPE_NORMAL
- en: 'To these two loss components, you add a third: the *total variation loss*,
    which operates on the pixels of the generated combination image. It encourages
    spatial continuity in the generated image, thus avoiding overly pixelated results.
    You can interpret it as a regularization loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.21 Total variation loss**'
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_loss <— function(x) {
  prefs: []
  type: TYPE_NORMAL
- en: a <— k_square(x[, NA:(img_height-1), NA:(img_width-1), ]—
  prefs: []
  type: TYPE_NORMAL
- en: x[, 2:NA             , NA:(img_width-1), ])
  prefs: []
  type: TYPE_NORMAL
- en: b <— k_square(x[, NA:(img_height-1), NA:(img_width-1), ]—
  prefs: []
  type: TYPE_NORMAL
- en: x[, NA:(img_height-1), 2:NA            , ])
  prefs: []
  type: TYPE_NORMAL
- en: sum((a + b) ^ 1.25)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The loss that you minimize is a weighted average of these three losses. To compute
    the content loss, you use only one upper layer—the block5_conv2 layer—whereas
    for the style loss, you use a list of layers that spans both low-level and high-level
    layers. You add the total variation loss at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the style-reference image and content image you’re using, you’ll
    likely want to tune the content_weight coefficient (the contribution of the content
    loss to the total loss). A higher content_weight means the target content will
    be more recognizable in the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.22 Defining the final loss that you’ll minimize
  prefs: []
  type: TYPE_NORMAL
- en: style_layer_names <— c(➊
  prefs: []
  type: TYPE_NORMAL
- en: '"block1_conv1",'
  prefs: []
  type: TYPE_NORMAL
- en: '"block2_conv1",'
  prefs: []
  type: TYPE_NORMAL
- en: '"block3_conv1",'
  prefs: []
  type: TYPE_NORMAL
- en: '"block4_conv1",'
  prefs: []
  type: TYPE_NORMAL
- en: '"block5_conv1"'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: content_layer_name <— "block5_conv2"➋
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_weight <— 1e—6➌
  prefs: []
  type: TYPE_NORMAL
- en: content_weight <— 2.5e—8➍
  prefs: []
  type: TYPE_NORMAL
- en: style_weight <— 1e—6➎
  prefs: []
  type: TYPE_NORMAL
- en: compute_loss <
  prefs: []
  type: TYPE_NORMAL
- en: function(combination_image, base_image, style_reference_image) {
  prefs: []
  type: TYPE_NORMAL
- en: input_tensor <
  prefs: []
  type: TYPE_NORMAL
- en: list(base_image,
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image,
  prefs: []
  type: TYPE_NORMAL
- en: combination_image) %>%
  prefs: []
  type: TYPE_NORMAL
- en: k_concatenate(axis = 1)
  prefs: []
  type: TYPE_NORMAL
- en: features <— feature_extractor(input_tensor)
  prefs: []
  type: TYPE_NORMAL
- en: layer_features <— features[[content_layer_name]]
  prefs: []
  type: TYPE_NORMAL
- en: base_image_features <— layer_features[1, , , ]
  prefs: []
  type: TYPE_NORMAL
- en: combination_features <— layer_features[3, , , ]
  prefs: []
  type: TYPE_NORMAL
- en: loss <— 0➏
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% `+`(➐
  prefs: []
  type: TYPE_NORMAL
- en: content_loss(base_image_features, combination_features) *
  prefs: []
  type: TYPE_NORMAL
- en: content_weight
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: for (layer_name in style_layer_names) {
  prefs: []
  type: TYPE_NORMAL
- en: layer_features <— features[[layer_name]]
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_features <— layer_features[2, , , ]
  prefs: []
  type: TYPE_NORMAL
- en: combination_features <— layer_features[3, , , ]
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% `+`(➑
  prefs: []
  type: TYPE_NORMAL
- en: style_loss(style_reference_features, combination_features) *
  prefs: []
  type: TYPE_NORMAL
- en: style_weight / length(style_layer_names)
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% `+`(➒
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_loss(combination_image) *
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_weight
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: loss➓
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The list of layers to use for the style loss**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The layer to use for the content loss**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The contribution weight of the total variation loss**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **The contribution weight of the content loss**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The contribution weight of the style loss**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Initialize the loss to 0.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add the content loss.**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Add the style loss for each style layer.**
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Add the total variation loss.**
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Return the sum of content loss, style loss, and total variation loss.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s set up the gradient—descent process. In the original Gatys et
    al. paper, optimization is performed using the L—BFGS algorithm, but that’s not
    available in Tensor-Flow, so we’ll just do mini—batch gradient descent with the
    SGD optimizer instead. We’ll leverage an optimizer feature you haven’t seen before:
    a learning-rate schedule. We’ll use it to gradually decrease the learning rate
    from a very high value (100) to a much smaller final value (about 20). That way,
    we’ll make fast progress in the early stages of training and then proceed more
    cautiously as we get closer to the loss minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.23 Setting up the gradient-descent process
  prefs: []
  type: TYPE_NORMAL
- en: compute_loss_and_grads <— tf_function(➊
  prefs: []
  type: TYPE_NORMAL
- en: function(combination_image, base_image, style_reference_image) {
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: loss <— compute_loss(combination_image,
  prefs: []
  type: TYPE_NORMAL
- en: base_image,
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(loss, combination_image)
  prefs: []
  type: TYPE_NORMAL
- en: list(loss, grads)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: optimizer <— optimizer_sgd(
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate_schedule_exponential_decay(
  prefs: []
  type: TYPE_NORMAL
- en: initial_learning_rate = 100, decay_steps = 100,➋
  prefs: []
  type: TYPE_NORMAL
- en: decay_rate = 0.96))➋
  prefs: []
  type: TYPE_NORMAL
- en: base_image <— preprocess_image(base_image_path)
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image <— preprocess_image(style_reference_image_path)
  prefs: []
  type: TYPE_NORMAL
- en: combination_image <
  prefs: []
  type: TYPE_NORMAL
- en: tf$Variable(preprocess_image(base_image_path))➌
  prefs: []
  type: TYPE_NORMAL
- en: output_dir <— fs::path("style-transfer-generated-images")
  prefs: []
  type: TYPE_NORMAL
- en: iterations <— 4000
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(iterations)) {
  prefs: []
  type: TYPE_NORMAL
- en: c(loss, grads) %<—% compute_loss_and_grads(
  prefs: []
  type: TYPE_NORMAL
- en: combination_image, base_image, style_reference_image)
  prefs: []
  type: TYPE_NORMAL
- en: optimizer$apply_gradients(list(➍
  prefs: []
  type: TYPE_NORMAL
- en: tuple(grads, combination_image)))
  prefs: []
  type: TYPE_NORMAL
- en: if ((i %% 100) == 0) {
  prefs: []
  type: TYPE_NORMAL
- en: 'cat(sprintf("Iteration %i: loss = %.2f\n", i, loss))'
  prefs: []
  type: TYPE_NORMAL
- en: img <— deprocess_image(combination_image)
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor(img)
  prefs: []
  type: TYPE_NORMAL
- en: fname <— sprintf("combination_image_at_iteration_%04i.png", i)
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$write_file(filename = output_dir / fname,➎
  prefs: []
  type: TYPE_NORMAL
- en: contents = tf$io$encode_png(img))
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We make the training step fast by compiling it as a tf_function().**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We'll start with a learning rate of 100 and decrease it by 4% every 100
    steps.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Use a tf$Variable() to store the combination image because we'll be updating
    it during training.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Update the combination image in a direction that reduces the style transfer
    loss.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Save the combination image at regular intervals.**
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12.12](#fig12-12) shows what you get. Keep in mind that what this technique
    achieves is merely a form of image retexturing, or texture transfer. It works
    best with style—reference images that are strongly textured and highly self-similar,
    and with content targets that don’t require high levels of detail to be recognizable.
    It typically can’t achieve fairly abstract feats such as transferring the style
    of one portrait to another. The algorithm is closer to classical signal processing
    than to AI, so don’t expect it to work like magic!'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0431-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.12 Style transfer result**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, note that this style-transfer algorithm is slow to run. But the
    transformation operated by the setup is simple enough that it can be learned by
    a small, fast feed-forward convnet as well—as long as you have appropriate training
    data available. Fast style transfer can thus be achieved by first spending a lot
    of compute cycles to generate input-output training examples for a fixed style-reference
    image, using the method outlined here, and then training a simple convnet to learn
    this style—specific transformation. Once that’s done, stylizing a given image
    is instantaneous: it’s just a forward pass of this small convnet.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Style transfer consists of creating a new image that preserves the contents
    of a target image while also capturing the style of a reference image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content can be captured by the high-level activations of a convnet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Style can be captured by the internal correlations of the activations of different
    layers of a convnet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, deep learning allows style transfer to be formulated as an optimization
    process using a loss defined with a pretrained convnet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting from this basic idea, many variants and refinements are possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.4 Generating images with variational autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most popular and successful application of creative AI today is image generation:
    learning latent visual spaces and sampling from them to create entirely new pictures
    interpolated from real ones—pictures of imaginary people, imaginary places, imaginary
    cats and dogs, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section and the next, we’ll review some high-level concepts pertaining
    to image generation, alongside implementation details relative to the two main
    techniques in this domain: *variational autoencoders* (VAEs) and *generative adversarial
    networks* (GANs). Note that the techniques I’ll present here aren’t specific to
    images—you could develop latent spaces of sound, music, or even text using GANs
    and VAEs—but in practice, the most interesting results have been obtained with
    pictures, and that’s what we’ll focus on here.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 Sampling from latent spaces of images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key idea of image generation is to develop a low-dimensional *latent space*
    of representations (which, like everything else in deep learning, is a vector
    space), where any point can be mapped to a “valid” image: an image that looks
    like the real thing. The module capable of realizing this mapping, taking as input
    a latent point and outputting an image (a grid of pixels), is called a *generator*
    (in the case of GANs) or a *decoder* (in the case of VAEs). Once such a latent
    space has been learned, you can sample points from it, and, by mapping them back
    to image space, generate images that have never been seen before (see [figure
    12.13](#fig12-13)). These new images are the in-betweens of the training images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0432-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.13 Learning a latent vector space of images and using it to sample
    new images**'
  prefs: []
  type: TYPE_NORMAL
- en: GANs and VAEs are two different strategies for learning such latent spaces of
    image representations, each with its own characteristics. VAEs are great for learning
    latent spaces that are well structured, where specific directions encode a meaningful
    axis of variation in the data (see [figure 12.14](#fig12-14)). GANs generate images
    that can potentially be highly realistic, but the latent space they come from
    may not have as much structure and continuity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0433-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.14 A continuous space of faces generated by Tom White using VAEs**'
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 Concept vectors for image editing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We already hinted at the idea of a *concept vector* when we covered word embeddings
    in chapter 11\. The idea is still the same: given a latent space of representations,
    or an embedding space, certain directions in the space may encode interesting
    axes of variation in the original data. In a latent space of images of faces,
    for instance, there may be a *smile vector*, such that if latent point z is the
    embedded representation of a certain face, then latent point z + s is the embedded
    representation of the same face, smiling. Once you’ve identified such a vector,
    it then becomes possible to edit images by projecting them into the latent space,
    moving their representation in a meaningful way, and then decoding them back to
    image space. Concept vectors exist for essentially any independent dimension of
    variation in image space—in the case of faces, you may discover vectors for adding
    sunglasses to a face, removing glasses, turning a male face into a female face,
    and so on. [Figure 12.15](#fig12-15) is an example of a smile vector, a concept
    vector discovered by Tom White, from the Victoria University School of Design
    in New Zealand, using VAEs trained on a dataset of faces of celebrities (the CelebA
    dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0434-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.15 The smile vector**'
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.3 Variational autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Variational autoencoders, simultaneously discovered by Kingma and Welling in
    December 2013^([5](#Rendnote5)) and Rezende, Mohamed, and Wierstra in January
    2014,^([6](#Rendnote6)) are a kind of generative model that’s especially appropriate
    for the task of image editing via concept vectors. They’re a modern take on autoencoders
    (a type of network that aims to encode an input to a low-dimensional latent space
    and then decode it back) that mixes ideas from deep learning with Bayesian inference.
  prefs: []
  type: TYPE_NORMAL
- en: A classical image autoencoder takes an image, maps it to a latent vector space
    via an encoder module, and then decodes it back to an output with the same dimensions
    as the original image, via a decoder module (see [figure 12.16](#fig12-16)). It’s
    then trained by using as target data the *same images* as the input images, meaning
    the autoencoder learns to reconstruct the original inputs. By imposing various
    constraints on the code (the output of the encoder), you can get the autoencoder
    to learn more— or less-interesting latent representations of the data. Most commonly,
    you’ll constrain the code to be low—dimensional and sparse (mostly zeros), in
    which case the encoder acts as a way to compress the input data into fewer bits
    of information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0434-02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.16 An autoencoder mapping an input *x* to a compressed representation
    and then decoding it back as *x*’**'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, such classical autoencoders don’t lead to particularly useful or
    nicely structured latent spaces. They’re not much good at compression, either.
    For these reasons, they have largely fallen out of fashion. VAEs, however, augment
    autoencoders with a little bit of statistical magic that forces them to learn
    continuous, highly structured latent spaces. They have turned out to be a powerful
    tool for image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of compressing its input image into a fixed code in the latent space,
    a VAE turns the image into the parameters of a statistical distribution: a mean
    and a variance. Essentially, this means we’re assuming the input image has been
    generated by a statistical process, and that the randomness of this process should
    be taken into account during encoding and decoding. The VAE then uses the mean
    and variance parameters to randomly sample one element of the distribution and
    decodes that element back to the original input (see [figure 12.17](#fig12-17)).
    The stochasticity of this process improves robustness and forces the latent space
    to encode meaningful representations everywhere: every point sampled in the latent
    space is decoded to a valid output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0435-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.17 A VAE maps an image to two vectors, z_mean and z_log_sigma,
    which define a probability distribution over the latent space, used to sample
    a latent point to decode.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In technical terms, here’s how a VAE works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** An encoder module turns the input sample, input_img, into two parameters
    in a latent space of representations, z_mean and z_log_variance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** You randomly sample a point z from the latent normal distribution that’s
    assumed to generate the input image, via z = z_mean + exp(z_log_variance) * epsilon,
    where epsilon is a random tensor of small values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** A decoder module maps this point in the latent space back to the original
    input image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because epsilon is random, the process ensures that every point that’s close
    to the latent location where you encoded input_img (z—mean) can be decoded to
    something similar to input_img, thus forcing the latent space to be continuously
    meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: Any two close points in the latent space will decode to highly similar images.
    Continuity, combined with the low dimensionality of the latent space, forces every
    direction in the latent space to encode a meaningful axis of variation of the
    data, making the latent space very structured and thus highly suitable to manipulation
    via concept vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters of a VAE are trained via two loss functions: a *reconstruction
    loss* that forces the decoded samples to match the initial inputs, and a *regularization
    loss* that helps learn well—rounded latent distributions and reduces overfitting
    to the training data. Schematically, the process looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: c(z_mean, z_log_variance) %<—% encoder(input_img)➊
  prefs: []
  type: TYPE_NORMAL
- en: z <— z_mean + exp(z_log_variance) * epsilon➋
  prefs: []
  type: TYPE_NORMAL
- en: reconstructed_img <— decoder(z) ➌
  prefs: []
  type: TYPE_NORMAL
- en: model <— keras_model(input_img, reconstructed_img)➍
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Encode the input into mean and variance parameters.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Draw a latent point using a small random epsilon.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Decode z back to an image.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Instantiate the autoencoder model, which maps an input image to its reconstruction.**
  prefs: []
  type: TYPE_NORMAL
- en: You can then train the model using the reconstruction loss and the regularization
    loss. For the regularization loss, we typically use an expression (the Kullback–Leibler
    divergence) meant to nudge the distribution of the encoder output toward a well—rounded
    normal distribution centered around 0\. This provides the encoder with a sensible
    assumption about the structure of the latent space it’s modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see what implementing a VAE looks like in practice!
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.4 Implementing a VAE with Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re going to be implementing a VAE that can generate MNIST digits. It’s going
    to have three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: An encoder network that turns a real image into a mean and a variance in the
    latent space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sampling layer that takes such a mean and variance and uses them to sample
    a random point from the latent space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder network that turns points from the latent space back into image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following listing shows the encoder network we’ll use, mapping images to
    the parameters of a probability distribution over the latent space. It’s a simple
    convnet that maps the input image x to two vectors, z_mean and z_log_var. One
    important detail is that we use strides for downsampling feature maps instead
    of max pooling. The last time we did this was in the image segmentation example
    in chapter 9\. Recall that, in general, strides are preferable to max pooling
    for any model that cares about *information location*—that is to say, *where*
    stuff is in the image—and this one does, because it will have to produce an image
    encoding that can be used to reconstruct a valid image.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.24 VAE encoder network**'
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <— 2➊
  prefs: []
  type: TYPE_NORMAL
- en: encoder_inputs <—  layer_input(shape = c(28, 28, 1))
  prefs: []
  type: TYPE_NORMAL
- en: x <— encoder_inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(32, 3, activation = "relu", strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(64, 3, activation = "relu", strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_flatten() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(16, activation = "relu")
  prefs: []
  type: TYPE_NORMAL
- en: z_mean <— x %>% layer_dense(latent_dim, name = "z_mean")➋
  prefs: []
  type: TYPE_NORMAL
- en: z_log_var <— x %>% layer_dense(latent_dim, name = "z_log_var")➋
  prefs: []
  type: TYPE_NORMAL
- en: encoder <— keras_model(encoder_inputs, list(z_mean, z_log_var),
  prefs: []
  type: TYPE_NORMAL
- en: name = "encoder")
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Dimensionality of the latent space: a 2D plane**'
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The input image ends up being encoded into these two parameters.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Its summary looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0437-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Next is the code for using z_mean and z_log_var, the parameters of the statistical
    distribution assumed to have produced input_img, to generate a latent space point
    z.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.25 Latent—space—sampling layer**'
  prefs: []
  type: TYPE_NORMAL
- en: layer_sampler <— new_layer_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "Sampler",
  prefs: []
  type: TYPE_NORMAL
- en: call = function(z_mean, z_log_var) {➊
  prefs: []
  type: TYPE_NORMAL
- en: epsilon <— tf$random$normal(shape = tf$shape(z_mean))➋
  prefs: []
  type: TYPE_NORMAL
- en: z_mean + exp(0.5 * z_log_var) * epsilon➌
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **z_mean and z_log_var here both will have shape (batch_size, latent_dim),
    for example, (128, 2).**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Draw a batch of random normal vectors.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Apply the VAE sampling formula.**
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the decoder implementation. We reshape the vector
    z to the dimensions of an image and then use a few convolution layers to obtain
    a final image output that has the same dimensions as the original input_img.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.26 VAE decoder network, mapping latent space points to images
  prefs: []
  type: TYPE_NORMAL
- en: latent_inputs <— layer_input(shape = c(latent_dim))➊
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs <— latent_inputs %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(7 * 7 * 64, activation = "relu") %>% ➋
  prefs: []
  type: TYPE_NORMAL
- en: layer_reshape(c(7, 7, 64)) %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(64, 3, activation = "relu",
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>% ➍
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(32, 3, activation = "relu",
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(1, 3, activation = "sigmoid",
  prefs: []
  type: TYPE_NORMAL
- en: padding = "same") ➎
  prefs: []
  type: TYPE_NORMAL
- en: decoder <— keras_model(latent_inputs, decoder_outputs,
  prefs: []
  type: TYPE_NORMAL
- en: name = "decoder")
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Input where we'll feed z**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Produce the same number of coefficients that we had at the level of the
    Flatten layer in the encoder.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Revert the layer_flatten() of the encoder.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Revert the layer_conv_2d() of the encoder.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The output ends up with shape (28, 28, 1).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Its summary looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: decoder
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0438-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now let’s create the VAE model itself. This is your first example of a model
    that isn’t doing supervised learning (an autoencoder is an example of *self-supervised*
    learning, because it uses its inputs as targets). Whenever you depart from classic
    supervised learning, it’s common to create a new_model_class() and implement a
    custom train_step() to specify the new training logic, a workflow you learned
    about in chapter 7\. That’s what we’ll do here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.27 VAE model with custom train_step()**'
  prefs: []
  type: TYPE_NORMAL
- en: model_vae <— new_model_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "VAE",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(encoder, decoder, …) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize(…)
  prefs: []
  type: TYPE_NORMAL
- en: self$encoder <— encoder➊
  prefs: []
  type: TYPE_NORMAL
- en: self$decoder <— decoder
  prefs: []
  type: TYPE_NORMAL
- en: self$sampler <— layer_sampler()
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker <
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "total_loss")➋
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker <➋
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "reconstruction_loss")➋
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker <➋
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "kl_loss")➋
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = mark_active(function() {➌
  prefs: []
  type: TYPE_NORMAL
- en: list(
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker,
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker,
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '}),'
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(data) {
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: c(z_mean, z_log_var) %<—% self$encoder(data)
  prefs: []
  type: TYPE_NORMAL
- en: z <— self$sampler(z_mean, z_log_var)
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction <— decoder(z)
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss <➍
  prefs: []
  type: TYPE_NORMAL
- en: loss_binary_crossentropy(data, reconstruction) %>%
  prefs: []
  type: TYPE_NORMAL
- en: sum(axis = c(2, 3)) %>% ➎
  prefs: []
  type: TYPE_NORMAL
- en: mean()➏
  prefs: []
  type: TYPE_NORMAL
- en: kl_loss <— -0.5 * (1 + z_log_var - z_mean^2 - exp(z_log_var))
  prefs: []
  type: TYPE_NORMAL
- en: total_loss <— reconstruction_loss + mean(kl_loss)➐
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(total_loss, self$trainable_weights)
  prefs: []
  type: TYPE_NORMAL
- en: self$optimizer$apply_gradients(zip_lists(grads, self$trainable_weights))
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker$update_state(total_loss)
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker$update_state(reconstruction_loss)
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker$update_state(kl_loss)
  prefs: []
  type: TYPE_NORMAL
- en: list(total_loss = self$total_loss_tracker$result(),
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss = self$reconstruction_loss_tracker$result(),
  prefs: []
  type: TYPE_NORMAL
- en: kl_loss = self$kl_loss_tracker$result())
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We assign to self instead of private because we want the layer weights automatically
    tracked by the Keras Model base class.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We use these metrics to keep track of the loss averages over each epoch.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **We list the metrics in an active property to enable the framework to reset
    them after each epoch (or between multiple calls to fit()/evaluate()).**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We sum the reconstruction loss over the spatial dimensions (second and third
    axes) and take its mean over the batch dimension.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Total loss for each case in the batch; preserve batch axis.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Take the mean of loss totals in the batch.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add the regularization term (Kullback–Leibler divergence).**
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’re ready to instantiate and train the model on MNIST digits. Because
    the loss is taken care of in the custom layer, we don’t specify an external loss
    at compile time (loss = NULL), which in turn means we won’t pass target data during
    training (as you can see, we pass only x_train to the model in fit()).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.28 Training the VAE
  prefs: []
  type: TYPE_NORMAL
- en: library(listarrays)➊
  prefs: []
  type: TYPE_NORMAL
- en: c(c(x_train, .), c(x_test, .)) %<—% dataset_mnist()
  prefs: []
  type: TYPE_NORMAL
- en: mnist_digits <
  prefs: []
  type: TYPE_NORMAL
- en: bind_on_rows(x_train, x_test) %>%➋
  prefs: []
  type: TYPE_NORMAL
- en: expand_dims(-1) %>%
  prefs: []
  type: TYPE_NORMAL
- en: '{ . / 255 }'
  prefs: []
  type: TYPE_NORMAL
- en: str(mnist_digits)
  prefs: []
  type: TYPE_NORMAL
- en: num [1:70000, 1:28, 1:28, 1] 0 0 0 0 0 0 0 0 0 0 …
  prefs: []
  type: TYPE_NORMAL
- en: vae <— model_vae(encoder, decoder)
  prefs: []
  type: TYPE_NORMAL
- en: vae %>% compile(optimizer = optimizer_adam())➌
  prefs: []
  type: TYPE_NORMAL
- en: vae %>% fit(mnist_digits, epochs = 30, batch_size = 128)➍
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Provide bind_on_rows() and other functions for manipulating R arrays.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We train on all MNIST digits, so we combine the training and test samples
    along the batch dim.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Note that we don't pass a loss argument in compile(), because the loss is
    already part of the train_step().**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Note that we don't pass targets in fit(), because train_step() doesn't expect
    any.**
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we can use the decoder network to turn arbitrary
    latent space vectors into images.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.29 Sampling a grid of images from the 2D latent space**'
  prefs: []
  type: TYPE_NORMAL
- en: n <— 30
  prefs: []
  type: TYPE_NORMAL
- en: digit_size <— 28
  prefs: []
  type: TYPE_NORMAL
- en: z_grid <➊
  prefs: []
  type: TYPE_NORMAL
- en: seq(-1, 1, length.out = n) %>%
  prefs: []
  type: TYPE_NORMAL
- en: expand.grid(., .) %>%
  prefs: []
  type: TYPE_NORMAL
- en: as.matrix()
  prefs: []
  type: TYPE_NORMAL
- en: decoded <— predict(vae$decoder, z_grid)➋
  prefs: []
  type: TYPE_NORMAL
- en: z_grid_i <— seq(n) %>% expand.grid(x = ., y = .)➌
  prefs: []
  type: TYPE_NORMAL
- en: figure <— array(0, c(digit_size * n, digit_size * n))➍
  prefs: []
  type: TYPE_NORMAL
- en: for (i in 1:nrow(z_grid_i)) {
  prefs: []
  type: TYPE_NORMAL
- en: c(xi, yi) %<—% z_grid_i[i, ]
  prefs: []
  type: TYPE_NORMAL
- en: digit <— decoded[i, , , ]
  prefs: []
  type: TYPE_NORMAL
- en: figure[seq(to = (n + 1 - xi) * digit_size, length.out = digit_size),
  prefs: []
  type: TYPE_NORMAL
- en: seq(to = yi * digit_size, length.out = digit_size)] <—
  prefs: []
  type: TYPE_NORMAL
- en: digit
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: par(pty = "s")➎
  prefs: []
  type: TYPE_NORMAL
- en: lim <— extendrange(r = c(-1, 1),
  prefs: []
  type: TYPE_NORMAL
- en: f = 1 - (n / (n+.5)))➐
  prefs: []
  type: TYPE_NORMAL
- en: plot(NULL, frame.plot = FALSE,
  prefs: []
  type: TYPE_NORMAL
- en: ylim = lim, xlim = lim,
  prefs: []
  type: TYPE_NORMAL
- en: xlab = ~z[1], ylab = ~z[2]) ➑
  prefs: []
  type: TYPE_NORMAL
- en: rasterImage(as.raster(1 - figure, max = 1),➑
  prefs: []
  type: TYPE_NORMAL
- en: lim[1], lim[1], lim[2], lim[2],
  prefs: []
  type: TYPE_NORMAL
- en: interpolate = FALSE)
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Create a 2D grid of linearly spaced samples.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Get the decoded digits.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Transform the decoded digits with shape (900, 28, 28, 1) to an R array with
    shape (28*30, 28*30) for plotting.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We'll display a grid of 30 × 30 digits (900 digits total).**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Square plot type**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Expand lim so (–1, 1) are at the center of a digit.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Pass a formula object to xlab for a proper subscript.**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Subtract from 1 to invert the colors.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The grid of sampled digits (see [figure 12.18](#fig12-18)) shows a completely
    continuous distribution of the different digit classes, with one digit morphing
    into another as you follow a path through latent space. Specific directions in
    this space have a meaning: for example, there are directions for “five-ness,”
    “one—ness,” and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0441-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.18 Grid of digits decoded from the latent space**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we’ll cover in detail the other major tool for generating
    artificial images: generative adversarial networks (GANs).'
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.5 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image generation with deep learning is done by learning latent spaces that
    capture statistical information about a dataset of images. By sampling and decoding
    points from the latent space, you can generate never-before-seen images. There
    are two major tools to do this: VAEs and GANs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VAEs result in highly structured, continuous latent representations. For this
    reason, they work well for doing all sorts of image editing in latent space: face
    swapping, turning a frowning face into a smiling face, and so on. They also work
    nicely for doing latent—space—based animations, such as animating a walk along
    a cross section of the latent space or showing a starting image slowly morphing
    into different images in a continuous way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs enable the generation of realistic single-frame images but may not induce
    latent spaces with solid structure and high continuity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most successful practical applications I have seen with images rely on VAEs,
    but GANs have enjoyed enduring popularity in the world of academic research. You’ll
    find out how they work and how to implement one in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Introduction to generative adversarial networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative adversarial networks (GANs), introduced in 2014 by Goodfellow et
    al.,^([7](#Rendnote7)) are an alternative to VAEs for learning latent spaces of
    images. They enable the generation of fairly realistic synthetic images by forcing
    the generated images to be statistically almost indistinguishable from real ones.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive way to understand GANs is to imagine a forger trying to create
    a fake Picasso painting. At first, the forger is pretty bad at the task. He mixes
    some of his fakes with authentic Picassos and shows them all to an art dealer.
    The art dealer makes an authenticity assessment for each painting and gives the
    forger feedback about what makes a Picasso look like a Picasso. The forger goes
    back to his studio to prepare some new fakes. As time goes on, the forger becomes
    increasingly competent at imitating the style of Picasso, and the art dealer becomes
    increasingly expert at spotting fakes. In the end, they have on their hands some
    excellent fake Picassos.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s what a GAN is: a forger network and an expert network, each being trained
    to best the other. As such, a GAN is made of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generator network*—Takes as input a random vector (a random point in the latent
    space), and decodes it into a synthetic image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Discriminator network (or adversary)*—Takes as input an image (real or synthetic),
    and predicts whether the image came from the training set or was created by the
    generator network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The generator network is trained to be able to fool the discriminator network,
    and thus it evolves toward generating increasingly realistic images as training
    goes on: artificial images that look indistinguishable from real ones, to the
    extent that it’s impossible for the discriminator network to tell the two apart
    (see [figure 12.19](#fig12-19)). Meanwhile, the discriminator is constantly adapting
    to the gradually improving capabilities of the generator, setting a high bar of
    realism for the generated images. Once training is over, the generator is capable
    of turning any point in its input space into a believable image. Unlike VAEs,
    this latent space has fewer explicit guarantees of meaningful structure; in particular,
    it isn’t continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0443-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.19 A generator transforms random latent vectors into images, and
    a discriminator seeks to tell real images from generated ones. The generator is
    trained to fool the discriminator.**'
  prefs: []
  type: TYPE_NORMAL
- en: Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike
    in any other training setup you’ve encountered in this book. Normally, gradient
    descent consists of rolling down hills in a static loss landscape. But with a
    GAN, every step taken down the hill changes the entire landscape a little. It’s
    a dynamic system where the optimization process is seeking not a minimum but an
    equilibrium between two forces. For this reason, GANs are notoriously difficult
    to train—getting a GAN to work requires lots of careful tuning of the model architecture
    and training parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.1 A schematic GAN implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we’ll explain how to implement a GAN in Keras in its barest
    form. GANs are advanced, so diving deeply into the technical details of architectures
    like that of the StyleGAN2 that generated the images in [figure 12.20](#fig12-20)
    would be out of scope for this book. The specific implementation we’ll use in
    this demonstration is a *deep convolutional GAN* (DCGAN): a very basic GAN where
    the generator and discriminator are deep convnets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll train our GAN on images from the large-scale CelebFaces Attributes dataset
    (known as CelebA), a dataset of 200,000 faces of celebrities ([http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)).
    To speed up training, we’ll resize the images to 64 × 64, so we’ll be learning
    to generate 64 × 64 images of human faces. Schematically, the GAN looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0444-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**[Figure 12.20](#fig12-20) Latent space dwellers. Images generated by [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)
    using a StyleGAN2 model. (Image credit: Phillip Wang is the website author. The
    model used is the StyleGAN2 model from Karras et al., [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958).)**'
  prefs: []
  type: TYPE_NORMAL
- en: A generator network maps vectors of shape (latent_dim) to images of shap (64,
    64, 3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discriminator network maps images of shape (64, 64, 3) to a binary score estimating
    the probability that the image is real.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A gan network chains the generator and the discriminator together: gan(x) =
    discriminator(generator(x)). Thus, this gan network maps latent space vectors
    to the discriminator’s assessment of the realism of these latent vectors as decoded
    by the generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train the discriminator using examples of real and fake images along wit
    “real”/”fake” labels, just as we train any regular image-classification model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To train the generator, we use the gradients of the generator’s weights with
    regard to the loss of the gan model. This means that at every step, we move the
    weights of the generator in a direction that makes the discriminator more likely
    to classify as “real” the images decoded by the generator. In other words, we
    train the generator to fool the discriminator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.5.2 A bag of tricks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of training GANs and tuning GAN implementations is notoriously
    difficult. You should keep in mind a number of known tricks. Like most things
    in deep learning, it’s more alchemy than science: these tricks are heuristics,
    not theory—backed guidelines.'
  prefs: []
  type: TYPE_NORMAL
- en: They’re supported by a level of intuitive understanding of the phenomenon at
    hand, and they’re known to work well empirically, although not necessarily in
    every context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few of the tricks used in the implementation of the GAN generator
    and discriminator in this section. It isn’t an exhaustive list of GAN—related
    tips; you’ll find many more across the GAN literature:'
  prefs: []
  type: TYPE_NORMAL
- en: We use strides instead of pooling for downsampling feature maps in the discriminator,
    just like we did in our VAE encoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We sample points from the latent space using a *normal distribution* (Gaussian
    distribution), not a uniform distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochasticity is good for inducing robustness. Because GAN training results
    in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing
    randomness during training helps prevent this. We introduce randomness by adding
    random noise to the labels for the discriminator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse gradients can hinder GAN training. In deep learning, sparsity is often
    a desirable property, but not in GANs. Two things can induce gradient sparsity:
    maxp—oling operations and relu activations. Instead of max pooling, we recommend
    using strided convolutions for downsampling, and we recommend using a layer_activation_leaky_relu()
    instead of a relu activation. It’s similar to relu, but it relaxes sparsity constraints
    by allowing small negative activation values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In generated images, it’s common to see checkerboard artifacts caused by unequal
    coverage of the pixel space in the generator (see [figure 12.21](#fig12-21)).
    To fix this, we use a kernel size that’s divisible by the stride size whenever
    we use a strided layer_conv_2d_transpose() or layer_conv_2d() in both the generator
    and the discriminator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0445-00.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.21 Checkerboard artifacts caused by mismatching strides and kernel
    sizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs**'
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.3 Getting our hands on the CelebA dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can download the dataset manually from the website: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).
    Because the dataset is hosted on Google Drive, you can also download it using
    gdown:'
  prefs: []
  type: TYPE_NORMAL
- en: reticulate::py_install("gdown", pip = TRUE)➊
  prefs: []
  type: TYPE_NORMAL
- en: system("gdown 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684")➋
  prefs: []
  type: TYPE_NORMAL
- en: Downloading…
  prefs: []
  type: TYPE_NORMAL
- en: 'From: https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684'
  prefs: []
  type: TYPE_NORMAL
- en: 'To: img_align_celeba.zip'
  prefs: []
  type: TYPE_NORMAL
- en: 32%|                      | 467M/1.44G [00:13<00:23, 41.3MB/s]
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Install gdown.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Download the compressed data using gdown.**
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve downloaded the data, unzip it to a celeba_gan folder
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.30 Getting the CelebA data
  prefs: []
  type: TYPE_NORMAL
- en: zip::unzip("img_align_celeba.zip", exdir = "celeba_gan")➊
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Uncompress the data.**
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve got the uncompressed images in a directory, you can use image_ dataset_from_directory()
    to turn it into a TF Dataset. Because we just need the images—there are no labels—we’ll
    specify label_mode = NULL.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.31 Creating a TF Dataset from a directory of images**'
  prefs: []
  type: TYPE_NORMAL
- en: dataset <— image_dataset_from_directory(
  prefs: []
  type: TYPE_NORMAL
- en: '"celeba_gan",'
  prefs: []
  type: TYPE_NORMAL
- en: label_mode = NULL,➊
  prefs: []
  type: TYPE_NORMAL
- en: image_size = c(64, 64),➋
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 32,➋
  prefs: []
  type: TYPE_NORMAL
- en: crop_to_aspect_ratio = TRUE➋
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Only the images will be returned—no labels.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We will resize the images to 64 × 64 by using a smart combination of cropping
    and resizing to preserve aspect ratio. We don't want face proportions to get distorted!**
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s rescale the images to the [0-1] range.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.32 Rescaling the images**'
  prefs: []
  type: TYPE_NORMAL
- en: library(tfdatasets)
  prefs: []
  type: TYPE_NORMAL
- en: dataset %<>% dataset_map(~ .x / 255)
  prefs: []
  type: TYPE_NORMAL
- en: You can use the following code to display a sample image.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.33 Displaying the first image**'
  prefs: []
  type: TYPE_NORMAL
- en: x <— dataset %>% as_iterator() %>% iter_next()
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor(x[1, , , ], max = 1)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0446-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 12.5.4 The discriminator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll develop a discriminator model that takes as input a candidate
    image (real or synthetic) and classifies it into one of two classes: “generated
    image” or “real image that comes from the training set.” One of the many issues
    that commonly arise with GANs is that the generator gets stuck with generated
    images that look like noise. A possible solution is to use dropout in the discriminator,
    so that’s what we will do here.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.34 The GAN discriminator network**'
  prefs: []
  type: TYPE_NORMAL
- en: discriminator <-
  prefs: []
  type: TYPE_NORMAL
- en: keras_model_sequential(name = "discriminator",
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = c(64, 64, 3)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(64, kernel_size = 4, strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(128, kernel_size = 4, strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(128, kernel_size = 4, strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_flatten() %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.2) %>% ➊
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **One dropout layer: an important trick!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the discriminator model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: discriminator
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0447-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 12.5.5 The generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s develop a generator model that turns a vector (from the latent space—
    during training it will be sampled at random) into a candidate image.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.35 GAN generator network
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <— 128➊
  prefs: []
  type: TYPE_NORMAL
- en: generator <
  prefs: []
  type: TYPE_NORMAL
- en: keras_model_sequential(name = "generator",
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = c(latent_dim)) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(8 * 8 * 128) %>% ➋
  prefs: []
  type: TYPE_NORMAL
- en: layer_reshape(c(8, 8, 128)) %>%➌
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(128, kernel_size = 4,
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>% ➍
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(256, kernel_size = 4,
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%➎
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(512, kernel_size = 4,
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(3, kernel_size = 5, padding = "same",
  prefs: []
  type: TYPE_NORMAL
- en: activation = "sigmoid")
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The latent space will be made of 128-dimensional vectors.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Produce the same number of coefficients we had at the level of the Flatten
    layer in the encoder.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Revert the layer_flatten() of the encoder.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Revert the layer_conv_2d() of the encoder.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Use Leaky Relu as our activation**
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the generator model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: generator
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0448-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 12.5.6 The adversarial network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we’ll set up the GAN, which chains the generator and the discriminator.
    When trained, this model will move the generator in a direction that improves
    its ability to fool the discriminator. This model turns latent-space points into
    a classification decision— “fake” or “real”—and it’s meant to be trained with
    labels that are always “these are real images.” So training gan will update the
    weights of generator in a way that makes discriminator more likely to predict
    “real” when looking at fake images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recapitulate, this is what the training loop looks like schematically. For
    each epoch, you do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Draw random points in the latent space (random noise).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Generate images with generator using this random noise.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** Mix the generated images with real ones.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**4** Train discriminator using these mixed images, with corresponding targets:
    either “real” (for the real images) or “fake” (for the generated images).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**5** Draw new random points in the latent space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6** Train generator using these random vectors, with targets that all say
    “these are real images.” This updates the weights of the generator to move them
    toward getting the discriminator to predict “these are real images” for generated
    images: this trains the generator to fool the discriminator.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s implement it. Like in our VAE example, we’ll use a new_model_class() with
    a custom train_step(). Note that we’ll use two optimizers (one for the generator
    and one for the discriminator), so we will also override compile() to allow for
    passing two optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.36 The GAN Model**'
  prefs: []
  type: TYPE_NORMAL
- en: GAN <— new_model_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "GAN",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(discriminator, generator, latent_dim) {
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize()
  prefs: []
  type: TYPE_NORMAL
- en: self$discriminator  <— discriminator
  prefs: []
  type: TYPE_NORMAL
- en: self$generator      <— generator
  prefs: []
  type: TYPE_NORMAL
- en: self$latent_dim     <— as.integer(latent_dim)
  prefs: []
  type: TYPE_NORMAL
- en: self$d_loss_metric  <— metric_mean(name = "d_loss")➊
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric  <— metric_mean(name = "g_loss")➊
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: compile = function(d_optimizer, g_optimizer, loss_fn) {
  prefs: []
  type: TYPE_NORMAL
- en: super$compile()
  prefs: []
  type: TYPE_NORMAL
- en: self$d_optimizer <— d_optimizer
  prefs: []
  type: TYPE_NORMAL
- en: self$g_optimizer <— g_optimizer
  prefs: []
  type: TYPE_NORMAL
- en: self$loss_fn <— loss_fn
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: metrics = mark_active(function() {
  prefs: []
  type: TYPE_NORMAL
- en: list(self$d_loss_metric,
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric)
  prefs: []
  type: TYPE_NORMAL
- en: '}),'
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(real_images) {➋
  prefs: []
  type: TYPE_NORMAL
- en: batch_size <— tf$shape(real_images)[1]
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <➌
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(batch_size, self$latent_dim))
  prefs: []
  type: TYPE_NORMAL
- en: generated_images <
  prefs: []
  type: TYPE_NORMAL
- en: self$generator(random_latent_vectors)➍
  prefs: []
  type: TYPE_NORMAL
- en: combined_images <
  prefs: []
  type: TYPE_NORMAL
- en: tf$concat(list(generated_images,
  prefs: []
  type: TYPE_NORMAL
- en: real_images),➎
  prefs: []
  type: TYPE_NORMAL
- en: axis = 0L)
  prefs: []
  type: TYPE_NORMAL
- en: labels <
  prefs: []
  type: TYPE_NORMAL
- en: tf$concat(list(tf$ones(tuple(batch_size, 1L)),➏
  prefs: []
  type: TYPE_NORMAL
- en: tf$zeros(tuple(batch_size, 1L))),
  prefs: []
  type: TYPE_NORMAL
- en: axis = 0L)
  prefs: []
  type: TYPE_NORMAL
- en: labels %<>% `+`(
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$uniform(tf$shape(.), maxval = 0.05))➐
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: predictions <— self$discriminator(combined_images)
  prefs: []
  type: TYPE_NORMAL
- en: d_loss <— self$loss_fn(labels, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(d_loss, self$discriminator$trainable_weights)
  prefs: []
  type: TYPE_NORMAL
- en: self$d_optimizer$apply_gradients(➑
  prefs: []
  type: TYPE_NORMAL
- en: zip_lists(grads, self$discriminator$trainable_weights))
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <➒
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(batch_size, self$latent_dim))
  prefs: []
  type: TYPE_NORMAL
- en: misleading_labels <— tf$zeros(tuple(batch_size, 1L))➓
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  prefs: []
  type: TYPE_NORMAL
- en: predictions <— random_latent_vectors %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$generator() %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$discriminator()
  prefs: []
  type: TYPE_NORMAL
- en: g_loss <— self$loss_fn(misleading_labels, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(g_loss, self$generator$trainable_weights)
  prefs: []
  type: TYPE_NORMAL
- en: self$g_optimizer$apply_gradients(⓫
  prefs: []
  type: TYPE_NORMAL
- en: zip_lists(grads, self$generator$trainable_weights))
  prefs: []
  type: TYPE_NORMAL
- en: self$d_loss_metric$update_state(d_loss)
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric$update_state(g_loss)
  prefs: []
  type: TYPE_NORMAL
- en: list(d_loss = self$d_loss_metric$result(),
  prefs: []
  type: TYPE_NORMAL
- en: g_loss = self$g_loss_metric$result())
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Set up metrics to track the two losses over each training epoch.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **train_step is called with a batch of real images.**
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Sample random points in the latent space.**
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Decode them to fake images.**
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Combine them with real images.**
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Assemble labels, discriminating real from fake images.**
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add random noise to the labels—an important trick!**
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Train the discriminator.**
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Sample random points in the latent space.**
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Assemble labels that say "these are all real images" (it's a lie!).**
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ **Train the generator.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start training, let’s also set up a callback to monitor our results:
    it will use the generator to create and save a number of fake images at the end
    of each epoch.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.37 A callback that samples generated images during training
  prefs: []
  type: TYPE_NORMAL
- en: callback_gan_monitor <— new_callback_class(
  prefs: []
  type: TYPE_NORMAL
- en: classname = "GANMonitor",
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(num_img = 3, latent_dim = 128,
  prefs: []
  type: TYPE_NORMAL
- en: dirpath = "gan_generated_images") {
  prefs: []
  type: TYPE_NORMAL
- en: private$num_img <— as.integer(num_img)
  prefs: []
  type: TYPE_NORMAL
- en: private$latent_dim <— as.integer(latent_dim)
  prefs: []
  type: TYPE_NORMAL
- en: private$dirpath <— fs::path(dirpath)
  prefs: []
  type: TYPE_NORMAL
- en: fs::dir_create(dirpath)
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_end = function(epoch, logs = NULL) {
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(private$num_img, private$latent_dim))
  prefs: []
  type: TYPE_NORMAL
- en: generated_images <— random_latent_vectors %>%
  prefs: []
  type: TYPE_NORMAL
- en: self$model$generator() %>%
  prefs: []
  type: TYPE_NORMAL
- en: '{ tf$saturate_cast(. * 255, "uint8") }➊'
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(private$num_img))
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$write_file(
  prefs: []
  type: TYPE_NORMAL
- en: filename = private$dirpath / sprintf("img_%03i_%02i.png", epoch, i),
  prefs: []
  type: TYPE_NORMAL
- en: contents = tf$io$encode_png(generated_images[i, , , ])
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Scale and clip to uint8 range of [0, 255], and cast to uint8.**
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can start training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.38 Compiling and training the GAN**'
  prefs: []
  type: TYPE_NORMAL
- en: epochs <— 100➊
  prefs: []
  type: TYPE_NORMAL
- en: gan <— GAN(discriminator = discriminator,➋
  prefs: []
  type: TYPE_NORMAL
- en: generator = generator,
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim = latent_dim)
  prefs: []
  type: TYPE_NORMAL
- en: gan %>% compile(
  prefs: []
  type: TYPE_NORMAL
- en: d_optimizer = optimizer_adam(learning_rate = 0.0001),
  prefs: []
  type: TYPE_NORMAL
- en: g_optimizer = optimizer_adam(learning_rate = 0.0001),
  prefs: []
  type: TYPE_NORMAL
- en: loss_fn = loss_binary_crossentropy()
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: gan %>% fit(
  prefs: []
  type: TYPE_NORMAL
- en: dataset,
  prefs: []
  type: TYPE_NORMAL
- en: epochs = epochs,
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callback_gan_monitor(num_img = 10, latent_dim = latent_dim)
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **You'll start getting interesting results after epoch 20.**
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Instantiate the GAN model.**
  prefs: []
  type: TYPE_NORMAL
- en: When training, you may see the adversarial loss begin to increase considerably,
    whereas the discriminative loss tends to zero—the discriminator may end up dominating
    the generator. If that’s the case, try reducing the discriminator learning rate
    and increasing the dropout rate of the discriminator. [Figure 12.22](#fig12-22)
    shows what our GAN is capable of generating after 30 epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0452-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.22 Some generated images around epoch 30**'
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.7 Wrapping up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A GAN consists of a generator network coupled with a discriminator network The
    discriminator is trained to differentiate between the output of the generator
    and real images from a training dataset, and the generator is trained to fool
    the discriminator. Remarkably, the generator never sees images from the training
    set directly; the information it has about the data comes from the discriminator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs are difficult to train, because training a GAN is a dynamic process rather
    than a simple gradient-descent process with a fixed loss landscape. Getting a
    GAN to train correctly requires using a number of heuristic tricks, as well as
    extensive tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs can potentially produce highly realistic images. But unlike VAEs, the latent
    space they learn doesn’t have a neat continuous structure and thus may not be
    suited for certain practical applications, such as image editing via latent-space
    concept vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These few techniques cover only the basics of this fast-expanding field. There’s
    a lot more to discover out there—generative deep learning is deserving of an entire
    book of its own.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use a sequence-to-sequence model to generate sequence data, one step
    at a time. This is applicable to text generation but also to note-by-note music
    generation or any other type of time-series data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepDream works by maximizing the convnet layer activations through gradient
    ascent in input space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the style-transfer algorithm, a content image and a style image are combined
    via gradient descent to produce an image with the high-level features of the content
    image and the local characteristics of the style image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAEs and GANs are models that learn a latent space of images and can then dream
    up entirely new images by sampling from the latent space. Concept vectors in the
    latent space can even be used for image editing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](#endnote1)) Iannis Xenakis, “Musiques formelles: nouveaux principes formels
    de composition musicale,” special issue of *La Revue musicale*, nos. 253–254 (1963).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([2](#endnote2)) Alex Graves, “Generating Sequences with Recurrent Neural Networks,”
    arXiv (2013), [https://arxiv.org/abs/1308.0850](https://arxiv.org/abs/1308.0850).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '^([3](#endnote3)) Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “DeepDream:
    A Code Example for Visualizing Neural Networks,” Google Research Blog, July 1,
    2015, [http://mng.bz/xXlM](http://mng.bz/xXlM).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([4](#endnote4)) Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A
    Neural Algorithm of Artistic Style,” arXiv (2015), [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([5](#endnote5)) Diederik P. Kingma and Max Welling, “Auto-Encoding Variational
    Bayes,” arXiv (2013), [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([6](#endnote6)) Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra,
    “Stochastic Backpropagation and Approximate Inference in Deep Generative Models,”
    arXiv (2014), [https://arxiv.org/abs/1401.4082](https://arxiv.org/abs/1401.4082).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([7](#endnote7)) Ian Goodfellow et al., “Generative Adversarial Networks,”
    arXiv (2014), [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
