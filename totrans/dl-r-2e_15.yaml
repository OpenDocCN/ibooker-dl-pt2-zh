- en: 12 Generative deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 生成深度学习
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Text generation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: DeepDream
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDream
- en: Neural style transfer
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经风格转换
- en: Variational autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: Generative adversarial networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: The potential of artificial intelligence to emulate human thought processes
    goes beyond passive tasks such as object recognition and mostly reactive tasks
    such as driving a car. It extends well into creative activities. When I first
    made the claim that in the not-so-distant future, most of the cultural content
    that we consume will be created with substantial help from AIs, I was met with
    utter disbelief, even from long-time machine learning practitioners. That was
    in 2014\. Fast-forward a few years, and the disbelief had receded at an incredible
    speed. In the summer of 2015, we were entertained by Google’s DeepDream algorithm
    turning an image into a psychedelic mess of dog eyes and pareidolic artifacts;
    in 2016, we started using smartphone applications to turn photos into paintings
    of various styles. In the summer of 2016, an experimental short movie, *Sunspring*,
    was directed using a script written by a long short-term memory (LSTM). Maybe
    you’ve recently listened to music that was generated by a neural network.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模拟人类思维过程的潜力不仅限于客观任务，如物体识别，也包括大部分是被动任务，如驾车。它还延伸到创造性活动。当我第一次声称在不久的将来，我们消费的大部分文化内容将在很大程度上在AI的帮助下创作时，甚至从事机器学习已久的从业者也对此表示怀疑。那是在2014年。几年后，怀疑以惊人的速度消退。2015年夏天，我们被Google的DeepDream算法转换成一幅充满狗眼和类似图像的迷幻图片所娱乐；2016年，我们开始使用智能手机应用程序将照片转换成各种风格的绘画；2016年夏天，一部实验性的短片《夕阳之泉》是由长短期记忆（LSTM）写的剧本导演而成的。也许你最近听过由神经网络生成的音乐。
- en: 'Granted, the artistic productions we’ve seen from AI so far have been fairly
    low quality. AI isn’t anywhere close to rivaling human screenwriters, painters,
    and composers. But replacing humans was always beside the point: artificial intelligence
    isn’t about replacing our own intelligence with something else; it’s about bringing
    into our lives and work *more* intelligence—intelligence of a different kind.
    In many fields, but especially in creative ones, AI will be used by humans as
    a tool to augment their own capabilities: more *augmented intelligence* than *artificial
    intelligence*.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，到目前为止，我们从AI中看到的艺术作品质量相当低。AI离得上人类编剧、画家和作曲家还远。但替代人类本来就不是重点：人工智能不是要用其他东西取代我们自己的智能，而是要给我们的生活和工作带来*更多*的智能——不同类型的智能。在许多领域，尤其是创造性领域，AI将被人类用作增强自身能力的工具：更多的*增强智能*而不是*人工智能*。
- en: 'A large part of artistic creation consists of simple pattern recognition and
    technical skill. And that’s precisely the part of the process that many find less
    attractive or even dispensable. That’s where AI comes in. Our perceptual modalities,
    our language, and our artwork all have statistical structure. Learning this structure
    is what deep learning algorithms excel at. Machine learning models can learn the
    statistical *latent space* of images, music, and stories, and they can then *sample*
    from this space, creating new artworks with characteristics similar to those the
    model has seen in its training data. Naturally, such sampling is hardly an act
    of artistic creation in itself. It’s a mere mathematical operation: the algorithm
    has no grounding in human life, human emotions, or our experience of the world;
    instead, it learns from an experience that has little in common with ours. It’s
    only our interpretation, as human spectators, that will give meaning to what the
    model generates. But in the hands of a skilled artist, algorithmic generation
    can be steered to become meaningful—and beautiful. Latent space sampling can become
    a brush that empowers the artist, augments our creative affordances, and expands
    the space of what we can imagine. What’s more, it can make artistic creation more
    accessible by eliminating the need for technical skill and practice, setting up
    a new medium of pure expression, factoring art apart from craft.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Iannis Xenakis, a visionary pioneer of electronic and algorithmic music, beautifully
    expressed this same idea in the 1960s, in the context of the application of automation
    technology to music composition:^([1](#Rendnote1))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '*Freed from tedious calculations, the composer is able to devote himself to
    the general problems that the new musical form poses and to explore the nooks
    and crannies of this form while modifying the values of the input data. For example,
    he may test all instrumental combinations from soloists, to chamber orchestras,
    to large orchestras. With the aid of electronic computers the composer becomes
    a sort of pilot: he presses the buttons, introduces coordinates, and supervises
    the controls of a cosmic vessel sailing in the space of sound, across sonic constellations
    and galaxies that he could formerly glimpse only as a distant dream.*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore from various angles the potential of deep learning
    to augment artistic creation. We’ll review sequence data generation (which can
    be used to generate text or music), DeepDream, and image generation using both
    variational autoencoders and generative adversarial networks. We’ll get your computer
    to dream up content never seen before; and maybe we’ll get you to dream, too,
    about the fantastic possibilities that lie at the intersection of technology and
    art. Let’s get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Text generation
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore how recurrent neural networks can be used to
    generate sequence data. We’ll use text generation as an example, but the exact
    same techniques can be generalized to any kind of sequence data: you could apply
    it to sequences of musical notes to generate new music, to time series of brushstroke
    data (perhaps recorded while an artist paints on an iPad) to generate paintings
    stroke by stroke, and so on.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨循环神经网络如何用于生成序列数据。我们以文本生成为例，但完全相同的技术可以推广到任何类型的序列数据：你可以将其应用于音乐音符序列以生成新音乐，应用于笔划数据的时间序列（也许是记录艺术家在iPad上绘画时记录下的）以逐笔生成绘画，等等。
- en: Sequence data generation is in no way limited to artistic content generation.
    It has been successfully applied to speech synthesis and to dialogue generation
    for chatbots. The Smart Reply feature that Google released in 2016, capable of
    automatically generating a selection of quick replies to emails or text messages,
    is powered by similar techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 序列数据生成绝不仅限于艺术内容生成。它已成功应用于语音合成和聊天机器人的对话生成。谷歌于2016年发布的Smart Reply功能，能够自动生成一系列快速回复电子邮件或短信的选项，就是由类似的技术驱动的。
- en: 12.1.1 A brief history of generative deep learning for sequence generation
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 生成式深度学习用于序列生成的简要历史
- en: In late 2014, few people had ever seen the initials LSTM, even in the machine
    learning community. Successful applications of sequence data generation with recurrent
    networks began to appear in the mainstream only in 2016\. But these techniques
    have a fairly long history, starting with the development of the LSTM algorithm
    in 1997 (discussed in chapter 10). This new algorithm was used early on to generate
    text character by character.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到了2014年末，很少有人在机器学习社区甚至见过LSTM这个缩写。成功应用循环网络生成序列数据的案例直到2016年才开始出现在主流中。但是这些技术具有相当长的历史，从1997年LSTM算法的开发开始（在第10章讨论过）。这个新算法最初用于逐字符生成文本。
- en: In 2002, Douglas Eck, then at Schmidhuber’s lab in Switzerland, applied LSTM
    to music generation for the first time, with promising results. Eck is now a researcher
    at Google Brain, and in 2016, he started a new research group there, called Magenta,
    focused on applying modern deep learning techniques to produce engaging music.
    Sometimes good ideas take 15 years to get started.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 2002年，当时在瑞士Schmidhuber实验室的Douglas Eck首次将LSTM应用于音乐生成，并取得了令人鼓舞的结果。Eck现在是Google
    Brain的研究员，在2016年，他在那里成立了一个名为Magenta的新研究组，专注于将现代深度学习技术应用于产生引人入胜的音乐。有时好的想法需要15年才能开始实施。
- en: 'In the late 2000s and early 2010s, Alex Graves did important pioneering work
    using recurrent networks for sequence data generation. In particular, his 2013
    work on applying recurrent mixture density networks to generate human-like handwriting
    using time series of pen positions is seen by some as a turning point.^([2](#Rendnote2))
    This specific application of neural networks at that specific moment in time captured
    for me the notion of *machines that dream* and was a significant inspiration around
    the time I started developing Keras. Graves left a similar commented-out remark
    hidden in a 2013 LaTeX file uploaded to the preprint server arXiv: “Generating
    sequential data is the closest computers get to dreaming.” Several years later,
    we take a lot of these developments for granted, but at the time it was difficult
    to watch Graves’s demonstrations and not walk away awe-inspired by the possibilities.
    Between 2015 and 2017, recurrent neural networks were successfully used for text
    and dialogue generation, music generation, and speech synthesis.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年末和2010年初，Alex Graves通过使用循环网络生成序列数据做出了重要的开创性工作。特别是，他在2013年将循环混合密度网络应用于使用笔位置的时间序列生成类似人类手写的工作被一些人视为一个转折点。在那个特定的时间点上，神经网络的这种特定应用捕捉到了“机器梦想”的概念，并且在我开始开发Keras的时候是一个重要的灵感来源。几年后，我们很多这样的发展已经司空见惯，但是在当时，很难看到Graves的演示而不对可能性感到敬畏。在2015年至2017年期间，循环神经网络成功用于文本和对话生成，音乐生成和语音合成。
- en: Then around 2017–2018, the Transformer architecture started taking over recurrent
    neural networks, not just for supervised natural language processing tasks but
    also for generative sequence models—in particular *language modeling* (word-level
    text generation). The best-known example of a generative Transformer would be
    GPT-3, a 175 billion parameter text-generation model trained by the startup OpenAI
    on an astound-ingly large text corpus, including most digitally available books,
    Wikipedia, and a large fraction of a crawl of the entire internet. GPT-3 made
    headlines in 2020 due to its capability to generate plausible-sounding text paragraphs
    on virtually any topic, a prowess that has fed a short-lived hype wave worthy
    of the most torrid AI summer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在2017-2018年，Transformer架构开始取代递归神经网络，不仅用于监督自然语言处理任务，也用于生成序列模型，特别是*语言建模*（词级文本生成）。最著名的生成式Transformer示例是GPT-3，这是一种1750亿参数的文本生成模型，由初创公司OpenAI在庞大的文本语料库上进行训练，包括大多数数字化的书籍，维基百科以及整个互联网爬取的大部分内容。GPT-3因其生成几乎任何主题的听起来可信的文本段落的能力而在2020年成为头条新闻，这种能力引发了最激烈的短暂人工智能热潮之一。
- en: 12.1.2 How do you generate sequence data?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 如何生成序列数据？
- en: 'The universal way to generate sequence data in deep learning is to train a
    model (usually a Transformer or an RNN) to predict the next token or next few
    tokens in a sequence, using the previous tokens as input. For instance, given
    the input “the cat is on the,” the model is trained to predict the target “mat,”
    the next word. As usual when working with text data, tokens are typically words
    or characters, and any network that can model the probability of the next token
    given the previous ones is called a *language model*. A language model captures
    the *latent space* of language: its statistical structure.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中生成序列数据的通用方法是训练一个模型（通常是Transformer或RNN），以预测序列中下一个标记或下几个标记，使用前面的标记作为输入。例如，给定输入“猫在上面”，模型会被训练以预测目标“垫子”，下一个单词。通常在处理文本数据时，标记通常是单词或字符，并且任何可以模拟给定先前标记情况下下一个标记的概率的网络都称为*语言模型*。语言模型捕捉了语言的*潜在空间*：它的统计结构。
- en: 'Once you have such a trained language model, you can *sample* from it (generate
    new sequences): you feed it an initial string of text (called *conditioning data*),
    ask it to generate the next character or the next word (you can even generate
    several tokens at once), add the generated output back to the input data, and
    repeat the process many times (see [figure 12.1](#fig12-1)). This loop allows
    you to generate sequences of arbitrary length that reflect the structure of the
    data on which the model was trained: sequences that look *almost* like human-written
    sentences.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了训练好的语言模型，你可以从模型中*采样*（生成新的序列）：你馈送一个初始的文本字符串（称为*调节数据*），请求它生成下一个字符或下一个单词（你甚至可以一次生成多个标记），将生成的输出添加回输入数据，并重复这个过程多次（参见[图12.1](#fig12-1)）。这个循环允许您生成任意长度的序列，反映了模型训练的数据结构：几乎像人类书写的句子。
- en: '![Image](../images/f0402-01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0402-01.jpg)'
- en: '**Figure 12.1 The process of word-by-word text generation using a language
    model**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.1 使用语言模型逐字逐句生成文本的过程**'
- en: 12.1.3 The importance of the sampling strategy
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.3 采样策略的重要性
- en: 'When generating text, the way you choose the next token is crucially important.
    A naive approach is *greedy sampling*, consisting of always choosing the most
    likely next character. But such an approach results in repetitive, predictable
    strings that don’t look like coherent language. A more interesting approach makes
    slightly more surprising choices: it introduces randomness in the sampling process
    by sampling from the probability distribution for the next character. This is
    called *stochastic sampling* (recall that *stochasticity* is what we call *randomness*
    in this field). In such a setup, if a word has a 0.3 probability of being next
    in the sentence according to the model, you’ll choose it 30% of the time. Note
    that greedy sampling can also be cast as sampling from a probability distribution:
    one where a certain word has probability 1 and all others have probability 0.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成文本时，选择下一个标记的方式非常重要。一种简单的方法是*贪心抽样*，总是选择可能性最高的下一个字符。但这种方法会产生重复、可预测的字符串，不像是连贯的语言。一种更有趣的方法是做出稍微意外的选择：通过从下一个字符的概率分布中抽样，在抽样过程中引入随机性。这被称为*随机抽样*（这里需要注意的是，在这个领域中，*随机性*称为*随机性*）。在这样的设置中，如果根据模型，某个词在句子中作为下一个出现的概率为0.3，那么你将有30%的概率选择它。需要注意的是，贪心抽样也可以看作是从概率分布中进行抽样：其中某个词的概率为1，其他所有词的概率都为0。
- en: 'Sampling probabilistically from the softmax output of the model is neat: it
    allows even unlikely words to be sampled some of the time, generating more interesting-looking
    sentences and sometimes showing creativity by coming up with new, realistic-sounding
    sentences that didn’t occur in the training data. But there’s one issue with this
    strategy: it doesn’t offer a way to *control the amount of randomness* in the
    sampling process.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的 softmax 输出中以概率的方式抽样是不错的方法：即使是不太可能的单词也有可能被抽样到，这样生成的句子更有趣，有时甚至能创造出之前在训练数据中没有出现过的、听起来很真实的句子。但这种策略存在一个问题：它没有提供一种*控制随机性的方法*。
- en: 'Why would you want more or less randomness? Consider an extreme case: pure
    random sampling, where you draw the next word from a uniform probability distribution,
    and every word is equally likely. This scheme has maximum randomness; in other
    words, this probability distribution has maximum entropy. Naturally, it won’t
    produce anything interesting. At the other extreme, greedy sampling doesn’t produce
    anything interesting, either, and has no randomness: the corresponding probability
    distribution has minimum entropy. Sampling from the “real” probability distribution—
    the distribution that is output by the model’s softmax function—constitutes an
    intermediate point between these two extremes. But there are many other intermediate
    points of higher or lower entropy that you may want to explore. Less entropy will
    give the generated sequences a more predictable structure (and, thus, they will
    potentially be more realistic looking), whereas more entropy will result in more
    surprising and creative sequences. When sampling from generative models, it’s
    always good to explore different amounts of randomness in the generation process.
    Because we—humans— are the ultimate judges of how interesting the generated data
    is, interestingness is highly subjective, and there’s no telling in advance where
    the point of optimal entropy lies.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你想要更多或者更少的随机性呢？考虑一个极端情况：纯随机抽样，你从一个均匀概率分布中抽取下一个词，每个词的概率都是相等的。这种方案具有最大的随机性；换句话说，该概率分布具有最大的熵。显然，它不会产生任何有趣的结果。另一方面，贪心抽样也不会产生有趣的结果，而且没有随机性：相应的概率分布具有最小的熵。从“真实”的概率分布中抽样——即模型的
    softmax 函数输出的分布——构成了这两个极端之间的一个中间点。但是，在更高或更低熵的许多其他中间点上也可以进行抽样，你可能想要在其中进行探索。较低的熵会给生成的序列提供一个更可预测的结构（因此，它们有可能看起来更真实），而较高的熵会产生更令人惊讶和富有创造力的序列。在从生成模型进行抽样时，探索不同随机性的产生过程是很有意义的。因为我们——人类——是对生成数据的有趣程度的终极评判者，所以有趣程度是非常主观的，无法事先确定最佳熵值所在的位置。
- en: 'To control the amount of stochasticity in the sampling process, we’ll introduce
    a parameter called the *softmax temperature*, which characterizes the entropy
    of the probability distribution used for sampling: it characterizes how surprising
    or predictable the choice of the next word will be. Given a temperature value,
    a new probability distribution is computed from the original one (the softmax
    output of the model) by reweighting it in the following way.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Higher temperatures result in sampling distributions of higher entropy that
    will generate more surprising and unstructured generated data, whereas a lower
    temperature will result in less randomness and much more predictable generated
    data (see [figure 12.2](#fig12-2)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Reweighting a probability distribution to a different temperature
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: reweight_distribution <-
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: function(original_distribution, temperature = 0.5) {
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: original_distribution %>% .➊
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '{ exp(log(.) / temperature) } %>%'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '{ . / sum(.) } ➋'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '} ➌'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **original_distribution is a 1D array of probability values that must sum
    to 1. temperature is a factor quantifying the entropy of the output distribution.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Return a reweighted version of the original distribution. The sum of the
    distribution may no longer be 1, so you divide it by its sum to obtain the new
    distribution.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Note that reweight_distribution() will work for both 1D R vectors and 1D
    Tensorflow tensors, because exp, log, /, and sum are all R generics.**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0404-01.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.2 Different reweightings of one probability distribution: Low temperature
    = more deterministic; high temperature = more random**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.4 Implementing text generation with Keras
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s put these ideas into practice in a Keras implementation. The first thing
    you need is a lot of text data that you can use to learn a language model. You
    can use any sufficiently large text file or set of text files—Wikipedia, *The
    Lord of the Rings*, and so on.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’ll keep working with the IMDB movie review dataset from
    the last chapter, and we’ll learn to generate never-read-before movie reviews.
    As such, our language model will be a model of the style and topics of these movie
    reviews specifically, rather than a general model of the English language.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: PREPARING THE DATA
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like in the previous chapter, let’s download and uncompress the IMDB movie
    reviews dataset. (This is the same dataset we downloaded in chapter 11.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Downloading and uncompressing the IMDB movie reviews dataset
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: url <— "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: filename <— basename(url)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: options(timeout = 60 * 10)➊
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: download.file(url, destfile = filename)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: untar(filename)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **10-minute timeout**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'You’re already familiar with the structure of the data: we get a folder named
    aclImdb containing two subfolders, one for negative-sentiment movie reviews and
    one for positive-sentiment reviews. There’s one text file per review. We’ll call
    text_dataset_ from_directory() with label_mode = NULL to create a TF Dataset that
    reads from these files and yields the text content of each file.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经熟悉数据的结构：我们得到一个名为aclImdb的文件夹，其中包含两个子文件夹，一个用于负面情感的电影评论，一个用于正面情感的评论。每个评论都有一个文本文件。我们将调用text_dataset_from_directory()并将label_mode
    = NULL作为参数，以创建一个TF数据集，该数据集从这些文件中读取并生成每个文件的文本内容。
- en: '**Listing 12.3 Creating a TF Dataset from text files (one file = one sample)**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单12.3 从文本文件创建TF数据集（一个文件 = 一个样本）**'
- en: library(tensorflow)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: library(tfdatasets)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: library(keras)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: dataset <— text_dataset_from_directory(
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: dataset <— text_dataset_from_directory(
- en: directory = "aclImdb",
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: directory = "aclImdb",
- en: label_mode = NULL,
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: label_mode = NULL,
- en: batch_size = 256)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 256)
- en: dataset <— dataset %>%
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: dataset <— dataset %>%
- en: dataset_map( ~ tf$strings$regex_replace(.x, "<br />", " "))➊
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map( ~ tf$strings$regex_replace(.x, "<br />", " "))➊
- en: ➊ **Strip the "<br />" HTML tag that occurs in many of the reviews. This did
    not matter much for text classification, but we wouldn't want to generate "<br
    />" tags in this example!**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **去除许多评论中出现的"<br />" HTML标签。这在文本分类中并不重要，但在这个例子中我们不想生成"<br />"标签！**
- en: 'Now let’s use a layer_text_vectorization() to compute the vocabulary we’ll
    be working with. We’ll use only the first sequence_length words of each review:
    our layer_ text_vectorization() will cut off anything beyond that when vectorizing
    a text.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用一个layer_text_vectorization()来计算我们将使用的词汇表。我们只使用每个评论的前sequence_length个单词：当向量化文本时，我们的layer_text_vectorization()将在超出这个范围时截断任何内容。
- en: Listing 12.4 Preparing a layer_text_vectorization()
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 清单12.4 准备一个layer_text_vectorization()
- en: sequence_length <— 100
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: sequence_length <— 100
- en: vocab_size <— 15000➊
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: vocab_size <— 15000➊
- en: text_vectorization <— layer_text_vectorization(
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <— layer_text_vectorization(
- en: max_tokens = vocab_size,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = vocab_size,
- en: output_mode = "int",➋
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "int",➋
- en: output_sequence_length = sequence_length➌
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: output_sequence_length = sequence_length➌
- en: )
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: adapt(text_vectorization, dataset)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: adapt(text_vectorization, dataset)
- en: ➊ **We'll consider only the top 15,000 most common words—anything else will
    be treated as the out-of-vocabulary token, "[UNK]".**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们将只考虑前15000个最常见的单词——其他任何单词都将被视为未知标记"[UNK]"。**
- en: ➋ **We want to return integer word index sequences.**
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们想返回整数单词索引序列。**
- en: ➌ **We'll work with inputs and targets of length 100 (but because we'll offset
    the targets by 1, the model will actually see sequences of length 99).**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **我们将使用长度为100的输入和目标（但因为我们将目标偏移1，所以模型实际上将看到长度为99的序列）。**
- en: Let’s use the layer to create a language-modeling dataset where input samples
    are vectorized texts and corresponding targets are the same texts offset by one
    word.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用该层来创建一个语言建模数据集，其中输入样本是向量化的文本，相应的目标是将文本偏移一个单词后的相同文本。
- en: Listing 12.5 Setting up a language-modeling dataset
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 清单12.5 设置语言建模数据集
- en: prepare_lm_dataset <— function(text_batch) {
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: prepare_lm_dataset <— function(text_batch) {
- en: vectorized_sequences <— text_vectorization(text_batch)➊
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: vectorized_sequences <— text_vectorization(text_batch)➊
- en: x <— vectorized_sequences[, NA:-2]➋
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: x <— vectorized_sequences[, NA:-2]➋
- en: y <— vectorized_sequences[, 2:NA]➌
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: y <— vectorized_sequences[, 2:NA]➌
- en: list(x, y)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: list(x, y)
- en: '}'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: lm_dataset <— dataset %>%
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: lm_dataset <— dataset %>%
- en: dataset_map(prepare_lm_dataset, num_parallel_calls = 4)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(prepare_lm_dataset, num_parallel_calls = 4)
- en: ➊ **Convert a batch of texts (strings) to a batch of integer sequences.**
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将一批文本（字符串）转换为一批整数序列。**
- en: ➋ **Create inputs by cutting off the last word of the sequences (drop last column).**
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **通过截取序列的最后一个单词来创建输入（删除最后一列）。**
- en: ➌ **Create targets by offsetting the sequences by 1 (drop first column).**
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **通过将序列偏移1来创建目标（删除第一列）。**
- en: A TRANSFORMER-BASED SEQUENCE-TO-SEQUENCE MODEL
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于TRANSFORMER的序列到序列模型
- en: We’ll train a model to predict a probability distribution over the next word
    in a sentence, given a number of initial words. When the model is trained, we’ll
    feed it with a prompt, sample the next word, add that word back to the prompt,
    and repeat, until we’ve generated a short paragraph.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个模型，来预测句子中下一个单词的概率分布，给定一些初始单词。当模型训练完成后，我们将向其提供一个提示，采样下一个单词，将该单词添加回提示中，并重复此过程，直到生成一个短段落。
- en: Like we did for temperature forecasting in chapter 10, we could train a model
    that takes as input a sequence of *N* words and simply predicts word *N* +1\.
    However, several issues exist with this setup in the context of sequence generation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: First, the model would learn to produce predictions only when *N* words were
    available, but it would be useful to be able to start predicting with fewer than
    *N* words. Otherwise, we’d be constrained to using only relatively long prompts
    (in our implementation, *N* = 100 words). We didn’t have this need in chapter
    10.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, many of our training sequences will be mostly overlapping. Consider
    N = 4. The text “A complete sentence must have, at minimum, three things: a subject,
    verb, and an object” would be used to generate the following training sequences:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: “A complete sentence must”
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “complete sentence must have”
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “sentence must have at”
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “and so on, until “verb and an object”
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model that treats each such sequence as an independent sample would have to
    do a lot of redundant work, re-encoding multiple times subsequences that it has
    largely seen before. In chapter 10, this wasn’t much of a problem, because we
    didn’t have that many training samples in the first place, and we needed to benchmark
    dense and convolutional models, for which redoing the work every time is the only
    option. We could try to alleviate this redundancy problem by using *strides* to
    sample our sequences— skipping a few words between two consecutive samples. But
    that would reduce our number of training samples while providing only a partial
    solution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these two issues, we’ll use a *sequence-to-sequence model*: we’ll
    feed sequences of *N* words (indexed from *1* to *N*) into our model, and we’ll
    predict the sequence offset by one (from *2* to *N+1*). We’ll use causal masking
    to make sure that, for any *i*, the model will use only words from *1* to *i*
    to predict the word *i + 1*. This means that we’re simultaneously training the
    model to solve *N* mostly overlapping but different problems: predicting the next
    words given a sequence of 1 <= i <= N prior words (see [figure 12.3](#fig12-3)).
    At generation time, even if you prompt the model with only a single word, it will
    be able to give you a probability distribution for the next possible words.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0407-01.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.3 Compared to plain next-word prediction, sequence-to-sequence
    modeling simultaneously optimizes for multiple prediction problems.**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we could have used a similar sequence-to-sequence setup on our temperature-forecasting
    problem in chapter 10: given a sequence of 120 hourly data points, learn to generate
    a sequence of 120 temperatures offset by 24 hours in the future. You’d be solving
    not only the initial problem but also the 119 related problems of forecasting
    temperature in 24 hours, given 1 <= i < 120 prior hourly data points. If you try
    to retrain the RNNs from chapter 10 in a sequence-to-sequence setup, you’ll find
    that you get similar but incrementally worse results, because the constraint of
    solving these additional 119 related problems with the same model interferes slightly
    with the task we actually do care about.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, you learned about the setup you can use for sequence-to-sequence
    learning in the general case: feed the source sequence into an encoder, and then
    feed both the encoded sequence and the target sequence into a decoder that tries
    to predict the same target sequence, offset by one step. When you’re doing text
    generation, there is no source sequence: you’re just trying to predict the next
    tokens in the target sequence given past tokens, which we can do using only the
    decoder. And thanks to causal padding, the decoder will look at only words *1…N*
    to predict the word *N+1*.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement our model—we’re going to reuse the building blocks we created
    in chapter 11: layer_positional_embedding() and layer_transformer_decoder().'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.6 A simple Transformer-based language model
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: embed_dim <- 256
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <- 2048
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: num_heads <- 2
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: transformer_decoder <-
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: layer_transformer_decoder(NULL, embed_dim, latent_dim, num_heads)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: outputs <- inputs %>%
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: transformer_decoder(., .) %>%
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(vocab_size, activation = "softmax")➊
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: model <—
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: keras_model(inputs, outputs) %>%
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: compile(loss = "sparse_categorical_crossentropy",
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: optimizer = "rmsprop")
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Softmax over possible vocabulary words, computed for each output sequence
    time step.**
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.5 A text-generation callback with variable-temperature sampling
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll use a callback to generate text using a range of different temperatures
    after every epoch. This allows you to see how the generated text evolves as the
    model begins to converge, as well as the impact of temperature in the sampling
    strategy. To seed text generation, we’ll use the prompt “this movie”: all of our
    generated texts will start with this.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s define some functions to generate sentences. Later, we’ll use these
    functions in a callback.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: vocab <— get_vocabulary(text_vectorization) ➊
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: sample_next <— function(predictions, temperature = 1.0) {
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: predictions %>%
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: reweight_distribution(temperature) %>% ➋
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: sample.int(length(.), 1, prob = .)➌
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: generate_sentence <—
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: function(model, prompt, generate_length, temperature) {
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— prompt➍
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(generate_length)) {➎
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: model_preds <— sentence %>%
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: array(dim = c(1, 1)) %>%
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization() %>%
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: predict(model, .) ➏
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: sampled_word <— model_preds %>%
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: .[1, i, ] %>%➐
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: sample_next(temperature) %>%➑
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: vocab[.]➒
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— paste(sentence, sampled_word)➓
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: sentence
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The vector we will use to convert word indices (integers) back to strings,
    to be used for text decoding**
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The temperature to use for sampling**
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Implement variable-temperature sampling from a probability distribution.**
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Prompt that we use to seed text generation**
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Iterate for how many words to generate.**
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Feed the current sequence into our model.**
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Retrieve the predictions for the last time step…**
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **…and use them to sample a new token…**
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **…and convert the token integer to a string.**
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Append the new word to the current sequence and repeat.**
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: sample_next() and generate_sentence() do the work of generating sentences from
    a model. They work eagerly; they call predict() to produce predictions as R arrays,
    call sample.int() to pick the next token, and build up the sentence as an R string
    with paste().
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we may want to generate many sentences, it makes sense to optimize
    it a little. We can speed up generate_sentence considerably (~25x) by rewriting
    it as a tf_function(). To do this, we just need to replace a few R functions with
    TensorFlow equivalents. Instead of for(i in seq()), we can write for(i in tf$range()).
    We can also substitute sample.int() with tf$random$categorical(), paste() with
    tf$strings$join(), and predict(model, .) with model(.). Here is what sample_ next()
    and generate_sentence() look like as tf_function()s:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: tf_sample_next <— function(predictions, temperature = 1.0) {
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: predictions %>%
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: reweight_distribution(temperature) %>%
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '{ log(.[tf$newaxis, ]) } %>% ➊'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$categorical(1L) %>%
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: tf$reshape(shape())➋
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: library(tfautograph)➌
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: tf_generate_sentence <— tf_function(
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: function(model, prompt, generate_length, temperature) {
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: withr::local_options(tensorflow.extract.style = "python")
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: vocab <— as_tensor(vocab)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— prompt %>% as_tensor(shape = c(1, 1))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: ag_loop_vars(sentence)➍
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: for (i in tf$range(generate_length)) {
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: model_preds <— sentence %>%
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: text_vectorization() %>%
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: model()
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: sampled_word <— model_preds %>%
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: .[0, i, ] %>%
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: tf_sample_next(temperature) %>%
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: vocab[.]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— sampled_word %>%
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '{ tf$strings$join(c(sentence, .), " ") }'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: sentence %>% tf$reshape(shape())➎
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **tf$random$catagorical() expects a batch of log probabilities.**
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **tf$random$catagorical() returns a scalar integer with shape (1, 1). Reshape
    to shape ().**
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **For ag_loop_vars() (more on this soon)**
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Provide a hint to the compiler that `sentence` is the only variable we want
    after iteration.**
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Reshape from (1, 1) to (). Note that tf$strings$join() preserves sentence's
    (1, 1) shape throughout iteration.**
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: On my machine, generating a sentence of 50 words takes approx 2.5 seconds with
    the eager generate_sentence(), and .1 seconds with tf_generate_sentence(), a 25×
    improvement! Remember, it always makes sense to prototype your code first by running
    it eagerly, and only move to using tf_function() once you have it working how
    you want.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '**for loops and autograph**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'One wrinkle with evaluating R functions eagerly before wrapping them with tf_
    function(fn, autograph = TRUE) (the default) is that autograph = TRUE gives capabilities
    that base R doesn’t have, like the ability for for to iterate over tensors. You
    can still evaluate expressions like for(i in tf$range()) or for(batch in tf_ dataset)
    eagerly by calling tfautograph::autograph() directly, like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: library(tfautograph)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: autograph({
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: for(i in tf$range(3L))
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: print(i)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(0, shape=(), dtype=int32)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(1, shape=(), dtype=int32)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(2, shape=(), dtype=int32)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: or
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: fn <— function(x) {
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: for(i in x) print(i)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: ag_fn <— autograph(fn)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: ag_fn(tf$range(3))
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(0.0, shape=(), dtype=float32)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(1.0, shape=(), dtype=float32)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(2.0, shape=(), dtype=float32)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In interactive sessions you can temporarily globally enable if, while, and for
    to accept tensors by calling tfautograph:::attach_ag_mask().
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: A for() loop that iterates over a tensor in a tf_function() builds a tf$while_loop(),
    and it inherits all same restrictions. Every tensor tracked by the loop must have
    a stable shape and dtype throughout iteration.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The call ag_loop_vars(sentence) gives the tf_function() compiler a hint that
    the only variable we’re interested in after the for loop is sentence. This informs
    the compiler that other tensors, like sampled_word, i, and model_preds, are loop-local
    variables and can be safely optimized away after the loop.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Note that iterating over a regular R object like for(i in seq(0, 49)) in a tf_
    function() would not build a tf$while_loop(), but would instead evaluate with
    regular R semantics and would result in the tf_function() tracing an unrolled
    loop (which is sometimes preferable, for short loops with a fixed number of iterations).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the callback where we’ll call tf_generate_sentence() to generate text
    during training:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.7 The text-generation callback
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: callback_text_generator <— new_callback_class(
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: classname = "TextGenerator",
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(prompt, generate_length,
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: temperatures = 1,
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: print_freq = 1L) {
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: private$prompt <— as_tensor(prompt, "string")
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: private$generate_length <— as_tensor(generate_length, "int32")
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: private$temperatures <— as.numeric(temperatures)➊
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: private$print_freq <— as.integer(print_freq)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_end = function(epoch, logs = NULL) {
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: if ((epoch %% private$print_freq) != 0)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: return()
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: for (temperature in private$temperatures) {➋
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: cat("== Generating with temperature", temperature, "\n")
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: sentence <— tf_generate_sentence(➌
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: self$model,
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: private$prompt,
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: private$generate_length,➍
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: as_tensor(temperature, "float32")
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: cat(as.character(sentence), "\n")
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: text_gen_callback <— callback_text_generator(
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: prompt = "This movie",
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: generate_length = 50,
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: temperatures = c(0.2, 0.5, 0.7, 1., 1.5) ➎
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We'll use a diverse range of temperatures to sample text, to demonstrate
    the effect of temperature on text generation.**
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **This is a regular R for loop iterating eagerly over an R vector.**
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Note we call this function with only tensors and the model, not R numeric
    or character vectors.**
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **These were already cast to Tensors in initialize().**
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The set of temperatures we generate text with**
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Let’s fit() this thing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.8 Fitting the language model**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: model %>%
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: fit(lm_dataset,
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 200,
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = list(text_gen_callback))
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some cherry-picked examples of what we’re able to generate after 200
    epochs of training. Note that punctuation isn’t part of our vocabulary, so none
    of our generated text has any punctuation:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: With temperature=0.2
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a [UNK] of the original movie and the first half hour of the
    movie is pretty good but it is a very good movie it is a good movie for the time
    period”
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a [UNK] of the movie it is a movie that is so bad that it is
    a [UNK] movie it is a movie that is so bad that it makes you laugh and cry at
    the same time it is not a movie i dont think ive ever seen”
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=0.5
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a [UNK] of the best genre movies of all time and it is not a
    good movie it is the only good thing about this movie i have seen it for the first
    time and i still remember it being a [UNK] movie i saw a lot of years”
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is a waste of time and money i have to say that this movie was a
    complete waste of time i was surprised to see that the movie was made up of a
    good movie and the movie was not very good but it was a waste of time and”
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=0.7
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is fun to watch and it is really funny to watch all the characters
    are extremely hilarious also the cat is a bit like a [UNK] [UNK] and a hat [UNK]
    the rules of the movie can be told in another scene saves it from being in the
    back of”
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie is about [UNK] and a couple of young people up on a small boat in
    the middle of nowhere one might find themselves being exposed to a [UNK] dentist
    they are killed by [UNK] i was a huge fan of the book and i havent seen the original
    so it”
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=1.0
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie was entertaining i felt the plot line was loud and touching but
    on a whole watch a stark contrast to the artistic of the original we watched the
    original version of england however whereas arc was a bit of a little too ordinary
    the [UNK] were the present parent [UNK]”
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie was a masterpiece away from the storyline but this movie was simply
    exciting and frustrating it really entertains friends like this the actors in
    this movie try to go straight from the sub thats image and they make it a really
    good tv show”
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With temperature=1.5
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie was possibly the worst film about that 80 women its as weird insightful
    actors like barker movies but in great buddies yes no decorated shield even [UNK]
    land dinosaur ralph ian was must make a play happened falls after miscast [UNK]
    bach not really not wrestlemania seriously sam didnt exist”
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “this movie could be so unbelievably lucas himself bringing our country wildly
    funny things has is for the garish serious and strong performances colin writing
    more detailed dominated but before and that images gears burning the plate patriotism
    we you expected dyan bosses devotion to must do your own duty and another”
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, a low temperature value results in very boring and repetitive
    text and can sometimes cause the generation process to get stuck in a loop. With
    higher temperatures, the generated text becomes more interesting, surprising,
    and even creative. With a very high temperature, the local structure starts to
    break down, and the output looks largely random. Here, a good generation temperature
    would seem to be about 0.7\. Always experiment with multiple sampling strategies!
    A clever balance between learned structure and randomness is what makes generation
    interesting.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that by training a bigger model, longer, on more data, you can achieve
    generated samples that look far more coherent and realistic than this one—the
    output of a model like GPT-3 is a good example of what can be done with language
    models (GPT-3 is effectively the same thing as what we trained in this example,
    but with a deep stack of Transformer decoders, and a much bigger training corpus).
    But don’t expect to ever generate any meaningful text, other than through random
    chance and the magic of your own interpretation: all you’re doing is sampling
    data from a statistical model of which words come after which words. Language
    models are all form and no substance.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Natural language is many things: a communication channel; a way to act on the
    world; a social lubricant; a way to formulate, store, and retrieve your own thoughts.
    These uses of languages are where its meaning originates. A deep learning “language
    model,” despite its name, captures effectively none of these fundamental aspects
    of language. It cannot communicate (it has nothing to communicate about and no
    one to communicate with), it cannot act on the world (it has no agency and no
    intent), it cannot be social, and it doesn’t have any thoughts to process with
    the help of words. Language is the operating system of the mind, and so, for language
    to be meaningful, it needs a mind to leverage it.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'What a language model does is capture the statistical structure of the observable
    artifacts—books, online movie reviews, tweets—that we generate as we use language
    to live our lives. The fact that these artifacts have a statistical structure
    at all is a side effect of how humans implement language. Here’s a thought experiment:
    what if our languages did a better job of compressing communications, much like
    computers do with most digital communications? Language would be no less meaningful
    and could still fulfill its many purposes, but it would lack any intrinsic statistical
    structure, thus making it impossible to model as you just did.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.6 Wrapping up
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can generate discrete sequence data by training a model to predict the next
    token(s), given previous tokens.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of text, such a model is called a *language model*. It can be based
    on either words or characters.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling the next token requires a balance between adhering to what the model
    judges likely, and introducing randomness.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to handle this is the notion of softmax temperature. Always experiment
    with different temperatures to find the right one.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.2 DeepDream
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*DeepDream* is an artistic image-modification technique that uses the representations
    learned by convolutional neural networks. It was first released by Google in the
    summer of 2015 as an implementation written using the Caffe deep learning library
    (this was several months before the first public release of TensorFlow).^([3](#Rendnote3))
    It quickly became an internet sensation thanks to the trippy pictures it could
    generate (see, for example, [figure 12.4](#fig12-4)), full of algorithmic pareidolia
    artifacts, bird feathers, and dog eyes—a byproduct of the fact that the DeepDream
    convnet was trained on ImageNet, where dog breeds and bird species are vastly
    overrepresented.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0414-01.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.4 Example of a DeepDream output image**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'The DeepDream algorithm is almost identical to the convnet filter-visualization
    technique introduced in chapter 9, consisting of running a convnet in reverse:
    doing gradient ascent on the input to the convnet to maximize the activation of
    a specific filter in an upper layer of the convnet. DeepDream uses this same idea,
    with a few simple differences:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: With DeepDream, you try to maximize the activation of entire layers rather than
    that of a specific filter, thus mixing together visualizations of large numbers
    of features at once.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You start not from blank, slightly noisy input, but rather from an existing
    image—thus the resulting effects latch on to preexisting visual patterns, distorting
    elements of the image in a somewhat artistic fashion.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input images are processed at different scales (called *octaves*), which
    improves the quality of the visualizations.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s make some DeepDreams.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Implementing DeepDream in Keras
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start by retrieving a test image to dream with. We’ll use a view of the
    rugged Northern California coast in the winter ([figure 12.5](#fig12-5)).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.9 Fetching the test image
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: base_image_path <— get_file(
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '"coast.jpg", origin = "https://img-datasets.s3.amazonaws.com/coast.jpg")'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: plot(as.raster(jpeg::readJPEG(base_image_path)))
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0415-01.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.15 Our test image**'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need a pretrained convnet. In Keras, many such convnets are available—
    VGG16, VGG19, Xception, ResNet50, and so on—all with weights pretrained on ImageNet.
    You can implement DeepDream with any of them, but your base model of choice will
    naturally affect your visualizations, because different architectures result in
    different learned features. The convnet used in the original DeepDream release
    was an Inception model, and in practice, Inception is known to produce nice-looking
    DeepDreams, so we’ll use the Inception V3 model that comes with Keras.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.10 Instantiating a pretrained InceptionV3 model**'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: model <— application_inception_v3(weights = "imagenet", include_top = FALSE)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use our pretrained convnet to create a feature extractor model that returns
    the activations of the various intermediate layers, listed in the following code.
    For each layer, we pick a scalar score that weights the contribution of the layer
    to the loss we will seek to maximize during the gradient-ascent process. If you
    want a complete list of layer names that you can use to pick new layers to play
    with, just use print(model).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.11 Configuring the contribution of each layer to the DeepDream loss
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: layer_settings <— c( ➊
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: mixed4 = 1,
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: mixed5 = 1.5,
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: mixed6 = 2,
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: mixed7 = 2.5
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: outputs <— list()
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: for(layer_name in names(layer_settings))
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: outputs[[layer_name]] <—➋
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: get_layer(model, layer_name)$output➋
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: feature_extractor <— keras_model(inputs = model$inputs,➌
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: outputs = outputs)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Layers for which we try to maximize activation, as well as their weight
    in the total loss. You can tweak these setting to obtain new visual effects.**
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Collect in a named list the output symbolic tensor from each layer.**
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **A model that returns the activation values for every target layer (as a
    named list)**
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll compute the *loss*: the quantity we’ll seek to maximize during
    the gradient-ascent process at each processing scale. In chapter 9, for filter
    visualization, we tried to maximize the value of a specific filter in a specific
    layer. Here, we’ll simultaneously maximize the activation of all filters in a
    number of layers. Specifically, we’ll maximize a weighted mean of the L2 norm
    of the activations of a set of high-level layers. The exact set of layers we choose
    (as well as their contribution to the final loss) has a major influence on the
    visuals we’ll be able to produce, so we want to make these parameters easily configurable.
    Lower layers result in geometric patterns, whereas higher layers result in visuals
    in which you can recognize some classes from ImageNet (e.g., birds or dogs). We’ll
    start from a somewhat arbitrary configuration involving four layers, but you’ll
    definitely want to explore many different configurations later.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.12 The DeepDream loss
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: compute_loss <— function(input_image) {
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: features <— feature_extractor(input_image)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: feature_losses <— names(features) %>%
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: lapply(function(name) {
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: coeff <— layer_settings[[name]]
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: activation <— features[[name]]➊
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: coeff * mean(activation[, 3:-3, 3:-3, ] ^ 2)➋
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Reduce(`+`, feature_losses)➌
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Extract activations.**
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We avoid border artifacts by involving only nonborder pixels in the loss.**
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **feature_losses is a list of scalar tensors. Sum up the loss from each feature.**
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s set up the gradient-ascent process that we will run at each octave.
    You’ll recognize that it’s the same thing as the filter-visualization technique
    from chapter 9! The DeepDream algorithm is simply a multiscale form of filter
    visualization.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.13 The DeepDream gradient-ascent process**'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: gradient_ascent_step <— tf_function(➊
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: function(image, learning_rate) {
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: tape$watch(image)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: loss <— compute_loss(image)➋
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(loss, image) %>%
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: tf$math$l2_normalize()➌
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: image %<>% `+`(learning_rate * grads)➍
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: list(loss, image)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: gradient_ascent_loop <—➎
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: function(image, iterations, learning_rate, max_loss = -Inf) {
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate %<>% as_tensor()
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: for(i in seq(iterations)) {➏
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: c(loss, image) %<—% gradient_ascent_step(image, learning_rate)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% as.numeric()
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: if(loss > max_loss)➐
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: break
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: writeLines(sprintf(
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '"… Loss value at step %i: %.2f", i, loss))'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: image
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We make the training step fast by compiling it as a tf_function().**
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Compute gradients of DeepDream loss with respect to the current image.**
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Normalize gradients (the same trick we used in chapter 9).**
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Repeatedly update the image in a way that increases the DeepDream loss.**
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **This runs gradient ascent for a given image scale (octave).**
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **This is a regular eager R for loop.**
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Break out if the loss crosses a certain threshold (overoptimizing would
    create unwanted image artifacts).**
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the outer loop of the DeepDream algorithm. First, we’ll define a list
    of *scales* (also called *octaves*) at which to process the images. We’ll process
    our image over three different such octaves. For each successive octave, from
    the smallest to the largest, we’ll run 20 gradient ascent steps via gradient_ascent_loop()
    to maximize the loss we previously defined. Between each octave, we’ll upscale
    the image by 40% (1.4×): we’ll start by processing a small image and then increasingly
    scale it up (see [figure 12.6](#fig12-6)).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0418-01.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.6 The DeepDream process: Successive scales of spatial processing
    (octaves) and detail reinjection upon upscaling**'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: We define the parameters of this process in the following code. Tweaking these
    parameters will allow you to achieve new effects!
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: step <— 20➊
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: num_octaves <— 3➋
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: octave_scale <— 1.4➌
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: iterations <— 30➍
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: max_loss <— 15➎
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Gradient ascent step size**
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Number of scales at which to run gradient ascent**
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Size ratio between successive scales**
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Number of gradient ascent steps per scale**
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **We’ll stop the gradient-ascent process for a scale if the loss gets higher
    than this.**
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: We’re also going to need a couple of utility functions to load and save images.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.14 Image processing utilities**'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: preprocess_image <— tf_function(function(image_path) {➊
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: image_path %>%
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$read_file() %>%
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$decode_image() %>%
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: tf$expand_dims(axis = 0L) %>%➋
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: tf$cast("float32") %>%➌
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: inception_v3_preprocess_input()
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: deprocess_image <— tf_function(function(img) {➍
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: img %>%
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: tf$squeeze(axis = 0L) %>%➎
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '{ (. * 127.5) + 127.5 } %>%➏'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: tf$saturate_cast("uint8")➐
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor <— function(x, …, max = 255,
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: plot_margins = c(0, 0, 0, 0)) {
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: if (!is.null(plot_margins))
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: withr::local_par(mar = plot_margins)➑
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: x %>%
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: as.array() %>%➒
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: drop() %>%
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: as.raster(max = max) %>%➓
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: plot(…, interpolate = FALSE)
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Util function to load, resize, and format pictures into appropriate arrays**
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Add batch axis, equivalent to .[tf$newaxis, all_dims()]. axis arg is 0-based.**
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Cast from 'uint8'.**
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Util function to convert a tensor array into a valid image and undo preprocessing**
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Drop first dim-the batch axis (must be size 1), the inverse of tf$expand_dims().**
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Rescale so values in [-1, 1] are remapped to [0, 255].**
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '➐ **saturate_case() clips values to the dtype range: [0, 255].**'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Default to no margins when plotting images.**
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Convert tensors to R arrays.**
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Convert to R native raster format.**
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '**withr::local_***'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Here we use withr::local_par() to set par() before calling plot(). local_ par()
    acts just like par(), except that it restores the previous par() settings when
    the function exits. Using funcions like local_par() or local_options() helps ensure
    that functions you write don’t permanently modify global state, which makes them
    more predictable and usable in more contexts.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'You can replace local_par() and do the equivalent with a separate on.exit()
    call like this:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor <— function()
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: <…>
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: opar <— par(mar = plot_margins)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: on.exit(par(opar))
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: <…>
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the outer loop. To avoid losing a lot of image detail after each successive
    scale-up (resulting in increasingly blurry or pixelated images), we can use a
    simple trick: after each scale-up, we’ll reinject the lost details back into the
    image, which is possible because we know what the original image should look like
    at the larger scale. Given a small image size *S* and a larger image size *L*,
    we can compute the difference between the original image resized to size *L* and
    the original resized to size *S*—this difference quantifies the details lost when
    going from *S* to *L*.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.15 Running gradient ascent over multiple successive octaves**'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: original_img <— preprocess_image(base_image_path)➊
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: original_HxW <— dim(original_img)[2:3]
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: calc_octave_HxW <— function(octave) {
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: as.integer(round(original_HxW / (octave_scale ^ octave)))
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: octaves <— seq(num_octaves - 1, 0) %>%➋
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '{ zip_lists(num = .,'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: HxW = lapply(., calc_octave_HxW)) }
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: str(octaves)
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: List of 3
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: $ :List of 2
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '..$ num: int 2'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '..$ HxW: int [1:2] 459 612'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: $ :List of 2
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '..$ num: int 1'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '..$ HxW: int [1:2] 643 857'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: $ :List of 2
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '..$ num: int 0'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '..$ HxW: int [1:2] 900 1200'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Load and preprocess the test image.**
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Compute the target shape of the image at different octaves.**
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: shrunk_original_img <— original_img %>% tf$image$resize(octaves[[1]]$HxW)
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: img <— original_img ➊
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: for (octave in octaves) {➋
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: cat(sprintf("Processing octave %i with shape (%s)\n",
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: octave$num, paste(octave$HxW, collapse = ", ")))
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: img <— img %>%
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: tf$image$resize(octave$HxW) %>%➌
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: gradient_ascent_loop(iterations = iterations, ➍
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate = step,
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: max_loss = max_loss)
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: upscaled_shrunk_original_img <— ➎
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: shrunk_original_img %>% tf$image$resize(octave$HxW)
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: same_size_original <—
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: original_img %>% tf$image$resize(octave$HxW)➏
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: lost_detail <—➐
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: same_size_original - upscaled_shrunk_original_img
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: img %<>% `+`(lost_detail)➑
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: shrunk_original_img <—
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: original_img %>% tf$image$resize(octave$HxW)
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: img <— deprocess_image(img)
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: img %>% display_image_tensor()
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: img %>%
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$encode_png() %>%
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$write_file("dream.png", .)➒
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Save a reference to the original image (we need to keep the original around).**
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Iterate over the different octaves.**
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Scale up the dream image.**
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Run gradient ascent, altering the dream.**
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '➎ **Scale up the smaller version of the original image: it will be pixelated.**'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Compute the high-quality version of the original image at this size.**
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **The difference between the two is the detail that was lost when detail up.**
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Reinject the lost detail into the dream.**
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Save the final result.**
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Because the original Inception V3 network was trained to recognize concepts
    in images of size 299 × 299, and given that the process involves scaling the images
    down by a reasonable factor, the DeepDream implementation produces much better
    results on images that are somewhere between 300 × 300 and 400 × 400\. Regardless,
    you can run the same code on images of any size and any ratio.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: On a GPU, it takes only a few seconds to run the whole thing. [Figure 12.7](#fig12-7)
    shows the result of our dream configuration on the test image.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0421-01.jpg)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.7 Running the DeepDream code on the test image**'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: I strongly suggest that you explore what you can do by adjusting which layers
    you use in your loss. Layers that are lower in the network contain more-local,
    less-abstract representations and lead to dream patterns that look more geometric.
    Layers that are higher up lead to more-recognizable visual patterns based on the
    most common objects found in ImageNet, such as dog eyes, bird feathers, and so
    on. You can use random generation of the parameters in the layer_settings vector
    to quickly explore many different layer combinations. [Figure 12.8](#fig12-8)
    shows a range of results obtained on an image of a delicious homemade pastry using
    different layer configurations.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Wrapping up
  id: totrans-494
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DeepDream consists of running a convnet in reverse to generate inputs based
    on the representations learned by the network.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results produced are fun and somewhat similar to the visual artifacts induced
    in humans by the disruption of the visual cortex via psychedelics.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the process isn’t specific to image models or even to convnets. It
    can be done for speech, music, and more.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0422-01.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.8 Trying a range of DeepDream configurations on an example image**'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Neural style transfer
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to DeepDream, another major development in deep-learning-driven
    image modification is *neural style transfer*, introduced by Leon Gatys et al.
    in the summer of 2015.^([4](#Rendnote4)) The neural style transfer algorithm has
    undergone many refinements and spawned many variations since its original introduction,
    and it has made its way into many smartphone photo apps. For simplicity, this
    section focuses on the formulation described in the original paper.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Neural style transfer consists of applying the style of a reference image to
    a target image while conserving the content of the target image. [Figure 12.9](#fig12-9)
    shows an example.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0422-02.jpg)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.9 A style transfer example**'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: In this context, *style* essentially means textures, colors, and visual patterns
    in the image, at various spatial scales, and the content is the higher-level macrostructure
    of the image. For instance, blue-and-yellow circular brushstrokes are considered
    to be the style in [figure 12.9](#fig12-9) (using *Starry Night* by Vincent van
    Gogh), and the buildings in the Tübingen photograph are considered to be the content.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: The idea of style transfer, which is tightly related to that of texture generation,
    has had a long history in the image-processing community prior to the development
    of neural style transfer in 2015\. But as it turns out, the deep-learning-based
    implementations of style transfer offer results unparalleled by what had been
    previously achieved with classical computer vision techniques, and they triggered
    an amazing renaissance in creative applications of computer vision.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: 'The key notion behind implementing style transfer is the same idea that’s central
    to all deep learning algorithms: you define a loss function to specify what you
    want to achieve, and you minimize this loss. We know what we want to achieve:
    conserving the content of the original image while adopting the style of the reference
    image. If we were able to mathematically define *content* and *style*, then an
    appropriate loss function to minimize would be the following:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: loss <— distance(style(reference_image) - style(combination_image)) +
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: distance(content(original_image) - content(combination_image))
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: Here, distance() is a norm function such as the L2 norm, content() is a function
    that takes an image and computes a representation of its content, and style()
    is a function that takes an image and computes a representation of its style.
    Minimizing this loss causes style(combination_image) to be close to style(reference_image),
    and content(combination_image) is close to content(original_image), thus achieving
    style transfer as we defined it.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental observation made by Gatys et al. was that deep convolutional neural
    networks offer a way to mathematically define the style and content functions.
    Let’s see how.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 The content loss
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you already know, activations from earlier layers in a network contain *local*
    information about the image, whereas activations from higher layers contain increasingly
    global, abstract information. Formulated in a different way, the activations of
    the different layers of a convnet provide a decomposition of the contents of an
    image over different spatial scales. Therefore, you’d expect the content of an
    image, which is more global and abstract, to be captured by the representations
    of the upper layers in a convnet.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: A good candidate for content loss is thus the L2 norm between the activations
    of an upper layer in a pretrained convnet, computed over the target image, and
    the activations of the same layer computed over the generated image. This guarantees
    that, as seen from the upper layer, the generated image will look similar to the
    original target image. Assuming that what the upper layers of a convnet see is
    really the content of their input images, this works as a way to preserve image
    content.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 The style loss
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The content loss uses only a single upper layer, but the style loss as defined
    by Gatys et al. uses multiple layers of a convnet: you try to capture the appearance
    of the style-reference image at all spatial scales extracted by the convnet, not
    just a single scale. For the style loss, Gatys et al. use the *Gram matrix* of
    a layer’s activations: the inner product of the feature maps of a given layer.
    This inner product can be understood as representing a map of the correlations
    between the layer’s features. These feature correlations capture the statistics
    of the patterns of a particular spatial scale, which empirically correspond to
    the appearance of the textures found at this scale.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the style loss aims to preserve similar internal correlations within
    the activations of different layers, across the style-reference image and the
    generated image. In turn, this guarantees that the textures found at different
    spatial scales look similar across the style-reference image and the generated
    image.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, you can use a pretrained convnet to define a loss that will do the
    following:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: Preserve content by maintaining similar high-level layer activations between
    the original image and the generated image. The convnet should “see” both the
    original image and the generated image as containing the same things.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Preserve style by maintaining similar *correlations* within activations for
    both low-level layers and high-level layers. Feature correlations capture *textures*:
    the generated image and the style-reference image should share the same textures
    at different spatial scales.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at a Keras implementation of the original 2015 neural style transfer
    algorithm. As you’ll see, it shares many similarities with the DeepDream implementation
    we developed in the previous section.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.3 Neural style transfer in Keras
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural style transfer can be implemented using any pretrained convnet. Here,
    we’ll use the VGG19 network used by Gatys et al. VGG19 is a simple variant of
    the VGG16 network introduced in chapter 5, with three more convolutional layers.
    Here’s the general process:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: Set up a network that computes VGG19 layer activations for the style-reference
    image, the base image, and the generated image at the same time.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the layer activations computed over these three images to define the loss
    function described earlier, which we’ll minimize to achieve style transfer.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a gradient-descent process to minimize this loss function.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by defining the paths to the style-reference image and the base
    image. To make sure that the processed images are a similar size (widely different
    sizes make style transfer more difficult), we’ll later resize them all to a shared
    height of 400 pixels.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.16 Getting the style and content images
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: base_image_path <— get_file(➊
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '"sf.jpg", origin = "https://img-datasets.s3.amazonaws.com/sf.jpg")'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image_path <— get_file(➋
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '"starry_night.jpg",'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: origin = "https://img-datasets.s3.amazonaws.com/starry_night.jpg")
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: c(original_height, original_width) %<—% {
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: base_image_path %>%
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$read_file() %>%
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$decode_image() %>%
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: dim() %>% .[1:2]
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: img_height <— 400➌
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: img_width <— round(img_height * (original_width / original_height))➌
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Path to the image we want to transform**
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Path to the style image**
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Dimensions of the generated picture**
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Our content image is shown in [figure 12.10](#fig12-10), and [figure 12.11](#fig12-11)
    shows our style image.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0425-01.jpg)'
  id: totrans-546
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.10 Content image: San Francisco from Nob Hill**'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0426-01.jpg)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.11 Style image: *Starry Night* by van Gogh**'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: We also need some auxiliary functions for loading, preprocessing, and postprocessing
    the images that go in and out of the VGG19 convnet.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.17 Auxiliary functions**'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: preprocess_image <— function(image_path) {➊
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: image_path %>%
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$read_file() %>%
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$decode_image() %>%
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: tf$image$resize(as.integer(c(img_height, img_width))) %>%
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: k_expand_dims(axis = 1) %>%➋
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: imagenet_preprocess_input()
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: deprocess_image <— tf_function(function(img) {➌
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: if (length(dim(img)) == 4)
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: img <— k_squeeze(img, axis = 1)➍
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: c(b, g, r) %<—% {
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: img %>%
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: k_reshape(c(img_height, img_width, 3)) %>%
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: k_unstack(axis = 3)➎
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: r %<>% `+`(123.68)➏
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: g %<>% `+`(103.939)➏
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: b %<>% `+`(116.779)➏
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: k_stack(c(r, g, b), axis = 3) %>%➐
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: k_clip(0, 255) %>%
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: k_cast("uint8")
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Util function to open, resize, and format pictures into appropriate arrays**
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Add a batch dimension.**
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Util function to convert a tensor into a valid image**
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Also accept an image with a batch-dim of size 1. (This will throw an error
    if the first axis is not size 1.)**
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Unstack along the third axis, and return a list of length 3.**
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Zero-center by removing the mean pixel value from ImageNet. This reverses
    a transformation done by imagenet_preprocess_input().**
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Note that we're reversing the order of the channels, BGR to RGB. This is
    also part of the reversal of imagenet_preprocess_input().**
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: Keras backend functions (k_*)
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: In this version of preprocess_image() and deprocess_image(), we used Keras backend
    functions, like k_expand_dims() but in earlier versions, we used functions from
    the tf module, like tf$expand_dims(). What’s the difference?
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: The Keras package contains an extensive suite of backend functions, all starting
    with the k_ prefix. They are a vestige from a time when the Keras library was
    designed to work with multiple backends. Today it’s more common to call directly
    to functions in tf module, where the functions typically expose more features
    and capabilities. One nicety of the keras::k_ backend functions, however, is that
    they all are 1-based and will often do automatic coercion of function arguments
    to integer as necessary. For example, k_expand_dims(axis = 1) is equivalent to
    tf$expand_dims(axis = 0L).
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: The backend functions are no longer actively developed, but they are covered
    by the TensorFlow stability promise, are maintained, and will not be going away
    anytime soon. You can safely use functions like k_expand_dims(), k_squeeze(),
    and k_stack() to do common tensor operations, especially when it’s easier to reason
    with consistent 1-based counting conventions. However, when you find the capabilities
    of the backend functions limiting, don’t hesitate to switch over to using the
    tf module functions directly. You can find additional documentation about backend
    functions at [https://keras.rstudio.com/articles/backend.html](https://www.keras.rstudio.com/articles/backend.html).
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up the VGG19 network. Like in the DeepDream example, we’ll use the
    pre-trained convnet to create a feature exactor model that returns the activations
    of intermediate layers—all layers in the model this time.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.18 Using a pretrained VGG19 model to create a feature extractor
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: model <— application_vgg19(weights = "imagenet",
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: include_top = FALSE)➊
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: outputs <— list()
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: for (layer in model$layers)
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: outputs[[layer$name]] <— layer$output
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: feature_extractor <— keras_model(inputs = model$inputs,➋
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: outputs = outputs)
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Build a VGG19 model loaded with pretrained ImageNet weights.**
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **A model that returns the activation values for every target layer (as a
    named list)**
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define the content loss, which will make sure the top layer of the VGG19
    convnet has a similar view of the style image and the combination image.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.19 Content loss
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: content_loss <— function(base_img, combination_img)
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: sum((combination_img - base_img) ^ 2)
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the style loss. It uses an auxiliary function to compute the Gram matrix
    of an input matrix: a map of the correlations found in the original feature matrix.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.20 Style loss
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: gram_matrix <— function(x) {➊
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: n_features <— tf$shape(x)[3]
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: x %>%
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: tf$reshape(c(-1L, n_features)) %>%➋
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: tf$matmul(t(.), .)➌
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: style_loss <— function(style_img, combination_img) {
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: S <— gram_matrix(style_img)
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: C <— gram_matrix(combination_img)
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: channels <— 3
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: size <— img_height * img_width
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: sum((S - C) ^ 2) /
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: (4 * (channels ^ 2) * (size ^ 2))
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **x has the shape (height, width, features).**
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Flatten the first two spatial axes, and preserve the features axis.**
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The output will have the shape (n_features, n_features).**
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: 'To these two loss components, you add a third: the *total variation loss*,
    which operates on the pixels of the generated combination image. It encourages
    spatial continuity in the generated image, thus avoiding overly pixelated results.
    You can interpret it as a regularization loss.'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.21 Total variation loss**'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_loss <— function(x) {
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: a <— k_square(x[, NA:(img_height-1), NA:(img_width-1), ]—
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: x[, 2:NA             , NA:(img_width-1), ])
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: b <— k_square(x[, NA:(img_height-1), NA:(img_width-1), ]—
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: x[, NA:(img_height-1), 2:NA            , ])
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: sum((a + b) ^ 1.25)
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: The loss that you minimize is a weighted average of these three losses. To compute
    the content loss, you use only one upper layer—the block5_conv2 layer—whereas
    for the style loss, you use a list of layers that spans both low-level and high-level
    layers. You add the total variation loss at the end.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the style-reference image and content image you’re using, you’ll
    likely want to tune the content_weight coefficient (the contribution of the content
    loss to the total loss). A higher content_weight means the target content will
    be more recognizable in the generated image.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.22 Defining the final loss that you’ll minimize
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: style_layer_names <— c(➊
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: '"block1_conv1",'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: '"block2_conv1",'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: '"block3_conv1",'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: '"block4_conv1",'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: '"block5_conv1"'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: content_layer_name <— "block5_conv2"➋
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_weight <— 1e—6➌
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: content_weight <— 2.5e—8➍
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: style_weight <— 1e—6➎
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: compute_loss <
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: function(combination_image, base_image, style_reference_image) {
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: input_tensor <
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: list(base_image,
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image,
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: combination_image) %>%
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: k_concatenate(axis = 1)
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: features <— feature_extractor(input_tensor)
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: layer_features <— features[[content_layer_name]]
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: base_image_features <— layer_features[1, , , ]
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: combination_features <— layer_features[3, , , ]
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: loss <— 0➏
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% `+`(➐
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: content_loss(base_image_features, combination_features) *
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: content_weight
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: for (layer_name in style_layer_names) {
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: layer_features <— features[[layer_name]]
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_features <— layer_features[2, , , ]
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: combination_features <— layer_features[3, , , ]
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% `+`(➑
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: style_loss(style_reference_features, combination_features) *
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: style_weight / length(style_layer_names)
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: loss %<>% `+`(➒
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_loss(combination_image) *
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_weight
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: loss➓
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The list of layers to use for the style loss**
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The layer to use for the content loss**
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The contribution weight of the total variation loss**
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **The contribution weight of the content loss**
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The contribution weight of the style loss**
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Initialize the loss to 0.**
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add the content loss.**
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Add the style loss for each style layer.**
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Add the total variation loss.**
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Return the sum of content loss, style loss, and total variation loss.**
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s set up the gradient—descent process. In the original Gatys et
    al. paper, optimization is performed using the L—BFGS algorithm, but that’s not
    available in Tensor-Flow, so we’ll just do mini—batch gradient descent with the
    SGD optimizer instead. We’ll leverage an optimizer feature you haven’t seen before:
    a learning-rate schedule. We’ll use it to gradually decrease the learning rate
    from a very high value (100) to a much smaller final value (about 20). That way,
    we’ll make fast progress in the early stages of training and then proceed more
    cautiously as we get closer to the loss minimum.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.23 Setting up the gradient-descent process
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: compute_loss_and_grads <— tf_function(➊
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: function(combination_image, base_image, style_reference_image) {
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: loss <— compute_loss(combination_image,
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: base_image,
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image)
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(loss, combination_image)
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: list(loss, grads)
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
- en: optimizer <— optimizer_sgd(
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: learning_rate_schedule_exponential_decay(
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: initial_learning_rate = 100, decay_steps = 100,➋
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: decay_rate = 0.96))➋
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: base_image <— preprocess_image(base_image_path)
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: style_reference_image <— preprocess_image(style_reference_image_path)
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: combination_image <
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: tf$Variable(preprocess_image(base_image_path))➌
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: output_dir <— fs::path("style-transfer-generated-images")
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: iterations <— 4000
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(iterations)) {
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: c(loss, grads) %<—% compute_loss_and_grads(
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: combination_image, base_image, style_reference_image)
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: optimizer$apply_gradients(list(➍
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: tuple(grads, combination_image)))
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: if ((i %% 100) == 0) {
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: 'cat(sprintf("Iteration %i: loss = %.2f\n", i, loss))'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: img <— deprocess_image(combination_image)
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor(img)
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: fname <— sprintf("combination_image_at_iteration_%04i.png", i)
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$write_file(filename = output_dir / fname,➎
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: contents = tf$io$encode_png(img))
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We make the training step fast by compiling it as a tf_function().**
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We'll start with a learning rate of 100 and decrease it by 4% every 100
    steps.**
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Use a tf$Variable() to store the combination image because we'll be updating
    it during training.**
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Update the combination image in a direction that reduces the style transfer
    loss.**
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Save the combination image at regular intervals.**
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12.12](#fig12-12) shows what you get. Keep in mind that what this technique
    achieves is merely a form of image retexturing, or texture transfer. It works
    best with style—reference images that are strongly textured and highly self-similar,
    and with content targets that don’t require high levels of detail to be recognizable.
    It typically can’t achieve fairly abstract feats such as transferring the style
    of one portrait to another. The algorithm is closer to classical signal processing
    than to AI, so don’t expect it to work like magic!'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0431-01.jpg)'
  id: totrans-726
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.12 Style transfer result**'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, note that this style-transfer algorithm is slow to run. But the
    transformation operated by the setup is simple enough that it can be learned by
    a small, fast feed-forward convnet as well—as long as you have appropriate training
    data available. Fast style transfer can thus be achieved by first spending a lot
    of compute cycles to generate input-output training examples for a fixed style-reference
    image, using the method outlined here, and then training a simple convnet to learn
    this style—specific transformation. Once that’s done, stylizing a given image
    is instantaneous: it’s just a forward pass of this small convnet.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4 Wrapping up
  id: totrans-729
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Style transfer consists of creating a new image that preserves the contents
    of a target image while also capturing the style of a reference image.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content can be captured by the high-level activations of a convnet.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Style can be captured by the internal correlations of the activations of different
    layers of a convnet.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, deep learning allows style transfer to be formulated as an optimization
    process using a loss defined with a pretrained convnet.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting from this basic idea, many variants and refinements are possible.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.4 Generating images with variational autoencoders
  id: totrans-735
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most popular and successful application of creative AI today is image generation:
    learning latent visual spaces and sampling from them to create entirely new pictures
    interpolated from real ones—pictures of imaginary people, imaginary places, imaginary
    cats and dogs, and so on.'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section and the next, we’ll review some high-level concepts pertaining
    to image generation, alongside implementation details relative to the two main
    techniques in this domain: *variational autoencoders* (VAEs) and *generative adversarial
    networks* (GANs). Note that the techniques I’ll present here aren’t specific to
    images—you could develop latent spaces of sound, music, or even text using GANs
    and VAEs—but in practice, the most interesting results have been obtained with
    pictures, and that’s what we’ll focus on here.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 Sampling from latent spaces of images
  id: totrans-738
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key idea of image generation is to develop a low-dimensional *latent space*
    of representations (which, like everything else in deep learning, is a vector
    space), where any point can be mapped to a “valid” image: an image that looks
    like the real thing. The module capable of realizing this mapping, taking as input
    a latent point and outputting an image (a grid of pixels), is called a *generator*
    (in the case of GANs) or a *decoder* (in the case of VAEs). Once such a latent
    space has been learned, you can sample points from it, and, by mapping them back
    to image space, generate images that have never been seen before (see [figure
    12.13](#fig12-13)). These new images are the in-betweens of the training images.'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0432-01.jpg)'
  id: totrans-740
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.13 Learning a latent vector space of images and using it to sample
    new images**'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: GANs and VAEs are two different strategies for learning such latent spaces of
    image representations, each with its own characteristics. VAEs are great for learning
    latent spaces that are well structured, where specific directions encode a meaningful
    axis of variation in the data (see [figure 12.14](#fig12-14)). GANs generate images
    that can potentially be highly realistic, but the latent space they come from
    may not have as much structure and continuity.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0433-01.jpg)'
  id: totrans-743
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.14 A continuous space of faces generated by Tom White using VAEs**'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 Concept vectors for image editing
  id: totrans-745
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We already hinted at the idea of a *concept vector* when we covered word embeddings
    in chapter 11\. The idea is still the same: given a latent space of representations,
    or an embedding space, certain directions in the space may encode interesting
    axes of variation in the original data. In a latent space of images of faces,
    for instance, there may be a *smile vector*, such that if latent point z is the
    embedded representation of a certain face, then latent point z + s is the embedded
    representation of the same face, smiling. Once you’ve identified such a vector,
    it then becomes possible to edit images by projecting them into the latent space,
    moving their representation in a meaningful way, and then decoding them back to
    image space. Concept vectors exist for essentially any independent dimension of
    variation in image space—in the case of faces, you may discover vectors for adding
    sunglasses to a face, removing glasses, turning a male face into a female face,
    and so on. [Figure 12.15](#fig12-15) is an example of a smile vector, a concept
    vector discovered by Tom White, from the Victoria University School of Design
    in New Zealand, using VAEs trained on a dataset of faces of celebrities (the CelebA
    dataset).'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0434-01.jpg)'
  id: totrans-747
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.15 The smile vector**'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.3 Variational autoencoders
  id: totrans-749
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Variational autoencoders, simultaneously discovered by Kingma and Welling in
    December 2013^([5](#Rendnote5)) and Rezende, Mohamed, and Wierstra in January
    2014,^([6](#Rendnote6)) are a kind of generative model that’s especially appropriate
    for the task of image editing via concept vectors. They’re a modern take on autoencoders
    (a type of network that aims to encode an input to a low-dimensional latent space
    and then decode it back) that mixes ideas from deep learning with Bayesian inference.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: A classical image autoencoder takes an image, maps it to a latent vector space
    via an encoder module, and then decodes it back to an output with the same dimensions
    as the original image, via a decoder module (see [figure 12.16](#fig12-16)). It’s
    then trained by using as target data the *same images* as the input images, meaning
    the autoencoder learns to reconstruct the original inputs. By imposing various
    constraints on the code (the output of the encoder), you can get the autoencoder
    to learn more— or less-interesting latent representations of the data. Most commonly,
    you’ll constrain the code to be low—dimensional and sparse (mostly zeros), in
    which case the encoder acts as a way to compress the input data into fewer bits
    of information.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0434-02.jpg)'
  id: totrans-752
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.16 An autoencoder mapping an input *x* to a compressed representation
    and then decoding it back as *x*’**'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: In practice, such classical autoencoders don’t lead to particularly useful or
    nicely structured latent spaces. They’re not much good at compression, either.
    For these reasons, they have largely fallen out of fashion. VAEs, however, augment
    autoencoders with a little bit of statistical magic that forces them to learn
    continuous, highly structured latent spaces. They have turned out to be a powerful
    tool for image generation.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of compressing its input image into a fixed code in the latent space,
    a VAE turns the image into the parameters of a statistical distribution: a mean
    and a variance. Essentially, this means we’re assuming the input image has been
    generated by a statistical process, and that the randomness of this process should
    be taken into account during encoding and decoding. The VAE then uses the mean
    and variance parameters to randomly sample one element of the distribution and
    decodes that element back to the original input (see [figure 12.17](#fig12-17)).
    The stochasticity of this process improves robustness and forces the latent space
    to encode meaningful representations everywhere: every point sampled in the latent
    space is decoded to a valid output.'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0435-01.jpg)'
  id: totrans-756
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.17 A VAE maps an image to two vectors, z_mean and z_log_sigma,
    which define a probability distribution over the latent space, used to sample
    a latent point to decode.**'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: 'In technical terms, here’s how a VAE works:'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
- en: '**1** An encoder module turns the input sample, input_img, into two parameters
    in a latent space of representations, z_mean and z_log_variance.'
  id: totrans-759
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** You randomly sample a point z from the latent normal distribution that’s
    assumed to generate the input image, via z = z_mean + exp(z_log_variance) * epsilon,
    where epsilon is a random tensor of small values.'
  id: totrans-760
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** A decoder module maps this point in the latent space back to the original
    input image.'
  id: totrans-761
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because epsilon is random, the process ensures that every point that’s close
    to the latent location where you encoded input_img (z—mean) can be decoded to
    something similar to input_img, thus forcing the latent space to be continuously
    meaningful.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: Any two close points in the latent space will decode to highly similar images.
    Continuity, combined with the low dimensionality of the latent space, forces every
    direction in the latent space to encode a meaningful axis of variation of the
    data, making the latent space very structured and thus highly suitable to manipulation
    via concept vectors.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters of a VAE are trained via two loss functions: a *reconstruction
    loss* that forces the decoded samples to match the initial inputs, and a *regularization
    loss* that helps learn well—rounded latent distributions and reduces overfitting
    to the training data. Schematically, the process looks like this:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: c(z_mean, z_log_variance) %<—% encoder(input_img)➊
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: z <— z_mean + exp(z_log_variance) * epsilon➋
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
- en: reconstructed_img <— decoder(z) ➌
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: model <— keras_model(input_img, reconstructed_img)➍
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Encode the input into mean and variance parameters.**
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Draw a latent point using a small random epsilon.**
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Decode z back to an image.**
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Instantiate the autoencoder model, which maps an input image to its reconstruction.**
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: You can then train the model using the reconstruction loss and the regularization
    loss. For the regularization loss, we typically use an expression (the Kullback–Leibler
    divergence) meant to nudge the distribution of the encoder output toward a well—rounded
    normal distribution centered around 0\. This provides the encoder with a sensible
    assumption about the structure of the latent space it’s modeling.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see what implementing a VAE looks like in practice!
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.4 Implementing a VAE with Keras
  id: totrans-775
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re going to be implementing a VAE that can generate MNIST digits. It’s going
    to have three parts:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: An encoder network that turns a real image into a mean and a variance in the
    latent space
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sampling layer that takes such a mean and variance and uses them to sample
    a random point from the latent space
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder network that turns points from the latent space back into image
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following listing shows the encoder network we’ll use, mapping images to
    the parameters of a probability distribution over the latent space. It’s a simple
    convnet that maps the input image x to two vectors, z_mean and z_log_var. One
    important detail is that we use strides for downsampling feature maps instead
    of max pooling. The last time we did this was in the image segmentation example
    in chapter 9\. Recall that, in general, strides are preferable to max pooling
    for any model that cares about *information location*—that is to say, *where*
    stuff is in the image—and this one does, because it will have to produce an image
    encoding that can be used to reconstruct a valid image.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.24 VAE encoder network**'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <— 2➊
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: encoder_inputs <—  layer_input(shape = c(28, 28, 1))
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: x <— encoder_inputs %>%
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(32, 3, activation = "relu", strides = 2, padding = "same") %>%
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(64, 3, activation = "relu", strides = 2, padding = "same") %>%
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: layer_flatten() %>%
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(16, activation = "relu")
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: z_mean <— x %>% layer_dense(latent_dim, name = "z_mean")➋
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: z_log_var <— x %>% layer_dense(latent_dim, name = "z_log_var")➋
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: encoder <— keras_model(encoder_inputs, list(z_mean, z_log_var),
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: name = "encoder")
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Dimensionality of the latent space: a 2D plane**'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The input image ends up being encoded into these two parameters.**
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
- en: 'Its summary looks like this:'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0437-01.jpg)'
  id: totrans-796
  prefs: []
  type: TYPE_IMG
- en: Next is the code for using z_mean and z_log_var, the parameters of the statistical
    distribution assumed to have produced input_img, to generate a latent space point
    z.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.25 Latent—space—sampling layer**'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
- en: layer_sampler <— new_layer_class(
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: classname = "Sampler",
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: call = function(z_mean, z_log_var) {➊
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: epsilon <— tf$random$normal(shape = tf$shape(z_mean))➋
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
- en: z_mean + exp(0.5 * z_log_var) * epsilon➌
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **z_mean and z_log_var here both will have shape (batch_size, latent_dim),
    for example, (128, 2).**
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Draw a batch of random normal vectors.**
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Apply the VAE sampling formula.**
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the decoder implementation. We reshape the vector
    z to the dimensions of an image and then use a few convolution layers to obtain
    a final image output that has the same dimensions as the original input_img.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.26 VAE decoder network, mapping latent space points to images
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: latent_inputs <— layer_input(shape = c(latent_dim))➊
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs <— latent_inputs %>%
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(7 * 7 * 64, activation = "relu") %>% ➋
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: layer_reshape(c(7, 7, 64)) %>%➌
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(64, 3, activation = "relu",
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>% ➍
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(32, 3, activation = "relu",
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(1, 3, activation = "sigmoid",
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: padding = "same") ➎
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: decoder <— keras_model(latent_inputs, decoder_outputs,
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: name = "decoder")
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Input where we'll feed z**
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Produce the same number of coefficients that we had at the level of the
    Flatten layer in the encoder.**
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Revert the layer_flatten() of the encoder.**
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Revert the layer_conv_2d() of the encoder.**
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The output ends up with shape (28, 28, 1).**
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: 'Its summary looks like this:'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: decoder
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0438-01.jpg)'
  id: totrans-830
  prefs: []
  type: TYPE_IMG
- en: Now let’s create the VAE model itself. This is your first example of a model
    that isn’t doing supervised learning (an autoencoder is an example of *self-supervised*
    learning, because it uses its inputs as targets). Whenever you depart from classic
    supervised learning, it’s common to create a new_model_class() and implement a
    custom train_step() to specify the new training logic, a workflow you learned
    about in chapter 7\. That’s what we’ll do here.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.27 VAE model with custom train_step()**'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
- en: model_vae <— new_model_class(
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: classname = "VAE",
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(encoder, decoder, …) {
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize(…)
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: self$encoder <— encoder➊
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: self$decoder <— decoder
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: self$sampler <— layer_sampler()
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker <
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "total_loss")➋
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker <➋
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "reconstruction_loss")➋
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker <➋
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "kl_loss")➋
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: metrics = mark_active(function() {➌
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
- en: list(
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker,
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker,
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: '}),'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(data) {
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: c(z_mean, z_log_var) %<—% self$encoder(data)
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: z <— self$sampler(z_mean, z_log_var)
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction <— decoder(z)
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss <➍
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
- en: loss_binary_crossentropy(data, reconstruction) %>%
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
- en: sum(axis = c(2, 3)) %>% ➎
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
- en: mean()➏
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: kl_loss <— -0.5 * (1 + z_log_var - z_mean^2 - exp(z_log_var))
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
- en: total_loss <— reconstruction_loss + mean(kl_loss)➐
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(total_loss, self$trainable_weights)
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
- en: self$optimizer$apply_gradients(zip_lists(grads, self$trainable_weights))
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker$update_state(total_loss)
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker$update_state(reconstruction_loss)
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker$update_state(kl_loss)
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
- en: list(total_loss = self$total_loss_tracker$result(),
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss = self$reconstruction_loss_tracker$result(),
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
- en: kl_loss = self$kl_loss_tracker$result())
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We assign to self instead of private because we want the layer weights automatically
    tracked by the Keras Model base class.**
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We use these metrics to keep track of the loss averages over each epoch.**
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **We list the metrics in an active property to enable the framework to reset
    them after each epoch (or between multiple calls to fit()/evaluate()).**
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We sum the reconstruction loss over the spatial dimensions (second and third
    axes) and take its mean over the batch dimension.**
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Total loss for each case in the batch; preserve batch axis.**
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Take the mean of loss totals in the batch.**
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add the regularization term (Kullback–Leibler divergence).**
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’re ready to instantiate and train the model on MNIST digits. Because
    the loss is taken care of in the custom layer, we don’t specify an external loss
    at compile time (loss = NULL), which in turn means we won’t pass target data during
    training (as you can see, we pass only x_train to the model in fit()).
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.28 Training the VAE
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
- en: library(listarrays)➊
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
- en: c(c(x_train, .), c(x_test, .)) %<—% dataset_mnist()
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
- en: mnist_digits <
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
- en: bind_on_rows(x_train, x_test) %>%➋
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: expand_dims(-1) %>%
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
- en: '{ . / 255 }'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: str(mnist_digits)
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
- en: num [1:70000, 1:28, 1:28, 1] 0 0 0 0 0 0 0 0 0 0 …
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
- en: vae <— model_vae(encoder, decoder)
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
- en: vae %>% compile(optimizer = optimizer_adam())➌
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
- en: vae %>% fit(mnist_digits, epochs = 30, batch_size = 128)➍
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Provide bind_on_rows() and other functions for manipulating R arrays.**
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We train on all MNIST digits, so we combine the training and test samples
    along the batch dim.**
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Note that we don't pass a loss argument in compile(), because the loss is
    already part of the train_step().**
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Note that we don't pass targets in fit(), because train_step() doesn't expect
    any.**
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we can use the decoder network to turn arbitrary
    latent space vectors into images.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.29 Sampling a grid of images from the 2D latent space**'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: n <— 30
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
- en: digit_size <— 28
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
- en: z_grid <➊
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
- en: seq(-1, 1, length.out = n) %>%
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
- en: expand.grid(., .) %>%
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
- en: as.matrix()
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
- en: decoded <— predict(vae$decoder, z_grid)➋
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
- en: z_grid_i <— seq(n) %>% expand.grid(x = ., y = .)➌
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
- en: figure <— array(0, c(digit_size * n, digit_size * n))➍
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
- en: for (i in 1:nrow(z_grid_i)) {
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
- en: c(xi, yi) %<—% z_grid_i[i, ]
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
- en: digit <— decoded[i, , , ]
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
- en: figure[seq(to = (n + 1 - xi) * digit_size, length.out = digit_size),
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
- en: seq(to = yi * digit_size, length.out = digit_size)] <—
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: digit
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
- en: par(pty = "s")➎
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
- en: lim <— extendrange(r = c(-1, 1),
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
- en: f = 1 - (n / (n+.5)))➐
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
- en: plot(NULL, frame.plot = FALSE,
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
- en: ylim = lim, xlim = lim,
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
- en: xlab = ~z[1], ylab = ~z[2]) ➑
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
- en: rasterImage(as.raster(1 - figure, max = 1),➑
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
- en: lim[1], lim[1], lim[2], lim[2],
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
- en: interpolate = FALSE)
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Create a 2D grid of linearly spaced samples.**
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Get the decoded digits.**
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Transform the decoded digits with shape (900, 28, 28, 1) to an R array with
    shape (28*30, 28*30) for plotting.**
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We'll display a grid of 30 × 30 digits (900 digits total).**
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Square plot type**
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Expand lim so (–1, 1) are at the center of a digit.**
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Pass a formula object to xlab for a proper subscript.**
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Subtract from 1 to invert the colors.**
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
- en: 'The grid of sampled digits (see [figure 12.18](#fig12-18)) shows a completely
    continuous distribution of the different digit classes, with one digit morphing
    into another as you follow a path through latent space. Specific directions in
    this space have a meaning: for example, there are directions for “five-ness,”
    “one—ness,” and so on.'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0441-01.jpg)'
  id: totrans-936
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.18 Grid of digits decoded from the latent space**'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we’ll cover in detail the other major tool for generating
    artificial images: generative adversarial networks (GANs).'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.5 Wrapping up
  id: totrans-939
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image generation with deep learning is done by learning latent spaces that
    capture statistical information about a dataset of images. By sampling and decoding
    points from the latent space, you can generate never-before-seen images. There
    are two major tools to do this: VAEs and GANs.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VAEs result in highly structured, continuous latent representations. For this
    reason, they work well for doing all sorts of image editing in latent space: face
    swapping, turning a frowning face into a smiling face, and so on. They also work
    nicely for doing latent—space—based animations, such as animating a walk along
    a cross section of the latent space or showing a starting image slowly morphing
    into different images in a continuous way.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs enable the generation of realistic single-frame images but may not induce
    latent spaces with solid structure and high continuity.
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most successful practical applications I have seen with images rely on VAEs,
    but GANs have enjoyed enduring popularity in the world of academic research. You’ll
    find out how they work and how to implement one in the next section.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Introduction to generative adversarial networks
  id: totrans-944
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative adversarial networks (GANs), introduced in 2014 by Goodfellow et
    al.,^([7](#Rendnote7)) are an alternative to VAEs for learning latent spaces of
    images. They enable the generation of fairly realistic synthetic images by forcing
    the generated images to be statistically almost indistinguishable from real ones.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive way to understand GANs is to imagine a forger trying to create
    a fake Picasso painting. At first, the forger is pretty bad at the task. He mixes
    some of his fakes with authentic Picassos and shows them all to an art dealer.
    The art dealer makes an authenticity assessment for each painting and gives the
    forger feedback about what makes a Picasso look like a Picasso. The forger goes
    back to his studio to prepare some new fakes. As time goes on, the forger becomes
    increasingly competent at imitating the style of Picasso, and the art dealer becomes
    increasingly expert at spotting fakes. In the end, they have on their hands some
    excellent fake Picassos.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s what a GAN is: a forger network and an expert network, each being trained
    to best the other. As such, a GAN is made of two parts:'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
- en: '*Generator network*—Takes as input a random vector (a random point in the latent
    space), and decodes it into a synthetic image'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Discriminator network (or adversary)*—Takes as input an image (real or synthetic),
    and predicts whether the image came from the training set or was created by the
    generator network'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The generator network is trained to be able to fool the discriminator network,
    and thus it evolves toward generating increasingly realistic images as training
    goes on: artificial images that look indistinguishable from real ones, to the
    extent that it’s impossible for the discriminator network to tell the two apart
    (see [figure 12.19](#fig12-19)). Meanwhile, the discriminator is constantly adapting
    to the gradually improving capabilities of the generator, setting a high bar of
    realism for the generated images. Once training is over, the generator is capable
    of turning any point in its input space into a believable image. Unlike VAEs,
    this latent space has fewer explicit guarantees of meaningful structure; in particular,
    it isn’t continuous.'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0443-01.jpg)'
  id: totrans-951
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.19 A generator transforms random latent vectors into images, and
    a discriminator seeks to tell real images from generated ones. The generator is
    trained to fool the discriminator.**'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
- en: Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike
    in any other training setup you’ve encountered in this book. Normally, gradient
    descent consists of rolling down hills in a static loss landscape. But with a
    GAN, every step taken down the hill changes the entire landscape a little. It’s
    a dynamic system where the optimization process is seeking not a minimum but an
    equilibrium between two forces. For this reason, GANs are notoriously difficult
    to train—getting a GAN to work requires lots of careful tuning of the model architecture
    and training parameters.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.1 A schematic GAN implementation
  id: totrans-954
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we’ll explain how to implement a GAN in Keras in its barest
    form. GANs are advanced, so diving deeply into the technical details of architectures
    like that of the StyleGAN2 that generated the images in [figure 12.20](#fig12-20)
    would be out of scope for this book. The specific implementation we’ll use in
    this demonstration is a *deep convolutional GAN* (DCGAN): a very basic GAN where
    the generator and discriminator are deep convnets.'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll train our GAN on images from the large-scale CelebFaces Attributes dataset
    (known as CelebA), a dataset of 200,000 faces of celebrities ([http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)).
    To speed up training, we’ll resize the images to 64 × 64, so we’ll be learning
    to generate 64 × 64 images of human faces. Schematically, the GAN looks like this:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0444-01.jpg)'
  id: totrans-957
  prefs: []
  type: TYPE_IMG
- en: '**[Figure 12.20](#fig12-20) Latent space dwellers. Images generated by [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)
    using a StyleGAN2 model. (Image credit: Phillip Wang is the website author. The
    model used is the StyleGAN2 model from Karras et al., [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958).)**'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
- en: A generator network maps vectors of shape (latent_dim) to images of shap (64,
    64, 3).
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discriminator network maps images of shape (64, 64, 3) to a binary score estimating
    the probability that the image is real.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A gan network chains the generator and the discriminator together: gan(x) =
    discriminator(generator(x)). Thus, this gan network maps latent space vectors
    to the discriminator’s assessment of the realism of these latent vectors as decoded
    by the generator.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train the discriminator using examples of real and fake images along wit
    “real”/”fake” labels, just as we train any regular image-classification model.
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To train the generator, we use the gradients of the generator’s weights with
    regard to the loss of the gan model. This means that at every step, we move the
    weights of the generator in a direction that makes the discriminator more likely
    to classify as “real” the images decoded by the generator. In other words, we
    train the generator to fool the discriminator.
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.5.2 A bag of tricks
  id: totrans-964
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of training GANs and tuning GAN implementations is notoriously
    difficult. You should keep in mind a number of known tricks. Like most things
    in deep learning, it’s more alchemy than science: these tricks are heuristics,
    not theory—backed guidelines.'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
- en: They’re supported by a level of intuitive understanding of the phenomenon at
    hand, and they’re known to work well empirically, although not necessarily in
    every context.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few of the tricks used in the implementation of the GAN generator
    and discriminator in this section. It isn’t an exhaustive list of GAN—related
    tips; you’ll find many more across the GAN literature:'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
- en: We use strides instead of pooling for downsampling feature maps in the discriminator,
    just like we did in our VAE encoder.
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We sample points from the latent space using a *normal distribution* (Gaussian
    distribution), not a uniform distribution.
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochasticity is good for inducing robustness. Because GAN training results
    in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing
    randomness during training helps prevent this. We introduce randomness by adding
    random noise to the labels for the discriminator.
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse gradients can hinder GAN training. In deep learning, sparsity is often
    a desirable property, but not in GANs. Two things can induce gradient sparsity:
    maxp—oling operations and relu activations. Instead of max pooling, we recommend
    using strided convolutions for downsampling, and we recommend using a layer_activation_leaky_relu()
    instead of a relu activation. It’s similar to relu, but it relaxes sparsity constraints
    by allowing small negative activation values.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In generated images, it’s common to see checkerboard artifacts caused by unequal
    coverage of the pixel space in the generator (see [figure 12.21](#fig12-21)).
    To fix this, we use a kernel size that’s divisible by the stride size whenever
    we use a strided layer_conv_2d_transpose() or layer_conv_2d() in both the generator
    and the discriminator.
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0445-00.jpg)'
  id: totrans-973
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.21 Checkerboard artifacts caused by mismatching strides and kernel
    sizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs**'
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.3 Getting our hands on the CelebA dataset
  id: totrans-975
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can download the dataset manually from the website: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).
    Because the dataset is hosted on Google Drive, you can also download it using
    gdown:'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
- en: reticulate::py_install("gdown", pip = TRUE)➊
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: system("gdown 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684")➋
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
- en: Downloading…
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: 'From: https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: 'To: img_align_celeba.zip'
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
- en: 32%|                      | 467M/1.44G [00:13<00:23, 41.3MB/s]
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Install gdown.**
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Download the compressed data using gdown.**
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve downloaded the data, unzip it to a celeba_gan folder
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.30 Getting the CelebA data
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: zip::unzip("img_align_celeba.zip", exdir = "celeba_gan")➊
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Uncompress the data.**
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve got the uncompressed images in a directory, you can use image_ dataset_from_directory()
    to turn it into a TF Dataset. Because we just need the images—there are no labels—we’ll
    specify label_mode = NULL.
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.31 Creating a TF Dataset from a directory of images**'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: dataset <— image_dataset_from_directory(
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: '"celeba_gan",'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
- en: label_mode = NULL,➊
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: image_size = c(64, 64),➋
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 32,➋
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: crop_to_aspect_ratio = TRUE➋
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Only the images will be returned—no labels.**
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We will resize the images to 64 × 64 by using a smart combination of cropping
    and resizing to preserve aspect ratio. We don't want face proportions to get distorted!**
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s rescale the images to the [0-1] range.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.32 Rescaling the images**'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
- en: library(tfdatasets)
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
- en: dataset %<>% dataset_map(~ .x / 255)
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: You can use the following code to display a sample image.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.33 Displaying the first image**'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: x <— dataset %>% as_iterator() %>% iter_next()
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor(x[1, , , ], max = 1)
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0446-01.jpg)'
  id: totrans-1008
  prefs: []
  type: TYPE_IMG
- en: 12.5.4 The discriminator
  id: totrans-1009
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll develop a discriminator model that takes as input a candidate
    image (real or synthetic) and classifies it into one of two classes: “generated
    image” or “real image that comes from the training set.” One of the many issues
    that commonly arise with GANs is that the generator gets stuck with generated
    images that look like noise. A possible solution is to use dropout in the discriminator,
    so that’s what we will do here.'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.34 The GAN discriminator network**'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
- en: discriminator <-
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: keras_model_sequential(name = "discriminator",
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = c(64, 64, 3)) %>%
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(64, kernel_size = 4, strides = 2, padding = "same") %>%
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(128, kernel_size = 4, strides = 2, padding = "same") %>%
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(128, kernel_size = 4, strides = 2, padding = "same") %>%
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: layer_flatten() %>%
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.2) %>% ➊
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **One dropout layer: an important trick!**'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the discriminator model summary:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
- en: discriminator
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0447-01.jpg)'
  id: totrans-1027
  prefs: []
  type: TYPE_IMG
- en: 12.5.5 The generator
  id: totrans-1028
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s develop a generator model that turns a vector (from the latent space—
    during training it will be sampled at random) into a candidate image.
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.35 GAN generator network
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <— 128➊
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: generator <
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
- en: keras_model_sequential(name = "generator",
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = c(latent_dim)) %>%
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(8 * 8 * 128) %>% ➋
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
- en: layer_reshape(c(8, 8, 128)) %>%➌
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(128, kernel_size = 4,
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>% ➍
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(256, kernel_size = 4,
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%➎
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(512, kernel_size = 4,
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(3, kernel_size = 5, padding = "same",
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: activation = "sigmoid")
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The latent space will be made of 128-dimensional vectors.**
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Produce the same number of coefficients we had at the level of the Flatten
    layer in the encoder.**
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Revert the layer_flatten() of the encoder.**
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Revert the layer_conv_2d() of the encoder.**
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Use Leaky Relu as our activation**
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the generator model summary:'
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
- en: generator
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0448-01.jpg)'
  id: totrans-1055
  prefs: []
  type: TYPE_IMG
- en: 12.5.6 The adversarial network
  id: totrans-1056
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we’ll set up the GAN, which chains the generator and the discriminator.
    When trained, this model will move the generator in a direction that improves
    its ability to fool the discriminator. This model turns latent-space points into
    a classification decision— “fake” or “real”—and it’s meant to be trained with
    labels that are always “these are real images.” So training gan will update the
    weights of generator in a way that makes discriminator more likely to predict
    “real” when looking at fake images.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
- en: 'To recapitulate, this is what the training loop looks like schematically. For
    each epoch, you do the following:'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Draw random points in the latent space (random noise).'
  id: totrans-1059
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Generate images with generator using this random noise.'
  id: totrans-1060
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** Mix the generated images with real ones.'
  id: totrans-1061
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**4** Train discriminator using these mixed images, with corresponding targets:
    either “real” (for the real images) or “fake” (for the generated images).'
  id: totrans-1062
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**5** Draw new random points in the latent space.'
  id: totrans-1063
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6** Train generator using these random vectors, with targets that all say
    “these are real images.” This updates the weights of the generator to move them
    toward getting the discriminator to predict “these are real images” for generated
    images: this trains the generator to fool the discriminator.'
  id: totrans-1064
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s implement it. Like in our VAE example, we’ll use a new_model_class() with
    a custom train_step(). Note that we’ll use two optimizers (one for the generator
    and one for the discriminator), so we will also override compile() to allow for
    passing two optimizers.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.36 The GAN Model**'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: GAN <— new_model_class(
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
- en: classname = "GAN",
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(discriminator, generator, latent_dim) {
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize()
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: self$discriminator  <— discriminator
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: self$generator      <— generator
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
- en: self$latent_dim     <— as.integer(latent_dim)
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: self$d_loss_metric  <— metric_mean(name = "d_loss")➊
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric  <— metric_mean(name = "g_loss")➊
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: compile = function(d_optimizer, g_optimizer, loss_fn) {
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
- en: super$compile()
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
- en: self$d_optimizer <— d_optimizer
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: self$g_optimizer <— g_optimizer
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
- en: self$loss_fn <— loss_fn
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
- en: metrics = mark_active(function() {
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
- en: list(self$d_loss_metric,
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric)
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
- en: '}),'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(real_images) {➋
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: batch_size <— tf$shape(real_images)[1]
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <➌
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(batch_size, self$latent_dim))
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
- en: generated_images <
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: self$generator(random_latent_vectors)➍
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: combined_images <
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
- en: tf$concat(list(generated_images,
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: real_images),➎
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
- en: axis = 0L)
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: labels <
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: tf$concat(list(tf$ones(tuple(batch_size, 1L)),➏
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
- en: tf$zeros(tuple(batch_size, 1L))),
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
- en: axis = 0L)
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
- en: labels %<>% `+`(
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$uniform(tf$shape(.), maxval = 0.05))➐
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
- en: predictions <— self$discriminator(combined_images)
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
- en: d_loss <— self$loss_fn(labels, predictions)
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(d_loss, self$discriminator$trainable_weights)
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: self$d_optimizer$apply_gradients(➑
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
- en: zip_lists(grads, self$discriminator$trainable_weights))
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <➒
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(batch_size, self$latent_dim))
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
- en: misleading_labels <— tf$zeros(tuple(batch_size, 1L))➓
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: predictions <— random_latent_vectors %>%
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: self$generator() %>%
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
- en: self$discriminator()
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: g_loss <— self$loss_fn(misleading_labels, predictions)
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(g_loss, self$generator$trainable_weights)
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: self$g_optimizer$apply_gradients(⓫
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
- en: zip_lists(grads, self$generator$trainable_weights))
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: self$d_loss_metric$update_state(d_loss)
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric$update_state(g_loss)
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
- en: list(d_loss = self$d_loss_metric$result(),
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
- en: g_loss = self$g_loss_metric$result())
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Set up metrics to track the two losses over each training epoch.**
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **train_step is called with a batch of real images.**
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Sample random points in the latent space.**
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Decode them to fake images.**
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Combine them with real images.**
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Assemble labels, discriminating real from fake images.**
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add random noise to the labels—an important trick!**
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Train the discriminator.**
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Sample random points in the latent space.**
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Assemble labels that say "these are all real images" (it's a lie!).**
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ **Train the generator.**
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start training, let’s also set up a callback to monitor our results:
    it will use the generator to create and save a number of fake images at the end
    of each epoch.'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.37 A callback that samples generated images during training
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
- en: callback_gan_monitor <— new_callback_class(
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: classname = "GANMonitor",
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(num_img = 3, latent_dim = 128,
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
- en: dirpath = "gan_generated_images") {
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
- en: private$num_img <— as.integer(num_img)
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: private$latent_dim <— as.integer(latent_dim)
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
- en: private$dirpath <— fs::path(dirpath)
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: fs::dir_create(dirpath)
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_end = function(epoch, logs = NULL) {
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(private$num_img, private$latent_dim))
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: generated_images <— random_latent_vectors %>%
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: self$model$generator() %>%
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
- en: '{ tf$saturate_cast(. * 255, "uint8") }➊'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(private$num_img))
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$write_file(
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: filename = private$dirpath / sprintf("img_%03i_%02i.png", epoch, i),
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
- en: contents = tf$io$encode_png(generated_images[i, , , ])
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Scale and clip to uint8 range of [0, 255], and cast to uint8.**
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can start training.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.38 Compiling and training the GAN**'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: epochs <— 100➊
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: gan <— GAN(discriminator = discriminator,➋
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: generator = generator,
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim = latent_dim)
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
- en: gan %>% compile(
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: d_optimizer = optimizer_adam(learning_rate = 0.0001),
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: g_optimizer = optimizer_adam(learning_rate = 0.0001),
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: loss_fn = loss_binary_crossentropy()
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: gan %>% fit(
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: dataset,
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
- en: epochs = epochs,
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callback_gan_monitor(num_img = 10, latent_dim = latent_dim)
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **You'll start getting interesting results after epoch 20.**
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Instantiate the GAN model.**
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
- en: When training, you may see the adversarial loss begin to increase considerably,
    whereas the discriminative loss tends to zero—the discriminator may end up dominating
    the generator. If that’s the case, try reducing the discriminator learning rate
    and increasing the dropout rate of the discriminator. [Figure 12.22](#fig12-22)
    shows what our GAN is capable of generating after 30 epochs of training.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0452-01.jpg)'
  id: totrans-1182
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.22 Some generated images around epoch 30**'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.7 Wrapping up
  id: totrans-1184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A GAN consists of a generator network coupled with a discriminator network The
    discriminator is trained to differentiate between the output of the generator
    and real images from a training dataset, and the generator is trained to fool
    the discriminator. Remarkably, the generator never sees images from the training
    set directly; the information it has about the data comes from the discriminator.
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs are difficult to train, because training a GAN is a dynamic process rather
    than a simple gradient-descent process with a fixed loss landscape. Getting a
    GAN to train correctly requires using a number of heuristic tricks, as well as
    extensive tuning.
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs can potentially produce highly realistic images. But unlike VAEs, the latent
    space they learn doesn’t have a neat continuous structure and thus may not be
    suited for certain practical applications, such as image editing via latent-space
    concept vectors.
  id: totrans-1187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These few techniques cover only the basics of this fast-expanding field. There’s
    a lot more to discover out there—generative deep learning is deserving of an entire
    book of its own.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use a sequence-to-sequence model to generate sequence data, one step
    at a time. This is applicable to text generation but also to note-by-note music
    generation or any other type of time-series data.
  id: totrans-1190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepDream works by maximizing the convnet layer activations through gradient
    ascent in input space.
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the style-transfer algorithm, a content image and a style image are combined
    via gradient descent to produce an image with the high-level features of the content
    image and the local characteristics of the style image.
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAEs and GANs are models that learn a latent space of images and can then dream
    up entirely new images by sampling from the latent space. Concept vectors in the
    latent space can even be used for image editing.
  id: totrans-1193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](#endnote1)) Iannis Xenakis, “Musiques formelles: nouveaux principes formels
    de composition musicale,” special issue of *La Revue musicale*, nos. 253–254 (1963).'
  id: totrans-1194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([2](#endnote2)) Alex Graves, “Generating Sequences with Recurrent Neural Networks,”
    arXiv (2013), [https://arxiv.org/abs/1308.0850](https://arxiv.org/abs/1308.0850).
  id: totrans-1195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '^([3](#endnote3)) Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “DeepDream:
    A Code Example for Visualizing Neural Networks,” Google Research Blog, July 1,
    2015, [http://mng.bz/xXlM](http://mng.bz/xXlM).'
  id: totrans-1196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([4](#endnote4)) Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A
    Neural Algorithm of Artistic Style,” arXiv (2015), [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576).
  id: totrans-1197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([5](#endnote5)) Diederik P. Kingma and Max Welling, “Auto-Encoding Variational
    Bayes,” arXiv (2013), [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).
  id: totrans-1198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([6](#endnote6)) Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra,
    “Stochastic Backpropagation and Approximate Inference in Deep Generative Models,”
    arXiv (2014), [https://arxiv.org/abs/1401.4082](https://arxiv.org/abs/1401.4082).
  id: totrans-1199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([7](#endnote7)) Ian Goodfellow et al., “Generative Adversarial Networks,”
    arXiv (2014), [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
  id: totrans-1200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
