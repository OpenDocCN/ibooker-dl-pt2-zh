- en: 12 Generative deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 生成深度学习
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Text generation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: DeepDream
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDream
- en: Neural style transfer
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经风格转换
- en: Variational autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: Generative adversarial networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: The potential of artificial intelligence to emulate human thought processes
    goes beyond passive tasks such as object recognition and mostly reactive tasks
    such as driving a car. It extends well into creative activities. When I first
    made the claim that in the not-so-distant future, most of the cultural content
    that we consume will be created with substantial help from AIs, I was met with
    utter disbelief, even from long-time machine learning practitioners. That was
    in 2014\. Fast-forward a few years, and the disbelief had receded at an incredible
    speed. In the summer of 2015, we were entertained by Google’s DeepDream algorithm
    turning an image into a psychedelic mess of dog eyes and pareidolic artifacts;
    in 2016, we started using smartphone applications to turn photos into paintings
    of various styles. In the summer of 2016, an experimental short movie, *Sunspring*,
    was directed using a script written by a long short-term memory (LSTM). Maybe
    you’ve recently listened to music that was generated by a neural network.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模拟人类思维过程的潜力不仅限于客观任务，如物体识别，也包括大部分是被动任务，如驾车。它还延伸到创造性活动。当我第一次声称在不久的将来，我们消费的大部分文化内容将在很大程度上在AI的帮助下创作时，甚至从事机器学习已久的从业者也对此表示怀疑。那是在2014年。几年后，怀疑以惊人的速度消退。2015年夏天，我们被Google的DeepDream算法转换成一幅充满狗眼和类似图像的迷幻图片所娱乐；2016年，我们开始使用智能手机应用程序将照片转换成各种风格的绘画；2016年夏天，一部实验性的短片《夕阳之泉》是由长短期记忆（LSTM）写的剧本导演而成的。也许你最近听过由神经网络生成的音乐。
- en: 'Granted, the artistic productions we’ve seen from AI so far have been fairly
    low quality. AI isn’t anywhere close to rivaling human screenwriters, painters,
    and composers. But replacing humans was always beside the point: artificial intelligence
    isn’t about replacing our own intelligence with something else; it’s about bringing
    into our lives and work *more* intelligence—intelligence of a different kind.
    In many fields, but especially in creative ones, AI will be used by humans as
    a tool to augment their own capabilities: more *augmented intelligence* than *artificial
    intelligence*.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，到目前为止，我们从AI中看到的艺术作品质量相当低。AI离得上人类编剧、画家和作曲家还远。但替代人类本来就不是重点：人工智能不是要用其他东西取代我们自己的智能，而是要给我们的生活和工作带来*更多*的智能——不同类型的智能。在许多领域，尤其是创造性领域，AI将被人类用作增强自身能力的工具：更多的*增强智能*而不是*人工智能*。
- en: 'A large part of artistic creation consists of simple pattern recognition and
    technical skill. And that’s precisely the part of the process that many find less
    attractive or even dispensable. That’s where AI comes in. Our perceptual modalities,
    our language, and our artwork all have statistical structure. Learning this structure
    is what deep learning algorithms excel at. Machine learning models can learn the
    statistical *latent space* of images, music, and stories, and they can then *sample*
    from this space, creating new artworks with characteristics similar to those the
    model has seen in its training data. Naturally, such sampling is hardly an act
    of artistic creation in itself. It’s a mere mathematical operation: the algorithm
    has no grounding in human life, human emotions, or our experience of the world;
    instead, it learns from an experience that has little in common with ours. It’s
    only our interpretation, as human spectators, that will give meaning to what the
    model generates. But in the hands of a skilled artist, algorithmic generation
    can be steered to become meaningful—and beautiful. Latent space sampling can become
    a brush that empowers the artist, augments our creative affordances, and expands
    the space of what we can imagine. What’s more, it can make artistic creation more
    accessible by eliminating the need for technical skill and practice, setting up
    a new medium of pure expression, factoring art apart from craft.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 艺术创作的一个重要部分是简单的模式识别和技术技能。而这恰恰是许多人觉得不那么吸引人甚至可以舍弃的部分。这就是 AI 发挥作用的地方。我们的感知模式，我们的语言和我们的艺术作品都具有统计结构。学习这种结构是深度学习算法擅长的。机器学习模型可以学习图像、音乐和故事的统计*潜在空间*，然后可以从这个空间中*采样*，创建具有与模型在训练数据中见过的相似特征的新艺术作品。当然，这样的采样本身几乎不是艺术创作的行为。这只是一种纯粹的数学操作：算法没有基于人类生活、人类情感或我们对世界的经验；相反，它是从一个与我们的经验几乎没有共同之处的经验中学习的。只有我们作为人类观众的解释才能赋予模型生成的东西意义。但在一个技艺精湛的艺术家手中，算法生成可以被引导变得有意义和美丽。潜在空间采样可以成为赋予艺术家力量的画笔，增强我们的创造能力，并扩展我们可以想象的空间。更重要的是，它可以通过消除对技术技能和实践的需求使艺术创作更加容易，建立起一种纯粹表达的新媒介，将艺术与工艺分开。
- en: Iannis Xenakis, a visionary pioneer of electronic and algorithmic music, beautifully
    expressed this same idea in the 1960s, in the context of the application of automation
    technology to music composition:^([1](#Rendnote1))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Iannis Xenakis，电子音乐和算法音乐的开创者，于 1960 年代在将自动化技术应用于音乐作曲的背景下美妙地表达了这一想法：^([1](#Rendnote1))
- en: '*Freed from tedious calculations, the composer is able to devote himself to
    the general problems that the new musical form poses and to explore the nooks
    and crannies of this form while modifying the values of the input data. For example,
    he may test all instrumental combinations from soloists, to chamber orchestras,
    to large orchestras. With the aid of electronic computers the composer becomes
    a sort of pilot: he presses the buttons, introduces coordinates, and supervises
    the controls of a cosmic vessel sailing in the space of sound, across sonic constellations
    and galaxies that he could formerly glimpse only as a distant dream.*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*从繁琐的计算中解脱出来，作曲家能够将自己专注于新音乐形式提出的一般问题，并在修改输入数据的值时探索这种形式的每个角落。例如，他可以测试从独奏家到室内乐队再到大型管弦乐队的所有器乐组合。在电子计算机的帮助下，作曲家成为一种飞行员：他按下按钮，输入坐标，并监督着一艘在声音空间中航行的宇宙飞船的控制，穿越他曾经只能将其看作是遥远梦想的声音星座和星系。*'
- en: In this chapter, we’ll explore from various angles the potential of deep learning
    to augment artistic creation. We’ll review sequence data generation (which can
    be used to generate text or music), DeepDream, and image generation using both
    variational autoencoders and generative adversarial networks. We’ll get your computer
    to dream up content never seen before; and maybe we’ll get you to dream, too,
    about the fantastic possibilities that lie at the intersection of technology and
    art. Let’s get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从各个角度探讨深度学习增强艺术创作的潜力。我们将回顾序列数据生成（可用于生成文本或音乐）、DeepDream，以及使用变分自动编码器和生成对抗网络进行图像生成。我们将让您的计算机做出以前从未见过的内容；也许我们还会让您梦想，梦想着技术和艺术交汇的奇妙可能性。让我们开始吧。
- en: 12.1 Text generation
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 文本生成
- en: 'In this section, we’ll explore how recurrent neural networks can be used to
    generate sequence data. We’ll use text generation as an example, but the exact
    same techniques can be generalized to any kind of sequence data: you could apply
    it to sequences of musical notes to generate new music, to time series of brushstroke
    data (perhaps recorded while an artist paints on an iPad) to generate paintings
    stroke by stroke, and so on.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨循环神经网络如何用于生成序列数据。我们以文本生成为例，但完全相同的技术可以推广到任何类型的序列数据：你可以将其应用于音乐音符序列以生成新音乐，应用于笔划数据的时间序列（也许是记录艺术家在iPad上绘画时记录下的）以逐笔生成绘画，等等。
- en: Sequence data generation is in no way limited to artistic content generation.
    It has been successfully applied to speech synthesis and to dialogue generation
    for chatbots. The Smart Reply feature that Google released in 2016, capable of
    automatically generating a selection of quick replies to emails or text messages,
    is powered by similar techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 序列数据生成绝不仅限于艺术内容生成。它已成功应用于语音合成和聊天机器人的对话生成。谷歌于2016年发布的Smart Reply功能，能够自动生成一系列快速回复电子邮件或短信的选项，就是由类似的技术驱动的。
- en: 12.1.1 A brief history of generative deep learning for sequence generation
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 生成式深度学习用于序列生成的简要历史
- en: In late 2014, few people had ever seen the initials LSTM, even in the machine
    learning community. Successful applications of sequence data generation with recurrent
    networks began to appear in the mainstream only in 2016\. But these techniques
    have a fairly long history, starting with the development of the LSTM algorithm
    in 1997 (discussed in chapter 10). This new algorithm was used early on to generate
    text character by character.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到了2014年末，很少有人在机器学习社区甚至见过LSTM这个缩写。成功应用循环网络生成序列数据的案例直到2016年才开始出现在主流中。但是这些技术具有相当长的历史，从1997年LSTM算法的开发开始（在第10章讨论过）。这个新算法最初用于逐字符生成文本。
- en: In 2002, Douglas Eck, then at Schmidhuber’s lab in Switzerland, applied LSTM
    to music generation for the first time, with promising results. Eck is now a researcher
    at Google Brain, and in 2016, he started a new research group there, called Magenta,
    focused on applying modern deep learning techniques to produce engaging music.
    Sometimes good ideas take 15 years to get started.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 2002年，当时在瑞士Schmidhuber实验室的Douglas Eck首次将LSTM应用于音乐生成，并取得了令人鼓舞的结果。Eck现在是Google
    Brain的研究员，在2016年，他在那里成立了一个名为Magenta的新研究组，专注于将现代深度学习技术应用于产生引人入胜的音乐。有时好的想法需要15年才能开始实施。
- en: 'In the late 2000s and early 2010s, Alex Graves did important pioneering work
    using recurrent networks for sequence data generation. In particular, his 2013
    work on applying recurrent mixture density networks to generate human-like handwriting
    using time series of pen positions is seen by some as a turning point.^([2](#Rendnote2))
    This specific application of neural networks at that specific moment in time captured
    for me the notion of *machines that dream* and was a significant inspiration around
    the time I started developing Keras. Graves left a similar commented-out remark
    hidden in a 2013 LaTeX file uploaded to the preprint server arXiv: “Generating
    sequential data is the closest computers get to dreaming.” Several years later,
    we take a lot of these developments for granted, but at the time it was difficult
    to watch Graves’s demonstrations and not walk away awe-inspired by the possibilities.
    Between 2015 and 2017, recurrent neural networks were successfully used for text
    and dialogue generation, music generation, and speech synthesis.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年末和2010年初，Alex Graves通过使用循环网络生成序列数据做出了重要的开创性工作。特别是，他在2013年将循环混合密度网络应用于使用笔位置的时间序列生成类似人类手写的工作被一些人视为一个转折点。在那个特定的时间点上，神经网络的这种特定应用捕捉到了“机器梦想”的概念，并且在我开始开发Keras的时候是一个重要的灵感来源。几年后，我们很多这样的发展已经司空见惯，但是在当时，很难看到Graves的演示而不对可能性感到敬畏。在2015年至2017年期间，循环神经网络成功用于文本和对话生成，音乐生成和语音合成。
- en: Then around 2017–2018, the Transformer architecture started taking over recurrent
    neural networks, not just for supervised natural language processing tasks but
    also for generative sequence models—in particular *language modeling* (word-level
    text generation). The best-known example of a generative Transformer would be
    GPT-3, a 175 billion parameter text-generation model trained by the startup OpenAI
    on an astound-ingly large text corpus, including most digitally available books,
    Wikipedia, and a large fraction of a crawl of the entire internet. GPT-3 made
    headlines in 2020 due to its capability to generate plausible-sounding text paragraphs
    on virtually any topic, a prowess that has fed a short-lived hype wave worthy
    of the most torrid AI summer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在2017-2018年，Transformer架构开始取代递归神经网络，不仅用于监督自然语言处理任务，也用于生成序列模型，特别是*语言建模*（词级文本生成）。最著名的生成式Transformer示例是GPT-3，这是一种1750亿参数的文本生成模型，由初创公司OpenAI在庞大的文本语料库上进行训练，包括大多数数字化的书籍，维基百科以及整个互联网爬取的大部分内容。GPT-3因其生成几乎任何主题的听起来可信的文本段落的能力而在2020年成为头条新闻，这种能力引发了最激烈的短暂人工智能热潮之一。
- en: 12.1.2 How do you generate sequence data?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 如何生成序列数据？
- en: 'The universal way to generate sequence data in deep learning is to train a
    model (usually a Transformer or an RNN) to predict the next token or next few
    tokens in a sequence, using the previous tokens as input. For instance, given
    the input “the cat is on the,” the model is trained to predict the target “mat,”
    the next word. As usual when working with text data, tokens are typically words
    or characters, and any network that can model the probability of the next token
    given the previous ones is called a *language model*. A language model captures
    the *latent space* of language: its statistical structure.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中生成序列数据的通用方法是训练一个模型（通常是Transformer或RNN），以预测序列中下一个标记或下几个标记，使用前面的标记作为输入。例如，给定输入“猫在上面”，模型会被训练以预测目标“垫子”，下一个单词。通常在处理文本数据时，标记通常是单词或字符，并且任何可以模拟给定先前标记情况下下一个标记的概率的网络都称为*语言模型*。语言模型捕捉了语言的*潜在空间*：它的统计结构。
- en: 'Once you have such a trained language model, you can *sample* from it (generate
    new sequences): you feed it an initial string of text (called *conditioning data*),
    ask it to generate the next character or the next word (you can even generate
    several tokens at once), add the generated output back to the input data, and
    repeat the process many times (see [figure 12.1](#fig12-1)). This loop allows
    you to generate sequences of arbitrary length that reflect the structure of the
    data on which the model was trained: sequences that look *almost* like human-written
    sentences.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了训练好的语言模型，你可以从模型中*采样*（生成新的序列）：你馈送一个初始的文本字符串（称为*调节数据*），请求它生成下一个字符或下一个单词（你甚至可以一次生成多个标记），将生成的输出添加回输入数据，并重复这个过程多次（参见[图12.1](#fig12-1)）。这个循环允许您生成任意长度的序列，反映了模型训练的数据结构：几乎像人类书写的句子。
- en: '![Image](../images/f0402-01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0402-01.jpg)'
- en: '**Figure 12.1 The process of word-by-word text generation using a language
    model**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.1 使用语言模型逐字逐句生成文本的过程**'
- en: 12.1.3 The importance of the sampling strategy
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.3 采样策略的重要性
- en: 'When generating text, the way you choose the next token is crucially important.
    A naive approach is *greedy sampling*, consisting of always choosing the most
    likely next character. But such an approach results in repetitive, predictable
    strings that don’t look like coherent language. A more interesting approach makes
    slightly more surprising choices: it introduces randomness in the sampling process
    by sampling from the probability distribution for the next character. This is
    called *stochastic sampling* (recall that *stochasticity* is what we call *randomness*
    in this field). In such a setup, if a word has a 0.3 probability of being next
    in the sentence according to the model, you’ll choose it 30% of the time. Note
    that greedy sampling can also be cast as sampling from a probability distribution:
    one where a certain word has probability 1 and all others have probability 0.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成文本时，选择下一个标记的方式非常重要。一种简单的方法是*贪心抽样*，总是选择可能性最高的下一个字符。但这种方法会产生重复、可预测的字符串，不像是连贯的语言。一种更有趣的方法是做出稍微意外的选择：通过从下一个字符的概率分布中抽样，在抽样过程中引入随机性。这被称为*随机抽样*（这里需要注意的是，在这个领域中，*随机性*称为*随机性*）。在这样的设置中，如果根据模型，某个词在句子中作为下一个出现的概率为0.3，那么你将有30%的概率选择它。需要注意的是，贪心抽样也可以看作是从概率分布中进行抽样：其中某个词的概率为1，其他所有词的概率都为0。
- en: 'Sampling probabilistically from the softmax output of the model is neat: it
    allows even unlikely words to be sampled some of the time, generating more interesting-looking
    sentences and sometimes showing creativity by coming up with new, realistic-sounding
    sentences that didn’t occur in the training data. But there’s one issue with this
    strategy: it doesn’t offer a way to *control the amount of randomness* in the
    sampling process.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的 softmax 输出中以概率的方式抽样是不错的方法：即使是不太可能的单词也有可能被抽样到，这样生成的句子更有趣，有时甚至能创造出之前在训练数据中没有出现过的、听起来很真实的句子。但这种策略存在一个问题：它没有提供一种*控制随机性的方法*。
- en: 'Why would you want more or less randomness? Consider an extreme case: pure
    random sampling, where you draw the next word from a uniform probability distribution,
    and every word is equally likely. This scheme has maximum randomness; in other
    words, this probability distribution has maximum entropy. Naturally, it won’t
    produce anything interesting. At the other extreme, greedy sampling doesn’t produce
    anything interesting, either, and has no randomness: the corresponding probability
    distribution has minimum entropy. Sampling from the “real” probability distribution—
    the distribution that is output by the model’s softmax function—constitutes an
    intermediate point between these two extremes. But there are many other intermediate
    points of higher or lower entropy that you may want to explore. Less entropy will
    give the generated sequences a more predictable structure (and, thus, they will
    potentially be more realistic looking), whereas more entropy will result in more
    surprising and creative sequences. When sampling from generative models, it’s
    always good to explore different amounts of randomness in the generation process.
    Because we—humans— are the ultimate judges of how interesting the generated data
    is, interestingness is highly subjective, and there’s no telling in advance where
    the point of optimal entropy lies.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你想要更多或者更少的随机性呢？考虑一个极端情况：纯随机抽样，你从一个均匀概率分布中抽取下一个词，每个词的概率都是相等的。这种方案具有最大的随机性；换句话说，该概率分布具有最大的熵。显然，它不会产生任何有趣的结果。另一方面，贪心抽样也不会产生有趣的结果，而且没有随机性：相应的概率分布具有最小的熵。从“真实”的概率分布中抽样——即模型的
    softmax 函数输出的分布——构成了这两个极端之间的一个中间点。但是，在更高或更低熵的许多其他中间点上也可以进行抽样，你可能想要在其中进行探索。较低的熵会给生成的序列提供一个更可预测的结构（因此，它们有可能看起来更真实），而较高的熵会产生更令人惊讶和富有创造力的序列。在从生成模型进行抽样时，探索不同随机性的产生过程是很有意义的。因为我们——人类——是对生成数据的有趣程度的终极评判者，所以有趣程度是非常主观的，无法事先确定最佳熵值所在的位置。
- en: 'To control the amount of stochasticity in the sampling process, we’ll introduce
    a parameter called the *softmax temperature*, which characterizes the entropy
    of the probability distribution used for sampling: it characterizes how surprising
    or predictable the choice of the next word will be. Given a temperature value,
    a new probability distribution is computed from the original one (the softmax
    output of the model) by reweighting it in the following way.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制采样过程中的随机性，我们将引入一个参数，称为*softmax temperature*，它描述了用于采样的概率分布的熵：它描述了选择下一个单词的选择是多么令人惊讶或可预测。给定一个温度值，可以通过以下方式从原始概率分布（模型的
    softmax 输出）计算出一个新的概率分布，即将其重新加权。
- en: Higher temperatures result in sampling distributions of higher entropy that
    will generate more surprising and unstructured generated data, whereas a lower
    temperature will result in less randomness and much more predictable generated
    data (see [figure 12.2](#fig12-2)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 较高的温度会导致更高熵的采样分布，将产生更令人惊讶和结构不明显的生成数据，而较低的温度将导致更少的随机性和更可预测的生成数据（参见 [图 12.2](#fig12-2)）。
- en: Listing 12.1 Reweighting a probability distribution to a different temperature
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 将概率分布重新加权为不同温度的示例
- en: reweight_distribution <-
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加权分布 <-
- en: function(original_distribution, temperature = 0.5) {
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: function(original_distribution, temperature = 0.5) {
- en: original_distribution %>% .➊
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: original_distribution %>% .➊
- en: '{ exp(log(.) / temperature) } %>%'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '{ exp(log(.) / temperature) } %>%'
- en: '{ . / sum(.) } ➋'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '{ . / sum(.) } ➋'
- en: '} ➌'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '} ➌'
- en: ➊ **original_distribution is a 1D array of probability values that must sum
    to 1. temperature is a factor quantifying the entropy of the output distribution.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **original_distribution 是一个概率值的一维数组，必须总和为1。temperature 是一个量化输出分布熵的因子。**
- en: ➋ **Return a reweighted version of the original distribution. The sum of the
    distribution may no longer be 1, so you divide it by its sum to obtain the new
    distribution.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **返回原始分布的重新加权版本。分布的总和可能不再为1，因此将其除以其总和以获得新的分布。**
- en: ➌ **Note that reweight_distribution() will work for both 1D R vectors and 1D
    Tensorflow tensors, because exp, log, /, and sum are all R generics.**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **请注意，reweight_distribution() 将适用于 1D R 向量和 1D Tensorflow 张量，因为 exp、log、/
    和 sum 都是 R 通用函数。**
- en: '![Image](../images/f0404-01.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0404-01.jpg)'
- en: '**Figure 12.2 Different reweightings of one probability distribution: Low temperature
    = more deterministic; high temperature = more random**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.2 对一个概率分布进行不同的重新加权：低温度 = 更确定性；高温度 = 更随机性**'
- en: 12.1.4 Implementing text generation with Keras
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.4 使用 Keras 实现文本生成
- en: Let’s put these ideas into practice in a Keras implementation. The first thing
    you need is a lot of text data that you can use to learn a language model. You
    can use any sufficiently large text file or set of text files—Wikipedia, *The
    Lord of the Rings*, and so on.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Keras 实现中将这些想法付诸实践。你首先需要大量的文本数据，可以用来学习语言模型。你可以使用任何足够大的文本文件或文本文件集 - 维基百科、《指环王》等。
- en: In this example, we’ll keep working with the IMDB movie review dataset from
    the last chapter, and we’ll learn to generate never-read-before movie reviews.
    As such, our language model will be a model of the style and topics of these movie
    reviews specifically, rather than a general model of the English language.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将继续使用上一章的 IMDB 电影评论数据集，并学习生成以前未读过的电影评论。因此，我们的语言模型将是针对这些电影评论的风格和主题的模型，而不是英语语言的通用模型。
- en: PREPARING THE DATA
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: Just like in the previous chapter, let’s download and uncompress the IMDB movie
    reviews dataset. (This is the same dataset we downloaded in chapter 11.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在前一章中一样，让我们下载并解压缩 IMDB 电影评论数据集。（这是我们在第 11 章中下载的同一数据集。）
- en: Listing 12.2 Downloading and uncompressing the IMDB movie reviews dataset
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 下载并解压缩 IMDB 电影评论数据集
- en: url <— "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: url <— "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
- en: filename <— basename(url)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: filename <— basename(url)
- en: options(timeout = 60 * 10)➊
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: options(timeout = 60 * 10)➊
- en: download.file(url, destfile = filename)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: download.file(url, destfile = filename)
- en: untar(filename)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: untar(filename)
- en: ➊ **10-minute timeout**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **10 分钟超时**
- en: 'You’re already familiar with the structure of the data: we get a folder named
    aclImdb containing two subfolders, one for negative-sentiment movie reviews and
    one for positive-sentiment reviews. There’s one text file per review. We’ll call
    text_dataset_ from_directory() with label_mode = NULL to create a TF Dataset that
    reads from these files and yields the text content of each file.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经熟悉数据的结构：我们得到一个名为aclImdb的文件夹，其中包含两个子文件夹，一个用于负面情感的电影评论，一个用于正面情感的评论。每个评论都有一个文本文件。我们将调用text_dataset_from_directory()并将label_mode
    = NULL作为参数，以创建一个TF数据集，该数据集从这些文件中读取并生成每个文件的文本内容。
- en: '**Listing 12.3 Creating a TF Dataset from text files (one file = one sample)**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单12.3 从文本文件创建TF数据集（一个文件 = 一个样本）**'
- en: library(tensorflow)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: library(tfdatasets)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: library(keras)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: dataset <— text_dataset_from_directory(
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: dataset <— text_dataset_from_directory(
- en: directory = "aclImdb",
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: directory = "aclImdb",
- en: label_mode = NULL,
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: label_mode = NULL,
- en: batch_size = 256)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 256)
- en: dataset <— dataset %>%
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: dataset <— dataset %>%
- en: dataset_map( ~ tf$strings$regex_replace(.x, "<br />", " "))➊
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map( ~ tf$strings$regex_replace(.x, "<br />", " "))➊
- en: ➊ **Strip the "<br />" HTML tag that occurs in many of the reviews. This did
    not matter much for text classification, but we wouldn't want to generate "<br
    />" tags in this example!**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **去除许多评论中出现的"<br />" HTML标签。这在文本分类中并不重要，但在这个例子中我们不想生成"<br />"标签！**
- en: 'Now let’s use a layer_text_vectorization() to compute the vocabulary we’ll
    be working with. We’ll use only the first sequence_length words of each review:
    our layer_ text_vectorization() will cut off anything beyond that when vectorizing
    a text.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用一个layer_text_vectorization()来计算我们将使用的词汇表。我们只使用每个评论的前sequence_length个单词：当向量化文本时，我们的layer_text_vectorization()将在超出这个范围时截断任何内容。
- en: Listing 12.4 Preparing a layer_text_vectorization()
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 清单12.4 准备一个layer_text_vectorization()
- en: sequence_length <— 100
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: sequence_length <— 100
- en: vocab_size <— 15000➊
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: vocab_size <— 15000➊
- en: text_vectorization <— layer_text_vectorization(
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <— layer_text_vectorization(
- en: max_tokens = vocab_size,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = vocab_size,
- en: output_mode = "int",➋
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "int",➋
- en: output_sequence_length = sequence_length➌
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: output_sequence_length = sequence_length➌
- en: )
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: adapt(text_vectorization, dataset)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: adapt(text_vectorization, dataset)
- en: ➊ **We'll consider only the top 15,000 most common words—anything else will
    be treated as the out-of-vocabulary token, "[UNK]".**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们将只考虑前15000个最常见的单词——其他任何单词都将被视为未知标记"[UNK]"。**
- en: ➋ **We want to return integer word index sequences.**
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们想返回整数单词索引序列。**
- en: ➌ **We'll work with inputs and targets of length 100 (but because we'll offset
    the targets by 1, the model will actually see sequences of length 99).**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **我们将使用长度为100的输入和目标（但因为我们将目标偏移1，所以模型实际上将看到长度为99的序列）。**
- en: Let’s use the layer to create a language-modeling dataset where input samples
    are vectorized texts and corresponding targets are the same texts offset by one
    word.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用该层来创建一个语言建模数据集，其中输入样本是向量化的文本，相应的目标是将文本偏移一个单词后的相同文本。
- en: Listing 12.5 Setting up a language-modeling dataset
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 清单12.5 设置语言建模数据集
- en: prepare_lm_dataset <— function(text_batch) {
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: prepare_lm_dataset <— function(text_batch) {
- en: vectorized_sequences <— text_vectorization(text_batch)➊
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: vectorized_sequences <— text_vectorization(text_batch)➊
- en: x <— vectorized_sequences[, NA:-2]➋
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: x <— vectorized_sequences[, NA:-2]➋
- en: y <— vectorized_sequences[, 2:NA]➌
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: y <— vectorized_sequences[, 2:NA]➌
- en: list(x, y)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: list(x, y)
- en: '}'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: lm_dataset <— dataset %>%
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: lm_dataset <— dataset %>%
- en: dataset_map(prepare_lm_dataset, num_parallel_calls = 4)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(prepare_lm_dataset, num_parallel_calls = 4)
- en: ➊ **Convert a batch of texts (strings) to a batch of integer sequences.**
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将一批文本（字符串）转换为一批整数序列。**
- en: ➋ **Create inputs by cutting off the last word of the sequences (drop last column).**
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **通过截取序列的最后一个单词来创建输入（删除最后一列）。**
- en: ➌ **Create targets by offsetting the sequences by 1 (drop first column).**
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **通过将序列偏移1来创建目标（删除第一列）。**
- en: A TRANSFORMER-BASED SEQUENCE-TO-SEQUENCE MODEL
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于TRANSFORMER的序列到序列模型
- en: We’ll train a model to predict a probability distribution over the next word
    in a sentence, given a number of initial words. When the model is trained, we’ll
    feed it with a prompt, sample the next word, add that word back to the prompt,
    and repeat, until we’ve generated a short paragraph.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个模型，来预测句子中下一个单词的概率分布，给定一些初始单词。当模型训练完成后，我们将向其提供一个提示，采样下一个单词，将该单词添加回提示中，并重复此过程，直到生成一个短段落。
- en: Like we did for temperature forecasting in chapter 10, we could train a model
    that takes as input a sequence of *N* words and simply predicts word *N* +1\.
    However, several issues exist with this setup in the context of sequence generation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在第 10 章中对温度预测所做的那样，我们可以训练一个模型，该模型以*N*个词的序列作为输入，简单地预测第*N*+1个词。然而，在序列生成的上下文中，这种设置存在几个问题。
- en: First, the model would learn to produce predictions only when *N* words were
    available, but it would be useful to be able to start predicting with fewer than
    *N* words. Otherwise, we’d be constrained to using only relatively long prompts
    (in our implementation, *N* = 100 words). We didn’t have this need in chapter
    10.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，模型只有在可用*N*个词时才能学会产生预测，但有时候只用少于*N*个词来开始预测是有用的。否则，我们将被限制为仅使用相对较长的提示（在我们的实现中，*N*
    = 100 个词）。我们在第 10 章中并不需要这样做。
- en: 'Second, many of our training sequences will be mostly overlapping. Consider
    N = 4. The text “A complete sentence must have, at minimum, three things: a subject,
    verb, and an object” would be used to generate the following training sequences:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，我们的训练序列中许多是大部分重叠的。考虑 N = 4。文本“A complete sentence must have, at minimum,
    three things: a subject, verb, and an object”将用于生成以下训练序列：'
- en: “A complete sentence must”
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “完整的句子必须”
- en: “complete sentence must have”
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “完整的句子必须有”
- en: “sentence must have at”
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “句子必须具有”
- en: “and so on, until “verb and an object”
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “等等，直到“动词和一个宾语”
- en: A model that treats each such sequence as an independent sample would have to
    do a lot of redundant work, re-encoding multiple times subsequences that it has
    largely seen before. In chapter 10, this wasn’t much of a problem, because we
    didn’t have that many training samples in the first place, and we needed to benchmark
    dense and convolutional models, for which redoing the work every time is the only
    option. We could try to alleviate this redundancy problem by using *strides* to
    sample our sequences— skipping a few words between two consecutive samples. But
    that would reduce our number of training samples while providing only a partial
    solution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个这样的序列视为独立样本的模型将不得不进行大量冗余工作，多次重新编码其大部分已经见过的子序列。在第 10 章中，这并不是什么大问题，因为我们一开始就没有那么多训练样本，并且我们需要对密集和卷积模型进行基准测试，每次都重新做工作是唯一的选择。我们可以尝试通过使用*步幅*来对序列进行采样——在两个连续样本之间跳过几个词来减轻这个冗余问题。但这将减少我们的训练样本数量，同时只提供部分解决方案。
- en: 'To address these two issues, we’ll use a *sequence-to-sequence model*: we’ll
    feed sequences of *N* words (indexed from *1* to *N*) into our model, and we’ll
    predict the sequence offset by one (from *2* to *N+1*). We’ll use causal masking
    to make sure that, for any *i*, the model will use only words from *1* to *i*
    to predict the word *i + 1*. This means that we’re simultaneously training the
    model to solve *N* mostly overlapping but different problems: predicting the next
    words given a sequence of 1 <= i <= N prior words (see [figure 12.3](#fig12-3)).
    At generation time, even if you prompt the model with only a single word, it will
    be able to give you a probability distribution for the next possible words.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这两个问题，我们将使用*序列到序列模型*：我们将序列*N*个词（从*1*到*N*索引）馈送到我们的模型中，并预测序列偏移一个（从*2*到*N+1*）。我们将使用因果屏蔽来确保，对于任何*i*，模型将只使用从*1*到*i*的词来预测第*i+1*个词。这意味着我们同时训练模型解决*N*个大部分重叠但不同的问题：在给定了1
    <= i <= N个先前词的序列的情况下预测下一个词（参见[图 12.3](#fig12-3)）。在生成时，即使你只用单个词提示模型，它也能给出下一个可能词的概率分布。
- en: '![Image](../images/f0407-01.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0407-01.jpg)'
- en: '**Figure 12.3 Compared to plain next-word prediction, sequence-to-sequence
    modeling simultaneously optimizes for multiple prediction problems.**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.3 与普通的下一个单词预测相比，序列到序列建模同时优化多个预测问题。**'
- en: 'Note that we could have used a similar sequence-to-sequence setup on our temperature-forecasting
    problem in chapter 10: given a sequence of 120 hourly data points, learn to generate
    a sequence of 120 temperatures offset by 24 hours in the future. You’d be solving
    not only the initial problem but also the 119 related problems of forecasting
    temperature in 24 hours, given 1 <= i < 120 prior hourly data points. If you try
    to retrain the RNNs from chapter 10 in a sequence-to-sequence setup, you’ll find
    that you get similar but incrementally worse results, because the constraint of
    solving these additional 119 related problems with the same model interferes slightly
    with the task we actually do care about.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在第10章中的温度预测问题中，我们可以使用类似的序列到序列设置：给定 120 个小时数据点的序列，学习生成一个序列，其中包含未来 24 小时的
    120 个温度数据点。您将不仅解决初始问题，还将解决预测 24 小时内温度的 119 个相关问题，给定 1 <= i < 120 的先前每小时数据点。如果您尝试在序列到序列设置中重新训练第10章中的
    RNN，您会发现您会获得类似但逐渐变差的结果，因为用相同模型解决这些额外的 119 个相关问题的约束会略微干扰我们实际关心的任务。
- en: 'In the previous chapter, you learned about the setup you can use for sequence-to-sequence
    learning in the general case: feed the source sequence into an encoder, and then
    feed both the encoded sequence and the target sequence into a decoder that tries
    to predict the same target sequence, offset by one step. When you’re doing text
    generation, there is no source sequence: you’re just trying to predict the next
    tokens in the target sequence given past tokens, which we can do using only the
    decoder. And thanks to causal padding, the decoder will look at only words *1…N*
    to predict the word *N+1*.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，您了解到了在一般情况下用于序列到序列学习的设置：将源序列输入到编码器中，然后将编码序列和目标序列一起输入到解码器中，解码器试图预测相同的目标序列，偏移一个步骤。当您进行文本生成时，没有源序列：您只是尝试预测目标序列中的下一个令牌，给定过去的令牌，我们可以仅使用解码器来完成。由于有因果填充，解码器将只查看单词
    *1…N* 来预测单词 *N+1*。
- en: 'Let’s implement our model—we’re going to reuse the building blocks we created
    in chapter 11: layer_positional_embedding() and layer_transformer_decoder().'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现我们的模型——我们将重用我们在第11章中创建的构建模块：layer_positional_embedding() 和 layer_transformer_decoder()。
- en: Listing 12.6 A simple Transformer-based language model
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.6 一个简单的基于 Transformer 的语言模型
- en: embed_dim <- 256
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: embed_dim <- 256
- en: latent_dim <- 2048
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: latent_dim <- 2048
- en: num_heads <- 2
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: num_heads <- 2
- en: transformer_decoder <-
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: transformer_decoder <-
- en: layer_transformer_decoder(NULL, embed_dim, latent_dim, num_heads)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: layer_transformer_decoder(NULL, embed_dim, latent_dim, num_heads)
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape(NA), dtype = "int64")
- en: outputs <- inputs %>%
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
- en: transformer_decoder(., .) %>%
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: transformer_decoder(., .) %>%
- en: layer_dense(vocab_size, activation = "softmax")➊
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(vocab_size, activation = "softmax")➊
- en: model <—
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: model <—
- en: keras_model(inputs, outputs) %>%
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: keras_model(inputs, outputs) %>%
- en: compile(loss = "sparse_categorical_crossentropy",
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: compile(loss = "sparse_categorical_crossentropy",
- en: optimizer = "rmsprop")
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop")
- en: ➊ **Softmax over possible vocabulary words, computed for each output sequence
    time step.**
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **对可能的词汇单词进行 softmax 计算，针对每个输出序列时间步。**
- en: 12.1.5 A text-generation callback with variable-temperature sampling
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.5 使用可变温度采样的文本生成回调
- en: 'We’ll use a callback to generate text using a range of different temperatures
    after every epoch. This allows you to see how the generated text evolves as the
    model begins to converge, as well as the impact of temperature in the sampling
    strategy. To seed text generation, we’ll use the prompt “this movie”: all of our
    generated texts will start with this.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个回调函数，在每个 epoch 后使用一系列不同的温度来生成文本。这样可以让您看到随着模型开始收敛，生成的文本如何演变，以及温度在采样策略中的影响。为了种子文本生成，我们将使用提示“这部电影”：我们所有生成的文本都将以此开始。
- en: First, let’s define some functions to generate sentences. Later, we’ll use these
    functions in a callback.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一些函数来生成句子。稍后，我们将在回调中使用这些函数。
- en: vocab <— get_vocabulary(text_vectorization) ➊
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: vocab <— get_vocabulary(text_vectorization) ➊
- en: sample_next <— function(predictions, temperature = 1.0) {
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: sample_next <— function(predictions, temperature = 1.0) {
- en: predictions %>%
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: predictions %>%
- en: reweight_distribution(temperature) %>% ➋
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: reweight_distribution(temperature) %>% ➋
- en: sample.int(length(.), 1, prob = .)➌
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: sample.int(length(.), 1, prob = .)➌
- en: '}'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: generate_sentence <—
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: generate_sentence <—
- en: function(model, prompt, generate_length, temperature) {
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: function(model, prompt, generate_length, temperature) {
- en: sentence <— prompt➍
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: sentence <— prompt➍
- en: for (i in seq(generate_length)) {➎
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(generate_length)) {➎
- en: model_preds <— sentence %>%
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: model_preds <— sentence %>%
- en: array(dim = c(1, 1)) %>%
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: array(dim = c(1, 1)) %>%
- en: text_vectorization() %>%
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization() %>%
- en: predict(model, .) ➏
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: predict(model, .) ➏
- en: sampled_word <— model_preds %>%
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_word <— model_preds %>%
- en: .[1, i, ] %>%➐
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: .[1, i, ] %>%➐
- en: sample_next(temperature) %>%➑
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: sample_next(temperature) %>%➑
- en: vocab[.]➒
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: vocab[.]➒
- en: sentence <— paste(sentence, sampled_word)➓
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: sentence <— paste(sentence, sampled_word)➓
- en: '}'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sentence
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: sentence
- en: '}'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **The vector we will use to convert word indices (integers) back to strings,
    to be used for text decoding**
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们将使用它来将单词索引（整数）转换回字符串，用于文本解码**
- en: ➋ **The temperature to use for sampling**
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **用于采样的温度**
- en: ➌ **Implement variable-temperature sampling from a probability distribution.**
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **实现从概率分布中进行可变温度抽样。**
- en: ➍ **Prompt that we use to seed text generation**
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **用于初始化文本生成的提示**
- en: ➎ **Iterate for how many words to generate.**
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **迭代生成多少个单词。**
- en: ➏ **Feed the current sequence into our model.**
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **将当前序列输入到我们的模型中。**
- en: ➐ **Retrieve the predictions for the last time step…**
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **检索最后一个时间步的预测结果……**
- en: ➑ **…and use them to sample a new token…**
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **……并用它们来采样一个新的标记……**
- en: ➒ **…and convert the token integer to a string.**
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **……并将标记整数转换为字符串。**
- en: ➓ **Append the new word to the current sequence and repeat.**
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ➓ **将新单词添加到当前序列并重复。**
- en: sample_next() and generate_sentence() do the work of generating sentences from
    a model. They work eagerly; they call predict() to produce predictions as R arrays,
    call sample.int() to pick the next token, and build up the sentence as an R string
    with paste().
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: sample_next()和generate_sentence()负责从模型生成句子的工作。它们会急切地工作；它们会调用predict()来生成R数组的预测结果，调用sample.int()来选择下一个标记，并使用paste()来构建句子作为R字符串。
- en: 'Because we may want to generate many sentences, it makes sense to optimize
    it a little. We can speed up generate_sentence considerably (~25x) by rewriting
    it as a tf_function(). To do this, we just need to replace a few R functions with
    TensorFlow equivalents. Instead of for(i in seq()), we can write for(i in tf$range()).
    We can also substitute sample.int() with tf$random$categorical(), paste() with
    tf$strings$join(), and predict(model, .) with model(.). Here is what sample_ next()
    and generate_sentence() look like as tf_function()s:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们可能想要生成许多句子，所以对此进行了一些优化是有意义的。我们可以通过将generate_sentence重写为tf_function()来大幅提高其速度（约25倍）。为此，我们只需要使用TensorFlow相应的函数替换一些R函数。我们可以将for(i
    in seq())替换为for(i in tf$range())。我们也可以用tf$random$categorical()替换sample.int()，用tf$strings$join()代替paste()，用model(.)代替predict(model,
    .)。sample_next()和generate_sentence()在tf_function()中的样子如下：
- en: tf_sample_next <— function(predictions, temperature = 1.0) {
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: tf_sample_next <— function(predictions, temperature = 1.0) {
- en: predictions %>%
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: predictions %>%
- en: reweight_distribution(temperature) %>%
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: reweight_distribution(temperature) %>%
- en: '{ log(.[tf$newaxis, ]) } %>% ➊'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '{ log(.[tf$newaxis, ]) } %>% ➊'
- en: tf$random$categorical(1L) %>%
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: tf$random$categorical(1L) %>%
- en: tf$reshape(shape())➋
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: tf$reshape(shape())➋
- en: '}'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: library(tfautograph)➌
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfautograph)➌
- en: tf_generate_sentence <— tf_function(
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: tf_generate_sentence <— tf_function(
- en: function(model, prompt, generate_length, temperature) {
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: function(model, prompt, generate_length, temperature) {
- en: withr::local_options(tensorflow.extract.style = "python")
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: withr::local_options(tensorflow.extract.style = "python")
- en: vocab <— as_tensor(vocab)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: vocab <— as_tensor(vocab)
- en: sentence <— prompt %>% as_tensor(shape = c(1, 1))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: sentence <— prompt %>% as_tensor(shape = c(1, 1))
- en: ag_loop_vars(sentence)➍
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ag_loop_vars(sentence)➍
- en: for (i in tf$range(generate_length)) {
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in tf$range(generate_length)) {
- en: model_preds <— sentence %>%
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: model_preds <— sentence %>%
- en: text_vectorization() %>%
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization() %>%
- en: model()
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: model()
- en: sampled_word <— model_preds %>%
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_word <— model_preds %>%
- en: .[0, i, ] %>%
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: .[0, i, ] %>%
- en: tf_sample_next(temperature) %>%
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: tf_sample_next(temperature) %>%
- en: vocab[.]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: vocab[.]
- en: sentence <— sampled_word %>%
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: sentence <— sampled_word %>%
- en: '{ tf$strings$join(c(sentence, .), " ") }'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '{ tf$strings$join(c(sentence, .), " ") }'
- en: '}'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sentence %>% tf$reshape(shape())➎
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: sentence %>% tf$reshape(shape())➎
- en: '}'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **tf$random$catagorical() expects a batch of log probabilities.**
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **tf$random$catagorical()期望一个批量的对数概率。**
- en: ➋ **tf$random$catagorical() returns a scalar integer with shape (1, 1). Reshape
    to shape ().**
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **tf$random$catagorical()返回形状为(1, 1)的标量整数。重新调整为形状().**
- en: ➌ **For ag_loop_vars() (more on this soon)**
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **对ag_loop_vars()（稍后详细介绍）**
- en: ➍ **Provide a hint to the compiler that `sentence` is the only variable we want
    after iteration.**
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **向编译器提供提示，说明`sentence`是我们在迭代后想要的唯一变量。**
- en: ➎ **Reshape from (1, 1) to (). Note that tf$strings$join() preserves sentence's
    (1, 1) shape throughout iteration.**
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **从(1, 1)形状重塑为()。注意tf$strings$join()在整个迭代过程中保持sentence的(1, 1)形状。**
- en: On my machine, generating a sentence of 50 words takes approx 2.5 seconds with
    the eager generate_sentence(), and .1 seconds with tf_generate_sentence(), a 25×
    improvement! Remember, it always makes sense to prototype your code first by running
    it eagerly, and only move to using tf_function() once you have it working how
    you want.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的机器上，使用急切生成 generate_sentence() 生成 50 个词的句子大约需要 2.5 秒，而使用 tf_generate_sentence()
    只需要 0.1 秒，提高了 25 倍！记住，先通过急切运行原型代码进行原型设计，只有在达到想要的效果后才使用 tf_function()。
- en: '**for loops and autograph**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**for 循环和 autograph**'
- en: 'One wrinkle with evaluating R functions eagerly before wrapping them with tf_
    function(fn, autograph = TRUE) (the default) is that autograph = TRUE gives capabilities
    that base R doesn’t have, like the ability for for to iterate over tensors. You
    can still evaluate expressions like for(i in tf$range()) or for(batch in tf_ dataset)
    eagerly by calling tfautograph::autograph() directly, like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 tf_ 函数(fn, autograph = TRUE)（默认设置）包装之前急切地评估 R 函数时，一个注意事项是 autograph = TRUE
    提供了基本 R 没有的功能，比如让 for 能够迭代张量。你仍然可以通过直接调用 tfautograph::autograph() 来急切地评估诸如 for(i
    in tf$range()) 或 for(batch in tf_ dataset) 这样的表达式，例如：
- en: library(tfautograph)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfautograph)
- en: autograph({
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: autograph({
- en: for(i in tf$range(3L))
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in tf$range(3L))
- en: print(i)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: print(i)
- en: '})'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: tf.Tensor(0, shape=(), dtype=int32)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(0, shape=(), dtype=int32)
- en: tf.Tensor(1, shape=(), dtype=int32)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(1, shape=(), dtype=int32)
- en: tf.Tensor(2, shape=(), dtype=int32)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(2, shape=(), dtype=int32)
- en: or
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: fn <— function(x) {
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: fn <— function(x) {
- en: for(i in x) print(i)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in x) print(i)
- en: '}'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ag_fn <— autograph(fn)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ag_fn <— autograph(fn)
- en: ag_fn(tf$range(3))
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ag_fn(tf$range(3))
- en: tf.Tensor(0.0, shape=(), dtype=float32)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(0.0, shape=(), dtype=float32)
- en: tf.Tensor(1.0, shape=(), dtype=float32)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(1.0, shape=(), dtype=float32)
- en: tf.Tensor(2.0, shape=(), dtype=float32)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(2.0, shape=(), dtype=float32)
- en: In interactive sessions you can temporarily globally enable if, while, and for
    to accept tensors by calling tfautograph:::attach_ag_mask().
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在交互式会话中，你可以通过调用 tfautograph:::attach_ag_mask() 临时全局启用 if、while 和 for 来接受张量。
- en: A for() loop that iterates over a tensor in a tf_function() builds a tf$while_loop(),
    and it inherits all same restrictions. Every tensor tracked by the loop must have
    a stable shape and dtype throughout iteration.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在 tf_function() 中迭代张量的 for() 循环构建了一个 tf$while_loop()，并且继承了所有相同的限制。循环跟踪的每个张量在整个迭代过程中必须具有稳定的形状和数据类型。
- en: The call ag_loop_vars(sentence) gives the tf_function() compiler a hint that
    the only variable we’re interested in after the for loop is sentence. This informs
    the compiler that other tensors, like sampled_word, i, and model_preds, are loop-local
    variables and can be safely optimized away after the loop.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 ag_loop_vars(sentence) 给 tf_function() 编译器一个提示，即我们在 for 循环之后感兴趣的唯一变量是 sentence。这通知编译器其他张量，如
    sampled_word、i 和 model_preds，都是循环局部变量，并且可以在循环后安全地优化掉。
- en: Note that iterating over a regular R object like for(i in seq(0, 49)) in a tf_
    function() would not build a tf$while_loop(), but would instead evaluate with
    regular R semantics and would result in the tf_function() tracing an unrolled
    loop (which is sometimes preferable, for short loops with a fixed number of iterations).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 tf_ 函数() 中迭代常规 R 对象（例如 for(i in seq(0, 49))）不会构建 tf$while_loop()，而是使用常规的
    R 语义进行评估，并且会导致 tf_function() 追踪展开的循环（有时这是可取的，对于迭代次数固定的短循环）。
- en: 'Here is the callback where we’ll call tf_generate_sentence() to generate text
    during training:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将在回调中调用 tf_generate_sentence() 来在训练期间生成文本的地方：
- en: Listing 12.7 The text-generation callback
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.7 文本生成回调
- en: callback_text_generator <— new_callback_class(
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: callback_text_generator <— new_callback_class(
- en: classname = "TextGenerator",
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "TextGenerator",
- en: initialize = function(prompt, generate_length,
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(prompt, generate_length,
- en: temperatures = 1,
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: temperatures = 1,
- en: print_freq = 1L) {
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: print_freq = 1L) {
- en: private$prompt <— as_tensor(prompt, "string")
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: private$prompt <— as_tensor(prompt, "string")
- en: private$generate_length <— as_tensor(generate_length, "int32")
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: private$generate_length <— as_tensor(generate_length, "int32")
- en: private$temperatures <— as.numeric(temperatures)➊
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: private$temperatures <— as.numeric(temperatures)➊
- en: private$print_freq <— as.integer(print_freq)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: private$print_freq <— as.integer(print_freq)
- en: '},'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: on_epoch_end = function(epoch, logs = NULL) {
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: on_epoch_end = function(epoch, logs = NULL) {
- en: if ((epoch %% private$print_freq) != 0)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: if ((epoch %% private$print_freq) != 0)
- en: return()
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: return()
- en: for (temperature in private$temperatures) {➋
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: for (temperature in private$temperatures) {➋
- en: cat("== Generating with temperature", temperature, "\n")
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: cat("== 使用温度生成", temperature, "\n")
- en: sentence <— tf_generate_sentence(➌
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: sentence <— tf_generate_sentence(➌
- en: self$model,
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: self$model,
- en: private$prompt,
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: private$prompt,
- en: private$generate_length,➍
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: private$generate_length,➍
- en: as_tensor(temperature, "float32")
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(temperature, "float32")
- en: )
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: cat(as.character(sentence), "\n")
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: cat(as.character(sentence), "\n")
- en: '}'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: text_gen_callback <— callback_text_generator(
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: text_gen_callback <— callback_text_generator(
- en: prompt = "This movie",
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: prompt = "这部电影",
- en: generate_length = 50,
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: generate_length = 50,
- en: temperatures = c(0.2, 0.5, 0.7, 1., 1.5) ➎
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: temperatures = c(0.2, 0.5, 0.7, 1., 1.5) ➎
- en: )
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **We'll use a diverse range of temperatures to sample text, to demonstrate
    the effect of temperature on text generation.**
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们将使用各种不同温度来对文本进行抽样，以演示温度对文本生成的影响。**
- en: ➋ **This is a regular R for loop iterating eagerly over an R vector.**
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **这是一个常规的 R 循环，急切地迭代 R 向量。**
- en: ➌ **Note we call this function with only tensors and the model, not R numeric
    or character vectors.**
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **请注意，我们仅使用张量和模型调用此函数，而不是 R 数值或字符向量。**
- en: ➍ **These were already cast to Tensors in initialize().**
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **这些在 initialize() 中已经被转换为张量了。**
- en: ➎ **The set of temperatures we generate text with**
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **我们生成文本的温度集合**
- en: Let’s fit() this thing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合() 这个东西。
- en: '**Listing 12.8 Fitting the language model**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 12.8 拟合语言模型**'
- en: model %>%
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: fit(lm_dataset,
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: fit(lm_dataset,
- en: epochs = 200,
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 200,
- en: callbacks = list(text_gen_callback))
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = list(text_gen_callback))
- en: 'Here are some cherry-picked examples of what we’re able to generate after 200
    epochs of training. Note that punctuation isn’t part of our vocabulary, so none
    of our generated text has any punctuation:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些经过精心挑选的例子，展示了我们在进行了 200 个 epochs 的训练后能够生成的内容。请注意，标点符号不是我们词汇表的一部分，因此我们生成的文本中没有任何标点符号：
- en: With temperature=0.2
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度=0.2
- en: “this movie is a [UNK] of the original movie and the first half hour of the
    movie is pretty good but it is a very good movie it is a good movie for the time
    period”
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影是原电影的[UNK]，前半个小时的电影相当不错，但它是一部非常好的电影，对于那个时期来说是一部好电影”
- en: “this movie is a [UNK] of the movie it is a movie that is so bad that it is
    a [UNK] movie it is a movie that is so bad that it makes you laugh and cry at
    the same time it is not a movie i dont think ive ever seen”
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“这部电影是电影的[UNK]，它是一部非常糟糕的电影，它是一部非常糟糕的电影，它使你笑和哭同时进行，这不是一部电影，我不认为我曾经看过” '
- en: With temperature=0.5
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度=0.5
- en: “this movie is a [UNK] of the best genre movies of all time and it is not a
    good movie it is the only good thing about this movie i have seen it for the first
    time and i still remember it being a [UNK] movie i saw a lot of years”
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影是有史以来最好的流派电影的[UNK]，它不是一部好电影，它是关于这部电影的唯一好事，我第一次看到它，我仍然记得它是一部[UNK]电影，我看了很多年”
- en: “this movie is a waste of time and money i have to say that this movie was a
    complete waste of time i was surprised to see that the movie was made up of a
    good movie and the movie was not very good but it was a waste of time and”
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影是浪费时间和金钱的，我不得不说这部电影完全是浪费时间的，我很惊讶地发现这部电影由一部好电影组成，而且这部电影并不是很好，但它是浪费时间的”
- en: With temperature=0.7
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度=0.7
- en: “this movie is fun to watch and it is really funny to watch all the characters
    are extremely hilarious also the cat is a bit like a [UNK] [UNK] and a hat [UNK]
    the rules of the movie can be told in another scene saves it from being in the
    back of”
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影很有趣，看起来真的很有趣，所有的角色都非常滑稽，而且猫也有点像一个[UNK][UNK]和一顶帽子[UNK]电影的规则可以在另一个场景中被告知，这使得它不会被放在后面”
- en: “this movie is about [UNK] and a couple of young people up on a small boat in
    the middle of nowhere one might find themselves being exposed to a [UNK] dentist
    they are killed by [UNK] i was a huge fan of the book and i havent seen the original
    so it”
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影是关于[UNK]和一对年轻人在无人之境的小船上的，一个人可能会发现自己暴露于[UNK]的牙医，他们被[UNK]杀死了，我是这本书的超级粉丝，我还没看过原版，所以”
- en: With temperature=1.0
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度=1.0
- en: “this movie was entertaining i felt the plot line was loud and touching but
    on a whole watch a stark contrast to the artistic of the original we watched the
    original version of england however whereas arc was a bit of a little too ordinary
    the [UNK] were the present parent [UNK]”
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影很有趣，看起来很有趣，所有角色都非常滑稽，而且猫也有点像一个[UNK][UNK]和一顶帽子[UNK]电影的规则可以在另一个场景中被告知，这使得它不会被放在后面”
- en: “this movie was a masterpiece away from the storyline but this movie was simply
    exciting and frustrating it really entertains friends like this the actors in
    this movie try to go straight from the sub thats image and they make it a really
    good tv show”
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影是一个杰作，远离了故事情节，但这部电影简直是令人兴奋和沮丧的，它真的很让朋友们开心，像这样的演员们试图直接从地下走向地下，他们把它变成了一个真正的好电视节目”
- en: With temperature=1.5
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度=1.5
- en: “this movie was possibly the worst film about that 80 women its as weird insightful
    actors like barker movies but in great buddies yes no decorated shield even [UNK]
    land dinosaur ralph ian was must make a play happened falls after miscast [UNK]
    bach not really not wrestlemania seriously sam didnt exist”
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影可能是关于那80个女人中最糟糕的一部电影，它就像是个古怪而有洞察力的演员，例如巴克尔电影，但是在大伙伴圈子里是个伟大的家伙。是的，不装饰有盾，甚至还有[UNK]的土地公园恐龙拉尔夫伊恩必须演一出戏发生故事之后被选派（混合[UNK]巴赫）实际上并不存在。”
- en: “this movie could be so unbelievably lucas himself bringing our country wildly
    funny things has is for the garish serious and strong performances colin writing
    more detailed dominated but before and that images gears burning the plate patriotism
    we you expected dyan bosses devotion to must do your own duty and another”
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这部电影可能是卢卡斯本人为我们国家带来的令人难以置信的有趣事情，而这些事情既夸张又严肃，演员们的表演精彩激烈，柯林的写作更加详细，但在这之前，还有那些燃烧的爱国主义画面，我们预期到你对职责的忠诚以及另一件事情的做法。”
- en: As you can see, a low temperature value results in very boring and repetitive
    text and can sometimes cause the generation process to get stuck in a loop. With
    higher temperatures, the generated text becomes more interesting, surprising,
    and even creative. With a very high temperature, the local structure starts to
    break down, and the output looks largely random. Here, a good generation temperature
    would seem to be about 0.7\. Always experiment with multiple sampling strategies!
    A clever balance between learned structure and randomness is what makes generation
    interesting.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，低温度值会导致非常乏味和重复的文本，并且有时会导致生成过程陷入循环。随着温度的升高，生成的文本变得更有趣，更令人惊讶，甚至更有创意。当温度变得非常高时，局部结构开始崩溃，输出看起来基本上是随机的。在这里，一个好的生成温度似乎是0.7左右。始终使用多个采样策略进行实验！通过学习结构和随机性之间的巧妙平衡，才能使生成变得有趣。
- en: 'Note that by training a bigger model, longer, on more data, you can achieve
    generated samples that look far more coherent and realistic than this one—the
    output of a model like GPT-3 is a good example of what can be done with language
    models (GPT-3 is effectively the same thing as what we trained in this example,
    but with a deep stack of Transformer decoders, and a much bigger training corpus).
    But don’t expect to ever generate any meaningful text, other than through random
    chance and the magic of your own interpretation: all you’re doing is sampling
    data from a statistical model of which words come after which words. Language
    models are all form and no substance.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过训练更大的模型，使用更多的数据，您可以获得生成样本，其凝聚度和真实性要比本例中的样本高得多——像GPT-3这样的模型输出是语言模型可以实现的例证（GPT-3实际上与我们在此示例中训练的模型相同，但具有深层的Transformer解码器堆栈和更大的训练语料库）。但是，除非通过随机偶然和您自己的解释魔力，否则不要期望能够生成任何有意义的文本：您所做的只是从统计模型中采样出来的数据，其中包含哪些词汇跟随哪些词汇的信息。语言模型只是形式而无实质。
- en: 'Natural language is many things: a communication channel; a way to act on the
    world; a social lubricant; a way to formulate, store, and retrieve your own thoughts.
    These uses of languages are where its meaning originates. A deep learning “language
    model,” despite its name, captures effectively none of these fundamental aspects
    of language. It cannot communicate (it has nothing to communicate about and no
    one to communicate with), it cannot act on the world (it has no agency and no
    intent), it cannot be social, and it doesn’t have any thoughts to process with
    the help of words. Language is the operating system of the mind, and so, for language
    to be meaningful, it needs a mind to leverage it.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言是很多事物：一种沟通渠道；一种在世界上行动的方式；一种社交润滑剂；一种构思、存储和提取自己思想的方式。这些语言使用是它的意义所来源之处。深度学习的“语言模型”，尽管它的名称如此，实际上捕捉到的是语言的这些基本方面。它不能进行交流（因为它没有任何东西可以交流的，也没有人可以交流），它不能在世界上发挥作用（因为它没有能力和目的），它不会社交，并且它没有任何需要通过单词进行处理的思想。语言是心灵的操作系统，所以，为了语言具有意义，它需要一个心灵来利用它。
- en: 'What a language model does is capture the statistical structure of the observable
    artifacts—books, online movie reviews, tweets—that we generate as we use language
    to live our lives. The fact that these artifacts have a statistical structure
    at all is a side effect of how humans implement language. Here’s a thought experiment:
    what if our languages did a better job of compressing communications, much like
    computers do with most digital communications? Language would be no less meaningful
    and could still fulfill its many purposes, but it would lack any intrinsic statistical
    structure, thus making it impossible to model as you just did.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的作用是捕获可观察到的工件的统计结构——书籍、在线电影评论、推文——我们在使用语言生活时生成的工件。这些工件具有统计结构的事实是人类实现语言的副作用。想象一下：如果我们的语言能更好地压缩通信，就像计算机对大多数数字通信所做的那样，会怎么样？语言不会失去任何意义，仍然可以完成其许多目的，但它将缺乏任何固有的统计结构，因此无法像你刚刚所做的那样进行建模。
- en: 12.1.6 Wrapping up
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.6 总结
- en: You can generate discrete sequence data by training a model to predict the next
    token(s), given previous tokens.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过训练模型预测给定前一个令牌的下一个令牌(s)，可以生成离散序列数据。
- en: In the case of text, such a model is called a *language model*. It can be based
    on either words or characters.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本的情况下，这样的模型称为*语言模型*。它可以基于单词或字符。
- en: Sampling the next token requires a balance between adhering to what the model
    judges likely, and introducing randomness.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对下一个令牌进行采样需要在遵循模型判断可能性和引入随机性之间保持平衡。
- en: One way to handle this is the notion of softmax temperature. Always experiment
    with different temperatures to find the right one.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理这种情况的一种方法是 softmax 温度的概念。始终尝试不同的温度以找到合适的温度。
- en: 12.2 DeepDream
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 DeepDream
- en: '*DeepDream* is an artistic image-modification technique that uses the representations
    learned by convolutional neural networks. It was first released by Google in the
    summer of 2015 as an implementation written using the Caffe deep learning library
    (this was several months before the first public release of TensorFlow).^([3](#Rendnote3))
    It quickly became an internet sensation thanks to the trippy pictures it could
    generate (see, for example, [figure 12.4](#fig12-4)), full of algorithmic pareidolia
    artifacts, bird feathers, and dog eyes—a byproduct of the fact that the DeepDream
    convnet was trained on ImageNet, where dog breeds and bird species are vastly
    overrepresented.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*DeepDream* 是一种艺术图像修改技术，它使用了卷积神经网络学习到的表示。它最初由谷歌在2015年夏季发布，使用了 Caffe 深度学习库来实现（这是在
    TensorFlow 的首次公开发布几个月前）。^([3](#Rendnote3)) 它很快就因为能够生成迷幻图片而成为互联网的轰动，这些图片充满了算法幻觉的工件、鸟类羽毛和狗眼——这是因为
    DeepDream convnet 是在 ImageNet 上训练的，那里狗品种和鸟类物种的数量远远超过其他物种。'
- en: '![Image](../images/f0414-01.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0414-01.jpg)'
- en: '**Figure 12.4 Example of a DeepDream output image**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.4 DeepDream 输出图像示例**'
- en: 'The DeepDream algorithm is almost identical to the convnet filter-visualization
    technique introduced in chapter 9, consisting of running a convnet in reverse:
    doing gradient ascent on the input to the convnet to maximize the activation of
    a specific filter in an upper layer of the convnet. DeepDream uses this same idea,
    with a few simple differences:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: DeepDream 算法与第 9 章介绍的卷积神经网络滤波器可视化技术几乎相同，包括对卷积神经网络的输入进行反向运行：对卷积神经网络中的特定滤波器的激活进行梯度上升。DeepDream
    使用了相同的思想，只是有一些简单的区别：
- en: With DeepDream, you try to maximize the activation of entire layers rather than
    that of a specific filter, thus mixing together visualizations of large numbers
    of features at once.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 DeepDream，你试图最大化整个层的激活，而不是特定滤波器的激活，因此同时混合了大量特征的可视化。
- en: You start not from blank, slightly noisy input, but rather from an existing
    image—thus the resulting effects latch on to preexisting visual patterns, distorting
    elements of the image in a somewhat artistic fashion.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是从空白的、略带噪音的输入开始，而是从一个现有的图像开始，因此产生的效果会附着在预先存在的视觉模式上，以某种艺术风格扭曲图像的元素。
- en: The input images are processed at different scales (called *octaves*), which
    improves the quality of the visualizations.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像在不同尺度（称为*octaves*）上处理，这提高了可视化的质量。
- en: Let’s make some DeepDreams.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们制作一些 DeepDreams。
- en: 12.2.1 Implementing DeepDream in Keras
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 在 Keras 中实现 DeepDream
- en: Let’s start by retrieving a test image to dream with. We’ll use a view of the
    rugged Northern California coast in the winter ([figure 12.5](#fig12-5)).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从获取一张用于梦想的测试图像开始。我们将使用冬季时节的加利福尼亚北部崎岖的海岸景色（[图 12.5](#fig12-5)）。
- en: Listing 12.9 Fetching the test image
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 12.9 获取测试图像
- en: base_image_path <— get_file(
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: base_image_path <— get_file(
- en: '"coast.jpg", origin = "https://img-datasets.s3.amazonaws.com/coast.jpg")'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '"coast.jpg", origin = "https://img-datasets.s3.amazonaws.com/coast.jpg")'
- en: plot(as.raster(jpeg::readJPEG(base_image_path)))
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: plot(as.raster(jpeg::readJPEG(base_image_path)))
- en: '![Image](../images/f0415-01.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0415-01.jpg)'
- en: '**Figure 12.15 Our test image**'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.15 我们的测试图像**'
- en: Next, we need a pretrained convnet. In Keras, many such convnets are available—
    VGG16, VGG19, Xception, ResNet50, and so on—all with weights pretrained on ImageNet.
    You can implement DeepDream with any of them, but your base model of choice will
    naturally affect your visualizations, because different architectures result in
    different learned features. The convnet used in the original DeepDream release
    was an Inception model, and in practice, Inception is known to produce nice-looking
    DeepDreams, so we’ll use the Inception V3 model that comes with Keras.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个预训练的卷积网络。在 Keras 中，有许多这样的卷积网络可用 — VGG16、VGG19、Xception、ResNet50 等等
    — 它们的权重都是在 ImageNet 上预训练的。你可以用任何一个实现 DeepDream，但你选择的基础模型自然会影响你的可视化效果，因为不同的架构会导致不同的学习特征。原始
    DeepDream 发布中使用的卷积网络是一个 Inception 模型，在实践中，Inception 被认为能产生外观良好的 DeepDream，所以我们将使用
    Keras 提供的 Inception V3 模型。
- en: '**Listing 12.10 Instantiating a pretrained InceptionV3 model**'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 12.10 实例化预训练的 InceptionV3 模型**'
- en: model <— application_inception_v3(weights = "imagenet", include_top = FALSE)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: model <— application_inception_v3(weights = "imagenet", include_top = FALSE)
- en: We’ll use our pretrained convnet to create a feature extractor model that returns
    the activations of the various intermediate layers, listed in the following code.
    For each layer, we pick a scalar score that weights the contribution of the layer
    to the loss we will seek to maximize during the gradient-ascent process. If you
    want a complete list of layer names that you can use to pick new layers to play
    with, just use print(model).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的预训练卷积网络创建一个特征提取器模型，该模型返回各个中间层的激活值，如下面的代码所列。对于每一层，我们选择一个标量分数，以加权各层对我们将在梯度上升过程中寻求最大化的损失的贡献。如果你想要一个完整的层名称列表，以便挑选新的层来尝试，只需使用
    print(model)。
- en: Listing 12.11 Configuring the contribution of each layer to the DeepDream loss
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 12.11 配置每层对 DeepDream 损失的贡献
- en: layer_settings <— c( ➊
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: layer_settings <— c( ➊
- en: mixed4 = 1,
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: mixed4 = 1,
- en: mixed5 = 1.5,
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: mixed5 = 1.5,
- en: mixed6 = 2,
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: mixed6 = 2,
- en: mixed7 = 2.5
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: mixed7 = 2.5
- en: )
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: outputs <— list()
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <— list()
- en: for(layer_name in names(layer_settings))
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: for(layer_name in names(layer_settings))
- en: outputs[[layer_name]] <—➋
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: outputs[[layer_name]] <—➋
- en: get_layer(model, layer_name)$output➋
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: get_layer(model, layer_name)$output➋
- en: feature_extractor <— keras_model(inputs = model$inputs,➌
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: feature_extractor <— keras_model(inputs = model$inputs,➌
- en: outputs = outputs)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 = 输出)
- en: ➊ **Layers for which we try to maximize activation, as well as their weight
    in the total loss. You can tweak these setting to obtain new visual effects.**
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们试图最大化激活的层，以及它们在总损失中的权重。你可以调整这些设置以获得新的视觉效果。**
- en: ➋ **Collect in a named list the output symbolic tensor from each layer.**
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在一个命名列表中收集每个层的输出符号张量。**
- en: ➌ **A model that returns the activation values for every target layer (as a
    named list)**
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **一个模型，返回每个目标层的激活值（作为命名列表）**
- en: 'Next, we’ll compute the *loss*: the quantity we’ll seek to maximize during
    the gradient-ascent process at each processing scale. In chapter 9, for filter
    visualization, we tried to maximize the value of a specific filter in a specific
    layer. Here, we’ll simultaneously maximize the activation of all filters in a
    number of layers. Specifically, we’ll maximize a weighted mean of the L2 norm
    of the activations of a set of high-level layers. The exact set of layers we choose
    (as well as their contribution to the final loss) has a major influence on the
    visuals we’ll be able to produce, so we want to make these parameters easily configurable.
    Lower layers result in geometric patterns, whereas higher layers result in visuals
    in which you can recognize some classes from ImageNet (e.g., birds or dogs). We’ll
    start from a somewhat arbitrary configuration involving four layers, but you’ll
    definitely want to explore many different configurations later.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算*损失*：我们将在每个处理尺度的梯度上升过程中寻求最大化的数量。在第 9 章中，对于滤波器可视化，我们试图最大化特定层中特定滤波器的值。在这里，我们将同时最大化一组高级层中所有滤波器的激活。具体来说，我们将最大化一组高级层激活的
    L2 范数的加权平均值。我们选择的确切层集合（以及它们对最终损失的贡献）对我们能够产生的视觉效果有很大影响，因此我们希望使这些参数易于配置。较低的层会产生几何图案，而较高的层会产生可以在
    ImageNet 中识别一些类别的视觉效果（例如，鸟类或狗）。我们将从一个相对任意的配置开始，涉及四个层，但您肯定会希望以后尝试许多不同的配置。
- en: Listing 12.12 The DeepDream loss
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.12 DeepDream 损失
- en: compute_loss <— function(input_image) {
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: compute_loss <— function(input_image) {
- en: features <— feature_extractor(input_image)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: features <— feature_extractor(input_image)
- en: feature_losses <— names(features) %>%
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: feature_losses <— names(features) %>%
- en: lapply(function(name) {
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: lapply(function(name) {
- en: coeff <— layer_settings[[name]]
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: coeff <— layer_settings[[name]]
- en: activation <— features[[name]]➊
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: activation <— features[[name]]➊
- en: coeff * mean(activation[, 3:-3, 3:-3, ] ^ 2)➋
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: coeff * mean(activation[, 3:-3, 3:-3, ] ^ 2)➋
- en: '})'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: Reduce(`+`, feature_losses)➌
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Reduce(`+`, feature_losses)➌
- en: '}'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Extract activations.**
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **提取激活。**
- en: ➋ **We avoid border artifacts by involving only nonborder pixels in the loss.**
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们通过仅涉及非边界像素来避免边界伪影。**
- en: ➌ **feature_losses is a list of scalar tensors. Sum up the loss from each feature.**
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **feature_losses 是一组标量张量。总结每个特征的损失。**
- en: Now let’s set up the gradient-ascent process that we will run at each octave.
    You’ll recognize that it’s the same thing as the filter-visualization technique
    from chapter 9! The DeepDream algorithm is simply a multiscale form of filter
    visualization.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设置梯度上升过程，我们将在每个八度运行。您会发现它与第 9 章中的滤波器可视化技术是相同的！DeepDream 算法只是滤波器可视化的多尺度形式。
- en: '**Listing 12.13 The DeepDream gradient-ascent process**'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 12.13 DeepDream 梯度上升过程**'
- en: gradient_ascent_step <— tf_function(➊
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: gradient_ascent_step <— tf_function(➊
- en: function(image, learning_rate) {
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: function(image, learning_rate) {
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: tape$watch(image)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: tape$watch(image)
- en: loss <— compute_loss(image)➋
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: loss <— compute_loss(image)➋
- en: '})'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: grads <— tape$gradient(loss, image) %>%
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: grads <— tape$gradient(loss, image) %>%
- en: tf$math$l2_normalize()➌
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: tf$math$l2_normalize()➌
- en: image %<>% `+`(learning_rate * grads)➍
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: image %<>% `+`(learning_rate * grads)➍
- en: list(loss, image)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: list(loss, image)
- en: '})'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradient_ascent_loop <—➎
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: gradient_ascent_loop <—➎
- en: function(image, iterations, learning_rate, max_loss = -Inf) {
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: function(image, iterations, learning_rate, max_loss = -Inf) {
- en: learning_rate %<>% as_tensor()
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: learning_rate %<>% as_tensor()
- en: for(i in seq(iterations)) {➏
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in seq(iterations)) {➏
- en: c(loss, image) %<—% gradient_ascent_step(image, learning_rate)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: c(loss, image) %<—% gradient_ascent_step(image, learning_rate)
- en: loss %<>% as.numeric()
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: loss %<>% as.numeric()
- en: if(loss > max_loss)➐
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: if(loss > max_loss)➐
- en: break
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: writeLines(sprintf(
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: writeLines(sprintf(
- en: '"… Loss value at step %i: %.2f", i, loss))'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '"… 在第 %i 步的损失值为 %.2f", i, loss))'
- en: '}'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: image
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: image
- en: '}'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **We make the training step fast by compiling it as a tf_function().**
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们通过将其编译为 tf_function() 来快速进行训练步骤。**
- en: ➋ **Compute gradients of DeepDream loss with respect to the current image.**
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **计算 DeepDream 损失相对于当前图像的梯度。**
- en: ➌ **Normalize gradients (the same trick we used in chapter 9).**
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **归一化梯度（我们在第 9 章中使用的相同技巧）。**
- en: ➍ **Repeatedly update the image in a way that increases the DeepDream loss.**
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **重复更新图像，以增加 DeepDream 损失。**
- en: ➎ **This runs gradient ascent for a given image scale (octave).**
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **这将为给定的图像尺度（八度）运行梯度上升。**
- en: ➏ **This is a regular eager R for loop.**
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **这是一个常规的 eager R 循环。**
- en: ➐ **Break out if the loss crosses a certain threshold (overoptimizing would
    create unwanted image artifacts).**
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **如果损失超过某个阈值，则退出（过度优化会产生不需要的图像伪影）。**
- en: 'Finally, the outer loop of the DeepDream algorithm. First, we’ll define a list
    of *scales* (also called *octaves*) at which to process the images. We’ll process
    our image over three different such octaves. For each successive octave, from
    the smallest to the largest, we’ll run 20 gradient ascent steps via gradient_ascent_loop()
    to maximize the loss we previously defined. Between each octave, we’ll upscale
    the image by 40% (1.4×): we’ll start by processing a small image and then increasingly
    scale it up (see [figure 12.6](#fig12-6)).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，DeepDream 算法的外部循环。首先，我们将定义一个 *尺度* 列表（也称为 *八度音阶*），用于处理图像。我们将在三个不同的八度音阶上处理我们的图像。对于每个连续的八度音阶，从最小到最大，我们将通过
    gradient_ascent_loop() 运行 20 步梯度上升，以最大化我们之前定义的损失。在每个八度音阶之间，我们将通过 40%（1.4×）放大图像：我们将从处理小图像开始，然后逐渐放大它（参见[图
    12.6](#fig12-6)）。
- en: '![Image](../images/f0418-01.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0418-01.jpg)'
- en: '**Figure 12.6 The DeepDream process: Successive scales of spatial processing
    (octaves) and detail reinjection upon upscaling**'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.6 DeepDream 过程：连续尺度的空间处理（八度音阶）和在放大时重新注入细节**'
- en: We define the parameters of this process in the following code. Tweaking these
    parameters will allow you to achieve new effects!
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的代码中定义了这个过程的参数。调整这些参数将使您能够实现新的效果！
- en: step <— 20➊
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 <— 20➊
- en: num_octaves <— 3➋
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 八度音阶数量 <— 3➋
- en: octave_scale <— 1.4➌
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 八度音阶比例 <— 1.4➌
- en: iterations <— 30➍
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数 <— 30➍
- en: max_loss <— 15➎
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 最大损失 <— 15➎
- en: ➊ **Gradient ascent step size**
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **梯度上升步长**
- en: ➋ **Number of scales at which to run gradient ascent**
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在哪些尺度上运行梯度上升的数量**
- en: ➌ **Size ratio between successive scales**
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **连续尺度之间的大小比例**
- en: ➍ **Number of gradient ascent steps per scale**
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **每个尺度的梯度上升步骤数**
- en: ➎ **We’ll stop the gradient-ascent process for a scale if the loss gets higher
    than this.**
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **如果损失超过这个值，我们将停止梯度上升过程的该尺度。**
- en: We’re also going to need a couple of utility functions to load and save images.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要几个实用函数来加载和保存图像。
- en: '**Listing 12.14 Image processing utilities**'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 12.14 图像处理实用程序**'
- en: preprocess_image <— tf_function(function(image_path) {➊
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图片 <— tf_function(function(image_path) {➊
- en: image_path %>%
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图像路径 %>%
- en: tf$io$read_file() %>%
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$read_file() %>%
- en: tf$io$decode_image() %>%
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$decode_image() %>%
- en: tf$expand_dims(axis = 0L) %>%➋
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: tf$expand_dims(axis = 0L) %>%➋
- en: tf$cast("float32") %>%➌
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: tf$cast("float32") %>%➌
- en: inception_v3_preprocess_input()
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: inception_v3_preprocess_input()
- en: '})'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: deprocess_image <— tf_function(function(img) {➍
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 反向处理图片 <— tf_function(function(img) {➍
- en: img %>%
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: img %>%
- en: tf$squeeze(axis = 0L) %>%➎
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: tf$squeeze(axis = 0L) %>%➎
- en: '{ (. * 127.5) + 127.5 } %>%➏'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '{ （。* 127.5）+ 127.5 } %>%➏'
- en: tf$saturate_cast("uint8")➐
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: tf$saturate_cast("uint8")➐
- en: '})'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: display_image_tensor <— function(x, …, max = 255,
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 显示图片张量 <— function(x, …, max = 255,
- en: plot_margins = c(0, 0, 0, 0)) {
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制边距 = c(0, 0, 0, 0)) {
- en: if (!is.null(plot_margins))
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如果（！is.null(plot_margins)）
- en: withr::local_par(mar = plot_margins)➑
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: withr::local_par(mar = plot_margins)➑
- en: x %>%
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: x %>%
- en: as.array() %>%➒
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为数组() %>%➒
- en: drop() %>%
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: drop() %>%
- en: as.raster(max = max) %>%➓
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为栅格(max = max) %>%➓
- en: plot(…, interpolate = FALSE)
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制（…，不插值 = FALSE）
- en: '}'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Util function to load, resize, and format pictures into appropriate arrays**
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **加载、调整大小和格式化图片到适当数组的实用函数**
- en: ➋ **Add batch axis, equivalent to .[tf$newaxis, all_dims()]. axis arg is 0-based.**
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **添加批处理轴，相当于 .[tf$newaxis, all_dims()]。轴参数基于 0。**
- en: ➌ **Cast from 'uint8'.**
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **从 'uint8' 转换。**
- en: ➍ **Util function to convert a tensor array into a valid image and undo preprocessing**
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **将张量数组转换为有效图像并撤消预处理的实用函数**
- en: ➎ **Drop first dim-the batch axis (must be size 1), the inverse of tf$expand_dims().**
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **删除第一个维度-批处理轴（必须为大小 1），tf$expand_dims() 的逆操作。**
- en: ➏ **Rescale so values in [-1, 1] are remapped to [0, 255].**
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **重新缩放，使值在[-1, 1]范围内重新映射到[0, 255]。**
- en: '➐ **saturate_case() clips values to the dtype range: [0, 255].**'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **saturate_case() 将值剪辑到 dtype 范围：[0, 255]。**
- en: ➑ **Default to no margins when plotting images.**
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **在绘制图像时默认没有边距。**
- en: ➒ **Convert tensors to R arrays.**
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **将张量转换为 R 数组。**
- en: ➓ **Convert to R native raster format.**
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ➓ **转换为 R 原生栅格格式。**
- en: '**withr::local_***'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**withr::local_***'
- en: Here we use withr::local_par() to set par() before calling plot(). local_ par()
    acts just like par(), except that it restores the previous par() settings when
    the function exits. Using funcions like local_par() or local_options() helps ensure
    that functions you write don’t permanently modify global state, which makes them
    more predictable and usable in more contexts.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 withr::local_par() 来设置 par()，然后调用 plot()。local_ par() 的作用就像 par()，只是在函数退出时它会恢复先前的
    par() 设置。使用像 local_par() 或 local_options() 这样的函数有助于确保您编写的函数不会永久修改全局状态，这使得它们在更多的上下文中更可预测和可用。
- en: 'You can replace local_par() and do the equivalent with a separate on.exit()
    call like this:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用一个单独的 on.exit() 调用来替换 local_par() 并执行相同的操作，如下所示：
- en: display_image_tensor <— function()
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor <— function()
- en: <…>
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: <…>
- en: opar <— par(mar = plot_margins)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: opar <— par(mar = plot_margins)
- en: on.exit(par(opar))
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: on.exit(par(opar))
- en: <…>
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: <…>
- en: '}'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'This is the outer loop. To avoid losing a lot of image detail after each successive
    scale-up (resulting in increasingly blurry or pixelated images), we can use a
    simple trick: after each scale-up, we’ll reinject the lost details back into the
    image, which is possible because we know what the original image should look like
    at the larger scale. Given a small image size *S* and a larger image size *L*,
    we can compute the difference between the original image resized to size *L* and
    the original resized to size *S*—this difference quantifies the details lost when
    going from *S* to *L*.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这是外循环。为了避免在每次连续放大后丢失大量图像细节（导致图像越来越模糊或像素化），我们可以使用一个简单的技巧：在每次放大后，我们将丢失的细节重新注入到图像中，这是可能的，因为我们知道原始图像在较大比例时应该是什么样子的。给定一个小图像尺寸
    *S* 和一个较大的图像尺寸 *L*，我们可以计算原始图像调整为尺寸 *L* 和尺寸 *S* 时的差异——这个差异量化了从 *S* 到 *L* 过程中丢失的细节。
- en: '**Listing 12.15 Running gradient ascent over multiple successive octaves**'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 12.15 在多个连续的八度上运行梯度上升**'
- en: original_img <— preprocess_image(base_image_path)➊
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: original_img <— preprocess_image(base_image_path)➊
- en: original_HxW <— dim(original_img)[2:3]
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: original_HxW <— dim(original_img)[2:3]
- en: calc_octave_HxW <— function(octave) {
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: calc_octave_HxW <— function(octave) {
- en: as.integer(round(original_HxW / (octave_scale ^ octave)))
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: as.integer(round(original_HxW / (octave_scale ^ octave)))
- en: '}'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: octaves <— seq(num_octaves - 1, 0) %>%➋
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: octaves <— seq(num_octaves - 1, 0) %>%➋
- en: '{ zip_lists(num = .,'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '{ zip_lists(num = .,'
- en: HxW = lapply(., calc_octave_HxW)) }
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: HxW = lapply(., calc_octave_HxW)) }
- en: str(octaves)
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: str(octaves)
- en: List of 3
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: List of 3
- en: $ :List of 2
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: $ :List of 2
- en: '..$ num: int 2'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ num: int 2'
- en: '..$ HxW: int [1:2] 459 612'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ HxW: int [1:2] 459 612'
- en: $ :List of 2
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: $ :List of 2
- en: '..$ num: int 1'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ num: int 1'
- en: '..$ HxW: int [1:2] 643 857'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ HxW: int [1:2] 643 857'
- en: $ :List of 2
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: $ :List of 2
- en: '..$ num: int 0'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ num: int 0'
- en: '..$ HxW: int [1:2] 900 1200'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ HxW: int [1:2] 900 1200'
- en: ➊ **Load and preprocess the test image.**
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **加载并预处理测试图像。**
- en: ➋ **Compute the target shape of the image at different octaves.**
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **计算不同八度图像的目标形状。**
- en: shrunk_original_img <— original_img %>% tf$image$resize(octaves[[1]]$HxW)
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: shrunk_original_img <— original_img %>% tf$image$resize(octaves[[1]]$HxW)
- en: img <— original_img ➊
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: img <— original_img ➊
- en: for (octave in octaves) {➋
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: for (octave in octaves) {➋
- en: cat(sprintf("Processing octave %i with shape (%s)\n",
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("Processing octave %i with shape (%s)\n",
- en: octave$num, paste(octave$HxW, collapse = ", ")))
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: octave$num, paste(octave$HxW, collapse = ", ")))
- en: img <— img %>%
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: img <— img %>%
- en: tf$image$resize(octave$HxW) %>%➌
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: tf$image$resize(octave$HxW) %>%➌
- en: gradient_ascent_loop(iterations = iterations, ➍
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: gradient_ascent_loop(iterations = iterations, ➍
- en: learning_rate = step,
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: learning_rate = step,
- en: max_loss = max_loss)
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: max_loss = max_loss)
- en: upscaled_shrunk_original_img <— ➎
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: upscaled_shrunk_original_img <— ➎
- en: shrunk_original_img %>% tf$image$resize(octave$HxW)
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: shrunk_original_img %>% tf$image$resize(octave$HxW)
- en: same_size_original <—
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: same_size_original <—
- en: original_img %>% tf$image$resize(octave$HxW)➏
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: original_img %>% tf$image$resize(octave$HxW)➏
- en: lost_detail <—➐
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: lost_detail <—➐
- en: same_size_original - upscaled_shrunk_original_img
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: same_size_original - upscaled_shrunk_original_img
- en: img %<>% `+`(lost_detail)➑
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: img %<>% `+`(lost_detail)➑
- en: shrunk_original_img <—
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: shrunk_original_img <—
- en: original_img %>% tf$image$resize(octave$HxW)
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: original_img %>% tf$image$resize(octave$HxW)
- en: '}'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: img <— deprocess_image(img)
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: img <— deprocess_image(img)
- en: img %>% display_image_tensor()
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: img %>% display_image_tensor()
- en: img %>%
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: img %>%
- en: tf$io$encode_png() %>%
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$encode_png() %>%
- en: tf$io$write_file("dream.png", .)➒
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$write_file("dream.png", .)➒
- en: ➊ **Save a reference to the original image (we need to keep the original around).**
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **保存对原始图像的引用（我们需要保留原始图像）。**
- en: ➋ **Iterate over the different octaves.**
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **迭代不同的八度。**
- en: ➌ **Scale up the dream image.**
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将梦想图像放大。**
- en: ➍ **Run gradient ascent, altering the dream.**
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **运行梯度上升，改变梦想。**
- en: '➎ **Scale up the smaller version of the original image: it will be pixelated.**'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **将原始图像的较小版本放大：它会出现像素化。**
- en: ➏ **Compute the high-quality version of the original image at this size.**
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **计算此尺寸下原始图像的高质量版本。**
- en: ➐ **The difference between the two is the detail that was lost when detail up.**
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **两者之间的区别是在放大时丢失的细节。**
- en: ➑ **Reinject the lost detail into the dream.**
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **重新注入丢失的细节到梦想中。**
- en: ➒ **Save the final result.**
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **保存最终结果。**
- en: Because the original Inception V3 network was trained to recognize concepts
    in images of size 299 × 299, and given that the process involves scaling the images
    down by a reasonable factor, the DeepDream implementation produces much better
    results on images that are somewhere between 300 × 300 and 400 × 400\. Regardless,
    you can run the same code on images of any size and any ratio.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 因为原始的 Inception V3 网络是在大小为 299 × 299 的图像上训练的，并且考虑到该过程涉及将图像按合理因子缩小，因此 DeepDream
    实现对大小介于 300 × 300 到 400 × 400 之间的图像产生更好的结果。不管怎样，您都可以在任何大小和比例的图像上运行相同的代码。
- en: On a GPU, it takes only a few seconds to run the whole thing. [Figure 12.7](#fig12-7)
    shows the result of our dream configuration on the test image.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上，整个过程只需几秒钟。[图 12.7](#fig12-7) 展示了我们在测试图像上的梦幻配置的结果。
- en: '![Image](../images/f0421-01.jpg)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0421-01.jpg)'
- en: '**Figure 12.7 Running the DeepDream code on the test image**'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.7 在测试图像上运行 DeepDream 代码**'
- en: I strongly suggest that you explore what you can do by adjusting which layers
    you use in your loss. Layers that are lower in the network contain more-local,
    less-abstract representations and lead to dream patterns that look more geometric.
    Layers that are higher up lead to more-recognizable visual patterns based on the
    most common objects found in ImageNet, such as dog eyes, bird feathers, and so
    on. You can use random generation of the parameters in the layer_settings vector
    to quickly explore many different layer combinations. [Figure 12.8](#fig12-8)
    shows a range of results obtained on an image of a delicious homemade pastry using
    different layer configurations.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您通过调整使用的损失中的哪些层来探索可以做什么。网络中较低的层包含更本地化、不太抽象的表示，并导致看起来更几何化的梦幻图案。位于较高位置的层会导致更具可识别性的视觉模式，基于ImageNet中发现的最常见的对象，例如狗的眼睛、鸟的羽毛等。您可以使用
    layer_settings 向量中参数的随机生成快速探索许多不同的层组合。[图 12.8](#fig12-8) 展示了在使用不同层配置的图像上获得的一系列结果。
- en: 12.2.2 Wrapping up
  id: totrans-494
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 总结
- en: DeepDream consists of running a convnet in reverse to generate inputs based
    on the representations learned by the network.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDream 包括运行卷积网络的逆向过程，以根据网络学到的表示生成输入。
- en: The results produced are fun and somewhat similar to the visual artifacts induced
    in humans by the disruption of the visual cortex via psychedelics.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的结果有趣且在某种程度上类似于通过致幻剂扰乱视觉皮层而在人类中引发的视觉现象。
- en: Note that the process isn’t specific to image models or even to convnets. It
    can be done for speech, music, and more.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，这个过程不限于图像模型，甚至不限于卷积网络。它可以用于语音、音乐等。
- en: '![Image](../images/f0422-01.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0422-01.jpg)'
- en: '**Figure 12.8 Trying a range of DeepDream configurations on an example image**'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.8 在示例图像上尝试一系列 DeepDream 配置**'
- en: 12.3 Neural style transfer
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 神经风格迁移
- en: In addition to DeepDream, another major development in deep-learning-driven
    image modification is *neural style transfer*, introduced by Leon Gatys et al.
    in the summer of 2015.^([4](#Rendnote4)) The neural style transfer algorithm has
    undergone many refinements and spawned many variations since its original introduction,
    and it has made its way into many smartphone photo apps. For simplicity, this
    section focuses on the formulation described in the original paper.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 DeepDream，深度学习驱动的图像修改的另一个重要发展是*神经风格迁移*，由Leon Gatys等人在2015年夏天引入。^([4](#Rendnote4))
    神经风格迁移算法自原始引入以来经历了许多改进，并产生了许多变体，并且已经应用到许多智能手机照片应用中。为简单起见，本节重点介绍了原始论文中描述的公式。
- en: Neural style transfer consists of applying the style of a reference image to
    a target image while conserving the content of the target image. [Figure 12.9](#fig12-9)
    shows an example.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移包括将参考图像的风格应用到目标图像上，同时保留目标图像的内容。[图 12.9](#fig12-9) 展示了一个例子。
- en: '![Image](../images/f0422-02.jpg)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0422-02.jpg)'
- en: '**Figure 12.9 A style transfer example**'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.9 一个风格迁移的例子**'
- en: In this context, *style* essentially means textures, colors, and visual patterns
    in the image, at various spatial scales, and the content is the higher-level macrostructure
    of the image. For instance, blue-and-yellow circular brushstrokes are considered
    to be the style in [figure 12.9](#fig12-9) (using *Starry Night* by Vincent van
    Gogh), and the buildings in the Tübingen photograph are considered to be the content.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个语境中，*风格* 实质上意味着图像中的纹理、颜色和视觉模式，以及不同空间尺度上的内容，而内容则是图像的更高级宏观结构。例如，[图 12.9](#fig12-9)（使用*星夜*的文森特·梵高）中的蓝色和黄色圆形笔触被认为是风格，而图宾根照片中的建筑被认为是内容。
- en: The idea of style transfer, which is tightly related to that of texture generation,
    has had a long history in the image-processing community prior to the development
    of neural style transfer in 2015\. But as it turns out, the deep-learning-based
    implementations of style transfer offer results unparalleled by what had been
    previously achieved with classical computer vision techniques, and they triggered
    an amazing renaissance in creative applications of computer vision.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 与纹理生成紧密相关的风格迁移的概念，在 2015 年神经风格迁移的发展之前，在图像处理社区中已经有了很长的历史。但事实证明，基于深度学习的风格迁移实现提供了无与伦比的结果，远远超过了以前通过经典计算机视觉技术所取得的成就，并引发了计算机视觉创意应用的惊人复兴。
- en: 'The key notion behind implementing style transfer is the same idea that’s central
    to all deep learning algorithms: you define a loss function to specify what you
    want to achieve, and you minimize this loss. We know what we want to achieve:
    conserving the content of the original image while adopting the style of the reference
    image. If we were able to mathematically define *content* and *style*, then an
    appropriate loss function to minimize would be the following:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 实现风格迁移的关键概念与所有深度学习算法的核心思想相同：你定义一个损失函数来指定你想要实现的目标，并最小化这个损失。我们知道我们想要实现什么：保留原始图像的内容同时采用参考图像的风格。如果我们能够在数学上定义*内容*和*风格*，那么一个适当的最小化损失函数将是以下内容：
- en: loss <— distance(style(reference_image) - style(combination_image)) +
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: loss <— 距离(style(reference_image) - style(combination_image)) +
- en: distance(content(original_image) - content(combination_image))
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 距离(content(original_image) - content(combination_image))
- en: Here, distance() is a norm function such as the L2 norm, content() is a function
    that takes an image and computes a representation of its content, and style()
    is a function that takes an image and computes a representation of its style.
    Minimizing this loss causes style(combination_image) to be close to style(reference_image),
    and content(combination_image) is close to content(original_image), thus achieving
    style transfer as we defined it.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，distance() 是一个诸如 L2 范数的范数函数，content() 是一个接受图像并计算其内容表示的函数，style() 是一个接受图像并计算其风格表示的函数。最小化这个损失会导致style(combination_image)接近style(reference_image)，而content(combination_image)接近content(original_image)，从而实现我们定义的风格迁移。
- en: A fundamental observation made by Gatys et al. was that deep convolutional neural
    networks offer a way to mathematically define the style and content functions.
    Let’s see how.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: Gatys 等人所做的一个基本观察是，深度卷积神经网络提供了一种数学定义风格和内容函数的方法。让我们看看如何做到这一点。
- en: 12.3.1 The content loss
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 内容损失
- en: As you already know, activations from earlier layers in a network contain *local*
    information about the image, whereas activations from higher layers contain increasingly
    global, abstract information. Formulated in a different way, the activations of
    the different layers of a convnet provide a decomposition of the contents of an
    image over different spatial scales. Therefore, you’d expect the content of an
    image, which is more global and abstract, to be captured by the representations
    of the upper layers in a convnet.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所了解的那样，网络中较早层的激活包含有关图像的*局部*信息，而较高层的激活包含越来越全局、抽象的信息。换句话说，卷积网络的不同层的激活提供了图像内容在不同空间尺度上的分解。因此，你会期望图像的内容，即更全局和抽象的部分，会被卷积网络中的上层表示所捕获。
- en: A good candidate for content loss is thus the L2 norm between the activations
    of an upper layer in a pretrained convnet, computed over the target image, and
    the activations of the same layer computed over the generated image. This guarantees
    that, as seen from the upper layer, the generated image will look similar to the
    original target image. Assuming that what the upper layers of a convnet see is
    really the content of their input images, this works as a way to preserve image
    content.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，内容损失的一个很好的候选是预训练卷积网络中上层的激活之间的 L2 范数，计算在目标图像上，以及在生成的图像上计算相同层的激活。这确保了，从上层看，生成的图像看起来与原始目标图像相似。假设卷积网络的上层看到的确实是其输入图像的内容，那么这就作为一种保留图像内容的方法。
- en: 12.3.2 The style loss
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.2 风格损失
- en: 'The content loss uses only a single upper layer, but the style loss as defined
    by Gatys et al. uses multiple layers of a convnet: you try to capture the appearance
    of the style-reference image at all spatial scales extracted by the convnet, not
    just a single scale. For the style loss, Gatys et al. use the *Gram matrix* of
    a layer’s activations: the inner product of the feature maps of a given layer.
    This inner product can be understood as representing a map of the correlations
    between the layer’s features. These feature correlations capture the statistics
    of the patterns of a particular spatial scale, which empirically correspond to
    the appearance of the textures found at this scale.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失仅使用单个上层，但由Gatys等人定义的风格损失使用卷积神经网络的多个层：您尝试捕获卷积神经网络提取的所有空间尺度的样式参考图像的外观，而不仅仅是一个单一尺度。对于风格损失，Gatys等人使用一个层激活的*Gram矩阵*：给定层的特征图的内积。这个内积可以理解为表示层特征之间的关联的地图。这些特征相关性捕获了特定空间尺度的模式的统计信息，这些模式在经验上对应于在这个尺度上发现的纹理的外观。
- en: Hence, the style loss aims to preserve similar internal correlations within
    the activations of different layers, across the style-reference image and the
    generated image. In turn, this guarantees that the textures found at different
    spatial scales look similar across the style-reference image and the generated
    image.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，风格损失旨在保留不同层次内部激活之间的相似内部关联，在样式参考图像和生成图像之间。反过来，这保证了在样式参考图像和生成图像之间看起来相似的不同空间尺度的纹理。
- en: 'In short, you can use a pretrained convnet to define a loss that will do the
    following:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，您可以使用预训练的卷积神经网络来定义以下损失：
- en: Preserve content by maintaining similar high-level layer activations between
    the original image and the generated image. The convnet should “see” both the
    original image and the generated image as containing the same things.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过保持原始图像和生成图像之间的类似高级层激活来保留内容。卷积神经网络应该“看到”原始图像和生成图像中包含相同的内容。
- en: 'Preserve style by maintaining similar *correlations* within activations for
    both low-level layers and high-level layers. Feature correlations capture *textures*:
    the generated image and the style-reference image should share the same textures
    at different spatial scales.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过保持低层和高层激活中的相似*相关性*来保留样式。特征相关性捕获*纹理*：生成图像和样式参考图像应该在不同的空间尺度上共享相同的纹理。
- en: Now let’s look at a Keras implementation of the original 2015 neural style transfer
    algorithm. As you’ll see, it shares many similarities with the DeepDream implementation
    we developed in the previous section.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下原始的2015年神经风格迁移算法的Keras实现。正如你将看到的那样，它与我们在前一节中开发的DeepDream实现有许多相似之处。
- en: 12.3.3 Neural style transfer in Keras
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.3 Keras中的神经风格迁移
- en: 'Neural style transfer can be implemented using any pretrained convnet. Here,
    we’ll use the VGG19 network used by Gatys et al. VGG19 is a simple variant of
    the VGG16 network introduced in chapter 5, with three more convolutional layers.
    Here’s the general process:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移可以使用任何预训练的卷积神经网络来实现。在这里，我们将使用Gatys等人使用的VGG19网络。VGG19是VGG16网络的一个简单变体，引入了三个额外的卷积层。以下是一般过程：
- en: Set up a network that computes VGG19 layer activations for the style-reference
    image, the base image, and the generated image at the same time.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个网络，同时计算样式参考图像、基础图像和生成图像的VGG19层激活。
- en: Use the layer activations computed over these three images to define the loss
    function described earlier, which we’ll minimize to achieve style transfer.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在这三个图像上计算的层激活来定义先前描述的损失函数，我们将最小化它以实现风格迁移。
- en: Set up a gradient-descent process to minimize this loss function.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个梯度下降过程来最小化这个损失函数。
- en: Let’s start by defining the paths to the style-reference image and the base
    image. To make sure that the processed images are a similar size (widely different
    sizes make style transfer more difficult), we’ll later resize them all to a shared
    height of 400 pixels.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义样式参考图像和基础图像的路径开始。为了确保处理后的图像具有相似的大小（大小差异很大会使风格迁移更加困难），我们将稍后将它们全部调整为共享高度为400像素。
- en: Listing 12.16 Getting the style and content images
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.16 获取风格和内容图像
- en: base_image_path <— get_file(➊
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: base_image_path <— get_file(➊
- en: '"sf.jpg", origin = "https://img-datasets.s3.amazonaws.com/sf.jpg")'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '"sf.jpg"，origin = "https://img-datasets.s3.amazonaws.com/sf.jpg")'
- en: style_reference_image_path <— get_file(➋
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: style_reference_image_path <— get_file(➋
- en: '"starry_night.jpg",'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '"starry_night.jpg",'
- en: origin = "https://img-datasets.s3.amazonaws.com/starry_night.jpg")
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: origin = "https://img-datasets.s3.amazonaws.com/starry_night.jpg")
- en: c(original_height, original_width) %<—% {
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: c(original_height, original_width) %<—% {
- en: base_image_path %>%
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: base_image_path %>%
- en: tf$io$read_file() %>%
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$read_file() %>%
- en: tf$io$decode_image() %>%
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$decode_image() %>%
- en: dim() %>% .[1:2]
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: dim() %>% .[1:2]
- en: '}'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: img_height <— 400➌
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: img_height <— 400➌
- en: img_width <— round(img_height * (original_width / original_height))➌
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: img_width <— round(img_height * (original_width / original_height))➌
- en: ➊ **Path to the image we want to transform**
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们想要转换的图像的路径**
- en: ➋ **Path to the style image**
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **风格图片的路径**
- en: ➌ **Dimensions of the generated picture**
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **生成图片的尺寸**
- en: Our content image is shown in [figure 12.10](#fig12-10), and [figure 12.11](#fig12-11)
    shows our style image.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的内容图片显示在[图 12.10](#fig12-10)，而[图 12.11](#fig12-11)显示了我们的风格图片。
- en: '![Image](../images/f0425-01.jpg)'
  id: totrans-546
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0425-01.jpg)'
- en: '**Figure 12.10 Content image: San Francisco from Nob Hill**'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.10 内容图片：旧金山诺布山**'
- en: '![Image](../images/f0426-01.jpg)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0426-01.jpg)'
- en: '**Figure 12.11 Style image: *Starry Night* by van Gogh**'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.11 风格图片：*星夜*，梵高**'
- en: We also need some auxiliary functions for loading, preprocessing, and postprocessing
    the images that go in and out of the VGG19 convnet.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一些用于加载、预处理和后处理进入和退出VGG19卷积网络的图像的辅助函数。
- en: '**Listing 12.17 Auxiliary functions**'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 12.17 辅助函数**'
- en: preprocess_image <— function(image_path) {➊
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: preprocess_image <— function(image_path) {➊
- en: image_path %>%
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: image_path %>%
- en: tf$io$read_file() %>%
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$read_file() %>%
- en: tf$io$decode_image() %>%
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$decode_image() %>%
- en: tf$image$resize(as.integer(c(img_height, img_width))) %>%
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: tf$image$resize(as.integer(c(img_height, img_width))) %>%
- en: k_expand_dims(axis = 1) %>%➋
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: k_expand_dims(axis = 1) %>%➋
- en: imagenet_preprocess_input()
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: imagenet_preprocess_input()
- en: '}'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: deprocess_image <— tf_function(function(img) {➌
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: deprocess_image <— tf_function(function(img) {➌
- en: if (length(dim(img)) == 4)
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: if (length(dim(img)) == 4)
- en: img <— k_squeeze(img, axis = 1)➍
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: img <— k_squeeze(img, axis = 1)➍
- en: c(b, g, r) %<—% {
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: c(b, g, r) %<—% {
- en: img %>%
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: img %>%
- en: k_reshape(c(img_height, img_width, 3)) %>%
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: k_reshape(c(img_height, img_width, 3)) %>%
- en: k_unstack(axis = 3)➎
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: k_unstack(axis = 3)➎
- en: '}'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: r %<>% `+`(123.68)➏
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: r %<>% `+`(123.68)➏
- en: g %<>% `+`(103.939)➏
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: g %<>% `+`(103.939)➏
- en: b %<>% `+`(116.779)➏
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: b %<>% `+`(116.779)➏
- en: k_stack(c(r, g, b), axis = 3) %>%➐
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: k_stack(c(r, g, b), axis = 3) %>%➐
- en: k_clip(0, 255) %>%
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: k_clip(0, 255) %>%
- en: k_cast("uint8")
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: k_cast("uint8")
- en: '})'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: ➊ **Util function to open, resize, and format pictures into appropriate arrays**
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **打开、调整大小和将图片格式化为适当的数组的实用函数**
- en: ➋ **Add a batch dimension.**
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **添加一个批次维度。**
- en: ➌ **Util function to convert a tensor into a valid image**
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将张量转换为有效图像的实用函数**
- en: ➍ **Also accept an image with a batch-dim of size 1. (This will throw an error
    if the first axis is not size 1.)**
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **还接受批次维度大小为1的图像。（如果第一个轴不是大小为1，则会引发错误。）**
- en: ➎ **Unstack along the third axis, and return a list of length 3.**
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **沿第三个轴拆分，并返回长度为3的列表。**
- en: ➏ **Zero-center by removing the mean pixel value from ImageNet. This reverses
    a transformation done by imagenet_preprocess_input().**
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **通过从ImageNet中移除平均像素值来将图像零居中。这是对imagenet_preprocess_input()执行的转换的逆操作。**
- en: ➐ **Note that we're reversing the order of the channels, BGR to RGB. This is
    also part of the reversal of imagenet_preprocess_input().**
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **请注意，我们正在颠倒通道的顺序，从BGR到RGB。这也是imagenet_preprocess_input()的反转的一部分。**
- en: Keras backend functions (k_*)
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: Keras后端函数（k_*）
- en: In this version of preprocess_image() and deprocess_image(), we used Keras backend
    functions, like k_expand_dims() but in earlier versions, we used functions from
    the tf module, like tf$expand_dims(). What’s the difference?
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本的preprocess_image()和deprocess_image()中，我们使用了Keras后端函数，比如k_expand_dims()，但在早期版本中，我们使用了tf模块中的函数，比如tf$expand_dims()。有什么区别呢？
- en: The Keras package contains an extensive suite of backend functions, all starting
    with the k_ prefix. They are a vestige from a time when the Keras library was
    designed to work with multiple backends. Today it’s more common to call directly
    to functions in tf module, where the functions typically expose more features
    and capabilities. One nicety of the keras::k_ backend functions, however, is that
    they all are 1-based and will often do automatic coercion of function arguments
    to integer as necessary. For example, k_expand_dims(axis = 1) is equivalent to
    tf$expand_dims(axis = 0L).
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包含一套广泛的后端函数，全部以k_前缀开头。它们是Keras库设计为与多个后端一起使用时的遗留物。如今更常见的是直接调用tf模块中的函数，这些函数通常暴露更多的功能和能力。然而，keras::k_后端函数的一个好处是它们全部是基于1的，并且通常会根据需要自动将函数参数强制转换为整数。例如，k_expand_dims(axis
    = 1)等同于tf$expand_dims(axis = 0L)。
- en: The backend functions are no longer actively developed, but they are covered
    by the TensorFlow stability promise, are maintained, and will not be going away
    anytime soon. You can safely use functions like k_expand_dims(), k_squeeze(),
    and k_stack() to do common tensor operations, especially when it’s easier to reason
    with consistent 1-based counting conventions. However, when you find the capabilities
    of the backend functions limiting, don’t hesitate to switch over to using the
    tf module functions directly. You can find additional documentation about backend
    functions at [https://keras.rstudio.com/articles/backend.html](https://www.keras.rstudio.com/articles/backend.html).
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up the VGG19 network. Like in the DeepDream example, we’ll use the
    pre-trained convnet to create a feature exactor model that returns the activations
    of intermediate layers—all layers in the model this time.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.18 Using a pretrained VGG19 model to create a feature extractor
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: model <— application_vgg19(weights = "imagenet",
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: include_top = FALSE)➊
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: outputs <— list()
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: for (layer in model$layers)
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: outputs[[layer$name]] <— layer$output
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: feature_extractor <— keras_model(inputs = model$inputs,➋
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: outputs = outputs)
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Build a VGG19 model loaded with pretrained ImageNet weights.**
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **A model that returns the activation values for every target layer (as a
    named list)**
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define the content loss, which will make sure the top layer of the VGG19
    convnet has a similar view of the style image and the combination image.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.19 Content loss
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: content_loss <— function(base_img, combination_img)
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: sum((combination_img - base_img) ^ 2)
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is the style loss. It uses an auxiliary function to compute the Gram matrix
    of an input matrix: a map of the correlations found in the original feature matrix.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.20 Style loss
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: gram_matrix <— function(x) {➊
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: n_features <— tf$shape(x)[3]
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: x %>%
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: tf$reshape(c(-1L, n_features)) %>%➋
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: tf$matmul(t(.), .)➌
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: style_loss <— function(style_img, combination_img) {
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: S <— gram_matrix(style_img)
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: C <— gram_matrix(combination_img)
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: channels <— 3
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: size <— img_height * img_width
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: sum((S - C) ^ 2) /
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: (4 * (channels ^ 2) * (size ^ 2))
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **x has the shape (height, width, features).**
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Flatten the first two spatial axes, and preserve the features axis.**
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **The output will have the shape (n_features, n_features).**
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: 'To these two loss components, you add a third: the *total variation loss*,
    which operates on the pixels of the generated combination image. It encourages
    spatial continuity in the generated image, thus avoiding overly pixelated results.
    You can interpret it as a regularization loss.'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.21 Total variation loss**'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: total_variation_loss <— function(x) {
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: a <— k_square(x[, NA:(img_height-1), NA:(img_width-1), ]—
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: x[, 2:NA             , NA:(img_width-1), ])
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: b <— k_square(x[, NA:(img_height-1), NA:(img_width-1), ]—
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: x[, NA:(img_height-1), 2:NA            , ])
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: sum((a + b) ^ 1.25)
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: The loss that you minimize is a weighted average of these three losses. To compute
    the content loss, you use only one upper layer—the block5_conv2 layer—whereas
    for the style loss, you use a list of layers that spans both low-level and high-level
    layers. You add the total variation loss at the end.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 你最小化的损失是这三种损失的加权平均值。为了计算内容损失，你只使用一个较高的层——block5_conv2层——而对于风格损失，你使用一个跨越低层和高层的层列表。你在最后添加了总变差损失。
- en: Depending on the style-reference image and content image you’re using, you’ll
    likely want to tune the content_weight coefficient (the contribution of the content
    loss to the total loss). A higher content_weight means the target content will
    be more recognizable in the generated image.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的风格参考图像和内容图像，你可能希望调整content_weight系数（内容损失对总损失的贡献）。较高的content_weight意味着生成图像中的目标内容将更容易被识别。
- en: Listing 12.22 Defining the final loss that you’ll minimize
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 清单12.22 定义你将最小化的最终损失
- en: style_layer_names <— c(➊
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: style_layer_names <— c(➊
- en: '"block1_conv1",'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '"block1_conv1",'
- en: '"block2_conv1",'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '"block2_conv1",'
- en: '"block3_conv1",'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '"block3_conv1",'
- en: '"block4_conv1",'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '"block4_conv1",'
- en: '"block5_conv1"'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '"block5_conv1"'
- en: )
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: content_layer_name <— "block5_conv2"➋
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: content_layer_name <— "block5_conv2"➋
- en: total_variation_weight <— 1e—6➌
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: total_variation_weight <— 1e—6➌
- en: content_weight <— 2.5e—8➍
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: content_weight <— 2.5e—8➍
- en: style_weight <— 1e—6➎
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: style_weight <— 1e—6➎
- en: compute_loss <
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失 <
- en: function(combination_image, base_image, style_reference_image) {
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 函数(组合图像, 基准图像, 风格参考图像) {
- en: input_tensor <
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: input_tensor <
- en: list(base_image,
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 列表(基准图像,
- en: style_reference_image,
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 风格参考图像,
- en: combination_image) %>%
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: combination_image) %>%
- en: k_concatenate(axis = 1)
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: k_concatenate(axis = 1)
- en: features <— feature_extractor(input_tensor)
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 <— 特征提取器(input_tensor)
- en: layer_features <— features[[content_layer_name]]
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: layer_features <— features[[content_layer_name]]
- en: base_image_features <— layer_features[1, , , ]
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: base_image_features <— layer_features[1, , , ]
- en: combination_features <— layer_features[3, , , ]
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: combination_features <— layer_features[3, , , ]
- en: loss <— 0➏
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 <— 0➏
- en: loss %<>% `+`(➐
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 %<>% `+`(➐
- en: content_loss(base_image_features, combination_features) *
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失(基准图像特征, 组合特征) *
- en: content_weight
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: content_weight
- en: )
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: for (layer_name in style_layer_names) {
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: for (layer_name in style_layer_names) {
- en: layer_features <— features[[layer_name]]
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: layer_features <— features[[layer_name]]
- en: style_reference_features <— layer_features[2, , , ]
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: style_reference_features <— layer_features[2, , , ]
- en: combination_features <— layer_features[3, , , ]
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: combination_features <— layer_features[3, , , ]
- en: loss %<>% `+`(➑
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 %<>% `+`(➑
- en: style_loss(style_reference_features, combination_features) *
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 风格损失(风格参考特征, 组合特征) *
- en: style_weight / length(style_layer_names)
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 风格权重 / 长度(风格层名称)
- en: )
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '}'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: loss %<>% `+`(➒
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 %<>% `+`(➒
- en: total_variation_loss(combination_image) *
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差损失(组合图像) *
- en: total_variation_weight
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: total_variation_weight
- en: )
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: loss➓
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 损失➓
- en: '}'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **The list of layers to use for the style loss**
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **用于风格损失的层列表**
- en: ➋ **The layer to use for the content loss**
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **用于内容损失的层**
- en: ➌ **The contribution weight of the total variation loss**
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **总变差损失的贡献权重**
- en: ➍ **The contribution weight of the content loss**
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **内容损失的贡献权重**
- en: ➎ **The contribution weight of the style loss**
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **风格损失的贡献权重**
- en: ➏ **Initialize the loss to 0.**
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **将损失初始化为0。**
- en: ➐ **Add the content loss.**
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **添加内容损失。**
- en: ➑ **Add the style loss for each style layer.**
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **为每个风格层添加风格损失。**
- en: ➒ **Add the total variation loss.**
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **添加总变差损失。**
- en: ➓ **Return the sum of content loss, style loss, and total variation loss.**
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: ➓ **返回内容损失、风格损失和总变差损失的总和。**
- en: 'Finally, let’s set up the gradient—descent process. In the original Gatys et
    al. paper, optimization is performed using the L—BFGS algorithm, but that’s not
    available in Tensor-Flow, so we’ll just do mini—batch gradient descent with the
    SGD optimizer instead. We’ll leverage an optimizer feature you haven’t seen before:
    a learning-rate schedule. We’ll use it to gradually decrease the learning rate
    from a very high value (100) to a much smaller final value (about 20). That way,
    we’ll make fast progress in the early stages of training and then proceed more
    cautiously as we get closer to the loss minimum.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们设置梯度下降过程。在原始的Gatys等人的论文中，优化是使用L-BFGS算法进行的，但在TensorFlow中不可用，所以我们将使用SGD优化器进行小批量梯度下降。我们将利用一个你以前没有见过的优化器特性：学习率调度。我们将逐渐减小学习率，从一个非常高的值（100）到一个更小的最终值（约为20）。这样，我们将在训练的早期阶段取得快速进展，然后在接近损失最小值时更加谨慎地进行。
- en: Listing 12.23 Setting up the gradient-descent process
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 清单12.23 设置梯度下降过程
- en: compute_loss_and_grads <— tf_function(➊
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失和梯度 <— tf_function(➊
- en: function(combination_image, base_image, style_reference_image) {
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 函数(组合图像, 基准图像, 风格参考图像) {
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: loss <— compute_loss(combination_image,
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: loss <— compute_loss(combination_image,
- en: base_image,
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: base_image,
- en: style_reference_image)
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: style_reference_image)
- en: '})'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: grads <— tape$gradient(loss, combination_image)
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: grads <— tape$gradient(loss, combination_image)
- en: list(loss, grads)
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: list(loss, grads)
- en: '})'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: optimizer <— optimizer_sgd(
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer <— optimizer_sgd(
- en: learning_rate_schedule_exponential_decay(
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: learning_rate_schedule_exponential_decay(
- en: initial_learning_rate = 100, decay_steps = 100,➋
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: initial_learning_rate = 100, decay_steps = 100,➋
- en: decay_rate = 0.96))➋
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: decay_rate = 0.96))➋
- en: base_image <— preprocess_image(base_image_path)
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: base_image <— preprocess_image(base_image_path)
- en: style_reference_image <— preprocess_image(style_reference_image_path)
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: style_reference_image <— preprocess_image(style_reference_image_path)
- en: combination_image <
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: combination_image <
- en: tf$Variable(preprocess_image(base_image_path))➌
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: tf$Variable(preprocess_image(base_image_path))➌
- en: output_dir <— fs::path("style-transfer-generated-images")
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: output_dir <— fs::path("style-transfer-generated-images")
- en: iterations <— 4000
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: iterations <— 4000
- en: for (i in seq(iterations)) {
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(iterations)) {
- en: c(loss, grads) %<—% compute_loss_and_grads(
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: c(loss, grads) %<—% compute_loss_and_grads(
- en: combination_image, base_image, style_reference_image)
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: combination_image, base_image, style_reference_image)
- en: optimizer$apply_gradients(list(➍
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer$apply_gradients(list(➍
- en: tuple(grads, combination_image)))
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: tuple(grads, combination_image)))
- en: if ((i %% 100) == 0) {
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: if ((i %% 100) == 0) {
- en: 'cat(sprintf("Iteration %i: loss = %.2f\n", i, loss))'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("迭代第%i次：损失 = %.2f\n", i, loss))
- en: img <— deprocess_image(combination_image)
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: img <— deprocess_image(combination_image)
- en: display_image_tensor(img)
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: display_image_tensor(img)
- en: fname <— sprintf("combination_image_at_iteration_%04i.png", i)
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: fname <— sprintf("combination_image_at_iteration_%04i.png", i)
- en: tf$io$write_file(filename = output_dir / fname,➎
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: tf$io$write_file(filename = output_dir / fname,➎
- en: contents = tf$io$encode_png(img))
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: contents = tf$io$encode_png(img))
- en: '}'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **We make the training step fast by compiling it as a tf_function().**
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们通过将训练步骤编译为tf_function()来加快训练速度。**
- en: ➋ **We'll start with a learning rate of 100 and decrease it by 4% every 100
    steps.**
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们将从学习率100开始，并在每100步时将其减少4%。**
- en: ➌ **Use a tf$Variable() to store the combination image because we'll be updating
    it during training.**
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **使用tf$Variable()存储组合图像，因为我们将在训练过程中更新它。**
- en: ➍ **Update the combination image in a direction that reduces the style transfer
    loss.**
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **在减少样式转移损失方向上更新组合图像。**
- en: ➎ **Save the combination image at regular intervals.**
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **定期保存组合图像。**
- en: '[Figure 12.12](#fig12-12) shows what you get. Keep in mind that what this technique
    achieves is merely a form of image retexturing, or texture transfer. It works
    best with style—reference images that are strongly textured and highly self-similar,
    and with content targets that don’t require high levels of detail to be recognizable.
    It typically can’t achieve fairly abstract feats such as transferring the style
    of one portrait to another. The algorithm is closer to classical signal processing
    than to AI, so don’t expect it to work like magic!'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.12](#fig12-12)展示了您将获得的结果。请记住，这种技术实现的仅仅是一种图像重纹理或纹理转移的形式。它最适合具有强烈纹理和高自相似性的风格参考图像以及不需要高细节级别才能识别的内容目标。它通常无法实现诸如将一个肖像的风格转移到另一个肖像之类的相当抽象的能力。该算法更接近于经典信号处理而不是人工智能，因此不要期望它像魔术一样起作用！'
- en: '![Image](../images/f0431-01.jpg)'
  id: totrans-726
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0431-01.jpg)'
- en: '**Figure 12.12 Style transfer result**'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.12 风格转移结果**'
- en: 'Additionally, note that this style-transfer algorithm is slow to run. But the
    transformation operated by the setup is simple enough that it can be learned by
    a small, fast feed-forward convnet as well—as long as you have appropriate training
    data available. Fast style transfer can thus be achieved by first spending a lot
    of compute cycles to generate input-output training examples for a fixed style-reference
    image, using the method outlined here, and then training a simple convnet to learn
    this style—specific transformation. Once that’s done, stylizing a given image
    is instantaneous: it’s just a forward pass of this small convnet.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，这种样式转移算法运行速度较慢。但是通过这种设置操作的变换足够简单，以至于它可以被一个小型、快速的前向卷积网络学习，只要您有适当的训练数据可用。因此，首先通过使用此处概述的方法花费大量计算周期为固定风格参考图像生成输入-输出训练示例，然后训练一个简单的卷积网络来学习这种特定于风格的转换，就能实现快速风格转移。一旦完成，给定图像的风格化就是瞬间完成：只需对此小型卷积网络进行正向传递。
- en: 12.3.4 Wrapping up
  id: totrans-729
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.4 结束
- en: Style transfer consists of creating a new image that preserves the contents
    of a target image while also capturing the style of a reference image.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样式转移包括创建一个新图像，保留目标图像的内容，同时捕捉参考图像的风格。
- en: Content can be captured by the high-level activations of a convnet.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容可以通过卷积网络的高级激活来捕捉。
- en: Style can be captured by the internal correlations of the activations of different
    layers of a convnet.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风格可以通过卷积网络不同层的激活之间的内部相关性来捕捉。
- en: Hence, deep learning allows style transfer to be formulated as an optimization
    process using a loss defined with a pretrained convnet.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，深度学习使得风格迁移可以被形式化为使用预训练卷积网络定义的损失函数的优化过程。
- en: Starting from this basic idea, many variants and refinements are possible.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这个基本思想出发，可以有很多变体和改进的可能性。
- en: 12.4 Generating images with variational autoencoders
  id: totrans-735
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 使用变分自编码器生成图像
- en: 'The most popular and successful application of creative AI today is image generation:
    learning latent visual spaces and sampling from them to create entirely new pictures
    interpolated from real ones—pictures of imaginary people, imaginary places, imaginary
    cats and dogs, and so on.'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 当今创造性人工智能的最流行和最成功的应用就是图像生成：学习潜在的视觉空间，并从中进行采样，以从实际图像中插值出全新的图片，如虚构的人物、虚构的地方、虚构的猫和狗等等。
- en: 'In this section and the next, we’ll review some high-level concepts pertaining
    to image generation, alongside implementation details relative to the two main
    techniques in this domain: *variational autoencoders* (VAEs) and *generative adversarial
    networks* (GANs). Note that the techniques I’ll present here aren’t specific to
    images—you could develop latent spaces of sound, music, or even text using GANs
    and VAEs—but in practice, the most interesting results have been obtained with
    pictures, and that’s what we’ll focus on here.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节和下一节中，我们将回顾与图像生成相关的一些高级概念，以及与这个领域中的两种主要技术相关的实现细节：*变分自编码器*（VAEs）和*生成对抗网络*（GANs）。请注意，我在这里介绍的技术并不局限于图像，你可以使用GANs和VAEs开发音频、音乐甚至文本的潜在空间，但实际上，最有趣的结果是在图像领域获得的，这也是我们在这里的重点。
- en: 12.4.1 Sampling from latent spaces of images
  id: totrans-738
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.1 从图像的潜在空间中采样
- en: 'The key idea of image generation is to develop a low-dimensional *latent space*
    of representations (which, like everything else in deep learning, is a vector
    space), where any point can be mapped to a “valid” image: an image that looks
    like the real thing. The module capable of realizing this mapping, taking as input
    a latent point and outputting an image (a grid of pixels), is called a *generator*
    (in the case of GANs) or a *decoder* (in the case of VAEs). Once such a latent
    space has been learned, you can sample points from it, and, by mapping them back
    to image space, generate images that have never been seen before (see [figure
    12.13](#fig12-13)). These new images are the in-betweens of the training images.'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成的关键思想是开发一个低维的*潜在空间*（就像深度学习中的其他所有东西一样，它是一个向量空间），任何点都可以映射到一个“有效”的图像：一个看起来像真实世界的图像。能够实现这种映射的模块，输入为潜在点，输出为图像（像素网格），被称为*生成器*（在GANs的情况下）或*解码器*（在VAEs的情况下）。一旦学习到这样一个潜在空间，你可以从中采样点，并通过将它们映射回图像空间，生成从未见过的图像（见
    [图12.13](#fig12-13)）。这些新图像就是训练图像之间的中间过渡图像。
- en: '![Image](../images/f0432-01.jpg)'
  id: totrans-740
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0432-01.jpg)'
- en: '**Figure 12.13 Learning a latent vector space of images and using it to sample
    new images**'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.13 学习图像的潜在向量空间并使用它来采样新图像**'
- en: GANs and VAEs are two different strategies for learning such latent spaces of
    image representations, each with its own characteristics. VAEs are great for learning
    latent spaces that are well structured, where specific directions encode a meaningful
    axis of variation in the data (see [figure 12.14](#fig12-14)). GANs generate images
    that can potentially be highly realistic, but the latent space they come from
    may not have as much structure and continuity.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 和 VAEs 是学习图像表示的潜在空间的两种不同策略，各自具有其特点。VAEs 适用于学习结构良好的潜在空间，其中特定方向对数据的变化有着有意义的编码（见
    [图12.14](#fig12-14)）。GANs 生成的图像可能非常逼真，但它们所来自的潜在空间可能没有很强的结构性和连续性。
- en: '![Image](../images/f0433-01.jpg)'
  id: totrans-743
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0433-01.jpg)'
- en: '**Figure 12.14 A continuous space of faces generated by Tom White using VAEs**'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.14 使用VAEs生成的连续面部空间，由Tom White生成**'
- en: 12.4.2 Concept vectors for image editing
  id: totrans-745
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.2 图像编辑的概念向量
- en: 'We already hinted at the idea of a *concept vector* when we covered word embeddings
    in chapter 11\. The idea is still the same: given a latent space of representations,
    or an embedding space, certain directions in the space may encode interesting
    axes of variation in the original data. In a latent space of images of faces,
    for instance, there may be a *smile vector*, such that if latent point z is the
    embedded representation of a certain face, then latent point z + s is the embedded
    representation of the same face, smiling. Once you’ve identified such a vector,
    it then becomes possible to edit images by projecting them into the latent space,
    moving their representation in a meaningful way, and then decoding them back to
    image space. Concept vectors exist for essentially any independent dimension of
    variation in image space—in the case of faces, you may discover vectors for adding
    sunglasses to a face, removing glasses, turning a male face into a female face,
    and so on. [Figure 12.15](#fig12-15) is an example of a smile vector, a concept
    vector discovered by Tom White, from the Victoria University School of Design
    in New Zealand, using VAEs trained on a dataset of faces of celebrities (the CelebA
    dataset).'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论第11章的词嵌入时，已经提及了*概念向量*的想法。思想仍然很简单：给定一个表示空间或嵌入空间的潜在空间，空间中的某些方向可能编码原始数据的有趣变化轴。例如，在面部图像的潜在空间中，可能存在一个*微笑向量*，使得如果潜在点z是某个面孔的嵌入表示，则潜在点z
    + s是相同面孔的嵌入表示，微笑着。一旦你识别出这样一个向量，就可以通过将图像投影到潜在空间中，以有意义的方式移动它们的表示，然后再将它们解码回图像空间进行编辑。对于图像空间中的任何独立变化维度，都存在概念向量，例如在面孔的情况下，通过训练VAE（CelebA数据集）的Tom
    White发现了添加太阳镜、摘掉眼镜、将男性面孔变成女性面孔等向量（见[图12.15](#fig12-15)）。
- en: '![Image](../images/f0434-01.jpg)'
  id: totrans-747
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0434-01.jpg)'
- en: '**Figure 12.15 The smile vector**'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.15 微笑向量**'
- en: 12.4.3 Variational autoencoders
  id: totrans-749
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.3 变分自编码器
- en: Variational autoencoders, simultaneously discovered by Kingma and Welling in
    December 2013^([5](#Rendnote5)) and Rezende, Mohamed, and Wierstra in January
    2014,^([6](#Rendnote6)) are a kind of generative model that’s especially appropriate
    for the task of image editing via concept vectors. They’re a modern take on autoencoders
    (a type of network that aims to encode an input to a low-dimensional latent space
    and then decode it back) that mixes ideas from deep learning with Bayesian inference.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器是由Kingma和Welling在2013年12月^[5](#Rendnote5)以及Rezende、Mohamed和Wierstra在2014年1月^[6](#Rendnote6)同时发现的一种生成模型，特别适用于通过概念向量进行图像编辑的任务。它们是对自编码器的现代化改进（自编码器是一种旨在将输入编码到低维潜在空间中，然后再进行解码的网络类型），将深度学习的思想与贝叶斯推理混合起来。
- en: A classical image autoencoder takes an image, maps it to a latent vector space
    via an encoder module, and then decodes it back to an output with the same dimensions
    as the original image, via a decoder module (see [figure 12.16](#fig12-16)). It’s
    then trained by using as target data the *same images* as the input images, meaning
    the autoencoder learns to reconstruct the original inputs. By imposing various
    constraints on the code (the output of the encoder), you can get the autoencoder
    to learn more— or less-interesting latent representations of the data. Most commonly,
    you’ll constrain the code to be low—dimensional and sparse (mostly zeros), in
    which case the encoder acts as a way to compress the input data into fewer bits
    of information.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的图像自编码器会将图像通过编码器模块映射到潜在向量空间中，然后通过解码器模块将其解码回与原始图像具有相同维度的输出（见[图12.16](#fig12-16)）。然后，通过使用*与输入图像相同的图像*作为目标数据进行训练，意味着自编码器学习重建原始输入。通过对代码（编码器的输出）施加各种限制，可以让自编码器学习更多或更少有趣的数据的潜在表示。最常见的情况是限制代码为低维和稀疏（大多数为零），在这种情况下，编码器充当将输入数据压缩为更少信息位的方式。
- en: '![Image](../images/f0434-02.jpg)'
  id: totrans-752
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0434-02.jpg)'
- en: '**Figure 12.16 An autoencoder mapping an input *x* to a compressed representation
    and then decoding it back as *x*’**'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.16 自编码器将输入*x*映射到压缩的表示，然后解码回*x***  '
- en: In practice, such classical autoencoders don’t lead to particularly useful or
    nicely structured latent spaces. They’re not much good at compression, either.
    For these reasons, they have largely fallen out of fashion. VAEs, however, augment
    autoencoders with a little bit of statistical magic that forces them to learn
    continuous, highly structured latent spaces. They have turned out to be a powerful
    tool for image generation.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这样的经典自编码器并不会导致特别有用或结构良好的潜在空间。它们在压缩方面也不太好。因此，出于这些原因，它们在很大程度上已经过时了。而VAE则通过一些统计魔法来增强自编码器，迫使它们学习连续、高度结构化的潜在空间。它们已经被证明是图像生成的强大工具。
- en: 'Instead of compressing its input image into a fixed code in the latent space,
    a VAE turns the image into the parameters of a statistical distribution: a mean
    and a variance. Essentially, this means we’re assuming the input image has been
    generated by a statistical process, and that the randomness of this process should
    be taken into account during encoding and decoding. The VAE then uses the mean
    and variance parameters to randomly sample one element of the distribution and
    decodes that element back to the original input (see [figure 12.17](#fig12-17)).
    The stochasticity of this process improves robustness and forces the latent space
    to encode meaningful representations everywhere: every point sampled in the latent
    space is decoded to a valid output.'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: VAE不是将其输入图像压缩成潜在空间中的固定代码，而是将图像转换为统计分布的参数：均值和方差。基本上，这意味着我们假设输入图像是由一个统计过程生成的，并且在编码和解码过程中应考虑到这个过程的随机性。然后，VAE使用均值和方差参数随机采样分布的一个元素，并将该元素解码回原始输入（见[图
    12.17](#fig12-17)）。这个过程的随机性提高了鲁棒性，并迫使潜在空间在任何地方都编码有意义的表示：在潜在空间中采样的每一点都被解码为有效的输出。
- en: '![Image](../images/f0435-01.jpg)'
  id: totrans-756
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0435-01.jpg)'
- en: '**Figure 12.17 A VAE maps an image to two vectors, z_mean and z_log_sigma,
    which define a probability distribution over the latent space, used to sample
    a latent point to decode.**'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.17 VAE将图像映射到两个向量 z_mean 和 z_log_sigma，它们定义了潜在空间上的概率分布，用于对潜在点进行采样以解码。**'
- en: 'In technical terms, here’s how a VAE works:'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，VAE的工作原理如下：
- en: '**1** An encoder module turns the input sample, input_img, into two parameters
    in a latent space of representations, z_mean and z_log_variance.'
  id: totrans-759
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 编码器模块将输入样本 input_img 转换为表示的潜在空间中的两个参数，z_mean 和 z_log_variance。'
- en: '**2** You randomly sample a point z from the latent normal distribution that’s
    assumed to generate the input image, via z = z_mean + exp(z_log_variance) * epsilon,
    where epsilon is a random tensor of small values.'
  id: totrans-760
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 你随机从假设生成输入图像的潜在正态分布中采样一个点 z，通过 z = z_mean + exp(z_log_variance) * epsilon，其中
    epsilon 是一个小值的随机张量。'
- en: '**3** A decoder module maps this point in the latent space back to the original
    input image.'
  id: totrans-761
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 解码器模块将潜在空间中的这一点映射回原始输入图像。'
- en: Because epsilon is random, the process ensures that every point that’s close
    to the latent location where you encoded input_img (z—mean) can be decoded to
    something similar to input_img, thus forcing the latent space to be continuously
    meaningful.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 epsilon 是随机的，这个过程确保了每一个接近编码输入图像（z_mean）的潜在位置的点都可以被解码为与输入图像类似的东西，从而迫使潜在空间连续有意义。
- en: Any two close points in the latent space will decode to highly similar images.
    Continuity, combined with the low dimensionality of the latent space, forces every
    direction in the latent space to encode a meaningful axis of variation of the
    data, making the latent space very structured and thus highly suitable to manipulation
    via concept vectors.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间中的任意两个接近点将解码为高度相似的图像。连续性，加上潜在空间的低维度，迫使潜在空间中的每个方向编码数据的有意义变化轴，使得潜在空间非常结构化，因此非常适合通过概念向量进行操作。
- en: 'The parameters of a VAE are trained via two loss functions: a *reconstruction
    loss* that forces the decoded samples to match the initial inputs, and a *regularization
    loss* that helps learn well—rounded latent distributions and reduces overfitting
    to the training data. Schematically, the process looks like this:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的参数通过两个损失函数进行训练：一个 *重构损失*，强制解码样本与初始输入匹配，一个 *正则化损失*，有助于学习良好的潜在分布并减少对训练数据的过度拟合。从图示上看，这个过程如下：
- en: c(z_mean, z_log_variance) %<—% encoder(input_img)➊
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: c(z_mean, z_log_variance) %<—% encoder(input_img)➊
- en: z <— z_mean + exp(z_log_variance) * epsilon➋
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: z <— z_mean + exp(z_log_variance) * epsilon➋
- en: reconstructed_img <— decoder(z) ➌
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: reconstructed_img <— decoder(z) ➌
- en: model <— keras_model(input_img, reconstructed_img)➍
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: model <— keras_model(input_img, reconstructed_img)➍
- en: ➊ **Encode the input into mean and variance parameters.**
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Draw a latent point using a small random epsilon.**
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Decode z back to an image.**
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Instantiate the autoencoder model, which maps an input image to its reconstruction.**
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: You can then train the model using the reconstruction loss and the regularization
    loss. For the regularization loss, we typically use an expression (the Kullback–Leibler
    divergence) meant to nudge the distribution of the encoder output toward a well—rounded
    normal distribution centered around 0\. This provides the encoder with a sensible
    assumption about the structure of the latent space it’s modeling.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see what implementing a VAE looks like in practice!
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.4 Implementing a VAE with Keras
  id: totrans-775
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re going to be implementing a VAE that can generate MNIST digits. It’s going
    to have three parts:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: An encoder network that turns a real image into a mean and a variance in the
    latent space
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sampling layer that takes such a mean and variance and uses them to sample
    a random point from the latent space
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder network that turns points from the latent space back into image
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following listing shows the encoder network we’ll use, mapping images to
    the parameters of a probability distribution over the latent space. It’s a simple
    convnet that maps the input image x to two vectors, z_mean and z_log_var. One
    important detail is that we use strides for downsampling feature maps instead
    of max pooling. The last time we did this was in the image segmentation example
    in chapter 9\. Recall that, in general, strides are preferable to max pooling
    for any model that cares about *information location*—that is to say, *where*
    stuff is in the image—and this one does, because it will have to produce an image
    encoding that can be used to reconstruct a valid image.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.24 VAE encoder network**'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <— 2➊
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: encoder_inputs <—  layer_input(shape = c(28, 28, 1))
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: x <— encoder_inputs %>%
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(32, 3, activation = "relu", strides = 2, padding = "same") %>%
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(64, 3, activation = "relu", strides = 2, padding = "same") %>%
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: layer_flatten() %>%
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(16, activation = "relu")
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: z_mean <— x %>% layer_dense(latent_dim, name = "z_mean")➋
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: z_log_var <— x %>% layer_dense(latent_dim, name = "z_log_var")➋
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: encoder <— keras_model(encoder_inputs, list(z_mean, z_log_var),
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: name = "encoder")
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **Dimensionality of the latent space: a 2D plane**'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **The input image ends up being encoded into these two parameters.**
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
- en: 'Its summary looks like this:'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0437-01.jpg)'
  id: totrans-796
  prefs: []
  type: TYPE_IMG
- en: Next is the code for using z_mean and z_log_var, the parameters of the statistical
    distribution assumed to have produced input_img, to generate a latent space point
    z.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.25 Latent—space—sampling layer**'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
- en: layer_sampler <— new_layer_class(
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: classname = "Sampler",
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: call = function(z_mean, z_log_var) {➊
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: epsilon <— tf$random$normal(shape = tf$shape(z_mean))➋
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
- en: z_mean + exp(0.5 * z_log_var) * epsilon➌
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **z_mean and z_log_var here both will have shape (batch_size, latent_dim),
    for example, (128, 2).**
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Draw a batch of random normal vectors.**
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Apply the VAE sampling formula.**
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the decoder implementation. We reshape the vector
    z to the dimensions of an image and then use a few convolution layers to obtain
    a final image output that has the same dimensions as the original input_img.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.26 VAE decoder network, mapping latent space points to images
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: latent_inputs <— layer_input(shape = c(latent_dim))➊
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: decoder_outputs <— latent_inputs %>%
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(7 * 7 * 64, activation = "relu") %>% ➋
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: layer_reshape(c(7, 7, 64)) %>%➌
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(64, 3, activation = "relu",
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>% ➍
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(32, 3, activation = "relu",
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(1, 3, activation = "sigmoid",
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: padding = "same") ➎
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: decoder <— keras_model(latent_inputs, decoder_outputs,
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: name = "decoder")
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Input where we'll feed z**
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Produce the same number of coefficients that we had at the level of the
    Flatten layer in the encoder.**
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Revert the layer_flatten() of the encoder.**
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Revert the layer_conv_2d() of the encoder.**
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **The output ends up with shape (28, 28, 1).**
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: 'Its summary looks like this:'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: decoder
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0438-01.jpg)'
  id: totrans-830
  prefs: []
  type: TYPE_IMG
- en: Now let’s create the VAE model itself. This is your first example of a model
    that isn’t doing supervised learning (an autoencoder is an example of *self-supervised*
    learning, because it uses its inputs as targets). Whenever you depart from classic
    supervised learning, it’s common to create a new_model_class() and implement a
    custom train_step() to specify the new training logic, a workflow you learned
    about in chapter 7\. That’s what we’ll do here.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.27 VAE model with custom train_step()**'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
- en: model_vae <— new_model_class(
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: classname = "VAE",
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(encoder, decoder, …) {
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize(…)
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: self$encoder <— encoder➊
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: self$decoder <— decoder
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: self$sampler <— layer_sampler()
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker <
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "total_loss")➋
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker <➋
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "reconstruction_loss")➋
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker <➋
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
- en: metric_mean(name = "kl_loss")➋
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: metrics = mark_active(function() {➌
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
- en: list(
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker,
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker,
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: '}),'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(data) {
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: c(z_mean, z_log_var) %<—% self$encoder(data)
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: z <— self$sampler(z_mean, z_log_var)
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction <— decoder(z)
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss <➍
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
- en: loss_binary_crossentropy(data, reconstruction) %>%
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
- en: sum(axis = c(2, 3)) %>% ➎
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
- en: mean()➏
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: kl_loss <— -0.5 * (1 + z_log_var - z_mean^2 - exp(z_log_var))
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
- en: total_loss <— reconstruction_loss + mean(kl_loss)➐
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(total_loss, self$trainable_weights)
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
- en: self$optimizer$apply_gradients(zip_lists(grads, self$trainable_weights))
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
- en: self$total_loss_tracker$update_state(total_loss)
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: self$reconstruction_loss_tracker$update_state(reconstruction_loss)
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: self$kl_loss_tracker$update_state(kl_loss)
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
- en: list(total_loss = self$total_loss_tracker$result(),
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: reconstruction_loss = self$reconstruction_loss_tracker$result(),
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
- en: kl_loss = self$kl_loss_tracker$result())
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **We assign to self instead of private because we want the layer weights automatically
    tracked by the Keras Model base class.**
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We use these metrics to keep track of the loss averages over each epoch.**
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **We list the metrics in an active property to enable the framework to reset
    them after each epoch (or between multiple calls to fit()/evaluate()).**
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We sum the reconstruction loss over the spatial dimensions (second and third
    axes) and take its mean over the batch dimension.**
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Total loss for each case in the batch; preserve batch axis.**
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Take the mean of loss totals in the batch.**
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add the regularization term (Kullback–Leibler divergence).**
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’re ready to instantiate and train the model on MNIST digits. Because
    the loss is taken care of in the custom layer, we don’t specify an external loss
    at compile time (loss = NULL), which in turn means we won’t pass target data during
    training (as you can see, we pass only x_train to the model in fit()).
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.28 Training the VAE
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
- en: library(listarrays)➊
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
- en: c(c(x_train, .), c(x_test, .)) %<—% dataset_mnist()
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
- en: mnist_digits <
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
- en: bind_on_rows(x_train, x_test) %>%➋
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: expand_dims(-1) %>%
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
- en: '{ . / 255 }'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: str(mnist_digits)
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
- en: num [1:70000, 1:28, 1:28, 1] 0 0 0 0 0 0 0 0 0 0 …
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
- en: vae <— model_vae(encoder, decoder)
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
- en: vae %>% compile(optimizer = optimizer_adam())➌
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
- en: vae %>% fit(mnist_digits, epochs = 30, batch_size = 128)➍
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Provide bind_on_rows() and other functions for manipulating R arrays.**
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We train on all MNIST digits, so we combine the training and test samples
    along the batch dim.**
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Note that we don't pass a loss argument in compile(), because the loss is
    already part of the train_step().**
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Note that we don't pass targets in fit(), because train_step() doesn't expect
    any.**
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we can use the decoder network to turn arbitrary
    latent space vectors into images.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.29 Sampling a grid of images from the 2D latent space**'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: n <— 30
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
- en: digit_size <— 28
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
- en: z_grid <➊
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
- en: seq(-1, 1, length.out = n) %>%
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
- en: expand.grid(., .) %>%
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
- en: as.matrix()
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
- en: decoded <— predict(vae$decoder, z_grid)➋
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
- en: z_grid_i <— seq(n) %>% expand.grid(x = ., y = .)➌
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
- en: figure <— array(0, c(digit_size * n, digit_size * n))➍
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
- en: for (i in 1:nrow(z_grid_i)) {
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
- en: c(xi, yi) %<—% z_grid_i[i, ]
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
- en: digit <— decoded[i, , , ]
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
- en: figure[seq(to = (n + 1 - xi) * digit_size, length.out = digit_size),
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
- en: seq(to = yi * digit_size, length.out = digit_size)] <—
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: digit
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
- en: par(pty = "s")➎
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
- en: lim <— extendrange(r = c(-1, 1),
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
- en: f = 1 - (n / (n+.5)))➐
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
- en: plot(NULL, frame.plot = FALSE,
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
- en: ylim = lim, xlim = lim,
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
- en: xlab = ~z[1], ylab = ~z[2]) ➑
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
- en: rasterImage(as.raster(1 - figure, max = 1),➑
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
- en: lim[1], lim[1], lim[2], lim[2],
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
- en: interpolate = FALSE)
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Create a 2D grid of linearly spaced samples.**
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Get the decoded digits.**
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Transform the decoded digits with shape (900, 28, 28, 1) to an R array with
    shape (28*30, 28*30) for plotting.**
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **We'll display a grid of 30 × 30 digits (900 digits total).**
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Square plot type**
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Expand lim so (–1, 1) are at the center of a digit.**
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Pass a formula object to xlab for a proper subscript.**
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Subtract from 1 to invert the colors.**
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
- en: 'The grid of sampled digits (see [figure 12.18](#fig12-18)) shows a completely
    continuous distribution of the different digit classes, with one digit morphing
    into another as you follow a path through latent space. Specific directions in
    this space have a meaning: for example, there are directions for “five-ness,”
    “one—ness,” and so on.'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0441-01.jpg)'
  id: totrans-936
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.18 Grid of digits decoded from the latent space**'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we’ll cover in detail the other major tool for generating
    artificial images: generative adversarial networks (GANs).'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.5 Wrapping up
  id: totrans-939
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image generation with deep learning is done by learning latent spaces that
    capture statistical information about a dataset of images. By sampling and decoding
    points from the latent space, you can generate never-before-seen images. There
    are two major tools to do this: VAEs and GANs.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VAEs result in highly structured, continuous latent representations. For this
    reason, they work well for doing all sorts of image editing in latent space: face
    swapping, turning a frowning face into a smiling face, and so on. They also work
    nicely for doing latent—space—based animations, such as animating a walk along
    a cross section of the latent space or showing a starting image slowly morphing
    into different images in a continuous way.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs enable the generation of realistic single-frame images but may not induce
    latent spaces with solid structure and high continuity.
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most successful practical applications I have seen with images rely on VAEs,
    but GANs have enjoyed enduring popularity in the world of academic research. You’ll
    find out how they work and how to implement one in the next section.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Introduction to generative adversarial networks
  id: totrans-944
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative adversarial networks (GANs), introduced in 2014 by Goodfellow et
    al.,^([7](#Rendnote7)) are an alternative to VAEs for learning latent spaces of
    images. They enable the generation of fairly realistic synthetic images by forcing
    the generated images to be statistically almost indistinguishable from real ones.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive way to understand GANs is to imagine a forger trying to create
    a fake Picasso painting. At first, the forger is pretty bad at the task. He mixes
    some of his fakes with authentic Picassos and shows them all to an art dealer.
    The art dealer makes an authenticity assessment for each painting and gives the
    forger feedback about what makes a Picasso look like a Picasso. The forger goes
    back to his studio to prepare some new fakes. As time goes on, the forger becomes
    increasingly competent at imitating the style of Picasso, and the art dealer becomes
    increasingly expert at spotting fakes. In the end, they have on their hands some
    excellent fake Picassos.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 GAN 的一种直观方式是想象一位赝造者试图制作一幅假的毕加索画。起初，赝造者在这个任务上并不在行。他把一些假画和真正的毕加索画混在一起，然后把它们展示给一个艺术品经销商。艺术品经销商为每幅画作进行真伪鉴别，并给赝造者反馈，告诉他什么因素使一幅画看起来像毕加索的作品。赝造者回到自己的工作室准备一些新的伪品。随着时间的推移，赝造者在模仿毕加索的风格方面变得越来越能胜任，而经销商在识别伪品方面也变得越来越熟练。最终，他们手里有了一些极好的假毕加索作品。
- en: 'That’s what a GAN is: a forger network and an expert network, each being trained
    to best the other. As such, a GAN is made of two parts:'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 GAN 的含义：一个规模化的轨迹生成和专家网络，每个都在训练过程中竭尽全力来胜过对方。因此，GAN 由两部分组成：
- en: '*Generator network*—Takes as input a random vector (a random point in the latent
    space), and decodes it into a synthetic image'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成器网络*—以随机向量（潜在空间中的一个随机点）作为输入，并将其解码为合成图像'
- en: '*Discriminator network (or adversary)*—Takes as input an image (real or synthetic),
    and predicts whether the image came from the training set or was created by the
    generator network'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*判别器网络（或对手）*—以图像（真实的或合成的）作为输入，并预测该图像是否来自训练集或由生成器网络生成'
- en: 'The generator network is trained to be able to fool the discriminator network,
    and thus it evolves toward generating increasingly realistic images as training
    goes on: artificial images that look indistinguishable from real ones, to the
    extent that it’s impossible for the discriminator network to tell the two apart
    (see [figure 12.19](#fig12-19)). Meanwhile, the discriminator is constantly adapting
    to the gradually improving capabilities of the generator, setting a high bar of
    realism for the generated images. Once training is over, the generator is capable
    of turning any point in its input space into a believable image. Unlike VAEs,
    this latent space has fewer explicit guarantees of meaningful structure; in particular,
    it isn’t continuous.'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络得到训练，以欺骗判别器网络，并因此朝着生成越来越逼真的图像的方向发展：人工图像看起来与真实图像无法区分，以至于判别器网络无法将两者区分开来（见[图12.19](#fig12-19)）。同时，判别器网络不断适应生成器逐渐提升的能力，为生成的图像设定了很高的真实性标准。训练结束后，生成器可以将输入空间中的任意点转化为一幅可信的图像。与
    VAE 不同，这个潜在空间没有明确保证具有有意义结构的特性；尤其是，它不是连续的。
- en: '![Image](../images/f0443-01.jpg)'
  id: totrans-951
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0443-01.jpg)'
- en: '**Figure 12.19 A generator transforms random latent vectors into images, and
    a discriminator seeks to tell real images from generated ones. The generator is
    trained to fool the discriminator.**'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.19 生成器将随机潜在向量转化为图像，判别器则试图区分真实图像和生成的图像。生成器的训练目标是欺骗判别器。**'
- en: Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike
    in any other training setup you’ve encountered in this book. Normally, gradient
    descent consists of rolling down hills in a static loss landscape. But with a
    GAN, every step taken down the hill changes the entire landscape a little. It’s
    a dynamic system where the optimization process is seeking not a minimum but an
    equilibrium between two forces. For this reason, GANs are notoriously difficult
    to train—getting a GAN to work requires lots of careful tuning of the model architecture
    and training parameters.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，GAN 是一个优化最小值不固定的系统，与本书中其他训练设置所遇到的情况不同。通常，梯度下降法是在静态损失空间中下坡前进的。但是使用 GAN
    时，沿着坡下降的每一步都会稍微改变整个损失空间。这是一个动态系统，优化过程寻求的不是最小值，而是两个力之间的平衡状态。因此，GAN 中的训练非常困难，要使其正常工作需要对模型结构和训练参数进行大量细致的调整。
- en: 12.5.1 A schematic GAN implementation
  id: totrans-954
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.1 一个概要的 GAN 实现
- en: 'In this section, we’ll explain how to implement a GAN in Keras in its barest
    form. GANs are advanced, so diving deeply into the technical details of architectures
    like that of the StyleGAN2 that generated the images in [figure 12.20](#fig12-20)
    would be out of scope for this book. The specific implementation we’ll use in
    this demonstration is a *deep convolutional GAN* (DCGAN): a very basic GAN where
    the generator and discriminator are deep convnets.'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll train our GAN on images from the large-scale CelebFaces Attributes dataset
    (known as CelebA), a dataset of 200,000 faces of celebrities ([http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)).
    To speed up training, we’ll resize the images to 64 × 64, so we’ll be learning
    to generate 64 × 64 images of human faces. Schematically, the GAN looks like this:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0444-01.jpg)'
  id: totrans-957
  prefs: []
  type: TYPE_IMG
- en: '**[Figure 12.20](#fig12-20) Latent space dwellers. Images generated by [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)
    using a StyleGAN2 model. (Image credit: Phillip Wang is the website author. The
    model used is the StyleGAN2 model from Karras et al., [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958).)**'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
- en: A generator network maps vectors of shape (latent_dim) to images of shap (64,
    64, 3).
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discriminator network maps images of shape (64, 64, 3) to a binary score estimating
    the probability that the image is real.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A gan network chains the generator and the discriminator together: gan(x) =
    discriminator(generator(x)). Thus, this gan network maps latent space vectors
    to the discriminator’s assessment of the realism of these latent vectors as decoded
    by the generator.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train the discriminator using examples of real and fake images along wit
    “real”/”fake” labels, just as we train any regular image-classification model.
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To train the generator, we use the gradients of the generator’s weights with
    regard to the loss of the gan model. This means that at every step, we move the
    weights of the generator in a direction that makes the discriminator more likely
    to classify as “real” the images decoded by the generator. In other words, we
    train the generator to fool the discriminator.
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.5.2 A bag of tricks
  id: totrans-964
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of training GANs and tuning GAN implementations is notoriously
    difficult. You should keep in mind a number of known tricks. Like most things
    in deep learning, it’s more alchemy than science: these tricks are heuristics,
    not theory—backed guidelines.'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
- en: They’re supported by a level of intuitive understanding of the phenomenon at
    hand, and they’re known to work well empirically, although not necessarily in
    every context.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few of the tricks used in the implementation of the GAN generator
    and discriminator in this section. It isn’t an exhaustive list of GAN—related
    tips; you’ll find many more across the GAN literature:'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
- en: We use strides instead of pooling for downsampling feature maps in the discriminator,
    just like we did in our VAE encoder.
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We sample points from the latent space using a *normal distribution* (Gaussian
    distribution), not a uniform distribution.
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochasticity is good for inducing robustness. Because GAN training results
    in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing
    randomness during training helps prevent this. We introduce randomness by adding
    random noise to the labels for the discriminator.
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse gradients can hinder GAN training. In deep learning, sparsity is often
    a desirable property, but not in GANs. Two things can induce gradient sparsity:
    maxp—oling operations and relu activations. Instead of max pooling, we recommend
    using strided convolutions for downsampling, and we recommend using a layer_activation_leaky_relu()
    instead of a relu activation. It’s similar to relu, but it relaxes sparsity constraints
    by allowing small negative activation values.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In generated images, it’s common to see checkerboard artifacts caused by unequal
    coverage of the pixel space in the generator (see [figure 12.21](#fig12-21)).
    To fix this, we use a kernel size that’s divisible by the stride size whenever
    we use a strided layer_conv_2d_transpose() or layer_conv_2d() in both the generator
    and the discriminator.
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Image](../images/f0445-00.jpg)'
  id: totrans-973
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.21 Checkerboard artifacts caused by mismatching strides and kernel
    sizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs**'
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.3 Getting our hands on the CelebA dataset
  id: totrans-975
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can download the dataset manually from the website: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).
    Because the dataset is hosted on Google Drive, you can also download it using
    gdown:'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
- en: reticulate::py_install("gdown", pip = TRUE)➊
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: system("gdown 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684")➋
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
- en: Downloading…
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: 'From: https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: 'To: img_align_celeba.zip'
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
- en: 32%|                      | 467M/1.44G [00:13<00:23, 41.3MB/s]
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Install gdown.**
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Download the compressed data using gdown.**
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve downloaded the data, unzip it to a celeba_gan folder
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.30 Getting the CelebA data
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: zip::unzip("img_align_celeba.zip", exdir = "celeba_gan")➊
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Uncompress the data.**
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve got the uncompressed images in a directory, you can use image_ dataset_from_directory()
    to turn it into a TF Dataset. Because we just need the images—there are no labels—we’ll
    specify label_mode = NULL.
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.31 Creating a TF Dataset from a directory of images**'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: dataset <— image_dataset_from_directory(
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: '"celeba_gan",'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
- en: label_mode = NULL,➊
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: image_size = c(64, 64),➋
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 32,➋
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: crop_to_aspect_ratio = TRUE➋
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Only the images will be returned—no labels.**
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **We will resize the images to 64 × 64 by using a smart combination of cropping
    and resizing to preserve aspect ratio. We don't want face proportions to get distorted!**
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s rescale the images to the [0-1] range.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.32 Rescaling the images**'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
- en: library(tfdatasets)
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
- en: dataset %<>% dataset_map(~ .x / 255)
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: You can use the following code to display a sample image.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.33 Displaying the first image**'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: x <— dataset %>% as_iterator() %>% iter_next()
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
- en: display_image_tensor(x[1, , , ], max = 1)
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0446-01.jpg)'
  id: totrans-1008
  prefs: []
  type: TYPE_IMG
- en: 12.5.4 The discriminator
  id: totrans-1009
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll develop a discriminator model that takes as input a candidate
    image (real or synthetic) and classifies it into one of two classes: “generated
    image” or “real image that comes from the training set.” One of the many issues
    that commonly arise with GANs is that the generator gets stuck with generated
    images that look like noise. A possible solution is to use dropout in the discriminator,
    so that’s what we will do here.'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.34 The GAN discriminator network**'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
- en: discriminator <-
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: keras_model_sequential(name = "discriminator",
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = c(64, 64, 3)) %>%
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(64, kernel_size = 4, strides = 2, padding = "same") %>%
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(128, kernel_size = 4, strides = 2, padding = "same") %>%
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(128, kernel_size = 4, strides = 2, padding = "same") %>%
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: layer_flatten() %>%
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: layer_dropout(0.2) %>% ➊
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: '➊ **One dropout layer: an important trick!**'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the discriminator model summary:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
- en: discriminator
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0447-01.jpg)'
  id: totrans-1027
  prefs: []
  type: TYPE_IMG
- en: 12.5.5 The generator
  id: totrans-1028
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s develop a generator model that turns a vector (from the latent space—
    during training it will be sampled at random) into a candidate image.
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.35 GAN generator network
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim <— 128➊
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: generator <
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
- en: keras_model_sequential(name = "generator",
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = c(latent_dim)) %>%
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
- en: layer_dense(8 * 8 * 128) %>% ➋
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
- en: layer_reshape(c(8, 8, 128)) %>%➌
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(128, kernel_size = 4,
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>% ➍
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(256, kernel_size = 4,
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%➎
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d_transpose(512, kernel_size = 4,
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
- en: strides = 2, padding = "same") %>%
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: layer_activation_leaky_relu(alpha = 0.2) %>%
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
- en: layer_conv_2d(3, kernel_size = 5, padding = "same",
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: activation = "sigmoid")
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **The latent space will be made of 128-dimensional vectors.**
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Produce the same number of coefficients we had at the level of the Flatten
    layer in the encoder.**
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Revert the layer_flatten() of the encoder.**
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Revert the layer_conv_2d() of the encoder.**
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Use Leaky Relu as our activation**
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the generator model summary:'
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
- en: generator
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0448-01.jpg)'
  id: totrans-1055
  prefs: []
  type: TYPE_IMG
- en: 12.5.6 The adversarial network
  id: totrans-1056
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we’ll set up the GAN, which chains the generator and the discriminator.
    When trained, this model will move the generator in a direction that improves
    its ability to fool the discriminator. This model turns latent-space points into
    a classification decision— “fake” or “real”—and it’s meant to be trained with
    labels that are always “these are real images.” So training gan will update the
    weights of generator in a way that makes discriminator more likely to predict
    “real” when looking at fake images.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
- en: 'To recapitulate, this is what the training loop looks like schematically. For
    each epoch, you do the following:'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Draw random points in the latent space (random noise).'
  id: totrans-1059
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**2** Generate images with generator using this random noise.'
  id: totrans-1060
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3** Mix the generated images with real ones.'
  id: totrans-1061
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**4** Train discriminator using these mixed images, with corresponding targets:
    either “real” (for the real images) or “fake” (for the generated images).'
  id: totrans-1062
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**5** Draw new random points in the latent space.'
  id: totrans-1063
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6** Train generator using these random vectors, with targets that all say
    “these are real images.” This updates the weights of the generator to move them
    toward getting the discriminator to predict “these are real images” for generated
    images: this trains the generator to fool the discriminator.'
  id: totrans-1064
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s implement it. Like in our VAE example, we’ll use a new_model_class() with
    a custom train_step(). Note that we’ll use two optimizers (one for the generator
    and one for the discriminator), so we will also override compile() to allow for
    passing two optimizers.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.36 The GAN Model**'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: GAN <— new_model_class(
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
- en: classname = "GAN",
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(discriminator, generator, latent_dim) {
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
- en: super$initialize()
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: self$discriminator  <— discriminator
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: self$generator      <— generator
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
- en: self$latent_dim     <— as.integer(latent_dim)
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: self$d_loss_metric  <— metric_mean(name = "d_loss")➊
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric  <— metric_mean(name = "g_loss")➊
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: compile = function(d_optimizer, g_optimizer, loss_fn) {
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
- en: super$compile()
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
- en: self$d_optimizer <— d_optimizer
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: self$g_optimizer <— g_optimizer
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
- en: self$loss_fn <— loss_fn
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
- en: metrics = mark_active(function() {
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
- en: list(self$d_loss_metric,
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric)
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
- en: '}),'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
- en: train_step = function(real_images) {➋
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: batch_size <— tf$shape(real_images)[1]
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <➌
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(batch_size, self$latent_dim))
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
- en: generated_images <
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: self$generator(random_latent_vectors)➍
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: combined_images <
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
- en: tf$concat(list(generated_images,
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: real_images),➎
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
- en: axis = 0L)
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: labels <
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: tf$concat(list(tf$ones(tuple(batch_size, 1L)),➏
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
- en: tf$zeros(tuple(batch_size, 1L))),
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
- en: axis = 0L)
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
- en: labels %<>% `+`(
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$uniform(tf$shape(.), maxval = 0.05))➐
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
- en: predictions <— self$discriminator(combined_images)
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
- en: d_loss <— self$loss_fn(labels, predictions)
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(d_loss, self$discriminator$trainable_weights)
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: self$d_optimizer$apply_gradients(➑
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
- en: zip_lists(grads, self$discriminator$trainable_weights))
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <➒
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(batch_size, self$latent_dim))
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
- en: misleading_labels <— tf$zeros(tuple(batch_size, 1L))➓
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: predictions <— random_latent_vectors %>%
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: self$generator() %>%
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
- en: self$discriminator()
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: g_loss <— self$loss_fn(misleading_labels, predictions)
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
- en: grads <— tape$gradient(g_loss, self$generator$trainable_weights)
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: self$g_optimizer$apply_gradients(⓫
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
- en: zip_lists(grads, self$generator$trainable_weights))
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: self$d_loss_metric$update_state(d_loss)
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: self$g_loss_metric$update_state(g_loss)
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
- en: list(d_loss = self$d_loss_metric$result(),
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
- en: g_loss = self$g_loss_metric$result())
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '})'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Set up metrics to track the two losses over each training epoch.**
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **train_step is called with a batch of real images.**
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
- en: ➌ **Sample random points in the latent space.**
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: ➍ **Decode them to fake images.**
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
- en: ➎ **Combine them with real images.**
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
- en: ➏ **Assemble labels, discriminating real from fake images.**
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
- en: ➐ **Add random noise to the labels—an important trick!**
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
- en: ➑ **Train the discriminator.**
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: ➒ **Sample random points in the latent space.**
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
- en: ➓ **Assemble labels that say "these are all real images" (it's a lie!).**
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
- en: ⓫ **Train the generator.**
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start training, let’s also set up a callback to monitor our results:
    it will use the generator to create and save a number of fake images at the end
    of each epoch.'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.37 A callback that samples generated images during training
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
- en: callback_gan_monitor <— new_callback_class(
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: classname = "GANMonitor",
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
- en: initialize = function(num_img = 3, latent_dim = 128,
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
- en: dirpath = "gan_generated_images") {
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
- en: private$num_img <— as.integer(num_img)
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: private$latent_dim <— as.integer(latent_dim)
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
- en: private$dirpath <— fs::path(dirpath)
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: fs::dir_create(dirpath)
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
- en: on_epoch_end = function(epoch, logs = NULL) {
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: random_latent_vectors <
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
- en: tf$random$normal(shape = c(private$num_img, private$latent_dim))
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: generated_images <— random_latent_vectors %>%
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: self$model$generator() %>%
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
- en: '{ tf$saturate_cast(. * 255, "uint8") }➊'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: for (i in seq(private$num_img))
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: tf$io$write_file(
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: filename = private$dirpath / sprintf("img_%03i_%02i.png", epoch, i),
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
- en: contents = tf$io$encode_png(generated_images[i, , , ])
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **Scale and clip to uint8 range of [0, 255], and cast to uint8.**
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can start training.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 12.38 Compiling and training the GAN**'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: epochs <— 100➊
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: gan <— GAN(discriminator = discriminator,➋
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: generator = generator,
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: latent_dim = latent_dim)
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
- en: gan %>% compile(
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: d_optimizer = optimizer_adam(learning_rate = 0.0001),
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: g_optimizer = optimizer_adam(learning_rate = 0.0001),
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: loss_fn = loss_binary_crossentropy()
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: gan %>% fit(
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: dataset,
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
- en: epochs = epochs,
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: callbacks = callback_gan_monitor(num_img = 10, latent_dim = latent_dim)
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: ➊ **You'll start getting interesting results after epoch 20.**
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: ➋ **Instantiate the GAN model.**
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
- en: When training, you may see the adversarial loss begin to increase considerably,
    whereas the discriminative loss tends to zero—the discriminator may end up dominating
    the generator. If that’s the case, try reducing the discriminator learning rate
    and increasing the dropout rate of the discriminator. [Figure 12.22](#fig12-22)
    shows what our GAN is capable of generating after 30 epochs of training.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/f0452-01.jpg)'
  id: totrans-1182
  prefs: []
  type: TYPE_IMG
- en: '**Figure 12.22 Some generated images around epoch 30**'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
- en: 12.5.7 Wrapping up
  id: totrans-1184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A GAN consists of a generator network coupled with a discriminator network The
    discriminator is trained to differentiate between the output of the generator
    and real images from a training dataset, and the generator is trained to fool
    the discriminator. Remarkably, the generator never sees images from the training
    set directly; the information it has about the data comes from the discriminator.
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs are difficult to train, because training a GAN is a dynamic process rather
    than a simple gradient-descent process with a fixed loss landscape. Getting a
    GAN to train correctly requires using a number of heuristic tricks, as well as
    extensive tuning.
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs can potentially produce highly realistic images. But unlike VAEs, the latent
    space they learn doesn’t have a neat continuous structure and thus may not be
    suited for certain practical applications, such as image editing via latent-space
    concept vectors.
  id: totrans-1187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These few techniques cover only the basics of this fast-expanding field. There’s
    a lot more to discover out there—generative deep learning is deserving of an entire
    book of its own.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use a sequence-to-sequence model to generate sequence data, one step
    at a time. This is applicable to text generation but also to note-by-note music
    generation or any other type of time-series data.
  id: totrans-1190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepDream works by maximizing the convnet layer activations through gradient
    ascent in input space.
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the style-transfer algorithm, a content image and a style image are combined
    via gradient descent to produce an image with the high-level features of the content
    image and the local characteristics of the style image.
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VAEs and GANs are models that learn a latent space of images and can then dream
    up entirely new images by sampling from the latent space. Concept vectors in the
    latent space can even be used for image editing.
  id: totrans-1193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](#endnote1)) Iannis Xenakis, “Musiques formelles: nouveaux principes formels
    de composition musicale,” special issue of *La Revue musicale*, nos. 253–254 (1963).'
  id: totrans-1194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([2](#endnote2)) Alex Graves, “Generating Sequences with Recurrent Neural Networks,”
    arXiv (2013), [https://arxiv.org/abs/1308.0850](https://arxiv.org/abs/1308.0850).
  id: totrans-1195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '^([3](#endnote3)) Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “DeepDream:
    A Code Example for Visualizing Neural Networks,” Google Research Blog, July 1,
    2015, [http://mng.bz/xXlM](http://mng.bz/xXlM).'
  id: totrans-1196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([4](#endnote4)) Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A
    Neural Algorithm of Artistic Style,” arXiv (2015), [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576).
  id: totrans-1197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([5](#endnote5)) Diederik P. Kingma and Max Welling, “Auto-Encoding Variational
    Bayes,” arXiv (2013), [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).
  id: totrans-1198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([6](#endnote6)) Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra,
    “Stochastic Backpropagation and Approximate Inference in Deep Generative Models,”
    arXiv (2014), [https://arxiv.org/abs/1401.4082](https://arxiv.org/abs/1401.4082).
  id: totrans-1199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ^([7](#endnote7)) Ian Goodfellow et al., “Generative Adversarial Networks,”
    arXiv (2014), [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
  id: totrans-1200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
