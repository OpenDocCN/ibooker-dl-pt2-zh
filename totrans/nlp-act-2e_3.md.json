["```py\n>>> import spacy\n>>> # spacy.cli.download(\"en_core_web_sm\") # #1\n>>> nlp = spacy.load(\"en_core_web_sm\")\n>>> sentence = ('It has also arisen in criminal justice, healthcare, and '\n...     'hiring, compounding existing racial, economic, and gender biases.')\n>>> doc = nlp(sentence)\n>>> tokens = [token.text for token in doc]\n>>> tokens\n['It', 'has', 'also', 'arisen', 'in', 'criminal', 'justice', ',',\n'healthcare', ',', 'and', 'hiring', ',', 'compounding', 'existing',\n'racial', ',', 'economic', ',', 'and', 'gender', 'biases', '.']\n```", "```py\n>>> from collections import Counter\n>>> bag_of_words = Counter(tokens)\n>>> bag_of_words\nCounter({',': 5, 'and': 2, 'It': 1, 'has': 1, 'also': 1, 'arisen': 1, ...})\n```", "```py\n>>> bag_of_words.most_common(3)  # #1\n[(',', 5), ('and', 2), ('It', 1)]\n```", "```py\n>>> import pandas as pd\n>>> most_common = dict(bag_of_words.most_common())  # #1\n>>> counts = pd.Series(most_common)  # #2\n>>> counts\n,              5\nand            2\nIt             1\nhas            1\nalso           1\n...\n```", "```py\n>>> len(counts)  # #1\n18\n>>> counts.sum()\n23\n>>> len(tokens)  # #2\n23\n>>> counts / counts.sum()  # #3\n,              0.217391\nand            0.086957\nIt             0.043478\nhas            0.043478\nalso           0.043478\n...\n```", "```py\n>>> counts['justice']\n1\n>>> counts['justice'] / counts.sum()\n0.043...\n```", "```py\n>>> sentence = \"Algorithmic bias has been cited in cases ranging from \" \\\n...     \"election outcomes to the spread of online hate speech.\"\n>>> tokens = [tok.text for tok in nlp(sentence)]\n>>> counts = Counter(tokens)\n>>> dict(counts)\n{'Algorithmic': 1, 'bias': 1, 'has': 1, 'been': 1, 'cited': 1,\n'in': 1, 'cases': 1, 'ranging': 1, 'from': 1, 'election': 1,\n'outcomes': 1, 'to': 1, 'the': 1, 'spread': 1, 'of': 1,\n'online': 1, 'hate': 1, 'speech': 1, '.': 1})\n```", "```py\n>>> from nlpia2 import wikipedia as wiki\n>>> page = wiki.page('Algorithmic Bias')  # #1\n>>> page.content[:70]\n'Algorithmic bias describes systematic and repeatable errors in a compu'\n```", "```py\n>>> import requests\n>>> url = ('https://gitlab.com/tangibleai/nlpia2/'\n...        '-/raw/main/src/nlpia2/ch03/bias_intro.txt')\n>>> response = requests.get(url)\n>>> response\n<Response [200]>\n```", "```py\n>>> bias_intro_bytes = response.content  # #1\n>>> bias_intro = response.text  # #2\n>>> assert bias_intro_bytes.decode() == bias_intro    # #3\n>>> bias_intro[:70]\n'Algorithmic bias describes systematic and repeatable errors in a compu'\n```", "```py\n>>> tokens = [tok.text for tok in nlp(bias_intro)]\n>>> counts = Counter(tokens)\n>>> counts\nCounter({'Algorithmic': 3, 'bias': 6, 'describes': 1, 'systematic': 2, ...\n>>> counts.most_common(5)\n[(',', 35), ('of', 16), ('.', 16), ('to', 15), ('and', 14)]\n```", "```py\n>>> counts.most_common()[-4:]\n('inputs', 1), ('between', 1), ('same', 1), ('service', 1)]\n```", "```py\n>>> docs = [nlp(s) for s in bias_intro.split('\\n')\n...         if s.strip()]  # #1\n>>> counts = []\n>>> for doc in docs:\n...     counts.append(Counter([\n...         t.text.lower() for t in doc]))  # #2\n>>> df = pd.DataFrame(counts)\n>>> df = df.fillna(0).astype(int)  # #3\n>>> len(df)\n16\n>>> df.head()\n  algorithmic bias describes  systematic  ... between  same service\n0           1    1         1           1  ...       0     0       0\n1           0    1         0           0  ...       0     0       0\n2           1    1         0           0  ...       0     0       0\n3           1    1         0           1  ...       0     0       0\n4           0    1         0           0  ...       0     0       0\n```", "```py\n>>> df.iloc[10]  # #1\nalgorithmic    0\nbias           0\ndescribes      0\nsystematic     0\nand            2\n...\nName: 10, Length: 246, dtype: int64\n```", "```py\n>>> docs_tokens = []\n>>> for doc in docs:\n...     docs_tokens.append([\n...         tok.text.lower() for tok in nlp(doc.text)])  # #1\n>>> len(docs_tokens[0])\n27\n```", "```py\n>>> all_doc_tokens = []\n>>> for tokens in docs_tokens:\n...     all_doc_tokens.extend(tokens)\n>>> len(all_doc_tokens)\n482\n```", "```py\n>>> vocab = set(all_doc_tokens)  # #1\n>>> vocab = sorted(vocab)  # #2\n>>> len(vocab)\n246\n>>> len(all_doc_tokens) / len(vocab)  # #3\n1.959...\n```", "```py\n>>> vocab  # #1\n['\"', \"'s\", ',', '-', '.', '2018', ';', 'a', 'ability',\n 'accurately', 'across', 'addressed', 'advanced', 'algorithm',\n 'algorithmic', 'algorithms', 'also', 'an', 'analysis',\n ...\n 'within', 'world', 'wrongful']\n```", "```py\n>>> count_vectors = []\n>>> for tokens in docs_tokens:\n...     count_vectors.append(Counter(tokens))\n>>> tf = pd.DataFrame(count_vectors)  # #1\n>>> tf = tf.T.sort_index().T\n>>> tf = tf.fillna(0).astype(int)\n>>> tf\n    \" 's , ... within world wrongful\n0   0   0  1  ...       0      0         0\n1   0   0  3  ...       0      0         0\n2   0   0  5  ...       0      0         0\n3   2   0  0  ...       0      0         0\n4   0   1  1  ...       0      0         0\n5   0   0  0  ...       0      0         0\n6   0   0  4  ...       0      1         0\n...\n11  0   0  1  ...       0      0         1\n12  0   0  3  ...       0      0         0\n13  0   0  1  ...       0      0         0\n14  0   0  2  ...       0      0         0\n15  2   0  4  ...       1      0         0\n16 rows Ã— 246 columns\n```", "```py\npip install scipy, scikit-learn\n```", "```py\n!pip install scipy, scikit-learn\n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [doc.text for doc in docs]\n>>> vectorizer = CountVectorizer()\n>>> count_vectors = vectorizer.fit_transform(corpus)  # #1\n>>> print(count_vectors.toarray()) # #2\n[[1 0 3 1 1 0 2 1 0 0 0 1 0 3 1 1]\n [1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0]\n [0 2 0 0 0 1 1 0 1 1 1 0 0 0 0 0]]\n```", "```py\n>>> v1 = np.array(list(range(5)))\n>>> v2 = pd.Series(reversed(range(5)))\n>>> slow_answer = sum([4.2 * (x1 * x2) for x1, x2 in zip(v1, v2)])\n>>> slow_answer\n42.0\n\n>>> faster_answer = sum(4.2 * v1 * v2)  # #1\n>>> faster_answer\n42.0\n\n>>> fastest_answer = 4.2 * v1.dot(v2)  # #2\n>>> fastest_answer\n42.0\n```", "```py\n>>> A.dot(B) == (np.linalg.norm(A) * np.linalg.norm(B)) * \\\n...     np.cos(angle_between_A_and_B)\n```", "```py\n>>> cos_similarity_between_A_and_B = np.cos(angle_between_A_and_B) \\\n...    = A.dot(B) / (np.linalg.norm(A) * np.linalg.norm(B))\n```", "```py\n>>> import math\n>>> def cosine_sim(vec1, vec2):\n...     vec1 = [val for val in vec1.values()]  # #1\n...     vec2 = [val for val in vec2.values()]\n...\n...     dot_prod = 0\n...     for i, v in enumerate(vec1):\n...         dot_prod += v * vec2[i]\n...\n...     mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n...     mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n...\n...     return dot_prod / (mag_1 * mag_2)\n```", "```py\n>>> from sklearn.metrics.pairwise import cosine_similarity\n>>> vec1 = tf.values[:1,:]  # #1\n>>> vec2 = tf.values[1:2,:]\n>>> cosine_similarity(vec1, vec2)\narray([[0.117...]])\n```", "```py\n>>> import copy\n>>> question = \"What is algorithmic bias?\"\n>>> ngram_docs = copy.copy(docs)\n>>> ngram_docs.append(question)\n```", "```py\n>>> question_vec = vectorizer.transform([new_sentence])\n>>> question_vec\n<1x240 sparse matrix of type '<class 'numpy.int64'>'\n    with 3 stored elements in Compressed Sparse Row format>\n```", "```py\n>>> question_vec.to_array()\narray([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... ]])\n```", "```py\n>>> vocab = list(zip(*sorted((i, tok) for tok, i in\n...     vectorizer.vocabulary_.items())))[1]\n>>> pd.Series(question_vec.to_array()[0], index=vocab).head(8)\n2018           0\nability        0\naccurately     0\nacross         0\naddressed      0\nadvanced       0\nalgorithm      0\nalgorithmic    1\n```", "```py\n>>> cosine_similarity(count_vectors, question_vector)\narray([[0.23570226],\n       [0.12451456],\n       [0.24743583],\n       [0.4330127 ],\n       [0.12909944],\n       ...\n```", "```py\n>>> docs[3]\nThe study of algorithmic bias is most concerned with algorithms\nthat reflect \"systematic and unfair\" discrimination.\n```", "```py\n>>> ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n>>> ngram_vectors = ngram_vectorizer.fit_transform(corpus)\n>>> ngram_vectors\n<16x616 sparse matrix of type '<class 'numpy.int64'>'\n    with 772 stored elements in Compressed Sparse Row format>\n```", "```py\n>>> vocab = list(zip(*sorted((i, tok) for tok, i in\n...     ngram_vectorizer.vocabulary_.items())))[1]\n>>> pd.DataFrame(ngram_vectors.toarray(),\n...     columns=vocab)['algorithmic bias']\n0     1\n1     0\n2     1\n3     1\n4     0\n```", "```py\n>>> from this import s\n>>> print(s)\nGur Mra bs Clguba, ol Gvz Crgref\nOrnhgvshy vf orggre guna htyl.\nRkcyvpvg vf orggre guna vzcyvpvg.\nFvzcyr vf orggre guna pbzcyrk.\n...\nNygubhtu arire vf bsgra orggre guna *evtug* abj.\nVs gur vzcyrzragngvba vf uneq gb rkcynva, vg'f n onq vqrn.\nVs gur vzcyrzragngvba vf rnfl gb rkcynva, vg znl or n tbbq vqrn.\nAnzrfcnprf ner bar ubaxvat terng vqrn -- yrg'f qb zber bs gubfr!\n```", "```py\n>>> char_vectorizer = CountVectorizer(\n...     ngram_range=(1,1), analyzer='char')  # #1\n>>> s_char_frequencies = char_vectorizer.fit_transform(s)\n>>> generate_histogram(\n...     s_char_frequencies, s_char_vectorizer)  # #2\n```", "```py\n>>> DATA_DIR = ('https://gitlab.com/tangibleai/nlpia/'\n...             '-/raw/master/src/nlpia/data')\n\n>>> url = DATA_DIR + '/machine_learning_full_article.txt'\n>>> ml_text = requests.get(url).content.decode()\n>>> ml_char_frequencies = char_vectorizer.fit_transform(ml_text)\n>>> generate_histogram(s_char_frequencies, s_char_vectorizer)\n```", "```py\n>>> peak_distance = ord('R') - ord('E')\n>>> peak_distance\n13\n>>> chr(ord('v') - peak_distance)  # #1\n'I'\n>>> chr(ord('n') - peak_distance)  # #2\n'A'\n```", "```py\n>>> chr(ord('W') - peak_distance)\n'J'\n```", "```py\n>>> import codecs\n>>> print(codecs.decode(s, 'rot-13'))\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n```", "```py\n>>> nltk.download('brown')  # #1\n>>> from nltk.corpus import brown\n>>> brown.words()[:10]  # #2\n['The',\n 'Fulton',\n 'County',\n 'Grand',\n 'Jury',\n 'said',\n 'Friday',\n 'an',\n 'investigation',\n 'of']\n>>> brown.tagged_words()[:5]  # #3\n[('The', 'AT'),\n ('Fulton', 'NP-TL'),\n ('County', 'NN-TL'),\n ('Grand', 'JJ-TL'),\n ('Jury', 'NN-TL')]\n>>> len(brown.words())\n1161192\n```", "```py\n>>> from collections import Counter\n>>> puncs = set((',', '.', '--', '-', '!', '?',\n...     ':', ';', '``', \"''\", '(', ')', '[', ']'))\n>>> word_list = (x.lower() for x in brown.words() if x not in puncs)\n>>> token_counts = Counter(word_list)\n>>> token_counts.most_common(10)\n[('the', 69971),\n ('of', 36412),\n ('and', 28853),\n ('to', 26158),\n ('a', 23195),\n ('in', 21337),\n ('that', 10594),\n ('is', 10109),\n ('was', 9815),\n ('he', 9548)]\n```", "```py\n>>> DATA_DIR = ('https://gitlab.com/tangibleai/nlpia/'\n...             '-/raw/master/src/nlpia/data')\n>>> url = DATA_DIR + '/bias_discrimination.txt'\n>>> bias_discrimination = requests.get(url).content.decode()\n>>> intro_tokens = [token.text for token in nlp(bias_intro.lower())]\n>>> disc_tokens = [token.text for token in nlp(bias_discrimination.lower())]\n>>> intro_total = len(intro_tokens)\n>>> intro_total\n479\n>>> disc_total = len (disc_tokens)\n>>> disc_total\n451\n```", "```py\n>>> intro_tf = {}\n>>> disc_tf = {}\n>>> intro_counts = Counter(intro_tokens)\n>>> intro_tf['bias'] = intro_counts['bias'] / intro_total\n>>> disc_counts = Counter(disc_tokens)\n>>> disc_tf['bias'] = disc_counts['bias'] / disc_total\n>>> 'Term Frequency of \"bias\" in intro is:{:.4f}'.format(intro_tf['bias'])\nTerm Frequency of \"bias\" in intro is:0.0167\n>>> 'Term Frequency of \"bias\" in discrimination chapter is: {:.4f}'\\\n...     .format(disc_tf['bias'])\n'Term Frequency of \"bias\" in discrimination chapter is: 0.0022'\n```", "```py\n>>> intro_tf['and'] = intro_counts['and'] / intro_total\n>>> disc_tf['and'] = disc_counts['and'] / disc_total\n>>> print('Term Frequency of \"and\" in intro is: {:.4f}'\\\n...     .format(intro_tf['and']))\nTerm Frequency of \"and\" in intro is: 0.0292\n>>> print('Term Frequency of \"and\" in discrimination chapter is: {:.4f}'\\\n...     .format(disc_tf['and']))\nTerm Frequency of \"and\" in discrimination chapter is: 0.0303\n```", "```py\n2 total documents / 2 documents contain \"and\"  = 2/2 = 1\n2 total documents / 2 documents contain \"bias\" = 2/2 = 1\n```", "```py\n2 total documents / 1 document contains \"black\" = 2/1 = 2\n```", "```py\n>>> num_docs_containing_and = 0\n>>> for doc in [intro_tokens, disc_tokens]:\n...     if 'and' in doc:\n...         num_docs_containing_and += 1  # #1\n```", "```py\n>>> intro_tf['black'] = intro_counts['black'] / intro_total\n>>> disc_tf['black'] = disc_counts['black'] / disc_total\n```", "```py\n>>> num_docs = 2\n>>> intro_idf = {}\n>>> disc_idf = {}\n>>> intro_idf['and'] = num_docs / num_docs_containing_and\n>>> disc_idf['and'] = num_docs / num_docs_containing_and\n>>> intro_idf['bias'] = num_docs / num_docs_containing_bias\n>>> disc_idf['bias'] = num_docs / num_docs_containing_bias\n>>> intro_idf['black'] = num_docs / num_docs_containing_black\n>>> disc_idf['black'] = num_docs / num_docs_containing_black\n```", "```py\n>>> intro_tfidf = {}\n>>> intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n>>> intro_tfidf['bias'] = intro_tf['bias'] * intro_idf['bias']\n>>> intro_tfidf['black'] = intro_tf['black'] * intro_idf['black']\n```", "```py\n>>> disc_tfidf = {}\n>>> disc_tfidf['and'] = disc_tf['and'] * disc_idf['and']\n>>> disc_tfidf['bias'] = disc_tf['bias'] * disc_idf['bias']\n>>> disc_tfidf['black'] = disc_tf['black'] * disc_idf['black']\n```", "```py\n>>> log_tf = log(term_occurences_in_doc) -\\\n...     log(num_terms_in_doc)  # #1\n>>> log_log_idf = log(log(total_num_docs) -\\\n...     log(num_docs_containing_term))  # #2\n>>> log_tf_idf = log_tf + log_log_idf  # #3\n```", "```py\n>>> doc_tfidf_vectors = []\n>>> for doc in docs:  # #1\n...     vec = copy.copy(zero_vector)  # #2\n...     tokens = [token.text for token in nlp(doc.lower())]\n...     token_counts = Counter(tokens)\n...\n...     for token, count in token_counts.items():\n...         docs_containing_key = 0\n...         for d in docs:\n...             if token in d:\n...                 docs_containing_key += 1\n...         tf = value / len(vocab)\n...         if docs_containing_key:\n...             idf = len(docs) / docs_containing_key\n...         else:\n...             idf = 0\n...         vec[key] = tf * idf\n...     doc_tfidf_vectors.append(vec)\n```", "```py\n>>> query = \"How long does it take to get to the store?\"\n>>> query_vec = copy.copy(zero_vector)  # #1\n\n>>> tokens = [token.text for token in nlp(query.lower())]\n>>> token_counts = Counter(tokens)\n\n>>> for key, value in token_counts.items():\n...     docs_containing_key = 0\n...     for _doc in docs:\n...       if key in _doc.lower():\n...         docs_containing_key += 1\n...     if docs_containing_key == 0:  # #1\n...         continue\n...     tf = value / len(tokens)\n...     idf = len(docs) / docs_containing_key\n...     query_vec[key] = tf * idf\n>>> cosine_sim(query_vec, doc_tfidf_vectors[0])\n0.5235048549676834\n>>> cosine_sim(query_vec, doc_tfidf_vectors[1])\n0.0\n>>> cosine_sim(query_vec, doc_tfidf_vectors[2])\n0.0\n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> corpus = docs\n>>> vectorizer = TfidfVectorizer(min_df=1) # #1\n>>> vectorizer = vectorizer.fit(corpus)  # #2\n>>> vectors = vectorizer.transform(corpus)  # #3\n>>> print(vectors.todense().round(2))  # #4\n[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 ... 0.21 0.   0.64 0.21 0.21]\n [0.37 0.   0.37 0.   0.   0.37 0.29 0.   ... 0.   0.49 0.   0.   0.  ]\n [0.   0.75 0.   0.   0.   0.29 0.22 0.   ... 0.   0.   0.   0.   0.  ]]\n```", "```py\nq_idf * dot(q_tf, d_tf[i]) * 1.5 / (dot(q_tf, d_tf[i]) + .25 + .75 * d_num_words[i] / d_num_words.mean()))\n```", "```py\n>>> DS_FAQ_URL = ('https://gitlab.com/tangibleai/qary/-/raw/main/'\n...     'src/qary/data/faq/faq-python-data-science-cleaned.csv')\n>>> qa_dataset = pd.read_csv(DS_FAQ_URL)\n```", "```py\n>>> vectorizer = TfidfVectorizer()\n>>> vectorizer.fit(df['question'])\n>>> tfidfvectors_sparse = vectorizer.transform(df['question'])  # #1\n>>> tfidfvectors = tfidfvectors_sparse.todense()  # #2\n```", "```py\n>>> def bot_reply(question):\n...    question_vector = vectorizer.transform([question]).todense()\n...    idx = question_vector.dot(tfidfvectors.T).argmax() # #1\n...\n...    print(\n...        f\"Your question:\\n {question}\\n\\n\"\n...        f\"Most similar FAQ question:\\n {df['question'][idx]}\\n\\n\"\n...        f\"Answer to that FAQ question:\\n {df['answer'][idx]}\\n\\n\"\n...    )\n```", "```py\n>>> bot_reply(\"What's overfitting a model?\")\nYour question:\n  What's overfitting a model?\n\nMost similar FAQ question:\n  What is overfitting?\n\nAnswer to that FAQ question:\n  When your test set accuracy is significantly lower than your training set accuracy?\n```", "```py\n>>> bot_reply('How do I decrease overfitting for Logistic Regression?')\nYour question:\n  How do I decrease overfitting for Logistic Regression?\nMost similar FAQ question:\n  How to decrease overfitting in boosting models?\nAnswer to that FAQ question:\n  What are some techniques to reduce overfitting in general? Will they work with boosting models?\n```"]