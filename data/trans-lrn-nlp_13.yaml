- en: 10 ALBERT, adapters, and multitask adaptation strategies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 ALBERT，适配器和多任务适配策略
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍
- en: Applying embedding factorization and parameter sharing across layers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对嵌入因子分解和层间参数共享进行应用
- en: Fine-tuning a model from the BERT family on multiple tasks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个任务上对BERT系列模型进行微调
- en: Splitting a transfer learning experiment into multiple steps
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将迁移学习实验分成多个步骤
- en: Applying adapters to a model from the BERT family
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对BERT系列模型应用适配器
- en: In the previous chapter, we began our coverage of some adaptation strategies
    for the deep NLP transfer learning modeling architectures that we have covered
    so far. In other words, given a pretrained architecture such as ELMo, BERT, or
    GPT, how can transfer learning be carried out more efficiently? We covered two
    critical ideas behind the method ULMFiT, namely the concepts of *discriminative
    fine-tuning* and *gradual unfreezing*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们开始介绍了到目前为止我们所涵盖的深度NLP迁移学习建模架构的一些适配策略。换句话说，给定一个预训练的架构，如ELMo、BERT或GPT，如何更有效地进行迁移学习？我们涵盖了ULMFiT方法背后的两个关键思想，即*区分性微调*和*逐渐解冻*的概念。
- en: The first adaptation strategy we will touch on in this chapter revolves around
    two ideas aimed at creating transformer-based language models that scale more
    favorably with a bigger vocabulary and longer input length. The first idea essentially
    involves clever factorization, or splitting up a larger matrix of weights into
    two smaller matrices, allowing you to increase the dimensions of one without affecting
    the dimensions of the other. The second idea involves sharing parameters across
    all layers. These two strategies are the bedrock of the method known as ALBERT,
    A Lite BERT.[¹](#pgfId-1258076) We use the implementation in the transformers
    library to get some hands-on experience with the method.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将要讨论的第一个适配策略围绕着两个目标展开，旨在创建更有利于具有更大词汇量和更长输入长度的基于transformer的语言模型。第一个想法实质上涉及巧妙的因子分解，或者将更大的权重矩阵分解为两个较小的矩阵，使您可以增加一个的维度而不影响另一个的维度。第二个想法涉及在所有层之间共享参数。这两个策略是ALBERT方法的基础，即A
    Lite BERT。我们使用transformers库中的实现来获得这种方法的一些实际经验。
- en: In chapter 4, we introduced the idea of multitask learning, where a model is
    trained to perform a variety of tasks at once. The resulting model is usually
    more generalizable to new scenarios and can result in better transfer. Unsurprisingly,
    this idea reappears in the context of adaptation strategies for pretrained NLP
    language models. When faced with a transfer scenario where there is insufficient
    training data to fine-tune on a given task, why not fine-tune on multiple tasks?
    Discussing this idea provides a great opportunity to introduce the (GLUE) dataset:[²](#pgfId-1258081)
    a collection of data for several tasks representative of human language reasoning.
    These tasks include detecting similarity between sentences, similarity between
    questions, paraphrasing, sentiment analysis, and question answering. We show how
    to quickly leverage the transformers library for multitask fine-tuning using this
    dataset. This exercise also demonstrates how to similarly fine-tune a model from
    the BERT family on a custom dataset from one of these important classes of problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们介绍了多任务学习的概念，即模型被训练为同时执行多种任务。由此产生的模型通常对新场景更具泛化能力，并且可能导致更好的迁移效果。毫不奇怪，这个想法在预训练的NLP语言模型的适配策略的背景下再次出现。当面临转移场景时，没有足够的训练数据来微调给定任务时，为什么不在多个任务上进行微调呢？讨论这个想法为介绍(GLUE)数据集提供了一个很好的机会：一个包含了几个代表人类语言推理任务的数据集。这些任务包括检测句子之间的相似性、问题之间的相似性、释义、情感分析和问答。我们展示了如何利用transformers库快速进行多任务微调使用这个数据集。这个练习还演示了如何在一个来自这些重要问题类别的自定义数据集上类似地微调BERT系列模型。
- en: In chapter 4, where we also discussed domain adaptation, we found that the similarity
    of the source and target domains plays a crucial role in the effectiveness of
    transfer learning. Greater similarity implies an easier transfer learning process
    in general. When the source and target are too dissimilar, you may find it impossible
    to carry out the process in a single step. In those circumstances, the idea of
    *sequential adaptation* may be used to break the overall desired transfer into
    simpler, more manageable steps. A language-based tool that fails to transfer between
    West and East Africa, for instance, may transfer successfully first between West
    Africa and Central Africa, and then between Central Africa and East Africa. In
    this chapter, we sequentially adapt “fill-in-the-blanks” objective pretrained
    BERT to a low-resource sentence similarity-detection scenario, by first adapting
    to a data-rich question similarity scenario.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章中，我们还讨论了领域自适应，在那里我们发现源域和目标域的相似性对于迁移学习的有效性起着至关重要的作用。更大的相似性通常意味着更容易的迁移学习过程。当源和目标过于不相似时，你可能会发现在一个步骤中执行该过程是不可能的。在这种情况下，可以使用“**顺序适应**”的概念将整体所需的转移分解成更简单、更易管理的步骤。例如，一个语言工具在西非和东非之间无法转移，但可以先在西非和中非之间成功转移，然后在中非和东非之间转移成功。在本章中，我们将“填空”目标预训练
    BERT 顺序适应到一个低资源句子相似度检测场景中，首先适应到一个数据丰富的问题相似度场景。
- en: The final adaptation strategy we will explore is the use of so-called *adaptation
    modules* or *adapters*. These are newly introduced modules of only a few parameters
    between layers of a pretrained neural network. Fine-tuning this modified model
    for new tasks requires training only these few additional parameters. The weights
    of the original network are kept the same. Virtually no loss in performance, compared
    to fine-tuning the entire model, is often observed when adding just 3-4% additional
    parameters per task.[³](#pgfId-1258089) These adapters are also modular and easily
    shared between researchers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨的最终适应策略是使用所谓的*适应模块*或*适配器*。这些是预训练神经网络层之间只有少量参数的新引入模块。对于新任务微调这个修改后的模型只需要训练这几个额外的参数。原始网络的权重保持不变。通常情况下，当每个任务只增加
    3-4% 的额外参数时，与微调整个模型相比，性能几乎没有损失。这些适配器也是模块化的，并且很容易在研究人员之间共享。
- en: 10.1 Embedding factorization and cross-layer parameter sharing
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 嵌入因子分解和跨层参数共享
- en: The adaptation strategies we discuss in this section revolve around two ideas
    aimed at creating transformer-based language models that scale more favorably
    with a bigger vocabulary and longer maximum input length. The first idea essentially
    involves clever factorization of a larger matrix of weights into two smaller matrices,
    allowing one to increase in dimension without affecting the dimensions of the
    other one. The second idea involves sharing parameters across all layers. These
    two strategies are the bedrock of the method known as ALBERT.[⁴](#pgfId-1258098)
    We again use the implementation in the transformers library to get some hands-on
    experience with the method. This serves both to give you a sense of what sorts
    of improvements are attained, as well as to arm you with the ability to use it
    in your own projects. We will be using the Amazon book reviews in the Multi-Domain
    Sentiment Dataset[⁵](#pgfId-1258102) from chapter 4 as our custom corpus for this
    experiment. This will allow you to gain further experience fine-tuning a pretrained
    transformer-based language model on a custom corpus, this time in English!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节讨论的适应策略围绕着两个想法，旨在创建具有更大词汇表和更长最大输入长度的基于 transformer 的语言模型。第一个想法基本上涉及将一个更大的权重矩阵巧妙地分解为两个较小的矩阵，使得其中一个可以在不影响另一个维度的情况下增加维度。第二个想法涉及在所有层之间共享参数。这两种策略是
    ALBERT 方法的基础。我们再次使用 transformers 库中的实现来获取一些与该方法有关的实际经验。这既可以让你对所获得的改进有所了解，也可以让你有能力在自己的项目中使用它。我们将使用第
    4 章中的 Multi-Domain Sentiment Dataset 中的亚马逊图书评论作为我们这次实验的自定义语料库。这将使您能够进一步体验在自定义语料库上微调预训练的基于
    transformer 的语言模型，这次是用英语！
- en: The first strategy, namely embedding factorization, is motivated by the observation
    that in BERT, the size of the input embedding is intrinsically linked to the dimension
    of its hidden layers. The tokenizer creates a one-hot encoded vector for each
    token—this vector is equal to 1 in the dimension corresponding to the token and
    0 otherwise. The dimension of this one-hot encoded vector is equal to the size
    of the vocabulary, *V*. The input embedding can be thought of as a matrix of dimension
    *V* by *E*, multiplying the one-hot encoded vector and projecting it into a dimension
    of size *E*. In earlier models such as BERT, this was equal to the hidden layer
    dimension *H*, so that this projection happened directly into the hidden layers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个策略，即嵌入因子分解，受到了观察的启发，即在BERT中，输入嵌入的大小与其隐藏层的维度密切相关。分词器为每个标记创建一个one-hot编码的向量——该向量在与标记对应的维度上等于1，在其他维度上等于0。这个one-hot编码向量的维度等于词汇表的大小，*V*。输入嵌入可以被看作是一个维度为*V*乘以*E*的矩阵，将one-hot编码的向量乘以它并投影到大小为*E*的维度中。在早期的模型（如BERT）中，这等于隐藏层的维度*H*，因此这个投影直接发生在隐藏层中。
- en: 'This means that when the size of the hidden layer increases, the dimension
    of the input embedding must increase as well, which can be very inefficient. On
    the other hand, the ALBERT authors observed that the role for the input embedding
    is to learn context-independent representations, whereas that of the hidden layers
    is to learn context-dependent representations—a harder problem. Motivated by this,
    they propose splitting the single-input embedding matrix into two matrices: one
    of dimension *V* by *E* and the other *E* by *H*, allowing *H* and *E* to be completely
    independent. Said differently, the one-hot encoded vectors can be first projected
    into an intermediate embedding of a smaller size and only then fed into the hidden
    layers. This allows the input embedding to have a significantly smaller size,
    even when the hidden layer dimensions are large or need to be scaled up. This
    design decision alone leads to an 80% reduction in the size of the matrix/matrices
    projecting the one-hot embedded vector into the hidden layers.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当隐藏层的大小增加时，输入嵌入的维度也必须增加，这可能非常低效。另一方面，ALBERT的作者观察到，输入嵌入的作用是学习上下文无关的表示，而隐藏层的作用是学习上下文相关的表示——这是一个更难的问题。受此启发，他们提出将单一输入嵌入矩阵分成两个矩阵：一个是*V*乘以*E*，另一个是*E*乘以*H*，允许*H*和*E*完全独立。换句话说，one-hot编码的向量可以首先投影到较小尺寸的中间嵌入中，然后再馈送到隐藏层。即使隐藏层的尺寸很大或需要扩展，这也使得输入嵌入可以具有显着较小的尺寸。仅此设计决策就导致将投影one-hot嵌入向量到隐藏层的矩阵/矩阵的尺寸减少了80%。
- en: The second strategy, cross-layer parameter sharing, is related to the *soft-parameter
    sharing multitask learning* scenario we discussed in chapter 4\. Corresponding
    weights across all layers are encouraged to be similar to each other by imposing
    appropriate constraints on them during learning. This serves as a regularization
    effect, reducing the risk of overfitting by reducing the number of degrees of
    available freedom. Taken together, the two techniques allowed the authors to build
    pretrained language models that outperformed both the GLUE and the SQuAD record
    performances at the time (February 2020). Compared to BERT, an approximately 90%
    reduction in parameter size was achieved with only a slight reduction in performance
    (less than 1% on SQuAD).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个策略，即跨层参数共享，与我们在第四章中讨论的*软参数共享多任务学习*场景相关。在学习过程中，通过对它们施加适当的约束，鼓励所有层之间的相应权重彼此相似。这起到了正则化的效果，通过减少可用自由度的数量来降低过拟合的风险。这两种技术的结合使得作者能够构建出在当时（2020年2月）超越了GLUE和SQuAD记录性能的预训练语言模型。与BERT相比，在参数大小上实现了约90%的减少，而性能只有轻微的下降（在SQuAD上不到1%）。
- en: Again, because a variety of checkpoints have been made available for direct
    loading and our focus here is on transfer learning, we do not repeat the training
    steps from scratch here. Instead, we work with a checkpoint analogous to the “base”
    BERT checkpoint we used in the previous chapter and in chapter 8 for our cross-lingual
    transfer learning experiments. This allows us to directly compare the performance
    and benefits of using this architecture over the original BERT, while also teaching
    you how to start using this architecture in your own projects.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，因为多种检查点可用于直接加载，我们不在此重复从头开始的训练步骤，因为我们的重点是迁移学习。相反，我们使用类似于我们在前一章和第8章中用于我们的跨语言迁移学习实验的“基础”BERT检查点。这使我们能够直接比较使用这种架构与原始BERT的性能和效益，并教你如何开始在自己的项目中使用这种架构。
- en: 10.1.1 Fine-tuning pretrained ALBERT on MDSD book reviews
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 在MDSD书评上对预训练的ALBERT进行微调
- en: We prepare the data using the same steps as in section 4.4, which we will not
    repeat here. These are also repeated in the Kaggle notebooks made available with
    this book. We start with the variable `data` produced by listing 4.6\. Assuming
    the same hyperparameter settings as section 4.4, this is a NumPy array of 2,000
    book review texts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备数据的步骤与第4.4节中的步骤相同，我们在此不再重复。这些步骤也在本书附带的Kaggle笔记本中重复出现。我们从列表4.6生成的变量`data`开始。假设与第4.4节相同的超参数设置，这是一个由2,000本书评文本组成的NumPy数组。
- en: 'Write this NumPy array to file using Pandas with the following code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将这个NumPy数组写入Pandas到文件中：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We start by initializing an Albert tokenizer to the pretrained checkpoint from
    the base ALBERT model as follows. We are using version 2 because it is the latest
    available version at the moment. You can find the list of all available ALBERT
    models at any point on the Hugging Face website.[⁶](#pgfId-1258124)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先初始化一个Albert分词器，使用基本ALBERT模型中的预训练检查点，如下所示。我们使用版本2是因为它是目前可用的最新版本。你可以在Hugging
    Face网站上随时找到所有可用的ALBERT模型列表。[⁶](#pgfId-1258124)
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Loads the ALBERT tokenizer
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载ALBERT分词器
- en: ❷ Uses the pretrained ALBERT tokenizer
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用预训练的ALBERT分词器
- en: 'Having prepared the tokenizer, load the base ALBERT checkpoint into an ALBERT
    masked language model, and display the number of parameters as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好分词器后，将基础ALBERT检查点加载到ALBERT遮盖语言模型中，并显示参数数量如下：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Uses masked language modeling
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用遮盖语言建模
- en: ❷ Initializes to the ALBERT checkpoint
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化到ALBERT检查点
- en: The output indicates that the model has 11.8 million parameters—this is a huge
    reduction in size versus BERT’s 178.6 million parameters from chapter 8 and DistilBERT’s
    135.5 million parameters from the previous chapter. In fact, this is a reduction
    of 15 times from the BERT model. Wow!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表明模型有1180万个参数——与第8章的BERT的178.6万个参数和直接BERT的135.5万个参数相比，这是一个巨大的缩小。事实上，这是与BERT模型相比的15倍缩小。哇！
- en: 'Next, as before, build a dataset with the tokenizer from the monolingual Twi
    text, using the convenient `LineByLineTextDataset` method included with transformers,
    shown next:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像之前一样，使用transformers中提供的方便的`LineByLineTextDataset`方法，使用单语Twi文本中的分词器构建数据集，如下所示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ How many lines to read at a time
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每次读取多少行
- en: 'Define a “data collator”—a helper method that creates a special object out
    of a batch of sample data lines (of length `block_size`)—as shown next. This special
    object is consummable by PyTorch for neural network training:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个“数据收集器”——一个帮助方法，将一个样本数据行批量（`block_size`长度）创建成一个特殊对象——如下所示。这个特殊对象可以被PyTorch用于神经网络训练：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Uses masked language modeling, and masks words with a probability of 0.15
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用遮盖语言建模，并用0.15的概率遮盖单词
- en: Here we use masked language modeling with 15% of the words to be randomly masked
    in our input data, and the model is asked to predict them during training.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了以15%的概率对我们的输入数据进行随机遮盖的遮盖语言建模，并要求模型在训练过程中对它们进行预测。
- en: 'Define standard training arguments, such as output directory and training batch
    size, as shown in the next code snippet. Note that we are training for 10 epochs
    this time, because the dataset is so much smaller than the monolingual Twi sample
    of over 600,000 used in the previous chapter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 定义标准的训练参数，如输出目录和训练批量大小，如下代码片段所示。注意，这一次我们训练10次，因为数据集比前一章中使用的超过60,000个单语Twi样本要小得多：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, use training arguments with the previously defined dataset and collator
    to define a “trainer” for one training epoch across the data as shown next:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用之前定义的数据集和整理器来定义一个“训练器”，以跨数据进行一个训练 epoch，如下所示：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Train and time how long training takes as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤训练并计时训练时间：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Over this small dataset, the 10 epochs take only approximately five minutes
    to finish training. The loss reaches a value of around 1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小数据集上，10 个 epochs 大约只需约五分钟就能完成训练。损失值达到约 1。
- en: 'Save the model as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式保存模型：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, let’s apply the pipelines API to predict the masked word in a fictional
    book review as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们按照以下步骤应用管道 API 来预测虚构书评中的遮蔽词：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Defines the fill-in-the-blanks pipeline
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义填空管道
- en: ❷ Predicts the masked token
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测遮蔽的标记
- en: 'This yields the following, very plausible, output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下非常合理的输出：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You may have observed by now that the sequence of steps we executed to fine-tune
    ALBERT on the custom book review corpus here is very similar to the sequence of
    steps we used with DistilBERT in the previous chapter. That sequence of steps
    is, in turn, quite similar to the sequence of steps we used with mBERT in chapter
    8\. We stress yet again that this recipe can be used as a blueprint with virtually
    any other architecture available in transformers. Although it is impossible for
    us to provide an example of fine-tuning on every possible type of application,
    this recipe should generalize, or at least serve as a good starting point, for
    many use cases. Consider, for instance, the scenario where you wanted to teach
    GPT-2 to write in some chosen style. Simply copy over the same code we have used
    here, point the dataset paths to a corpus of your chosen writing style, and change
    the tokenizer and model references from `AlbertTokenizer` / `AlbertForMaskedLM`
    to `GPT2Tokenizer` / `GPT2LMHeadModel`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您可能已经观察到，我们在此处对自定义书评语料库对 ALBERT 进行微调的步骤序列与我们在上一章中使用 DistilBERT 的步骤序列非常相似。这一系列步骤反过来又与我们在第
    8 章中使用的 mBERT 的步骤序列非常相似。我们再次强调，这个配方可以用作 transformers 中几乎任何其他架构的蓝图。虽然我们无法提供在每种可能的应用类型上微调的示例，但这个配方应该可以推广，或者至少作为许多用例的良好起点。例如，考虑一种情况，您想要教
    GPT-2 以某种选择的风格写作。只需复制我们在这里使用的相同代码，将数据集路径指向您选择的写作风格的语料库，并将标记器和模型引用从 `AlbertTokenizer`
    / `AlbertForMaskedLM` 更改为 `GPT2Tokenizer` / `GPT2LMHeadModel`。
- en: 'One thing to note is that all the PyTorch transformers models have all the
    layers unfrozen for training by default. To freeze all layers, you can execute
    the following code snippet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，所有 PyTorch transformers 模型默认情况下都会解冻所有层进行训练。要冻结所有层，您可以执行以下代码片段：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can freeze only some parameters using analogous code snippets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用类似的代码片段仅冻结一些参数。
- en: In the next section, where we will discuss multitask fine-tuning, we will have
    yet another opportunity to look at fine-tuning of these types of models, this
    time for various tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论多任务微调，我们将有另一个机会来看看这些类型模型的微调，这次是针对各种任务。
- en: 10.2 Multitask fine-tuning
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 多任务微调
- en: In section 3 of chapter 4, we introduced the idea of multitask learning, where
    a model is trained to perform a variety of tasks instead of just one. The resulting
    model is usually more generalizable to new scenarios and can result in better
    transfer and performance. Unsurprisingly, this idea reappears in the context of
    adaptation strategies for pretrained NLP language models, where models fine-tuned
    on multiple tasks have been observed to be more robust and performant.[⁷](#pgfId-1258231)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章的第 3 节中，我们介绍了多任务学习的概念，其中模型被训练执行各种任务，而不仅仅是一个任务。结果模型通常对新场景更具一般性，并且可以实现更好的转移和性能。毫不奇怪，这个想法再次出现在预训练
    NLP 语言模型的适应策略的背景下，微调在多个任务上的模型观察到更加健壮和有效。[⁷](#pgfId-1258231)
- en: Our discussion of this idea here provides a great opportunity to introduce the
    *General Language Understanding Evaluation* (GLUE) dataset,[⁸](#pgfId-1258236)
    a collection of data for several tasks representative of human language reasoning.
    This dataset includes tasks such as detecting similarity between sentences, similarity
    between questions, paraphrasing, sentiment analysis, and question answering. In
    this section, we demonstrate how to quickly leverage the transformers library
    to fine-tune the various transformer-based pretrained models we have discussed
    on various tasks from the GLUE dataset. This exercise also demonstrates how to
    analogously fine-tune a model from the BERT family on a custom dataset from one
    of the important classes of problems included in GLUE.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论这个想法提供了一个很好的机会来介绍*通用语言理解评估*（GLUE）数据集，[⁸](#pgfId-1258236)这是一个包含几个人类语言推理代表性任务数据的集合。这个数据集包括检测句子相似性、问题相似性、释义、情感分析和问题回答等任务。在本节中，我们演示了如何快速利用transformers库对我们讨论的各种基于transformers的预训练模型在GLUE数据集的各种任务上进行微调。这个练习还演示了如何按类似方式微调来自BERT系列的模型，以解决GLUE中包含的重要问题类别之一的自定义数据集。
- en: We also demonstrate *sequential adaptation*—the process of breaking up an overall
    desired transfer experiment into simpler, more manageable steps. Consider a hypothetical
    scenario where a language-based tool fails to transfer between West and East Africa—it
    still may transfer successfully first between West Africa and Central Africa,
    and then between Central Africa and East Africa. This is related to the idea of
    multitask fine-tuning in the sense that it is essentially that carried out sequentially,
    one after the other. Instead of fine-tuning the model on several tasks simultaneously—which
    is how multitask fine-tuning is typically conceptualized—sequential adaptation
    fine-tunes on one task first, and then the other.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还演示了*顺序适应*——将总体所需的转移实验过程分解成更简单、更易管理的步骤的过程。考虑一个假设的情景，即基于语言的工具在西非和东非之间无法完成转移——首先它可能在西非和中非之间成功转移，然后在中非和东非之间成功转移。这与多任务微调的想法相关，因为它本质上是按顺序进行的，一步接一步进行。与通常理解的多任务微调方法不同，顺序适应首先在一个任务上进行微调，然后再在另一个任务上进行微调。
- en: In this section, we demo multitask fine-tuning and sequential adaptation by
    fine-tuning some pretrained transformer-based language models on several tasks
    from the GLUE dataset. Specifically, we focus on a question similarity task known
    as the *Quora Question Pair* (QQP) task, as well as the *Semantic Textual Similarity
    Benchmark* (SST-B) task for measuring similarity between a pair of sentences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过在GLUE数据集的几个任务上对一些预训练的基于transformers的语言模型进行多任务微调和顺序适应来演示。具体来说，我们关注的是一个被称为*Quora问题对*（QQP）任务的问题相似度任务，以及用于衡量一对句子之间相似性的*语义文本相似性基准*（SST-B）任务。
- en: 10.2.1 General Language Understanding Dataset (GLUE)
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 通用语言理解数据集（GLUE）
- en: The General Language Understanding Dataset (GLUE) was introduced to provide
    a challenging set of benchmark datasets for a diverse set of natural language
    understanding tasks. These tasks were selected to represent the implicit agreement
    among researchers in NLP over the years about what constitutes an interesting,
    challenging, and relevant set of problems. In table 10.1, we summarize the tasks
    available in the dataset and data counts for each task.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通用语言理解数据集（GLUE）旨在提供一系列多样的自然语言理解任务的具有挑战性的基准数据集。这些任务被选中，以代表多年来在自然语言处理领域研究人员之间达成的一种关于什么构成有趣、具有挑战性和相关问题的隐含共识。在表10.1中，我们总结了数据集中可用任务和每个任务的数据计数。
- en: Table 10.1 List of tasks, descriptions, and data counts for each task made available
    in the original General Language Understanding Dataset (GLUE)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1 原始通用语言理解数据集（GLUE）中提供的任务、描述和数据计数列表
- en: '| Task Name | Data Count | Description |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 任务名称 | 数据量 | 描述 |'
- en: '| The Corpus of Linguistic Acceptability (CoLA) | 8,500 train, 1,000 test |
    Determines whether or not an English sentence is grammatical |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 语言可接受性语料库（CoLA） | 训练 8,500，测试 1,000 | 确定一个英语句子是否符合语法规范 |'
- en: '| The Stanford Sentiment Treebank (SST2) | 67,000 train, 1,800 test | Detects
    the sentiment of a given sentence—positive or negative |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 斯坦福情感树库（SST2） | 训练 67,000，测试 1800 | 检测给定句子的情感-积极或消极 |'
- en: '| Microsoft Research Paraphrase Corpus (MRPC) | 3,700 train, 1,700 test | Determines
    whether one sentence is a paraphrase of another |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Microsoft Research Paraphrase Corpus (MRPC) | 3,700 train, 1,700 test | 确定一个句子是否是另一个句子的释义
    |'
- en: '| Semantic Textual Similarity Benchmark (STS-B) | 7,000 train, 1,400 test |
    On a scale of 1 to 5, predicts the similarity score between a pair of sentences
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Semantic Textual Similarity Benchmark (STS-B) | 7,000 train, 1,400 test |
    预测一对句子之间的相似度分数，范围在 1 到 5 之间 |'
- en: '| Quora Question Pairs (QQP) | 3,640,000 train, 391,000 test | Determines whether
    a pair of Quora questions are semantically equivalent |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Quora Question Pairs (QQP) | 3,640,000 train, 391,000 test | 确定一对 Quora 问题是否语义上等同
    |'
- en: '| Multi-Genre Natural Language Inference (MultiNLI) | 393,000 train, 20,000
    test | Determines whether a premise sentence implies/entails or contradicts a
    hypothesis sentence |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Multi-Genre Natural Language Inference (MultiNLI) | 393,000 train, 20,000
    test | 确定一个前提句子是暗示/蕴含还是与一个假设句子相矛盾 |'
- en: '| Question-Answering Natural Language Inference (QNLI) | 105,000 train, 5,400
    test | Detects whether the context sentence contains an answer to the question
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Question-Answering Natural Language Inference (QNLI) | 105,000 train, 5,400
    test | 检测上下文句子是否包含对问题的答案 |'
- en: '| Recognizing Textual Entailment (RTE) | 2,500 train, 3,000 test | Measures
    the textual entailment between the premise and the hypothesis, similarly to MultiNLI
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Recognizing Textual Entailment (RTE) | 2,500 train, 3,000 test | 测量前提和假设之间的文本蕴含关系，类似于
    MultiNLI |'
- en: '| Winograd Schema Challenge (WNLI) | 634 train, 146 test | Determines to which
    noun from a set of possible options an ambiguous pronoun refers |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Winograd Schema Challenge (WNLI) | 634 train, 146 test | 确定模棱两可的代词指的是一组可能选项中的哪一个名词
    |'
- en: As can be seen from the table, the original GLUE dataset covered a variety of
    tasks with different amounts of data available. This is to encourage the sharing
    of knowledge between different tasks, which is the essence of the multitask fine-tuning
    idea we are exploring in this section of the chapter. We now briefly describe
    the various tasks in the table.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，原始的 GLUE 数据集涵盖了各种任务，并且可用的数据量不同。这是为了鼓励不同任务之间的知识共享，这也是我们在本章节中探讨的多任务微调理念的核心。接下来我们简要描述表中的各项任务。
- en: The first two tasks—the *Corpus of Linguistic Acceptability* (CoLA) and the
    *Stanford Sentiment Treebank* (SST2)—are single-sentence tasks. The former tries
    to determine if a given English sentence is grammatically correct, whereas the
    latter tries to detect whether the sentiment expressed in a sentence is positive
    or negative.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个任务——*Corpus of Linguistic Acceptability* (CoLA) 和 *Stanford Sentiment Treebank*
    (SST2)——是单句任务。前者试图确定一个给定的英文句子是否语法正确，而后者试图检测句子中表达的情感是积极还是消极。
- en: The following three tasks—*Microsoft Research Paraphrase Corpus* (MRPC), *Semantic
    Textual Similarity Benchmark* (STS-B), and *Quora Question Pairs* (QQP)—are classified
    as similarity tasks. These involve comparisons between two sentences in various
    ways. MRPC tries to detect if one sentence is a paraphrase of another, that is,
    if it expresses the same concepts. STS-B measures the similarity between a pair
    of sentences on a continuous scale between 1 and 5\. QQP tries to detect if one
    Quora question is equivalent to another.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三项任务——*Microsoft Research Paraphrase Corpus* (MRPC)、*Semantic Textual Similarity
    Benchmark* (STS-B) 和 *Quora Question Pairs* (QQP)——被归类为相似性任务。这些任务涉及以各种方式比较两个句子。MRPC
    试图检测一个句子是否是另一个句子的释义，即是否表达了相同的概念。STS-B 在连续范围 1 到 5 之间测量一对句子的相似度。QQP 试图检测一个 Quora
    问题是否等同于另一个。
- en: The remaining four tasks are classified as inference tasks. The *Multi-Genre
    Natural Language Inference* (MultiNLI) task attempts to determine if a given sentence
    implies another sentence or contradicts it—it measures *entailment*. The *Question-Answering
    Natural Language Inference* (QNLI) task is similar to the SQuAD[⁹](#pgfId-1258329)
    dataset we discussed and used in chapter 8 to illustrate question answering. As
    a reminder, that dataset is composed of a context paragraph, a question about
    it, and the start and end positional indicators of an answer to the question in
    the context paragraph, if one exists. QNLI essentially turns this idea into a
    sentence-pair task by pairing each context sentence with the question and attempting
    to predict if the answer is in that context sentence. The *Recognizing Textual
    Entailment* (RTE) task is similar to MultiNLI in that it measures entailment between
    a pair of sentences. Finally, the *Winograd Schema Challenge* (WNLI) dataset attempts
    to detect to which noun from a set of available options an ambiguous pronoun in
    a sentence refers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的四个任务被分类为推理任务。*多体裁自然语言推理*（MultiNLI）任务试图确定给定的句子是否暗示另一个句子或是否与之矛盾――它衡量*蕴涵*。*问答自然语言推理*（QNLI）任务类似于我们讨论并在第8章中用于说明问答的SQuAD[⁹](#pgfId-1258329)数据集。提醒一下，该数据集由上下文段落、对其提出的问题以及答案在上下文段落中的开始和结束位置指示符组成，如果存在的话。QNLI基本上将这个想法转化为一个句对任务，通过将每个上下文句子与问题配对，并尝试预测答案是否在该上下文句子中。*识别文本蕴涵*（RTE）任务类似于MultiNLI，因为它衡量两个句子之间的蕴涵关系。最后，*Winograd
    Schema Challenge*（WNLI）数据集试图检测一个含糊指代词在句子中指代可用选项中的哪个名词。
- en: Since the inception of GLUE, another dataset called SuperGLUE[^(10)](#pgfId-1258335)
    has been introduced as well. This new version became necessary as modern methods
    recently began achieving close to perfect performances on many parts of GLUE.
    SuperGLUE was developed to be more challenging and thus to provide more “dynamic
    range” for comparing methods. We focus on GLUE here, but we do think it is important
    to keep the existence of SuperGLUE in mind as you become more of an expert on
    NLP.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 自GLUE成立以来，还引入了另一个名为SuperGLUE[^(10)](#pgfId-1258335)的数据集。这个新版本是必要的，因为最近的现代方法在GLUE的许多部分几乎达到了完美的性能。SuperGLUE的开发是为了更具挑战性，因此为比较方法提供更多的“动态范围”。我们在这里关注GLUE，但我们认为在您成为NLP专家时，牢记SuperGLUE的存在是很重要的。
- en: We will do some experiments with QQP and STS-B GLUE tasks as illustrative examples
    in the rest of this section. To start off, in the next subsection we demonstrate
    how to fine-tune pretrained BERT on any one of the tasks we have presented. We
    underscore that while we use STS-B as the example fine-tuning task in this case,
    the same sequence of steps is directly applicable for any of the presented tasks.
    We also alert you that this exercise prepares you to fine-tune BERT on your own
    custom dataset from any of the task categories presented.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将以QQP和STS-B GLUE任务做一些实验，作为本节的说明性示例。首先，在下一小节中，我们展示如何对我们提出的任何任务中的一个任务进行微调预训练的BERT。我们强调，虽然在这种情况下，我们使用STS-B作为示例微调任务，但对于任何呈现的任务，相同的步骤序列直接适用。我们还提醒您，此练习是为了准备您在自己的自定义数据集上对BERT进行微调，该数据集来自我们提出的任何任务类别。
- en: 10.2.2 Fine-tuning on a single GLUE task
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 在单个GLUE任务上进行微调
- en: In this subsection, we see how we can quickly fine-tune a pretrained model from
    the transformers family on a task from the GLUE benchmark set. Recall that BERT
    was pretrained on the “fill-in-the-blanks” and “next-sentence prediction” objectives.
    Here, we further fine-tune this pretrained BERT on the STS-B similarity task GLUE
    data. This exercise serves as an example of how you could carry this out on any
    other task in GLUE, as well as on any of your own custom datasets belonging to
    one of these important classes of problems.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们看到如何快速微调transformers家族中的预训练模型，以处理GLUE基准集中的任务。回想一下，BERT是在“填空”和“下一个句子预测”目标上进行预训练的。在这里，我们进一步微调这个预训练的BERT来处理GLUE数据上的STS-B相似性任务。这个练习作为如何在GLUE的任何其他任务以及属于这些重要问题类别之一的任何自定义数据集上进行操作的示例。
- en: 'The first thing we do is clone the transformers repository and install the
    necessary requirements using the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是克隆transformers存储库，并使用以下代码安装必要的要求：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Clones the (specified version of) transformers repository
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 克隆（指定版本的）transformers存储库
- en: ❷ Installs the necessary requirements
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 安装必要的要求
- en: ❸ Fixes the transformers version for reproducibility
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为了可重现性，固定transformers版本
- en: Please ignore dependency conflict messages in our Kaggle notebook—they are irrelevant
    to the libraries we are using here, as long as you fork our notebook instead of
    starting a new one from scratch.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在我们的 Kaggle 笔记本中忽略依赖冲突消息——这些消息与我们在此处使用的库无关，只要您复制我们的笔记本而不是从头开始创建一个新的。
- en: 'Next, download GLUE data as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按以下方式下载 GLUE 数据：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Downloads the GLUE data for all tasks
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 下载所有任务的 GLUE 数据
- en: 'This creates a GLUE directory, with a subdirectory in it named after each GLUE
    task and containing the data for that task. We can take a look at what is contained
    in GLUE/STS-B as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建一个名为 GLUE 的目录，其中包含一个子目录，该子目录以每个 GLUE 任务命名，并包含该任务的数据。我们可以按以下方式查看 GLUE/STS-B
    中包含的内容：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produced the following output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Moreover, we can take a peek at a slice of the STS-B training data as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以用以下方式查看一部分 STS-B 训练数据：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生以下输出：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Before proceeding, we note that in order to use the scripts discussed here to
    fine-tune the model on your own custom data, you just need to convert your data
    into the format shown and point the scripts to its location!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们注意到，为了使用这里讨论的脚本来在您自己的自定义数据上对模型进行精细调优，您只需要将您的数据转换为所示格式并指定脚本所在的位置即可！
- en: 'To fine-tune the “vanilla” `bert-base-cased` BERT checkpoint on the STS-B GLUE
    task for three epochs—with a batch size of 32, a maximum input sequence length
    of 256, and a learning rate of 2e-5—we execute the following command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 STS-B GLUE 任务上对“vanilla” `bert-base-cased` BERT checkpoint 进行三轮训练——批量大小为
    32，最大输入序列长度为 256，学习率为 2e-5——我们执行以下命令：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ This is a “magic” command for timing in a Jupyter notebook.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是 Jupyter 笔记本中计时的“魔法”命令。
- en: 'This takes under 10 minutes to execute. Note that in the code, we specified
    the output directory to be /tmp/STS-B/. This folder contains the fine-tuned model
    and evaluation results. Then, to see the performance that was attained, we simply
    execute the following to print the results to screen:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这一操作执行时间不超过 10 分钟。请注意，在代码中，我们指定了输出目录为 /tmp/STS-B/。该文件夹包含了经过精细调优的模型和评估结果。然后，为了查看所取得的性能，我们只需执行以下命令将结果打印到屏幕上：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This yields the following output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生以下输出：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: These represent the final figures for the metrics used on this problem, namely
    the Pearson and Spearman correlation coefficients. Without delving into too much
    detail, these coefficients measure the correlation between the ground truth similarities
    provided in the dataset and the similarities obtained by our fine-tuned model
    on the test set. Higher values for these coefficients indicate a better model
    due to greater correlation with the ground truth. We see that a performance approaching
    89% is attained for both coefficients. A quick look at the current GLUE leaderboard[^(11)](#pgfId-1258396)
    as of this writing (early October 2020) indicates that the top 20 performances
    recorded worldwide vary between approximately 87% at the low end and 93% at the
    high end. These top performances also perform well on the other tasks in GLUE,
    although we have only fine-tuned on a single task so far. It is nevertheless impressive
    that we can obtain a performance so close to the state of the art this quickly.
    Note from table 10.1 that the amount of training data for this task is only 7,000
    samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代表了用于此问题的度量标准的最终数值，即皮尔逊相关系数和斯皮尔曼相关系数。不深入细节，这些系数衡量了数据集中提供的真实相似度与我们在测试集上精细调优模型获得的相似度之间的相关性。这些系数的较高值表明了更好的模型，因为它们与真实结果的关联更大。我们看到对于这两个系数都达到了接近
    89% 的性能。在撰写本文时（2020 年 10 月初），当前的 GLUE 排行榜[^(11)](#pgfId-1258396)显示，在全球范围内，排名前
    20 的性能大约在 87% 到 93% 之间变化。这些排名靠前的性能也在 GLUE 的其他任务上表现良好，尽管我们目前只对一个任务进行了精细调优。但我们可以快速取得如此接近最新技术水平的性能仍然令人印象深刻。请注意从表
    10.1 中得知，用于此任务的训练数据量仅为 7,000 个样本。
- en: In the next subsection, we will fine-tune the model further on an additional
    task—Quora Question Pairs (QQP)—and thereby further illustrate the concepts of
    multitask learning and sequential adaptation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将进一步在另一个任务——Quora 问题对（QQP）上对模型进行精细调优，并进一步阐明多任务学习和顺序适应的概念。
- en: 10.2.3 Sequential adaptation
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 顺序适应
- en: 'In this subsection, we will see if fine-tuning on the Quora Question Pairs
    (QQP) task, before fine-tuning on the STS-B task, can yield a better performance.
    Recall from table 10.1 that QQP has 364,000 training samples whereas STS-B has
    7,000 samples. Clearly, QQP has considerably more data. Training on QQP first
    can then be interpreted as applying a sequential adaptation multitask learning
    strategy to handle a low-resource scenario where the amount of training data is
    less than ideal: only 7,000 samples.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将看到在STS-B任务上进行微调之前，在Quora问答对（QQP）任务上进行微调是否会产生更好的性能。请回顾表10.1，其中QQP有364,000个训练样本，而STS-B有7,000个样本。显然，QQP具有更多的数据。首先在QQP上训练可以被解释为应用一种顺序适应多任务学习策略来处理一个低资源的场景，其中训练数据量不理想：只有7,000个样本。
- en: 'We start the exercise assuming the transformers repository has been cloned,
    necessary requirements have been installed, and the GLUE data has been downloaded,
    as shown in the previous subsection. Now, the next thing to do is to fine-tune
    the “vanilla” `bert-base-cased` BERT checkpoint on the QQP GLUE task for one epoch,
    with a batch size of 32, a maximum input sequence length of 256, and a learning
    rate of 2e-5\. Note that we use just one epoch this time, instead of three as
    in the previous subsection, because the training data is now so much larger. Each
    epoch—which involves passing over the training set once—now covers 364,000 samples,
    which we gauge to be sufficient. We use the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始本练习，假设已经克隆了transformers存储库，已安装了必要的要求，并已下载了GLUE数据，如前一小节所示。现在，要做的下一件事是在QQP
    GLUE任务上对“普通”的`bert-base-cased` BERT检查点进行微调，一次迭代，批处理大小为32，最大输入序列长度为256，学习率为2e-5。请注意，这次我们只使用一个迭代，而不是前一小节中的三个，因为训练数据现在要大得多。现在每个迭代（涉及一次通过训练集）涵盖了364,000个样本，我们认为这已足够。我们使用以下代码：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The training epoch takes about 2 hours and 40 minutes to execute. As before,
    we can check the attained performance on the QQP task as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时长约为2小时40分钟。与以前一样，我们可以检查QQP任务上的性能如下：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This attains the following performance:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这达到了以下性能：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can then load the QQP-fine-tuned model as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以按以下方式加载QQP微调的模型：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Initializes to our fine-tuned model checkpoint
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化为我们的微调模型检查点
- en: ❷ Uses sequence classification this time, because it is the form of the problem
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这次使用序列分类，因为这是问题的形式
- en: Having loaded the fine-tuned model, let’s extract its encoder so that we can
    use it in a successive model that can then be further fine-tuned on the STS-B
    task. Note that this is similar to the hard-parameter sharing scenario we analyzed
    in chapter 4\. We illustrate this scenario in figure 10.1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了微调模型之后，让我们提取其编码器，以便我们可以在后续模型中使用它，然后可以进一步在STS-B任务上进行微调。请注意，这类似于我们在第4章中分析的硬参数共享场景。我们在图10.1中说明了这种情况。
- en: '![10_01](../Images/10_01.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![10_01](../Images/10_01.png)'
- en: Figure 10.1 The hard-parameter sharing multitask learning scenario we are exploring
    in this section. The model is first fine-tuned on QQP, which is a data-rich scenario,
    and then STS-B, which is a low-resource scenario. The sequential nature of this
    experiment classifies it as sequential adaptation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 我们在本节探讨的硬参数共享多任务学习场景。模型首先在QQP上进行微调，这是一个数据丰富的场景，然后是STS-B，这是一个资源稀缺的场景。这个实验的顺序性质将其分类为顺序适应。
- en: 'The encoder shared between tasks is clearly shown in the figure. The encoder
    is extracted and used to initialize a model for fine-tuning on STS-B using the
    following code snippet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图中清楚地显示了任务之间共享的编码器。编码器被提取并用于初始化一个模型，以进行在STS-B上的微调，代码片段如下：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Gets the fine-tuned QQP model encoder
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取经过微调的QQP模型编码器
- en: ❷ Makes sure the vocabulary and output sizes of an STS-B configuration are set
    to be consistent
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确保STS-B配置的词汇量和输出大小设置一致
- en: ❸ STS-B is a regression problem and requires only one output; QQP is a binary
    classification task and thus has two outputs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ STS-B是一个回归问题，只需要一个输出；QQP是一个二元分类任务，因此有两个输出。
- en: ❹ Initializes the STS-B model with similar settings to QQP
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用与QQP类似的设置初始化STS-B模型
- en: ❺ Set its encoder to the QQP encoder
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[设置其编码器为QQP编码器](https://wiki.example.org/set_qqp_encoder)'
- en: 'Save the initialized STS-B model for further fine-tuning as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将初始化的STS-B模型保存以供进一步微调，方法如下：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Make sure the vocabulary from the QQP model is available, as shown next:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 确保从QQP模型中获取了词汇表，如下所示：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now fine-tune the previously QQP fine-tuned model on STS-B, using the same
    settings as in the previous subsection, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用与前一小节相同的设置，在STS-B上微调先前微调的QQP模型，操作如下：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The three training epochs take only about seven and half minutes to execute,
    given that the training set size is only 7,000\. We check the attained performance
    as usual using the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个训练时代只需大约七分半钟即可执行，只有7,000个训练集大小。像往常一样，我们使用以下内容来检查获得的性能：
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following performance is observed:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到以下性能：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We have attained an improvement over the previous subsection, where fine-tuning
    only on STS-B was carried out. The `eval_corr` attained there was about 88.9%,
    whereas we attain 89.3% here. The successive adaptation multitask learning experiment
    has thus been observed to be beneficial and to have resulted in a measurable improvement
    in performance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经取得了比之前的小节更好的表现，只是在STS-B上进行微调。那里的`eval_corr`约为88.9％，而我们在这里达到了89.3％。因此，连续适应的多任务学习实验被证明具有益处，并导致性能的可衡量提高。
- en: In the next section, we will see if we can fine-tune similarly to new scenarios
    even more efficiently than we did here. We will investigate introducing so-called
    adaptation modules, or adapters, in between the layers of a pretrained language
    model to adapt to new scenarios. This approach holds promise because the number
    of introduced parameters is very small, and they can be pretrained and shared
    by the NLP community efficiently.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '在下一节中，我们将探讨是否可以比我们在这里做得更加高效地将模型微调到新的情况。我们将研究在预训练语言模型的层之间引入称为适应模块或适配器的方法，以适应新情况。这种方法很有前途，因为引入的参数数量非常少，可以有效地预训练和分享NLP社区。 '
- en: 10.3 Adapters
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 适配器
- en: The next adaptation strategy we explore is the use of so-called *adaptation
    modules* or *adapters*. The key idea behind them is shown in figure 10.2, which
    introduces them as additional layers in the vanilla transformer encoder from figure
    7.6 in chapter 7.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索的下一个适应策略是使用所谓的适配模块或适配器。它们背后的关键思想如图10.2所示，介绍了它们作为第7章中图7.6中香草transformers编码器中的附加层。
- en: '![10_02](../Images/10_02.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![10_02](../Images/10_02.png)'
- en: Figure 10.2 Newly introduced adapter layers in the “vanilla” transformer encoder
    from figure 7.6
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2在“香草”transformers编码器的图7.6中新引入的适配器层
- en: As can be seen in the figure, these adapters are newly introduced modules of
    only a few parameters between layers of a pretrained neural network. Fine-tuning
    the modified model for new tasks requires training only these few additional parameters—the
    weights of the original network are kept the same. Virtually no loss in performance,
    compared to fine-tuning the entire model, is often observed when adding just 3-4%
    additional parameters per task.[^(12)](#pgfId-1258480) In practice, this additional
    number of parameters is equivalent to the disk space of about 1 additional megabyte,
    which is very low by modern standards.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，这些适配器是预训练神经网络层之间仅有几个参数的新引入模块。为了将修改后的模型微调到新任务上，只需要训练这些少量的额外参数——原始网络的权重保持不变。与微调整个模型相比，通常仅添加每个任务3-4％的额外参数，几乎没有性能损失。[^(12)](#pgfId-1258480)
    实际上，这些额外参数相当于大约1兆字节的磁盘空间，这在现代标准下非常低。
- en: These adapters are modular, allowing for ready extendibility and easy sharing
    of experience among researchers. In fact, a project named AdapterHub,[^(13)](#pgfId-1258485)
    which is built on the transformers library we have been using, aims to be the
    central repository for sharing such modules. In this section, we will use this
    project to build a BERT model fine-tuned on the Stanford Sentiment Treebank (SST2)
    task. This is equivalent to what we did in the previous section when fine-tuning
    on the STS-B subset of GLUE and will allow you to quickly gain an appreciation
    for the advantages afforded by the adapter framework versus what we did before.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些适配器是模块化的，允许易于扩展和研究人员之间的经验共享。实际上，一个名为AdapterHub[^(13)](#pgfId-1258485)的项目是在我们使用的transformers库上构建的，旨在成为共享这些模块的中央存储库。在该部分中，我们将使用此项目构建在斯坦福情感树库（SST2）任务上微调的BERT模型。这相当于我们在先前的小节中微调STS-B
    GLUE子集所做的事情，将使您迅速了解适配器框架所提供的优势与我们之前所做的有何不同。
- en: 'Let’s install the AdapterHub library as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按以下方式安装AdapterHub库：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Import the required classes and load the required adapter with just three lines
    of code as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需类并仅使用三行代码加载所需的适配器，如下所示：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Checkpoint to fine-tune
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶要微调的检查点
- en: ❷ Task-specific adapter selection specification
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 任务特定的适配器选择规范
- en: Available adapters and usage instructions are listed on the AdapterHub website.[^(14)](#pgfId-1258498)
    That is literally all we had to do to adapt the BERT checkpoint to the SST2 sentiment-classification
    task. Comparing this with our fine-tuning steps from the previous section should
    make the utility of the adapter methodology obvious. Instead of fine-tuning, we
    just load additional modules and keep moving!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器和使用说明在AdapterHub网站上列出 [^(14)](#pgfId-1258498) 这就是我们要对BERT检查点适应SST2情感分类任务所做的一切。将这与上一节的微调步骤进行比较，就可以显而易见地看出适配器方法的实用性。我们无需进行微调，只需加载附加的模块即可继续前进！
- en: Note that in our code we used the `bert-base-uncased` checkpoint, and the adapter
    we are loading was fine-tuned on the UKP Sentential Argument Mining Corpus,[^(15)](#pgfId-1258504)
    due to the constraints of what is currently available in the AdapterHub repository.
    AdapterHub is an early-stage project, and we expect that significantly more adapters
    will be made available over time. At the time of writing in October of 2020, close
    to 200 adapters are available.[^(16)](#pgfId-1258508)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们的代码中，我们使用了`bert-base-uncased`检查点，并且我们要加载的适配器是在UKP句子论证挖掘语料库上进行了微调 [^(15)](#pgfId-1258504)，这是因为目前AdapterHub存储库中只有部分可用的内容。AdapterHub是一个早期项目，我们预计随着时间的推移会提供更多的适配器。在撰写本文的2020年10月，已经有接近200个适配器可用。[^(16)](#pgfId-1258508)
- en: 'As a final action of this section and the chapter, let’s convince ourselves
    that the model we have built here is actually working as a sentiment-classification
    engine. We do this using the following code snippet, which compares the sentiment
    of two sentences: “That was an amazing contribution, good!” and “That is very
    bad for the environment.”'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本节和本章的最终行动，让我们通过以下代码片段来确信我们构建的模型实际上作为情感分类引擎运行。我们使用以下两个句子的情感进行比较：“那是非常出色的贡献，好！”和“那对环境非常糟糕。”
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Uses a regular pretrained tokenizer
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用常规预训练的分词器
- en: ❷ Sentence A
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 句子A
- en: ❸ Sentence B
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 句子B
- en: ❹ Makes prediction A
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 进行了A的预测
- en: ❺ Makes prediction B
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 进行了B的预测
- en: ❻ Displays the prediction probabilities for sentence A
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 显示了句子A的预测概率
- en: ❼ Displays the prediction probabilities for sentence B
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 显示了句子B的预测概率
- en: 'This produces the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The shown predictions can be interpreted as a pair of probabilities, the first
    of which indicates the probability of the input being “negative” and the second,
    the probability of being “positive.” We see that the sentence “That was an amazing
    contribution, good!” is strongly positive with a probability of 99.9%. The sentence
    “That is very bad for the environment,” on the other hand, is negative, with a
    probability of 81.6%. That certainly makes sense and validates our experiment.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 所示的预测可以被解释为一对概率，第一个指示了输入是“负面”的概率，第二个是“正面”的概率。我们看到句子“那是非常出色的贡献，好！”有99.9%的强烈正面概率。另一方面，句子“那对环境非常糟糕。”则是负面的，概率为81.6%。这当然是合理的，并验证了我们的实验。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Applying embedding factorization and parameter sharing across layers yields
    a more parameter-efficient model.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用嵌入因子分解和层间参数共享可以产生更加参数高效的模型。
- en: Fine-tuning a model from the BERT family on multiple tasks simultaneously, that
    is, multitask fine-tuning, yields a more generalizable model.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在BERT系列模型上同时进行多任务微调，也就是多任务微调，会产生更具一般性的模型。
- en: Employing adapters on a model from the BERT family can simplify fine-tuning.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在BERT系列模型上使用适配器可以简化微调。
- en: '1. Z. Lan et al., “ALBERT: A Lite BERT for Self-Supervised Learning of Language
    Representations,” ICLR (2020).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 1. Z. Lan等人，“ALBERT:自监督学习语言表示的Lite BERT”，ICLR(2020)。
- en: '2. A. Wang et al., “Glue: A Multi-Task Benchmark and Analysis Platform for
    Natural Language Understanding,” ICLR (2019).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 2. A. Wang等人，“GLUE:自然语言理解多任务基准和分析平台”，ICLR(2019)。
- en: 3. N. Houlsby et al., “Parameter-Efficient Transfer Learning for NLP,” ICML
    (2019).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 3. N. Houlsby等人，“NLP参数高效迁移学习”，ICML(2019)。
- en: '4. Z. Lan et al., “ALBERT: A Lite BERT for Self-Supervised Learning of Language
    Representations,” ICLR (2020).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 4. Z. Lan等人，“ALBERT:自监督学习语言表示的Lite BERT”，ICLR(2020)。
- en: 5. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 5. [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)
- en: 6. [https://huggingface.co/models?filter=albert](https://huggingface.co/models?filter=albert)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 6. [https://huggingface.co/models?filter=albert](https://huggingface.co/models?filter=albert)
- en: 7. X. Liu et al., “Multi-Task Deep Neural Networks for Natural Language Understanding,”
    ACL Proceedings (2019).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 7. X. Liu等，“用于自然语言理解的多任务深度神经网络”，ACL会议记录（2019）。
- en: '8. A. Wang et al., “GLUE: A Multi-Task Benchmark and Analysis Platform for
    Natural Language Understanding,” ICLR (2019).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 8. A. Wang等，“GLUE：自然语言理解的多任务基准和分析平台”，ICLR（2019）。
- en: '9. P. Rajpurkar et al., “SQuAD: 100,000+ Questions for Machine Comprehension
    of Text,” arXiv (2016).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 9. P. Rajpurkar等，“SQuAD：用于机器文本理解的100,000+问题”，arXiv（2016）。
- en: '10. A. Wang et al., “Glue: A Multi-Task Benchmark and Analysis Platform for
    Natural Language Understanding,” ICLR (2019).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 10. A. Wang等，“GLUE：自然语言理解的多任务基准和分析平台”，ICLR（2019）。
- en: 11. [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 11. [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)
- en: 12. N. Houlsby et al., “Parameter-Efficient Transfer Learning for NLP,” ICML
    (2019).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 12. N. Houlsby等，“用于NLP的参数高效迁移学习”，ICML（2019）。
- en: 13. [https://adapterhub.ml/](https://adapterhub.ml/)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 13. [https://adapterhub.ml/](https://adapterhub.ml/)
- en: 14. [https://adapterhub.ml/explore](https://adapterhub.ml/explore)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 14. [https://adapterhub.ml/explore](https://adapterhub.ml/explore)
- en: 15. [http://mng.bz/7j0e](http://mng.bz/7j0e)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 15. [http://mng.bz/7j0e](http://mng.bz/7j0e)
- en: 16. [https://adapterhub.ml/explore](https://adapterhub.ml/explore)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 16. [https://adapterhub.ml/explore](https://adapterhub.ml/explore)
