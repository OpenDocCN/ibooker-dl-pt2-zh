- en: 12 Machine learning pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 机器学习管道
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Understanding machine learning pipelines with experiment management and hyperparameter
    optimization
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解具有实验管理和超参数优化的机器学习管道
- en: Implementing Docker containers for the DC taxi model to reduce boilerplate code
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了减少样板代码，为 DC 出租车模型实现 Docker 容器
- en: Deploying a machine learning pipeline to train the model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署机器学习管道以训练模型
- en: 'Thus far, you have learned about the individual stages or steps of machine
    learning in isolation. Focusing on one step of machine learning at a time helped
    to concentrate your effort on a more manageable scope of work. However, to deploy
    a production machine learning system it is necessary to integrate these steps
    into a single pipeline: the outputs of a step flowing into the inputs of the subsequent
    steps of the pipeline. Further, the pipeline should be flexible enough to enable
    the hyperparameter optimization (HPO) process to manage and to experiment with
    the specific tasks executed across the stages of the pipeline.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了机器学习的各个独立阶段或步骤。一次只专注于机器学习的一个步骤有助于集中精力处理更可管理的工作范围。然而，要部署一个生产机器学习系统，有必要将这些步骤集成到一个单一的管道中：一个步骤的输出流入到管道后续步骤的输入中。此外，管道应该足够灵活，以便启用超参数优化（HPO）过程来管理并对管道各阶段执行的具体任务进行实验。
- en: In this chapter, you will learn about the concepts and the tools you can use
    to integrate the machine learning pipeline, deploy it to AWS, and train a DC Taxi
    fare estimation machine learning model using experiment management and hyperparameter
    optimization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解到用于集成机器学习管道、部署到 AWS 并使用实验管理和超参数优化训练 DC 出租车车费估算机器学习模型的概念和工具。
- en: 12.1 Describing the machine learning pipeline
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 描述机器学习管道
- en: This section introduces the core concepts needed to explain the machine learning
    pipeline implementation described in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了解释本章描述的机器学习管道实现所需的核心概念。
- en: To clarify the scope of the machine learning pipeline described in this chapter,
    it is helpful to start with the description of the inputs and outputs of the entire
    pipeline. On the input side, the pipeline expects a data set produced from exploratory
    data analysis (EDA) and data quality (data cleanup) processes. The output of the
    machine learning pipeline is one or more trained machine learning model(s), meaning
    that the scope of the pipeline excludes the steps from the deployment of the model
    to production. Since the inputs and the outputs of the pipeline require either
    human-computer interaction (EDA and data quality) or repeatable automation (model
    deployment), they are both out of scope for HPO.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清本章描述的机器学习管道的范围，有助于从整个管道的输入和输出描述开始。在输入方面，管道期望的是从探索性数据分析（EDA）和数据质量（数据清理）过程产生的数据集。机器学习管道的输出是一个或多个训练好的机器学习模型，这意味着管道的范围不包括将模型部署到生产环境的步骤。由于管道的输入和输出要求人机交互（EDA
    和数据质量）或可重复自动化（模型部署），它们都不在 HPO 的范围内。
- en: For an illustration of the desired features of a machine learning pipeline look
    at figure 12.1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解机器学习管道所需的期望特性，请参考图 12.1。
- en: '![12-01](Images/12-01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![12-01](Images/12-01.png)'
- en: Figure 12.1 A unified machine learning pipeline enables hyperparameter optimization
    at every stage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 一个统一的机器学习管道可以在每个阶段进行超参数优化。
- en: In the diagram, the data preparation, feature engineering, and machine learning
    model training stages are managed by HPO. Using HPO-managed stages may result
    in experiments about whether
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，数据准备、特征工程和机器学习模型训练阶段由 HPO 管理。使用由 HPO 管理的阶段可能会导致关于是否
- en: During the data preparation stage, the training examples with missing numeric
    features are dropped from the training data set or are updated to replace missing
    values with the expected (mean) values for the features
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据准备阶段，具有缺失数值特征的训练示例将从训练数据集中删除或更新为将缺失值替换为特征的预期（均值）值
- en: During the feature engineering stage, numeric location features (such as latitude
    or longitude coordinates) are converted into categorical features using binning
    into categories with 64 or 128 distinct values
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征工程阶段，数值位置特征（如纬度或经度坐标）将通过分箱转换为具有 64 或 128 个不同值的分类特征
- en: During the machine learning training stage, the model is trained using stochastic
    gradient descent (SGD) or the Adam optimizer
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习训练阶段，模型使用随机梯度下降（SGD）或 Adam 优化器进行训练
- en: 'Although it may appear that implementing the pipeline in figure 12.1 should
    be complex, by using a collection of PyTorch and complementary frameworks you
    will be able to deploy it by the conclusion of this section. The implementation
    of the pipeline in this section relies on the following technologies:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管实施图 12.1 中的管道可能看起来很复杂，但通过使用一系列 PyTorch 和配套框架，您将能够在本节结束时部署它。本节中管道的实施依赖于以下技术：
- en: '*MLFlow*—For open source experiment management'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MLFlow* — 用于开源实验管理'
- en: '*Optuna*—For hyperparameter optimization'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Optuna* — 用于超参数优化'
- en: '*Docker*—For pipeline component packaging and reproducible execution'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Docker* — 用于管道组件打包和可重复执行'
- en: '*PyTorch Lighting*—For PyTorch machine learning model training and validation'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PyTorch Lightning* — 用于 PyTorch 机器学习模型训练和验证'
- en: '*Kaen*—For provisioning and management of the pipeline across AWS and other
    public cloud providers'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaen* — 用于跨 AWS 和其他公共云提供商的管道供应和管理'
- en: Before proceeding, it is helpful to summarize the key concepts that are going
    to describe the HPO aspects of the pipeline in more detail. The diagram in figure
    12.2 clarifies the relationship across the pipeline, HPO, and associated concepts.
    Experiment management platforms such as MLFlow (other examples include Weights
    & Biases, Comet.ML, and Neptune.AI) store and manage experiment instances such
    that each instance corresponds to a different machine learning pipeline. For example,
    an experiment management platform may store an experiment instance for a machine
    learning pipeline implemented to train DC Taxi fare estimation models and a different
    experiment instance for a machine learning pipeline that trains natural language
    processing models for online chatbots. The experiment instances are isolated from
    each other but are managed by a single experiment management platform.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，总结一下将更详细地描述管道的 HPO 方面的关键概念是有帮助的。图 12.2 中的图表澄清了管道、HPO 和相关概念之间的关系。像 MLFlow
    这样的实验管理平台（其他示例包括 Weights & Biases、Comet.ML 和 Neptune.AI）存储和管理实验实例，以便每个实例对应于不同的机器学习管道。例如，实验管理平台可以为实现训练
    DC 出租车票价估算模型的机器学习管道存储一个实验实例，并为用于在线聊天机器人的自然语言处理模型训练的机器学习管道存储一个不同的实验实例。实验实例彼此隔离，但由单个实验管理平台管理。
- en: '![12-02](Images/12-02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![12-02](Images/12-02.png)'
- en: Figure 12.2 An experiment manager controls the execution of pipeline execution
    (job) instances according to HPO settings.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 实验管理器根据 HPO 设置控制管道执行（作业）实例。
- en: Each experiment instance uses a *parent run* as a collection of one or more
    machine learning pipeline executions (*child runs*). The parent run is configured
    with the settings that apply across multiple pipeline executions, for instance
    the value of the pseudorandom number seed used by an HPO engine such as Optuna.
    The parent run also specifies the total number of child runs (machine learning
    pipeline executions) that should be executed to complete the parent run. Since
    each machine learning pipeline execution also corresponds to a unique combination
    of hyperparameter key/value pairs, the number of the child runs specified by the
    parent run also specifies the total number of HPO trials (sets of values) that
    should be produced by the HPO engine to complete the parent run.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个实验实例使用 *父运行* 作为一个或多个机器学习管道执行（*子运行*）的集合。父运行配置为应用于多个管道执行的设置，例如 HPO 引擎（如 Optuna）使用的伪随机数种子的值。父运行还指定应执行以完成父运行的子运行（机器学习管道执行）的总数。由于每个机器学习管道执行还对应于一组唯一的超参数键/值对的组合，父运行指定的子运行数还指定了
    HPO 引擎应生成的 HPO 试验（值集）的总数，以完成父运行。
- en: The machine learning pipeline code along with the services for experiment management,
    hyperparameter optimization, and machine learning model training are deployed
    as Docker containers interconnected by a virtual private cloud (VPC) network in
    a cloud provider. This deployment is illustrated in figure 12.3.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道代码与实验管理、超参数优化和机器学习模型训练的服务一起部署为 Docker 容器，在云提供商的虚拟专用云（VPC）网络中相互连接。该部署如图
    12.3 所示。
- en: '![12-03](Images/12-03.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![12-03](Images/12-03.png)'
- en: Figure 12.3 A machine learning pipeline with HPO is deployed as a collection
    of Docker containers with at least one management and one worker node as well
    as optional management and worker nodes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 具有 HPO 的机器学习管道部署为包含至少一个管理节点和一个工作节点以及可选管理节点和工作节点的一组 Docker 容器。
- en: As shown in the figure, in order to deploy a machine learning pipeline with
    HPO, at least two Docker containers are connected on a virtual private cloud’s
    network, with at least one manager and at least one worker node in the deployment.
    The manager node(s) host container(s) with the
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，为了部署具有 HPO 的机器学习管道，至少需要两个 Docker 容器在虚拟专用云网络上连接，部署中至少有一个管理器和至少一个工作节点。管理节点托管具有
- en: Experiment management service (e.g., MLFlow)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验管理服务（例如 MLFlow）
- en: HPO engine (e.g., Optuna) running as a service integrated with experiment management
    (e.g., Kaen’s BaseOptunaService)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPO 引擎（例如 Optuna）作为与实验管理集成的服务运行（例如 Kaen 的 BaseOptunaService）
- en: Experiment management user interface
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验管理用户界面
- en: Worker management service for scheduling and orchestration of the machine learning
    pipeline child runs across the worker nodes
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点管理服务，用于在工作节点上安排和编排机器学习管道子运行
- en: The worker node(s) host the Docker containers with the machine learning model
    (e.g., PyTorch code) along with the code describing how to train, validate, and
    test the machine learning model based on the hyperparameters (e.g., PyTorch Lightning
    code).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点托管具有机器学习模型（例如 PyTorch 代码）的 Docker 容器，以及描述如何根据超参数（例如 PyTorch Lightning 代码）训练、验证和测试机器学习模型的代码。
- en: Note that the life cycle of the manager and worker nodes is different from the
    life cycle of the Docker container execution on the nodes. This means that the
    same nodes can host the execution of multiple container instances and multiple
    machine learning pipeline runs without having to be provisioned or de-provisioned.
    Also, while the containers on the management nodes are long running, for example
    to provide the experiment service user interface and hyperparameter optimization
    engine services across multiple machine learning pipeline executions, the containers
    on the worker nodes stay running only for the duration of the machine learning
    pipeline execution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，管理节点和工作节点的生命周期与节点上 Docker 容器执行的生命周期不同。这意味着相同的节点可以托管多个容器实例的执行和多个机器学习管道运行，而无需进行配置或取消配置。此外，管理节点上的容器是长时间运行的，例如为了在多个机器学习管道执行期间提供实验服务用户界面和超参数优化引擎服务，而工作节点上的容器仅在机器学习管道执行期间保持运行。
- en: Although the deployment configuration described in this section may appear complex,
    provisioning of the nodes, machine learning middleware (experiment management,
    hyperparameter optimization, and so on), as well as the orchestration of the machine
    learning pipeline execution across the worker nodes, are handled entirely by the
    Kaen framework and associated Docker containers. You are going to learn more about
    the framework and how to build your machine learning pipeline on top of existing
    Kaen containers later in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本节描述的部署配置可能看起来复杂，但节点的提供、机器学习中间件（实验管理、超参数优化等）以及机器学习管道在工作节点之间的执行的编排完全由 Kaen
    框架和相关的 Docker 容器处理。您将在本章后面学习有关该框架以及如何在现有的 Kaen 容器上构建您的机器学习管道的更多信息。
- en: 12.2 Enabling PyTorch-distributed training support with Kaen
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 使用 Kaen 启用 PyTorch 分布式训练支持
- en: This section illustrates how to add PyTorch-distributed training support using
    the PyTorch DistributedDataParallel class. By the conclusion of this section,
    the train method for the DC taxi fare model will be extended to integrate with
    the Kaen framework for distributed training in a cloud environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本节说明了如何使用 PyTorch DistributedDataParallel 类添加 PyTorch 分布式训练支持。通过本节的结束，DC 出租车费用模型的
    train 方法将被扩展以与云环境中的分布式训练框架 Kaen 集成。
- en: Unlike the code and Jupyter notebook instructions from the previous chapters
    of this book, the code in the remainder of this chapter requires that your environment
    has Docker and Kaen installed. You can find more about installing and getting
    started with Docker in appendix B. To install Kaen to an environment with an existing
    Docker installation, execute
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书前几章的代码和 Jupyter 笔记本说明不同，本章剩余部分的代码要求您的环境已安装 Docker 和 Kaen。有关安装 Docker 和开始使用的更多信息，请参阅附录
    B。要将 Kaen 安装到已存在 Docker 安装的环境中，请执行
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: which will download and install the kaen command line interface (CLI) in your
    shell environment. For example, if Kaen is installed correctly, you can get help
    about the Kaen commands using
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载并安装 kaen 命令行界面（CLI）到您的 shell 环境中。例如，如果 Kaen 安装正确，您可以使用以下命令获取有关 Kaen 命令的帮助：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'which should produce an output resembling the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生类似以下的输出：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To execute the instructions in the remainder of this book, launch a Kaen Jupyter
    environment using
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行本书其余部分的说明，请启动 Kaen Jupyter 环境，使用以下命令：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'from your shell environment, which should launch a specialized Jupyter notebook
    environment as a new Docker container in your local Docker host. The kaen jupyter
    command should also navigate your default browser to the Jupyter home page and
    output text in the shell that resembles the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从您的 shell 环境中执行以下命令，这应该会在您本地的 Docker 主机上启动一个专门的 Jupyter 笔记本环境作为一个新的 Docker 容器。kaen
    jupyter 命令还应该将您的默认浏览器导航到 Jupyter 主页，并在 shell 中输出类似以下的文本：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: which specifies the URL that you can use in your browser to open the newly launched
    Jupyter instance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它指定了您可以在浏览器中使用的 URL，以打开新启动的 Jupyter 实例。
- en: In the Jupyter environment, create and open a new notebook. For example, you
    can name the notebook ch12.ipynb. As the first step in the notebook, you should
    execute the shell command
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 环境中，创建并打开一个新的笔记本。例如，您可以将笔记本命名为 ch12.ipynb。作为笔记本中的第一步，您应该执行 shell 命令
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: to create an src director for your code in this environment. Recall that when
    you use the exclamation sign ! in a Python code cell in Jupyter, the command that
    follows it is executed in the underlying bash shell. So, the result of running
    the code is to create an src directory in the filesystem.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在此环境中为您的代码创建一个 src 目录。请记住，在 Jupyter 中的 Python 代码单元格中使用感叹号！时，其后的命令将在底层的 bash
    shell 中执行。因此，运行该代码的结果是在文件系统中创建一个 src 目录。
- en: Next, save the latest version of the DC taxi model (as described in the chapter
    11) to a model_v1.py file in the src directory using the %%writefile magic.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用 %%writefile 魔术将 DC 出租车模型的最新版本（如第 11 章所述）保存到 src 目录中的 model_v1.py 文件中。
- en: Listing 12.1 Saving the implementation to model_v1.py
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.1 将实现保存到 model_v1.py
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Since the code in listing 12.1 saved version 1 of the DC taxi model to a file
    named model_v1.py, the entry point (in a trainer.py file of the src directory)
    to the process of building and testing this version of the model starts by loading
    the DC taxi model instance from the model_v1 package:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于列表 12.1 中的代码将 DC 出租车模型的版本 1 保存到名为 model_v1.py 的文件中，因此构建和测试该模型版本的过程的入口点（在 src
    目录的 trainer.py 文件中）从加载 model_v1 包中的 DC 出租车模型实例开始：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Initialize the pseudorandom number seed using the hyperparameters or a current
    timestamp.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用超参数或当前时间戳初始化伪随机数种子。
- en: ❷ Automatically update the DC taxi model to take advantage of multiple trainer
    nodes if available.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 自动更新 DC 出租车模型，以利用多个训练节点（如果有的话）。
- en: ❸ As described in chapter 8, in a distributed cluster the shard_size is often
    distinct from . . .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 正如第 8 章中所述，在分布式集群中，shard_size 往往不同于 . . .
- en: ❹ . . . the batch_size used to compute the gradients.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ . . . 用于计算梯度的 batch_size。
- en: At this point you can unit test trainer.py by running the following from your
    shell environment.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时，您可以通过从您的 shell 环境中运行以下命令对 trainer.py 进行单元测试。
- en: Listing 12.2 Run a simple test to confirm that the implementation works as expected.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.2 运行一个简单的测试来确认实现是否按预期工作。
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This should train, test, and report on the metrics of your model using a small
    sample of the DC taxi data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会使用 DC 出租车数据的小样本训练、测试并报告您模型的度量标准。
- en: 12.2.1 Understanding PyTorch-distributed training settings
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 理解 PyTorch 分布式训练设置
- en: This section illustrates the configuration of the environment variables and
    related settings expected by PyTorch models when performing distributed training.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本节说明了执行分布式训练时 PyTorch 模型所期望的环境变量和相关设置的配置。
- en: The distributed training approach for PyTorch models is surprisingly straightforward
    to enable. Although native PyTorch does not provide integration with cloud providers
    such as AWS, Azure, or GCP, the code in listing 12.3 illustrates how to use the
    Kaen framework ([http://kaen.ai](http://kaen.ai)) to bridge PyTorch and PyTorch
    Lightning with distributed training in cloud providers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 模型的分布式训练方法出奇的简单。尽管原生的 PyTorch 不提供与 AWS、Azure 或 GCP 等云提供商的集成，但列表 12.3
    中的代码说明了如何使用 Kaen 框架（[http://kaen.ai](http://kaen.ai)）在云提供商中桥接 PyTorch 和 PyTorch
    Lightning，实现分布式训练。
- en: The PyTorch-specific implementation used by the kaen.torch.init_process_ group
    method enables distributed training for the DC taxi model, as specified by the
    model PyTorch Lightning module, where the PyTorch torch.nn.Sequential layers are
    stored in the model.layers attribute.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: kaen.torch.init_process_ group 方法使用的 PyTorch 特定实现，使得 DC 出租车模型的分布式训练成为可能，如模型
    PyTorch Lightning 模块所指定的，其中 PyTorch torch.nn.Sequential 层存储在 model.layers 属性中。
- en: Listing 12.3 Kaen framework configuring PyTorch model
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 12.3 Kaen 框架配置 PyTorch 模型
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Set PyTorch MASTER_ADDR to localhost address unless otherwise specified by
    Kaen.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 PyTorch 的 MASTER_ADDR 设置为本地主机地址，除非 Kaen 另有规定。
- en: ❷ Set PyTorch MASTER_PORT to 12355 unless otherwise specified in the MASTER_PORT
    variable.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将 PyTorch 的 MASTER_PORT 设置为 12355，除非 MASTER_PORT 变量中另有规定。
- en: ❸ Use the CPU-based gloo backend unless otherwise specified by KAEN_BACKEND.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 除非 KAEN_BACKEND 另有规定，否则使用基于 CPU 的 gloo 后端。
- en: ❹ Initialize the distributed data parallel training rank . . .
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化分布式数据并行训练的等级 . . .
- en: ❺ . . . and the count of training nodes based on KAEN_RANK and KAEN_WORLD_SIZE
    variables.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ . . . 并根据 KAEN_RANK 和 KAEN_WORLD_SIZE 变量设置训练节点的计数。
- en: ❻ Ensure that the distributed training process group is ready to train.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 确保分布式训练进程组已准备好进行训练。
- en: ❼ Enable distributed training for the model using DistributedDataParallel.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用 DistributedDataParallel 为模型启用分布式训练。
- en: When training a PyTorch model using the DistributedDataParallel implementation,
    there are several prerequisites that must be met before the training can start.
    First, the distributed training library must be configured with the MASTER_ADDR
    and MASTER_PORT environment variables for the model training manager node on the
    network. These values must be specified even when DistributedDataParallel is used
    in a scenario with a single node. In a single-node scenario, MASTER_ADDR and MASTER_
    PORT are initialized to the values 127.0.0.1 and 12355, respectively. When the
    distributed training cluster consists of more than a single node, MASTER_ADDR
    must correspond to the IP address of the manager node (node rank 0, per the description
    in chapter 11) in the distributed node in the cluster.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 DistributedDataParallel 实现训练 PyTorch 模型时，在训练开始之前必须满足几个先决条件。首先，必须为网络上的模型训练管理节点配置分布式训练库的
    MASTER_ADDR 和 MASTER_PORT 环境变量。即使在单节点场景中使用 DistributedDataParallel，也必须指定这些值。在单节点场景中，MASTER_ADDR
    和 MASTER_ PORT 的值分别初始化为 127.0.0.1 和 12355。当分布式训练集群由多个节点组成时，MASTER_ADDR 必须对应于集群中的管理节点的
    IP 地址（根据第 11 章中的描述，即节点等级为 0 的节点）。
- en: The Kaen framework can initialize the runtime environment of your model training
    environment with the runtime IP address for the manager node used for PyTorch
    training. Hence, in the example, MASTER_ADDR is initialized to the value of KAEN_
    JOB_MANAGER_IP if the latter environment variable is set by the Kaen framework
    and to 127.0.0.1 (for single-node training) otherwise. In the example, MASTER_PORT
    is initialized to 12355 by default unless a different value is preset before starting
    the training runtime.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Kaen 框架可以使用 PyTorch 训练的管理节点的运行时 IP 地址初始化您的模型训练环境。因此，在示例中，如果 Kaen 框架设置了后者的环境变量，则将
    MASTER_ADDR 初始化为 KAEN_ JOB_MANAGER_IP 的值，否则将其初始化为 127.0.0.1（用于单节点训练）。在示例中，默认情况下将
    MASTER_PORT 初始化为 12355，除非在启动训练运行时之前预先设置了不同的值。
- en: Notice that the init_method parameter to the init_process_group method is hardcoded
    to env:// to ensure that the distributed training initialization happens according
    to the values of the MASTER_ADDR and MASTER_PORT environment variables described
    earlier. Although it is possible to use a file or a key/value store for initialization,
    the environment-based approach is demonstrated in this example because it is natively
    supported by the Kaen framework.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，init_process_group 方法的 init_method 参数被硬编码为 env://，以确保分布式训练初始化根据前述的 MASTER_ADDR
    和 MASTER_PORT 环境变量的值发生。虽然可以使用文件或键/值存储进行初始化，但在本示例中演示了基于环境的方法，因为它是 Kaen 框架本地支持的。 '
- en: 'In addition to the initialization method, notice that init_process_group is
    invoked with the values for the BACKEND, WORKER, and REPLICAS settings. The BACKEND
    setting corresponds to the name of one of several distributed communication backend
    libraries supported by PyTorch. (The details of the features supported by the
    libraries are available here: [https://pytorch.org/docs/stable/distributed.html](https://pytorch.org/docs/stable/distributed.html).)
    gloo is used to enable distributed training over CPUs while nccl is used for GPU-based
    distributed training. Since CPU-based distributed training is easier, cheaper,
    and often faster to provision in cloud providers like AWS, this chapter focuses
    on CPU-based training first and then on how to introduce the changes needed to
    support GPU-based training.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了初始化方法之外，请注意 init_process_group 是使用 BACKEND、WORKER 和 REPLICAS 设置的值调用的。BACKEND
    设置对应于 PyTorch 支持的几个分布式通信后端库之一的名称。 （这些库支持的特性的详细信息在此处可用：[https://pytorch.org/docs/stable/distributed.html](https://pytorch.org/docs/stable/distributed.html)。）gloo
    用于启用基于 CPU 的分布式训练，而 nccl 则用于基于 GPU 的分布式训练。由于基于 CPU 的分布式训练更容易、更便宜，并且通常更快地在云提供商（如
    AWS）中预配，因此本章首先关注基于 CPU 的训练，然后再介绍如何引入支持基于 GPU 的训练所需的更改。
- en: The values for RANK and WORLD_SIZE needed to initialize distributed training
    are also provided by the Kaen framework. The WORLD_SIZE value corresponds to a
    natural count (i.e., starting with one) of the integer count of the nodes used
    in distributed training, while the RANK value corresponds to a zero-based integer
    index of the node executing the Python runtime training in the PyTorch model.
    Note that both RANK and WORLD_SIZE are initialized based on the Kaen’s framework’s
    environment variable settings. For example, if you instantiate a Kaen training
    environment with only a single training node, then KAEN_WORLD_SIZE is set to 1
    while the RANK value of the single training node is set to 0. In contrast, for
    a distributed Kaen training environment consisting of 16 nodes, KAEN_WORLD_SIZE
    is initialized to 16 and each of the training nodes is assigned a RANK value in
    the range from [0, 15], in other words inclusive of both the start (0) index and
    the end (15) index.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化分布式训练所需的 RANK 和 WORLD_SIZE 值也由 Kaen 框架提供。WORLD_SIZE 值对应于用于分布式训练的节点的整数计数的自然计数（即，从一开始），而
    RANK 值对应于在 PyTorch 模型中执行 Python 运行时训练的节点的从零开始的整数索引。请注意，RANK 和 WORLD_SIZE 都是根据
    Kaen 框架的环境变量设置进行初始化的。例如，如果您实例化一个仅包含单个训练节点的 Kaen 训练环境，则 KAEN_WORLD_SIZE 设置为 1，而单个训练节点的
    RANK 值设置为 0。相比之下，对于由 16 个节点组成的分布式 Kaen 训练环境，KAEN_WORLD_SIZE 初始化为 16，并且每个训练节点分配了范围为
    [0, 15] 的 RANK 值，换句话说，包括起始（0）索引和结束（15）索引。
- en: Lastly, notice that the DistributedDataParallel training is initialized only
    once, after checking the is_initialized status. The initialization involves executing
    init_ process_group using the backend, rank, and world_size settings described
    earlier in this section. Once the initialization completes (in other words, the
    init_process_ group returns), the DistributedDataParallel instance is wrapped
    around the PyTorch nn.Module-based model instance, which is assigned to model.nn
    in the example. At this point, the model is ready to be trained by a distributed
    cluster.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，仅在检查 is_initialized 状态之后才初始化 DistributedDataParallel 训练。初始化涉及使用此部分前述的
    backend、rank 和 world_size 设置执行 init_process_group。一旦初始化完成（换句话说，init_process_group
    返回），就会将 DistributedDataParallel 实例包装在基于 PyTorch nn.Module 的模型实例周围，并将其分配给示例中的 model.nn。此时，模型已准备好通过分布式集群进行训练。
- en: 12.3 Unit testing model training in a local Kaen container
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 在本地 Kaen 容器中对模型训练进行单元测试
- en: This section describes how to unit test the model implementation in a local
    Kaen container prior to deploying the code to a cloud environment like AWS.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分描述了如何在本地的 Kaen 容器中对模型实现进行单元测试，然后再将代码部署到像 AWS 这样的云环境中。
- en: Although the code implementation supports distributed training, it can be tested
    without having to provision (and pay for) a distributed training environment in
    a cloud provider. You will start the unit test by downloading a Kaen-provided
    base container image provided for PyTorch models targeted at AWS.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码实现支持分布式训练，但无需在云提供商中预留（和支付）分布式训练环境即可进行测试。您将通过下载为面向 AWS 的 PyTorch 模型提供的 Kaen
    提供的基础容器镜像来开始单元测试。
- en: 'Ensure that you can authenticate with DockerHub where you can download the
    base container image. Once you execute the following code snippet in your Kaen
    Jupyter environment, you will be prompted to enter your DockerHub username, which
    is then stored in the DOCKER_HUB_USER Python variable:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您可以使用 DockerHub 进行身份验证，您可以下载基础容器镜像。一旦您在 Kaen Jupyter 环境中执行以下代码片段，您将被提示输入您的
    DockerHub 用户名，然后将其存储在 DOCKER_HUB_USER Python 变量中：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, enter the DockerHub password for your username when prompted. Notice
    that the password is cleared out from the DOCKER_HUB_PASSWORD variable after the
    authentication is finished:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在提示时输入您的用户名的 DockerHub 密码。请注意，验证完成后，密码将从 DOCKER_HUB_PASSWORD 变量中清除出来：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You should see an output with the message Login Succeeded if you specified valid
    DockerHub credentials.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您指定了有效的 DockerHub 凭据，您应该看到一个显示“登录成功”的输出消息。
- en: The base PyTorch Docker image is quite large, about 1.9 GB. The Kaen-based PyTorch
    image (kaenai/pytorch-mlflow-aws-base:latest), which adds binaries with support
    for AWS and MLFlow, is roughly 2 GB in size, so be prepared that the following
    download will take a few minutes, depending on the speed of your internet connection.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 PyTorch Docker 镜像相当庞大，大约为 1.9 GB。基于 Kaen 的 PyTorch 镜像 (kaenai/pytorch-mlflow-aws-base:latest)，添加了支持
    AWS 和 MLFlow 的二进制文件，大约大小为 2 GB，所以请做好准备，以下下载将根据您的互联网连接速度需要几分钟。
- en: To execute the download, run
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行下载，请运行
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once the download completes, you can package your source code to an image derived
    from kaenai/pytorch-mlflow-aws-base:latest using the following Dockerfile. Notice
    that the file simply copies the Python source code to the /workspace directory
    of the image filesystem:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，您可以使用以下 Dockerfile 将您的源代码打包到一个从 kaenai/pytorch-mlflow-aws-base:latest
    派生的镜像中。请注意，该文件只是将 Python 源代码复制到镜像文件系统的 /workspace 目录中：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since the source code files model_v1.py and trainer.py described earlier in
    this chapter were saved to an src directory, notice that the following command
    to build your Docker image uses the src/ directory as the root of the Docker image
    build process. To ensure that the image you build can be uploaded to DockerHub,
    the image is tagged using {DOCKER_HUB_USER} as a prefix:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章早些时候描述的源代码文件 model_v1.py 和 trainer.py 被保存到了一个 src 目录中，请注意以下命令构建您的 Docker
    镜像时将 src/ 目录作为 Docker 镜像构建过程的根目录。为了确保您构建的镜像可以上传到 DockerHub，镜像使用 {DOCKER_HUB_USER}
    作为前缀进行标记：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After the docker build command is finished, you can run you newly created Docker
    container using
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 docker build 命令完成后，您可以使用以下命令运行您新创建的 Docker 容器
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: which should produce an output identical to the output of listing 12.2\. Why
    should you bother creating the Docker image? Recall that having the Docker image
    will simplify deployment and training of your model in the cloud provider environment
    such as AWS. How will the image be shared from your local environment to the cloud
    provider environment? In general, Docker images are shared using Docker Registry
    instances such as DockerHub.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生一个与列表 12.2 输出相同的输出。为什么要费心创建 Docker 镜像呢？回想一下，拥有 Docker 镜像将简化在诸如 AWS 等云服务提供商环境中部署和训练模型的过程。如何将镜像从您的本地环境共享到云服务提供商环境？通常，Docker
    镜像是使用 DockerHub 等 Docker Registry 实例进行共享的。
- en: To push (upload) your newly built image to DockerHub, execute
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要推送（上传）您新构建的镜像到 DockerHub，执行
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: which should complete in just a few seconds since the docker push operation
    will need to push only the content of the source code (Python files) to DockerHub.
    The rest of your dctaxi image is mounted from the base kaenai/pytorch-mlflow-aws-base:latest
    image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 docker push 操作只需要推送源代码（Python 文件）的内容到 DockerHub，所以应该在几秒钟内完成。您的 dctaxi 镜像的其余部分是从基础
    kaenai/pytorch-mlflow-aws-base:latest 镜像挂载的。
- en: 12.4 Hyperparameter optimization with Optuna
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 使用 Optuna 进行超参数优化
- en: This section teaches about Optuna for HPO and how to use the Kaen framework
    to add support for HPO to the DC taxi fare estimation model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了用于 HPO 的 Optuna 和如何使用 Kaen 框架为 DC 出租车车费估算模型添加 HPO 支持。
- en: Thus far, you have been unit testing your implementation using a static set
    of hyperparameter values for model training. Recall from chapter 11 that you can
    use Optuna to perform hyperparameter optimization (HPO) for your code.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经使用静态的超参数值对模型训练进行了单元测试。回想一下第 11 章，您可以使用 Optuna 对您的代码执行超参数优化（HPO）。
- en: 'Optuna is one of several HPO frameworks supported by Kaen. To incorporate support
    for HPO in distributed training, you need to use one of the Kaen-based Docker
    images that expose Optuna as a service to your code and implement a sub-classable
    Python class named BaseOptunaService. Recall from chapter 11 that hyperparameters
    in Optuna are specified using the trial API. The BaseOptunaService in Kaen provides
    access to the Optuna trial instance to subclasses of BaseOptunaService. For example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 是 Kaen 支持的几种 HPO 框架之一。要在分布式训练中纳入对 HPO 的支持，您需要使用一个将 Optuna 作为服务将其公开给您的代码的基于
    Kaen 的 Docker 映像，并实现一个可子类化的 Python 类命名为 BaseOptunaService。回想一下，Optuna 中的超参数是使用试验
    API 指定的。Kaen 中的 BaseOptunaService 提供了对 BaseOptunaService 子类中的 Optuna 试验实例的访问。例如：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The _trial attribute references an Optuna trial instance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ _trial 属性引用 Optuna 试验实例。
- en: ❷ The trial instance supports Optuna trial API methods such as suggest_int.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 试验实例支持 Optuna 试验 API 方法，例如 suggest_int。
- en: 'Notice that there is a single hyperparameter requested from Optuna in the dictionary
    instance returned by the hparams method. The suggest_int method is one of several
    methods available from the Optuna trial API to obtain a value for a hyperparameter.
    (Other methods available from the trial interface are described here: [https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#).)
    In the example, the suggest_int(''seed'', 0, np.iinfo(np.int32).max) method specifies
    that Optuna should recommend values for the pseudorandom number seed generator
    from 0 up to and including the maximum positive 32-bit integer.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 hparams 方法返回的字典实例中，有一个超参数向 Optuna 请求。suggest_int 方法是 Optuna 试验 API 中可用的几种方法之一，用于获取超参数的值。
    （试验接口中可用的其他方法在这里描述：[https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#)。）
    在本例中，suggest_int('seed', 0, np.iinfo(np.int32).max) 方法指定 Optuna 应推荐从 0 到包括正32位整数的最大值的伪随机数种子生成器的值。
- en: 'Recall that the training of the DcTaxiModel depends on additional hyperparameter
    values, including optimizer, bins, lr (the learning rate), num_hidden_neurons,
    batch_size, and max_batches. The implementation of these hyperparameters using
    the Optuna trial API was covered in chapter 11\. To enable support for these hyperparameters
    in the implementation of the DcTaxiHpoService class, you need to expand the dictionary
    returned by the hparams method with the Optuna specification for the hyperparameter
    values that should be tried during HPO:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，DcTaxiModel 的训练还依赖于额外的超参数值，包括 optimizer、bins、lr（学习率）、num_hidden_neurons、batch_size
    和 max_batches。本书第 11 章介绍了使用 Optuna 试验 API 实现这些超参数。要在 DcTaxiHpoService 类的实现中启用对这些超参数的支持，您需要扩展
    hparams 方法返回的字典，使用 Optuna 对应超参数值的规范来尝试：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In addition to a trial, Optuna uses a concept of a *study* (corresponding to
    an MLFlow parent run), which is a collection of trials. In the Kaen framework,
    Optuna studies are used to generate reports about the summary statistics of the
    trials and to generate reports in the form of custom visualizations of the completed
    trials.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了试验，Optuna 还使用了一个“研究”（相当于 MLFlow 父运行），它是试验的集合。在 Kaen 框架中，Optuna 研究用于生成关于试验摘要统计信息的报告，以及生成形式为已完成试验的自定义可视化报告。
- en: 'To persist the summary statistics about the trials, you can use the trials_dataframe
    method of the Optuna study API, which returns a pandas DataFrame describing the
    completed trials along with summary statistics of the associated hyperparameter
    values. Notice that in the following example the data frame is persisted to an
    html file based on the name of the experiment:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要持久化试验摘要统计信息，您可以使用 Optuna 研究 API 的 trials_dataframe 方法，该方法返回一个 pandas DataFrame，描述了已完成试验以及关联超参数值的摘要统计信息。请注意，在下面的示例中，数据帧基于实验名称持久化为
    html 文件：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the example, the calls to the Optuna APIs are executed in the content of
    the on_ experiment_end method, which is, unsurprisingly, invoked by the BaseOptunaService
    base class after the conclusion of the experiment. After persisting the html file
    with the summary statistics of the experiment, the remainder of the method’s implementation
    generates and persists visualizations of the study using the Optuna visualization
    package ([http://mng.bz/4Kxw](http://mng.bz/4Kxw)). Notice that for each visualization
    the corresponding image is persisted to a png file.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，对 Optuna API 的调用是在 on_experiment_end 方法的上下文中执行的，该方法在实验结束后由 BaseOptunaService
    基类调用。在将包含实验摘要统计信息的 html 文件持久化后，方法的剩余部分生成并持久化使用 Optuna 可视化包 ([http://mng.bz/4Kxw](http://mng.bz/4Kxw))
    的研究的可视化效果。请注意，对于每个可视化效果，相应的图像都会持久化到一个 png 文件中。
- en: The mlflow_client in the code acts as a generic reference to the MLFlow Client
    API ([http://mng.bz/QqjG](http://mng.bz/QqjG)), enabling reads from and writes
    to MLFlow as well as to monitoring the progress of the experiment. The parent_run
    variable is a reference to the “parent” run, or, in other words, a collection
    of trials or executions of the experiment with specific configuration of hyperparameter
    values suggested by the Optuna HPO service.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的 mlflow_client 充当对 MLFlow 客户端 API ([http://mng.bz/QqjG](http://mng.bz/QqjG))
    的通用引用，使得可以从 MLFlow 中读取和写入数据，并监视实验的进展。parent_run 变量是对“父”运行的引用，或者说是具有 Optuna HPO
    服务建议的特定超参数值配置的一系列试验或执行。
- en: 'The entire HPO implementation described in this chapter is shown in the following
    code snippet. Notice that the snippet saves the implementation source code as
    an hpo.py file in your src folder:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述的整个 HPO 实现如下代码片段所示。请注意，该片段将实现源代码保存为 src 文件夹中的 hpo.py 文件：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the source code in place, you are ready to package it as a Docker container.
    Start by pulling a base Kaen container for Optuna and MLFlow:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有了源代码，你就可以将其打包成一个 Docker 容器。首先拉取一个用于 Optuna 和 MLFlow 的基本 Kaen 容器：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once that’s finished, create a Dockerfile for a derived image using
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，使用以下命令为派生图像创建一个 Dockerfile：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Notice that the package prefix for your DcTaxiHpoService implementation corresponds
    to the filename hpo.py, as specified by the KAEN_HPO_SERVICE_NAME and the KAEN_HPO_SERVICE_PREFIX
    environment variables, respectively. Once the Dockerfile is saved, build the image
    by running
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你的 DcTaxiHpoService 实现的软件包前缀对应于文件名 hpo.py，分别由 KAEN_HPO_SERVICE_NAME 和 KAEN_HPO_SERVICE_PREFIX
    环境变量指定。保存 Dockerfile 后，通过以下命令构建图像：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: and push it to DockerHub using
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 并将其推送到 DockerHub：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 12.4.1 Enabling MLFlow support
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.1 启用 MLFlow 支持
- en: This section describes how to add integration between your DcTaxiModel and the
    MLFlow framework in order to manage and track HPO experiments.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述如何在你的 DcTaxiModel 和 MLFlow 框架之间添加集成，以管理和跟踪 HPO 实验。
- en: Although the base kaenai/pytorch-mlflow-aws-base:latest image includes support
    for MLFlow, the implementation of training in trainer.py does not take advantage
    of MLFlow experiment management and tracking. Since MLFlow uses the concept of
    an experiment to organize a collection of HPO trials and run, Kaen provides a
    BaseMLFlowClient class, which can be used to implement an MLFlow-managed experiment
    for DcTaxiModel. The subclasses of BaseMLFlowClient are responsible for instantiating
    the untrained PyTorch model instances using the hyperparameter values that BaseMLFlowClient
    fetches from MLFlow and Optuna.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基本 kaenai/pytorch-mlflow-aws-base:latest 图像包含对 MLFlow 的支持，但 trainer.py 中的训练实现没有利用
    MLFlow 实验管理和跟踪功能。由于 MLFlow 使用实验的概念来组织一系列 HPO 试验和运行，Kaen 提供了一个 BaseMLFlowClient
    类，可以用来为 DcTaxiModel 实现一个由 MLFlow 管理的实验。BaseMLFlowClient 的子类负责使用 BaseMLFlowClient
    从 MLFlow 和 Optuna 获取的超参数值实例化未训练的 PyTorch 模型实例。
- en: 'Start by saving an instance of your BaseMLFlowClient subclass named DcTaxiExperiment
    by running the following in your Kaen Jupyter environment:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在你的 Kaen Jupyter 环境中运行以下命令保存名为 DcTaxiExperiment 的 BaseMLFlowClient 子类的一个实例：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This saves the code to train your model to the src/experiment.py file.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这将代码保存到 src/experiment.py 文件中。
- en: With the experiment support in place, you are ready to build the updated dctaxi
    image using
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有了实验支持，你就可以使用以下命令构建更新的 dctaxi 图像：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: which specifies a new entry point into the image using experiment.DcTaxiExperiment
    from experiment.py by changing the default values of the KAEN_HPO_CLIENT_PREFIX
    and the KAEN_HPO_CLIENT_NAME environment variables.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 它指定了一个新的入口点进入镜像，使用 experiment.py 中的 experiment.DcTaxiExperiment 来更改 KAEN_HPO_CLIENT_PREFIX
    和 KAEN_HPO_CLIENT_NAME 环境变量的默认值。
- en: As previously, build your dctaxi image using
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，使用以下命令构建你的 dctaxi 镜像。
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: and push it to DockerHub using
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用以下命令将其推送到 DockerHub。
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 12.4.2 Using HPO for DcTaxiModel in a local Kaen provider
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.2 在本地 Kaen 提供程序中为 DcTaxiModel 使用 HPO
- en: At this point, you are prepared to build a Docker container capable of suggesting
    hyperparameter optimization trials and managing the associated experimental runs
    of the trials. In the container, the hyperparameter values are suggested by Optuna,
    and the experiments based on the values are managed by MLFlow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你已经准备好构建一个能够建议超参数优化试验并管理试验运行的 Docker 容器。在容器中，超参数值由 Optuna 建议，并且基于这些值的试验由
    MLFlow 管理。
- en: Before provisioning the more expensive cloud provider, it is a good idea to
    start by provisioning a local Kaen provider so that you can unit test your HPO
    and model training code. You can create a Kaen training *dojo* by executing
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置更昂贵的云提供程序之前，最好先通过配置本地 Kaen 提供程序来开始，以便你可以对 HPO 和模型训练代码进行单元测试。你可以通过执行以下命令创建一个
    Kaen 训练 *道场*。
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: which should return an alphanumeric identifier for the newly created Kaen dojo.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该返回新创建的 Kaen 道场的字母数字标识符。
- en: You can list available Kaen dojos in your workspace using
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令列出工作空间中可用的 Kaen 道场。
- en: '[PRE30]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: which should print out the ID of the dojo you just created.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该打印出你刚刚创建的道场的 ID。
- en: 'You will want the identifier of the dojo saved as a Python variable for future
    use, and you can do so using the Jupyter syntax for assignment of bash scripts
    to Python variables as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你将希望将道场的标识符保存为 Python 变量以供将来使用，你可以使用以下 Jupyter 语法将 bash 脚本赋值给 Python 变量。
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Before a Kaen dojo can be used for training, it should be activated. Activate
    the dojo specified by the identifier in the MOST_RECENT_DOJO variable by running
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kaen 道场用于训练之前，它应该被激活。通过运行以下命令激活由 MOST_RECENT_DOJO 变量中的标识符指定的道场。
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Since the Jupyter ! shell shortcut provides access to Python variables, in the
    previous code snippet the {MOST_RECENT_DOJO} syntax is replaced with the value
    of the corresponding Python variable. You can confirm that the dojo is active
    by inspecting it using
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Jupyter 的 ! shell 快捷方式提供了对 Python 变量的访问，因此在前面的代码片段中，{MOST_RECENT_DOJO} 语法将替换为相应
    Python 变量的值。你可以通过检查来确认道场是否激活。
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: which should include an output line with KAEN_DOJO_STATUS=active.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该包含一个输出行，其中包含 KAEN_DOJO_STATUS=active。
- en: Before you can start a training job in the dojo, you need to create one specifying
    both the dojo and the Kaen image for training.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能够在道场中启动训练作业之前，你需要创建一个指定了道场和用于训练的 Kaen 镜像的作业。
- en: To create a job to train the DcTaxiModel, execute
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个训练 DcTaxiModel 的作业，请执行以下命令。
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: which will attempt to pull the specified image from DockerHub and if successful
    will return the alphanumeric identifier for the job.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 它将尝试从 DockerHub 拉取指定的镜像，如果成功，则会返回作业的字母数字标识符。
- en: Just as with the dojo, you can save the identifier of the job to a Python variable
    using
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 与道场一样，你可以使用以下命令将作业标识保存到 Python 变量中。
- en: '[PRE35]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: which should print out the identifier of the job you created.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该打印出你创建的作业的标识符。
- en: Every job in Kaen is configured with dedicated networking settings you can inspect
    by running
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Kaen 中的每个作业都配置了专用的网络设置，你可以通过运行以下命令来检查。
- en: '[PRE36]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Since you have not yet enabled HPO for this job, the inspected job settings
    do not include the information about the HPO image used to serve MLFlow experiment
    management and Optuna hyperparameter values. You can configure the job with a
    single run of HPO by executing
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你还没有为这个作业启用 HPO，所以检查的作业设置中不包含用于提供 MLFlow 实验管理和 Optuna 超参数值的 HPO 镜像的信息。你可以通过执行以下命令来配置作业进行一次
    HPO 运行。
- en: '[PRE37]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: which overrides the default settings of your dctaxi-hpo image to specify that
    the hpo.DcTaxiHpoService class should be used to start the HPO service. The executed
    statement also configures the MLFlow UI port 5001 using the --port setting.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 它会覆盖你的 dctaxi-hpo 镜像的默认设置，以指定使用 hpo.DcTaxiHpoService 类来启动 HPO 服务。执行的语句还使用 --port
    设置配置了 MLFlow UI 端口 5001。
- en: 'Assuming the hpo enable command completes successfully, you can inspect the
    job again to observe the HPO-specific settings:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 hpo enable 命令成功完成，你可以再次检查作业以观察与 HPO 相关的设置：
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Notice that at this time the output includes the KAEN_HPO_MANAGER_IP for the
    IP address of the internal Docker network (specified by KAEN_JOB_SUBNET) that
    handles the communication across your container instances.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此时输出中包括 KAEN_HPO_MANAGER_IP，用于内部 Docker 网络的 IP 地址（由 KAEN_JOB_SUBNET 指定），该网络处理容器实例之间的通信。
- en: At this time, the HPO service should be up and running, so you should be able
    to access the MLFlow user interface by navigating your browser to http://127.0.0.1:5001,
    which should show a screen similar to the one in figure 12.4\. Note that you need
    to open the MLFlow experiment that starts with a job prefix on the left-side bar
    of the MLFlow interface before you can explore the details of the HPO experiment.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，HPO 服务应该已经启动并运行，因此您应该能够通过将浏览器导航到 http://127.0.0.1:5001 访问 MLFlow 用户界面，该界面应该显示类似于图
    12.4 的屏幕。请注意，在您探索 HPO 实验的详细信息之前，您需要在 MLFlow 界面的左侧边栏中打开以 job 前缀开头的 MLFlow 实验。
- en: '![12-04](Images/12-04.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![12-04](Images/12-04.png)'
- en: Figure 12.4 Screen capture of the MLFlow browser-based interface illustrating
    the parent run and the sole child run for the experiment instance
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 屏幕截图显示了 MLFlow 基于浏览器的界面，展示了实验实例的父运行和唯一的子运行
- en: Since at this point you have just started the HPO service, your experiment consists
    of just a single parent run with a single child run. The parent or main run has
    a one-to-one relationship with the MLFlow experiment and contains the individual
    child runs that define specific hyperparameter configurations that should be used
    by the machine learning pipeline execution instances. If you navigate to the child
    run in the MLFlow user interface, you should see a screen resembling the screenshot
    in figure 12.5.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此时您刚刚启动了 HPO 服务，因此您的实验只包括一个父运行和一个子运行。主运行与 MLFlow 实验具有一对一的关系，并包含定义应由机器学习管道执行实例使用的特定超参数配置的各个子运行。如果您在
    MLFlow 用户界面中导航到子运行，则应看到类似于图 12.5 截图的屏幕。
- en: '![12-05](Images/12-05.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![12-05](Images/12-05.png)'
- en: Figure 12.5 MLFlow screen capture of the settings suggested by Optuna HPO for
    the child run
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 MLFlow 屏幕截图显示了 Optuna HPO 建议的设置，用于子运行
- en: 'To start training your model in the local provider using the data available
    in your AWS bucket, you need to configure environment variables with your AWS
    credentials. In the following code snippet, replace the Python None with your
    matching AWS credentials for AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_DEFAULT_REGION.
    Also, perform the same replacement for your BUCKET_ID value and execute the code
    to configure the corresponding environment variables in your Kaen Jupyter environment:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用您 AWS 存储桶中的可用数据在本地提供程序中开始训练模型，您需要配置环境变量与您的 AWS 凭据。在以下代码片段中，将 Python None
    替换为您的匹配 AWS 凭据的值，用于 AWS_ACCESS_KEY_ID、AWS_SECRET_ACCESS_KEY 和 AWS_DEFAULT_REGION。同时，对于您的
    BUCKET_ID 值，进行相同的替换，并执行代码以在您的 Kaen Jupyter 环境中配置相应的环境变量：
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'I recommend that you execute the following sequence of echo commands from your
    bash shell to ensure that all of the environment variables are configured as expected:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您从您的 bash shell 中执行以下一系列 echo 命令，以确保所有环境变量都配置如预期：
- en: '[PRE40]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now you are ready to start training your model by running kaen job start. For
    simplicity, start by training with a single training worker (as specified by --replicas
    1). Notice that the KAEN_OSDS environment variables in the command are pointing
    to your data CSV files in the AWS bucket:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以通过运行 kaen job start 来开始训练您的模型。为了简单起见，首先使用单个训练工作者进行训练（由 --replicas 1 指定）。请注意，命令中的
    KAEN_OSDS 环境变量指向您在 AWS 存储桶中的数据 CSV 文件：
- en: '[PRE41]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: While the training job is running, you should be able to navigate to the details
    of the child run in the MLFlow user interface, and assuming that your training
    process ran for at least 25 training steps, the resulting graph for the train_rmse
    metric should resemble the one in figure 12.6.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练作业正在运行时，您应该能够在 MLFlow 用户界面中导航到子运行的详细信息，假设您的训练过程至少运行了 25 个训练步骤，则 train_rmse
    指标的结果图表应该类似于图 12.6 中的图表。
- en: '![12-06](Images/12-06.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![12-06](Images/12-06.png)'
- en: Figure 12.6 MLFlow screen capture of the graph for the train_rmse metric
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 MLFlow 屏幕截图显示了 train_rmse 指标的图表
- en: 12.4.3 Training with the Kaen AWS provider
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.3 使用 Kaen AWS 提供程序进行训练
- en: This section illustrates how to use the Kaen framework to train your containers
    in an AWS virtual private cloud environment instead of your local provider so
    you can take advantage of the elastic, horizontal scaling available in AWS.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本节说明了如何使用 Kaen 框架在 AWS 虚拟私有云环境中训练容器，而不是在本地提供者中，这样你就可以利用 AWS 中提供的弹性、水平扩展。
- en: 'To create a Kaen dojo in AWS, you need to use the --provider aws setting when
    running kaen init. By default, when using the AWS provider, Kaen provisions t3.micro
    instances as both worker and manager nodes in AWS. Although the t3.micro instances
    are low-cost defaults suitable for simple demos, for the DcTaxiModel, I recommend
    provisioning t3.large instances as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 AWS 中创建一个 Kaen 道场，你需要在运行 kaen init 时使用 --provider aws 设置。默认情况下，在使用 AWS 提供者时，Kaen
    会在 AWS 中将 t3.micro 实例作为 worker 和 manager 节点。尽管 t3.micro 实例是适用于简单演示的低成本默认值，但对于
    DcTaxiModel，我建议按照以下方式提供 t3.large 实例：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This, upon a successful provisioning, should report the dojo ID.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功创建后，应该报告道场 ID。
- en: To configure the MOST_RECENT_DOJO Python variable, you should execute
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置 MOST_RECENT_DOJO Python 变量，你应该执行以下操作：
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: and then activate the dojo using
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下命令激活道场：
- en: '[PRE44]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Notice that if you provision underpowered AWS node instances (such as t3.micro),
    the activation process could take a while. Once the activation is finished correctly,
    you should be able to inspect the Dojo using
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你提供了性能不足的 AWS 节点实例（例如 t3.micro），激活过程可能需要一些时间。一旦激活正确完成，你应该能够使用以下命令检查道场：
- en: '[PRE45]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: and the output should include a line that starts with KAEN_DOJO_STATUS=active
    and the timestamp of when the activation completed.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应包含以 KAEN_DOJO_STATUS=active 开头的一行以及激活完成的时间戳。
- en: 'Just as with a local provider, to perform training in AWS, you should start
    by creating a job:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 与本地提供者一样，要在 AWS 中执行训练，你应该首先创建一个作业：
- en: '[PRE46]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Unlike the case of the local provider, running kaen job create in the AWS provider
    may take a while. This is caused by the fact that the dctaxi image that you pushed
    to DockerHub needs to be downloaded to the AWS node in your dojo. After the job
    is created, you should save the ID of the job to the MOST_RECENT_JOB Python variable
    using
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与本地提供者不同，在 AWS 提供者中运行 kaen job create 可能需要一些时间。这是因为你推送到 DockerHub 的 dctaxi 镜像需要下载到
    AWS 道场中的 AWS 节点。作业创建完成后，你应该使用以下命令将作业 ID 保存到 MOST_RECENT_JOB Python 变量中：
- en: '[PRE47]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: which also sets the MOST_RECENT_JOB environment variable to the value matching
    the corresponding Python variable.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这也将 MOST_RECENT_JOB 环境变量设置为与相应 Python 变量匹配的值。
- en: 'Next, enable HPO for the job using the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令为作业启用 HPO：
- en: '[PRE48]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Once the kaen hpo enable operation is finished, you can open the MLFlow user
    interface by constructing the URL in your notebook using
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 kaen hpo enable 操作完成，你可以通过构造笔记本中的 URL 打开 MLFlow 用户界面：
- en: '[PRE49]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: and navigating to the URL in your browser. Since it may take a few seconds for
    the MLFlow UI to become available (depending on the performance of your AWS management
    node instances), you may need to refresh your browser to get access to this interface.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 并在浏览器中导航到 URL。由于 MLFlow UI 可能需要几秒钟才能变为可用（取决于 AWS 管理节点实例的性能），你可能需要刷新浏览器才能访问该界面。
- en: 'To start the training, the kaen job start command is identical to the one you
    used before:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，kaen job start 命令与之前使用的相同：
- en: '[PRE50]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As in the case with the local provider, you can navigate your browser to the
    MLFlow UI and monitor the metrics as the model trains.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 与本地提供者的情况类似，你可以在浏览器中导航到 MLFlow UI 并在模型训练时监视指标。
- en: When you are done, do not forget to remove the AWS training dojo using
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，不要忘记使用以下命令移除 AWS 训练道场：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概要
- en: Experiment management and hyperparameter optimization are integral phases of
    a machine learning pipeline.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验管理和超参数优化是机器学习流水线的组成部分。
- en: Docker containers facilitate packaging, deployment, and integration of machine
    learning code with the machine learning pipeline services.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 容器便于将机器学习代码打包、部署和集成到机器学习流水线服务中。
- en: Training a machine learning model translates to numerous experiments executed
    as instances of machine learning pipeline runs.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练机器学习模型意味着执行大量作为机器学习流水线运行实例的实验。
