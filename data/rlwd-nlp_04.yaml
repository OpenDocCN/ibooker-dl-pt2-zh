- en: 2 Your first NLP application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 您的第一个NLP应用程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括：
- en: Building a sentiment analyzer using AllenNLP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AllenNLP构建情感分析器
- en: Applying basic machine learning concepts (datasets, classification, and regression)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用基本的机器学习概念（数据集，分类和回归）
- en: Employing neural network concepts (word embeddings, recurrent neural networks,
    linear layers)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用神经网络概念（词嵌入，循环神经网络，线性层）
- en: Training the model through reducing loss
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少损失训练模型
- en: Evaluating and deploying your model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估和部署您的模型
- en: In section 1.1.2, we saw how not to do NLP. In this chapter, we are going to
    discuss how to do NLP in a more principled, modern way. Specifically, we’d like
    to build a sentiment analyzer using a neural network. Even though the sentiment
    analyzer we are going to build is a simple application and the library (AllenNLP)
    takes care of most heavy lifting, it is a full-fledged NLP application that covers
    a lot of basic components of modern NLP and machine learning. I’ll introduce important
    terms and concepts along the way. Don’t worry if you don’t understand some concepts
    at first. We will revisit most of the concepts introduced here in later chapters.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在1.1.2节中，我们看到了如何使用NLP。在本章中，我们将讨论如何以更有原则性和现代化的方式进行NLP。具体而言，我们希望使用神经网络构建一个情感分析器。尽管我们要构建的情感分析器是一个简单的应用程序，并且该库（AllenNLP）会处理大部分工作，但它是一个成熟的NLP应用程序，涵盖了许多现代NLP和机器学习的基本组件。我将沿途介绍重要的术语和概念。如果您一开始不理解某些概念，请不要担心。我们将在后面的章节中再次讨论在此处介绍的大部分概念。
- en: 2.1 Introducing sentiment analysis
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 介绍情感分析
- en: In the scenario described in section 1.1.2, you wanted to extract users’ subjective
    opinions from online survey results. You have a collection of textual data in
    response to a free-response question, but you are missing the answers to the “How
    do you like our product?” question, which you’d like to recover from the text.
    This task is called *sentiment analysis*, which is a text analytic technique used
    in the automatic identification and categorization of subjective information within
    text. The technique is widely used in quantifying opinions, emotions, and so on
    that are written in an unstructured way and, thus, hard to quantify otherwise.
    Sentiment analysis is applied to a wide variety of textual resources such as survey,
    reviews, and social media posts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在1.1.2节中描述的情景中，您希望从在线调查结果中提取用户的主观意见。您拥有对自由回答问题的文本数据集合，但缺少对“您对我们的产品有何评价？”问题的答案，您希望从文本中恢复它们。这个任务称为*情感分析*，是一种在文本中自动识别和分类主观信息的文本分析技术。该技术广泛应用于量化以非结构化方式书写的意见、情感等方面的文本资源。情感分析应用于各种文本资源，如调查、评论和社交媒体帖子。
- en: In machine learning, *classification* means categorizing something into a set
    of predefined, discrete categories. One of the most basic tasks in sentiment analysis
    is the classification of *polarity*, that is, to classify whether the expressed
    opinion is positive, negative, or neutral. You could use more than three classes,
    for example, strongly positive, positive, neutral, negative, or strongly negative.
    This may sound familiar to you if you have used a website (such as Amazon) where
    people can review things using a five-point scale expressed by the number of stars.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，*分类*意味着将某样东西归类为一组预定义的离散类别。情感分析中最基本的任务之一就是*极性*的分类，即将表达的观点分类为正面、负面或中性。您可以使用超过三个类别，例如强正面、正面、中性、负面或强负面。如果您使用过可以使用五级评分表达的网站（如亚马逊），那么这可能听起来很熟悉。
- en: Classification of polarity is one type of sentence classification task. Another
    type of sentence classification task is spam filtering, where each sentence is
    categorized into two classes—spam or not spam. It’s called *binary classification*
    if there are only two classes. If there are more than two classes (the five-star
    classification system mentioned earlier, for example), it’s called *multiclass
    classification*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 极性分类是一种句子分类任务。另一种句子分类任务是垃圾邮件过滤，其中每个句子被分类为两类——垃圾邮件或非垃圾邮件。如果只有两个类别，则称为*二元分类*。如果有超过两个类别（前面提到的五星级分类系统，例如），则称为*多类分类*。
- en: 'In contrast, when the prediction is a continuous value instead of discrete
    categories, it’s called *regression*. If you’d like to predict the price of a
    house based on its properties, such as its neighborhood, numbers of bedrooms and
    bathrooms, and square footage, it’s a regression problem. If you attempt to predict
    stock prices based on the information collected from news articles and social
    media posts, it’s also a regression problem. (Disclaimer: I’m not suggesting this
    is an appropriate approach to stock price prediction. I’m not even sure if it
    works.) As I mentioned earlier, most linguistic units such as characters, words,
    and part-of-speech tags are discrete. For this reason, most uses of machine learning
    in NLP are classification, not regression.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当预测是连续值而不是离散类别时，称之为 *回归*。如果你想根据房屋的属性来预测房屋的价格，比如它的社区、卧室和浴室的数量以及平方英尺，那就是一个回归问题。如果你尝试根据从新闻文章和社交媒体帖子中收集到的信息来预测股票价格，那也是一个回归问题。（免责声明：我并不是在建议这是预测股价的适当方法。我甚至不确定它是否有效。）正如我之前提到的，大多数语言单位，如字符、单词和词性标签，都是离散的。因此，自然语言处理中大多数使用的机器学习都是分类，而不是回归。
- en: NOTE *Logistic regression*, a widely used statistical model, is usually used
    for classification, even though it has “regression” in its name. Yes, I know it’s
    confusing!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 *逻辑回归*，一种广泛使用的统计模型，通常用于分类，尽管它的名字中有“回归”一词。是的，我知道这很令人困惑！
- en: Many modern NLP applications, including the sentiment analyzer we are going
    to build in this chapter (shown in figure 2.1), are built based on the *supervised
    machine learning* paradigm. Supervised machine learning is one type of machine
    learning where the algorithm is trained with data that has supervision signals—the
    desired outcome for individual input. The algorithm is trained in such a way that
    it reproduces the signals as closely as possible. For sentiment analysis, this
    means that the system is trained on data that contains the desired labels for
    each input sentence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代自然语言处理应用，包括我们将在本章中构建的情感分析器（如图 2.1 所示），都是基于 *监督式机器学习* 范式构建的。监督式机器学习是一种机器学习类型，其中算法是通过具有监督信号的数据进行训练的——对于每个输入都有期望的结果。该算法被训练成尽可能准确地重现这些信号。对于情感分析，这意味着系统是在包含每个输入句子的所需标签的数据上进行训练的。
- en: '![CH02_F01_Hagiwara](../Images/CH02_F01_Hagiwara.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F01_Hagiwara](../Images/CH02_F01_Hagiwara.png)'
- en: Figure 2.1 Sentiment analysis pipeline
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 情感分析流水线
- en: 2.2 Working with NLP datasets
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 处理自然语言处理数据集
- en: As we discussed in the previous section, many modern NLP applications are developed
    using supervised machine learning, where algorithms are trained from data annotated
    with desired outcomes, instead of using handwritten rules. Almost by definition,
    data is a critical part for machine learning, and it is important to understand
    how it is structured and used with machine learning algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的，许多现代自然语言处理应用都是使用监督式机器学习开发的，其中算法是从标有期望结果的数据中训练出来的，而不是使用手写规则。几乎可以说，数据是机器学习的关键部分，因此了解它是如何结构化并与机器学习算法一起使用的至关重要。
- en: 2.2.1 What is a dataset?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 什么是数据集？
- en: A *dataset* simply means a collection of data. If you are familiar with relational
    databases, you can think of a dataset as a dump of one table. It consists of pieces
    of data that follow the same format. In database terms, each piece of the data
    corresponds to a record, or a row in a table. A record can have any number of
    fields, which correspond to columns in a database.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据集* 简单地意味着一组数据。如果你熟悉关系型数据库，你可以将数据集想象成一个表的转储。它由符合相同格式的数据片段组成。在数据库术语中，数据的每个片段对应一个记录，或者表中的一行。记录可以有任意数量的字段，对应数据库中的列。'
- en: 'In NLP, records in a dataset are usually some type of linguistic units, such
    as words, sentences, or documents. A dataset of natural language texts is called
    a *corpus* (plural: *corpora*). As an example, let’s think of a (hypothetical)
    dataset for spam filtering. Each record in this dataset is a pair of a piece of
    text and a label, where the text is a sentence or a paragraph (e.g., from an email)
    and the label specifies whether the text is spam. Both the text and the label
    are the fields of a record.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，数据集中的记录通常是某种类型的语言单位，比如单词、句子或文档。自然语言文本的数据集称为 *语料库*（复数形式为 *语料库*）。举个例子，我们来想象一个（假想的）用于垃圾邮件过滤的数据集。该数据集中的每条记录都是一对文本和标签，其中文本是一句话或一段文字（例如，来自一封电子邮件），而标签指定文本是否是垃圾邮件。文本和标签都是记录的字段。
- en: Some NLP datasets and corpora have more complex structures. For example, a dataset
    may contain a collection of sentences, where each sentence is annotated with detailed
    linguistic information, such as part-of-speech tags, parse trees, dependency structures,
    and semantic roles. If a dataset contains a collection of sentences annotated
    with their parse trees, the dataset is called a *treebank*. The most famous example
    of this is Penn Treebank (PTB) ([http://realworldnlpbook.com/ch2.html#ptb](http://realworldnlpbook.com/ch2.html#ptb)),
    which has been serving as the de facto standard dataset for training and evaluating
    NLP tasks such as part-of-speech (POS) tagging and parsing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一些自然语言处理数据集和语料库具有更复杂的结构。例如，一个数据集可能包含一系列句子，其中每个句子都用详细的语言信息进行了注释，例如词性标签、句法树、依存结构和语义角色。如果一个数据集包含了一系列句子，并且这些句子带有它们的句法树注释，那么这个数据集被称为*树库*。最著名的例子是宾夕法尼亚树库（Penn
    Treebank，PTB）([http://realworldnlpbook.com/ch2.html#ptb](http://realworldnlpbook.com/ch2.html#ptb))，它一直作为培训和评估自然语言处理任务（如词性标注和句法分析）的事实标准数据集。
- en: A closely related term to a record is an *instance*. In machine learning, an
    instance is a basic unit for which the prediction is made. For example, in the
    spam-filtering task mentioned earlier, an instance is one piece of text, because
    predictions (spam or not spam) are made for individual texts. An instance is usually
    created from a record in a dataset, as is the case for the spam-filtering task,
    but this is not always the case—for example, if you take a treebank and use it
    to train an NLP task that detects all nouns in a sentence, then each word, not
    a sentence, becomes an instance, because prediction (noun or not noun) is made
    for each word. Finally, a *label* is a piece of information attached to some linguistic
    unit in a dataset. A spam-filtering dataset has labels that correspond to whether
    each text is a spam. A treebank may have one label per word for its part of speech.
    Labels are usually used as training signals (i.e., answers for the training algorithm)
    in a supervised machine learning setting. See figure 2.2 for a depiction of these
    parts of a dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与记录密切相关的术语是*实例*。在机器学习中，实例是进行预测的基本单位。例如，在前面提到的垃圾邮件过滤任务中，一个实例是一段文本，因为对单个文本进行预测（垃圾邮件或非垃圾邮件）。实例通常是从数据集中的记录创建的，就像在垃圾邮件过滤任务中一样，但并非总是如此——例如，如果您拿一个树库来训练一个NLP任务，该任务检测句子中的所有名词，那么每个单词，而不是一个句子，就成为一个实例，因为对每个单词进行预测（名词或非名词）。最后，*标签*是附加到数据集中某些语言单位的信息片段。一个垃圾邮件过滤数据集有与每个文本是否为垃圾邮件相对应的标签。一个树库可能具有每个词的词性标签的标签。标签通常在监督式机器学习环境中用作训练信号（即训练算法的答案）。请参见图2.2，了解数据集的这些部分的描绘。
- en: '![CH02_F02_Hagiwara](../Images/CH02_F02_Hagiwara.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F02_Hagiwara](../Images/CH02_F02_Hagiwara.png)'
- en: Figure 2.2 Datasets, records, fields, instances, and labels
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 数据集、记录、字段、实例和标签
- en: 2.2.2 Stanford Sentiment Treebank
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 斯坦福情感树库
- en: 'To build a sentiment analyzer, we are going to use the Stanford Sentiment Treebank
    (SST; [https://nlp.stanford.edu/sentiment/](https://nlp.stanford.edu/sentiment/)),
    one of the most widely used sentiment analysis datasets as of today. Go ahead
    and download the dataset from the Train, Dev, Test Splits in PTB Tree Format link.
    One feature that differentiates SST from other datasets is the fact that sentiment
    labels are assigned not only to sentences but also to every word and phrase in
    sentences. For example, some excerpts from the dataset follow:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建情感分析器，我们将使用斯坦福情感树库（SST；[https://nlp.stanford.edu/sentiment/](https://nlp.stanford.edu/sentiment/)），这是截至目前最广泛使用的情感分析数据集之一。前往链接中的Train,
    Dev, Test Splits in PTB Tree Format下载数据集。SST与其他数据集的一个不同之处在于，情感标签不仅分配给句子，而且分配给句子中的每个单词和短语。例如，数据集的一些摘录如下：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Don’t worry about the details for now—these trees are written in S-expressions
    that are painfully hard to read for humans (unless you are a Lisp programmer).
    Notice the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在不用担心细节——这些树以人类难以阅读的S表达式编写（除非你是Lisp程序员）。请注意以下内容：
- en: Each sentence is annotated with sentiment labels (4 and 1).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个句子都带有情感标签（4 和 1）。
- en: Each word is also annotated, for example, (4 masterpiece) and (1 not).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词也被注释了，例如，（4 masterpiece）和（1 not）。
- en: Every single phrase is also annotated, for example, (4 (2 another) (4 masterpiece)).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个短语也被注释了，例如，（4（2 another）（4 masterpiece））。
- en: 'This property of the dataset enables us to study the complex semantic interactions
    between words and phrases. For example, let’s consider the polarity of the following
    sentence as a whole:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的这种属性使我们能够研究单词和短语之间的复杂语义交互。 例如，让我们将以下句子的极性作为一个整体来考虑：
- en: The movie was actually neither that funny, nor super witty.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这部电影实际上既不是那么有趣，也不是非常机智。
- en: The above statement would definitely be a negative, although, if you focus on
    the individual words (such as *funny*, *witty*), you might be fooled into thinking
    it’s a positive. If you built a simple classifier that takes “votes” from individual
    words (e.g., the sentence is positive if a majority of its words are positive),
    such classifiers would have difficulties classifying this example correctly. To
    correctly classify the polarity of this sentence, you need to understand the semantic
    impact of the negation “neither . . . nor.” For this property, SST has been used
    as the standard benchmark for neural network models that can capture the syntactic
    structures of sentences ([http://realworldnlpbook.com/ch2.html#socher13](http://realworldnlpbook.com/ch2.html#socher13)).
    However, in this chapter, we are going to ignore all the labels assigned to internal
    phrases and use only labels for sentences.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的陈述肯定是一个负面的，尽管，如果你专注于单词的个别词语（比如*有趣*、*机智*），你可能会被愚弄成认为它是一个积极的。 如果您构建一个简单的分类器，它从单词的个别“投票”中获取结果（例如，如果其大多数单词为积极，则句子为积极），这样的分类器将难以正确分类此示例。
    要正确分类此句子的极性，您需要理解否定“既不…也不”的语义影响。 为了这个属性，SST已被用作可以捕获句子的句法结构的神经网络模型的标准基准（[http://realworldnlpbook.com/ch2.html#socher13](http://realworldnlpbook.com/ch2.html#socher13)）。
    但是，在本章中，我们将忽略分配给内部短语的所有标签，并仅使用句子的标签。
- en: 2.2.3 Train, validation, and test sets
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 训练、验证和测试集
- en: Before we move on to show how to use SST datasets and start building our own
    sentiment analyzer, I’d like to touch upon some important concepts in machine
    learning. In NLP and ML, it is common to use a couple of different types of datasets
    to develop and evaluate models. A widely used best practice is to use three different
    types of dataset *splits*—train, validation, and test sets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续展示如何使用SST数据集并开始构建我们自己的情感分析器之前，我想简要介绍一些机器学习中的重要概念。 在NLP和ML中，通常使用几种不同类型的数据集来开发和评估模型是常见的。
    一个广泛使用的最佳实践是使用三种不同类型的数据集*拆分*——训练、验证和测试集。
- en: A *train* (or *training*) set is the main dataset used to train the NLP/ML models.
    Instances from the train set are usually fed to the ML training pipeline directly
    and used to learn parameters of the model. Train sets are usually the biggest
    among the three types of splits discussed here.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练*（或*训练*）集是用于训练NLP/ML模型的主要数据集。 通常将来自训练集的实例直接馈送到ML训练管道中，并用于学习模型的参数。 训练集通常是这里讨论的三种类型的拆分中最大的。'
- en: A *validation* set (also called a *dev* or *development* set) is used for *model
    selection*. Model selection is a process where appropriate NLP/ML models are selected
    among all possible models that can be trained using the train set, and here’s
    why it’s necessary. Let’s think of a situation where you have two machine learning
    algorithms, A and B, with which you want to train an NLP model. You use both algorithms
    and obtain models A and B. Now, how can you know which model is better?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*验证*集（也称为*开发*或*开发*集）用于*模型选择*。 模型选择是一个过程，在这个过程中，从所有可能使用训练集训练的模型中选择适当的NLP/ML模型，并且这是为什么它是必要的。
    让我们想象一种情况，在这种情况下，您有两种机器学习算法A和B，您希望用它们来训练一个NLP模型。 您同时使用这两个算法，并获得了模型A和B。 现在，您如何知道哪个模型更好呢？'
- en: “That’s easy,” you might say. “Evaluate them both on the train set.” At first
    glance, this may sound like a good idea. You run both models A and B on the train
    set and see how they perform in terms of metrics such as accuracy. Why do people
    bother to use a separate validation set for selecting models?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “那很容易，”您可能会说。“在训练集上评估它们两个。”乍一看，这似乎是个好主意。 您在训练集上运行模型A和B，并查看它们在准确度等度量方面的表现。 为什么人们要费心使用单独的验证集来选择模型？
- en: The answer is *overfitting*—another important concept in NLP and ML. Overfitting
    is a situation where a trained model fits the train set so well that it loses
    its generalizability. Let’s think of an extreme case to illustrate the point here.
    Assume algorithm B is a very, very powerful one that remembers everything as-is.
    Think of it as a big associative array (or dict in Python) that can store all
    the pairs of instances and labels it has ever encountered. For the spam-filtering
    task, this means that the model stores the exact texts and their labels as they
    are presented when the model is being trained. If the exact same text is presented
    when the model is evaluated, it just returns what’s stored as its label. On the
    other hand, if the presented text is even slightly different from any other texts
    it has in memory, the model has no clue, because it’s never seen it before.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是 *过拟合* —— 自然语言处理和机器学习中另一个重要概念。过拟合是指训练模型在训练集上拟合得非常好，以至于失去了其泛化能力的情况。让我们想象一个极端情况来说明这一点。假设算法
    B 是一个非常非常强大的算法，可以完全记住所有东西。可以把它想象成一个大的关联数组（或 Python 中的字典），它可以存储它曾经遇到过的所有实例和标签对。对于垃圾邮件过滤任务来说，这意味着模型会以训练时呈现的确切文本及其标签的形式进行存储。如果在评估模型时呈现相同的文本，它将返回存储的标签。另一方面，如果呈现的文本与其记忆中的任何其他文本略有不同，模型就一无所知，因为它以前从未见过。
- en: How do you think this model would perform if it was evaluated on the train set?
    The answer is . . . yes, 100%! Because the model remembers all the instances from
    the train set, it can just “replay” the entire dataset and classify it perfectly.
    Now, would this algorithm make a good spam filter if you installed it on your
    email software? Absolutely not! Because countless spam emails look very similar
    to existing ones but are slightly different, or completely new, the model has
    no clue if the input email is even one character different from what’s stored
    in the memory, and the model would be useless when deployed in production. In
    other words, it has poor (in fact, zero) generalizability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为这个模型在训练集上进行评估时会表现如何？答案是……是的，100%！因为模型记住了训练集中的所有实例，所以它可以简单地“重播”整个数据集并进行完美分类。现在，如果你在电子邮件软件上安装了这个算法，它会成为一个好的垃圾邮件过滤器吗？绝对不会！因为无数的垃圾邮件看起来与现有邮件非常相似，但略有不同，或者完全是新的，所以如果输入的电子邮件与存储在内存中的内容只有一个字符的不同，模型就一无所知，并且在投入生产时将毫无用处。换句话说，它的泛化能力非常差（事实上是零）。
- en: How could you prevent choosing such a model? By using a validation set! A validation
    set consists of separate instances that are collected in a similar way to the
    train set. Because they are independent from the train set, if you run your trained
    model on the validation set, you’ll get a good idea how the model would perform
    outside the train set. In other words, the validation set gives a proxy for the
    model’s generalizability. Imagine if the model trained by the earlier “remember
    all” algorithm was evaluated on a validation set. Because the instances in the
    validation set are similar to but independent from the ones in the train set,
    you’d get very low accuracy and know the model would perform poorly, even before
    deploying it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何防止选择这样的模型呢？通过使用验证集！验证集由与训练集类似的独立实例组成。因为它们与训练集独立，所以如果你在验证集上运行训练过的模型，你就可以很好地了解模型在训练集之外的表现。换句话说，验证集为模型的泛化能力提供了一个代理。想象一下，如果之前的“记住所有”算法训练的模型在验证集上进行评估。因为验证集中的实例与训练集中的实例类似但独立，所以你会得到非常低的准确率，知道模型的性能会很差，甚至在部署之前。
- en: The validation set is also used for tuning *hyperparameters*. A hyperparameter
    is a parameter about a machine learning algorithm or about a model that is being
    trained. For example, if you repeat the training loop (also called an *epoch*—see
    later for more explanation) for *N* times, this *N* is a hyperparameter. If you
    increase the number of layers of the neural network, you just changed one hyperparameter
    about the model. Machine learning algorithms and models usually have a number
    of hyperparameters, and it is crucial to tune them for them to perform optimally.
    You can do this by training multiple models with different hyperparameters and
    evaluating them on a validation set. In fact, you can think of models with different
    hyperparameters as separate models, even if they have the same structure, and
    hyperparameter tuning can be considered one type of model selection.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集还用于调整*超参数*。超参数是关于机器学习算法或正在训练的模型的参数。例如，如果你将训练循环（也称为*epoch*，关于更多解释请见后文）重复*N*次，那么这个*N*就是一个超参数。如果你增加神经网络的层数，你就改变了关于模型的一个超参数。机器学习算法和模型通常有许多超参数，调整它们对模型的性能至关重要。你可以通过训练多个具有不同超参数的模型并在验证集上评估它们来做到这一点。事实上，你可以将具有不同超参数的模型视为不同的模型，即使它们具有相同的结构，超参数调整可以被视为一种模型选择。
- en: Finally, a *test* set is used to evaluate the model using a new, unseen set
    of data. It consists of instances that are independent from the train and validation
    sets. It gives you a good idea how the model would perform “in the wild.”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*测试*集用于使用新的、未见过的数据对模型进行评估。它包含的实例与训练集和验证集是独立的。它可以让你很好地了解模型在“野外”中的表现。
- en: You might wonder why yet another separate dataset is necessary for evaluating
    the model’s generalizability. Can’t you just use the validation set for this?
    Again, you shouldn’t rely solely on a train set and a validation set to measure
    the generalizability of your model, because your model could also overfit to the
    validation set in a subtle way. This point is less intuitive, but let me give
    you an example. Imagine you are frantically experimenting with a ton of different
    spam-filtering models. You wrote a script that automatically trains a spam-filtering
    model. The script also automatically evaluates the trained models on the validation
    set. If you run this script 1,000 times with different combinations of algorithms
    and hyperparameters and pick one model with the highest validation set performance,
    would it also perform the best on the completely new, unseen instances? Probably
    not. If you try a large number of models, some of them happen to perform relatively
    well on the validation set purely by chance (because the predictions inherently
    have some noise, and/or because those models happen to have some characteristics
    that make them perform better on the validation set), but this is no guarantee
    that those models perform well outside of the validation set. In other words,
    it could be possible to overfit the model to the validation set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么需要另外一个独立的数据集来评估模型的泛化能力。难道你不能只使用验证集吗？再次强调，你不应该仅仅依赖于训练集和验证集来衡量你的模型的泛化能力，因为你的模型也可能以微妙的方式对验证集进行过拟合。这一点不太直观，但让我举个例子。想象一下，你正在疯狂地尝试大量不同的垃圾邮件过滤模型。你编写了一个脚本，可以自动训练一个垃圾邮件过滤模型。该脚本还会自动在验证集上评估训练好的模型。如果你用不同的算法和超参数组合运行此脚本
    1,000 次，并选择在验证集上性能最好的一个模型，那么它是否也会在完全新的、未见过的实例上表现最好呢？可能不会。如果你尝试大量的模型，其中一些可能纯粹是由于偶然性而在验证集上表现相对较好（因为预测本质上存在一些噪音，和/或者因为这些模型恰好具有一些使它们在验证集上表现更好的特性），但这并不能保证这些模型在验证集之外表现良好。换句话说，可能会将模型过度拟合到验证集上。
- en: In summary, when training NLP models, use a train set to train your model candidates,
    use a validation set to choose good ones, and use a test set to evaluate them.
    Many public datasets used for NLP and ML evaluation are already split into train/validation/test
    sets. If you just have a single dataset, you can split it into those three datasets
    by yourself. An 80:10:10 split is commonly used. Figure 2.3 depicts the train/validation/test
    split as well as the entire training pipeline.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在训练 NLP 模型时，使用一个训练集来训练你的模型候选者，使用一个验证集来选择好的模型，并使用一个测试集来评估它们。用于 NLP 和 ML 评估的许多公共数据集已经分成了训练/验证/测试集。如果你只有一个数据集，你可以自己将其分成这三个数据集。常用的是
    80:10:10 分割。图 2.3 描绘了训练/验证/测试分割以及整个训练流水线。
- en: '![CH02_F03_Hagiwara](../Images/CH02_F03_Hagiwara.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F03_Hagiwara](../Images/CH02_F03_Hagiwara.png)'
- en: Figure 2.3 Train/validation/test split and the training pipeline
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 训练/验证/测试分割和训练流水线
- en: 2.2.4 Loading SST datasets using AllenNLP
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 使用 AllenNLP 加载 SST 数据集
- en: 'Finally, let’s see how we can actually load datasets in code. In the remainder
    of this chapter, we assume that you have already installed AllenNLP (version 2.5.0)
    and the corresponding version of the allennlp-models package by running the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，让我们看看如何在代码中实际加载数据集。在本章的其余部分，我们假设你已经安装了 AllenNLP（版本 2.5.0）和相应版本的 allennlp-models
    包，通过运行以下命令： '
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'and imported necessary classes and modules as shown here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 并导入了如下所示的必要类和模块：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Unfortunately, as of this writing, AllenNLP does not officially support Windows.
    But don’t worry—all the code in this chapter (and all the code in this book, for
    that matter) is available as Google Colab notebooks ([http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb)),
    where you can run and modify the code and see the results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，截至目前为止，AllenNLP 并不官方支持 Windows。但别担心——本章节中的所有代码（实际上，本书中的所有代码）都可以作为 Google
    Colab 笔记本 ([http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb))
    使用，你可以在那里运行和修改代码并查看结果。
- en: 'You also need to define the following two constants used in the code snippets:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要定义以下两个在代码片段中使用的常量：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'AllenNLP already supports an abstraction called DatasetReader, which takes
    care of reading a dataset from the original format (be it raw text or some exotic
    XML-based format) and returns it as a collection of instances. We are going to
    use StanfordSentimentTreeBankDatasetReader(), which is a type of DatasetReader
    that specifically deals with SST datasets, as shown here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 已经支持一个名为 DatasetReader 的抽象，它负责从原始格式（无论是原始文本还是一些奇特的基于 XML 的格式）中读取数据集并将其返回为一组实例。我们将使用
    StanfordSentimentTreeBankDatasetReader()，它是一种特定处理 SST 数据集的 DatasetReader，如下所示：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This snippet will create a dataset reader for SST datasets and define the paths
    for the train and dev text files.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此片段将为 SST 数据集创建一个数据集读取器，并定义训练和开发文本文件的路径。
- en: 2.3 Using word embeddings
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 使用词嵌入
- en: From this section on, we’ll start building the neural network architecture for
    the sentiment analyzer. *Architecture* is just another word for the structure
    of neural networks. Building neural networks is a lot like building structures
    such as houses. The first step is to figure out how to feed the input (e.g., sentences
    for sentiment analysis) into the network.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一部分开始，我们将开始构建情感分析器的神经网络架构。*Architecture* 只是神经网络结构的另一个词。构建神经网络很像建造房屋等结构。第一步是弄清楚如何将输入（例如，情感分析的句子）馈送到网络中。
- en: As we have seen previously, everything in NLP is discrete, meaning there is
    no predictable relationship between the forms and the meanings (remember “rat”
    and “sat”). On the other hand, neural networks are best at dealing with something
    numerical and continuous, meaning everything in neural networks needs to be float
    numbers. How can we “bridge” between these two worlds—discrete and continuous?
    The key is the use of word embeddings, which we are going to discuss in detail
    in this section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，自然语言处理中的所有内容都是离散的，这意味着形式和含义之间没有可预测的关系（记得“rat”和“sat”）。另一方面，神经网络最擅长处理数字和连续的东西，这意味着神经网络中的所有内容都需要是浮点数。我们如何在这两个世界之间“搭桥”——离散和连续？关键在于词嵌入的使用，我们将在本节中详细讨论。
- en: 2.3.1 What are word embeddings?
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 什么是词嵌入？
- en: '*Word* *embeddings* are one of the most important concepts in modern NLP. Technically,
    an embedding is a continuous vector representation of something that is usually
    discrete. A word embedding is a continuous vector representation of a word. If
    you are not familiar with the concept of vectors, *vector* is a mathematical name
    for single-dimensional arrays of numbers. In simpler terms, word embeddings are
    a way to represent each word with a 300-element array (or an array of any other
    size) filled with nonzero float numbers. It is conceptually very simple. Then,
    why has it been so important and prevalent in modern NLP?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*Word* *embeddings* 是现代自然语言处理中最重要的概念之一。从技术上讲，嵌入是通常离散的东西的连续向量表示。词嵌入是一个词的连续向量表示。如果你对向量的概念不熟悉，*vector*
    是数学上对数字的单维数组的名称。简单来说，词嵌入是用一个 300 元素数组（或任何其他大小的数组）填充的非零浮点数来表示每个单词的一种方式。概念上非常简单。那么，为什么它在现代自然语言处理中如此重要和普遍呢？'
- en: 'As I mentioned in chapter 1, the history of NLP is actually the history of
    continuous battle against “discreteness” of language. In the eyes of computers,
    “cat” is no closer to “dog” than it is to “pizza.” One way to deal with discrete
    words programmatically is to assign indices to individual words as follows (here
    we simply assume that these indices are assigned alphabetically):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在第1章中提到的，自然语言处理的历史实际上是对语言“离散性”的持续战斗的历史。在计算机眼中，“猫”和“狗”的距离与它们与“披萨”的距离是相同的。编程上处理离散词的一种方法是为各个词分配索引，如下所示（这里我们简单地假设这些索引按字母顺序分配）：
- en: index("cat") = 1
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: index("cat") = 1
- en: index("dog") = 2
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: index("dog") = 2
- en: index("pizza") = 3
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: index("pizza") = 3
- en: '...'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: These assignments are usually managed by a lookup table. The entire, finite
    set of words that one NLP application or task deals with is called *vocabulary*.
    But this method isn’t any better than dealing with raw words. Just because words
    are now represented by numbers doesn’t mean you can do arithmetic operations on
    them and conclude that “cat” is equally similar to “dog” (difference between 1
    and 2), as “dog” is to “pizza” (difference between 2 and 3). Those indices are
    still discrete and arbitrary.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分配通常由查找表管理。一个NLP应用或任务处理的整个有限单词集被称为*词汇*。但是这种方法并不比处理原始单词更好。仅仅因为单词现在用数字表示，就不意味着你可以对它们进行算术运算，并得出“猫”与“狗”（1和2之间的差异）同样相似，就像“狗”与“披萨”（2和3之间的差异）一样。这些指数仍然是离散和任意的。
- en: “What if we can represent them on a numerical scale?” some NLP researchers wondered
    decades ago. Can we think of some sort of numerical scale where words are
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: “如果我们可以在数值尺度上表示它们呢？”几十年前，一些自然语言处理研究人员想知道。我们能否想出一种数值尺度，其中单词是
- en: represented as points, so that semantically closer words (e.g., “dog” and “cat,”
    which are both animals) are also geometrically closer? Conceptually, the numerical
    scale would look like the one shown in figure 2.4.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以点表示，以使语义上更接近的词（例如，“狗”和“猫”，它们都是动物）在几何上也更接近？从概念上讲，数值尺度将类似于图2.4中所示的尺度。
- en: '![CH02_F04_Hagiwara](../Images/CH02_F04_Hagiwara.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F04_Hagiwara](../Images/CH02_F04_Hagiwara.png)'
- en: Figure 2.4 Word embeddings in a 1-D space
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 一维空间中的词嵌入
- en: This is a step forward. Now we can represent the fact that “cat” and “dog” are
    more similar to each other than “pizza” is to those words. But still, “pizza”
    is slightly closer to “dog” than it is to “cat.” What if you wanted to place it
    somewhere that is equally far from “cat” and “dog?” Maybe only one dimension is
    too limiting. How about adding another dimension to this, as shown in figure 2.5?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个进步。现在我们可以表示“猫”和“狗”彼此之间比“披萨”更相似的事实。但是，“披萨”仍然比“猫”更接近“狗”。如果你想把它放在一个距离“猫”和“狗”都一样远的地方怎么办？也许只有一个维度太限制了。在这个基础上再添加一个维度如何，如图2.5所示？
- en: '![CH02_F05_Hagiwara](../Images/CH02_F05_Hagiwara.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F05_Hagiwara](../Images/CH02_F05_Hagiwara.png)'
- en: Figure 2.5 Word embeddings in a 2-D space
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 二维空间中的词嵌入
- en: 'Much better! Because computers are really good at dealing with multidimensional
    spaces (because you can just represent points by arrays), you can simply keep
    doing this until you have a sufficient number of dimensions. Let’s have three
    dimensions. In this 3-D space, you can represent those three words as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 许多改进！因为计算机在处理多维空间方面非常擅长（因为你可以简单地用数组表示点），你可以一直这样做，直到你有足够数量的维度。让我们有三个维度。在这个三维空间中，你可以将这三个词表示如下：
- en: vec("cat") = [0.7, 0.5, 0.1]
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("cat") = [0.7, 0.5, 0.1]
- en: vec("dog") = [0.8, 0.3, 0.1]
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("dog") = [0.8, 0.3, 0.1]
- en: vec("pizza") = [0.1, 0.2, 0.8]
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("pizza") = [0.1, 0.2, 0.8]
- en: Figure 2.6 illustrates this three-dimensional space.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6说明了这个三维空间。
- en: '![CH02_F06_Hagiwara](../Images/CH02_F06_Hagiwara.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F06_Hagiwara](../Images/CH02_F06_Hagiwara.png)'
- en: Figure 2.6 Word embeddings in a 3-D space
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 三维空间中的词嵌入
- en: The x-axis (the first element) here represents some concept of “animal-ness”
    and the z-axis (the third dimension) corresponds to “food-ness.” (I’m making these
    numbers up, but you get the point.) This is essentially what word embeddings are.
    You just embedded those words in a three-dimensional space. By using those vectors,
    you already “know” how the basic building blocks of the language work. For example,
    if you wanted to identify animal names, then you would just look at the first
    element of each word vector and see if the value is high enough. This is a great
    jump start compared to the raw word indices!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 x 轴（第一个元素）表示“动物性”的某种概念，而 z 轴（第三维）对应于“食物性”。（我编造了这些数字，但你明白我的意思。）这就是单词嵌入的本质。您只是将这些单词嵌入到了一个三维空间中。通过使用这些向量，您已经“知道”了语言的基本构建块是如何工作的。例如，如果您想要识别动物名称，那么您只需查看每个单词向量的第一个元素，并查看值是否足够高。与原始单词索引相比，这是一个很好的起点！
- en: You may be wondering where those numbers come from in practice. These numbers
    are actually “learned” using some machine learning algorithms and a large text
    dataset. We’ll discuss this further in chapter 3.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道这些数字实际上来自哪里。这些数字实际上是使用一些机器学习算法和大型文本数据集“学习”的。我们将在第三章中进一步讨论这一点。
- en: 'By the way, we have a much simpler method to “embed” words into a multidimensional
    space. Think of a multidimensional space that has as many dimensions as there
    are words. Then, give to each word a vector that is filled with zeros but just
    one 1, as shown next:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，我们有一种更简单的方法将单词“嵌入”到多维空间中。想象一个具有与单词数量相同的维度的多维空间。然后，给每个单词一个向量，其中填充了零但只有一个
    1，如下所示：
- en: vec("cat") = [1, 0, 0]
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("cat") = [1, 0, 0]
- en: vec("dog") = [0, 1, 0]
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("dog") = [0, 1, 0]
- en: vec("pizza") = [0, 0, 1]
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("pizza") = [0, 0, 1]
- en: Notice that each vector has only one 1 at the position corresponding to the
    word’s index. These special vectors are called *one-hot vectors*. These vectors
    are not very useful themselves in representing semantic relationship between those
    words—the three words are all at the equal distance from each other—but they are
    still (a very dumb kind of) embeddings. They are often used as the input to a
    machine learning algorithm when embeddings are not available.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个向量在对应单词的索引位置只有一个 1。这些特殊向量称为*one-hot向量*。这些向量本身并不非常有用，不能很好地表示这些单词之间的语义关系——这三个单词彼此之间的距离都是相等的——但它们仍然（是一种非常愚蠢的）嵌入。当嵌入不可用时，它们通常被用作机器学习算法的输入。
- en: 2.3.2 Using word embeddings for sentiment analysis
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 使用单词嵌入进行情感分析
- en: 'First, we create dataset loaders that take care of loading data and passing
    it to the training pipeline, as shown next (more discussion on this data later
    in this chapter):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建数据集加载器，负责加载数据并将其传递给训练流水线，如下所示（稍后在本章中对此数据进行更多讨论）：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'AllenNLP provides a useful Vocabulary class that manages mappings from some
    linguistic units (such as characters, words, and labels) to their IDs. You can
    tell the class to create a Vocabulary instance from a set of instances as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP 提供了一个有用的 Vocabulary 类，管理着一些语言单位（如字符、单词和标签）到它们的 ID 的映射。您可以告诉该类从一组实例中创建一个
    Vocabulary 实例，如下所示：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, you need to initialize an Embedding instance, which takes care of converting
    IDs to embeddings, as shown in the next code snippet. The size (dimension) of
    the embeddings is determined by EMBEDDING_DIM:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要初始化一个 Embedding 实例，它负责将 ID 转换为嵌入，如下代码片段所示。嵌入的大小（维度）由 EMBEDDING_DIM 决定：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, you need to specify which index names correspond to which embeddings
    and pass it to BasicTextFieldEmbedder as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要指定哪些索引名称对应于哪些嵌入，并将其传递给 BasicTextFieldEmbedder，如下所示：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now you can use word_embeddings to convert words (or more precisely, tokens,
    which I’ll talk more about in chapter 3) to their embeddings.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用 word_embeddings 将单词（或更准确地说是标记，我将在第三章中更详细地讨论）转换为它们的嵌入。
- en: 2.4 Neural networks
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 神经网络
- en: An increasingly large number of modern NLP applications are built using neural
    networks. You may have seen many amazing things that modern neural network models
    can achieve in the domain of computer vision and game playing (such as self-driving
    cars and Go-playing algorithms defeating human champions), and NLP is no exception.
    We are going to use neural networks for most of the NLP examples and applications
    we are going to build in this book. In this section, we discuss what neural networks
    are and why they are powerful.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的现代自然语言处理应用程序是使用神经网络构建的。你可能已经看到了许多现代神经网络模型在计算机视觉和游戏领域所取得的惊人成就（例如自动驾驶汽车和打败人类冠军的围棋算法），而自然语言处理也不例外。在本书中，我们将使用神经网络来构建大多数自然语言处理示例和应用程序。在本节中，我们讨论了神经网络是什么以及它们为什么如此强大。
- en: 2.4.1 What are neural networks?
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 什么是神经网络？
- en: Neural networks are at the core of modern NLP (and many other related AI fields,
    such as computer vision). It is such an important, vast research topic that it’d
    take a book (or maybe several books) to fully explain what it is and all the related
    models, algorithms, and so on. In this section, I’ll briefly explain the gist
    of it and will go into more details in later chapters as needed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是现代自然语言处理（以及许多其他相关人工智能领域，如计算机视觉）的核心。它是如此重要，如此广泛的研究主题，以至于需要一本书（或者可能是几本书）来全面解释它是什么以及所有相关的模型、算法等。在本节中，我将简要解释其要点，并根据需要在后面的章节中详细介绍。
- en: In short, a *neural network* (also called an *artificial neural network*) is
    a generic mathematical model that transforms a vector to another vector. That’s
    it. Contrary to what you may have read and heard in popular media, its essence
    is simple. If you are familiar with programming terms, think of it as a function
    that takes a vector, does some computation inside, and produces another vector
    as the return value. Then why is it such as big deal? How is it different from
    normal functions in programming?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*神经网络*（也称为*人工神经网络*）是一个通用的数学模型，它将一个向量转换为另一个向量。就是这样。与你在大众媒体中读到和听到的内容相反，它的本质是简单的。如果你熟悉编程术语，可以将其看作是一个接受一个向量，内部进行一些计算，并将另一个向量作为返回值的函数。那么它为什么如此重要呢？它与编程中的普通函数有何不同呢？
- en: The first difference is that neural networks are *trainable*. Think of it not
    just as a fixed function but more as a “template” for a set of related functions.
    If you use a programming language and write a function that includes several mathematical
    equations with some constants, you always get the same result if you feed the
    same input. On the contrary, neural networks can receive “feedback” (how close
    the output is to your desired output) and adjust their internal constants. Those
    “magic” constants are called *weights* or, more generally, *parameters**.* Next
    time you run it, you expect that its answer is closer to what you want.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个区别在于神经网络是*可训练的*。不要把它仅仅看作是一个固定的函数，而更像是一组相关函数的“模板”。如果你使用编程语言编写了一个包含一些常数的数学方程组的函数，当你输入相同的输入时，你总是会得到相同的结果。相反，神经网络可以接收“反馈”（输出与期望输出的接近程度）并调整其内部常数。那些“神奇”的常数被称为*权重*或更普遍地称为*参数*。下次运行时，你期望它的答案更接近你想要的结果。
- en: The second difference is its mathematical power. It’d be overly complicated
    if you were to use your favorite programming language and write a function that
    does, for example, sentiment analysis, if at all possible. (Remember the poor
    software engineer from chapter 1?) In theory, given enough model power and training
    data, neural networks are known to be able to approximate any continuous functions.
    This means that, whatever your problem is, neural networks can solve it if there’s
    a relationship between the input and the output and if you provide the model with
    enough computational power and training data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个区别在于它的数学能力。如果可能的话，如果你要使用你最喜欢的编程语言编写一个执行情感分析等功能的函数，那将会非常复杂。（还记得第1章中那个可怜的软件工程师吗？）理论上，只要有足够的模型能力和训练数据，神经网络就能够近似于任何连续函数。这意味着，无论你的问题是什么，只要输入和输出之间存在关系，并且你为模型提供足够的计算能力和训练数据，神经网络就能够解决它。
- en: Neural networks achieve this by learning functions that are not *linear*. What
    does it mean for a function to be linear? A linear function is a function where,
    if you change the input by x, the output will always change by c * x, where c
    is a constant number. For example, 2.0 * x is linear, because the return value
    always increases by 2.0 if you change x by 1.0\. If you plot this on a graph,
    the relationship between the input and the output forms a straight line, which
    is why it’s called linear. On the other hand, 2.0 * x * x is not linear, because
    how much the return value changes depends not only on how much you change x but
    also on the value of x.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过学习*非线性*函数来实现这一点。什么是线性函数呢？线性函数是指，如果你将输入改变了x，输出将始终以c * x的常数倍变化，其中c是一个常数。例如，2.0
    * x是线性的，因为如果你将x改变1.0，返回值总是增加2.0。如果你将这个函数画在图上，输入和输出之间的关系形成一条直线，这就是为什么它被称为线性的原因。另一方面，2.0
    * x * x不是线性的，因为返回值的变化量不仅取决于你改变x的量，还取决于x的值。
- en: What this means is that a linear function cannot capture a more complex relationship
    between the input and the output and between the input variables. On the contrary,
    natural phenomena such as language are highly nonlinear. If you change the input
    by x (e.g., a word in a sentence), how much the output changes depends not only
    on how much x is changed but also on many other factors such as the value of x
    itself (e.g., what word you changed x to) and what other variables (e.g., the
    context of x) are. Neural networks, which are nonlinear mathematic models, have
    the potential to capture such complex interactions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着线性函数无法捕捉输入和输出之间以及输入变量之间的更复杂的关系。相反，诸如语言之类的自然现象是高度非线性的。如果你改变了输入x（例如，句子中的一个词），输出的变化量不仅取决于你改变了多少x，还取决于许多其他因素，如x本身的值（例如，你将x改变为什么词）以及其他变量（例如，x的上下文）是什么。神经网络，这种非线性数学模型，有可能捕捉到这样复杂的相互作用。
- en: 2.4.2 Recurrent neural networks (RNNs) and linear layers
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 循环神经网络（RNNs）和线性层
- en: Two special types of neural network components are important for sentiment analysis—recurrent
    neural networks (RNNs) and linear layers. I’ll explain them in detail in later
    chapters, but I’ll briefly describe what they are and their roles in sentiment
    analysis (or in general, sentence classification).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 两种特殊类型的神经网络组件对情感分析非常重要——循环神经网络（RNNs）和线性层。我将在后面的章节中详细解释它们，但我会简要描述它们是什么以及它们在情感分析（或一般而言，句子分类）中的作用。
- en: 'A *recurrent neural network* (RNN) is a neural network with loops, as shown
    in figure 2.7\. It has an internal structure that is applied to the input again
    and again. Using the programming analogy, it’s like writing a function that contains
    for word in sentence: that loops over each word in the input sentence. It can
    either output the interim values of the internal variables of the loop, or the
    final values of the variables after the loop is finished, or both. If you just
    take the final values, you can use an RNN as a function that transforms a sentence
    to a vector with a fixed length. In many NLP tasks, you can use an RNN to transform
    a sentence to an embedding of the sentence. Remember word embeddings? They were
    fixed-length representation of words. Similarly, RNNs can produce fixed-length
    representation of sentences.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*循环神经网络*（RNN）是一种带有循环的神经网络，如图2.7所示。它具有一个内部结构，该结构被一次又一次地应用于输入。用编程的类比来说，这就像编写一个包含for
    word in sentence:循环遍历输入句子中的每个单词的函数。它可以输出循环内部变量的中间值，或者循环完成后变量的最终值，或者两者兼而有之。如果你只取最终值，你可以将RNN用作将句子转换为具有固定长度的向量的函数。在许多自然语言处理任务中，你可以使用RNN将句子转换为句子的嵌入。还记得词嵌入吗？它们是单词的固定长度表示。类似地，RNN可以产生句子的固定长度表示。'
- en: '![CH02_F07_Hagiwara](../Images/CH02_F07_Hagiwara.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F07_Hagiwara](../Images/CH02_F07_Hagiwara.png)'
- en: Figure 2.7 Recurrent neural network (RNN)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 循环神经网络（RNN）
- en: Another type of neural network component we’ll be using here is linear layers.
    A *linear layer*, also called a *fully connected layer*, transforms a vector to
    another vector in a linear fashion. As mentioned earlier, *layer* is just a fancier
    term for a substructure of neural networks, because you can stack them on top
    of each other to form a larger structure.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将使用的另一种类型的神经网络组件是线性层。*线性层*，也称为*全连接层*，以线性方式将一个向量转换为另一个向量。正如前面提到的，*层*只是神经网络的一个子结构的花哨术语，因为你可以将它们堆叠在一起形成一个更大的结构。
- en: Remember, neural networks can learn nonlinear relationships between the input
    and the output. Why would we want something that is more constrained (linear)
    at all? Linear layers are used for compressing (or less often, expanding) vectors
    by reducing (or increasing) the dimensionality. For example, assume you receive
    a 64-dimensional vector (an array of 64 float numbers) from an RNN as a sentence
    embedding, but all you care about is a smaller number of values that are essential
    for your prediction. In sentiment analysis, you may care about only five values
    that correspond to five different sentiment labels, namely, strongly positive,
    positive, neutral, negative, and strongly negative. But you have no idea how to
    extract those five values from the embedded 64 values. This is exactly where a
    linear layer comes in handy—you can add a layer that transforms a 64-dimensional
    vector to a 5-dimensional one, and the neural networks figure out how to do that
    well, as shown in figure 2.8.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，神经网络可以学习输入和输出之间的非线性关系。为什么我们想要有更受限制（线性）的东西呢？线性层用于通过减少（或增加）维度来压缩（或扩展）向量。例如，假设你从
    RNN 接收到一个 64 维的向量（64 个浮点数的数组）作为句子的嵌入，但你只关心对预测有重要作用的少量数值。在情感分析中，你可能只关心与五种不同情感标签对应的五个数值，即极强正面、正面、中性、负面和极强负面。但是你无法从嵌入的
    64 个数值中提取出这五个数值。这正是线性层派上用场的地方 - 你可以添加一个层，将一个 64 维的向量转换为一个 5 维的向量，而神经网络会想办法做得很好，如图
    2.8 所示。
- en: '![CH02_F08_Hagiwara](../Images/CH02_F08_Hagiwara.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F08_Hagiwara](../Images/CH02_F08_Hagiwara.png)'
- en: Figure 2.8 Linear layer
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 线性层
- en: 2.4.3 Architecture for sentiment analysis
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 情感分析的架构
- en: 'Now you are ready to put the components together to build the neural network
    for the sentiment analyzer. First, you need to create the RNN as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好将各个组件组合起来构建情感分析器的神经网络了。首先，你需要按照以下步骤创建 RNN：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Don’t worry too much about PytorchSeq2VecWrapper and batch_first=True. Here,
    you are creating an RNN (or more specifically, one type of RNN called LSTM, which
    stands for *long short-term memory*). The size of the input vector is EMBEDDING_DIM,
    which we saw earlier, and that of the output vector is HIDDEN_DIM.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 不要太担心 PytorchSeq2VecWrapper 和 batch_first=True。在这里，你正在创建一个 RNN（或更具体地说，一种叫做 LSTM
    的 RNN）。输入向量的大小是 EMBEDDING_DIM，我们之前看到的，而输出向量的大小是 HIDDEN_DIM。
- en: 'Next, you need to create a linear layer, as shown here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要创建一个线性层，如下所示：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The size of the input vector is defined by in_features, whereas out_features
    is that of the output vector. Because we are transforming the sentence embedding
    to a vector whose elements correspond to five sentiment labels, we need to specify
    the size of the encoder output and obtain the total number of labels from vocab.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量的大小由 in_features 定义，而输出向量的大小则由 out_features 定义。因为我们要将句子嵌入转换为一个向量，其元素对应于五个情感标签，所以我们需要指定编码器输出的大小，并从词汇表中获取标签的总数。
- en: Finally, we can connect those components and build a model as shown in the following
    code.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以连接这些组件并构建一个模型，如下所示的代码。
- en: Listing 2.1 Building a sentiment analyzer model
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 构建情感分析模型
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Defines the loss function (cross entropy)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义损失函数（交叉熵）
- en: ❷ The forward() function is where most of the computation happens in a model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ forward() 函数是模型中大部分计算发生的地方。
- en: ❸ Computes the loss and assigns it to the “loss” key of the returned dict
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算损失并将其分配给返回字典中的“loss”键
- en: I want you to focus on the forward() function which is the most important function
    that every neural network model has. Its role is to take the input, pass it through
    subcomponents of the neural network, and produce the output. Although the function
    has some unfamiliar logics that we haven’t covered yet (such as mask and loss),
    what’s important here is the fact that you can chain the subcomponents of the
    model (word embeddings, RNN, and the linear layer) as if they were functions that
    transform the input (tokens), and you get something called *logits* at the end
    of the pipeline. Logit is a term in statistics that has a specific meaning, but
    here, you can think of it as something like a score for a class. The higher the
    score is for a specific label, the more confident that the label is the correct
    one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你专注于最重要的函数 forward()，每个神经网络模型都有它。它的作用是接收输入，经过神经网络的子组件处理后，产生输出。虽然这个函数有一些我们尚未涉及的陌生逻辑（例如掩码和损失），但重要的是你可以像将输入（标记）转换的函数一样将模型的子组件（词嵌入，RNN和线性层）链接在一起，并在管道的末尾得到一些称为*logits*的东西。在统计学中，logit是一个具有特定含义的术语，但在这里，你可以将其视为类别的分数。对于特定标签的分数越高，表示该标签是正确的信心就越大。
- en: 2.5 Loss functions and optimization
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 损失函数和优化
- en: Neural networks are trained using supervised learning. As mentioned earlier,
    supervised learning is a type of machine learning that learns a function that
    maps inputs to outputs based on a large amount of labeled data. So far, I covered
    only about how neural networks take an input and produce an output. How can we
    make it so that neural networks produce the output that we actually want?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络使用有监督学习进行训练。如前所述，有监督学习是一种基于大量标记数据学习将输入映射到输出的机器学习类型。到目前为止，我只介绍了神经网络如何接收输入并生成输出。我们如何才能使神经网络生成我们实际想要的输出呢？
- en: Neural networks are not just like regular functions that you usually write in
    programming languages. They are *trainable*, meaning that they can receive some
    feedback and change their internal parameters so that they can produce more accurate
    outputs, even for the same inputs next time around. Notice there are two parts
    to this—receiving feedback and adjusting parameters, which are done through loss
    functions and optimization, respectively, which I’ll explain next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络不仅仅是像常规的编程语言中的函数那样。它们是*可训练的*，意味着它们可以接收一些反馈并调整其内部参数，以便下一次为相同的输入产生更准确的输出。请注意，这包含两个部分-接收反馈和调整参数，分别通过损失函数和优化来实现，下面我将解释它们。
- en: A *loss function* is a function that measures how far an output of a machine
    learning model is from a desired one. The difference between an actual output
    and a desired one is called the *loss*. Loss is also called *cost* in some contexts.
    Either way, the bigger the loss, the worse it is, and you want it as close to
    zero as possible. Take sentiment analysis, for example. If the model thinks a
    sentence is 100% negative, but the training data says it’s strongly positive,
    the loss will be big. On the other hand, if the model thinks a sentence is maybe
    80% negative and the training label is indeed negative, the loss will be small.
    It will be zero if both match exactly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数*是衡量机器学习模型输出与期望输出之间距离的函数。实际输出与期望输出之间的差异称为*损失*。在某些情况下，损失也称为*成本*。无论哪种情况，损失越大，则模型越差，你希望它尽可能接近零。例如，以情感分析为例。如果模型认为一句话是100%的负面，但训练数据显示它是非常积极的，那么损失会很大。另一方面，如果模型认为一句话可能是80%的负面，而训练标签确实是负面的，那么损失会很小。如果两者完全匹配，损失将为零。'
- en: 'PyTorch provides a wide range of functions to compute losses. What we need
    here is called *cross-entropy loss*, which is often used for classification problems,
    as shown here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了广泛的函数来计算损失。我们在这里需要的是*交叉熵损失*，它通常用于分类问题，如下所示：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It can be used later by passing a prediction and labels from the training set
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 后续可以通过以下方式将预测和来自训练集的标签传递给它来使用：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then, this is where the magic happens. Neural networks, thanks to their mathematical
    properties, know how to change their internal parameters to make the loss smaller.
    Upon receiving some large loss, the neural network goes, “Oops, sorry, that was
    my mistake, but I’ll do better next round!” and changes its parameters. Remember
    I talked about a function that you write in a programming language that has some
    magic constants in it? Neural networks act like that function but know exactly
    how to change the magic constants to reduce the loss. They do this for each and
    every instance in the training data, so that they can produce more correct answers
    for as many instances as possible. Of course, they can’t reach the perfect answer
    after adjusting the parameters only once. It requires multiple passes, called
    *epochs*, over the training data. Figure 2.9 shows the overall training procedure
    for neural networks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这就是魔术发生的地方。 由于其数学属性，神经网络知道如何改变其内部参数以使损失变小。 在接收到一些大损失后，神经网络会说：“哎呀，抱歉，那是我的错，但我下一轮会做得更好！”
    并更改其参数。 记得我说过编写一个具有一些魔术常量的编程语言的函数吗？ 神经网络就像那样的函数，但它们确切地知道如何改变魔术常量以减少损失。 它们对训练数据中的每个实例都这样做，以便尽可能为尽可能多的实例产生更多的正确答案。
    当然，它们在调整参数仅一次后就不能达到完美的答案。 需要对训练数据进行多次通过，称为*epochs*。 图 2.9 显示了神经网络的整体训练过程。
- en: '![CH02_F09_Hagiwara](../Images/CH02_F09_Hagiwara.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F09_Hagiwara](../Images/CH02_F09_Hagiwara.png)'
- en: Figure 2.9 Overall training procedure for neural networks
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 神经网络的整体训练过程
- en: 'The process where a neural network computes an output from an input using the
    current set of parameters is called the *forward pass*. This is why the main function
    in listing 2.1 is called forward(). The way the loss is fed back to the neural
    network is called *backpropagation*. An algorithm called *stochastic gradient
    descent* (SGD) is often used to minimize the loss. The process where the loss
    is minimized is called *optimization*, and the algorithm (such as SGD) used to
    achieve this is called the *optimizer*. You can initialize an optimizer using
    PyTorch as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络从输入中使用当前参数集计算输出的过程称为*前向传递*。 这就是为什么列表 2.1 中的主要函数被称为 forward()。 将损失反馈给神经网络的方式称为*反向传播*。
    通常使用一种称为*随机梯度下降*（SGD）的算法来最小化损失。 将损失最小化的过程称为*优化*，用于实现此目的的算法（例如 SGD）称为*优化器*。 您可以使用
    PyTorch 初始化优化器如下：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we are using one type of optimizer called Adam. There are many types of
    optimizers proposed in the neural network community, but the consensus is that
    there is no single optimization algorithm that works well for any problem, and
    you should be ready to experiment with multiple ones for your own problem.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用一种称为 Adam 的优化器。 在神经网络社区中提出了许多类型的优化器，但共识是没有一种优化算法适用于任何问题，您应该准备为您自己的问题尝试多种优化算法。
- en: OK, that was a lot of technical terms. You don’t need to know the details of
    those algorithms for now, but it’d be helpful if you learn just the terms and
    what they roughly mean. If you write the entire training process in Python pseudocode,
    it will appear as shown in listing 2.2\. Note that there are two nested loops,
    one over epochs and another over instances.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，那是很多技术术语。 暂时你不需要了解这些算法的细节，但如果你学习一下这些术语及其大致含义会很有帮助。 如果你用 Python 伪代码来写整个训练过程，它将显示为第
    2.2 列表所示。 请注意，有两个嵌套循环，一个是在 epochs 上，另一个是在 instances 上。
- en: Listing 2.2 Pseudocode for the neural network training loop
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2.2 列表神经网络训练循环的伪代码
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 2.6 Training your own classifier
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 训练您自己的分类器
- en: In this section, we are going to train our own classifier using AllenNLP’s training
    framework. I’ll also touch upon the concept of batching, an important practical
    concept that is used in training neural network models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 AllenNLP 的训练框架来训练我们自己的分类器。 我还将简要介绍批处理的概念，这是在训练神经网络模型中使用的重要实用概念。
- en: 2.6.1 Batching
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.1 批处理
- en: So far, I have left out one piece of detail—batching. We assumed that an optimization
    step happens for each and every instance, as you saw in the earlier pseudocode.
    In practice, however, we usually group a number of instances together and feed
    them to a neural network, updating model parameters per each group, not per each
    instance. We call this group of instances a *batch*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我忽略了一个细节——批处理。 我们假设每个实例都会进行一次优化步骤，就像您在之前的伪代码中看到的那样。 但实际上，我们通常会将若干个实例分组并将它们馈送到神经网络中，每个组更新模型参数，而不是每个实例。
    我们将这组实例称为一个*批次*。
- en: Batching is a good idea for a couple of reasons. The first is stability. Any
    data is inherently noisy. Your dataset may contain sampling and labeling errors.
    If you update your model parameters for every instance, and if some instances
    contain errors, the update is influenced too much by the noise. But if you group
    instances into batches and compute the loss for the entire batch, not for individual
    instances, you can “average out” small errors and the feedback to your model stabilizes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理是一个好主意，原因有几个。第一个是稳定性。任何数据都存在噪声。您的数据集可能包含采样和标记错误。如果您为每个实例更新模型参数，并且某些实例包含错误，则更新受到噪声的影响太大。但是，如果您将实例分组为批次，并为整个批次计算损失，而不是为单个实例计算，您可以“平均”小错误，并且反馈到您的模型会稳定下来。
- en: The second reason is speed. Training neural networks involves a huge number
    of arithmetic operations such as matrix additions and multiplications, and it
    is often done on GPUs (graphics processing units). Because GPUs are designed so
    that they can process a huge number of arithmetic operations in parallel, it is
    often efficient if you pass a large amount of data and process it at once instead
    of passing instances one by one. Think of a GPU as a factory overseas that manufactures
    products based on your specifications. Because factories are often optimized for
    manufacturing a small variety of products at a large quantity, and there is overhead
    in communicating and shipping products, it is more efficient if you make a small
    number of orders for manufacturing a large quantity of products instead of making
    a large number of orders for manufacturing a small quantity of products, even
    if you want the same quantity of products in total in either way.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是速度。训练神经网络涉及大量的算术操作，如矩阵加法和乘法，并且通常在GPU（图形处理单元）上进行。因为GPU被设计成可以并行处理大量的算术操作，所以如果您一次传递大量数据并一次处理它，而不是逐个传递实例，通常会更有效率。把GPU想象成一个海外的工厂，根据您的规格制造产品。因为工厂通常被优化为大量制造少量种类的产品，并且在通信和运输产品方面存在开销，所以如果您为大量产品制造少量订单，而不是为少量产品制造大量订单，即使您希望以任何方式获得相同数量的产品，也更有效率。
- en: 'It is easy to group instances into batches using AllenNLP. The framework uses
    PyTorch’s DataLoader abstraction, which takes care of receiving instances and
    returning batches. We’ll use a BucketBatchSampler that groups instances into buckets
    of similar lengths, as shown in the next code snippet. I’ll discuss why it’s important
    in later chapters:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AllenNLP轻松将实例分组成批次。该框架使用PyTorch的DataLoader抽象，负责接收实例并返回批次。我们将使用一个BucketBatchSampler，它将实例分组成长度相似的桶，如下代码片段所示。我将在后面的章节中讨论它的重要性：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The parameter batch_size specifies the size of the batch (the number of instances
    in a batch). There is often a “sweet spot” in adjusting this parameter. It should
    be large enough to have any effect of the batching I mentioned earlier, but also
    small enough so that batches fit in the GPU memory, because factories have the
    maximum capacity of products they can manufacture at once.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 参数batch_size指定了批量的大小（批量中的实例数）。调整此参数通常有一个“最佳点”。它应该足够大，以产生我之前提到的批处理的任何效果，但也应该足够小，以便批次适合GPU内存，因为工厂有一次可以制造的产品的最大容量。
- en: 2.6.2 Putting everything together
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.2 将一切放在一起
- en: 'Now you are ready to train your sentiment analyzer. We assume that you already
    defined and initialized your model as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好训练情感分析器了。我们假设您已经定义并初始化了您的模型如下：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: See the full code listing ([http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb))
    for what the model looks like and how to use it.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 查看完整的代码清单（[http://www.realworldnlpbook.com/ch2.html#sst-nb](http://www.realworldnlpbook.com/ch2.html#sst-nb)），了解模型的外观和如何使用它。
- en: 'AllenNLP provides the Trainer class, which acts as a framework for putting
    all the components together and managing the training pipeline, as shown here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP提供了Trainer类，它作为将所有组件放在一起并管理训练流水线的框架，如下所示：
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You provide the model, optimizer, iterator, train set, dev set, and the number
    of epochs you want to the trainer and invoke the train method. The last parameter,
    cuda_device, tells the trainer which device (CPU or GPU) to use to use for training.
    Here, we are explicitly using the CPU. This will run the neural network training
    loop described in listing 2.2 and display the progress, including the evaluation
    metrics.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你向训练器提供模型、优化器、迭代器、训练集、开发集和你想要的时期数，并调用 train 方法。最后一个参数，cuda_device，告诉训练器使用哪个设备（CPU
    或 GPU）进行训练。在这里，我们明确地使用 CPU。这将运行列在列表 2.2 中的神经网络训练循环，并显示进展情况，包括评估指标。
- en: 2.7 Evaluating your classifier
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 评估你的分类器
- en: When training an NLP/ML model, you should always monitor how the loss changes
    over time. If the training is working as expected, you should see the loss decrease
    over time. It doesn’t always decrease each epoch, but it should decrease as a
    general trend, because this is exactly what you told the optimizer to do. If it’s
    increasing or showing weird values (such as NaN), it’s usually a sign that your
    model is too limiting or there’s a bug in your code.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练自然语言处理/机器学习模型时，你应该始终监控损失随时间的变化。如果训练正常进行，你应该看到损失随时间而减少。它不一定每个时期都会减少，但作为一般趋势，它应该会减少，因为这正是你告诉优化器要做的事情。如果它在增加或显示出奇怪的值（如
    NaN），通常意味着你的模型过于局限或代码中存在错误的迹象。
- en: In addition to the loss, it is important to monitor other evaluation metrics
    you care about in your task. Loss is a purely mathematical concept that measures
    the closeness between your model and the answer, but smaller losses do not always
    guarantee better performance in the NLP task.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 除了损失之外，监控你在任务中关心的其他评估指标也很重要。损失是一个纯数学概念，衡量了模型与答案之间的接近程度，但较小的损失并不总是能保证在自然语言处理任务中获得更好的性能。
- en: You can use a number of evaluation metrics, depending on the nature of your
    NLP task, but some that you need to know no matter what task you are working on
    include accuracy, precision, recall, and F-measure. Roughly speaking, these metrics
    measure how precisely your model’s predictions match the expected answers defined
    by the dataset. For now, it suffices to know that they are used to measure how
    good your classifier is (more details coming in chapter 4).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用许多评估指标，取决于你的自然语言处理任务的性质，但无论你在做什么任务，你都需要了解的一些指标包括准确率、精确率、召回率和 F-度量。粗略地说，这些指标衡量了你的模型预测与数据集定义的预期答案匹配的程度。暂时来说，知道它们用于衡量分类器的好坏就足够了（更多细节将在第四章介绍）。
- en: To monitor and report evaluation metrics during training using AllenNLP, you
    need to implement the get_metrics() method in your model class, which returns
    a dict from metric names to their values, as shown next.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要在训练期间使用 AllenNLP 监控和报告评估指标，你需要在你的模型类中实现 get_metrics() 方法，该方法返回从指标名称到它们的值的字典，如下所示。
- en: Listing 2.3 Defining evaluation metrics
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 定义评估指标
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'self.accuracy and self.f1_measure are defined in __init__() as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: self.accuracy 和 self.f1_measure 在 __init__() 中定义如下：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When you run trainer.train() with the metrics defined, you’ll see progress
    bars like these after every epoch:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用定义好的指标运行 trainer.train() 时，你会在每个时期后看到类似下面的进度条：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can see that the training framework reports these metrics both for the train
    and the validation sets. This is useful not only for evaluating your model but
    also for monitoring the progress of the training. If you see any unusual values,
    such as extremely low or high numbers, you’ll know that something is wrong, even
    before the training completes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到训练框架报告了这些指标，分别针对训练集和验证集。这不仅有助于评估模型，还有助于监控训练的进展。如果看到任何异常值，比如极低或极高的数字，你会知道出了问题，甚至在训练完成之前就能发现。
- en: You may have noticed a large gap between the train and the validation metrics.
    Specifically, the metrics for the train set are a lot higher than those for the
    validation set. This is a common symptom of overfitting, which I mentioned earlier,
    where a model fits to a train set so well that it loses generalizability outside
    of it. This is why it’s important to monitor the metrics using a validation set
    as well, because you won’t know if it’s just doing well or overfitting only by
    looking at the training set metrics!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到训练集和验证集的指标之间存在很大的差距。具体来说，训练集的指标比验证集的指标高得多。这是过拟合的常见症状，我之前提到过，即模型在训练集上拟合得非常好，以至于失去了在外部的泛化能力。这就是为什么监控指标使用验证集也很重要，因为仅仅通过观察训练集的指标，你无法知道它是表现良好还是过拟合！
- en: 2.8 Deploying your application
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 部署你的应用程序
- en: The final step in making your own NLP application is deploying it. Training
    your model is only half the story. You need to set it up so that it can make predictions
    for new instances it has never seen. Making sure the model is serving predictions
    is critical in real-world NLP applications, and a lot of development efforts may
    go into this stage. In this section, I’m going to show what it’s like to deploy
    the model we just trained using AllenNLP. This topic is discussed in more detail
    in chapter 11.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 制作自己的NLP应用程序的最后一步是部署它。训练模型只是故事的一半。你需要设置它，以便它可以为它从未见过的新实例进行预测。确保模型提供预测在实际的NLP应用程序中是至关重要的，而且在这个阶段可能会投入大量的开发工作。在本节中，我将展示如何使用AllenNLP部署我们刚刚训练的模型。这个主题在第11章中会更详细地讨论。
- en: 2.8.1 Making predictions
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.1 进行预测
- en: To make predictions for new instances your model has never seen (called *test
    instances*), you need to pass them through the same neural network pipeline as
    you did for training. It has to be exactly the same—otherwise, you’ll risk skewing
    the result. This is called *training-serving skew*, which I’ll explain in chapter
    11.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要对你的模型从未见过的新实例进行预测（称为*测试实例*），你需要通过与训练相同的神经网络管道来传递它们。它必须完全相同——否则，你将冒着结果扭曲的风险。这被称为*训练-服务偏差*，我将在第11章中解释。
- en: 'AllenNLP provides a convenient abstraction called *predictors*, whose job it
    is to receive an input in its raw form (e.g., raw string), pass it through the
    preprocessing and neural network pipeline, and give back the result. I wrote a
    specific predictor for SST called SentenceClassifierPredictor ([http://realworldnlpbook.com/ch2
    .html#predictor](http://realworldnlpbook.com/ch2.html#predictor)), which you can
    call as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP提供了一个方便的抽象称为*预测器*，它的工作是接收原始形式的输入（例如，原始字符串），将其通过预处理和神经网络管道传递，并返回结果。我为SST编写了一个特定的预测器称为SentenceClassifierPredictor（[http://realworldnlpbook.com/ch2
    .html#predictor](http://realworldnlpbook.com/ch2.html#predictor)），你可以按照以下方式调用它：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Note that the predictor returns the raw output from the model, which is logits
    in this case. Remember, logits are some sort of scores corresponding to target
    labels, so if you want the predicted label itself, you need to convert it to the
    label. You don’t need to understand all the details for now, but this can be done
    by first taking the argmax of the logits, which returns the index of the logit
    with the maximum value, and then by looking up the label by the ID, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，预测器返回模型的原始输出，在这种情况下是logits。记住，logits是与目标标签对应的一些分数，所以如果你想要预测的标签本身，你需要将其转换为标签。你现在不需要理解所有的细节，但可以通过首先取logits的argmax来完成这个操作，argmax返回具有最大值的logit的索引，然后通过查找ID来获取标签，如下所示：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If this prints out a “4,” congratulations! Label “4” corresponds to “very positive,”
    so your sentiment analyzer just predicted that the sentence “This is the best
    movie ever!” is very positive, which is indeed correct.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个打印出“4”，那么恭喜你！标签“4”对应着“非常积极”，所以你的情感分析器刚刚预测到句子“这是有史以来最好的电影！”是非常积极的，这的确是正确的。
- en: 2.8.2 Serving predictions
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.2 提供预测
- en: 'Finally, you can easily deploy the trained model using AllenNLP. If you use
    a JSON configuration file (which I’ll explain in chapter 4), you can save your
    trained model onto disk and then quickly fire up a web-based interface where you
    can make requests to your model. To do this, you need to install allennlp-server,
    a plugin for AllenNLP that provides a web interface for prediction, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用AllenNLP轻松部署训练好的模型。如果你使用JSON配置文件（我将在第四章中解释），你可以将训练好的模型保存到磁盘上，然后快速启动一个基于Web的界面，你可以向你的模型发送请求。要做到这一点，你需要安装allennlp-server，这是一个为AllenNLP提供预测的Web接口的插件，如下所示：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Assuming your model is saved under examples/sentiment/model, you can run a
    Python-based web application using the following AllenNLP command:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的模型保存在examples/sentiment/model下，你可以使用以下AllenNLP命令运行一个基于Python的Web应用程序：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If you open http:/./localhost:8000/ using your browser, you’ll see the interface
    shown in figure 2.10.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用浏览器打开http:/./localhost:8000/，你将看到图2.10中显示的界面。
- en: '![CH02_F10_Hagiwara](../Images/CH02_F10_Hagiwara.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![CH02_F10_Hagiwara](../Images/CH02_F10_Hagiwara.png)'
- en: Figure 2.10 Running the sentiment analyzer on a web browser
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 在Web浏览器上运行情感分析器
- en: Try typing some sentences in the sentence text box, and click Predict. You should
    see the logits values on the right side of the screen. They are just a raw array
    of logits and hard to read, but you can see that the fourth value (which corresponds
    to the label “very positive”) is the largest and the model is working as expected.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在句子文本框中输入一些句子，然后点击预测。您应该在屏幕右侧看到逻辑值。它们只是一组原始的逻辑值，很难阅读，但您可以看到第四个值（对应标签“非常积极”）是最大的，模型正在按预期工作。
- en: 'You can also directly make POST requests to the backend from the command line
    as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以直接从命令行向后端进行POST请求，如下所示：
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This should return the same JSON as you saw above:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回与上面看到的相同的JSON：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: OK, that’s it for now. We covered a lot in this chapter, but don’t worry—I just
    wanted to show you that it is easy to build an NLP application that actually works.
    You may have found some books or online tutorials about neural networks and deep
    learning intimidating, or you may have even given up on learning before creating
    anything that works. Notice that I didn’t even mention any concepts such as neurons,
    activations, gradient, and partial derivatives, which other learning materials
    teach at the very beginning. These concepts are indeed important and helpful to
    know, but thanks to powerful frameworks such as AllenNLP, you are also able to
    build practical NLP applications without fully understanding their details. In
    later chapters, I’ll go into more details and discuss these concepts as needed.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，就到这里吧。在这一章中我们讨论了很多内容，但不要担心——我只是想告诉你，构建一个实际可用的自然语言处理应用程序是很容易的。也许你曾经发现一些关于神经网络和深度学习的书籍或在线教程让人望而生畏，甚至在创建任何实际可用的东西之前就放弃了学习。请注意，我甚至没有提到任何诸如神经元、激活、梯度和偏导数等概念，这些概念通常是其他学习资料在最初阶段教授的。这些概念确实很重要，而且有助于了解，但多亏了强大的框架如AllenNLP，你也能够构建实用的自然语言处理应用程序，而不必完全了解其细节。在后面的章节中，我将更详细地讨论这些概念。
- en: Summary
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Sentiment analysis is a text analytic technique to automatically identify subjective
    information within text, such as its polarity (positive or negative).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析是一种文本分析技术，用于自动识别文本中的主观信息，如其极性（积极或消极）。
- en: Train, dev, and test sets are used to train, choose, and evaluate machine learning
    models.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集、开发集和测试集用于训练、选择和评估机器学习模型。
- en: Word embeddings represent the meaning of words using vectors of real numbers.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入使用实数向量表示单词的含义。
- en: Recurrent neural networks (RNNs) and linear layers are used to convert a vector
    to another vector of different size.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）和线性层用于将一个向量转换为另一个不同大小的向量。
- en: Neural networks are trained using an optimizer so that the loss (discrepancy
    between the actual and the desired output) is minimized.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优化器训练神经网络，以使损失（实际输出与期望输出之间的差异）最小化。
- en: It is important to monitor the metrics for the train and the dev sets during
    training to avoid overfitting.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中监视训练集和开发集的指标非常重要，以避免过拟合。
