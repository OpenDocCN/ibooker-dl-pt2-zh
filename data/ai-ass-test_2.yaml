- en: 2 Large language models and prompt engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 大型语言模型和提示工程
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章包括
- en: Outlining the fundamentals of how large language models work
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述大型语言模型的基本工作原理
- en: Understanding the risks of using large language models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解使用大型语言模型的风险
- en: Defining prompt engineering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义提示工程
- en: Experimenting with prompt engineering to return various outputs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提示工程进行实验以返回各种输出
- en: Solving problems with prompt engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用提示工程解决问题
- en: In the previous chapter, we learnt that it’s important to take the time to familiarize
    ourselves with new tools, and it’s that mindset we’ll be adopting in this chapter.
    Throughout this book, we’ll be exploring how to specifically use generative AI
    tools such as Open AI’s ChatGPT and GitHub Copilot, which are built on large language
    models, or LLMs. There are many ways in which AI is used in testing, but what
    makes LLMs so interesting is their adaptability to different situations—hence,
    their rise in popularity. So before we look at how we can adopt LLM tools into
    our day-to-day testing, let’s first learn a bit about what LLMs are and how they
    work, and how to get the most benefit out of them, by learning the concept of
    prompt engineering.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学到了花时间熟悉新工具很重要，而且这种心态我们将在本章中采用。在整本书中，我们将探讨如何专门使用生成式AI工具，例如Open AI的ChatGPT和GitHub
    Copilot，它们是构建在大型语言模型或LLMs上的。虽然AI在测试中有许多用途，但LLMs之所以如此有趣，是因为它们适应不同情况的能力，因此它们越来越受欢迎。因此，在我们看如何将LLM工具应用到我们的日常测试之前，让我们先了解一下LLMs是什么，它们是如何工作的，以及通过学习提示工程的概念，如何从中获得最大的好处。
- en: 'What has made LLMs like ChatGPT dominate tech headlines throughout 2023? Consider
    this sample interaction with ChatGPT that I had:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使得像ChatGPT这样的LLM主导了2023年的技术头条？考虑一下我与ChatGPT的这次互动：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Based on this quick “conversation,” we can see that:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这次快速的“对话”，我们可以看出：
- en: I can interact with ChatGPT with natural language. No traditional programming
    experience was required to get results from ChatGPT.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以用自然语言与ChatGPT进行交互。要从ChatGPT获取结果并不需要传统的编程经验。
- en: The output from ChatGPT is also in natural language. It’s easy to understand
    and react to.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT的输出也是用自然语言。它易于理解和回应。
- en: Advocates of LLMs are celebrating that these types of AI tools have democratized
    the use of AI, allowing anyone to use them to get results. However, this democratization
    is a double-edged sword. The nature in which we interact with LLMs can give us
    the illusion that we’re talking with a machine that reasons in the same way we,
    as humans, do. But making that assumption can impact our ability to get the most
    out of an LLM. So to get the best results out of tools like ChatGPT, it helps
    to understand how they work (at least in layman’s terms) to better understand
    how they can fit into our testing activities and how to extract the most value
    from them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的倡导者庆祝这些类型的人工智能工具已经使AI的使用实现了民主化，允许任何人使用它们来获取结果。然而，这种民主化是一把双刃剑。我们与LLMs互动的方式可能会让我们产生错觉，认为我们正在和一个像我们人类一样推理的机器对话。但是做出这样的假设可能会影响我们充分利用LLM的能力。因此，为了更好地利用诸如ChatGPT之类的工具，了解它们的工作方式（至少在外行人的看法下）有助于更好地理解它们如何适应我们的测试活动，并如何从中获得最大价值。
- en: 2.1 Large language models, explained
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型，解释
- en: How does a person with relatively little experience in building AI systems explain
    how a complex LLM system works? Fortunately, in the Computerphile video “AI Language
    Models & Transformers” ([www.youtube.com/watch?v=rURRYI66E54](www.youtube.com.html)),
    Rob Miles offers an example that can help us gain a fundamental grasp on what
    LLMs do. (I strongly recommend watching all his videos on AI.)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对缺乏构建AI系统经验的人如何解释复杂的LLM系统的工作方式？幸运的是，在Computerphile视频“AI Language Models &
    Transformers”（[www.youtube.com/watch?v=rURRYI66E54](www.youtube.com.html)），Rob
    Miles提供了一个示例，可以帮助我们基本了解LLMs的工作原理。（我强烈推荐观看他关于AI的所有视频。）
- en: 'Take out your phone and open up a messaging app, or any other app that causes
    your keyboard to appear. Above the keyboard, you’ll likely see a selection of
    suggested words to insert into your message. For example, my keyboard offers me
    these suggestions: *I,* *I am,* and *The.* Selecting one of these options, such
    as *I am,* causes the suggestions to update. For me, it offered the options *away,*
    *away for,* and *now.* Selecting the option away *for* once again updates the
    options to select from. So how does the keyboard know which options to show and
    which ones not to?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 拿出你的手机并打开一个消息应用程序，或任何导致键盘出现的其他应用程序。在键盘上方，你可能会看到一系列建议插入到消息中的单词。例如，我的键盘为我提供了这些建议：*I*、*I
    am* 和 *The*。选择其中一个选项，例如 *I am*，会导致建议更新。对我来说，它提供了 *away*、*away for* 和 *now* 的选项。再次选择
    *away for* 选项将更新可供选择的选项。那么键盘是如何知道应该显示哪些选项以及不显示哪些选项呢？
- en: In your keyboard is an AI model that behaves in a similar manner to LLMs. This
    description is an oversimplification, but at its core, the keyboard on your phone
    is using the same machine learning approach as an LLM, by leveraging probability.
    Language is a complex and fluid set of rules, meaning any attempt to codify relationships
    explicitly is almost impossible. So instead, a model is trained on massive data
    sets to implicitly learn the relationships in language and to create a probability
    distribution that is used to predict what the next word might be. This can best
    be described by visualizing the options available from the keyboard example, as
    shown in Figure 2.1
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的键盘中是一个与 LLM 类似的 AI 模型。这种描述是简化的，但在其核心上，你手机上的键盘正使用与 LLM 相同的机器学习方法，通过利用概率。语言是一套复杂且流动的规则，这意味着试图明确规定关系几乎是不可能的。因此，模型被训练在大规模数据集上，以隐含地学习语言中的关系，并创建一个概率分布，用来预测下一个单词可能是什么。这可以通过可视化键盘示例中提供的选项最好的描述，如图
    2.1 所示。
- en: Figure 2.1 Probability distribution in action
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.1 行动中的概率分布
- en: '![](images/02__image001.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](images/02__image001.png)'
- en: As we can see when we select the term *I am,* our model within our keyboard
    has been trained to assign probabilities to a vast range of words. Some of these
    will have a high probability of coming after ‘I am’ such as ‘away’ and some will
    have a low probability, such as ‘sandalwood’. As mentioned before, these probabilities
    are coming from a model that has completed a training process, known as unsupervised
    learning, in which vast amounts of data have been sent to an algorithm to process.
    It’s from that training process that a model is created with complex weights and
    balances within it that give the model its predictive abilities.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在选择术语 *I am* 时所见，键盘中的模型已经被训练为对大量单词进行概率分配。其中一些在 “I am” 之后有很高的概率，比如 “away”，而一些则概率较低，比如
    “sandalwood”。如前所述，这些概率来自已经完成训练过程的模型，称为无监督学习，其中大量数据被发送给一个算法进行处理。正是通过这个训练过程，一个模型被创建，其中包含复杂的权重和平衡，使模型具有预测能力。
- en: Supervised learning and unsupervised learning
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习
- en: When training AI, two of the more dominant techniques to use are supervised
    learning and unsupervised learning. Which learning approach is used will determine
    how data has been structured and sent to an algorithm. *Supervised* learning uses
    data that has been organized, labeled, and paired with an output. For example,
    a medical data set might contain labeled data that includes BMI, Age, and Sex,
    for example, which is paired with a labeled outcome about whether they suffered
    a specific illness—say, a heart attack or stroke. *Unsupervised* learning, on
    the other hand, uses data that isn’t labeled and has no output data. The idea
    is that when an algorithm is trained on this type of data, it learns the implicit
    patterns within the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 AI 时，使用的两个主要技术是监督学习和无监督学习。使用哪种学习方法将决定数据如何被结构化并发送给算法。*监督*学习使用已被组织、标记并与输出配对的数据。例如，一个医学数据集可能包含标记的数据，包括
    BMI、年龄和性别，例如，这与标记的结果配对，例如他们是否患有特定疾病，例如心脏病或中风。*无监督*学习，另一方面，使用未标记且没有输出数据的数据。其思想是，当算法在这种类型的数据上进行训练时，它会学习数据中的隐含模式。
- en: Chances are good that if you play around with the predictive function on your
    keyboard, the output will differ from mine—even if we have the same phone and
    operating system. This is because once the model has been trained and is utilized
    within our phones, it’s still being fine-tuned by what we type into our phones.
    I travel for work, so I have to let people know when I am away and when I’m available.
    (It is perhaps a damning indictment of my work-life balance!) So terms such as
    ‘I am’ and ‘away’ have an increased probability as they are words I use more regularly.
    This is known as Reinforcement Learning with Human Feedback, or RLHF.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在键盘上玩弄预测功能，很可能输出与我的不同，即使我们使用的是相同的手机和操作系统。这是因为一旦模型被训练并在我们的手机中使用，它仍然会受到我们在手机上输入的内容的微调影响。我出差工作，所以我必须让人们知道我何时离开和何时可用。（这也许是对我工作与生活的平衡的一种谴责！）因此，“我是”和“离开”这样的术语具有增加的概率，因为它们是我更经常使用的词汇。这被称为人类反馈强化学习，或
    RLHF。
- en: 'Again, comparing predictive messaging on a phone to an LLM is an oversimplification,
    but the comparison holds true. LLMs also use non-supervised learning and RLHF.
    The difference, however, is that although an AI model on a phone can look at perhaps
    the last five words typed to predict the next, LLMs use cutting-edge techniques,
    such as these:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，将手机上的预测消息与大型语言模型进行比较是一个过于简化的比较，但这种比较是成立的。大型语言模型还使用非监督学习和 RLHF。然而，不同之处在于，虽然手机上的
    AI 模型可以查看可能是最近输入的五个词来预测下一个词，但大型语言模型使用了尖端技术，例如：
- en: '*G*enerative *p*retrained *t*ransformers (which is what makes the GPT abbreviation
    in *ChatGPT*)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成预训练变换器*（这是 *ChatGPT* 中 GPT 缩写的含义）'
- en: Powerful hardware infrastructure using thousands of servers
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数千台服务器的强大硬件基础设施
- en: Training data on a scale that would dwarf what our humble keyboard model will
    have been trained on
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在规模上训练数据将使我们的简单键盘模型所接触到的培训数据相形见绌
- en: Do we need to know the intricacies of each of these points? Not really, but
    it helps us to appreciate a key aspect of LLMs. The output of LLMs, no matter
    how powerful, is probabilistic. LLMs are not a repository of information, there
    is structured knowledge stored within them like we would see on the wider internet.
    This means that how it comes to conclusions differs from how we humans come to
    conclusions (probability rather than experience), which is what makes them so
    powerful—but also risky to use if we aren’t vigilant about how we use them.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解这些要点的复杂性吗？实际上不需要，但这有助于我们欣赏大型语言模型的一个关键方面。大型语言模型的输出，无论多么强大，都是概率性的。大型语言模型不是信息的存储库，其中没有像我们在更广泛的互联网上看到的结构化知识。这意味着它做出结论的方式不同于人类做出结论的方式（概率而不是经验），这就是它们如此强大但如果我们对如何使用它们不加警惕的话也是有风险的原因。
- en: 2.2 Avoiding the risks of using large language models
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 避免使用大型语言模型的风险
- en: Having an AI predict what word goes after another isn’t an easy task, and though
    current LLMs have seen an explosion in ability, there are risks we need to be
    conscious of. Let’s take a look at a few of them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个 AI 预测接下来的一个词并不是一件容易的事情，尽管当前的大型语言模型的能力有了爆炸性的增长，但我们需要意识到其中的风险。让我们来看看其中的一些。
- en: 2.2.1 Hallucinations
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 幻觉
- en: 'The challenge with text prediction is to ensure that the output of an LLM makes
    sense and is rooted in reality. For example, back in chapter 1, when I asked ChatGPT
    to write me an introduction to this book, it shared the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预测的挑战在于确保大型语言模型的输出是合乎逻辑并且根植于现实。例如，在第 1 章中，当我要求 ChatGPT 为这本书写个介绍时，它分享了以下内容：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Initially, when LLMs were being developed, their output didn’t make much sense.
    The text would be readable, but it lacked structure or grammatical sense. If we
    read this example, it parses perfectly well, and it makes sense. However, as I
    mentioned, the book that ChatGPT describes doesn’t exist. This is known, in the
    context of an LLM, as a *hallucination.* The LLM is able to output a clear statement
    in a way that grants it some authority, but what has been written is false.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，在开发大型语言模型时，它们的输出并不合乎逻辑。文本可以读懂，但缺乏结构或语法上的意义。如果我们阅读这个例子，它解析得非常好，而且有意义。然而，正如我所提到的，ChatGPT
    描述的这本书并不存在。在大型语言模型的上下文中，这被称为*幻觉*。大型语言模型能够清晰地输出一个陈述，从而赋予它一些权威性，但所写的是错误的。
- en: 'Why an LLM hallucinates is not entirely clear. One of the challenges of working
    with LLMs is that they act like a black box. It’s difficult to monitor how an
    LLM reached a specific conclusion, which is compounded by its indeterminate nature.
    Just because I got an output that contained a hallucination doesn’t mean that
    others will do the same in the future. (This is where Reinforced Learning with
    Human Feedback (RLHF) helps to combat hallucinations: we can inform the model
    whether its output is false and it will learn from that).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么LLM会产生幻觉并不完全清楚。与LLM合作的一个挑战是它们的行为就像一个黑匣子。很难监控LLM是如何得出特定结论的，这又加剧了它的不确定性。仅仅因为我得到了一个包含幻觉的输出，并不意味着其他人将来也会得到相同的结果。（这就是强化学习与人类反馈（RLHF）帮助对抗幻觉的地方：我们可以告知模型其输出是否是错误的，它将从中学习）。
- en: The risk of hallucinations means we must always maintain an element of skepticism
    when interpreting the output of an LLM. We need to be mindful that what is being
    returned from an LLM is predictive and not always correct. We can’t turn off our
    critical thinking just because a tool appears to be behaving in a way that mimics
    how a human might.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**幻觉的风险**意味着我们在解释LLM输出时必须始终保持一种怀疑的态度。我们需要注意，LLM返回的内容是具有预测性质的，并不总是正确的。我们不能因为一个工具表现出模仿人类行为的方式就关闭我们的批判性思维。'
- en: 2.2.2 Data provenance
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**2.2.2 数据来源**'
- en: For most users of LLMs, it’s not just how the model precisely works that is
    a black box to us, but also the data it has been trained on. Since ChatGPT's explosion
    in popularity, the conversation around data ownership and copyright has intensified.
    Companies like X (formerly known as Twitter) and Reddit have accused OpenAI of
    stealing their data wholesale, and, at the time of writing, a class action lawsuit
    against OpenAI has been filed by a collection of authors who accuse the company
    of breaching copyright law by training models on their works ([www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books](05.html)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数LLM用户来说，对于模型确切工作方式的不了解对我们来说并不是唯一的黑匣子，还有它所训练的数据。自ChatGPT爆红以来，围绕数据所有权和版权的讨论日益加剧。公司如X（前身为Twitter）和Reddit指责OpenAI大规模盗用了他们的数据，并且在撰写本文时，一群作者已经对OpenAI提起了集体诉讼，指控该公司通过训练模型侵犯了他们的版权（[www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books](05.html)）。
- en: The results from these debates are yet to be seen, but if we bring this topic
    back to the world of software development, we must be mindful of what material
    an LLM has been trained on. For example, ChatGPT at one point would return nonsensical
    responses when specific phrases were sent to it, all because it had been trained
    on data from the subreddit r/counting, which is full of data that is, on the surface,
    seemingly nonsensical itself. You can learn more from Computerphile about this
    weird behavior at [www.youtube.com/watch?v=WO2X3oZEJOA](www.youtube.com.html)).
    If an LLM has been trained on garbage, it will output garbage.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些争论的结果尚未见分晓，但如果我们把这个话题带回软件开发的世界，我们必须注意LLM已经训练过的材料。例如，ChatGPT曾经在发送特定短语时返回荒谬的回复，这完全是因为它曾经受过来自r/counting子论坛的数据训练，该数据在表面上看起来似乎也是毫无意义的。您可以在Computerphile了解更多关于这种怪异行为的信息（[www.youtube.com/watch?v=WO2X3oZEJOA](www.youtube.com.html)）。如果LLM受到垃圾数据的训练，它将输出垃圾。
- en: This becomes important when we consider tools such as GitHub Copilot, for example,
    uses the same GPT model that ChatGPT uses. Copilot has been fine-tuned differently,
    using the billions of lines of code stored in GitHub so that it can act as an
    assistant and suggest code snippets as we develop our codebase. We’ll explore
    in later chapters how we can put Copilot to good use, but again we should be critical
    of what it suggests and not blindly accept everything it offers as a suggestion.
    Why? Ask yourself, are you happy with the code you’ve created in the past? Do
    you trust all the code others have created? If a large population of engineers
    is prone to implementing bad patterns, then that is what tools like Copilot will
    have been trained on. The point is a little hyperbolic, because a lot of good
    developers and testers out there do good work—good work that Copilot is trained
    on. But it’s a thought exercise worth considering every now and then just to ensure
    that we remember who is in the driver’s seat when building applications with LLMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到像GitHub Copilot这样的工具时，就变得非常重要了。例如，Copilot使用了ChatGPT使用的相同的GPT模型，通过使用GitHub存储的数十亿行代码来进行微调，以便在我们开发代码库时充当助手并提供建议代码片段。我们将在后续章节中探讨如何充分利用Copilot，但我们应该对其建议持批判态度，不能盲目接受它提供的一切建议。为什么呢？问问自己，你对过去所创造的代码满意吗？你相信别人所创造的所有代码吗？如果有大量的工程师往往采用不好的编程模式，那么这就是Copilot所训练的。这个观点有些夸张，因为有很多优秀的开发人员和测试人员在做出很好的工作，而Copilot的训练也基于这些好的工作。但这是一个值得思考的思维实验，以确保我们记住在使用LLMs构建应用程序时，驾驶座位上的是谁。
- en: 2.2.3 Data privacy
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 数据隐私
- en: Just as we need to be mindful of what an LLM outputs, we also have to consider
    what we enter into them. The temptation to share material with LLMs to find answers
    to problems we’re facing will be strong. But we have to ask ourselves, where is
    the data we send being stored? As mentioned earlier, LLMs are continuously being
    tweaked through RLFH feedback. Companies like OpenAI and GitHub will take the
    information we share, store it, and use it for future model training (GitHub does
    offer some privacy controls over what it can store though).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要关注LLM的输出内容，同样也需要考虑我们输入给它们的内容。面临问题时，与LLMs分享材料以寻找答案的冲动会很大。但我们必须问问自己，我们发送的数据存储在哪里？如前所述，LLM持续通过RLFH反馈进行调整。OpenAI和GitHub等公司将获取我们分享的信息，存储并用于未来的模型训练（GitHub虽然提供一些隐私控制机制来限制其可以存储的内容）。
- en: 'This can be problematic when working for companies (or for ourselves) who want
    to keep their intellectual property private. Take Samsung, whose employees accidentally
    leaked confidential material through the use of ChatGPT, as described here by
    TechRadar:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些希望保持其知识产权私有的公司（或我们自己），这可能会引起问题。以三星为例，其员工通过使用ChatGPT意外泄露了机密材料，TechRadar对此进行了描述：
- en: The company allowed engineers at its semiconductor arm to use the AI writer
    to help fix problems with their source code. But in doing so, the workers inputted
    confidential data, such as the source code itself for a new program and internal
    meeting notes data relating to their hardware.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 公司允许其半导体部门的工程师使用AI编写器来帮助修复源代码中的问题。但在此过程中，工作人员输入了机密数据，如新程序的源代码本身和与其硬件有关的内部会议笔记数据。
- en: You can read about it at [www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt](news.html).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt](news.html)上阅读相关详情。
- en: As the adoption of LLMs begins to increase across organizations, we may begin
    to see an increase in policies that restrict what we can and can’t use LLMs for.
    Some may ban the use of third-party LLMs and some organisations will opt to train
    and deploy their own internal LLMs for internal use (a topic we will explore in
    part 3 of this book). The result of those decisions will be highly contextual
    but they will impact what type of LLMs we use and what data we can and cannot
    send, underlying our need to be mindful of what we send to LLMs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织对LLM的采用不断增加，我们可能会看到一些限制我们可以和不能使用LLM的政策的增加。有些组织可能禁止使用第三方LLM，而有些组织则选择培训和部署自己的内部LLM供内部使用（这是本书第3部分要讨论的话题）。这些决策的结果将高度依赖上下文，但它们将影响我们使用何种类型的LLM以及什么数据可以发送和不能发送，这也强调了我们需要注意发送给LLM的内容。
- en: It’s also important to keep customer privacy in mind, as we have an obligation
    to not only the companies we work for (especially for those who sign Non-disclosure
    agreements) but also our users. We have a legal and moral duty to protect user
    data from being spread into the wild, where we have no oversight.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 还要重视客户隐私，因为我们有责任不仅对我们工作的公司（特别是那些签署了保密协议的公司）负责，还对我们的用户负责。我们有法律和道德义务保护用户数据不被传播到无法监督的地方。
- en: In conclusion, though LLMs provide a wealth of opportunities, we must avoid
    the trap of anthropomorphizing them. Treating LLMs as if they have come to conclusions
    in the same way as we as humans do is a fallacy. It can entrench a level of trust
    in the output that is dangerous and likely mean that we aren’t getting the most
    benefit out of them. However, if we learn to leverage the probabilistic nature
    of LLMs when we instruct them, we can increase our chances of creating outputs
    that can help us improve efficiency—which is where prompt engineering can help
    us.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然LLM提供了丰富的机会，但我们必须避免将它们拟人化的陷阱。将LLM视为像我们人类一样得出结论的方式是错误的。这可能会使我们对输出产生一种危险的信任，并且可能意味着我们没有充分利用它们。但是，如果我们学会利用LLM在指导它们时的概率性质，我们可以增加创建能够帮助我们提高效率的输出的机会，而这正是提示工程可以帮助我们的地方。
- en: 2.3 Improving results with prompt engineering
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 使用提示工程改进结果
- en: We are using natural language to prompt an LLM to return a desired outcome,
    but because they are probabilistic, we can communicate with them in a way that
    differs from normal interaction with humans. As LLMs have developed, a new field
    of engineering has appeared, known as *prompt engineering,* which contains a collection
    of patterns and techniques that we can use to increase the likelihood that we
    get a desired output from an LLM.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用自然语言提示LLM返回所需的结果，但由于它们是概率性的，我们可以以与与人类正常交流不同的方式与它们交流。随着LLM的发展，出现了一个新的工程领域，被称为*提示工程*，其中包含一系列我们可以使用的模式和技术，以增加我们从LLM获得所需输出的可能性。
- en: What is a prompt?
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是提示？
- en: 'In this book, we’ll use the term *prompt* regularly, as this will be our primary
    means of communicating with LLMs. When we use the term prompt we are simply referring
    to the natural language input that is sent to an LLM. For example, in the first
    example of this chapter I sent the following prompt:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将经常使用术语*提示*，因为这将是我们与LLM进行通信的主要方式。当我们使用术语提示时，我们只是指发送给LLM的自然语言输入。例如，在本章的第一个示例中，我发送了以下提示：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: which prompted the LLM to return a response to me.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这促使LLM向我返回了一个响应。
- en: In the coming chapters, we will use prompt engineering heavily to trigger LLMs
    to deliver a range of useful content for various testing activities. But before
    we begin, it’s worthwhile learning the fundamentals of prompt engineering so that
    we can see how prompts are built to maximize output from LLMs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将大量使用提示工程来触发LLM为各种测试活动提供一系列有用的内容。但在我们开始之前，值得学习提示工程的基础知识，以便我们可以看到如何构建提示以最大化LLM的输出。
- en: 'To help us better understand what prompt engineering is, consider these two
    prompts sent to an LLM. The first is a general question:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们更好地理解什么是提示工程，考虑发送给LLM的这两个提示。第一个是一个普通的问题：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The second is a more detailed prompt:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是一个更详细的提示：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Comparing the two, we can see that the second example is more detailed, with
    explicit requests and examples to outline what we might expect the LLM to return.
    Although the intention is similar, the output from each is drastically different.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两者，我们可以看到第二个示例更加详细，具有明确的请求和示例来概述我们可能期望LLM返回的内容。尽管意图相似，但每个的输出却截然不同。
- en: 'Compare the output from the first example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 比较第一个示例的输出：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'against the second example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对第二个示例进行对比：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Imagine that we want to use the proposed risks in each example to guide our
    testing. The first example has outputted suggestions that are vague and abstract.
    We would still need to do a fair bit of work to break down the large topics, such
    as security risks, whereas with the second example, we have specific, actionable
    risks that we could use easily. And the goal of using tools like LLM is to *reduce*
    the workload, not increase it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想要使用每个示例中提出的风险来指导我们的测试。第一个示例输出的建议模糊而抽象。我们仍然需要做大量工作来分解大主题，例如安全风险，而对于第二个示例，我们有具体的可操作风险，可以轻松使用。而使用LLM等工具的目标是*减少*工作量，而不是增加。
- en: Our second prompt yields better results because the instructions it gives have
    been considered, and are detailed and clear, which is what prompt engineering,
    at its core, is about. Though both prompts are using natural language, with prompt
    engineering we are aware of how an LLM works and what we want it to return carefully
    to inform how we write a prompt so that we maximize the chances of a desired outcome.
    When using prompt engineering, we appreciate that although an LLM communicates
    in plain language, how it processes our request differs from how a human might
    do so, which means we can adopt specific techniques to steer an LLM in the direction
    we want.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个提示产生了更好的结果，因为它给出的指示经过了考虑，而且是详细和清晰的，这正是提示工程的核心所在。尽管两个提示都使用自然语言，但是在提示工程中，我们清楚地了解了LLM的工作原理以及我们希望它返回什么，从而仔细思考如何编写提示以最大程度地实现预期结果。在使用提示工程时，我们意识到，虽然LLM以简单的语言进行沟通，但它处理我们的请求的方式与人类可能不同，这意味着我们可以采用特定的技术来引导LLM朝着我们希望的方向发展。
- en: 2.4 Examining the principles of prompt engineering
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 探究提示工程的原则
- en: As LLMs have developed, so have the patterns and techniques of prompt engineering.
    Many courses and blog posts have been written around prompt engineering, but one
    notable collection of principles, which we’ll explore shortly, has been created
    by Isa Fulford and Andrew Ng and their respective teams. A collaboration from
    OpenAI’s LLM knowledge and Deeplearning.ai’s teaching platform has created a course
    called ChatGPT Prompt Engineering for Developers, which features a series of principles
    and tactics that can be used in prompts to get the most out of LLMs. If you have
    the time, I encourage you to take the short course found at [https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers](chatgpt-prompt-engineering-for-developers.html).
    ([https://www.promptingguide.ai/](www.promptingguide.ai.html) is also a useful
    reference.) Though the course references ChatGPT specifically, the principles
    taught there can be applied across many LLMs. So let’s explore these principles
    and tactics to get comfortable with prompting LLMs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的发展，提示工程的模式和技术也在不断发展。许多课程和博客文章围绕提示工程编写，但是一个值得注意的原则集合是由Isa Fulford和Andrew
    Ng及其各自的团队创建的，我们稍后将进行探讨。OpenAI的LLM知识与Deeplearning.ai的教学平台的合作创建了一个名为ChatGPT Prompt
    Engineering for Developers的课程，其中包含一系列可以用于提示中以充分利用LLM的原则和策略。如果你有时间，我建议你参加[https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers](chatgpt-prompt-engineering-for-developers.html)的短期课程。（[https://www.promptingguide.ai/](www.promptingguide.ai.html)也是一个有用的参考。）尽管该课程特指ChatGPT，但其中教授的原则可以应用于许多LLM。所以让我们探究这些原则和策略，以便熟悉提示LLM。
- en: '2.4.1 Principle 1: Write clear and specific instructions'
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 原则1：编写清晰明确的指示
- en: 'This first principle might seem obvious at first glance—it’s always sensible
    to provide instructions to others that are clear and specific. But what this principle
    is actually suggesting is that we want to write prompts that are clear and specific
    *for an LLM.* And that will mean something different from what might be clear
    and specific to a human. To achieve this concept, Fulford and Ng teach four tactics
    to achieve clear and specific prompts: use delimiters, ask for structured output,
    check for assumptions, and few-shot prompting. In the next few sections, let’s
    examine each one in more detail.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这个第一个原则似乎很明显——为他人提供清晰明确的指示总是明智的。但这个原则实际上建议我们要编写*针对LLM清晰明确的提示*。这将意味着与对人类清晰明确的提示有所不同。为了实现这个概念，Fulford和Ng教授了四种策略来实现清晰明确的提示：使用分隔符，要求结构化输出，检查假设以及少次提示。在接下来的几节中，让我们更详细地研究每一种策略。
- en: '2.4.2 Tactic 1: Use delimiters'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 策略1：使用分隔符
- en: 'When writing prompts, we may need to provide different content and data that
    serve different purposes. For example, the start of our prompt might include instructions
    on what we want an LLM to produce, whereas the end of our prompt might include
    raw data that we want to process. LLMs are capable of guessing our intentions
    for different sections of our prompts, but because our goal is to be as clear
    as possible, we can aid the process by using *delimiters,* which are characters
    used to separate strings, to state our intentions for different parts of our prompt.
    Take this prompt as an example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写提示时，我们可能需要提供不同的内容和数据来满足不同的目的。例如，我们的提示开头可能包括关于我们希望LLM产生什么的说明，而提示的最后可能包括我们希望处理的原始数据。LLMs能够猜测我们对提示不同部分的意图，但由于我们的目标是尽可能清晰明了，我们可以通过使用*分隔符*来帮助这个过程，分隔符是用于分隔字符串的字符，以在提示的不同部分表明我们的意图。以这个提示为例：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When running this prompt within ChatGPT, I received the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此提示在ChatGPT中，我收到以下输出：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see through the use of delimiters, the correct table name `rooms`
    has been added and the column names, formats and ranges have been correctly randomized.
    This is made possible by the clear expectations and rules we set that are distinguished
    by various delimiters throughout the prompt.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，通过使用分隔符，正确的表名`rooms`已经被添加，并且列名、格式和范围已经被正确地随机化。这是通过在提示中通过各种分隔符来明确设定我们所设定的期望和规则而实现的。
- en: 'The delimiters help to make the prompt clearer, but they also make it easy
    to modify. For example, if we want to reconfigure the generated data, we could
    enter another line such as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔符有助于使提示更清晰，但也使其易于修改。例如，如果我们想重新配置生成的数据，我们可以输入另一行，如：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So, when writing prompts that contain a large amount of data that changes context,
    we can use delimiters to make clear what is being provided in a prompt at a specific
    point.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在撰写包含大量随上下文变化的数据的提示时，我们可以使用分隔符清楚地表示在特定点的提示中提供了什么。
- en: Activity
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Using the delimiter prompt example, create new instructions—this time, for a
    booking that would include information about who made the booking, contact details,
    and check-in and check-out dates.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分隔符提示示例，创建新的指令-这次是针对包括预订人、联系方式、入住和退房日期信息的预订。
- en: '2.4.3 Tactic 2: Ask for structured output'
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 策略 2：要求结构化输出
- en: 'One facet of LLMs that makes them useful is that they have the ability to provide
    outputs in structured formats. As we make our way through this book, we’ll explore
    just how useful this ability is, but as a rule, we must always remember that we
    need to be clear in a prompt about what structured format we want to see used.
    Take this prompt as an example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的一个有用的方面是它们具有以结构化格式提供输出的能力。在阅读本书时，我们将探索这种能力的有用性，但是作为一条规则，我们必须始终记住，我们需要在提示中清楚地说明我们想要看到使用的结构化格式。以这个提示为例：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We begin the prompt by clearly stating what format we want to see our object
    in, in this case, JSON, before we then start outlining the structure of the object.
    When I sent this prompt to ChatGPT, the following result was returned:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先明确说明我们想要以JSON格式查看我们的对象，然后开始概述对象的结构。当我将此提示发送给ChatGPT时，返回了以下结果：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As the prompt clearly states, the expected format and structure are two distinct
    instructions, meaning we can modify our instructions to quickly change the format
    structure by sending an additional prompt, such as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如提示中明确说明的，预期格式和结构是两个不同的指令，这意味着我们可以通过发送附加的提示来修改指令，从而快速更改格式结构，例如：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Sending this prompt to ChatGPT returned the following output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将此提示发送给ChatGPT返回以下输出：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice the structure is the same (as well as the randomized data). By explicitly
    stating what format we desire, we can instruct an LLM with exactly what format
    we want at a given time and then alternate formats with ease.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意结构是相同的（以及随机化的数据）。通过明确说明我们想要的格式，我们可以指示LLM在特定时间以及轻松地交替格式。
- en: Activity
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Attempt to create an object that contains multiple parameters with different
    data types in different formats. Try prompting an LLM to convert your object from
    one format to another—for example, from JSON to XML.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试创建一个包含不同数据类型和不同格式的多个参数的对象。尝试提示一个LLM将你的对象从一种格式转换为另一种格式，例如从JSON转换为XML。
- en: '2.4.4 Tactic 3: Check for assumptions'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 策略 3：检查假设
- en: 'As we learned earlier, LLMs can hallucinate, by generating output that contains
    information that is incorrect or disconnected from reality. It goes without saying
    that we want to reduce the risk of producing hallucinations, which is where our
    third tactic, checking for assumptions, can help. LLMs are more likely to hallucinate
    if they are provided with prompts that are focused on edge cases around a problem
    we want to solve. If an LLM is not properly instructed, it’s more likely to make
    a guess at an answer than to outright inform us that it cannot provide a useful
    answer. So, if we want to avoid guesswork, we need to provide instructions in
    our prompt to allow the LLM to bail out if it cannot execute our request. Consider
    this example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前学到的那样，LLM可能会产生幻觉，生成包含不正确或脱离现实的信息的输出。不用说，我们希望减少产生幻觉的风险，这就是我们的第三种策略，检查假设，可以帮助的地方。如果向LLM提供了围绕我们想要解决的问题的边缘情况的提示，那么LLM更有可能产生幻觉。如果LLM没有得到适当的指示，它更有可能对答案进行猜测，而不是直接告诉我们它无法提供有用的答案。因此，如果我们想避免猜测，我们需要在提示中提供指示，让LLM在无法执行我们的请求时退出。考虑这个例子：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running this prompt with a collection of email addresses that can be extracted
    returned the following output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可以提取的电子邮件地址集运行此提示返回以下输出：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'But here’s what happens when I ran the prompt again without email addresses:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我再次运行提示而不包含电子邮件地址时会发生什么：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then I received the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我收到了以下内容：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This response from the LLM was a direct reference to this section of the prompt:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的这个响应直接参考了提示的这一部分：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'which prevented the LLM from providing an output that is incorrect. For example,
    when I ran the prompt without the assumption check, the following was returned:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这阻止了LLM提供不正确输出。例如，当我没有进行假设检查运行提示时，返回以下内容：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This is an excellent example of an LLM hallucination. The object we provided
    contained no email addresses, so the LLM used guesswork and incorrectly started
    generating new email addresses based on existing data. However, with the assumption
    check in place, we prevented the hallucination from occurring in the first place.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是LLM幻觉的一个很好的例子。我们提供的对象不包含电子邮件地址，因此LLM使用猜测，并基于现有数据错误地开始生成新的电子邮件地址。然而，有了假设检查，我们防止了幻觉的发生。
- en: Activity
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Create a prompt that filters specific data out of a list of raw data and then
    outputs the results (For example, filter fruit from a list of different foodstuffs.)
    Then modify the data to include an edge case (for example, incorrect or missing
    data). Observe what the LLM outputs, and then attempt to correct the output by
    adding to the prompt some instructions that follow the check-for-assumptions prompt.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个过滤掉原始数据列表中特定数据并输出结果的提示（例如，在不同食物列表中过滤水果）。然后修改数据以包含一个边缘情况（例如，不正确或缺失的数据）。观察LLM输出的内容，然后尝试通过向提示添加跟随检查假设提示的指令来更正输出。
- en: '2.4.5 Tactic 4: Few-shot prompting'
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 策略 4：Few-shot prompting
- en: 'As we’ve progressed through this chapter and learned new concepts about LLMs
    and prompting, key points have been clarified through the use of examples. They
    are essential tools when it comes to not just teaching but also communicating
    with one another. This is no different for LLMs. *Few-shot prompting* basically
    means providing explicit examples to clarify instructions (The word *few* in this
    context indicates how many examples you share. A prompt with no examples would
    be a *zero-shot prompt*). Take this example of a prompt that uses examples:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在本章中的进展和对LLM和提示的新概念的学习，通过示例澄清了关键点。它们是不仅仅是教学，而且是相互交流的重要工具。对于LLM来说也是如此。 *Few-shot
    prompting* 基本上意味着提供明确的示例来澄清说明（在这种情况下，*few* 这个词指示你分享了多少示例。一个没有示例的提示将是 *zero-shot
    prompt*）。看一个使用示例的提示的例子：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Sending this prompt to ChatGPT returned the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将此提示发送给ChatGPT返回以下输出：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Consider the format of `Explore <Target> using <Resource> to discover <Information>.`
    It is rule-based, but it leans heavily on natural language to communicate the
    rules (unlike JSON or XML structures that rely on explicit delimiter rules). By
    providing examples in our prompt, we can help contextualize what we mean by `<Target>`,
    `<Resource>` and `<Information>` and reduce the risk of the LLM guessing what
    that means.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑格式`使用<Resource>探索<Target>以发现<Information>`。它是基于规则的，但它在使用自然语言来传达规则时有很大倾向（不像依赖显式分隔符规则的JSON或XML结构）。通过在我们的提示中提供示例，我们可以帮助界定我们所指的`<Target>`、`<Resource>`和`<Information>`，并减少LLM猜测它们含义的风险。
- en: Activity
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Create a prompt that uses the few-shot tactic. In your prompt, provide the instructions
    you expect the LLM to follow and then add at least two examples to help guide
    it in providing a desired outcome.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个使用few-shot策略的提示。在您的提示中，提供您期望LLM遵循的说明，然后至少添加两个示例以帮助指导它提供所需的结果。
- en: '2.4.6 Principle 2: Give the model time to “think”'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.6 原则2：给模型一些“思考”的时间
- en: It may seem a little unusual, given that we’ve been considering LLMs as probabilistic
    machines and not entities that can think, to see a principle that encourages us
    to give a model time to “think.” Surely the value of using LLMs is that they can
    respond with answers to complex questions much faster than we humans can?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们一直将LLMs视为概率机器而不是可以思考的实体，看到一个鼓励我们给模型一些“思考”时间的原则似乎有点不寻常。毕竟，使用LLMs的价值在于它们能够比我们人类更快地回答复杂问题吗？
- en: 'However, the focus of principle 2 isn’t on how an LLM thinks, but rather on
    how we present complex tasks to a probabilistic engine. In the ChatGPT Prompt
    Engineering for Developers course mentioned a little earlier, Fulford gives a
    useful analogy as she introduces this principle:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，原则2的重点不在于LLM如何思考，而在于我们如何向概率引擎呈现复杂任务。在稍早提到的针对开发者的ChatGPT提示工程课程中，富尔福德在介绍这个原则时给出了一个有用的类比：
- en: “If you give a model a task that’s too complex to do in a short amount of time
    . . . it may make up a guess which is likely to be incorrect.”
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果你给一个模型一个在短时间内无法完成的任务……它可能会做出一个可能是错误的猜测。”
- en: She explains that the same situation would happen if humans were given a complex
    task with limited time to respond. We’d rely on educated guesswork and likely
    come up with a less than satisfactory answer. So principle 2 offers tactics to
    help us write prompts that break down tasks and encourage LLMs to evaluate output
    to once again maximize the chances of a desired response.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 她解释说，如果人类被要求在有限的时间内完成复杂任务，同样的情况也会发生。我们将依赖于有根据的猜测，并且可能得出一个不太满意的答案。因此，原则2提供了策略来帮助我们编写提示，将任务拆分，并鼓励LLMs评估输出，以再次最大化产生所需响应的可能性。
- en: '2.4.7 Tactic 1: Specify the steps to complete the task'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.7 策略1：指定完成任务的步骤
- en: 'The first tactic is relatively straightforward once we are confident about
    using the tactics of principle 1 (Write clear and specific instructions.) By using
    delimiters, we can break a complex task into individual steps for an LLM to take
    to solve the larger task. Take a look at this prompt:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种策略相对直接，一旦我们对原则1（编写清晰明确的说明）的策略有信心，就可以轻松掌握。通过使用分隔符，我们可以将复杂的任务分解为单个步骤，供LLM执行以解决更大的任务。看看这个提示：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When sent to ChatGPT, it returned this result:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当发送给ChatGPT时，它返回了这个结果：
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is a snippet from the output I received. It created many more risks and
    charters, but the first example from each section demonstrates the LLM responding
    to each subtask, one by one, to create an output.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我收到的输出片段。它创建了许多风险和章程，但每个部分的第一个示例都演示了LLM逐个回应每个子任务以创建输出。
- en: Activity
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Create a prompt that requires a complex task to be carried out. Attempt to break
    out the complex task into multiple subtasks that the LLM can carry out.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个需要执行复杂任务的提示。尝试将复杂任务拆分为LLM可以执行的多个子任务。
- en: '2.4.8 Tactic 2: Instruct the model to work out its own solution first'
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.8 策略2：指导模型先解决自己的方案
- en: 'Our final tactic focuses not on the process of creating an output, but rather
    on evaluating the output itself. Similar to checking for assumptions, ask an LLM
    to evaluate the output to confirm that it aligns with what it has been instructed
    to produce. Let’s look at an example to gain a better understanding of how this
    would work. First, let’s look at a prompt that doesn’t ask an LLM to work out
    its solution first:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的策略不是关注创建输出的过程，而是关注评估输出本身。类似于检查假设，要求LLM评估输出以确认其与其被指示产生的内容是否一致。让我们看一个示例，以更好地理解这将如何运作。首先，让我们看一个不要求LLM首先解决其解决方案的提示：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Sending this to ChatGPT, the following result was returned:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将此发送到ChatGPT，返回以下结果：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This looks like reasonable Java code for a unit test, but if this were added
    to a suite of unit checks, it would fail. It would fail because the method `authDB.deleteToken`
    in the production code provided has not been handled correctly. Specifically,
    if we wanted this unit check to work, we would need to mock `authDB.deleteToken`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像一个合理的Java代码用于单元测试，但如果这被添加到一组单元检查中，它会失败。这会失败，因为在提供的生产代码中，方法`authDB.deleteToken`没有被正确处理。具体来说，如果我们希望此单元检查工作，我们需要模拟`authDB.deleteToken`。
- en: 'Now, if we run the prompt again but this time have it evaluate its solution
    before outputting a final answer, we get a different result. So first we change
    the prompt to the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们再次运行提示，但这次在输出最终答案之前让它评估其解决方案，我们会得到不同的结果。所以首先我们将提示更改为以下内容：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Sending this to ChatGPT returned this result:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 将此发送到ChatGPT返回以下结果：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This time, we can see that because we asked the LLM to evaluate its solution
    before returning a result, the outputted unit check uses `Mockito` to mock the
    `authDB.deleteToken`. So, if we observe issues with LLMs outputting erroneous
    solutions or they start hallucinating, we can add an instruction to evaluate solutions
    first to minimize the occurrence of hallucinations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们可以看到因为我们要求LLM在返回结果之前评估其解决方案，输出的单元检查使用了`Mockito`来模拟`authDB.deleteToken`。因此，如果我们观察到LLMs输出错误的解决方案或它们开始产生幻觉，我们可以添加一条指令首先评估解决方案，以最小化幻觉的发生。
- en: Activity
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Create a prompt that requires an LLM to work out a solution to a problem. Observe
    its output and see if the solution it produces is correct. Then add instructions
    to have the LLM evaluate the solution. What happens? Does the solution change?
    Is it an improvement?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个提示，需要LLM解决问题的解决方案。观察其输出，并查看所产生的解决方案是否正确。然后添加指令，让LLM评估解决方案。会发生什么？解决方案会改变吗？是不是有所改善？
- en: 2.5 Working with different LLMs
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 使用不同的LLM
- en: So far, we’ve spoken about LLMs in a broad sense while using OpenAI’s ChatGPT
    in the previous examples to demonstrate how they work in general. However, ChatGPT
    is just one of the many different LLMs that are available to us to use. So, before
    we conclude the chapter, let’s familiarize ourselves with the ways in which LLMs
    are different from one another and learn about some of the currently popular models
    and communities so that we can increase our chances of finding the right LLM for
    the job.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在广义上讨论了LLMs，同时在之前的示例中使用OpenAI的ChatGPT来演示它们的工作原理。然而，ChatGPT只是我们可以使用的众多不同LLMs中的一个。因此，在我们结束本章之前，让我们熟悉LLMs之间的差异，并了解一些当前流行的模型和社区，以增加我们找到适合工作的正确LLM的机会。
- en: 2.5.1 Comparing large language models
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 比较大型语言模型
- en: 'What makes an LLM good? How do we determine whether a model is worth using?
    These are not easy questions to answer. The complex nature of LLMs and how they’re
    trained and what data was used close these systems off to deep analysis, and comprise
    an area that some researchers are trying to improve or shed light upon. However,
    that doesn’t mean we shouldn’t educate ourselves on some of the key aspects of
    LLMs and how they impact them. We might not all be AI researchers attempting to
    explore the deep inner workings of LLMs, but we are, or will be, users of them
    and we will want to know that what we spending resources on is giving us value.
    So to help us break down some of the jargon and give us some grounding on how
    LLMs differ from one another, let’s go through some key attributes that are discussed
    in the world of LLMs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使得一个LLM优秀？我们如何确定一个模型是否值得使用？这些都不是容易回答的问题。LLM的复杂性质以及它们的训练方式和使用的数据使得这些系统难以进行深入分析，并构成了一些研究人员试图改进或阐明的领域。然而，这并不意味着我们不应该了解LLM的一些关键方面以及它们如何影响它们。我们可能并不都是尝试探索LLM深层内部运作的人工智能研究人员，但我们是或将成为它们的用户，我们会想知道我们投入资源的东西是否给了我们价值。因此，为了帮助我们分解一些术语并为我们提供一些关于LLM如何相互区别的基础知识，让我们来看看LLM领域讨论的一些关键属性：
- en: Parameter count
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参数数量
- en: Take a look at different LLMs and you’ll likely see talk of LLMs having a “175
    Billion” or “1 Trillion” parameter count. It sometimes can feel like marketing
    speak, but parameter count does have an impact on how an LLM performs. The parameter
    count essentially relates to the amount of statistical weights that exist within
    a model. Each individual weight provides a piece of the statistical puzzle that
    makes up an LLM. So, roughly speaking, the more parameters an LLM has, the better
    it will perform. The parameter count also can give us a sense of cost. The higher
    the parameter count, the more expensive it is to run, a cost that may be, in part,
    handed down to users.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 看看不同的LLM，你可能会看到关于LLM拥有“1750亿”或“1万亿”参数数量的说法。有时候这可能听起来像是营销说辞，但参数数量确实会影响LLM的表现。参数数量本质上与模型内部存在的统计权重量有关。每个单独的权重都提供了组成LLM的统计谜题的一部分。因此，粗略地说，LLM拥有的参数越多，其性能就越好。参数数量也可以让我们了解成本。参数数量越高，运行成本就越高，这种成本可能在一定程度上转嫁给用户。
- en: Training Data
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据
- en: LLMs require huge quantities of data to be trained on, so the size of data and
    quality of data it has been trained on will have an impact on the quality of an
    LLM. If we want an LLM to be accurate in how it responds to requests, it’s not
    enough to just throw as much data as possible. It needs to be data that can help
    influence the probability of a model in a sensible manner. For example, the Reddit
    example we explored earlier in the chapter, in which the subreddit r/counting
    being used to train ChatGPT caused it to hallucinate in strange ways, demonstrates
    that more isn’t necessarily better. Still, similar to parameter count, the more
    high-quality data an LLM has been trained on, the better it will likely perform.
    The challenge lies in knowing what data an LLM has been trained on—something that
    corporate creators of AI are keen to keep a secret.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: LLM需要大量的数据进行训练，因此数据的规模和质量对LLM的质量会产生影响。如果我们希望LLM在回应请求时准确无误，仅仅提供尽可能多的数据是不够的。需要的是能够以合理方式影响模型概率的数据。例如，我们在本章前面探讨过的Reddit示例，其中使用subreddit
    r/counting来训练ChatGPT导致它产生奇怪的幻觉，表明更多不一定就是更好。然而，与参数数量类似，LLM训练过的高质量数据越多，其性能可能就越好。挑战在于了解LLM训练过的数据是什么——这是AI企业创造者们极力保守秘密的一件事情。
- en: Extensibility and Integration
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可扩展性和整合性
- en: Just as with any other tool, the value of an LLM can be increased further if
    it can offer other features beyond its core abilities, such as integrating into
    existing systems or training models further for our specific needs. What features
    are available to integrate and extend LLMs largely depends on who was responsible
    for training them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他工具一样，如果LLM能够提供除核心功能之外的其他功能，如整合到现有系统中或进一步为我们的特定需求训练模型，那么它的价值可以进一步提高。LLM可以整合和扩展的功能主要取决于谁负责对它们进行训练。
- en: For example, OpenAI offers paid-for API access to their models. But beyond an
    instruction feature that allows you to tweak output with a simple prompt, there
    is no ability to further fine-tune and deploy one of their GPT models for private
    use. Compare this to Meta’s LlaMa model, which has been open-sourced, allowing
    the AI community to download and further train to their own requirements, though
    they have to build their own infrastructure to deploy the model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI 提供付费 API 访问其模型。但除了一项指令功能可以通过简单提示来调整输出外，没有能力进一步调整并部署其中一个他们的GPT模型供私人使用。相比之下，Meta
    的 LlaMa 模型是开源的，允许AI社区下载并根据自己的要求进一步训练，尽管他们必须构建自己的基础设施来部署该模型。
- en: As LLM platforms grow, we will see advances in not just their ability to respond
    to prompts but also the features around them and their access. So it’s necessary
    to keep said features in mind when evaluating what to work with.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM平台的发展，我们将看到它们不仅在回应提示方面取得进步，而且在周围的功能和访问方面也会有所提升。因此，在评估要使用的内容时，记住这些功能是必要的。
- en: Quality of responses
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回应质量
- en: Arguably, the most important factor to consider is whether an LLM is providing
    responses that are legible, useful and free (or as close to free) of hallucination
    as possible. Although criteria such as parameter count and training data are useful
    indicators of an LLM's performance, it’s up to us to understand what we want to
    use an LLM for and then determine how each responds toward our prompts and help
    in solving our specific problems. Not all challenges we face need the largest,
    most expensive LLM in the market. So it’s important that we take the time to try
    out different models, compare their outputs and then make a judgment for ourselves.
    For example, GPT models from OpenAI are found to perform better with code examples
    than, say, Google Bard. These details have been discovered through experimentation
    and observation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为，考虑LLM是否提供易读、有用和尽可能免费（或接近免费）幻觉的回复是最重要的因素之一。尽管参数数量和训练数据等标准是LLM性能的有用指标，但我们需要明确我们想要使用LLM做什么，然后确定每个LLM对我们的提示作出怎样的响应，并在解决我们具体问题时提供怎样的帮助。我们面临的挑战并不都需要市场上最大、最昂贵的LLM。因此，我们要花时间尝试不同的模型，比较它们的输出，然后自行判断。例如，OpenAI的GPT模型在处理代码示例方面表现更好，而Google
    Bard则不然。这些细节是通过实验和观察发现的。
- en: These criteria we’ve explored are by no means an exhaustive list, but once again
    they demonstrate that there is more to consider about LLMs once we get past the
    initial glamour of how they respond. Different LLMs perform in different ways,
    helping us with different challenges. So let’s take a look at some of the more
    popular models and platforms that are currently available.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨的这些标准绝不是一个穷尽列表，但再次表明，一旦我们克服了它们如何回应的初始魅力，就会发现关于LLMs还有更多要考虑的东西。不同的LLM以不同的方式执行，帮助我们解决不同的挑战。因此，让我们来看看当前可用的一些更受欢迎的模型和平台。
- en: 2.5.2 Examining popular Large Language Models
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 检查流行的大型语言模型
- en: Since OpenAI’s launch of ChatGPT, there has been an explosion in releases of
    LLMs from various organisations. It’s not to say that these models and related
    work weren’t around before ChatGPT’s release, but the public focus has certainly
    intensified and more and more marketing and release announcements have focused
    on companies releasing their LLM offerings. Here are some of the more common/popular
    LLMs that have been released since the end of 2022.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 OpenAI 推出 ChatGPT 以来，各种组织发布的大型语言模型（LLMs）数量激增。这并不是说在 ChatGPT 推出之前没有这些模型和相关工作，但公众关注度确实加强了，越来越多的营销和发布公告聚焦在公司发布的LLM产品上。以下是自
    2022 年底以来发布的一些更常见/流行的 LLMs。
- en: Keeping up with LLMs
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 跟上LLMs
- en: It’s worth noting that the situation with the launch of LLMs and their related
    features is extremely fluid and has grown at quite a fast pace. Therefore, it’s
    likely that some of what we’ll explore will differ from the time of writing in
    mid-2023 to the time you are reading this book. However, what this list demonstrates
    is some of the bigger names in the LLM space that are worth exploring.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，LLMs的推出及其相关功能的情况非常复杂，发展速度相当快。因此，我们所探讨的一些内容可能与撰写时（2023 年中期）到你阅读本书时有所不同。然而，这个列表展示了LLM领域中一些值得探索的大型名字。
- en: OpenAI
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenAI
- en: At the time of writing, OpenAI is the most ubiquitous of organizations offering
    LLMs for use. Although OpenAI have been working on LLM models for quite some time,
    releasing their GPT-3 model in 2020, it was their release of ChatGPT in November
    2022 that kickstarted the popular wave of interest and use of LLMs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，OpenAI是提供LLM供使用的组织中最普遍的。虽然OpenAI已经在LLM模型上工作了相当长的时间，他们在2020年发布了他们的GPT-3模型，但是他们在2022年11月发布的ChatGPT才引发了广泛的兴趣和LLM的使用浪潮。
- en: 'OpenAI offers a range of different LLM models, but the two that stand out are
    GPT-3.5-Turbo and GPT-4, which you can learn about more in their docs: [https://platform.openai.com/docs/models/overview](models.html).
    These two models are used as *foundation* models, or models that can be trained
    further for specific purposes, for a range of products such as ChatGPT, GitHub
    Copilot and Microsoft Bing AI.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了一系列不同的LLM模型，但最突出的两个是GPT-3.5-Turbo和GPT-4，您可以在他们的文档中了解更多信息：[https://platform.openai.com/docs/models/overview](models.html)。这两个模型被用作*基础*模型，或者说可以进一步针对特定目的进行训练的模型，用于一系列产品，如ChatGPT、GitHub
    Copilot和Microsoft Bing AI。
- en: In addition to their models, OpenAI has offered a range of features such as
    API access to their direct GPT-3.5-Turbo and GPT-4 models, and a collection of
    apps that integrate with ChatGPT (if you subscribe to their plus membership).
    It’s by far the most popular LLM (for now) and has kicked started a race with
    organizations to release their own LLMs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了他们的模型外，OpenAI还提供了一系列功能，如直接访问他们的GPT-3.5-Turbo和GPT-4模型的API，以及一系列与ChatGPT集成的应用程序（如果您订阅了他们的plus会员）。这是迄今为止最受欢迎的LLM（目前为止）并已启动了一个与组织竞争发布他们自己的LLM的竞赛。
- en: Although we’ve already explored some prompts with ChatGPT, you can always access
    and experiment with ChatGPT at [https://chat.openai.com/](chat.openai.com.html).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经尝试过一些与ChatGPT相关的提示，但您随时可以访问并尝试ChatGPT，网址是[https://chat.openai.com/](chat.openai.com.html)。
- en: Sticking with OpenAI
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 坚持使用OpenAI
- en: Though there are many different large language models that I encourage you to
    use, for the sake of consistency, we will stick with ChatGPT-3.5-Turbo. It’s not
    necessarily the most powerful LLM at this time, but it is the most ubiquitous—and
    free. That said, if you want to try out these prompts with other LLM models, feel
    free. But be aware that their responses will likely differ from what is shared
    in this book.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多不同的大型语言模型，我鼓励您使用，但为了保持一致，我们将坚持使用ChatGPT-3.5-Turbo。这不一定是目前最强大的LLM，但它是最普遍的，而且是免费的。也就是说，如果您想尝试其他LLM模型的这些提示，可以随意尝试。但请注意，它们的响应可能与本书中分享的内容不同。
- en: PaLM
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PaLM
- en: As soon as OpenAI released ChatGPT it was only a matter of time before Google
    released their own LLM with chat services, and they did so in March 2023\. Based
    on their PaLM 2 LLM, a 540 billion parameter model, Google sought to compete with
    ChatGPT and offered a similar chat-based experience with Bard.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦OpenAI发布了ChatGPT，Google很快就会发布他们自己的LLM与聊天服务，他们于2023年3月发布了。基于他们的PaLM 2 LLM，一个拥有5400亿参数的模型，Google试图与ChatGPT竞争，并提供了类似的基于聊天的体验，称为Bard。
- en: Similar to OpenAI, Google offers access to PaLM 2 via their Google Cloud platform
    (which can be found at [https://developers.generativeai.google/](developers.generativeai.google.html))
    and has recently started offering apps that work similarly to OpenAI’s ChatGPT
    apps, with the added integration into other Google Suite tools such as Google
    Drive and Gmail.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与OpenAI类似，Google通过他们的Google Cloud平台提供了对PaLM 2 的访问（网址为[https://developers.generativeai.google/](developers.generativeai.google.html)），并最近开始提供与OpenAI的ChatGPT应用类似的应用程序，同时还可以集成到其他Google
    Suite工具中，如Google Drive和Gmail。
- en: You can access and experiment with Bard at [https://bard.google.com/chat](bard.google.com.html).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问并尝试Bard，网址是[https://bard.google.com/chat](bard.google.com.html)。
- en: LLaMa
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLaMa
- en: 'LLaMa, which is the name for a collection of models, was first released by
    Meta in July 2023\. What sets LLaMa apart from OpenAI’s GPT models and Google’s
    PaLM is that LLaMa is open source. In addition to the open-source license, LLaMa
    comes in a range of sizes: 7, 13 and 70 billion parameters, respectively. The
    combination of these sizes and their access means that LLaMa has been adopted
    by the AI community as a popular foundational model. The flip side of this access,
    though, is that Meta doesn’t provide a public platform to train and run versions
    of LLaMa. So data sets, and infrastructure, have to be personally sourced to use
    it.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMa 是 Meta 在 2023 年 7 月首次发布的一个模型集合的名称。LLaMa 与 OpenAI 的 GPT 模型和谷歌的 PaLM 有所不同之处在于，LLaMa
    是开源的。除了开源许可证外，LLaMa 还有多种规模：分别是 70 亿、13 亿和 7 亿个参数。这些规模和它们的可访问性的结合使得 LLaMa 已成为 AI
    社区中一个受欢迎的基础模型。然而，这种可访问性的另一面是，Meta 并不提供公共平台来训练和运行 LLaMa 的版本。因此，必须个人获取数据集和基础设施才能使用它。
- en: 'More details on LLaMa can be found at the following links:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 LLaMa 的详细信息可以在以下链接中找到：
- en: '[https://www.llama2.ai/](www.llama2.ai.html)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.llama2.ai/](www.llama2.ai.html)'
- en: '[https://llama-2.ai/download/](download.html)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://llama-2.ai/download/](download.html)'
- en: Huggingface
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Huggingface
- en: Unlike the other entries in our list, Huggingface offers no proprietary model
    but instead facilitates an AI community that contains a wide variety of different
    models, most of which are open source. Looking at their index page of models,
    found at [https://huggingface.co/models](huggingface.co.html), we can see hundreds
    of thousands of differently trained models that have come from different companies
    and research labs. Huggingface also offers datasets for training, apps, and documentation
    that allow the reader to dive deeper into how models are built. All of these resources
    are available so that the AI community can access pre-trained models, tweak them
    and further train them for specific use, something that we’ll be exploring further
    in Part 3 of this book.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与列表中的其他条目不同，Huggingface 并没有提供专有模型，而是促进了一个包含各种不同模型的 AI 社区，其中大多数模型都是开源的。查看他们的模型索引页面，位于
    [https://huggingface.co/models](huggingface.co.html)，我们可以看到来自不同公司和研究实验室的数十万个经过不同训练的模型。Huggingface
    还提供了用于训练、应用程序和文档的数据集，使读者能够深入了解模型的构建方式。所有这些资源都可用于 AI 社区访问预训练模型，对其进行微调，并针对特定用途进行进一步训练，这是我们将在本书第
    3 部分进一步探讨的内容。
- en: The marketplace for LLMs has sizeably grown in a short amount of time, both
    commercially and in open-source, and similar to other areas of software development,
    being proactive in what new LLMs are appearing can be beneficial. However, it
    can also be overwhelming and not necessarily feasible to keep up with everything
    that is happening at once. So instead of attempting to keep abreast of all the
    comings and goings in the AI community, we can opt to explore LLMs when we want
    to use LLMs to solve specific problems. Having a problem can help frame our criteria
    around which tools work best for us and which don’t.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在短时间内，LLMs 的市场规模在商业和开源方面都有了显著增长，与软件开发的其他领域类似，积极主动地了解新的 LLMs 出现可能是有益的。然而，尝试一直跟上一切可能是令人不知所措的，也不一定可行。因此，与其试图紧跟
    AI 社区的一切动态，我们可以选择在想要使用 LLMs 解决特定问题时探索它们。拥有问题可以帮助我们构建关于哪些工具最适合我们以及哪些不适合的标准。
- en: Activity
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Either select an earlier prompt from this chapter or create one of your own
    and submit it to different LLMs. Note how each responds and compares. Do some
    feel more conversational? How do they handle either receiving or sending code
    examples? Which ones provide the best response in your opinion?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章选择一个早期提示，或者创建一个你自己的提示，并将其提交给不同的 LLMs。注意每个 LLM 的响应和比较。有些是否感觉更像是对话？它们如何处理接收或发送代码示例？在你的观点中，哪个提供了最佳响应？
- en: 2.6 Creating a library of prompts
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 创建提示库
- en: 'One of the benefits of prompts is that once they are created, they can be used
    repeatedly. As a consequence, a lot of collections of prompts for different roles
    and tasks are appearing online. For example, here are a few collections that I’ve
    seen shared recently:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 提示的一个好处是一旦创建，它们就可以被重复使用。因此，出现了很多在线共享的不同角色和任务的提示集合。例如，这里是最近我看到的一些集合：
- en: 'Awesome ChatGPT Prompts, GitHub: [https://github.com/f/awesome-chatgpt-prompts](f.html)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '令人惊叹的 ChatGPT 提示，GitHub: [https://github.com/f/awesome-chatgpt-prompts](f.html)'
- en: '50 ChatGPT Prompts for Developers, Dev.to: [https://dev.to/mursalfk/50-chatgpt-prompts-for-developers-4bp6](mursalfk.html)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '50 ChatGPT Developers提示，Dev.to: [https://dev.to/mursalfk/50-chatgpt-prompts-for-developers-4bp6](mursalfk.html)'
- en: 'ChatGPT Cheat Sheet, Hackr.io: [https://hackr.io/blog/chatgpt-cheat-sheet-for-developer](blog.html)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ChatGPT Cheat Sheet, Hackr.io: [https://hackr.io/blog/chatgpt-cheat-sheet-for-developer](blog.html)'
- en: This list is not at all exhaustive and the sample collections aren’t necessarily
    related to testing, but they are worth looking through to learn how others have
    created prompts, as well as giving us the opportunity to determine which prompts
    would actually be effective and which wouldn’t.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表并不全面，示例集合也不一定与测试相关，但是浏览这些示例集合是值得的，以了解其他人是如何创建提示的，同时也给我们一个机会来确定哪些提示实际上会有效，哪些不会。
- en: Though prompt collections shared publicly can be useful, it’s likely we’ll end
    up creating prompts that are used for specific contexts. So it’s worthwhile getting
    into the habit of storing prompts that prove to be useful in some sort of repository
    for us, and others, to quickly use. Where you store these will depend on what
    and who they are used for. If they’re for public use, then sharing a repository
    of prompts or adding to existing collections might be valuable. If we’re creating
    and using them while developing company products, then we need to treat them in
    the same way as our production code and store them somewhere private so that we
    don’t violate any policies around intellectual property. Finally, we may also
    consider version control so that we can tweak and track prompts as we learn more
    about working with LLMs and as the LLMs themselves evolve.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管公开分享的提示集可能很有用，但我们很可能最终会创建用于特定情境的提示。所以，养成把证明有用的提示存储在某种仓库中的习惯是值得的，这样我们和其他人就可以快速使用它们。你存储它们的位置将取决于它们用于什么以及谁来使用它们。如果它们是用于公共使用，那么分享提示仓库或添加到现有集合可能会很有价值。如果我们在开发公司产品时创建和使用它们，那么我们需要像对待生产代码一样对待它们，并将它们存储在某个私密位置，以免违反有关知识产权的任何政策。最后，我们还可以考虑版本控制，这样我们就可以在学习更多关于与LLMs合作以及LLMs自身发展的过程中调整和跟踪提示。
- en: Wherever they are stored, the idea is to create a repository of prompts that
    are quick and easy to access so that once a prompt has been created for a specific
    activity, it can be reused multiple times rapidly so that we can get as much value
    from them to improve our productivity.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 无论它们存储在何处，想法都是创建一个可以快速轻松访问的提示仓库，以便一旦为特定活动创建了提示，它就可以被多次快速重复使用，以便我们可以从中获得尽可能多的价值来提高我们的生产力。
- en: Activity
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Create a space where you can store future prompts for you and your team to use.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空间，您和您的团队可以存储未来的提示以供使用。
- en: Using prompts from this book
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用本书中的提示
- en: 'In the spirit of storing prompts for future use and to help you, the reader,
    with trying out the prompt examples within this book, you can find each prompt
    example in the following GitHub repository:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储未来使用的提示并帮助您，读者，在尝试本书中的提示示例时，您可以在以下GitHub仓库中找到每个提示示例：
- en: '[https://github.com/mwinteringham/llm-prompts-for-testing](mwinteringham.html)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/mwinteringham/llm-prompts-for-testing](mwinteringham.html)'
- en: This will enable you to quickly copy and paste the prompts into your chosen
    LLM as we go through each chapter. Saving you the task of having to type the whole
    prompt in manually. There will be sections in certain prompts where you will need
    to add your own custom content or context to use them. To make them clear, instructions
    on what is required to add to the prompt will be found in the prompt and will
    be formatted in all caps and inside square brackets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使您能够在我们完成每章时快速复制并粘贴提示到您选择的LLM中。这样，您就不必手动键入整个提示。在某些提示的部分中，您将需要添加您自己的自定义内容或上下文来使用它们。为了使它们清晰，对于需要添加到提示中的内容的说明将在提示中找到，并将以全大写字母和方括号内的形式进行格式化。
- en: 2.7 Solving problems by using prompts
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 通过提示解决问题
- en: The tactics and tooling we’ve learned about in this chapter help provide us
    with a framework to use LLMs and design specific prompts for specific testing
    activities. We should be mindful, though, that although these tactics improve
    our chances of getting desired results, they are not foolproof. For example, when
    we ask an LLM to evaluate its output, the LLM isn’t actually evaluating its output
    like a traditional application might. It’s simply moving the predictive needle
    further toward an output that aligns with our requirements.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本章学到的策略和工具能够帮助我们框架使用LLM并为特定的测试活动设计特定的提示。然而，我们要注意，尽管这些策略能够提高我们获得所需结果的几率，但它们并不是百分之百可靠的。例如，当我们要求LLM评估其输出时，LLM实际上并不像传统应用程序那样评估其输出。它只是将预测的指针移向与我们需求相符的输出。
- en: Therefore it is necessary for us to develop the skills to write prompts that
    help us solve our problems effectively and in a way that doesn’t end up diminishing
    the time saved using LLMs (For example, we don’t want to spend hours tweaking
    prompts.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有必要培养写提示的技能，以便在解决问题时既能有效，又不会消耗使用LLM带来的时间（例如，我们不想花几小时调整提示）。
- en: Single prompting versus multi-prompting
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单个提示与多个提示的对比
- en: Throughout this chapter we’ve explored how to use principles and tactics to
    create individual prompts that are effective as possible at maximizing desired
    output from an LLM. However, tools like ChatGPT, Bard and Claude allow us to conduct
    ‘conversations’ with LLMs in which the history of the conversations influence
    the output of future responses in said ‘conversation’. This raises the question,
    would it be easier to try multiple prompts in a conversation to tweak output?
    Though this can be effective, we do run the risk that the longer a conversation
    progresses, the higher the risk of hallucinations occurring as an LLM attempts
    to overfit responses to our requests. This is why tools like BingAI are limited
    with the amount of responses they can give in a given conversation. However, more
    importantly, more doesn’t necessarily mean better. The garbage in, garbage out
    rule applies with both single and multiple prompts. Relying on multiple prompts
    in one conversation means we become less clear and precise in what we are asking
    for, which adds delays and increases hallucination, thus negating the value of
    using an LLM in the first place. In conclusion, whether we want to send a single
    prompt to get what we want or send multiple prompts, adopting the principles and
    tactics created by Isa Fulford and Andrew Ng will increase our productivity with
    LLMs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了如何使用原则和策略创建个别提示，以尽可能有效地从LLM中获得期望的输出。然而，像ChatGPT、Bard和Claude这样的工具允许我们与LLM进行“对话”，其中对话的历史会影响未来回复的输出。这就引出了一个问题，是否在对话中尝试多个提示以调整输出会更容易？虽然这可能有效，但我们也面临着一个风险，即随着对话的进行，幻觉发生的风险越高，因为LLM试图过度适应我们的请求。这就是为什么像BingAI这样的工具在给定对话中给出回复的数量是有限的原因。然而，更重要的是，多并不意味着更好。垃圾进，垃圾出的规则适用于单个和多个提示。在一次对话中依赖多个提示意味着我们在请求方面变得更加模糊和不精确，这会增加延迟并增加幻觉的发生，从而抵消了使用LLM的价值。总之，无论我们是想发送一个单独的提示来获取我们想要的结果，还是发送多个提示，采用Isa
    Fulford和Andrew Ng所创建的原则和策略将提高我们使用LLM的生产力。
- en: 'This means being able to identify specific issues that LLMs can help with and
    then utilizing prompt engineering to maximize the chances of extracting valuable
    information from an LLM. This is what we’ll explore throughout the rest of this
    book: when and how to use LLMs.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要能够确定LLM能够帮助解决的特定问题，并通过提示工程的方式最大化从LLM中提取有价值信息的机会。这将是我们在本书剩余部分要探索的内容：何时以及如何使用LLM。
- en: As we progress, we’ll also learn that prompts come in many shapes and sizes.
    Throughout this chapter, we’ve looked at prompts that are manually written by
    us humans. But, as we’ll learn, tools like GitHub Copilot auto-generate prompts
    as we write our code. That doesn’t mean we can’t still infuse the principles and
    tactics into our ways of working, but it does take time, awareness, and practice
    to develop.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的进展，我们还将学到提示有各种各样的形式和大小。在本章中，我们看了人工编写的提示。但是，我们会了解到，像GitHub Copilot这样的工具会在我们编写代码时自动生成提示。这并不意味着我们不能将原则和策略融入到我们的工作方式中，但这需要时间、意识和实践来培养。
- en: Activity
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动
- en: Before you continue reading this book and learn about different types of prompts
    for different testing activities, use the knowledge of chapters 1 and 2 and consider
    a specific testing task that you do and attempt to build a prompt that can help
    you with your work.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续阅读本书并了解不同类型的提示用于不同的测试活动之前，请使用第1章和第2章的知识，并考虑你所做的特定测试任务，并尝试构建一个提示，可以帮助你的工作。
- en: 2.8 Summary
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 总结
- en: LLMs are trained on vast amounts of data using sophisticated algorithms to analyze
    our requests and predict an output.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs使用复杂的算法对大量数据进行训练，以分析我们的请求并预测输出。
- en: The predictive nature of LLMs makes them quite adaptable but also means they
    come with some risks.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs的预测性质使它们非常适应，但也意味着它们存在一些风险。
- en: LLMs can sometimes output *hallucinations,* or text that sounds authoritative
    and correct when in fact it is completely false.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs有时会输出*幻觉*，或者听起来权威和正确的文本，实际上完全是错误的。
- en: The data that LLMs are trained on may contain errors, gaps, and assumptions,
    and we must be mindful of that when using them.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs所训练的数据可能存在错误、空白和假设，我们在使用它们时必须注意这一点。
- en: We must also be mindful of the data we share with LLMs so as not to cause unauthorized
    leaks of business or user information.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也要注意与LLMs分享数据，以免造成未经授权的商业或用户信息泄漏。
- en: Prompt engineering is a collection of principles and tactics we can use to maximize
    the chances of an LLM returning a desired output.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程是我们可以使用的一系列原则和策略，以最大化LLM返回期望输出的机会。
- en: We can leverage the knowledge that LLMs are predictive in nature and use it
    to our advantage through the use of prompt engineering.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以利用LLMs具有预测性质的知识，并通过提示工程来利用它。
- en: Using delimiters can help us clarify instructions and parameters in a prompt.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用界定符可以帮助我们在提示中澄清指示和参数。
- en: An LLM can output data in various formats, but it requires us to explicitly
    state which structure format we want in a prompt.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可以在各种格式中输出数据，但需要我们明确指出我们在提示中需要哪种结构格式。
- en: We can reduce hallucinations from LLMs by using the check-for-assumption tactic.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过使用检查假设的策略来减少LLMs的幻觉。
- en: Providing examples in a prompt can help ensure that an LLM provides an output
    in a desired format or context.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中提供例子可以帮助确保LLM以期望的格式或上下文提供输出。
- en: Specifying specific subtasks in a prompt can help an LLM process complex tasks
    successfully.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中指定特定的子任务可以帮助LLM成功处理复杂的任务。
- en: Asking LLMs to evaluate solutions to problems can also reduce errors and maximize
    outcomes.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求LLMs评估问题的解决方案也可以减少错误并最大化成果。
- en: Knowing when to use LLMs and developing skills with prompt engineering is the
    key to success, regardless of what tooling we use.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道何时使用LLMs并发展提示工程技能是成功的关键，无论我们使用什么工具。
