- en: 9 Building and Running Your Own Large Language Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 建立和运行您自己的大型语言模型
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Why you might want to build your own large language model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么您可能想要建立自己的大型语言模型
- en: Selecting an LLM model to serve as your base for a custom configuration
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个LLM模型作为您自定义配置的基础
- en: How (in very general terms) model fine tuning works
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何（在非常一般的术语中）进行模型微调
- en: Build (or modify) your own LLM? But didn’t OpenAI (and their investors) spend
    billions of dollars optimizing and training their GPT? Is it possible to generate
    even remotely competitive results through a do-it-yourself project using local
    hardware?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 建立（或修改）您自己的LLM？但OpenAI（及其投资者）花费数十亿美元优化和训练他们的GPT。通过使用本地硬件进行自己的项目，是否有可能生成甚至远程竞争力的结果？
- en: Incredibly, at this point in the whirlwind evolution of LLM technologies, the
    answer to that question is "yes." Due to the existence of Meta’s open source LLaMA
    model, an unauthorized leak of the model’s weights (Which I’ll explain in just
    a moment), and a lot of remarkable public contributions, there are now hundreds
    of high-powered but resource-friendly LLMs available for anyone to download, optionally
    modify, and run.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 令人难以置信的是，在LLM技术的旋风演变到这一步时，对这个问题的答案是"是的"。由于Meta的开源LLaMA模型的存在，模型权重的未经授权泄漏（我将在接下来解释），以及许多卓越的公共贡献，现在有数百种高效而又资源友好的LLM可供任何人下载、选择性修改和运行。
- en: Having said that, if operating at this depth of technology tinkering isn’t your
    thing - and especially if you don’t have access to the right kind of hardware
    - feel free to skip to the next chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果操作到这种技术深度不是你的菜 - 尤其是如果你没有合适的硬件访问权限 - 那么可以跳过到下一章。
- en: 9.1 Some background to building your own model
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 建立自己的模型的一些背景知识
- en: 'Before we explain how all that works, we should address the bigger question:
    why would anyone *want* to build their own LLM? Here are some things worth considering:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解释所有这些是如何工作之前，我们应该解决一个更大的问题：为什么有人*想要*建立自己的LLM？以下是值得考虑的一些事项：
- en: By building your own LLM, you have greater control over its architecture, training
    data, and fine-tuning. This allows you to tailor the model specifically to your
    needs and domain. You can optimize it for a particular task, industry, or application,
    which may lead to improved performance and more accurate results.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过建立自己的LLM，您可以更好地控制其架构、训练数据和微调。这使您能够将模型专门定制到您的需求和领域。您可以为特定任务、行业或应用程序进行优化，这可能会带来更好的性能和更准确的结果。
- en: Some organizations may have strict data privacy requirements or sensitive information
    that cannot be shared with a third-party service. In fact, Samsung recently banned
    its employees from using GPT or Bard out of fear that their interactions could
    inadvertently leak proprietary company information. Building your own LLM ensures
    that all data and processing remain within your organization, reducing privacy
    and security concerns.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些组织可能有严格的数据隐私要求或无法与第三方服务共享的敏感信息。事实上，三星最近禁止其员工使用GPT或Bard，因为他们担心他们的互动可能会意外泄漏专有公司信息。建立自己的LLM可以确保所有数据和处理都留在您的组织内部，减少隐私和安全方面的担忧。
- en: If your application requires specialized knowledge or operates in a niche domain,
    building your own LLM allows you to incorporate specific data sources and domain
    expertise into the training process. This can enhance the model’s understanding
    and generate more relevant responses tailored to your specific domain.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的应用需要专业知识或在一个小众领域操作，建立自己的LLM可以让您将特定的数据源和领域专业知识纳入到训练过程中。这可以增强模型的理解能力，并生成更符合您特定领域需求的响应。
- en: Pretrained models like GPT are designed to be general-purpose and work reasonably
    well across various domains. However, for specific tasks, building a custom LLM
    can potentially result in improved performance and efficiency. You can optimize
    the architecture, training methods, and configuration settings to achieve better
    results on your particular use case.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像GPT这样的预训练模型被设计为通用型，并且在各种领域都能工作得相当好。然而，对于特定任务，建立一个定制的LLM可能会导致性能和效率的提升。您可以优化架构、训练方法和配置设置，以在特定用例上获得更好的结果。
- en: Building your own LLM gives you ownership and control over the model’s intellectual
    property. You can modify, extend, and distribute the model to meet your requirements,
    without being bound by the limitations or licensing agreements associated with
    using existing models.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自己的LLM可以让你拥有对模型的知识产权的所有权和控制权。你可以修改、扩展和分发模型以满足你的需求，而不受使用现有模型所带来的限制或许可协议的约束。
- en: In the wake of the Meta leak, many smart individuals in the community focused
    their attention on building LLM variations that could accomplish much more with
    much less hardware. Quantization, for instance, involved compressing models so
    they could even run on computers without graphic processor unit (GPUs). Ultra
    efficient fine tuning techniques, including something called Low-Rank Adaptation
    (LoRA), allowed for model fine tuning that consumes a tiny fraction of the resources
    and time that were previously required.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Meta泄露事件后，社区中许多聪明的人们将注意力集中在构建可以用更少硬件完成更多任务的LLM变体上。例如，量化是一种将模型压缩以便在没有图形处理器单元（GPU）的计算机上运行的方法。超高效的微调技术，包括一种称为Low-Rank
    Adaptation (LoRA)的技术，使得模型微调所需的资源和时间大大减少。
- en: All of this was noted in a [widely-read internal Google document](p.html) that
    somehow found its way to the open internet. The unknown author forcefully made
    the point that the big players - including OpenAI, Meta, and Google - had, for
    all intents and purposes, lost their competitive advantage in the AI space. From
    here on in, big advances in the technology would be happening out in the wild,
    far beyond the control of either big companies or governments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些内容都在一份[p.html](p.html)上被广泛阅读的内部Google文档中提到，不知何故，这份文档进入了公开的互联网。未知作者强烈指出，包括OpenAI、Meta和Google在内的大公司在人工智能领域已经完全失去了竞争优势。从现在开始，技术上的重大进展将发生在野外，远离大公司或政府的控制之外。
- en: 'So why you might want your own LLM? Because, at this point, it’s possible to
    enjoy whole new levels of customization and optimization. How will it work? Well,
    since there’s really no reason I can think of that you would want to start your
    own LLM project from the bottom up, I’ll assume you’re interested in an existing
    platform. That’ll leave you with three choices: a model, a set of weights, and
    whether you’ll also want to fine tune the model you do choose.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么你可能想要拥有自己的LLM呢？因为此时，你可以享受全新的定制和优化水平。它会如何工作呢？嗯，因为我实在想不到你有任何理由从零开始启动自己的LLM项目，所以我假设你对现有平台感兴趣。你会面临三个选择：一个模型，一组权重，以及你是否还想对所选择的模型进行微调。
- en: Building an LLM can mean different things to different people, and that’s because
    what we call "LLMs" are made up of multiple moving parts. Technically, there’s
    input encoding, the neural network architecture, an embedding layer, hidden layers,
    an attention mechanism, training data, a decoding algorithm and boatloads of training
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个LLM对不同的人可能意味着不同的事情，这是因为我们所谓的"LLMs"由多个组成部分组成。从技术上讲，有输入编码、神经网络架构、嵌入层、隐藏层、注意力机制、训练数据、解码算法以及大量的训练数据。
- en: To be honest, I don’t really fully understand what most of those are or what
    they’re supposed to do. For our purposes right now, it’s enough to think of the
    code defining the encoding and general architecture as the *model* and, for transformer-based
    LLMs at least (in other words, LLMs that are meant to work like GPT), and the
    "attention mechanism" as being responsible for defining the *weights*. An attention
    mechanism, by the way, permits the modelling of context and relationships between
    words or tokens in a more sophisticated manner.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 老实说，我真的不太明白大部分这些是什么，以及它们应该做什么。就我们现在的目的而言，仅仅将定义编码和一般架构的代码视为*模型*就足够了。至于基于Transformer的语言模型（LLMs），至少对于像GPT那样的LLMs，我们可以将"注意力机制"视为定义*权重*的原因。顺便说一句，注意力机制能够以更复杂的方式对上下文和单词或令牌之间的关系进行建模。
- en: What exactly are weights? In a neural network, each connection between neurons
    is assigned a weight, which represents the strength or importance of that connection.
    For a model, these weights are learnable parameters that are adjusted during the
    training process, where the LLM is exposed to a large amount of training data
    and learns to predict the next word or generate coherent text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 权重到底是什么？在神经网络中，每个神经元之间的连接都被赋予一个权重，表示该连接的强度或重要性。对于一个模型来说，这些权重是可学习的参数，在训练过程中进行调整。在训练过程中，LLM暴露于大量的训练数据，并学会预测下一个单词或生成连贯的文本。
- en: The weights determine how information flows through the network and how it influences
    the final predictions or outputs of the LLM. By adjusting the weights, the model
    can learn to assign higher importance to certain input features or patterns and
    make more accurate predictions based on the training data it has been exposed
    to. Without weights an LLM model is pretty much useless.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 权重确定了信息如何通过网络流动以及它们如何影响LLM的最终预测或输出。通过调整权重，模型可以学习将更高的重要性赋予某些输入特征或模式，并根据其接触到的训练数据进行更准确的预测。没有权重，LLM模型几乎没有用处。
- en: 9.2 Selecting a base LLM model for configuration
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择基本LLM模型进行配置
- en: An excellent place to begin your research is the [Hugging Face Open LLM Leaderboard](HuggingFaceH4.html)
    which lists the evaluated performance of many freely available transformer-based
    LLMs. You can toggle each of the assessment columns to narrow down your search
    by specific features. Those features include "ARC" - the A12 Reasoning Challenge
    - which tests models on how they answer questions about high school science. Clicking
    the About tab on that page will give you excellent descriptions of all the assessment
    criteria.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的开始你的研究的地方是[Hugging Face开放LLM排行榜](HuggingFaceH4.html)，其中列出了许多免费提供的基于transformers的LLM的评估性能。您可以切换每个评估列，以通过特定功能缩小搜索范围。这些功能包括“ARC”
    - A12推理挑战 - 它测试模型如何回答有关高中科学的问题。点击该页面上的“关于”选项卡将为您提供所有评估标准的优秀描述。
- en: As you browse the alternatives in that list, you’ll see that there are a few
    key *families* of LLMs, like Meta’s LLaMA and Together Computer’s RedPajama. There
    are also models that were derived from other models. [OpenLLaMA](openlm-research.html),
    for instance, is a "reproduction of Meta AI’s LLaMA 7B" model that was "trained
    on the RedPajama dataset."
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在列表中浏览替代品时，您会注意到有几个关键的LLM系列，例如Meta的LLaMA和Together Computer的RedPajama。还有一些从其他模型派生出来的模型。例如，[OpenLLaMA](openlm-research.html)是一个“复制Meta
    AI的LLaMA 7B”模型，该模型在“RedPajama数据集”上进行了训练。
- en: 'You’ll notice how model names usually include their parameter size (in billions):
    7B, 13B, 33B, 65B, etc. As a rule, the more parameters used to build a model,
    the better hardware you’ll need to run it. Clicking through to the individual
    documentation pages for a model will often show you how many *tokens* were used
    for the model’s training. A larger model might have incorporated well over a trillion
    tokens.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到模型名称通常包括其参数大小（以十亿为单位）：7B、13B、33B、65B等等。一般来说，用于构建模型的参数越多，您就需要更好的硬件来运行它。点击模型的单独文档页面通常会向您显示模型训练所使用的*标记*数量。一个较大的模型可能会包含超过一万亿个标记。
- en: Once you’ve selected a model, you’ll normally head over to its GitHub page where
    there will usually be instructions for usage and for how to clone or download
    the model itself. A good example of that is [the llama.cpp LLaMA inference](tree.html).
    But even once you’ve got the software on your machine, you’ll usually still need
    to download a set of weights separately.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您选择了一个模型，您通常会转到其GitHub页面，那里通常会有使用说明以及如何克隆或下载模型本身的说明。其中一个很好的例子是[llama.cpp LLaMA推理](tree.html)。但是即使您已经在您的机器上获得了软件，通常您仍然需要单独下载一组权重。
- en: Why don’t they just bundle weights with their models? For one thing, you might
    need a custom combination for your specific task. But there’s something else going
    on, too. Some weight sets are [available only once your request is approved](1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA.html).
    And many of the sets that are freely available come from…​shall we say…​dubious
    sources. In that context, it’s probably just not practical to offer them all together
    in one place.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 他们为什么不直接将权重与他们的模型捆绑在一起呢？首先，您可能需要为您的特定任务定制一种权重组合。但还有另一件事情正在进行。有些权重集只有在您的请求获得批准后才能获得。而且，许多免费提供的集合来自……我们应该说……可疑的来源。在这种情况下，将它们都放在一个地方提供可能实际上并不实际。
- en: Having said that, the [Alpaca-LoRA](tloen.html) and [RedPajama-INCITE-3B](blog.html)
    models come with scripts that can fetch a weight set for you as part of the build
    process. We’ll walk through a RedPyjama build example in just a minute.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，[Alpaca-LoRA](tloen.html)和[RedPajama-INCITE-3B](blog.html)模型附带了可以在构建过程中为您获取权重集的脚本。我们将在接下来的几分钟内演示一个RedPyjama构建示例。
- en: 'One final consideration when choosing an LLM: you’ll need to make sure that
    the model will run on the hardware you’ve got. Because of their heavy reliance
    on compute processing power, most models require graphic processor units (GPUs)
    and, sometimes, significant free dedicated *video RAM* memory. If you’re planning
    to use a regular consumer laptop or desktop for the task, make sure you’re working
    with a CPU-only model. Alternatively, you can always rent all the GPU-power you
    need from a cloud provider like [Amazon Web Services](nvidia.html).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择LLM时的最后一个考虑因素是，你需要确保模型能够在你拥有的硬件上运行。由于它们严重依赖计算处理能力，大多数模型都需要图形处理单元（GPUs），有时还需要大量的专用*视频内存*。如果你计划在任务中使用常规的消费者笔记本电脑或台式机，请确保你使用的是仅CPU模型。另外，你也可以随时向云提供商（例如[亚马逊网络服务](nvidia.html)）租用所需的所有GPU计算能力。
- en: 9.3 Configuring and building your model
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 配置和构建你的模型
- en: If you try the following instructions for yourself, you may find yourself chugging
    along happily as your LLM builds when, suddenly, everything grinds to a screeching
    halt. "But" you exclaim, "I followed the model’s instructions perfectly."
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试以下说明，你可能会发现在你的LLM构建时一切都进行得很顺利，但突然间一切都停滞了下来。"但是"你喊道，"我完美地遵循了模型的指令。"
- en: 'You did indeed. Too perfectly, in fact. You see, those instructions will often
    require just a bit of customization before they’ll work. The most common change
    involves this command parameter:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你确实做到了。实际上，太完美了。你知道，这些说明通常需要进行一点定制才能起作用。最常见的更改涉及到这个命令参数：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'That `/path/to/downloaded/…​` bit is supposed to be updated to reflect the
    actual file system location where the `.bin` pretrained weights files you’re supposed
    to have downloaded is stored. Which might look something like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那个`/path/to/downloaded/...`应该被更新以反映存储着你所下载的`.bin`预训练权重文件的实际文件系统位置。可能看起来像这样：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This [documentation page](blog.html) nicely walks us through the downloading
    and launch of their model. You’d begin by cloning the base archive:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[文档页面](blog.html)很好地指导我们如何下载和启动他们的模型。你可以从克隆基本存档开始：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You will then run `make` to, in this case, build the environment necessary for
    a quantized (compressed) chat session.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你将运行`make`来，在这种情况下，构建一个压缩的（quantized）聊天环境所必需的。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This script will actually download and build the appropriate set of quantized
    weights:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本实际上会下载并构建适当的一组量化（quantized）权重：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, you can fire up the chat with the `redpajama-chat` command that targets
    the `ggml-RedPajama-INCITE-Chat-3B-v1-f16.bin` weights file and passes a long
    list of configuration parameters (any of which can be altered to fit your needs).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用`redpajama-chat`命令启动聊天，该命令以`ggml-RedPajama-INCITE-Chat-3B-v1-f16.bin`权重文件为目标，并传递一个长长的配置参数列表（其中任何一个都可以根据你的需求进行更改）。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Git archive comes with Python scripts to help you further customize your
    experience. You can, for instance, experiment with various quantized methods by
    passing arguments like `--quantize-output-type q4_1` against the `./examples/redpajama/scripts/quantize-gptneox.py`
    script.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Git存档附带了Python脚本，帮助你进一步定制你的体验。例如，你可以通过向`./examples/redpajama/scripts/quantize-gptneox.py`脚本传递诸如`--quantize-output-type
    q4_1`之类的参数，来尝试各种量化方法。
- en: 9.4 Fine tuning your model
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 调整你的模型
- en: Fine tuning involves much more work than the configuration we just saw. If you’ve
    got a GPU, then you can consider fine tuning downloaded models for yourself. As
    a benchmark, one popular high-end GPU card that’ll work for many LLM build operations
    would include the Nvidia 3090 that was, once upon a time, primarily marketed for
    gaming computers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 调整需要比我们刚刚看到的配置更多的工作。如果你有GPU，那么你可以考虑自己调整下载的模型。作为一个基准，一张流行的高端GPU卡，对于许多LLM构建操作来说是有效的，可能包括曾经主要用于游戏电脑的Nvidia
    3090。
- en: As far as I can tell (never having owned one myself) the 3090 will come with
    24GB of graphics RAM. That, apparently, will be good enough for fine tuning using
    the [efficient LoRA method](training.html) we mentioned earlier. Otherwise, you
    might have to chain together *multiple* Nvidia 3090s. That won’t be cheap (3090s
    seem to go for $1,400 or so each), but it’s still in a different galaxy from the
    way OpenAI, Meta, and Google have been doing things.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就我所知（从未拥有过自己的）3090将配备24GB的图形内存。显然，这对于使用我们之前提到的[高效的LoRA方法](training.html)进行调整足够了。否则，你可能需要将*多个*
    Nvidia 3090链接在一起。这不会便宜（3090似乎每张要价约1400美元），但它与OpenAI、Meta和Google一直在做的事情完全不同。
- en: One difference between fine tuning and simply configuring a model (the way we
    just saw) is that fine tuning involves re-training your model on data sets typically
    consisting of hundreds of billions of tokens (each of which us roughly equivalent
    to a single word). It’s these large sets that, hopefully, allow the model to capture
    general language patterns and knowledge. The real customization happens here,
    where you’re free to use your own data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 微调和简单配置模型（我们刚刚看到的方式）之间的一个区别是，微调涉及重新在通常包含数千亿令牌（每个令牌大致相当于一个单词）的数据集上训练您的模型。正是这些大型数据集，有望使模型捕捉到通用的语言模式和知识。真正的定制工作发生在这里，你可以自由使用自己的数据。
- en: Having said all that, I’m not going to show you how any of this works on a practical
    level. Not only do I lack the hardware to make it work, but I suspect that’s also
    true for you. But it is worth at least thinking about it in general terms.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管说了这么多，我不打算在实践层面向你展示任何工作原理。我不仅缺乏使其工作的硬件，而且我怀疑你也是如此。但至少从一般的角度来思考一下是值得的。
- en: 9.4.1 Creating a data set
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 创建数据集
- en: To build a model that’s specialized for use by lawyers or medical professionals,
    as an example, you’d want a data set that’s heavy on legal or medical content.
    But given the sheer volume of content necessary to train an effective model, you
    can appreciate why you’ll want some more powerful hardware.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要构建一个专门供律师或医疗专业人士使用的模型，你需要一个包含大量法律或医疗内容的数据集。但考虑到训练有效模型所需的内容量之大，你可以理解为什么你需要一些更强大的硬件。
- en: Building your data set and then executing the fine tuning build are way past
    the scope of this book. Not to mention, of course, that the way they’re done will
    almost certainly have changed unrecognizably by the time your read these words.
    So if there’s a fine tuning event somewhere in your future this, sadly, is not
    where you’re going to find out how it’ll go.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 构建您的数据集，然后执行微调构建已经超出了本书的范围。更不用说，当你阅读这些文字时，它们的执行方式几乎肯定已经发生了不可识别的变化。所以如果在你的未来有微调事件，很遗憾，这里不是你找到答案的地方。
- en: 9.4.2 Training your model
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 训练您的模型
- en: Because they’re terms used a lot in the context of training and fine tuning
    LLMs, I should briefly describe the **zero-shot** and **few-shot** approaches
    to model training. Both zero-shot and few-shot training will normally follow the
    pre-training phrase where the model is exposed to its large training data sets.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们在LLMs的训练和微调的背景下经常被使用，我应该简要描述一下**零样本**和**少样本**方法来进行模型训练。零样本和少样本训练通常会在模型暴露于其大型训练数据集的预训练阶段之后进行。
- en: Zero-shot learning involves using a language model to perform a task for which
    it hasn’t received any specific training. Instead, it leverages its general language
    understanding to complete the task based on a prompt or instruction. The key idea
    is that the model can generalize from its pre-trained knowledge and adapt it to
    the new task at hand. By providing a detailed prompt that specifies the desired
    task and format, the model can generate relevant outputs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习涉及使用语言模型执行其没有接受任何特定训练的任务。相反，它利用其对语言的一般理解来根据提示或指令完成任务。关键的想法是模型可以从其预先训练的知识中概括，并将其适应于新任务。通过提供详细的提示，指定所需的任务和格式，模型可以生成相关的输出。
- en: 'For example, you can instruct the model with a zero-shot prompt like, "Translate
    the following English sentence into French: Hello, how are you?" even if the model
    hasn’t been fine-tuned specifically for translation tasks. The model will then
    generate the translated output based on its understanding of language and the
    prompt.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，即使模型尚未针对翻译任务进行特定的微调，你也可以使用零样本提示指导模型，比如，“将以下英语句子翻译成法语：Hello, how are you?”。然后，该模型将基于其对语言和提示的理解生成翻译输出。
- en: Few-shot learning involves providing a limited amount of task-specific training
    examples or demonstrations to the language model, allowing it to quickly adapt
    to the new task. While zero-shot learning doesn’t involve any task-specific training,
    few-shot learning provides a small number of examples to help the model better
    understand the task. By conditioning the model on these few examples, it can learn
    to perform the desired task more accurately.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习涉及提供少量特定任务的训练示例或演示给语言模型，以便它可以快速适应新任务。虽然零样本学习不涉及任何特定任务的训练，但少样本学习提供了少量示例来帮助模型更好地理解任务。通过在这些少量示例上对模型进行条件设置，它可以学会更准确地执行所需的任务。
- en: For instance, if you want the model to summarize news articles, you might provide
    a few examples of article summaries along with the articles themselves. The model
    can then use this information to generate summaries for other articles.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想让模型总结新闻文章，您可能会提供一些文章摘要的例子以及文章本身。然后，模型可以使用这些信息为其他文章生成摘要。
- en: Both zero-shot and few-shot learning approaches allow language models to perform
    various tasks without requiring extensive fine-tuning or training on large datasets.
    They showcase the impressive generalization capabilities of these models, enabling
    them to apply their language understanding to a wide range of tasks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习和少样本学习方法都允许语言模型在不需要进行广泛的微调或在大型数据集上进行训练的情况下执行各种任务。它们展示了这些模型惊人的泛化能力，使它们能够将其语言理解应用于各种任务。
- en: 9.5 Summary
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 摘要
- en: Custom-build large language models can solve problems for which off-the-shelf
    models aren’t appropriate
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制大型语言模型可以解决那些通用模型不合适的问题。
- en: Configuring your own model requires starting with a base LLM
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置您自己的模型需要从一个base LLM开始。
- en: Fine tuning your a new model requires access to your own data set and significant
    hardware resources
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对新模型进行良好的调整需要访问您自己的数据集和重要的硬件资源。
