- en: Chapter 10\. Generative deep learning
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章. 生成深度学习
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*这一章涵盖了*'
- en: What generative deep learning is, its applications, and how it differs from
    the deep-learning tasks we’ve seen so far
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成深度学习是什么，它的应用以及它与我们迄今看到的深度学习任务有何不同
- en: How to generate text using an RNN
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用RNN生成文本
- en: What latent space is and how it can form the basis of generating novel images,
    through the example of variational autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是潜在空间以及它如何成为生成新图像的基础，通过变分自编码器示例
- en: The basics of generative adversarial networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络的基础知识
- en: Some of the most impressive tasks demonstrated by deep neural networks have
    involved generating images, sounds, and text that look or sound real. Nowadays,
    deep neural networks are capable of creating highly realistic human face images,^([[1](#ch10fn1)])
    synthesizing natural-sounding speech,^([[2](#ch10fn2)]) and composing compellingly
    coherent text,^([[3](#ch10fn3)]) just to name a few achievements. Such *generative*
    models are useful for a number of reasons, including aiding artistic creation,
    conditionally modifying existing content, and augmenting existing datasets to
    support other deep-learning tasks.^([[4](#ch10fn4)])
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络展示了生成看起来或听起来真实的图像、声音和文本的一些令人印象深刻的任务。如今，深度神经网络能够创建高度真实的人脸图像，^([[1](#ch10fn1)])合成自然音质的语音，^([[2](#ch10fn2)])以及组织连贯有力的文本，^([[3](#ch10fn3)])这仅仅是一些成就的名单。这种*生成*模型在许多方面都很有用，包括辅助艺术创作，有条件地修改现有内容，以及增强现有数据集以支持其他深度学习任务。^([[4](#ch10fn4)])
- en: ¹
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tero Karras, Samuli Laine, and Timo Aila, “A Style-Based Generator Architecture
    for Generative Adversarial Networks,” submitted 12 Dec. 2018, [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948).
    See a live demo at [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/).
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tero Karras, Samuli Laine 和 Timo Aila, “一种基于风格的生成对抗网络,” 提交日期：2018年12月12日, [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948).
    在 [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/) 查看演示。
- en: ²
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Aäron van den Oord and Sander Dieleman, “WaveNet: A Generative Model for Raw
    Audio,” blog, 8 Sept. 2016, [http://mng.bz/MOrn](http://mng.bz/MOrn).'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Aäron van den Oord 和 Sander Dieleman, “WaveNet: 一种用于原始音频的生成模型,” 博客, 2016年9月8日,
    [http://mng.bz/MOrn](http://mng.bz/MOrn).'
- en: ³
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “Better Language Models and Their Implications,” OpenAI, 2019, [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/).
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “更好的语言模型及其影响”，OpenAI, 2019, [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/).
- en: ⁴
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Antreas Antoniou, Amos Storkey, and Harrison Edwards, “Data Augmentation Generative
    Adversarial Networks,” submitted 12 Nov. 2017, [https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340).
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Antreas Antoniou, Amos Storkey 和 Harrison Edwards, “数据增强生成对抗网络,” 提交日期：2017年11月12日,
    [https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340).
- en: Apart from practical applications such as putting makeup on the selfie of a
    potential cosmetic customer, generative models are also worth studying for theoretical
    reasons. Generative and discriminative modeling are two fundamentally different
    types of models in machine learning. All the models we’ve studied in this book
    so far are *discriminative* models. Such models are designed to map an input into
    a discrete or continuous value without caring about the process through which
    the input is generated. Recall the classifiers for phishing websites, iris flowers,
    MNIST digits, and speech sounds, as well as the regressor for housing prices we’ve
    built. By contrast, generative models are designed to mathematically mimic the
    process through which the examples of different classes are generated. But once
    a generative model has learned this generative knowledge, it can perform discriminative
    tasks as well. So generative models can be said to “understand” the data better
    compared to discriminative models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在潜在顾客的自拍照上化妆等实际应用外，生成模型还值得从理论上研究。生成模型和判别模型是机器学习中两种根本不同类型的模型。到目前为止，我们在本书中研究的所有模型都是*判别*模型。这些模型旨在将输入映射到离散或连续的值，而不关心生成输入的过程。回想一下，我们构建的网络针对钓鱼网站、鸢尾花、MNIST数字和音频声音的分类器，以及对房价进行回归的模型。相比之下，生成模型旨在数学地模拟不同类别示例生成的过程。但是一旦生成模型学习到这种生成性知识，它也可以执行判别性任务。因此，与判别模型相比，可以说生成模型“更好地理解”数据。
- en: This section covers the foundations of deep generative models for text and images.
    By the end of the chapter, you should be familiar with the ideas behind RNN-based
    language models, image-oriented autoencoders, and generative adversarial networks.
    You should also be familiar with the pattern in which such models are implemented
    in TensorFlow.js and be capable of applying these models to your own dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了文本和图像的深度生成模型的基础知识。在本章结束时，您应该熟悉基于 RNN 的语言模型、面向图像的自编码器和生成对抗网络的思想。您还应该熟悉这些模型在
    TensorFlow.js 中的实现方式，并能够将这些模型应用到您自己的数据集上。
- en: 10.1\. Generating text with LSTM
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1\. 使用 LSTM 生成文本
- en: Let’s start from text generation. To do that, we will use RNNs, which we introduced
    in the previous chapter. Although the technique you’ll see here generates text,
    it is not limited to this particular output domain. The technique can be adapted
    to generate other types of sequences, such as music—given the ability to represent
    musical notes in a suitable way and find an adequate training dataset.^([[5](#ch10fn5)])
    Similar ideas can be applied to generate pen strokes in sketching so that nice-looking
    sketches^([[6](#ch10fn6)]) or even realistic-looking Kanjis^([[7](#ch10fn7)])
    can be generated.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从文本生成开始。为此，我们将使用我们在前一章中介绍的 RNN。虽然您将在这里看到的技术生成文本，但它并不局限于这个特定的输出领域。该技术可以适应生成其他类型的序列，比如音乐——只要能够以合适的方式表示音符，并找到一个足够的训练数据集。[[5](#ch10fn5)]类似的思想可以应用于生成素描中的笔画，以便生成漂亮的素描[[6](#ch10fn6)]，甚至是看起来逼真的汉字[[7](#ch10fn7)]。
- en: ⁵
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, see Performance-RNN from Google’s Magenta Project: [https://magenta.tensorflow.org/performance-rnn](https://magenta.tensorflow.org/performance-rnn).'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，请参阅 Google 的 Magenta 项目中的 Performance-RNN：[https://magenta.tensorflow.org/performance-rnn](https://magenta.tensorflow.org/performance-rnn)。
- en: ⁶
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, see Sketch-RNN by David Ha and Douglas Eck: [http://mng.bz/omyv](http://mng.bz/omyv).'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，请参阅 David Ha 和 Douglas Eck 的 Sketch-RNN：[http://mng.bz/omyv](http://mng.bz/omyv)。
- en: ⁷
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: David Ha, “Recurrent Net Dreams Up Fake Chinese Characters in Vector Format
    with TensorFlow,” blog, 28 Dec. 2015, [http://mng.bz/nvX4](http://mng.bz/nvX4).
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: David Ha，“Recurrent Net Dreams Up Fake Chinese Characters in Vector Format with
    TensorFlow”，博客，2015年12月28日，[http://mng.bz/nvX4](http://mng.bz/nvX4)。
- en: '10.1.1\. Next-character predictor: A simple way to generate text'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.1\. 下一个字符预测器：生成文本的简单方法
- en: First, let’s define the text-generation task. Suppose we have a corpus of text
    data of decent size (at least a few megabytes) as the training input, such as
    the complete works of Shakespeare (a very long string). We want to train a model
    to generate new texts that *look like* the training data as much as possible.
    The key phrase here is, of course, “look like.” For now, let’s be content with
    not precisely defining what “look like” means. The meaning will become clearer
    after we show the method and the results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义文本生成任务。假设我们有一个相当大的文本数据语料库（至少几兆字节）作为训练输入，比如莎士比亚的全部作品（一个非常长的字符串）。我们想要训练一个模型，尽可能地生成看起来*像*训练数据的新文本。这里的关键词当然是“看起来”。现在，让我们满足于不精确地定义“看起来”的含义。在展示方法和结果之后，这个意义将变得更加清晰。
- en: 'Let’s think about how to formulate this task in the paradigm of deep learning.
    In the date-conversion example covered in the previous chapter, we saw how a precisely
    formatted output sequence can be generated from a casually formatted input one.
    That text-to-text conversion task had a well-defined answer: the correct date
    string in the ISO-8601 format. However, the text-generation task here doesn’t
    seem to fit this bill. There is no explicit input sequence, and the “correct”
    output is not well-defined; we just want to generate something that “looks real.”
    What can we do?'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考如何在深度学习范式中制定这个任务。在前一章节涉及的日期转换示例中，我们看到一个精确格式化的输出序列可以从一个随意格式化的输入序列中生成。那个文本到文本的转换任务有一个明确定义的答案：ISO-8601
    格式中的正确日期字符串。然而，这里的文本生成任务似乎不适合这一要求。没有明确的输入序列，并且“正确”的输出并没有明确定义；我们只想生成一些“看起来真实的东西”。我们能做什么呢？
- en: A solution is to build a model to predict what character will come after a sequence
    of characters. This is called *next-character prediction*. For instance, a well-trained
    model on the Shakespeare dataset should predict the character “u” with a high
    probability when given the character string “Love looks not with the eyes, b”
    as the input. However, that generates only one character. How do we use the model
    to generate a sequence of characters? To do that, we simply form a new input sequence
    of the same length as before by shifting the previous input to the left by one
    character, discarding the first character, and sticking the newly generated character
    (“u”) at the end. This gives us a new input for our next-character predictor,
    namely, “ove looks not with the eyes, bu” in this case. Given this new input sequence,
    the model should predict the character “t” with a high probability. This process,
    which is illustrated in [figure 10.1](#ch10fig01), can be repeated as many times
    as necessary to generate a sequence as long as desired. Of course, we need an
    initial snippet of text as the starting point. For that, we can just sample randomly
    from the text corpus.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方案是构建一个模型，预测在一系列字符之后会出现什么字符。这被称为 *下一个字符预测*。例如，对于在莎士比亚数据集上训练良好的模型，当给定字符串“Love
    looks not with the eyes, b”作为输入时，应该以高概率预测字符“u”。然而，这只生成一个字符。我们如何使用模型生成一系列字符？为了做到这一点，我们简单地形成一个与之前相同长度的新输入序列，方法是将前一个输入向左移动一个字符，丢弃第一个字符，并将新生成的字符（“u”）粘贴到末尾。在这种情况下，我们的下一个字符预测器的新输入就是“ove
    looks not with the eyes, bu”。给定这个新的输入序列，模型应该以高概率预测字符“t”。这个过程，如[图10.1](#ch10fig01)所示，可以重复多次，直到生成所需长度的序列。当然，我们需要一个初始的文本片段作为起点。为此，我们可以从文本语料库中随机抽样。
- en: Figure 10.1\. A schematic illustration of how an RNN-based next-character predictor
    can be used to generate a sequence of text from an initial input snippet of text
    as the seed. At each step, the RNN predicts the next character using the input
    text. Then, the input text is concatenated with the predicted next character and
    discards the first character. The result forms the input for the next step. At
    each step, the RNN outputs the probability scores for all possible characters
    in the character set. To determine the actual next character, a random sampling
    is carried out.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1\. 用基于 RNN 的下一个字符预测器生成文本序列的示意图，以初始输入文本片段作为种子。在每个步骤中，RNN 使用输入文本预测下一个字符。然后，将输入文本与预测的下一个字符连接起来，丢弃第一个字符。结果形成下一个步骤的输入。在每个步骤中，RNN
    输出字符集中所有可能字符的概率分数。为了确定实际的下一个字符，进行随机抽样。
- en: '![](10fig01a_alt.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig01a_alt.jpg)'
- en: This formulation turns the sequence-generation task into a sequence-based classification
    problem. This problem is similar to what we saw in the IMDb sentiment-analysis
    problem in [chapter 9](kindle_split_021.html#ch09), in which a binary class was
    predicted from an input of a fixed length. The model for text generation does
    essentially the same thing, although it is a multiclass-classification problem
    involving *N* possible classes, where *N* is the size of the character set—namely,
    the number of all unique characters in the text dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表述将序列生成任务转化为基于序列的分类问题。这个问题类似于我们在[第9章](kindle_split_021.html#ch09)中看到的 IMDb
    情感分析问题，其中从固定长度的输入中预测二进制类别。文本生成模型基本上做了同样的事情，尽管它是一个多类别分类问题，涉及到 *N* 个可能的类别，其中 *N*
    是字符集的大小——即文本数据集中所有唯一字符的数量。
- en: This next-character-prediction formulation has a long history in natural language
    processing and computer science. Claude Shannon, the pioneer of information theory,
    conducted an experiment in which human participants were asked to guess the next
    letter after seeing a short snippet of English text.^([[8](#ch10fn8)]) Through
    this experiment, he was able to estimate the average amount of uncertainty in
    every letter of the typical English texts, given the context. This uncertainty,
    which turned out to be about 1.3 bits of entropy, tells us the average amount
    of information carried by every letter in English.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种下一个字符预测的表述在自然语言处理和计算机科学中有着悠久的历史。信息论先驱克劳德·香农进行了一项实验，在实验中，被要求的人类参与者在看到一小段英文文本后猜测下一个字母。[[8](#ch10fn8)]
    通过这个实验，他能够估计出在给定上下文的情况下，典型英文文本中每个字母的平均不确定性。这种不确定性约为1.3位的熵，告诉我们每个英文字母所携带的平均信息量。
- en: ⁸
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The original 1951 paper is accessible at [http://mng.bz/5AzB](http://mng.bz/5AzB).
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1951 年的原始论文可在[http://mng.bz/5AzB](http://mng.bz/5AzB)中获取。
- en: The 1.3 bits result is less than the number of bits if the 26 letters appeared
    in a completely random fashion, which would be log[2](26) = 4.7 bits. This matches
    our intuition because we know letters do not appear randomly in English. Instead,
    they follow patterns. At a lower level, only certain sequences of letters are
    valid English words. At a higher level, only a certain ordering of words satisfies
    English grammar. At an even higher level, only a subset of grammatically valid
    sentences actually make real sense.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当字母以完全随机的方式出现时，1.3 位的结果比如果 26 个字母完全随机出现所需的位数要少，该数值为 log[2](26) = 4.7 位数。这符合我们的直觉，因为我们知道英语字母并不是随机出现的，而是具有某些模式。在更低的层次上，只有某些字母序列是有效的英语单词。在更高的层次上，只有某些单词的排序满足英语语法。在更高的层次上，只有某些语法上有效的句子实际上是有意义的。
- en: 'If you think about it, this is what our text-generation task is fundamentally
    about: learning these patterns on all these levels. Realize that our model is
    essentially trained to do what Shannon’s subjects did—that is, guess the next
    character. Let’s now take a look at the example code and how it works. Keep Shannon’s
    result of 1.3 bits in mind because we’ll come back to it later.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑一下，这正是我们的文本生成任务的基础所在：学习所有这些层面的模式。注意，我们的模型基本上是被训练来做 Shannon 实验中的那个志愿者所做的事情——也就是猜测下一个字符。现在，让我们来看一下示例代码以及它是如何工作的。请记住
    Shannon 的 1.3 位结果，因为我们稍后会回到它。
- en: 10.1.2\. The LSTM-text-generation example
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.2《LSTM-text-generation》示例
- en: The lstm-text-generation example in the tfjs-examples repository involves training
    an LSTM-based next-character predictor and using it to generate new text. The
    training and generation steps both happen in JavaScript using TensorFlow.js. You
    can run the example either in the browser or in the backend environment with Node.js.
    While the former approach provides a more visual and interactive interface, the
    latter gives you faster training speed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 tfjs-examples 仓库中的 `lstm-text-generation` 示例中，我们训练了一个基于 LSTM 的下一个字符预测器，并利用它生成了新的文本。训练和生成都在
    JavaScript 中使用 TensorFlow.js 完成。你可以在浏览器中或者使用 Node.js 运行示例。前者提供了更加图形化和交互式的界面，但后者具有更快的训练速度。
- en: 'To see the example running in the browser, use these commands:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要在浏览器中查看此示例的运行情况，请使用以下命令：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the page that pops up, you can select and load one of four provided text
    datasets to train the model on. We will use the Shakespeare dataset in the following
    discussion. Once the data is loaded, you can create a model for it by clicking
    the Create Model button. A text box allows you to adjust the number of units that
    the created LSTM will have. It is set to 128 by default. But you can experiment
    with other values, such as 64\. If you enter multiple numbers separated by commas
    (for example, `128,128`), the model created will contain multiple LSTM layers
    stacked on top of each other.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在弹出的页面中，你可以选择并加载四个提供的文本数据集中的一个来训练模型。在下面的讨论中，我们将使用莎士比亚的数据集。一旦数据加载完成，你可以点击“创建模型”按钮为它创建一个模型。一个文本框允许你调整创建的
    LSTM 将具有的单元数。它默认设置为 128。但你也可以尝试其他值，例如 64。如果你输入由逗号分隔的多个数字（例如 `128,128`），则创建的模型将包含多个叠放在一起的
    LSTM 层。
- en: 'To perform training on the backend using tfjs-node or tfjs-node-gpu, use the
    command `yarn train` instead of `yarn watch`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 若要使用 tfjs-node 或 tfjs-node-gpu 在后端执行训练，请使用 `yarn train` 命令而不是 `yarn watch`：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you have a CUDA-enabled GPU set up properly, you can add the `--gpu` flag
    to the command to let the training happen on your GPU, which will further increase
    the training speed. The flag `--lstmLayerSize` plays the same role as the LSTM-size
    text box in the browser version of the example. The previous command will create
    and train a model consisting of two LSTM layers, both with 128 units, stacked
    on top of each other.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经正确地设置了 CUDA-enabled GPU，可以在命令中添加 `--gpu` 标志，让训练过程在 GPU 上运行，这将进一步加快训练速度。`--lstmLayerSize`
    标志在浏览器版本的示例中起到了 LSTM-size 文本框的作用。前面的命令将创建并训练一个由两个 LSTM 层组成的模型，每个 LSTM 层都有 128
    个单元，叠放在一起。
- en: The model being trained here has a stacked-LSTM architecture. What does *stacking*
    LSTM layers mean? It is conceptually similar to stacking multiple dense layers
    in an MLP, which increases the MLP’s capacity. In a similar fashion, stacking
    multiple LSTMs allows an input sequence to go through multiple stages of seq2seq
    representational transformation before being converted into a final regression
    or classification output by the final LSTM layer. [Figure 10.2](#ch10fig02) gives
    a schematic illustration of this architecture. One important thing to notice is
    the fact that the first LSTM has its `returnSequence` property set to `true` and
    hence generates a sequence of output that includes the output for every single
    item of the input sequence. This makes it possible to feed the output of the first
    LSTM into the second one, as an LSTM layer expects a sequential input instead
    of a single-item input.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此处正在训练的模型具有堆叠 LSTM 架构。堆叠 LSTM 层是什么意思？在概念上类似于在 MLP 中堆叠多个密集层，这增加了 MLP 的容量。类似地，堆叠多个
    LSTM 允许输入序列在被最终 LSTM 层转换为最终回归或分类输出之前经历多个 seq2seq 表示转换阶段。[图 10.2](#ch10fig02)给出了这种架构的图解。一个重要的事情要注意的是，第一个
    LSTM 的`returnSequence`属性被设置为`true`，因此生成包括输入序列的每个单个项目的输出序列。这使得可以将第一个 LSTM 的输出馈送到第二个
    LSTM 中，因为 LSTM 层期望顺序输入而不是单个项目输入。
- en: Figure 10.2\. How stacking multiple LSTM layers works in a model. In this case,
    two LSTM layers are stacked together. The first one has its `returnSequence` property
    set to `true` and hence outputs a sequence of items. The sequential output of
    the first LSTM is received by the second LSTM as its input. The second LSTM outputs
    a single item instead of a sequence of items. The single item could be regression
    prediction or an array of softmax probabilities, which forms the final output
    of the model.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.2\. 在模型中如何堆叠多个 LSTM 层。在这种情况下，两个 LSTM 层被堆叠在一起。第一个 LSTM 的`returnSequence`属性被设置为`true`，因此输出一个项目序列。第一个
    LSTM 的序列输出被传递给第二个 LSTM 作为其输入。第二个 LSTM 输出一个单独的项目而不是项目序列。单个项目可以是回归预测或 softmax 概率数组，它形成模型的最终输出。
- en: '![](10fig01.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig01.jpg)'
- en: '[Listing 10.1](#ch10ex01) contains the code that builds next-character prediction
    models with the architecture shown in [figure 10.2](#ch10fig02) (excerpted from
    lstm-text-generation/model.js). Notice that unlike the diagram, the code includes
    a dense layer as the model’s final output. The dense layer has a softmax activation.
    Recall that the softmax activation normalizes the outputs so that they have values
    between 0 and 1 and sum to 1, like a probability distribution. So, the final dense
    layer’s output represents the predicted probabilities of the unique characters.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 10.1](#ch10ex01) 包含构建下一个字符预测模型的代码，其架构如[图 10.2](#ch10fig02)所示（摘自 lstm-text-generation/model.js）。请注意，与图表不同，代码包括一个稠密层作为模型的最终输出。密集层具有
    softmax 激活。回想一下，softmax 激活将输出归一化，使其值介于 0 和 1 之间，并总和为 1，就像概率分布一样。因此，最终的密集层输出表示唯一字符的预测概率。'
- en: The `lstmLayerSize` argument of the `createModel`() function controls the number
    of LSTM layers and the size of each. The first LSTM layer has its input shape
    configured based on `sampleLen` (how many characters the model takes at a time)
    and `charSetSize` (how many unique characters there are in the text data). For
    the browser-based example, `sampleLen` is hard-coded to 40; for the Node.js-based
    training script, it is adjustable via the `--sampleLen` flag. `charSetSize` has
    a value of 71 for the Shakespeare dataset. The character set includes the upper-
    and lowercase English letters, punctuation, the space, the line break, and several
    other special characters. Given these parameters, the model created by the function
    in [listing 10.1](#ch10ex01) has an input shape of `[40, 71]` (ignoring the batch
    dimension). This shape corresponds to 40 one-hot-encoded characters. The model’s
    output shape is `[71]` (again, ignoring the batch dimension), which is the softmax
    probability value for the 71 possible choices of the next character.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`createModel()` 函数的 `lstmLayerSize` 参数控制 LSTM 层的数量和每个层的大小。第一个 LSTM 层的输入形状根据
    `sampleLen`（模型一次接收多少个字符）和 `charSetSize`（文本数据中有多少个唯一字符）进行配置。对于基于浏览器的示例，`sampleLen`
    是硬编码为 40 的；对于基于 Node.js 的训练脚本，可以通过 `--sampleLen` 标志进行调整。对于莎士比亚数据集，`charSetSize`
    的值为 71。字符集包括大写和小写英文字母、标点符号、空格、换行符和几个其他特殊字符。给定这些参数，[清单 10.1](#ch10ex01) 中的函数创建的模型具有输入形状
    `[40, 71]`（忽略批处理维度）。该形状对应于 40 个 one-hot 编码字符。模型的输出形状是 `[71]`（同样忽略批处理维度），这是下一个字符的
    71 种可能选择的 softmax 概率值。'
- en: Listing 10.1\. Building a multilayer LSTM model for next-character prediction
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 10.1\. 构建一个用于下一个字符预测的多层 LSTM 模型
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** The length of the model’s input sequence'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 模型输入序列的长度'
- en: '***2*** The number of all possible unique characters'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 所有可能的唯一字符的数量'
- en: '***3*** Size of the LSTM layers of the model, as a single number or an array
    of numbers'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 模型的 LSTM 层的大小，可以是单个数字或数字数组'
- en: '***4*** The model begins with a stack of LSTM layers.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 模型以一堆 LSTM 层开始。'
- en: '***5*** Sets returnSequences to true so that multiple LSTM layers can be stacked'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 设置 `returnSequences` 为 `true` 以便可以堆叠多个 LSTM 层'
- en: '***6*** The first LSTM layer is special in that it needs to specify its input
    shape.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 第一个 LSTM 层是特殊的，因为它需要指定其输入形状。'
- en: '***7*** The model ends with a dense layer with a softmax activation over all
    possible characters, reflecting the classification nature of the next-character
    prediction problem.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 模型以一个密集层结束，其上有一个 softmax 激活函数，适用于所有可能的字符，反映了下一个字符预测问题的分类特性。'
- en: 'To prepare the model for training, we compile it with the categorical cross-entropy
    loss, as the model is essentially a 71-way classifier. For the optimizer, we use
    RMSProp, which is a popular choice for recurrent models:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备模型进行训练，我们使用分类交叉熵损失对其进行编译，因为该模型本质上是一个 71 路分类器。对于优化器，我们使用 RMSProp，这是递归模型的常用选择：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The data that goes into the model’s training consists of pairs of input text
    snippets and the characters that follow each of them, all encoded as one-hot vectors
    (see [figure 10.1](#ch10fig01)). The class `TextData` defined in lstm-text-generation/data.js
    contains the logic to generate such tensor data from the training text corpus.
    The code there is somewhat tedious, but the idea is simple: randomly sample snippets
    of fixed length from the very long string that is our text corpus, and convert
    them into one-hot tensor representations.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输入模型训练的数据包括输入文本片段和每个片段后面的字符的对，所有这些都编码为 one-hot 向量（参见[图 10.1](#ch10fig01)）。在
    lstm-text-generation/data.js 中定义的 `TextData` 类包含从训练文本语料库生成此类张量数据的逻辑。那里的代码有点乏味，但思想很简单：随机从我们的文本语料库中的非常长的字符串中抽取固定长度的片段，并将它们转换为
    one-hot 张量表示。
- en: If you are using the web-based demo, the Model Training section of the page
    allows you to adjust hyperparameters such as the number of training epochs, the
    number of examples that go into each epoch, the learning rate, and so forth. Click
    the Train Model button to kick off the model-training process. For Node.js-based
    training, these hyperparameters are adjustable through the command-line flags.
    For details, you can get help messages by entering the `yarn train --help` command.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用基于 Web 的演示，页面的“模型训练”部分允许您调整超参数，例如训练时期的数量、每个时期进入的示例数量、学习率等等。单击“训练模型”按钮启动模型训练过程。对于基于
    Node.js 的训练，这些超参数可以通过命令行标志进行调整。有关详细信息，您可以通过输入 `yarn train --help` 命令获取帮助消息。
- en: Depending on the number of training epochs you specified and the size of the
    model, the training should take anywhere between a few minutes to a couple of
    hours. The Node.js-based training job automatically prints a number of sample
    text snippets generated by the model after every training epoch (see [table 10.1](#ch10table01)).
    As the training progresses, you should see the loss value go down continuously
    from the initial value of approximately 3.2 and converge in the range of 1.4–1.5\.
    As the loss decreases after about 120 epochs, the quality of the generated text
    should improve, such that toward the end of the training, the text should look
    *somewhat* Shakespearean, and the validation loss should approach the neighborhood
    of 1.5—not too far from the 1.3 bits/character information uncertainty from Shannon’s
    experiment. But note that given our training paradigm and model capacity, the
    generated text will never look like the actual Shakespeare’s writing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您指定的训练周期数和模型大小，训练时间可能会在几分钟到几个小时之间不等。基于 Node.js 的训练作业在每个训练周期结束后会自动打印模型生成的一些示例文本片段（见
    [表格 10.1](#ch10table01)）。随着训练的进行，您应该看到损失值从初始值约为 3.2 不断降低，并在 1.4–1.5 的范围内收敛。大约经过
    120 个周期后，损失减小后，生成的文本质量应该会提高，以至于在训练结束时，文本应该看起来*有些*像莎士比亚的作品，而验证损失应该接近 1.5 左右——并不远离香农实验中的每字符信息不确定性
    1.3 比特。但请注意，考虑到我们的训练范式和模型容量，生成的文本永远不会像实际的莎士比亚的写作。
- en: 'Table 10.1\. Samples of text generated by the LSTM-based next-character prediction
    model. The generation is based on the seed text. Initial seed text: `"` in hourly
    synod about thy particular prosperity, and lo".^([[a](#ch10fntn1)]) Actual text
    that follows the seed text (for comparison): "ve thee no worse than thy old father
    Menenius does! ...".'
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表格 10.1\. 基于 LSTM 的下一字符预测模型生成的文本样本。生成基于种子文本。初始种子文本：" "在每小时的关于你的特定繁荣的议会中，和 lo"。^([[a](#ch10fntn1)])
    根据种子文本后续的实际文本（用于比较）："爱你不会比你的老父亲梅奈尼乌斯对你更差！..."。
- en: ^a
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From *Shakespeare’s Coriolanus*, act 5, scene 2\. Note that the sample includes
    line breaks and stops in the middle of a word (love).
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 摘自 *莎士比亚的《科里奥兰纳斯》*，第 5 幕，第 2 场。请注意，示例中包括换行和单词中间的停顿（love）。
- en: '| Epochs of training | Validation loss | T = 0 | T = 0.25 | T = 0.5 | T = 0.75
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 训练周期 | 验证损失 | T = 0 | T = 0.25 | T = 0.5 | T = 0.75 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 5 | 2.44 | "rle the the the the the the the the the the the the the the the
    the the the the the the the the the the the the the the " | "te ans and and and
    and and warl torle an at an yawl and tand and an an ind an an in thall ang ind
    an tord and and and wa" | "te toll nlatese ant ann, tomdenl, teurteeinlndting
    fall ald antetetell linde ing thathere taod winld mlinl theens tord y" | "p, af
    ane me pfleh; fove this? Iretltard efidestind ants anl het insethou loellr ard,
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 2.44 | "rle the the the the the the the the the the the the the the the
    the the the the the the the the the the the the the the " | "te ans and and and
    and and warl torle an at an yawl and tand and an an ind an an in thall ang ind
    an tord and and and wa" | "te toll nlatese ant ann, tomdenl, teurteeinlndting
    fall ald antetetell linde ing thathere taod winld mlinl theens tord y" | "p, af
    ane me pfleh; fove this? Iretltard efidestind ants anl het insethou loellr ard,
    |'
- en: '| 25 | 1.96 | "ve tray the stanter an truent to the stanter to the stanter
    to the stanter to the stanter to the stanter to the stanter " | "ve to the enter
    an truint to the surt an truin to me truent me the will tray mane but a bean to
    the stanter an trust tra" | "ve of marter at it not me shank to an him truece
    preater the beaty atweath and that marient shall me the manst on hath s" | "rd;
    not an an beilloters An bentest the like have bencest on it love gray to dreath
    avalace the lien I am sach me, m" |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 1.96 | "ve tray the stanter an truent to the stanter to the stanter
    to the stanter to the stanter to the stanter to the stanter " | "ve to the enter
    an truint to the surt an truin to me truent me the will tray mane but a bean to
    the stanter an trust tra" | "ve of marter at it not me shank to an him truece
    preater the beaty atweath and that marient shall me the manst on hath s" | "rd;
    not an an beilloters An bentest the like have bencest on it love gray to dreath
    avalace the lien I am sach me, m" |'
- en: '| 50 | 1.67 | "rds the world the world the world the world the world the world
    the world the world the world the world the worl" | "ngs they are their shall
    the englents the world the world the stand the provicess their string shall the
    world I" | "nger of the hath the forgest as you for sear the device of thee shall,
    them at a hame, The now the would have bo" | "ngs, he coll, As heirs to me which
    upon to my light fronest prowirness foir. I be chall do vall twell. SIR C" |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 1.67 | "世界的世界的世界的世界的世界的世界的世界的世界的世界的世界的世界" | "他们是他们的英语是世界的世界的立场的证明了他们的弦应该世界我"
    | "他们的愤怒的苦恼的，因为你对于你的设备的现在的将会" | "是我的光，我将做vall twell。斯伯"'
- en: '| 100 | 1.61 | "nd the sough the sought That the more the man the forth and
    the strange as the sought That the more the man the " | "nd the sough as the sought
    In the consude the more of the princes and show her art the compont " | "rds as
    the manner. To the charit and the stranger and house a tarron. A tommern the bear
    you art this a contents, " | "nd their conswents That thou be three as me a thout
    thou do end, The longers and an heart and not strange. A G" |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 1.61 | "越多的人越多，越奇怪的是，越奇怪的是，越多的人越多" | "越多的人越多越多" | "越多的人越多。为了这样一个内容，"
    | "和他们的consent，你将会变成三个。长的和一个心脏和不奇怪的。一位G"'
- en: '| 120 | 1.49 | "ve the strike the strike the strike the strike the strikes
    the strike And the strike the strike the strike A" | "ve the fair brother, And
    this in the strike my sort the strike, The strike the sound in the dear strike
    And " | "ve the stratter for soul. Monty to digning him your poising. This for
    his brother be this did fool. A mock''d" | "ve of his trusdum him. poins thinks
    him where sudy''s such then you; And soul they will I would from in my than s"
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 120 | 1.49 | "打击的打击的打击的打击的打击的打击和打击的打击的打击" | "亲爱的打击我的排序的打击，打击打击，亲爱的打击和" |
    "为他的兄弟成为这样的嘲笑。一个模仿的" | "这是我的灵魂。Monty 诽谤他你的矫正。这是为了他的兄弟，这是愚蠢的" | "相信他。因此他们会从我的灵魂中走出来"'
- en: '[Table 10.1](#ch10table01) shows some texts sampled under four different *temperature
    values*, a parameter that controls the randomness of the generated text. In the
    samples of generated text, you may have noticed that lower temperature values
    are associated with more repetitive and mechanical-looking text, while higher
    values are associated with less-predictable text. The highest temperature value
    demonstrated by the Node.js-based training script is 0.75 by default, and it sometimes
    leads to character sequences that look like English but are not actually English
    words (such as “stratter” and “poins” in the samples in the table). In the next
    section, we’ll examine how temperature works and why it is called temperature.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 10.1](#ch10table01) 展示了在四个不同 *温度值* 下采样的一些文本，这是一个控制生成文本随机性的参数。在生成文本的样本中，您可能已经注意到，较低的温度值与更多重复和机械化的文本相关联，而较高的值与不可预测的文本相关联。由
    Node.js 的训练脚本演示的最高温度值默认为 0.75，有时会导致看起来像英语但实际上不是英语单词的字符序列（例如表格中的“stratter”和“poins”）。在接下来的部分中，我们将探讨温度是如何工作的，以及为什么它被称为温度。'
- en: '10.1.3\. Temperature: Adjustable randomness in the generated text'
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.3\. 温度：生成文本中的可调随机性
- en: 'The function `sample()` in [listing 10.2](#ch10ex02) is responsible for determining
    which character will be chosen based on the model’s output probabilities at each
    step of the text-generation process. As you can see, the algorithm is somewhat
    complex: it involves calls to three low-level TensorFlow.js operations: `tf.div()`,
    `tf.log()`, and `tf.multinomial()`. Why do we use this complicated algorithm instead
    of simply picking the choice with the highest probability score, which would take
    a single `argMax()` call?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.2](#ch10ex02) 中的函数 `sample()` 负责根据模型在文本生成过程的每一步的输出概率来确定选择哪个字符。正如您所见，该算法有些复杂：它涉及到三个低级
    TensorFlow.js 操作的调用：`tf.div()`、`tf.log()` 和 `tf.multinomial()`。为什么我们使用这种复杂的算法而不是简单地选择具有最高概率得分的选项，这将需要一个单独的
    `argMax()` 调用呢？'
- en: If we did that, the output of the text-generation process would be *deterministic*.
    That is, it would give you exactly the same output if you ran it multiple times.
    The deep neural networks we’ve seen so far are all deterministic, in the sense
    that given an input tensor, the output tensor is completely determined by the
    network’s topology and the values of its weights. If so desired, you can write
    a unit test to assert its output value (see [chapter 12](kindle_split_025.html#ch12)
    for a discussion of testing machine-learning algorithms). This determinism is
    *not* ideal for our text-generation task. After all, writing is a creative process.
    It is much more interesting to have some randomness in the generated text, even
    when the same seed text is given. This is what the `tf.multinomial()` operation
    and the temperature parameter are useful for. `tf.multinomial()` is the source
    of randomness, while temperature controls the degree of randomness.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做，文本生成过程的输出将是*确定性*的。也就是说，如果你多次运行它，它将给出完全相同的输出。到目前为止，我们所见到的深度神经网络都是确定性的，也就是说，给定一个输入张量，输出张量完全由网络的拓扑结构和其权重值决定。如果需要的话，你可以编写一个单元测试来断言其输出值（见[第12章](kindle_split_025.html#ch12)讨论机器学习算法的测试）。对于我们的文本生成任务来说，这种确定性*并不*理想。毕竟，写作是一个创造性的过程。即使给出相同的种子文本，生成的文本也更有趣些带有一些随机性。这就是`tf.multinomial()`操作和温度参数有用的地方。`tf.multinomial()`是随机性的来源，而温度控制着随机性的程度。
- en: Listing 10.2\. The stochastic sampling function, with a temperature parameter
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2。带有温度参数的随机抽样函数
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1*** The dense layer of the model outputs normalized probability scores;
    we use log() to convert them to unnormalized logits before dividing them by the
    temperature.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 模型的密集层输出归一化的概率分数；我们使用log()将它们转换为未归一化的logits，然后再除以温度。'
- en: '***2*** We protect against division-by-zero errors with a small positive number.
    The result of the division is logits with adjusted uncertainty.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 我们用一个小的正数来防止除以零的错误。除法的结果是调整了不确定性的logits。'
- en: '***3*** tf.multinomial() is a stochastic sampling function. It’s like a multisided
    die with unequal per-side probabilities as determined by logPreds—the temperature-scaled
    logits.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** `tf.multinomial()`是一个随机抽样函数。它就像一个多面的骰子，每个面的概率不相等，由logPreds——经过温度缩放的logits来确定。'
- en: 'The most important part of the `sample()` function in [listing 10.2](#ch10ex02)
    is the following line:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.2](#ch10ex02)的`sample()`函数中最重要的部分是以下行：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It takes the `probs` (the probability outputs from the model) and converts
    them into `logPreds`, the logarithms of the probabilities scaled by a factor.
    What do the logarithm operation (`tf.log()`) and the scaling (`tf.div()`) do?
    We’ll explain that through an example. For the sake of simplicity, let’s assume
    there are only three choices (three characters in our character set). Suppose
    our next-character predictor yields the following three probability scores given
    a certain input sequence:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 它获取了`probs`（模型的概率输出）并将它们转换为`logPreds`，概率的对数乘以一个因子。对数运算（`tf.log()`）和缩放（`tf.div()`）做了什么？我们将通过一个例子来解释。为了简单起见，假设只有三个选择（字符集中的三个字符）。假设我们的下一个字符预测器在给定某个输入序列时产生了以下三个概率分数：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s see how two different temperature values alter these probabilities. First,
    let’s look at a relatively lower temperature: 0.25\. The scaled logits are'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看两个不同的温度值如何改变这些概率。首先，让我们看一个相对较低的温度：0.25。缩放后的logits是
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To understand what the logits mean, we convert them back to actual probability
    scores by using the softmax equation, which involves taking the exponential of
    the logits and normalizing them:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解logits的含义，我们通过使用softmax方程将它们转换回实际的概率分数，这涉及将logits的指数和归一化：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, our logits from temperature = 0.25 correspond to a highly concentrated
    probability distribution in which the second choice has a much higher probability
    compared to the other two choices (see the second panel in [figure 10.3](#ch10fig03)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，当温度为0.25时，我们的logits对应一个高度集中的概率分布，在这个分布中，第二个选择的概率远高于其他两个选择（见[图10.3](#ch10fig03)的第二面板）。
- en: Figure 10.3\. The probability scores after scaling by different values of temperature
    (T). A lower value of T leads to a more concentrated (less stochastic) distribution;
    a higher value of T causes the distribution to be more equal among the classes
    (more stochastic). A T-value of 1 corresponds to the original probabilities (no
    change). Note that the relative ranking of the three choices is always preserved
    regardless of the value of T.
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.3\. 不同温度（T）值缩放后的概率得分。较低的 T 值导致分布更集中（更少随机）；较高的 T 值导致分布在类别之间更均等（更多随机）。T 值为
    1 对应于原始概率（无变化）。请注意，无论 T 的值如何，三个选择的相对排名始终保持不变。
- en: '![](10fig02_alt.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig02_alt.jpg)'
- en: What if we use a higher temperature, say 0.75? By repeating the same calculation,
    we get
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用更高的温度，比如说 0.75，通过重复相同的计算，我们得到
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is a much less “peaked” distribution compared to the one from before, when
    the temperature was 0.25 (see the fourth panel in [figure 10.3](#ch10fig03)).
    But it is still more peaked compared to the original distribution. As you might
    have realized, a temperature of 1 will give you exactly the original probabilities
    ([figure 10.3](#ch10fig03), fifth panel). A temperature higher than 1 leads to
    a more “equalized” probability distribution among the choices ([figure 10.3](#ch10fig03),
    sixth panel), while the ranking among the choices always remains the same.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的情况相比，这是一个峰值较低的分布，当温度为 0.25 时（请参阅[图 10.3](#ch10fig03)的第四面板）。但是与原始分布相比，它仍然更尖峭。你可能已经意识到，温度为
    1 时，你将得到与原始概率完全相同的结果（[图 10.3](#ch10fig03)，第五面板）。大于 1 的温度值会导致选择之间的概率分布更“均等”（[图
    10.3](#ch10fig03)，第六面板），而选择之间的排名始终保持不变。
- en: These converted probabilities (or rather, the logarithms of them) are then fed
    to the `tf.multinomial()` function, which acts like a multifaced die, with unequal
    probabilities of the faces controlled by the input argument. This gives us the
    final choice of the next character.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换后的概率（或者说它们的对数）然后被馈送到 `tf.multinomial()` 函数中，该函数的作用类似于一个多面骰子，其面的不等概率由输入参数控制。这给我们了下一个字符的最终选择。
- en: So, this is how the temperature parameter controls the randomness of the generated
    text. The term *temperature* has its origin in thermodynamics, from which we know
    that a system with a higher temperature has a higher degree of chaos inside it.
    The analogy is appropriate here because when we increase the temperature value
    in our code, we get more chaotic-looking text. There is a “sweet medium” for the
    temperature value. Below it, the generated text looks too repetitive and mechanical;
    above it, the text looks too unpredictable and wacky.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是温度参数如何控制生成文本的随机性。术语 *temperature* 源自热力学，我们知道，温度较高的系统内部混乱程度较高。这个类比在这里是合适的，因为当我们在代码中增加温度值时，生成的文本看起来更加混乱。温度值有一个“甜蜜的中间值”。在此之下，生成的文本看起来太重复和机械化；在此之上，文本看起来太不可预测和古怪。
- en: This concludes our tour of the text-generating LSTM. Note that this methodology
    is very general and is applicable to many other sequences with proper modifications.
    For instance, if trained on a sufficiently large dataset of musical scores, an
    LSTM can be used to compose music by iteratively predicting the next musical note
    from the ones that come before it.^([[9](#ch10fn9)])
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对文本生成 LSTM 的介绍。请注意，这种方法非常通用，可以应用于许多其他序列，只需进行适当的修改即可。例如，如果在足够大的音乐分数数据集上进行训练，LSTM
    可以通过逐步从之前的音符中预测下一个音符来作曲。^([[9](#ch10fn9)])
- en: ⁹
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Allen Huang and Raymond Wu, “Deep Learning for Music,” submitted 15 June 2016,
    [https://arxiv.org/abs/1606.04930](https://arxiv.org/abs/1606.04930).
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Allen Huang 和 Raymond Wu，“Deep Learning for Music”，2016 年 6 月 15 日提交，[https://arxiv.org/abs/1606.04930](https://arxiv.org/abs/1606.04930)。
- en: '10.2\. Variational autoencoders: Finding an efficient and structured vec- ctor
    representation of images'
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2\. 变分自动编码器：找到图像的高效和结构化的向量表示
- en: 'The previous section gave you a quick tour of how deep learning can be used
    to generate sequential data such as text. In the remaining parts of this chapter,
    we will look at how to build neural networks to generate images. We will examine
    two types of models: variational autoencoder (VAE) and generative adversarial
    network (GAN). Compared to a GAN, the VAE has a longer history and is structurally
    simpler. So, it forms a good on-ramp for you to get into the fast-moving world
    of deep-learning-based image generation.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的部分为您介绍了如何使用深度学习来生成文本等连续数据。在本章的剩余部分，我们将讨论如何构建神经网络来生成图像。我们将研究两种类型的模型：变分自编码器（VAE）和生成对抗网络（GAN）。与GAN相比，VAE的历史更悠久，结构更简单。因此，它为您进入基于深度学习的图像生成的快速领域提供了很好的入口。
- en: '10.2.1\. Classical autoencoder and VAE: Basic ideas'
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '10.2.1\. 传统自编码器和VAE: 基本概念'
- en: '[Figure 10.4](#ch10fig04) shows the overall architecture of an autoencoder
    schematically. At first glance, an autoencoder is a funny model because its input
    and output models are images of the same size. At the most basic level, the loss
    function of an autoencoder is the MSE between the input and output. This means
    that, if trained properly, an autoencoder will take an image and output an essentially
    identical image. What on earth would a model like that be useful for?'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.4](#ch10fig04)以示意方式显示了自编码器的整体架构。乍一看，自编码器是一个有趣的模型，因为它的输入和输出模型的图像大小是相同的。在最基本的层面上，自编码器的损失函数是输入和输出之间的均方误差（MSE）。这意味着，如果经过适当训练，自编码器将接受一个图像，并输出一个几乎相同的图像。这种模型到底有什么用呢？'
- en: Figure 10.4\. The architecture of a classical autoencoder
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.4\. 传统自编码器的架构
- en: '![](10fig03_alt.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig03_alt.jpg)'
- en: 'In fact, autoencoders are an important type of generative model and are far
    from useless. The answer to the prior question lies in the hourglass-shaped architecture
    ([figure 10.4](#ch10fig04)). The thinnest, middle part of an autoencoder is a
    vector with a much smaller number of elements compared to the input and output
    images. Hence, the image-to-image transformation performed by an autoencoder is
    nontrivial: it first turns the input image into a highly compressed representation
    and then reconstructs the image from that representation without using any additional
    information. The efficient representation at the middle is referred to as the
    *latent vector*, or the *z-vector*. We will use these two terms interchangeably.
    The vector space in which these vectors reside is called the *latent space*, or
    the *z-space*. The part of the autoencoder that converts the input image to the
    latent vector can be called the *encoder*; the later part that converts the latent
    vector back to an image is called the *decoder*.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，自编码器是一种重要的生成模型，而且绝不是无用的。对于前面的问题答案在于小时钟形状的架构（[图10.4](#ch10fig04)）。自编码器的最细部分是一个与输入和输出图像相比具有更少元素的向量。因此，由自编码器执行的图像转换是非平凡的：它首先将输入图像转变为高压缩形式的表示，然后在不使用任何额外信息的情况下从该表示中重新构建图像。中间的有效表示称为*潜在向量*，或者*z-向量*。我们将这两个术语互换使用。这些向量所在的向量空间称为*潜在空间*，或者*z-空间*。将输入图像转换为潜在向量的自编码器部分称为*编码器*；将潜在向量转换回图像的后面部分称为*解码器*。
- en: The latent vector can be hundreds of times smaller compared to the image itself,
    as we’ll show through a concrete example shortly. Therefore, the encoder portion
    of a trained autoencoder is a remarkably efficient dimensionality reducer. Its
    summarization of the input image is highly succinct yet contains enough essential
    information to allow the decoder to reproduce the input image faithfully without
    using any extra bits of information. The fact that the decoder can do that is
    also remarkable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 和图像本身相比，潜在向量可以小几百倍，我们很快会通过一个具体的例子进行展示。因此，经过训练的自编码器的编码器部分是一个非常高效的维度约简器。它对输入图像的总结非常简洁，但包含足够重要的信息，以使得解码器可以忠实地复制输入图像，而不需要使用额外的信息。解码器能够做到这一点，这也是非常了不起的。
- en: We can also look at an autoencoder from an information-theory point of view.
    Let’s say the input and output images each contain *N* bits of information. Naively,
    *N* is the number of pixels multiplied by the bit depth of each pixel. By contrast,
    the latent vector in the middle of the autoencoder can hold only a very small
    amount of information because of its small size (say, *m* bits). If *m* were smaller
    than *N*, it would be theoretically impossible to reconstruct the image from the
    latent vector. However, pixels in images are not completely random (an image made
    of completely random pixels looks like static noise). Instead, the pixels follow
    certain patterns, such as color continuity and characteristics of the type of
    real-world objects being depicted. This causes the value of *N* to be much smaller
    than the naive calculation based on the number and depth of the pixels. It is
    the autoencoder’s job to learn this pattern; this is also the reason why autoencoders
    can work.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从信息理论的角度来看待自编码器。假设输入和输出图像各包含*N*比特的信息。从表面上看，*N*是每个像素的位深度乘以像素数量。相比之下，自编码器中间的潜在向量由于其小的大小（假设为*m*比特），只能保存极少量的信息。如果*m*小于*N*，那么从潜在向量重构出图像就在理论上不可能。然而，图像中的像素不是完全随机的（完全由随机像素组成的图像看起来像静态噪音）。相反，像素遵循某些模式，比如颜色连续性和所描绘的现实世界对象的特征。这导致*N*的值比基于像素数量和深度的表面计算要小得多。自编码器的任务是学习这种模式；这也是自编码器能够工作的原因。
- en: After an autoencoder is trained, its decoder part can be used without the encoder.
    Given any latent vector, it can generate an image that conforms to the patterns
    and styles of the training images. This fits the description of a generative model
    nicely. Furthermore, the latent space will hopefully contain some nice, interpretable
    structure. In particular, each dimension of the latent space may be associated
    with a meaningful aspect of the image. For instance, suppose we’ve trained an
    autoencoder on images of human faces; perhaps one of the latent space’s dimensions
    will be associated with the degree of smiling. When you fix the values in all
    other dimensions of a latent vector and vary only the value on the “smile dimension,”
    the images produced by the decoder will be exactly the same face but with varying
    degrees of smiling (see, for example, [figure 10.5](#ch10fig05)). This will enable
    interesting applications, such as changing the degree of smiling of an input face
    image while leaving all other aspects unchanged. This can be done through the
    following steps. First, obtain the latent vector of the input by applying the
    encoder. Then, modify only the “smile dimension” of the vector; finally, run the
    modified latent vector through the decoder.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器训练完成后，其解码器部分可以单独使用，给定任何潜在向量，它都可以生成符合训练图像的模式和风格的图像。这很好地符合了生成模型的描述。此外，潜在空间将有望包含一些良好的可解释结构。具体而言，潜在空间的每个维度可能与图像的某个有意义的方面相关联。例如，假设我们在人脸图像上训练了一个自编码器，也许潜在空间的某个维度将与微笑程度相关。当你固定潜在向量中所有其他维度的值，仅变化“微笑维度”的值时，解码器产生的图像将是同一张脸，但微笑程度不同（例如，参见[图10.5](#ch10fig05)）。这将使得有趣的应用成为可能，例如在保持所有其他方面不变的情况下，改变输入人脸图像的微笑程度。可以通过以下步骤来完成此操作。首先，通过应用编码器获取输入的潜在向量。然后，仅修改向量的“微笑维度”即可；最后，通过解码器运行修改后的潜在向量。
- en: Figure 10.5\. The “smile dimension.” An example of desired structure in latent
    spaces learned by autoencoders.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5. “微笑维度”。自编码器所学习的潜在空间中期望的结构的示例。
- en: '![](10fig04_alt.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig04_alt.jpg)'
- en: Unfortunately, *classical autoencoders* of the architecture shown in [figure
    10.4](#ch10fig04) don’t lead to particularly useful or nicely structured latent
    spaces. They are not very good at compression, either. For these reasons, they
    largely fell out of fashion by 2013\. VAEs—discovered almost simultaneously by
    Diederik Kingma and Max Welling in December 2013^([[10](#ch10fn10)]) and Danilo
    Rezende, Shakir Mohamed, and Daan Wiestra in January 2014^([[11](#ch10fn11)])—augment
    autoencoders with a little bit of statistical magic, which forces the models to
    learn continuous and highly structured latent spaces. VAEs have turned out to
    be a powerful type of generative image model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，[图10.4](#ch10fig04)中所示的 *经典自编码器* 并不能产生特别有用和良好结构的潜变量空间。它们在压缩方面也不太出色。因此，到2013年，它们在很大程度上已经不再流行了。VAE（Variational
    Autoencoder）则在2013年12月由Diederik Kingma和Max Welling几乎同时发现^([[10](#ch10fn10)])，而在2014年1月由Danilo
    Rezende、Shakir Mohamed和Daan Wiestra发现^([[11](#ch10fn11)])，通过一点统计魔法增加了自编码器的能力，强制模型学习连续且高度结构化的潜变量空间。VAE已经证明是一种强大的生成式图像模型。
- en: ^(10)
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes,” submitted
    20 Dec. 2013, [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Diederik P. Kingma和Max Welling，“Auto-Encoding Variational Bayes”，2013年12月20日提交，[https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)。
- en: ^(11)
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra, “Stochastic Backpropagation
    and Approximate Inference in Deep Generative Models,” submitted 16 Jan. 2014,
    [https://arxiv.org/abs/1401.4082](https://arxiv.org/abs/1401.4082).
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Danilo Jimenez Rezende，Shakir Mohamed和Daan Wierstra，“Stochastic Backpropagation
    and Approximate Inference in Deep Generative Models”，2014年1月16日提交，[https://arxiv.org/abs/1401.4082](https://arxiv.org/abs/1401.4082)。
- en: 'A VAE, instead of compressing its input image into a fixed vector in the latent
    space, turns the image into the parameters of a statistical distribution—specifically,
    those of a *Gaussian distribution*. As you may recall from high school math, a
    Gaussian distribution has two parameters: the mean and the variance (or, equivalently,
    the standard deviation). A VAE maps every input image into a mean. The only additional
    complexity is that the mean and the variance can be higher than one-dimensional
    if the latent space is more than 1D, as we’ll see in the following example. Essentially,
    we are assuming that the images are generated via a stochastic process and that
    the randomness of this process should be taken into account during encoding and
    decoding. The VAE then uses the mean and variance parameters to randomly sample
    one vector from the distribution and decode that element back to the size of the
    original input (see [figure 10.6](#ch10fig06)). This stochasticity is one of the
    key ways in which VAE improves robustness and forces the latent space to encode
    meaningful representations everywhere: every point sampled in the latent space
    should be a valid image output when decoded by the decoder.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: VAE不是将输入图像压缩为潜变量空间中的固定向量，而是将图像转化为统计分布的参数——具体来说是**高斯分布**的参数。高斯分布有两个参数：均值和方差（或者等效地，标准差）。VAE将每个输入图像映射到一个均值上。唯一的额外复杂性在于，如果潜变量空间超过1D，则均值和方差可以是高于一维的，正如我们将在下面的例子中看到的那样。本质上，我们假设图像是通过随机过程生成的，并且在编码和解码过程中应该考虑到这个过程的随机性。然后，VAE使用均值和方差参数从分布中随机采样一个向量，并使用该随机向量将其解码回原始输入的大小（参见[图10.6](#ch10fig06)）。这种随机性是VAE改善鲁棒性、强迫潜变量空间在每个位置都编码有意义表示的关键方式之一：在解码器解码时，潜变量空间中采样的每个点应该是一个有效的图像输出。
- en: Figure 10.6\. Comparing how a classical autoencoder (panel A) and a VAE (panel
    B) work. A classical autoencoder maps an input image to a fixed latent vector
    and performs decoding using that vector. By contrast, a VAE maps an input image
    to a distribution, described by a mean and a variance, draws a random latent vector
    from this distribution, and generates the decoded image using that random vector.
    The T-shirt image is an example from the Fashion-MNIST dataset.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6\. 比较经典自编码器（面板A）和VAE（面板B）的工作原理。经典自编码器将输入图像映射到一个固定的潜变量向量上，并使用该向量进行解码。相比之下，VAE将输入图像映射到一个由均值和方差描述的分布上，从该分布中随机采样一个潜变量向量，并使用该随机向量生成解码后的图像。这个T恤图案是来自Fashion-MNIST数据集的一个例子。
- en: '![](10fig05_alt.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig05_alt.jpg)'
- en: Next, we will show you a VAE in action by using the Fashion-MNIST dataset. As
    its name indicates, Fashion-MNIST^([[12](#ch10fn12)]) is inspired by the MNIST
    hand-written digit dataset, but contains images of clothing and fashion items.
    Like the MNIST images, the Fashion-MNIST images are 28 × 28 grayscale images.
    There are exactly 10 classes of clothing and fashion items (such as T-shirt, pullover,
    shoe, and bag; see [figure 10.6](#ch10fig06) for an example). However, the Fashion-MNIST
    dataset is slightly “harder” for machine-learning algorithms compared to the MNIST
    dataset, with the current state-of-the-art test-set accuracy standing at approximately
    96.5%, much lower compared to the 99.75% state-of-the-art accuracy on the MNIST
    dataset.^([[13](#ch10fn13)]) We will use TensorFlow.js to build a VAE and train
    it on the Fashion-MNIST dataset. We’ll then use the decoder of the VAE to sample
    from the 2D latent space and observe the structure inside that space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过使用 Fashion-MNIST 数据集展示 VAE 的工作原理。正如其名称所示，Fashion-MNIST^([[12](#ch10fn12)])
    受到了 MNIST 手写数字数据集的启发，但包含了服装和时尚物品的图像。与 MNIST 图像一样，Fashion-MNIST 图像是 28 × 28 的灰度图像。有着确切的10个服装和时尚物品类别（如
    T 恤、套头衫、鞋子和包袋；请参见 [图 10.6](#ch10fig06) 作为示例）。然而，与 MNIST 数据集相比，Fashion-MNIST 数据集对机器学习算法来说略微“更难”，当前最先进的测试集准确率约为
    96.5%，远低于 MNIST 数据集的 99.75% 最先进准确率。^([[13](#ch10fn13)]) 我们将使用 TensorFlow.js 构建一个
    VAE 并在 Fashion-MNIST 数据集上对其进行训练。然后，我们将使用 VAE 的解码器从2D潜在空间中对样本进行采样，并观察该空间内部的结构。
- en: ^(12)
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Han Xiao, Kashif Rasul, and Roland Vollgraf, “Fashion-MNIST: A Novel Image
    Dataset for Benchmarking Machine Learning Algorithms,” submitted 25 Aug. 2017,
    [https://arxiv.org/abs/1708.07747](https://arxiv.org/abs/1708.07747).'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Han Xiao、Kashif Rasul 和 Roland Vollgraf，“Fashion-MNIST: 用于机器学习算法基准测试的新型图像数据集”，提交于2017年8月25日，[https://arxiv.org/abs/1708.07747](https://arxiv.org/abs/1708.07747)。'
- en: ^(13)
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(13)
- en: ''
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Source: “State-of-the-Art Result for All Machine Learning Problems,” GitHub,
    2019, [http://mng.bz/6w0o](http://mng.bz/6w0o).'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：“所有机器学习问题的最新技术结果”，GitHub，2019年，[http://mng.bz/6w0o](http://mng.bz/6w0o)。
- en: '10.2.2\. A detailed example of VAE: The Fashion-MNIST example'
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.2\. VAE 的详细示例：Fashion-MNIST 示例
- en: 'To check out the fashion-mnist-vae example, use the following commands:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看 fashion-mnist-vae 示例，请使用以下命令：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This example consists of two parts: training the VAE in Node.js and using the
    VAE decoder to generate images in the browser. To start the training part, use'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子由两部分组成：在 Node.js 中训练 VAE 和使用 VAE 解码器在浏览器中生成图像。要开始训练部分，请使用以下命令
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you have a CUDA-enabled GPU set up properly, you can use the `--gpu` flag
    to get a boost in the training speed:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正确设置了 CUDA 启用的 GPU，则可以使用 `--gpu` 标志来加速训练：
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The training should take about five minutes on a reasonably update-to-date
    desktop equipped with a CUDA GPU, and under an hour without the GPU. Once the
    training is complete, use the following command to build and launch the browser
    frontend:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在配备有 CUDA GPU 的合理更新的台式机上大约需要五分钟，没有 GPU 的情况下则需要不到一个小时。训练完成后，使用以下命令构建并启动浏览器前端：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The frontend will load the VAE’s decoder, generate a number of images by using
    a 2D grid of regularly spaced latent vectors, and display the images on the page.
    This will give you an appreciation of the structure of the latent space.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 前端将加载 VAE 的解码器，通过使用正则化的 2D 网格的潜在向量生成多个图像，并在页面上显示这些图像。这将让您欣赏到潜在空间的结构。
- en: 'In technical terms, here is a how a VAE works:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，这就是 VAE 的工作原理：
- en: 'The encoder turns the input samples into two parameters in a latent space:
    `zMean` and `zLogVar`, the mean and the logarithm of the variance (log variance),
    respectively. Each of the two vectors has the same length as the dimensionality
    of the latent space.^([[14](#ch10fn14)]) For example, our latent space will be
    2D, so `zMean` and `zLogVar` will each be a length-2 vector. Why do we use log
    variance (`zLogVar`) instead of the variance itself? Because variances are by
    definition required to be nonnegative, but there is no easy way to enforce that
    sign requirement on a layer’s output. By contrast, log variance is allowed to
    have any sign. By using the logarithm, we don’t have to worry about the sign of
    the layers’ outputs. Log variance can be easily converted to the corresponding
    variance through a simple exponentiation (`tf.exp()`) operation.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器将输入样本转换为潜在空间中的两个参数：`zMean` 和 `zLogVar`，分别是均值和方差的对数（对数方差）。这两个向量的长度与潜在空间的维度相同。例如，我们的潜在空间将是
    2D，因此 `zMean` 和 `zLogVar` 将分别是长度为 2 的向量。为什么我们使用对数方差（`zLogVar`）而不是方差本身？因为方差必须是非负的，但没有简单的方法来强制该层输出的符号要求。相比之下，对数方差允许具有任何符号。通过使用对数，我们不必担心层输出的符号。对数方差可以通过简单的指数运算（`tf.exp()`）操作轻松地转换为相应的方差。
- en: ^(14)
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(14)
- en: ''
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Strictly speaking, the covariance matrix of the length-*N* latent vector is
    an *N* × *N* matrix. However, `zLogVar` is a length-*N* vector because we constrain
    the covariance matrix to be diagonal—that is, there is no correlation between
    two different elements of the latent vector.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 严格来说，长度为 *N* 的潜在向量的协方差矩阵是一个 *N* × *N* 矩阵。然而，`zLogVar` 是一个长度为 *N* 的向量，因为我们将协方差矩阵约束为对角线矩阵——即，潜在向量的两个不同元素之间没有相关性。
- en: The VAE algorithm randomly samples a latent vector from the latent normal distribution
    by using a vector called `epsilon`—a random vector of the same length as `zMean`
    and `zLogVar`. In simple math equations, this step, which is referred to as *reparameterization*
    in the literature, looks like
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VAE 算法通过使用一个称为 `epsilon` 的向量——与 `zMean` 和 `zLogVar` 的长度相同的随机向量——从潜在正态分布中随机抽样一个潜在向量。在简单的数学方程中，这一步骤在文献中被称为*重参数化*，看起来像是
- en: '[PRE14]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The multiplication by 0.5 converts the variance to the standard deviation, which
    is based on the fact that the standard deviation is the square root of the variance.
    The equivalent JavaScript code is
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 乘以 0.5 将方差转换为标准差，这基于标准差是方差的平方根的事实。等效的 JavaScript 代码是
- en: '[PRE15]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: (See [listing 10.3](#ch10ex03).) Then, `z` will be fed to the decoder portion
    of the VAE so that an output image can be generated.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (见 [listing 10.3](#ch10ex03)。) 然后，`z` 将被馈送到 VAE 的解码器部分，以便生成输出图像。
- en: In our implementation of VAE, the latent-vector-sampling step is performed by
    a custom layer called `ZLayer` ([listing 10.3](#ch10ex03)). We briefly saw a custom
    TensorFlow.js layer in [chapter 9](kindle_split_021.html#ch09) (the `GetLastTimestepLayer`
    layer that we used in the attention-based date converter). The custom layer used
    by our VAE is slightly more complex and deserves some explanation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 VAE 实现中，潜在向量抽样步骤是由一个名为 `ZLayer` 的自定义层执行的（见 [listing 10.3](#ch10ex03)）。我们在
    [第 9 章](kindle_split_021.html#ch09) 中简要介绍了一个自定义 TensorFlow.js 层（我们在基于注意力的日期转换器中使用的
    `GetLastTimestepLayer` 层）。我们 VAE 使用的自定义层略微复杂，值得解释一下。
- en: 'The `ZLayer` class has two key methods: `computeOutputShape()` and `call()`.
    `computeOutputShape()` are used by TensorFlow.js to infer the output shape of
    the `Layer` instance given the shape(s) of the input. The `call()` method contains
    the actual math. It contains the equation line introduced previously. The following
    code is excerpted from fashion-mnist-vae/model.js.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`ZLayer` 类有两个关键方法：`computeOutputShape()` 和 `call()`。`computeOutputShape()`
    被 TensorFlow.js 用来推断给定输入形状的 `Layer` 实例的输出形状。`call()` 方法包含了实际的数学计算。它包含了先前介绍的方程行。下面的代码摘自
    fashion-mnist-vae/model.js。'
- en: Listing 10.3\. Sampling from the latent space (z-space) with a custom layer
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[listing 10.3](#ch10ex03) 抽样潜在空间（z 空间）的代码示例'
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1*** Checks to make sure that we have exactly two inputs: zMean and zLogVar'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 检查确保我们只有两个输入：zMean 和 zLogVar'
- en: '***2*** The shape of the output (z) will be the same as the shape of zMean.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 输出（z）的形状将与 zMean 的形状相同。'
- en: '***3*** Gets a random batch of epsilon from the unit Gaussian distribution'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 从单位高斯分布中获取一个随机批次的 epsilon'
- en: '***4*** This is where the sampling of z-vectors happens: zMean + standardDeviation
    * epsilon.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 这是 z 向量抽样发生的地方：zMean + standardDeviation * epsilon。'
- en: '***5*** The static className property is set in case the layer is to be serialized.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 如果要对该层进行序列化，则设置静态的className属性。'
- en: '***6*** Registers the class to support deserialization'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 注册类以支持反序列化'
- en: 'As [listing 10.4](#ch10ex04) shows, `ZLayer` is instantiated and gets used
    as a part of the encoder. The encoder is written as a functional model, instead
    of the simpler sequential model, because it has a nonlinear internal structure
    and produces three outputs: `zMean`, `zLogVar`, and `z` (see the schematic in
    [figure 10.7](#ch10fig07)). The encoder outputs `z` because it will get used by
    the decoder, but why does the encoder include `zMean` and `zLogVar` in the outputs?
    It’s because they will be used to calculate the loss function of the VAE, as you
    will see shortly.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如[清单10.4](#ch10ex04)所示，`ZLayer`被实例化并被用作编码器的一部分。编码器被编写为一个功能型模型，而不是更简单的顺序模型，因为它具有非线性的内部结构，并且产生三个输出：`zMean`、`zLogVar`和`z`（参见[图10.7](#ch10fig07)中的示意图）。编码器输出`z`是因为它将被解码器使用，但为什么编码器包括`zMean`和`zLogVar`在输出中？这是因为它们将用于计算VAE的损失函数，很快你就会看到。
- en: Figure 10.7\. Schematic illustration of the TensorFlow.js implementation of
    VAE, including the internal details of the encoder and decoder parts and the custom
    loss function and optimizer that support VAE training.
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7。TensorFlow.js实现VAE的示意图，包括编码器和解码器部分的内部细节以及支持VAE训练的自定义损失函数和优化器。
- en: '![](10fig06_alt.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig06_alt.jpg)'
- en: In addition to `ZLayer`, the encoder consists of two one-hidden-layer MLPs.
    They are used to convert the flattened input Fashion-MNIST images into the `zMean`
    and `zLogVar` vectors, respectively. The two MLPs share the same hidden layer
    but use separate output layers. This branching model topology is also made possible
    by the fact that the encoder is a functional model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`ZLayer`，编码器还包括两个单隐藏层的MLP。它们用于将扁平化的输入Fashion-MNIST图像转换为`zMean`和`zLogVar`向量，分别。这两个MLP共享相同的隐藏层，但使用单独的输出层。这种分支模型拓扑结构也是由于编码器是一个功能型模型。
- en: Listing 10.4\. The encoder part of our VAE (excerpt from fashion-mnist-vae/model.js)
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单10.4。我们VAE的编码器部分（摘自fashion-mnist-vae/model.js）
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '***1*** At the base of the encoder is a simple MLP with one hidden layer.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 编码器底部是一个简单的MLP，有一个隐藏层。'
- en: '***2*** Unlike a normal MLP, we put two layers downstream from the hidden dense
    layer to predict zMean and zLogVar, respectively. This is also the reason why
    we use a functional model instead of the simpler sequential model type.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 与普通的MLP不同，我们在隐藏的密集层之后放置了两个层，分别用于预测zMean和zLogVar。这也是我们使用功能型模型而不是更简单的顺序模型类型的原因。'
- en: '***3*** Instantiates our custom ZLayer and uses it to draw random samples that
    follow the distribution specified by zMean and zLogVar'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 实例化我们自定义的ZLayer，并使用它来生成遵循由zMean和zLogVar指定的分布的随机样本'
- en: The code in [listing 10.5](#ch10ex05) builds the decoder. Compared to the encoder,
    the decoder has a simpler topology. It uses an MLP to convert the input z-vector
    (that is, the latent vector) into an image of the same shape as the encoder’s
    input. Note that the way in which our VAE handles images is somewhat simplistic
    and unusual in that it flattens the images into 1D vectors and hence discards
    the spatial information. Image-oriented VAEs typically use convolutional and pooling
    layers, but due to the simplicity of our images (their small size and the fact
    that there is only one color channel), the flattening approach works well enough
    for the purpose of this example.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单10.5](#ch10ex05)中的代码构建了解码器。与编码器相比，解码器的拓扑结构更简单。它使用一个MLP将输入的z向量（即潜在向量）转换为与编码器输入相同形状的图像。请注意，我们的VAE处理图像的方式有些简单和不寻常，因为它将图像扁平化为1D向量，因此丢弃了空间信息。面向图像的VAE通常使用卷积和池化层，但由于我们图像的简单性（其尺寸较小且仅有一个颜色通道），扁平化方法足够简单地处理此示例的目的。'
- en: Listing 10.5\. The decoder part of our VAE (excerpt from fashion-mnist-vae/model.js)
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单10.5。我们VAE的解码器部分（摘自fashion-mnist-vae/model.js）
- en: '[PRE18]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1*** The decoder is a simple MLP that converts a latent (z) vector into
    a (flattened) image.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 解码器是一个简单的MLP，将潜在（z）向量转换为（扁平化的）图像。'
- en: '***2*** Sigmoid activation is a good choice for the output layer because it
    makes sure that the pixel values of the output image are bounded between 0 and
    1.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** Sigmoid激活是输出层的一个好选择，因为它确保输出图像的像素值被限制在0和1之间。'
- en: 'To combine the encoder and decoder into a single `tf.LayerModel` object that
    is the VAE, the code in [listing 10.6](#ch10ex06) extracts the third output (z-vector)
    of the encoder and runs it through the decoder. Then the combined model exposes
    the decoded image as its output, along with three additional outputs: the `zMean`,
    `zLogVar`, and z-vectors. This completes the definition of the VAE model’s topology.
    In order to train the model, we need two more things: the loss function and an
    optimizer. The code in the following listing was excerpted from fashion-mnist-vae/model.js.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 将编码器和解码器合并成一个名为VAE的单个`tf.LayerModel`对象时，[列表10.6](#ch10ex06)中的代码会提取编码器的第三个输出（z向量）并将其通过解码器运行。然后，组合模型会将解码图像暴露为其输出，同时还有其他三个输出：`zMean`、`zLogVar`和z向量。这完成了VAE模型拓扑结构的定义。为了训练模型，我们需要两个东西：损失函数和优化器。以下列表中的代码摘自fashion-mnist-vae/model.js。
- en: Listing 10.6\. Putting the encoder and decoder together into the VAE
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将编码器和解码器放在一起组成VAE时，列表10.6中完成。
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1*** The input to the VAE is the same as the input to the encoder: the original
    input image.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** VAE的输入与编码器的输入相同：原始输入图像。'
- en: '***2*** Of all three outputs of the encoder, only the last one (z) goes into
    the decoder.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 在编码器的所有三个输出中，只有最后一个（z）进入解码器。'
- en: '***3*** We use the functional model API due to the model’s nonlinear topology.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 由于模型的非线性拓扑结构，我们使用功能模型API。'
- en: '***4*** The output of the VAE model object includes the decoded image in addition
    to zMean, zLogVar, and *z*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** VAE模型对象的输出除了zMean、zLogVar和*z*之外还包括解码图像。'
- en: 'When we were visiting the simple-object-detection model in [chapter 5](kindle_split_016.html#ch05),
    we described the way in which custom loss functions can be defined in TensorFlow.js.
    Here, a custom loss function is needed to train the VAE. This is because the loss
    function will be the sum of two terms: one that quantifies the discrepancy between
    the input and output and one that quantifies the statistical properties of the
    latent space. This is reminiscent of the simple-object-detection model’s custom
    loss function, which was a sum of a term for object classification and another
    for object localization.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们访问[第5章](kindle_split_016.html#ch05)中的simple-object-detection模型时，我们描述了如何在TensorFlow.js中定义自定义损失函数的方式。在这里，需要自定义损失函数来训练VAE。这是因为损失函数将是两个项的总和：一个量化输入和输出之间的差异，另一个量化潜在空间的统计属性。这让人想起了simple-object-detection模型的自定义损失函数，其中一个项用于对象分类，另一个项用于对象定位。
- en: As you can see from the code in [listing 10.7](#ch10ex07) (excerpted from fashion-mnist-vae/
    model.js), defining the input-output discrepancy term is straightforward. We simply
    calculate the MSE between the original input and the decoder’s output. However,
    the statistical term, called the *Kullbach-Liebler* (KL) divergence, is more mathematically
    involved. We will spare you the detailed math,^([[15](#ch10fn15)]) but on an intuitive
    level, the KL divergence term (`klLoss` in the code) encourages the distributions
    for different input images to be more evenly distributed around the center of
    the latent space, which makes it easier for the decoder to interpolate between
    the images. Therefore, the `klLoss` term can be thought of as a regularization
    term added on top of the main input-output discrepancy term of the VAE.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从[列表10.7](#ch10ex07)中的代码中所见（摘自fashion-mnist-vae/model.js），定义输入输出差异项是直接的。我们简单地计算原始输入和解码器输出之间的均方误差（MSE）。然而，统计项，称为*Kullbach-Liebler*（KL）散度，数学上更加复杂。我们会免去详细的数学[^15]，但从直觉上讲，KL散度项（代码中的klLoss）鼓励不同输入图像的分布更均匀地分布在潜在空间的中心周围，这使得解码器更容易在图像之间进行插值。因此，`klLoss`项可以被视为VAE的主要输入输出差异项之上添加的正则化项。
- en: ^(15)
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(15)
- en: ''
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This blog post by Irhum Shafkat includes a deeper discussion of the math behind
    the KL divergence: [http://mng.bz/vlvr](http://mng.bz/vlvr).'
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Irhum Shafkat的这篇博文包含了对KL散度背后数学的更深入讨论：[http://mng.bz/vlvr](http://mng.bz/vlvr)。
- en: Listing 10.7\. The loss function for the VAE
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第10.7节列出了VAE的损失函数。
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1*** Computes a “reconstruction loss” term. The goal of minimizing this
    term is to make the model outputs match the input data.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 计算“重构损失”项。最小化此项的目标是使模型输出与输入数据匹配。'
- en: '***2*** Computes the KL-divergence between zLogVar and zMean. Minimizing this
    term aims to make the distribution of the latent variable more normally distributed
    around the center of the latent space.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 计算zLogVar和zMean之间的KL散度。最小化此项旨在使潜变量的分布更接近于潜在空间的中心处正态分布。'
- en: '***3*** Sums the image reconstruction loss and the KL-divergence loss into
    the final VAE loss'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 将图像重建损失和KL-散度损失汇总到最终的VAE损失中'
- en: Another missing piece for our VAE training is the optimizer and the training
    step that uses it. The type of optimizer is the popular ADAM optimizer (`tf.train
    .adam()`). The training step for the VAE differs from all other models we’ve seen
    in this book in that it doesn’t use the `fit()` or `fitDataset()` method of the
    model object. Instead, it calls the `minimize()` method of the optimizer ([listing
    10.8](#ch10ex08)). This is because the KL-divergence term of the custom loss function
    uses two of the model’s four outputs, but in TensorFlow.js, the `fit()` and `fitDataset()`
    methods work only if each of the model’s outputs has a loss function that doesn’t
    depend on any other output.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们VAE训练的另一个缺失部分是优化器及其使用的训练步骤。优化器的类型是流行的ADAM优化器（`tf.train .adam()`）。VAE的训练步骤与本书中所有其他模型不同，因为它不使用模型对象的`fit()`或`fitDataset()`方法。相反，它调用优化器的`minimize()`方法（[列表10.8](#ch10ex08)）。这是因为自定义损失函数的KL-散度项使用模型的四个输出中的两个，但在TensorFlow.js中，只有在模型的每个输出都具有不依赖于任何其他输出的损失函数时，`fit()`和`fitDataset()`方法才能正常工作。
- en: 'As [listing 10.8](#ch10ex08) shows, the `minimize()` function is called with
    an arrow function as the only argument. This arrow function returns the loss under
    the current batch of flattened images (`reshaped` in the code), which is closed
    over by the function. `minimize()` calculates the gradient of the loss with respect
    to all the trainable weights of the VAE (including the encoder and decoder), adjusts
    them according to the ADAM algorithm, and then applies updates to the weights
    in directions opposite to the adjusted gradients. This completes a single step
    of training. This step is performed repeatedly, over all images in the Fashion-MNIST
    dataset, and constitutes an epoch of training. The `yarn train` command performs
    multiple epochs of training (default: 5 epochs), after which the loss value converges,
    and the decoder part of the VAE is saved to disk. The reason the encoder part
    isn’t saved is that it won’t be used in the following, browser-based demo step.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表10.8](#ch10ex08)所示，`minimize()`函数以箭头函数作为唯一参数进行调用。这个箭头函数返回当前批次的扁平化图像的损失（代码中的`reshaped`），这个损失被函数闭包。`minimize()`计算损失相对于VAE的所有可训练权重的梯度（包括编码器和解码器），根据ADAM算法调整它们，然后根据调整后的梯度在权重的相反方向应用更新。这完成了一次训练步骤。这一步骤重复进行，遍历Fashion-MNIST数据集中的所有图像，并构成一个训练时期。`yarn
    train` 命令执行多个训练周期（默认：5个周期），在此之后损失值收敛，并且VAE的解码器部分被保存到磁盘上。编码器部分不保存的原因是它不会在接下来的基于浏览器的演示步骤中使用。
- en: Listing 10.8\. The training loop of the VAE (excerpt from fashion-mnist-vae/train.js)
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.8\. VAE的训练循环（摘自fashion-mnist-vae/train.js）
- en: '[PRE21]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1*** Gets a batch of (flattened) Fashion-MNIST images'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 获取一批（扁平化的）Fashion-MNIST图像'
- en: '***2*** A single step of VAE training: makes a prediction with the VAE and
    computes the loss so that optimizer.minimize can adjust all the trainable weights
    of the model'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** VAE训练的单个步骤：使用VAE进行预测，并计算损失，以便optimizer.minimize可以调整模型的所有可训练权重'
- en: '***3*** Since we are not using the stock fit() method, we cannot use the built-in
    progress bar and hence must print status updates to the console ourselves.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 由于我们不使用默认的`fit()`方法，因此不能使用内置的进度条，必须自己打印控制台上的状态更新。'
- en: '***4*** At the end of every training epoch, generates an image using the decoder
    and prints it to the console for preview'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 在每个训练周期结束时，使用解码器生成一幅图像，并将其打印到控制台以进行预览'
- en: The web page brought up by the `yarn watch` command will load the saved decoder
    and use it to generate a grid of images similar to what’s shown in [figure 10.8](#ch10fig08).
    These images are obtained from a regular grid of latent vectors in the 2D latent
    space. The upper and lower limit along each of the two latent dimensions can be
    adjusted in the UI.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn watch` 命令打开的网页将加载保存的解码器，并使用它生成类似于[图10.8](#ch10fig08)所示的图像网格。这些图像是从二维潜在空间中的正则网格的潜在向量获得的。每个潜在维度上的上限和下限可以在UI中进行调整。'
- en: Figure 10.8\. Sampling the latent space of the VAE after training. This figure
    shows a 20 × 20 grid of decoder outputs. This grid corresponds to a regularly
    spaced grid of 20 × 20 2D latent vectors, of which each dimension is in the interval
    of [–4, 4].
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8\. 在训练后对VAE的潜在空间进行采样。该图显示了一个20 × 20的解码器输出网格。该网格对应于一个20 × 20的二维潜在向量的正则间隔网格，其中每个维度位于[-4,
    4]的区间内。
- en: '![](10fig07.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig07.jpg)'
- en: The grid of images shows a completely continuous distribution of different types
    of clothing from the Fashion-MNIST dataset, with one clothing type morphing gradually
    into another type as you follow a continuous path through the latent space (for
    example, pullover to T-shirt, T-shirt to pants, boots to shoes). Specific directions
    in the latent space have a meaning inside a subdomain of the latent space. For
    example, near the top section of the latent space, the horizontal dimension appears
    to represent “bootness versus shoeness;” around the bottom-right corner of the
    latent space, the horizontal dimension seems to represent “T-shirtness versus
    pantsness,” and so forth.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图像网格显示了来自 Fashion-MNIST 数据集的完全连续的不同类型的服装，一种服装类型在潜在空间中沿着连续路径逐渐变形为另一种类型（例如，套头衫变成
    T 恤，T 恤变成裤子，靴子变成鞋子）。潜在空间的特定方向在潜在空间的子域内具有一定的意义。例如，在潜在空间的顶部区域附近，水平维度似乎代表“靴子特性与鞋子特性；”在潜在空间的右下角附近，水平维度似乎代表“T
    恤特性与裤子特性”，依此类推。
- en: 'In the next section, we will cover another major type of model for generating
    images: GANs.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍另一种生成图像的主要模型类型：GANs。
- en: 10.3\. Image generation with GANs
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3\. 使用 GANs 进行图像生成
- en: Since Ian Goodfellow and his colleagues introduced GANs in 2014,^([[16](#ch10fn16)])
    the technique has seen rapid growth in interest and sophistication. Today, GANs
    have become a powerful tool for generating images and other modalities of data.
    They are capable of outputting high-resolution images that in some cases are indistinguishable
    from real ones to human eyes. See the human face images generated by NVIDIA’s
    StyleGANs in [figure 10.9](#ch10fig09).^([[17](#ch10fn17)]) If not for the occasional
    artifact spots on the face and the unnatural-looking scenes in the background,
    it would be virtually impossible for a human viewer to tell these generated images
    apart from real ones.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 Ian Goodfellow 和他的同事在2014年引入了 GANs^([[16](#ch10fn16)]) 这项技术以来，它的兴趣和复杂程度迅速增长。如今，GANs
    已经成为生成图像和其他数据模态的强大工具。它们能够输出高分辨率图像，有些情况下，这些图像在人类眼中几乎无法与真实图像区分开来。查看 NVIDIA 的 StyleGANs
    生成的人脸图像，如 [图 10.9](#ch10fig09)^([[17](#ch10fn17)]) 所示。如果不是人脸上偶尔出现的瑕疵点和背景中不自然的场景，人类观察者几乎无法将这些生成的图像与真实图像区分开来。
- en: ^(16)
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(16)
- en: ''
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ian Goodfellow et al., “Generative Adversarial Nets,” *NIPS Proceedings*, 2014,
    [http://mng.bz/4ePv](http://mng.bz/4ePv).
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ian Goodfellow 等人，“生成对抗网络”，*NIPS 会议论文集*，2014年，[http://mng.bz/4ePv](http://mng.bz/4ePv)。
- en: ^(17)
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(17)
- en: ''
  id: totrans-221
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Website at [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com).
    For the academic paper, see Tero Karras, Samuli Laine, and Timo Aila, “A Style-Based
    Generator Architecture for Generative Adversarial Networks,” submitted 12 Dec.
    2018, [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948).
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://thispersondoesnotexist.com](https://thispersondoesnotexist.com) 的网站。有关学术论文，请参阅
    Tero Karras，Samuli Laine 和 Timo Aila，“用于生成对抗网络的基于样式的生成器架构”，于2018年12月12日提交，[https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)。'
- en: Figure 10.9\. Example human-face images generated by NVIDIA’s StyleGAN, sampled
    from [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)
    in April 2019
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.9\. NVIDIA 的 StyleGAN 生成的示例人脸图像，从 [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)
    中采样于 2019 年 4 月
- en: '![](10fig08_alt.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig08_alt.jpg)'
- en: Apart from generating compelling images “out of the blue,” the images generated
    by GANs can be conditioned on certain input data or parameters, which leads to
    a variety of more task-specific and useful applications. For example, GANs can
    be used to generate a higher-resolution image from a low-resolution input (image
    super-resolution), fill in missing parts of an image (image inpainting), convert
    a black-and-white image into a color one (image colorization), generate an image
    given a text description, and generate the image of a person in a given pose given
    an input image of the same person in another pose. In addition, new types of GANs
    have been developed to generate nonimage outputs, such as music.^([[18](#ch10fn18)])
    Apart from the obvious value of generating an unlimited amount of realistic-looking
    material, which is desired in domains such as art, music production, and game
    design, GANs have other applications, such as assisting deep learning by generating
    training examples in cases where such examples are costly to acquire. For instance,
    GANs are being used to generate realistic-looking street scenes for training self-driving
    neural networks.^([[19](#ch10fn19)])
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 除了“从蓝天中生成引人注目的图像”之外，GAN 生成的图像还可以根据某些输入数据或参数进行条件约束，这带来了更多的特定任务和有用的应用。例如，GAN 可以用于从低分辨率输入（图像超分辨率）生成更高分辨率的图像，填补图像的缺失部分（图像修复），将黑白图像转换为彩色图像（图像着色），根据文本描述生成图像以及根据输入图像中同一人采取的姿势生成该人的图像。此外，已经开发了新类型的
    GAN 用于生成非图像输出，例如音乐。^([[18](#ch10fn18)]) 除了在艺术、音乐制作和游戏设计等领域中生成无限量的逼真材料的明显价值之外，GAN
    还有其他应用，例如通过在获取此类样本代价高昂的情况下生成训练示例来辅助深度学习。例如，GAN 正被用于为训练自动驾驶神经网络生成逼真的街景图像。^([[19](#ch10fn19)])
- en: ^(18)
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(18)
- en: ''
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See the MuseGAN project from Hao-Wen Dong et al.: [https://salu133445.github.io/musegan/](https://salu133445.github.io/musegan/).'
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Hao-Wen Dong 等人的 MuseGAN 项目：[https://salu133445.github.io/musegan/](https://salu133445.github.io/musegan/)。
- en: ^(19)
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(19)
- en: ''
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: James Vincent, “Nvidia Uses AI to Make it Snow on Streets that Are Always Sunny,”
    *The Verge*, 5 Dec. 2017, [http://mng.bz/Q0oQ](http://mng.bz/Q0oQ).
  id: totrans-231
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: James Vincent，《NVIDIA 使用 AI 让永远阳光的街道下雪》，*The Verge*，2017 年 12 月 5 日，[http://mng.bz/Q0oQ](http://mng.bz/Q0oQ)。
- en: Although VAEs and GANs are both generative models, they are based on different
    ideas. While VAEs ensure the quality of generated examples by using an MSE loss
    between the original input and the decoder output, a GAN makes sure its outputs
    are realistic by employing a *discriminator*, as we’ll soon explain. In addition,
    many variants of GANs allow inputs to consist of not only the latent-space vector
    but also conditioning inputs, such as a desired image class. The ACGAN we’ll explore
    next is a good example of this. In this type of GAN with mixed inputs, latent
    spaces are no longer even continuous with respect to the network inputs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 VAE 和 GAN 都是生成模型，但它们基于不同的思想。VAE 通过使用原始输入和解码器输出之间的均方误差损失来确保生成的示例的质量，而 GAN
    则通过使用*鉴别器*来确保其输出逼真，我们很快就会解释。此外，GAN 的许多变体允许输入不仅包含潜空间向量，还包括条件输入，例如所需的图像类别。我们将要探索的
    ACGAN 就是这方面的一个很好的例子。在这种具有混合输入的 GAN 类型中，潜空间不再与网络输入具有连续性。
- en: In this section, we will dive into a relatively simple type of GAN. Specifically,
    we will train an *auxiliary classifier* GAN (ACGAN)^([[20](#ch10fn20)]) on the
    familiar MNIST hand-written digit dataset. This will give us a model capable of
    generating digit images that look just like the real MNIST digits. At the same
    time, we will be able to control what digit class (0 through 9) each generated
    image belongs to, thanks to the “auxiliary classifier” part of ACGAN. In order
    to understand how ACGAN works, let’s do it one step at a time. First, we will
    explain how the base “GAN” part of ACGAN works. Then, we will describe the additional
    mechanisms by which ACGAN makes the class identity controllable.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个部分，我们将深入研究一种相对简单的 GAN 类型。具体而言，我们将在熟悉的 MNIST 手写数字数据集上训练一个*辅助分类器* GAN (ACGAN)^([[20](#ch10fn20)])。这将给我们一个能够生成与真实
    MNIST 数字完全相似的数字图像的模型。同时，由于 ACGAN 的“辅助分类器”部分，我们将能够控制每个生成图像所属的数字类别（0 到 9）。为了理解 ACGAN
    的工作原理，让我们一步一步来。首先，我们将解释 ACGAN 的基本“GAN”部分如何工作。然后，我们将描述 ACGAN 通过额外的机制如何使类别标识具有可控性。
- en: ^(20)
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(20)
- en: ''
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Augustus Odena, Christopher Olah, and Jonathon Shlens, “Conditional Image Synthesis
    with Auxiliary Classifier GANs,” submitted 30 Oct. 2016, [https://arxiv.org/abs/1610.09585](https://arxiv.org/abs/1610.09585).
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Augustus Odena、Christopher Olah和Jonathon Shlens，“带辅助分类器GAN的条件图像合成”，2016年10月30日提交，[https://arxiv.org/abs/1610.09585](https://arxiv.org/abs/1610.09585)。
- en: 10.3.1\. The basic idea behind GANs
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1\. GANs背后的基本思想
- en: 'How does a GAN learn to generate realistic-looking images? It achieves this
    through an interplay between two subparts that it comprises: a *generator* and
    a *discriminator*. Think of the generator as a counterfeiter whose goal is to
    create high-quality fake Picasso paintings; the discriminator is like an art dealer
    whose job is to tell fake Picasso paintings apart from real ones. The counterfeiter
    (generator) strives to create better and better fake paintings in order to fool
    the art dealer (the discriminator), while the art dealer’s job is to become a
    better and better critiquer of the paintings so as *not* to be fooled by the counterfeiter.
    This antagonism between our two players is the reason behind the “adversarial”
    part of the name “GAN.” Intriguingly, the counterfeiter and art dealer end up
    *helping* each other become better, despite apparently being adversaries.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）是如何学习生成逼真图片的？它通过其包含的两个子部分之间的相互作用来实现这一点：一个*生成器*和一个*鉴别器*。把生成器想象成一个伪造者，其目标是创建高质量的假毕加索画作；而鉴别器则像是一位艺术品经销商，其工作是将假的毕加索画作与真实的区分开来。伪造者（生成器）努力创建越来越好的假画作以欺骗艺术品经销商（鉴别器），而艺术品经销商的工作是成为对画作的评判者越来越好，从而*不被*伪造者欺骗。我们两个角色之间的这种对抗是“GAN”名称中“对抗性”部分的原因。有趣的是，伪造者和艺术品经销商最终互相*帮助*变得更好，尽管表面上是对手。
- en: 'In the beginning, the counterfeiter (generator) is bad at creating realistic-looking
    Picassos because its weights are initialized randomly. As a result, the art dealer
    (discriminator) quickly learns to tell real and fake Picassos apart. Here is an
    important part of how all of this works: every time the counterfeiter brings a
    new painting to the art dealer, they are provided with detailed feedback (from
    the art dealer) about which parts of the painting look wrong and how to change
    the painting to make it look more real. The counterfeiter learns and remembers
    this so that next time they come to the art dealer, their painting will look slightly
    better. This process repeats many times. It turns out, if all the parameters are
    set properly, we will end up with a skillful counterfeiter (generator). Of course,
    we will also get a skillful discriminator (art dealer), but we usually need only
    the generator after the GAN is trained.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，伪造者（生成器）在创建逼真的毕加索画作方面表现糟糕，因为其权重是随机初始化的。结果，艺术品经销商（鉴别器）很快就学会了区分真假毕加索画作。这里是所有这些工作的重要部分：每次伪造者给艺术品经销商带来一幅新画作时，他们都会得到详细的反馈（来自艺术品经销商），指出画作的哪些部分看起来不对劲，以及如何改变画作使其看起来更真实。伪造者学习并记住这一点，以便下次他们来到艺术品经销商那里时，他们的画作看起来会稍微好一些。这个过程重复多次。结果发现，如果所有参数都设置正确，我们最终会得到一个技艺精湛的伪造者（生成器）。当然，我们也会得到一个技艺精湛的鉴别器（艺术品经销商），但通常在GAN训练完成后我们只需要生成器。
- en: '[Figure 10.10](#ch10fig10) provides a more detailed look at how the discriminator
    part of a generic GAN model is trained. In order to train the discriminator, we
    need a batch of generated images and a batch of real ones. The generated ones
    are generated by the generator. But the generator can’t make images out of thin
    air. Instead, it needs to be given a random vector as the input. The latent vectors
    are conceptually similar to the ones we used for VAEs in [section 10.2](#ch10lev1sec2).
    For each image generated by the generator, the latent vector is a 1D tensor of
    shape `[latentSize]`. But like most training procedures in this book, we perform
    the step for a batch of images at a time. Therefore, the latent vector has a shape
    of `[batchSize, latentSize]`. The real images are directly drawn from the actual
    MNIST dataset. For symmetry, we draw `batchSize` real images (exactly the same
    number as the generated ones) for each step of training.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.10](#ch10fig10)更详细地描述了如何训练通用 GAN 模型的判别器部分。为了训练判别器，我们需要一批生成的图像和一批真实图像。生成的图像由生成器生成。但生成器无法从空气中制作图像。相反，它需要作为输入的随机向量。潜在向量在概念上类似于我们在[第
    10.2 节](#ch10lev1sec2)中用于 VAE 的向量。对于生成器生成的每个图像，潜在向量是形状为`[latentSize]`的一维张量。但像本书中大多数训练过程一样，我们一次对一批图像执行步骤。因此，潜在向量的形状为`[batchSize,
    latentSize]`。真实图像直接从实际 MNIST 数据集中提取。为了对称起见，我们在每个训练步骤中绘制与生成的图像完全相同数量的`batchSize`真实图像。'
- en: Figure 10.10\. A schematic diagram illustrating the algorithm by which the discriminator
    part of a GAN is trained. Notice that this diagram omits the digit-class part
    of the ACGAN for the sake of simplicity. For a complete diagram of generator training
    in ACGAN, see [figure 10.13](#ch10fig13).
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.10\. 示出 GAN 判别器部分训练算法的示意图。请注意，为简单起见，该图省略了 ACGAN 的数字类部分。有关 ACGAN 生成器训练的完整图表，请参见[图
    10.13](#ch10fig13)。
- en: '![](10fig09_alt.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig09_alt.jpg)'
- en: The generated images and real ones are then concatenated into a single batch
    of images, represented as a tensor of shape `[2 * batchSize, 28, 28, 1]`. The
    discriminator is executed on this batch of combined images, which outputs predicted
    probability scores for whether each image is real. These probability scores can
    be easily tested against the ground truth (we know which ones are real and which
    ones are generated!) through the binary cross-entropy loss function. Then, the
    familiar backpropagation algorithm does its job, updating the weight parameters
    of the discriminator with the help of an optimizer (not shown in the figure).
    This step nudges the discriminator a bit toward making correct predictions. Notice
    that the generator merely participates in this training step by providing generated
    samples, but it’s not updated by the backpropagation process. It is the next training
    step that updates the generator ([figure 10.11](#ch10fig11)).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像和真实图像随后被连接成一批图像，表示为形状为`[2 * batchSize, 28, 28, 1]`的张量。判别器在这批合并图像上执行，输出每个图像是真实的预测概率分数。这些概率分数可以轻松地通过二元交叉熵损失函数与基准真值进行测试（我们知道哪些是真实的，哪些是生成的！）。然后，熟悉的反向传播算法发挥作用，借助优化器（图中未显示）更新判别器的权重参数。这一步使判别器略微朝着正确的预测方向推进。请注意，生成器仅通过提供生成的样本参与此训练步骤，但它不会通过反向传播过程进行更新。下一步训练将更新生成器（[图
    10.11](#ch10fig11)）。
- en: Figure 10.11\. A schematic diagram illustrating the algorithm by which the generator
    part of a GAN is trained. Notice that this diagram omits the digit-class part
    of the ACGAN for the sake of simplicity. For a complete diagram of ACGAN’s generator-training
    process, see [figure 10.14](#ch10fig14).
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.11\. 示出 GAN 生成器部分训练算法的示意图。请注意，为简单起见，该图省略了 ACGAN 的数字类部分。有关 ACGAN 生成器训练过程的完整图表，请参见[图
    10.14](#ch10fig14)。
- en: '![](10fig10_alt.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig10_alt.jpg)'
- en: '[Figure 10.11](#ch10fig11) illustrates the generator-training step. We let
    the generator make another batch of generated images. But unlike the discriminator-training
    step, we don’t need any real MNIST images. The discriminator is given this batch
    of generated images along with a batch of binary realness labels. We *pretend*
    that the generated images are real by setting the realness labels to all 1s. Pause
    for a moment and let that sink in: this is the most important trick in GAN training.
    Of course the images are all generated (not real), but we let the realness label
    say they are real anyway. The discriminator may (correctly) assign low realness
    probabilities to some or all of the input images. But if it does so, the binary
    cross-entropy loss will end up with a large value, thanks to the bogus realness
    labels. This will cause the backpropagation to update the generator in a way that
    nudges the discriminator’s realness scores a little higher. Note that the backpropagation
    updates *only* the generator. It leaves the discriminator untouched. This is another
    important trick: it ensures that the generator ends up making slightly more realistic-looking
    images, instead of the discriminator lowering its bar for what’s real. This is
    achieved by freezing the discriminator part of the model, an operation we’ve used
    for transfer learning in [chapter 5](kindle_split_016.html#ch05).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.11](#ch10fig11)说明了生成器的训练步骤。我们让生成器生成另一批生成图像。但与鉴别器的训练步骤不同，我们不需要任何真实的MNIST图像。鉴别器被赋予了这批生成图像以及一批二进制真实性标签。我们假装这些生成图像都是真实的，将真实性标签设置为全部为1。静下心来思考一下：这是GAN训练中最重要的技巧。当然，这些图像都是生成的（并非真实的），但我们让真实性标签表明它们是真实的。鉴别器可能（正确地）对一些或所有的输入图像分配较低的真实性概率。但是如果这样做，由于虚假的真实性标签，二进制交叉熵损失将得到较大的值。这将导致反向传播更新生成器，以使鉴别器的真实性得分稍微增加。请注意，反向传播**只更新**生成器，不对鉴别器进行任何更改。这是另一个重要的技巧：它确保生成器最终产生的图像看起来更真实一些，而不是降低鉴别器对真实性的要求。这是通过冻结模型的鉴别器部分实现的，这是我们在[第5章](kindle_split_016.html#ch05)中用于迁移学习的一种操作。'
- en: 'To summarize the generator-training step: we freeze the discriminator and feed
    an all-1 realness label to it, despite the fact that it is given generated images
    generated by the generator. As a result, the weight updates to the generator will
    cause it to generate images that look slightly more real to the discriminator.
    This way of training the generator will work only if the discriminator is reasonably
    good at telling what’s real and what’s generated. How do we ensure that? The answer
    is the discriminator-training step we already talked about. Therefore, you can
    see that the two training steps form an intricate yin-and-yang dynamic, in which
    the two parts of the GAN counter and help each other at the same time.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 总结生成器训练步骤：冻结鉴别器并向其提供全是1的真实性标签，尽管它得到的是由生成器生成的生成图像。由于这样，对生成器的权重更新将导致其生成的图像在鉴别器中看起来稍微更真实。只有当鉴别器相当擅长区分真实和生成的图像时，这种训练生成器的方式才会奏效。我们如何确保这一点？答案是我们已经讨论过的鉴别器训练步骤。因此，你可以看到这两个训练步骤形成了一种复杂的阴阳动态，其中GAN的两个部分相互抵触并互相帮助。
- en: That concludes our high-level overview of generic GAN training. In the next
    section, we will look at the internal architecture of the discriminator and generator
    and how they incorporate the information about image class.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是对通用GAN训练的高级概览。在下一节中，我们将介绍鉴别器和生成器的内部架构以及它们如何融入有关图像类别的信息。
- en: 10.3.2\. The building blocks of ACGAN
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.2\. ACGAN的构建模块
- en: '[Listing 10.9](#ch10ex09) shows the TensorFlow.js code that creates the discriminator
    part of the MNIST ACGAN (excerpted from mnist-acgan/gan.js). At the core of the
    discriminator is a deep convnet similar to the ones we saw in [chapter 4](kindle_split_015.html#ch04).
    Its input has the canonical shape of MNIST images, namely `[28, 28, 1]`. The input
    image passes through four 2D convolutional (conv2d) layers before being flattened
    and processed by two dense layers. One dense layer outputs a binary prediction
    for the realness of the input image, while the other outputs the softmax probabilities
    for the 10 digit classes. The discriminator is a functional model that has both
    dense layers’ outputs. Panel A of [figure 10.12](#ch10fig12) provides a schematic
    view of the discriminator’s one-input-two-output topology.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 10.9](#ch10ex09) 显示了创建 MNIST ACGAN 判别器部分的 TensorFlow.js 代码（摘自 mnist-acgan/gan.js）。在判别器的核心是一种类似于我们在[第四章](kindle_split_015.html#ch04)中看到的深度卷积网络。其输入具有
    MNIST 图像的经典形状，即 `[28, 28, 1]`。输入图像通过四个 2D 的卷积（conv2d）层，然后被展平并经过两个全连接层处理。其中一个全连接层为输入图像的真实性二进制预测输出，另一个输出
    10 个数字类别的 softmax 概率。判别器是一个有两个全连接层输出的函数模型。图 10.12 的面板 A 提供了判别器的一个输入-两个输出拓扑结构的示意图。'
- en: Figure 10.12\. Schematic diagrams of the internal topology of the discriminator
    (panel A) and generator (panel B) parts of ACGAN. Certain details (the dropout
    layers in the discriminator) are omitted for simplicity. See [listings 10.9](#ch10ex09)
    and [10.10](#ch10ex10) for the detailed code.
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.12。ACGAN 的判别器（面板 A）和生成器（面板 B）部分的内部拓扑示意图。为简洁起见，省略了某些细节（例如判别器中的 dropout 层）。有关详细的代码，请参见[清单
    10.9](#ch10ex09) 和 [10.10](#ch10ex10)。
- en: '![](10fig11_alt.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig11_alt.jpg)'
- en: Listing 10.9\. Creating the discriminator part of ACGAN
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 10.9。创建 ACGAN 的判别器部分。
- en: '[PRE22]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1*** The discriminator takes only one input: images in the MNIST format'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 判别器只接受 MNIST 格式的图像作为输入。'
- en: '***2*** Dropout layers are used to counteract overfitting.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用 Dropout 层来对抗过拟合。'
- en: '***3*** The first of the discriminator’s two outputs: the probability score
    from the binary realness classification'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 判别器的两个输出之一是二进制真实性分类的概率分数。'
- en: '***4*** The second output is the softmax probabilities for the 10 MNIST digit
    classes.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 第二个输出是 10 个 MNIST 数字类别的 softmax 概率。'
- en: The code in [listing 10.10](#ch10ex10) is responsible for creating the ACGAN
    generator. As we’ve alluded to before, the generator’s generation process requires
    an input called a *latent vector* (named `latent` in the code). This is reflected
    in the `inputShape` parameter of its first dense layer. However, if you examine
    the code more carefully, you can see that the generator actually takes *two* inputs.
    This is illustrated in panel B of [figure 10.12](#ch10fig12). In addition to the
    latent vector, which is a 1D tensor of shape `[latentSize]`, the generator requires
    an additional input, which is named `imageClass` and has a simple shape of `[1]`.
    This is the way in which we tell the model which MNIST digit class (0 to 9) it
    is commanded to generate. For example, if we want the model to generate an image
    for digit 8, we should feed a tensor value of `tf.tensor2d([[8]])` to the second
    input (remember that the model always expects batched tensors, even if there is
    only one example). Likewise, if we want the model to generate two images, one
    for the digit 8 and one for 9, then the fed tensor should be `tensor2d([[8], [9]])`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 10.10](#ch10ex10) 中的代码负责创建 ACGAN 生成器。正如我们之前暗示的那样，生成器的生成过程需要一个叫做*潜在向量*（代码中称为
    `latent`）的输入。这体现在其第一个全连接层的 `inputShape` 参数中。然而，如果你仔细检查代码，就会发现生成器实际上接受*两个*输入。这在
    [图 10.12](#ch10fig12) 的面板 B 中有描述。除了潜在向量外，也就是一个形状为 `[latentSize]` 的一维张量，生成器需要一个额外的输入，名为
    `imageClass`，形状简单，为 `[1]`。这是告诉模型要生成哪个 MNIST 数字（0 到 9）的方式。例如，如果我们想要模型生成数字 8 的图像，我们应该将形状为
    `tf.tensor2d([[8]])` 的张量值输入到第二个输入（请记住，即使只有一个示例，模型也始终期望批量张量）。同样，如果我们想要模型生成两个图像，一个是数字
    8，另一个是数字 9，则馈送的张量应为 `tf.tensor2d([[8], [9]])`。'
- en: 'As soon as the `imageClass` input enters the generator, an embedding layer
    transforms it into a tensor of the same shape as `latent` (`[latentSize]`). This
    step is mathematically similar to the embedding-lookup procedure we used in the
    sentiment-analysis and date-conversion models in [chapter 9](kindle_split_021.html#ch09).
    The desired digit class is an integer quantity analogous to the word indices in
    the sentiment-analysis data and the character indices in the date-conversion data.
    It is transformed into a 1D vector in the same way that word and character indices
    were transformed into 1D vectors. However, we use embedding lookup on `imageClass`
    here for a different purpose: to merge it with the `latent` vector and form a
    single, combined vector (named `h` in [listing 10.10](#ch10ex10).) This merging
    is done through a `multiply` layer, which performs element-by-element multiplication
    between the two vectors of identical shapes. The resultant tensor has the same
    shape as the inputs (`[latentSize]`) and goes into later parts of the generator.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 `imageClass` 输入进入生成器，嵌入层将其转换为与 `latent` 相同形状的张量 (`[latentSize]`)。这一步在数学上类似于我们在
    [第 9 章](kindle_split_021.html#ch09) 中用于情感分析和日期转换模型的嵌入查找过程。期望的数字类别是一个整数量，类似于情感分析数据中的单词索引和日期转换数据中的字符索引。它被转换为与单词和字符索引转换为
    1D 向量的方式相同的 1D 向量。然而，我们在这里对 `imageClass` 使用嵌入查找是为了不同的目的：将其与 `latent` 向量合并并形成一个单一的组合向量（在
    [清单 10.10](#ch10ex10) 中命名为 `h`）。这个合并是通过一个 `multiply` 层完成的，该层在两个相同形状的向量之间执行逐元素相乘。结果张量的形状与输入相同
    (`[latentSize]`)，并传入生成器的后续部分。
- en: The generator immediately applies a dense layer on the combined latent vector
    (`h`) and reshapes it into a 3D shape of `[3, 3, 384]`. This reshaping yields
    an image-like tensor, which can then be transformed by the following parts of
    the generator into an image that has the canonical MNIST shape (`[28, 28, 1]`).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器立即在合并的潜在向量 (`h`) 上应用一个密集层，并将其重塑为 3D 形状 `[3, 3, 384]`。这种重塑产生了一个类似图像的张量，随后可以由生成器的后续部分转换为具有标准
    MNIST 形状 (`[28, 28, 1]`) 的图像。
- en: Instead of using the familiar conv2d layers to transform the input, the generator
    uses the conv2dTranspose layer to transform its image tensors. Roughly speaking,
    conv2dTranspose performs the inverse operation to conv2d (sometimes referred to
    as *deconvolution*). The output of a conv2d layer generally has smaller height
    and width compared to its input (except for the rare cases in which the `kernelSize`
    is 1), as you can see in the convnets in [chapter 4](kindle_split_015.html#ch04).
    However, a conv2dTranspose layer generally has a larger height and weight in its
    output than its input. In other words, while a conv2d layer typically *shrinks*
    the dimensions of its input, a typical conv2dTranspose layer *expands* them. This
    is why, in the generator, the first conv2dTranspose layer takes an input with
    height 3 and width 3, but the last conv2dTranspose layer outputs height 28 and
    width 28\. This is how the generator turns an input latent vector and a digit
    index into an image in the standard MNIST image dimensions. The code in the following
    listing is excerpted from mnist-acgan/gan.js; some error-checking code is removed
    for clarity.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器不使用熟悉的 conv2d 层来转换输入，而是使用 conv2dTranspose 层来转换其图像张量。粗略地说，conv2dTranspose
    执行与 conv2d 的逆操作（有时称为*反卷积*）。conv2d 层的输出通常比其输入具有更小的高度和宽度（除了 `kernelSize` 为 1 的情况之外），如您在
    [第 4 章](kindle_split_015.html#ch04) 中的 convnets 中所见。然而，conv2dTranspose 层的输出通常比其输入具有更大的高度和宽度。换句话说，虽然
    conv2d 层通常*缩小*其输入的维度，但典型的 conv2dTranspose 层*扩展*它们。这就是为什么在生成器中，第一个 conv2dTranspose
    层接受高度为 3 和宽度为 3 的输入，但最后一个 conv2dTranspose 层输出高度为 28 和宽度为 28 的原因。这就是生成器将输入潜在向量和数字索引转换为标准
    MNIST 图像尺寸的图像的方式。以下清单中的代码摘录自 mnist-acgan/gan.js; 为了清晰起见，删除了一些错误检查代码。
- en: Listing 10.10\. Creating the generator part of ACGAN
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 10.10\. 创建 ACGAN 的生成器部分
- en: '[PRE23]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1*** The number of units is chosen so that when the output is reshaped and
    fed through the subsequent conv2dTranspose layers, the tensor that comes out at
    the end has the exact shape that matches MNIST images ([28, 28, 1]).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 单元的数量被选择为当输出被重塑并通过后续的 conv2dTranspose 层时，最终输出的张量的形状与 MNIST 图像完全匹配 ([28,
    28, 1])。'
- en: '***2*** Upsamples from [3, 3, ...] to [7, 7, ...]'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 从 [3, 3, ...] 上采样至 [7, 7, ...]'
- en: '***3*** Upsamples to [14, 14, ...]'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 上采样至 [14, 14, ...]'
- en: '***4*** Upsamples to [28, 28, ...]'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 上采样至 [28, 28, ...]'
- en: '***5*** This is the first of the two inputs of the generator: the latent (z-space)
    vector that is used as the “seed” of the fake-image generation.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 这是生成器的两个输入之一：作为伪图像生成的“种子”的潜在（z-空间）向量。'
- en: '***6*** The second input of the generator: class labels that control which
    of the 10 MNIST digit classes the generated images should belong to'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 生成器的第二个输入：控制生成的图像属于哪个 MNIST 数字类别的类标签'
- en: '***7*** Converts the desired label to a vector of length latentSize through
    embedding lookup'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 通过嵌入查找将期望标签转换为长度为 latentSize 的向量'
- en: '***8*** Combines the latent vector and the class conditional embedding through
    multiplication'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 通过乘法将潜在向量和类别条件嵌入组合起来'
- en: '***9*** The model is finally created, with the sequential convnet as its core.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9*** 最终创建模型，以顺序卷积网络为核心。'
- en: 10.3.3\. Diving deeper into the training of ACGAN
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.3\. 更深入地了解 ACGAN 的训练
- en: The last section should have given you a better understanding of the internal
    structure of ACGAN’s discriminator and generator and how they incorporate the
    digit-class information (the “AC” part of ACGAN’s name). With this knowledge,
    we are ready to expand on [figures 10.10](#ch10fig10) and [10.11](#ch10fig11)
    in order to form a thorough understanding of how ACGAN is trained.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一节应该让你更好地理解了 ACGAN 的鉴别器和生成器的内部结构，以及它们如何整合数字类别信息（ACGAN 名字中的“AC”部分）。有了这些知识，我们就可以扩展
    [figures 10.10](#ch10fig10) 和 [10.11](#ch10fig11)，以全面了解 ACGAN 的训练方式。
- en: '[Figure 10.13](#ch10fig13) is an expanded version of [figure 10.10](#ch10fig10).
    It shows the training of ACGAN’s discriminator part. Compared to before, this
    training step not only improves the discriminator’s ability to tell real and generated
    (fake) images apart but also hones its ability to determine which digit class
    a given image (including real and generated) belongs to. To make it easier to
    compare with the simpler diagram from before, we grayed out the parts already
    seen in [figure 10.10](#ch10fig10) and highlighted the new parts. First, note
    that the generator now has an additional input (Digit Class), which makes it possible
    to specify what digits the generator should generate. In addition, the discriminator
    outputs not only a realness prediction but also a digit-class prediction. As a
    result, both output heads of the discriminator need to be trained. The training
    of the realness-predicting part remains the same as before ([figure 10.10](#ch10fig10));
    the training of the class-predicting part relies on the fact that we know what
    digit classes the generated and real images belong to. The two heads of the model
    are compiled with different loss functions, reflecting the different nature of
    the two predictions. For the realness prediction, we use the binary cross-entropy
    loss, but for the digit-class prediction, we use the sparse categorical cross-entropy
    loss. You can see this in the following line from mnist-acgan/gan.js:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 10.13](#ch10fig13) 是 [figure 10.10](#ch10fig10) 的扩展版本。它展示了 ACGAN 的鉴别器部分的训练。与之前相比，这一训练步骤不仅提高了鉴别器区分真实和生成（伪造）图像的能力，还磨练了其确定给定图像（包括真实和生成的图像）属于哪个数字类别的能力。为了更容易与之前的简单图表进行比较，我们将已在
    [figure 10.10](#ch10fig10) 中看到的部分灰暗显示，并突出显示新的部分。首先，注意到生成器现在有了一个额外的输入（数字类别），这使得指定生成器应该生成什么数字成为可能。此外，鉴别器不仅输出真实性预测，还输出数字类别预测。因此，鉴别器的两个输出头都需要进行训练。对于真实性预测的训练与之前相同（[figure
    10.10](#ch10fig10)）；类别预测部分的训练依赖于我们知道生成和真实图像属于哪些数字类别。模型的两个头部编译了不同的损失函数，反映了两种预测的不同性质。对于真实性预测，我们使用二元交叉熵损失，但对于数字类别预测，我们使用了稀疏分类交叉熵损失。你可以在
    mnist-acgan/gan.js 的下一行中看到这一点：'
- en: '[PRE24]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Figure 10.13\. A schematic diagram illustrating the algorithm by which the discriminator
    part of ACGAN is trained. This diagram adds to the one in [figure 10.10](#ch10fig10)
    by showing the parts that have to do with the digit class. The remaining parts
    of the diagram, which have already appeared in [figure 10.10](#ch10fig10), are
    grayed out.
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.13\. 说明 ACGAN 的鉴别器部分是如何训练的示意图。这个图表在 [figure 10.10](#ch10fig10) 的基础上添加了与数字类别相关的部分。图表的其余部分已经在
    [figure 10.10](#ch10fig10) 中出现，并且被灰暗显示。
- en: '![](10fig12_alt.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig12_alt.jpg)'
- en: As the two curved arrows in [figure 10.13](#ch10fig13) show, the gradients backpropagated
    from both losses are added on top of each other when updating the discriminator’s
    weights. [Figure 10.14](#ch10fig14) is an expanded version of [figure 10.11](#ch10fig11)
    and provides a detailed schematic view of how ACGAN’s generator portion is trained.
    This diagram shows how the generator learns to generate correct images given a
    specified digit class, in addition to learning how to generate real-looking images.
    Similar to [figure 10.13](#ch10fig13), the new parts are highlighted, while the
    parts that already exist in [figure 10.11](#ch10fig11) are grayed out. From the
    highlighted parts, you can see that the labels we feed into the training step
    now include not only the realness labels but also the digit-class labels. As before,
    the realness labels are all intentionally bogus. But the newly added digit-class
    labels are more honest, in the sense that we indeed gave these class labels to
    the generator.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图10.13](#ch10fig13) 中的两条弯曲箭头所示，当更新鉴别器的权重时，通过反向传播的梯度会相互叠加。[图10.14](#ch10fig14)
    是 [图10.11](#ch10fig11) 的扩展版本，提供了 ACGAN 生成器部分训练的详细示意图。该图显示了生成器学习如何根据指定的数字类别生成正确的图像，以及学习如何生成真实的图像。与
    [图10.13](#ch10fig13) 类似，新添加的部分被突出显示，而已经存在于 [图10.11](#ch10fig11) 的部分则被隐藏。从突出显示的部分中，可以看到我们在训练步骤中输入的标签现在不仅包括真实性标签，还包括数字类别标签。与以前一样，真实性标签都是故意虚假的。但是新添加的数字类别标签更加真实，因为我们确实将这些类别标签给了生成器。
- en: Figure 10.14\. A schematic diagram illustrating the algorithm by which the generator
    part of ACGAN is trained. This diagram adds to the one in [figure 10.11](#ch10fig11)
    by showing the parts that have to do with the digit class. The remaining parts
    of the diagram, which have already appeared in [figure 10.11](#ch10fig11), are
    grayed out.
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.14. 示意图，说明 ACGAN 的生成器部分是如何训练的。这个图是 [图10.11](#ch10fig11) 的扩展，显示了与数字类别相关的部分。图的其余部分已经在[图10.11](#ch10fig11)
    中出现，已被隐藏。
- en: '![](10fig13_alt.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig13_alt.jpg)'
- en: Previously, we’ve seen that any discrepancies between the bogus realness labels
    and the discriminator’s realness probability output are used to update the generator
    of ACGAN in a way that makes it better at “fooling” the discriminator. Here, the
    digit-class prediction from the discriminator plays a similar role. For instance,
    if we tell the generator to generate an image for the digit 8, but the discriminator
    classifies the image as 9, the value of the sparse categorical cross entropy will
    be high, and the gradients associated with it will have large magnitudes. As a
    result, the updates to the generator’s weights will cause the generator to generate
    an image that looks more like an 8 (according to the discriminator.) Obviously,
    this way of training the generator will work only if the discriminator is sufficiently
    good at classifying images into the 10 MNIST digit classes. This is what the previous
    discriminator training step helps to ensure. Again, we are seeing the yin-and-yang
    dynamics between the discriminator and generator portions at play during the training
    of ACGAN.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 先前，我们看到虚假真实标签与鉴别器的真实概率输出之间的任何差异会被用来更新 ACGAN 的生成器，使其在“欺骗”鉴别器方面更加优秀。在这里，鉴别器的数字分类预测发挥了类似的作用。例如，如果我们告诉生成器生成一个数字8的图像，但是鉴别器将图像分类为9，则稀疏分类交叉熵的值将较高，并且与之关联的梯度将有较大的幅度。因此，生成器权重的更新将导致生成器生成一个更像数字8的图像（根据鉴别器的判断）。显然，只有当鉴别器在将图像分类为10个
    MNIST 数字类别方面足够好时，这种训练生成器的方法才会起作用。这就是前一个鉴别器训练步骤所帮助确保的。再次强调，在 ACGAN 的训练过程中，我们看到了鉴别器和生成器部分之间的阴阳动力学。
- en: 'GAN training: A bag of tricks'
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GAN 训练：一大堆诡计
- en: 'The process of training and tuning GANs is notoriously difficult. The training
    scripts you see in the mnist-acgan example are the crystallization of a tremendous
    amount of trial-and-error by researchers. Like most things in deep learning, it’s
    more like an art than an exact science: these tricks are heuristics, not backed
    by systematic theories. They are supported by a level of intuitive understanding
    of the phenomenon at hand, and they are known to work well empirically, although
    not necessarily in every situation.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和调整GAN的过程众所周知地困难。您在mnist-acgan示例中看到的训练脚本是研究人员大量试错的结晶。像深度学习中的大多数事物一样，这更像是一种艺术而不是精确科学：这些技巧是启发式的，没有系统理论的支持。它们得到了对手头现象的直觉理解，并且在经验上被证明效果良好，尽管不一定在每种情况下都有效。
- en: 'The following is a list of noteworthy tricks used in the ACGAN in this section:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本节中ACGAN中使用的一些值得注意的技巧列表：
- en: We use tanh as the activation of the last conv2dTranspose layer in the generator.
    The tanh activation is seen less frequently in other types of models.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在生成器的最后一个conv2dTranspose层中使用tanh作为激活函数。在其他类型的模型中，tanh激活函数出现得较少。
- en: 'Randomness is good for inducing robustness. Because GAN training may result
    in a dynamic equilibrium, GANs are prone to getting stuck in all sorts of ways.
    Introducing randomness during training helps prevent this. We introduce randomness
    in two ways: by using dropout in the discriminator and by using a “soft one” value
    (0.95) for the realness labels for the discriminator.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机性有助于诱导鲁棒性。因为GAN的训练可能导致动态平衡，所以GAN很容易陷入各种各样的困境中。在训练过程中引入随机性有助于防止这种情况发生。我们通过两种方式引入随机性：在鉴别器中使用dropout，以及为鉴别器的真实标签使用“soft
    one”值（0.95）。
- en: 'Sparse gradients (gradients in which many values are zero) can hinder GAN training.
    In other types of deep learning, sparsity is often a desirable property, but not
    so in GANs. Two things can cause sparsity in gradients: the max pooling operation
    and relu activations. Instead of max pooling, strided convolutions are recommended
    for downsampling, which is exactly what’s shown in the generator-creating code
    in [listing 10.10](#ch10ex10). Instead of the usual relu activation, it’s recommended
    to use the leakyReLU activation, of which the negative part has a small negative
    value, instead of strictly zero. This is also shown in [listing 10.10](#ch10ex10).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏梯度（许多值为零的梯度）可能会妨碍GAN的训练。在其他类型的深度学习中，稀疏性通常是一种理想的特性，但在GAN中不是这样。梯度中的稀疏性可能由两个因素引起：最大池化操作和relu激活函数。建议使用步幅卷积进行下采样，而不是最大池化，这正是生成器创建代码中所示的内容。建议使用leakyReLU激活函数，其中负部分具有小的负值，而不是严格的零。这也在[清单10.10](#ch10ex10)中显示。
- en: 10.3.4\. Seeing the MNIST ACGAN training and generation
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.4\. 查看MNIST ACGAN训练和生成
- en: 'The mnist-acgan example can be checked out and prepared with the following
    commands:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: mnist-acgan示例可以通过以下命令检出和准备：
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Running the example involves two stages: training in Node.js and generation
    in the browser. To start the training process, simply use the following command:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 运行示例涉及两个阶段：在Node.js中进行训练，然后在浏览器中进行生成。要启动训练过程，只需使用以下命令：
- en: '[PRE26]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The training uses tfjs-node by default. However, like in the examples involving
    convnets we’ve seen before, using tfjs-node-gpu can significantly improve the
    training speed. If you have a CUDA-enabled GPU set up properly on your machine,
    you can append the `--gpu` flag to the `yarn train` command to achieve that. Training
    the ACGAN takes at least a couple of hours. For this long-running training job,
    you can monitor the progress with TensorBoard by using the `--logDir` flag:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 训练默认使用tfjs-node。然而，像我们之前见过的涉及卷积神经网络的示例一样，使用tfjs-node-gpu可以显著提高训练速度。如果您的计算机上正确设置了支持CUDA的GPU，您可以在`yarn
    train`命令中追加`--gpu`标志来实现。训练ACGAN至少需要几个小时。对于这个长时间运行的训练任务，您可以使用`--logDir`标志通过TensorBoard监控进度：
- en: '[PRE27]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Once the TensorBoard process has been brought up with the following command
    in a separate terminal,
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在单独的终端中使用以下命令启动了TensorBoard进程，
- en: '[PRE28]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: you can navigate to the TensorBoard URL (as printed out by the TensorBoard server
    process) in your browser to look at the loss curves. [Figure 10.15](#ch10fig15)
    shows some example loss curves from the training process. One distinct feature
    of loss curves from GAN training is the fact that they don’t always trend downward
    like the loss curves of most other types of neural networks. Instead, the losses
    from the discriminator (dLoss in the figure) and the generator (gLoss in the figure)
    both change in nonmonotonic ways and form an intricate dance with one another.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在浏览器中导航到 TensorBoard URL（由 TensorBoard 服务器进程打印）以查看损失曲线。[图 10.15](#ch10fig15)
    显示了训练过程中的一些示例损失曲线。GAN 训练的损失曲线的一个显著特征是，它们并不总是像大多数其他类型的神经网络的损失曲线那样趋向于下降。相反，判别器的损失（图中的
    dLoss）和生成器的损失（图中的 gLoss）都以非单调方式变化，并相互交织形成复杂的舞蹈。
- en: Figure 10.15\. Sample loss curves from the ACGAN training job. dLoss is the
    loss from the discriminator training step. Specifically, it is the sum of the
    binary cross entropy from the realness prediction and the sparse categorical cross
    entropy from the digit-class prediction. gLoss is the loss from the generator
    training step. Like dLoss, gLoss is the sum of the losses from the binary realness
    classification and the multiclass digit classification.
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.15\. ACGAN 训练作业中的样本损失曲线。dLoss 是判别器训练步骤的损失。具体来说，它是真实性预测的二元交叉熵和数字类别预测的稀疏分类交叉熵的总和。gLoss
    是生成器训练步骤的损失。与 dLoss 类似，gLoss 是来自二元真实性分类和多类数字分类的损失的总和。
- en: '![](10fig14_alt.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig14_alt.jpg)'
- en: 'Toward the end of the training, neither loss gets close to zero. Instead, they
    just level out (converge). At that point, the training process ends and saves
    the generator part of the model to the disk for serving during the in-browser
    generation step:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练接近结束时，两者的损失都不会接近零。相反，它们只是趋于平稳（收敛）。此时，训练过程结束并将模型的生成器部分保存到磁盘上，以便在浏览器内生成步骤中进行服务：
- en: '[PRE29]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: To run the in-browser generation demo, use the command `yarn watch`. It will
    compile mnist-acgan/index.js and the associated HTML and CSS assets, after which
    it will pop open a tab in your browser and show the demo page.^([[21](#ch10fn21)])
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行浏览器内生成演示，请使用命令 `yarn watch`。它将编译 mnist-acgan/index.js 和相关的 HTML 和 CSS 资源，然后会在您的浏览器中打开一个标签页并显示演示页面。^([[21](#ch10fn21)])
- en: ^(21)
  id: totrans-305
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(21)
- en: ''
  id: totrans-306
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can also skip the training and building step entirely and directly navigate
    to the hosted demo page at [http://mng.bz/4eGw](http://mng.bz/4eGw).
  id: totrans-307
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您还可以完全跳过训练和构建步骤，直接导航到托管的演示页面，网址为 [http://mng.bz/4eGw](http://mng.bz/4eGw)。
- en: 'The demo page loads the trained ACGAN generator saved from the previous stage.
    Since the discriminator is not really useful for this demo stage, it is neither
    saved nor loaded. With the generator loaded, we can construct a batch of latent
    vectors, along with a batch of desired digit-class indices, and call the generator’s
    `predict()` with them. The code that does this is in mnist-acgan/index.js:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 演示页面加载了从前一阶段保存的训练好的 ACGAN 生成器。由于判别器在此演示阶段并不真正有用，因此它既不保存也不加载。有了生成器加载后，我们可以构建一批潜在向量，以及一批期望的数字类别索引，并调用生成器的
    `predict()`。执行此操作的代码位于 mnist-acgan/index.js 中：
- en: '[PRE30]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our batch of digit-class labels is always an ordered vector of 10 elements,
    from 0 to 9\. This is why the batch of generated images is always an orderly array
    of images from 0 to 9\. These images are stitched together with the `tf.concat()`
    function and rendered in a `div` element on the page (see the top image in [figure
    10.16](#ch10fig16)). Compared with randomly sampled real MNIST images (see the
    bottom image in [figure 10.16](#ch10fig16)), these ACGAN-generated images look
    just like the real ones. In addition, their digit-class identities look correct.
    This shows that our ACGAN training was successful. If you want to see more outputs
    from the ACGAN generator, click the Generator button on the page. Each time the
    button is clicked, a new batch of 10 fake images will be generated and shown on
    the page. You can play with that and get an intuitive sense of the quality of
    the image generation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数字类别标签批次始终是一个有序的 10 元素向量，从 0 到 9。这就是为什么生成的图像批次总是一个从 0 到 9 的有序图像数组。这些图像使用
    `tf.concat()` 函数拼接在一起，并在页面上的 `div` 元素中呈现（参见[图 10.16](#ch10fig16) 中的顶部图像）。与随机抽样的真实
    MNIST 图像（参见[图 10.16](#ch10fig16) 中的底部图像）相比，这些 ACGAN 生成的图像看起来就像真实的一样。此外，它们的数字类别身份看起来是正确的。这表明我们的
    ACGAN 训练是成功的。如果您想查看 ACGAN 生成器的更多输出，请点击页面上的 Generator 按钮。每次点击按钮，都会生成并显示页面上的新批次包含
    10 张假图像。您可以玩一下，直观地感受图像生成的质量。
- en: Figure 10.16\. Sample generated images (the 10 x 1 top panel) from the generator
    part of a trained ACGAN. The bottom panel, which contains a 10 x 10 grid of real
    MNIST images, is shown for comparison. By clicking the Show Z-vector Sliders button,
    you can open a section filled with 100 sliders. These sliders allow you to change
    the elements of the latent vector (the z- vector) and observe the effects on the
    generated MNIST images. Note that if you change the sliders one at a time, most
    of them will have tiny and unnoticeable effects on the images. But occasionally,
    you’ll be able to find a slider with a larger and more noticeable effect.
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.16\. ACGAN 训练模型的生成器部分生成的样本图片（顶部的 10 x 1 面板）。底部的面板展示了一个 10 x 10 的真实 MNIST
    图像网格，以进行比较。点击“显示 Z-向量滑块”按钮，您可以打开一个填满了 100 个滑块的区域。这些滑块允许您改变潜在向量（z-向量）的元素，并观察其对生成的
    MNIST 图像的影响。请注意，如果您逐个更改滑块，大多数滑块对图像的影响都很微小且不易察觉。但偶尔，您会发现一个具有更大且更明显影响的滑块。
- en: '![](10fig15_alt.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig15_alt.jpg)'
- en: Materials for further reading
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步阅读材料
- en: Ian Goodfellow, Yoshua Bengio, and Aaron Courville, “Deep Generative Models,”
    *Deep Learning*, chapter 20, MIT Press, 2017.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ian Goodfellow、Yoshua Bengio 和 Aaron Courville，“深度生成模型”，*深度学习*，第 20 章，麻省理工学院出版社，2017
    年。
- en: 'Jakub Langr and Vladimir Bok, *GANs in Action: Deep Learning with Generative
    Adversarial Networks*, Manning Publications, 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jakub Langr 和 Vladimir Bok，《GAN 行动中：生成对抗网络的深度学习》，Manning 出版社，2019 年。
- en: Andrej Karpathy, “The Unreasonable Effectiveness of Recurrent Neural Networks,”
    blog, 21 May 2015, [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy，“循环神经网络的不合理有效性”，博客，2015 年 5 月 21 日，[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。
- en: Jonathan Hui, “GAN—What is Generative Adversary Networks GAN?” Medium, 19 June
    2018, [http://mng.bz/Q0N6](http://mng.bz/Q0N6).
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jonathan Hui，“GAN—什么是生成对抗网络 GAN？” Medium，2018 年 6 月 19 日，[http://mng.bz/Q0N6](http://mng.bz/Q0N6)。
- en: 'GAN Lab, an interactive, web-based environment for understanding and exploring
    how GANs work, built using TensorFlow.js: Minsuk Kahng et al., [https://poloclub.github.io/ganlab/](https://poloclub.github.io/ganlab/).'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN 实验室，一个用 TensorFlow.js 构建的交互式网络环境，用于理解和探索 GAN 的工作原理：Minsuk Kahng 等人，[https://poloclub.github.io/ganlab/](https://poloclub.github.io/ganlab/)。
- en: Exercises
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: 'Apart from the Shakespeare text corpus, the lstm-text-generation example has
    a few other text datasets configured and ready for you to explore. Run the training
    on them, and observe the effects. For instance, use the unminified TensorFlow.js
    code as the training dataset. During and after the model’s training, observe if
    the generated text exhibits the following patterns of JavaScript source code and
    how the temperature parameter affects the patterns:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了莎士比亚文本语料库外，lstm-text-generation 示例还配置了其他几个文本数据集，并准备好供您探索。运行它们的训练，并观察其效果。例如，使用未压缩的
    TensorFlow.js 代码作为训练数据集。在模型训练期间和之后，观察生成的文本是否表现出以下 JavaScript 源代码的模式以及温度参数如何影响这些模式：
- en: Shorter-range patterns such as keywords (for example, “for” and “function”)
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较短程模式，例如关键字（例如，“for”和“function”）
- en: Medium-range patterns such as the line-by-line organization of the code
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 中程模式，例如代码的逐行组织
- en: Longer-range patterns such as pairing of parentheses and square brackets, and
    the fact that each “function” keyword must be followed by a pair of parentheses
    and a pair of curly braces
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较长程模式，例如括号和方括号的配对，以及每个“function”关键字后必须跟着一对括号和一对花括号
- en: In the fashion-mnist-vae example, what happens if you take the KL divergence
    term out of the VAE’s custom loss? Test that by modifying the `vaeLoss()` function
    in fashion-mnist-vae/model.js ([listing 10.7](#ch10ex07)). Do the sampled images
    from the latent space still look like the Fashion-MNIST images? Does the space
    still exhibit any interpretable patterns?
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 fashion-mnist-vae 示例中，如果您将 VAE 的自定义损失中的 KL 散度项删除会发生什么？通过修改 fashion-mnist-vae/model.js
    中的 `vaeLoss()` 函数（[清单 10.7](#ch10ex07)）来测试。从潜在空间采样的图像是否仍然看起来像 Fashion-MNIST 图像？空间是否仍然展现出可解释的模式？
- en: 'In the mnist-acgan example, try collapsing the 10 digit classes into 5 (0 and
    1 will become the first class, 2 and 3 the second class, and so forth), and observe
    how that changes the output of the ACGAN after training. What do you expect to
    see in the generated images? For instance, what do you expect the ACGAN to generate
    when you specify that the first class is desired? Hint: to make this change, you
    need to modify the `loadLabels()` function in mnist-acgan/data.js. The constant
    `NUM_CLASSES` in gan.js needs to be modified accordingly. In addition, the `sampledLabels`
    variable in the `generateAnd-VisualizeImages()` function (in index.js) also needs
    to be revised.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 mnist-acgan 示例中，尝试将 10 个数字类别合并为 5 个（0 和 1 将成为第一类，2 和 3 将成为第二类，依此类推），并观察在训练后这如何改变
    ACGAN 的输出。当您指定第一类时，您期望看到生成的图像是什么？例如，当您指定第一类时，您期望 ACGAN 生成什么？提示：要进行此更改，您需要修改 mnist-acgan/data.js
    中的 `loadLabels()` 函数。需要相应修改 gan.js 中的常量 `NUM_CLASSES`。此外，`generateAnd-VisualizeImages()`
    函数（位于 index.js 中）中的 `sampledLabels` 变量也需要修改。
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: Generative models are different from the discriminative ones we’ve studied throughout
    earlier chapters of this book in that they are designed to model the process in
    which examples of the training dataset are generated, along with their statistical
    distributions. Due to this design, they are capable of generating new examples
    that conform to the distributions and hence appear similar to the real training
    data.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型与我们在本书早期章节中学习的判别模型不同，因为它们旨在模拟训练数据集的生成过程，以及它们的统计分布。由于这种设计，它们能够生成符合分布并且看起来类似于真实训练数据的新样本。
- en: 'We introduce one way to model the structure of text datasets: next-character
    prediction. LSTMs can be used to perform this task in an iterative fashion to
    generate text of arbitrary length. The temperature parameter controls the stochasticity
    (how random and unpredictable) the generated text is.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了一种模拟文本数据集结构的方法：下一个字符预测。LSTM 可以用来以迭代方式执行此任务，以生成任意长度的文本。温度参数控制生成文本的随机性（多么随机和不可预测）。
- en: Autoencoders are a type of generative model that consists of an encoder and
    a decoder. First, the encoder compresses the input data into a concise representation
    called the latent vector, or z-vector. Then, the decoder tries to reconstruct
    the input data by using just the latent vector. Through the training process,
    the encoder becomes an efficient data summarizer, and the decoder is endowed with
    knowledge of the statistical distribution of the examples. A VAE adds some additional
    statistical constraints on the latent vectors so that the latent spaces comprising
    those vectors display continuously varying and interpretable structures after
    the VAE is trained.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器是一种由编码器和解码器组成的生成模型。首先，编码器将输入数据压缩为称为潜在向量或 z-向量的简明表示。然后，解码器尝试仅使用潜在向量来重构输入数据。通过训练过程，编码器变成了一个高效的数据摘要生成器，解码器则具有对示例的统计分布的知识。VAE
    对潜在向量添加了一些额外的统计约束，使得在 VAE 训练后组成这些向量的潜在空间显示出连续变化和可解释的结构。
- en: GANs are based on the idea of a simultaneous competition and cooperation between
    a discriminator and a generator. The discriminator tries to distinguish real data
    examples from the generated ones, while the generator aims at generating fake
    examples that “fool” the discriminator. Through joint training, the generator
    part will eventually become capable of generating realistic-looking examples.
    An ACGAN adds class information to the basic GAN architecture to make it possible
    to specify what class of examples to generate.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN 基于鉴别器和生成器之间的竞争和合作的想法。鉴别器试图区分真实数据示例和生成的数据示例，而生成器旨在生成“欺骗”鉴别器的虚假示例。通过联合训练，生成器部分最终将能够生成逼真的示例。ACGAN
    在基本 GAN 架构中添加了类信息，以便指定要生成的示例的类别。
