- en: 8 Introduction to deep learning for computer vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 计算机视觉的深度学习介绍
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容包括*'
- en: Understanding convolutional neural networks (convnets)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积神经网络（卷积网络）
- en: Using data augmentation to mitigate overfitting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据增强来减轻过拟合
- en: Using a pretrained convnet to do feature extraction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的卷积网络进行特征提取
- en: Fine-tuning a pretrained convnet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对预训练的卷积网络进行微调
- en: Computer vision is the earliest and biggest success story of deep learning.
    Every day, you’re interacting with deep vision models—via Google Photos, Google
    image search, YouTube, video filters in camera apps, OCR software, and many more.
    These models are also at the heart of cutting-edge research in autonomous driving,
    robotics, AI-assisted medical diagnosis, autonomous retail checkout systems, and
    even autonomous farming.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是深度学习的最早和最大的成功故事。每天，您都在通过Google照片、Google图像搜索、YouTube、相机应用中的视频滤镜、OCR软件等与深度视觉模型进行交互。这些模型还是自动驾驶、机器人技术、AI辅助医学诊断、自动零售结账系统甚至自动农业等尖端研究的核心。
- en: Computer vision is the problem domain that led to the initial rise of deep learning
    between 2011 and 2015\. A type of deep learning model called *convolutional neural
    networks* started getting remarkably good results on image-classification competitions
    around that time, first with Dan Ciresan winning two niche competitions (the ICDAR
    2011 Chinese character recognition competition and the IJCNN 2011 German traffic
    signs recognition competition), and then more notably in fall 2012 with Hinton’s
    group winning the high-profile ImageNet large-scale visual recognition challenge.
    Many more promising results quickly started bubbling up in other computer vision
    tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是在2011年至2015年间深度学习初期崛起的问题领域。一种称为*卷积神经网络*的深度学习模型开始在那个时候在图像分类竞赛中取得了非常好的成绩，首先是丹·西雷赛安（Dan
    Ciresan）在两个小众竞赛中获胜（ICDAR 2011年中文字符识别竞赛和IJCNN 2011年德国交通标志识别竞赛），然后在2012年秋季更为显著，辛顿（Hinton）的团队赢得了备受瞩目的ImageNet大规模视觉识别挑战赛。很快，许多更有前景的成果开始涌现在其他计算机视觉任务中。
- en: Interestingly, these early successes weren’t quite enough to make deep learning
    mainstream at the time—it took a few years. The computer vision research community
    had spent many years investing in methods other than neural networks, and it wasn’t
    quite ready to give up on them just because there was a new kid on the block.
    In 2013 and 2014, deep learning still faced intense skepticism from many senior
    computer vision researchers. It was only in 2016 that it finally became dominant.
    I (François) remember exhorting an ex-professor of mine, in February 2014, to
    pivot to deep learning. “It’s the next big thing!” I would say. “Well, maybe it’s
    just a fad,” he replied. By 2016, his entire lab was doing deep learning. There’s
    no stopping an idea whose time has come.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这些早期的成功并没有让深度学习在当时成为主流——这需要几年的时间。计算机视觉研究界花了很多年投资于除了神经网络之外的方法，它们并不完全准备放弃它们，只因为有了新的玩家。在2013年和2014年，深度学习仍然面临着来自许多资深计算机视觉研究人员的激烈质疑。直到2016年，它才最终占据主导地位。我（弗朗索瓦）还记得在2014年2月劝告我的一位前教授转向深度学习。“这是下一个大事！”我会说。“嗯，也许它只是一时的热门话题，”他回答道。到了2016年，他的整个实验室都在进行深度学习。一个时代到来的理念是无法阻挡的。
- en: This chapter introduces convolutional neural networks, also known as *convnet*,
    the type of deep learning model that is now used almost universally in computer
    vision applications. You’ll learn to apply convnets to image-classification problems—in
    particular those involving small training datasets, which are the most common
    use case if you aren’t a large tech company.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了卷积神经网络，也称为*卷积网络*，这种类型的深度学习模型现在几乎在计算机视觉应用中被普遍使用。您将学习将卷积网络应用于图像分类问题，特别是那些涉及小训练数据集的问题，如果您不是一家大型技术公司，则这是最常见的用例。
- en: 8.1 Introduction to convnets
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 卷积网络简介
- en: We’re about to dive into the theory of what convnets are and why they have been
    so successful at computer vision tasks. But first, let’s take a practical look
    at a simple convnet example that classifies MNIST digits, a task we performed
    in chapter 2 using a densely connected network (our test accuracy then was 97.8%).
    Even though the convnet will be basic, its accuracy will blow our densely connected
    model from chapter 2 out of the water.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将深入探讨卷积网络是什么以及为什么它们在计算机视觉任务中取得如此成功的理论。但首先，让我们以一个简单的卷积网络示例来实际了解一下，该示例对MNIST数字进行分类，这是我们在第2章中使用全连接网络执行的任务（当时我们的测试准确率为97.8%）。尽管卷积网络将是基本的，但其准确性将远远超出我们第2章中的全连接模型。
- en: The following listing shows what a basic convnet looks like. It’s a stack of
    layer_ conv_2d() and layer_max_pooling_2d() layers. You’ll see in a minute exactly
    what they do. We’ll build the model using the Functional API, which we introduced
    in the previous chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了一个基本卷积神经网络的外观。它是一堆`layer_conv_2d()`和`layer_max_pooling_2d()`层。你很快就会明白它们的作用。我们将使用我们在上一章介绍的
    Functional API 来构建模型。
- en: Listing 8.1 Instantiating a small convnet
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.1 实例化一个小型卷积神经网络](https://example.org) '
- en: inputs <— layer_input(shape = c(28, 28, 1))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputs <— layer_input(shape = c(28, 28, 1))`'
- en: outputs <— inputs %>%
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`outputs <— inputs` %>%'
- en: layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu")` %>%'
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_max_pooling_2d(pool_size = 2)` %>%'
- en: layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu")` %>%'
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_max_pooling_2d(pool_size = 2)` %>%'
- en: layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu")` %>%'
- en: layer_flatten() %>% layer_dense(10, activation = "softmax")
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_flatten() %>% layer_dense(10, activation = "softmax")`'
- en: model <— keras_model(inputs, outputs)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`model <— keras_model(inputs, outputs)`'
- en: Importantly, a convnet takes as input tensors of shape (image_height, image_width,
    image_channels), not including the batch dimension. In this case, we’ll configure
    the convnet to process inputs of size (28, 28, 1), which is the format of MNIST
    images.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，卷积神经网络的输入是形状为（图像高度、图像宽度、图像通道数）的张量，不包括批处理维度。在这种情况下，我们将配置卷积神经网络以处理大小为（28、28、1）的输入，这是
    MNIST 图像的格式。
- en: Let’s display the architecture of our convnet.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示我们卷积神经网络的结构。
- en: Listing 8.2 Displaying the model’s summary
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.2 显示模型摘要](https://example.org)'
- en: model
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`'
- en: '![Image](../images/f0222-01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0222-01.jpg)'
- en: You can see that the output of every Conv2D and MaxPooling2D layer is a rank
    3 tensor of shape (height, width, channels). The width and height dimensions tend
    to shrink as you go deeper in the model. The number of channels is controlled
    by the first argument passed to the layer_conv_2d() layers (32, 64, or 128).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到每个 Conv2D 和 MaxPooling2D 层的输出都是形状为（高度，宽度，通道数）的秩为 3 的张量。随着模型的加深，宽度和高度维度会变小。通道的数量由传递给`layer_conv_2d()`层的第一个参数（32、64
    或 128）来控制。
- en: 'After the last Conv2D layer, we end up with an output of shape (3, 3, 128)—a
    3 × 3 feature map of 128 channels. The next step is to feed this output into a
    densely connected classifier like those you’re already familiar with: a stack
    of Dense layers. These classifiers process vectors, which are 1D, whereas the
    current output is a rank 3 tensor. To bridge the gap, we flatten the 3D outputs
    to 1D with a Flatten layer before adding the Dense layers. Finally, we do 10-way
    classification, so our last layer has 10 outputs and a softmax activation.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个 Conv2D 层之后，我们得到形状为（3、3、128）的输出，即一个 3 × 3 的具有 128 个通道的特征图。下一步是将此输出馈送到一个密集连接的分类器中，就像您已经熟悉的那样：一堆
    Dense 层。这些分类器处理向量，这是 1D 的，而当前的输出是一个秩为 3 的张量。为了弥合差距，我们使用 Flatten 层将 3D 输出展平为 1D，然后再添加
    Dense 层。最后，我们进行 10 分类，因此我们的最后一层有 10 个输出和 softmax 激活。
- en: Now, let’s train the convnet on the MNIST digits. We’ll reuse a lot of the code
    from the MNIST example in chapter 2\. Because we’re doing 10-way classification
    with a softmax output, we’ll use the categorical cross-entropy loss, and because
    our labels are integers, we’ll use the sparse version, sparse_categorical_crossentropy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 MNIST 数字上训练卷积神经网络。我们将重用第 2 章中 MNIST 示例中的大量代码。因为我们要进行 10 分类，并带有 softmax
    输出，所以我们将使用分类交叉熵损失，因为我们的标签是整数，所以我们将使用稀疏版本的稀疏分类交叉熵。
- en: Listing 8.3 Training the convnet on MNIST images
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.3 在 MNIST 图像上训练卷积神经网络](https://example.org)'
- en: c(c(train_images, train_labels), c(test_images, test_labels)) %<—%
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`c(c(train_images, train_labels), c(test_images, test_labels))` `%<—%`'
- en: dataset_mnist()
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset_mnist()`'
- en: train_images <— array_reshape(train_images, c(60000, 28, 28, 1)) / 255
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_images <— array_reshape(train_images, c(60000, 28, 28, 1)) / 255`'
- en: test_images <— array_reshape(test_images, c(10000, 28, 28, 1)) / 255
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_images <— array_reshape(test_images, c(10000, 28, 28, 1)) / 255`'
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`model %>% compile(optimizer = "rmsprop",`'
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss = "sparse_categorical_crossentropy"`'
- en: metrics = c("accuracy"))
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`metrics = c("accuracy"))`'
- en: model %>% fit(train_images, train_labels, epochs = 5, batch_size = 64)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`model %>% fit(train_images, train_labels, epochs = 5, batch_size = 64)`'
- en: Let’s evaluate the model on the test data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试数据上评估模型。
- en: Listing 8.4 Evaluating the convnet
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.4 评估卷积神经网络](https://example.org)'
- en: result <— evaluate(model, test_images, test_labels)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`result <— evaluate(model, test_images, test_labels)`'
- en: cat("Test accuracy:", result['accuracy'], "\n")
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`cat("测试准确率:", result[''accuracy''], "\n")`'
- en: 'Test accuracy: 0.9915'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率：0.9915
- en: 'Whereas the densely connected model from chapter 2 had a test accuracy of 97.8%,
    the basic convnet has a test accuracy of 99.1%: we decreased the error rate by
    about 60% (relative). Not bad!'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 而第2章中的密集连接模型的测试准确率为97.8%，基本卷积神经网络的测试准确率为99.1%：我们将错误率减少了约60%（相对）。不错！
- en: Why does this simple convnet work so well, compared to a densely connected model?
    To answer this, let’s dive into what the Conv2D and MaxPooling2D layers do.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个简单的卷积神经网络效果这么好，相比之下，与密集连接模型相比如此？为了回答这个问题，让我们深入了解一下Conv2D和MaxPooling2D层的作用。
- en: 8.1.1 The convolution operation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 卷积操作
- en: 'The fundamental difference between a densely connected layer and a convolution
    layer is this: Dense layers learn global patterns in their input feature space
    (e.g., for a MNIST digit, patterns involving all pixels), whereas convolution
    layers learn local patterns—in the case of images, patterns found in small 2D
    windows of the inputs (see [figure 8.1](#fig8-1)). In the previous example, these
    windows were all 3 × 3.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 密集连接层和卷积层之间的根本区别在于：密集层学习其输入特征空间中的全局模式（例如，对于MNIST数字，涉及所有像素的模式），而卷积层学习局部模式——在图像的情况下，输入的小2D窗口中发现的模式（参见[图8.1](#fig8-1)）。在先前的示例中，这些窗口都是3×3。
- en: '![Image](../images/f0223-01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0223-01.jpg)'
- en: '**Figure 8.1 Images can be broken into local patterns such as edges, textures,
    and so on.**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.1 图像可以被分解为边缘、纹理等局部模式。**'
- en: 'This key characteristic gives convnets two interesting properties:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这一关键特性赋予了卷积神经网络两个有趣的特性：
- en: '*The patterns they learn are translation-invariant*—After learning a certain
    pattern in the lower-right corner of a picture, a convnet can recognize it anywhere—for
    example, in the upper-left corner. A densely connected model would have to learn
    the pattern anew if it appeared at a new location. This makes convnets data-efficient
    when processing images (because the *visual world is fundamentally translation
    invariant*): they need fewer training samples to learn representations that have
    generalization power.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它们学习的模式是平移不变的*——在图片的右下角学习了某种模式后，卷积神经网络可以在任何地方识别它——例如，在左上角。密集连接模型如果出现在新位置，就必须重新学习该模式。这使得处理图像时，卷积神经网络在数据效率上更具优势（因为*视觉世界在本质上是平移不变的*）：它们需要更少的训练样本来学习具有泛化能力的表示。'
- en: '*They can learn spatial hierarchies of patterns*—A first convolution layer
    will learn small local patterns such as edges, a second convolution layer will
    learn larger patterns made of the features of the first layers, and so on (see
    [figure 8.2](#fig8-2)). This allows convnets to efficiently learn increasingly
    complex and abstract visual concepts, because *the visual world is fundamentally
    spatially hierarchical*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它们可以学习空间模式的层次结构*——第一个卷积层将学习小的局部模式，例如边缘，第二个卷积层将学习由第一层特征组成的较大模式，依此类推（参见[图8.2](#fig8-2)）。这使得卷积神经网络能够高效地学习越来越复杂和抽象的视觉概念，因为*视觉世界在本质上是空间层次结构*。'
- en: '![Image](../images/f0224-01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0224-01.jpg)'
- en: '**Figure 8.2 The visual world forms a spatial hierarchy of visual modules:
    elementary lines or textures combine into simple objects such as eyes or ears,
    which combine into high-level concepts such as “cat.”**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.2 视觉世界形成了视觉模块的空间层次结构：基本线条或纹理结合成简单对象，如眼睛或耳朵，它们又结合成“猫”等高级概念。**'
- en: 'Convolutions operate over rank 3 tensors called *feature maps*, with two spatial
    axes (*height* and *width*) as well as a *depth* axis (also called the *channels*
    axis). For an RGB image, the dimension of the depth axis is 3, because the image
    has three color channels: red, green, and blue. For a black-and-white picture,
    like the MNIST digits, the depth is 1 (levels of gray). The convolution operation
    extracts patches from its input feature map and applies the same transformation
    to all of these patches, producing an *output feature map*. This output feature
    map is still a rank 3 tensor: it has a width and a height. Its depth can be arbitrary,
    because the output depth is a parameter of the layer, and the different channels
    in that depth axis no longer stand for specific colors as in RGB input; rather,
    they stand for *filters*. Filters encode specific aspects of the input data: at
    a high level, a single filter could encode the concept “presence of a face in
    the input,” for instance.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作在称为*特征图*的三阶张量上进行，具有两个空间轴（*高度*和*宽度*）以及一个*深度*轴（也称为*通道*轴）。对于 RGB 图像，深度轴的维数为
    3，因为图像具有三个颜色通道：红色、绿色和蓝色。对于黑白图片，如 MNIST 数字，深度为 1（灰度级）。卷积操作从其输入特征图中提取补丁，并对所有这些补丁应用相同的变换，生成一个*输出特征图*。此输出特征图仍然是一个三阶张量：它具有宽度和高度。它的深度可以是任意的，因为输出深度是层的一个参数，该深度轴中的不同通道不再代表
    RGB 输入中的特定颜色；相反，它们代表*滤波器*。滤波器编码输入数据的特定方面：在高层次上，单个滤波器可以编码“输入中存在面部”的概念，例如。
- en: 'In the MNIST example, the first convolution layer takes a feature map of size
    (28, 28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters
    over its input. Each of these 32 output channels contains a 26 × 26 grid of values,
    which is a *response map* of the filter over the input, indicating the response
    of that filter pattern at different locations in the input (see [figure 8.3](#fig8-3)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MNIST 示例中，第一卷积层接受尺寸为 (28, 28, 1) 的特征图，并输出尺寸为 (26, 26, 32) 的特征图：它在其输入上计算 32
    个滤波器。这 32 个输出通道中的每一个都包含一个 26 × 26 的值网格，这是该滤波器在输入上的*响应图*，指示了该滤波器模式在输入的不同位置的响应（参见[图
    8.3](#fig8-3)）。
- en: '![Image](../images/f0225-01.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0225-01.jpg)'
- en: '**Figure 8.3 The concept of a response map: A 2D map of the presence of a pattern
    at different locations in an input**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.3 响应图的概念：输入中不同位置的模式的 2D 地图**'
- en: 'That is what the term *feature map* means: every dimension in the depth axis
    is a *feature* (or filter), and the rank 2 tensor output[, , n] is the 2D spatial
    *map* of the response of this filter over the input.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是术语*特征图*的含义：深度轴中的每个维度都是一个*特征*（或滤波器），而二阶张量输出[, , n] 是此滤波器在输入上的 2D 空间*响应图*。
- en: 'Convolutions are defined by two key parameters:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积由两个关键参数定义：
- en: '*Size of the patches extracted from the inputs*—These are typically 3 × 3 or
    5 × 5\. In the example, they were 3 × 3, which is a common choice.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从输入中提取的补丁的大小* —— 这些通常是 3 × 3 或 5 × 5。在示例中，它们是 3 × 3，这是一个常见的选择。'
- en: '*Depth of the output feature map*—This is the number of filters computed by
    the convolution. The example started with a depth of 32 and ended with a depth
    of 64.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出特征图的深度* —— 这是由卷积计算的滤波器数量。示例从深度为 32 开始，最终达到了深度为 64。'
- en: 'In layer_conv_2d(), these parameters are the first arguments passed to the
    layer (after the inputs to compose with): inputs %>% layer_conv_2d(output_depth,
    c(window_ height, window_width)).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 layer_conv_2d() 中，这些参数是传递给层的第一个参数（与输入组合的参数之后）：inputs %>% layer_conv_2d(output_depth,
    c(window_ height, window_width))。
- en: A convolution works by *sliding* these windows of size 3 × 3 or 5 × 5 over the
    3D input feature map, stopping at every possible location, and extracting the
    3D patch of surrounding features (shape (window_height, window_width, input_depth)).
    Each such 3D patch is then transformed into a 1D vector of shape (output_depth),
    which is done via a tensor product with a learned weight matrix, called the *convolution
    kernel*—the same kernel is reused across every patch. All of these vectors (one
    per patch) are then spatially reassembled into a 3D output map of shape (height,
    width, output_ depth). Every spatial location in the output feature map corresponds
    to the same location in the input feature map (e.g., the lower-right corner of
    the output contains information about the lower-right corner of the input). For
    instance, with 3 × 3 windows, the vector output[i, j, ] comes from the 3D patch
    input[(i-1):(i+1), (j-1):(j+1), ]. The full process is detailed in [figure 8.4](#fig8-4).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的工作原理是通过*滑动*这些大小为 3 × 3 或 5 × 5 的窗口，遍历 3D 输入特征图，停留在每个可能的位置，并提取周围特征的 3D 补丁（形状为
    (window_height, window_width, input_depth)）。然后将每个这样的 3D 补丁转化为形状为 (output_depth)
    的 1D 向量，通过与一个称为*卷积核*的学习权重矩阵进行张量积操作——同一个卷积核会在每个补丁中重复使用。所有这些向量（每个补丁一个）然后会在空间上重新组合成一个形状为
    (height, width, output_depth) 的 3D 输出图。在输出特征图的每个空间位置都对应于输入特征图的相同位置（例如，输出的右下角包含了输入的右下角的信息）。例如，使用
    3 × 3 窗口，向量 output[i, j, ] 来自于 3D 补丁 input[(i-1):(i+1), (j-1):(j+1), ]。完整的过程详见[图
    8.4](#fig8-4)。
- en: '![Image](../images/f0226-01.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0226-01.jpg)'
- en: '**Figure 8.4 How convolution works.**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.4 卷积的工作原理。**'
- en: 'Note that the output width and height may differ from the input width and height
    for two reasons:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出的宽度和高度可能与输入的宽度和高度不同，原因有两个：
- en: Border effects, which can be countered by padding the input feature map
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界效应可以通过对输入特征图进行填充来抵消。
- en: The use of *strides*, which I’ll define in a secon
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将在一秒钟内定义*步幅*的使用。
- en: Let’s take a deeper look at these notions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解这些概念。
- en: UNDERSTANDING BORDER EFFECTS AND PADDING
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解边界效应和填充
- en: 'Consider a 5 × 5 feature map (25 tiles total). There are only 9 tiles around
    which you can center a 3 × 3 window, forming a 3 × 3 grid (see [figure 8.5](#fig8-5)).
    Hence, the output feature map will be 3 × 3\. It shrinks a little: by exactly
    two tiles alongside each dimension, in this case. You can see this border effect
    in action in the earlier example: you start with 28 × 28 inputs, which become
    26 × 26 after the first convolution layer.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 5 × 5 的特征图（总共 25 个格子）。你只能在其中 9 个格子周围居中 3 × 3 窗口，形成一个 3 × 3 的网格（见 [图 8.5](#fig8-5)）。因此，输出特征图将是
    3 × 3。它会略微缩小：在这种情况下，每个维度正好缩小两个格子。在早期的例子中，你可以看到这种边界效应：开始时输入是 28 × 28，在第一卷积层之后变成了
    26 × 26。
- en: If you want to get an output feature map with the same spatial dimensions as
    the input, you can use *padding*. Padding consists of adding an appropriate number
    of rows and columns on each side of the input feature map so as to make it possible
    to fit center convolution windows around every input tile. For a 3 × 3 window,
    you add one column on the right, one column on the left, one row at the top, and
    one row at the bottom. For a 5 × 5 window, you add two rows (see [figure 8.6](#fig8-6)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想获得与输入相同空间维度的输出特征图，您可以使用*填充*。填充包括在输入特征图的每一侧添加适当数量的行和列，以便在每个输入格子周围可以放置中心卷积窗口。对于
    3 × 3 窗口，您需要在右边添加一列，左边添加一列，顶部添加一行，底部添加一行。对于 5 × 5 窗口，您需要添加两行（详见 [图 8.6](#fig8-6)）。
- en: '![Image](../images/f0227-01.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0227-01.jpg)'
- en: '**Figure 8.5 Valid locations of 3 × 3 patches in a 5 × 5 input feature map**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.5 在 5 × 5 输入特征图中 3 × 3 补丁的有效位置**'
- en: '![Image](../images/f0227-02.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0227-02.jpg)'
- en: '**Figure 8.6 Padding a 5 × 5 input to be able to extract 25 3 × 3 patches**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.6 对 5 × 5 输入进行填充，以能够提取 25 个 3 × 3 补丁**'
- en: 'In layer_conv_2d(), padding is configurable via the padding argument, which
    takes two values: “valid”, which means no padding (only valid window locations
    will be used), and “same”, which means “pad in such a way as to have an output
    with the same width and height as the input.” The padding argument defaults to
    “valid”.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 layer_conv_2d() 中，填充可通过 padding 参数进行配置，padding 参数可以取两个值：“valid”，表示不进行填充（只使用有效的窗口位置），以及“same”，表示“以这样一种方式填充，使得输出的宽度和高度与输入相同。”
    padding 参数的默认值是“valid”。
- en: UNDERSTANDING CONVOLUTION STRIDES
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解卷积步幅
- en: 'The other factor that can influence output size is the notion of *strides*.
    Our description of convolution so far has assumed that the center tiles of the
    convolution windows are all contiguous. But the distance between two successive
    windows is a parameter of the convolution, called its *stride*, which defaults
    to 1\. It’s possible to have *strided convolutions*: convolutions with a stride
    higher than 1\. In [figure 8.7](#fig8-7), you can see the patches extracted by
    a 3 × 3 convolution with stride 2 over a 5 × 5 input (without padding).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 可以影响输出大小的另一个因素是*步幅*的概念。到目前为止，我们对卷积的描述假定卷积窗口的中心瓦片都是连续的。但是，两个连续窗口之间的距离是卷积的一个参数，称为其*步幅*，默认为1。可以进行*步进卷积*：步幅大于1的卷积。在[图8.7](#fig8-7)中，您可以看到在没有填充的情况下，3×3卷积以步幅2在5×5输入上提取的补丁。
- en: Using stride 2 means the width and height of the feature map are downsampled
    by a factor of 2 (in addition to any changes induced by border effects). Strided
    convolutions are rarely used in classification models, but they come in handy
    for some types of models, as you will see in the next chapter.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步长2意味着特征映射的宽度和高度被下采样了2倍（除了由边界效应引起的任何变化）。步进卷积在分类模型中很少使用，但对于某些类型的模型非常方便，你将在下一章中看到。
- en: In classification models, instead of strides, we tend to use the *max-pooling*
    operation to downsample feature maps, which you saw in action in our first convnet
    example. Let’s look at it in more depth.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类模型中，我们倾向于使用*最大池化*操作来对特征映射进行下采样，你在我们的第一个卷积神经网络示例中看到了它的作用。让我们更深入地研究一下。
- en: '![Image](../images/f0228-01.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0228-01.jpg)'
- en: '**Figure 8.7 3 × 3 convolution patches with 2 × 2 strides**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.7 3 × 3卷积补丁，步长为2 × 2**'
- en: 8.1.2 The max-pooling operation
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 最大池化操作
- en: 'In the convnet example, you may have noticed that the size of the feature maps
    is halved after every layer_max_pooling_2d(). For instance, before the first layer_max_
    pooling_2d(), the feature map is 26 × 26, but the max-pooling operation halves
    it to 13 × 13\. That’s the role of max pooling: to aggressively downsample feature
    maps, much like strided convolutions.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络示例中，您可能已经注意到，在每个layer_max_pooling_2d()之后，特征映射的大小都会减半。例如，在第一个layer_max_pooling_2d()之前，特征映射为26×26，但最大池化操作将其减半为13×13。这就是最大池化的作用：对特征映射进行积极地下采样，就像步进卷积一样。
- en: Max pooling consists of extracting windows from the input feature maps and outputting
    the max value of each channel. It’s conceptually similar to convolution, except
    that instead of transforming local patches via a learned linear transformation
    (the convolution kernel), they’re transformed via a hardcoded max tensor operation.
    A big difference from convolution is that max pooling is usually done with 2 ×
    2 windows and stride 2, to downsample the feature maps by a factor of 2\. On the
    other hand, convolution is typically done with 3 × 3 windows and no stride (stride
    1).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化包括从输入特征映射中提取窗口，并输出每个通道的最大值。概念上类似于卷积，不同之处在于，不是通过学习的线性转换（卷积核）来转换局部补丁，而是通过硬编码的最大张量操作来转换它们。与卷积的一个重大区别是，最大池化通常使用2×2的窗口和步长2来进行，以将特征映射下采样2倍。另一方面，卷积通常使用3×3的窗口和无步长（步长为1）进行。
- en: Why downsample feature maps this way? Why not remove the max-pooling layers
    and keep fairly large feature maps all the way up? Let’s look at this option.
    Our model would then look like the following listing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要以这种方式下采样特征映射？为什么不删除最大池化层，并一直保持相当大的特征映射？让我们看看这个选择。我们的模型将如下列表所示。
- en: Listing 8.5 Listing 8.5 An incorrectly structured convnet missing its max-pooling
    layers
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 列表8.5 缺少最大池化层的结构不正确的卷积神经网络
- en: inputs <— layer_input(shape = c(28, 28, 1))
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <— layer_input(shape = c(28, 28, 1))
- en: outputs <— inputs %>%
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <— inputs %>%
- en: layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
- en: layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
- en: layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
- en: layer_flatten() %>%
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: layer_flatten() %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: model_no_max_pool <— keras_model(inputs = inputs, outputs = outputs)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: model_no_max_pool <— keras_model(inputs = inputs, outputs = outputs)
- en: 'Here’s a summary of the model:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型的摘要：
- en: model_no_max_pool
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: model_no_max_pool
- en: '![Image](../images/f0229-01.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0229-01.jpg)'
- en: 'What’s wrong with this setup? Two things:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置有什么问题？有两个问题：
- en: It isn’t conducive to learning a spatial hierarchy of features. The 3 × 3 windows
    in the third layer will contain information coming from only 7 × 7 windows in
    the initial input. The high-level patterns learned by the convnet will still be
    very small with regard to the initial input, which may not be enough to learn
    to classify digits (try recognizing a digit by looking at it through windows that
    are only 7 × 7 pixels!). We need the features from the previous convolution layer
    to contain information about the totality of the input.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不利于学习空间特征的层次结构。第三层中的3×3窗口仅包含来自最初输入的7×7窗口的信息。卷积网络学习的高级模式与初始输入相比仍然非常小，这可能不足以学习分类数字（尝试通过仅使用7×7像素的窗口查看数字来识别数字！）。我们需要来自前一个卷积层的特征包含关于整个输入的信息。
- en: The final feature map has 22 × 22 × 128 = 61,952 total coefficients per sample
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的特征映射每个样本有22×22×128=61,952个系数
- en: In short, the reason to use downsampling is to reduce the number of feature-map
    coefficients to process, as well as to induce spatial-filter hierarchies by making
    successive convolution layers look at increasingly large windows (in terms of
    the fraction of the original input they cover).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用下采样的原因是减少要处理的特征图系数的数量，并通过使连续的卷积层查看越来越大的窗口（就涵盖原始输入的部分而言）来引导空间滤波器层次结构。
- en: Note that max pooling isn’t the only way you can achieve such downsampling.
    As you already know, you can also use strides in the prior convolution layer.
    And you can use average pooling instead of max pooling, where each local input
    patch is transformed by taking the average value of each channel over the patch,
    rather than the max. But max pooling tends to work better than these alternative
    solutions. The reason is that features tend to encode the spatial presence of
    some pattern or concept over the different tiles of the feature map (hence the
    term *feature map*), and it’s more informative to look at the *maximal presence*
    of different features than at their *average presence*. The most reasonable subsampling
    strategy is to first produce dense maps of features (via unstrided convolutions)
    and then look at the maximal activation of the features over small patches, rather
    than looking at sparser windows of the inputs (via strided convolutions) or averaging
    input patches, which could cause you to miss or dilute feature-presence information.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最大池化并不是您可以实现这种下采样的唯一方法。正如您已经知道的那样，您还可以在之前的卷积层中使用步长。您还可以使用平均池化，而不是最大池化，其中每个本地输入补丁通过在补丁上每个通道的平均值进行变换，而不是最大值。但是，最大池化比这些替代解决方案更有效。原因是特征倾向于在特征地图的不同瓷砖上编码某些模式或概念的空间存在（因此是特征图一词），查看不同特征的最大存在而不是平均存在更具信息性。最合理的子采样策略是首先通过未简化的卷积产生密集的特征映射，然后查看特征在小补丁上的最大激活，而不是查看输入的更稀疏的窗口（通过分步卷积）或平均输入补丁，这可能导致您错过或稀释特征存在信息。
- en: At this point, you should understand the basics of convnets—feature maps, convolution,
    and max pooling—and you should know how to build a small convnet to solve a toy
    problem such as MNIST digits classification. Now let’s move on to more useful,
    practical applications.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该了解卷积神经网络的基础知识-特征图、卷积和最大池化-并且应该知道如何构建一个小型卷积神经网络来解决玩具问题，如MNIST数字分类。现在让我们继续探讨更有用，实用的应用。
- en: 8.2 Training a convnet from scratch on a small dataset
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 从头开始在小数据集上训练convnet
- en: Having to train an image-classification model using very little data is a common
    situation, which you’ll likely encounter in practice if you ever do computer vision
    in a professional context. A “few” samples can mean anywhere from a few hundred
    to a few tens of thousands of images. As a practical example, we’ll focus on classifying
    images as dogs or cats in a dataset containing 5,000 pictures of cats and dogs
    (2,500 cats, 2,500 dogs). We’ll use 2,000 pictures for training, 1,000 for validation,
    and 2,000 for testing.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用很少的数据来训练图像分类模型是一种常见情况，在实践中，如果您在专业上进行计算机视觉，可能会遇到这种情况。“一些”样本可以从几百个到几万个图像不等。作为实际示例，我们将专注于将图像分类为猫或狗的元素，该数据集包含5,000张猫和狗的图片（2,500只猫，2,500只狗）。我们将使用2,000张图片进行训练，1,000张用于验证，2,000张用于测试。
- en: 'In this section, we’ll review one basic strategy to tackle this problem: training
    a new model from scratch using what little data you have. We’ll start by naively
    training a small convnet on the 2,000 training samples, without any regularization,
    to set a baseline for what can be achieved. This will get us to a classification
    accuracy of about 70%. At that point, the main issue will be overfitting. Then
    we’ll introduce *data augmentation*, a powerful technique for mitigating overfitting
    in computer vision. By using data augmentation, we’ll improve the model to reach
    an accuracy of 80–85%.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一种解决这个问题的基本策略：使用很少的数据从头开始训练一个新模型。我们将首先使用没有正则化的小型卷积神经网络对2000个训练样本进行简单训练，以建立一个基线模型来评估可达到的效果。这将使我们的分类准确性达到大约70%。此时，主要问题在于过拟合。然后，我们将介绍数据增强——一种处理计算机视觉中的过拟合的强大技术。通过使用数据增强，我们将改善模型，使其准确率提高到80-85%。
- en: 'In the next section, we’ll review two more essential techniques for applying
    deep learning to small datasets: *feature extraction with a pretrained model*
    (which will get us to an accuracy of 97.5%) and *fine-tuning a pretrained model*
    (which will get us to a final accuracy of 98.5%). Together, these three strategies—training
    a small model from scratch, doing feature extraction using a pretrained model,
    and fine-tuning a pre-trained model—will constitute your future toolbox for tackling
    the problem of performing image classification with small datasets.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾应用深度学习于小数据集的另外两种关键技术：*使用预训练模型进行特征提取*（将准确率提高到97.5%）和*微调预训练模型*（将准确率提高到最终的98.5%）。这三种战略——从头开始训练小模型、使用预训练模型进行特征提取，以及微调预训练模型——将为您应对小数据集的图像分类问题提供工具箱。
- en: 8.2.1 The relevance of deep learning for small data problems
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 深度学习在小数据问题中的相关性。
- en: What qualifies as “enough samples” to train a model is relative—relative to
    the size and depth of the model you’re trying to train, for starters. It isn’t
    possible to train a convnet to solve a complex problem with just a few tens of
    samples, but a few hundred can potentially suffice if the model is small and well
    regularized and the task is simple. Because convnets learn local, translation-invariant
    features, they’re highly data efficient on perceptual problems. Training a convnet
    from scratch on a very small image dataset will yield reasonable results despite
    a relative lack of data, without the need for any custom feature engineering.
    You’ll see this in action in this section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型所需的“足够样本”是相对的——首先相对于你要训练的模型的规模和深度。使用仅有几十个样本训练卷积神经网络以解决复杂问题是不可能的，但是对于小型、规范良好的模型和简单任务，数百个样本可能足够了。因为卷积神经网络学习局部、平移不变的特征，它们在感知性问题上非常数据高效。即使在非常小的图像数据集上从头开始训练卷积神经网络，也可以产生合理的结果，而不需要进行任何自定义的特征工程。你将在本节中看到这一点的实践演示。
- en: 'What’s more, deep learning models are by nature highly repurposable: you can
    take, say, an image-classification or speech-to-text model trained on a large-scale
    data-set and reuse it on a significantly different problem with only minor changes.
    Specifically, in the case of computer vision, many pretrained models (usually
    trained on the ImageNet dataset) are now publicly available for download and can
    be used to bootstrap powerful vision models out of very little data. This is one
    of the greatest strengths of deep learning: feature reuse. You’ll explore this
    in the next section. Let’s start by getting our hands on the data.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习模型具有高度重用性的本质特点：你可以使用一种基于大规模数据集训练的图像分类或语音转文本模型，并在只做出微小修改的情况下将其应用于完全不同的问题。尤其是在计算机视觉领域，很多预训练模型（通常在ImageNet数据集上训练）现在已经公开提供下载，可以使用非常少量的数据来引导强大的视觉模型。这是深度学习的最大优势之一：特征重用。你将在下一节中详细了解这一点。首先，我们需要开始处理数据。
- en: 8.2.2 Downloading the data
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 下载数据
- en: The Dogs vs. Cats dataset that we will use isn’t packaged with Keras. It was
    made available by Kaggle as part of a computer vision competition in late 2013,
    back when convnets weren’t mainstream. You can download the original dataset from
    [http://www.kaggle.com/c/dogs-vs-cats/data](http://www.kaggle.com/c/dogs-vs-cats/data)
    (you’ll need to create a Kaggle account if you don’t already have one—don’t worry,
    the process is painless). You can also use the Kaggle command line API to download
    the dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 Dogs vs. Cats 数据集并不随 Keras 打包。这是 Kaggle 在 2013 年末作为计算机视觉竞赛的一部分提供的，当时
    convnets 还不是主流。您可以从 [http://www.kaggle.com/c/dogs-vs-cats/data](http://www.kaggle.com/c/dogs-vs-cats/data)
    下载原始数据集（如果您还没有 Kaggle 帐户，您需要创建一个——别担心，这个过程很简单）。您也可以使用 Kaggle 命令行 API 下载数据集。
- en: '**Downloading a Kaggle dataset**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载 Kaggle 数据集**'
- en: Kaggle makes available an easy-to-use API to programmatically download Kaggle-hosted
    datasets. You can use it to download the Dogs vs. Cats dataset to your local computer,
    for instance. Downloading this dataset is as easy as running a single command
    in R.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 提供了一个易于使用的 API 来以编程方式下载托管在 Kaggle 上的数据集。您可以使用它将 Dogs vs. Cats 数据集下载到您的本地计算机。例如，通过在
    R 中运行单个命令，就可以轻松下载此数据集。
- en: However, access to the API is restricted to Kaggle users, so to run the preceding
    command, you first need to authenticate yourself. The kaggle package will look
    for your login credentials in a JSON file located at ~/.kaggle/kaggle.json. Let’s
    create this file.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，API 的访问权限受限于 Kaggle 用户，因此要运行上述命令，您首先需要进行身份验证。kaggle 包将在位于 ~/.kaggle/kaggle.json
    的 JSON 文件中查找您的登录凭据。让我们创建这个文件。
- en: First, you need to create a Kaggle API key and download it to your local machine.
    Just navigate to the Kaggle website in a web browser, log in, and go to the My
    Account page. In your account settings, you’ll find an API section. Clicking the
    Create New API Token button will generate a kaggle.json key file and will download
    it to your machine.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要创建一个 Kaggle API 密钥并将其下载到本地计算机。只需在 web 浏览器中导航到 Kaggle 网站，登录，然后转到“我的帐户”页面。在您的帐户设置中，您会找到一个
    API 部分。点击“创建新的 API 令牌”按钮将生成一个名为 kaggle.json 的密钥文件，并将其下载到您的计算机上。
- en: Finally, create a ~/.kaggle folder. As a security best practice, you should
    also make sure that the file is readable only by the current user, yourself. (This
    applies only if you’re on Mac or Linux, not Windows.)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建一个 ~/.kaggle 文件夹。作为安全最佳实践，您还应确保该文件仅由当前用户自己可读（仅适用于 Mac 或 Linux，而不是 Windows）。
- en: Because we’ll be doing a nontrivial amount of filesystem operations in the coming
    chapters, we’ll use the fs R package, which is a little nicer to work with than
    base R filesystem functions. (You can install it from CRAN with install.packages(“fs”).)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在接下来的章节中我们将执行大量的文件系统操作，我们将使用 fs R 包，它比基本的 R 文件系统函数更易于使用。（您可以通过 install.packages(“fs”)
    从 CRAN 安装它。）
- en: 'Prepare the Kaggle API key:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 准备 Kaggle API 密钥：
- en: library(fs)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: library(fs)
- en: dir_create("~/.kaggle")
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: dir_create("~/.kaggle")
- en: file_move("~/Downloads/kaggle.json", "~/.kaggle/")
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: file_move("~/Downloads/kaggle.json", "~/.kaggle/")
- en: file_chmod("~/.kaggle/kaggle.json", "0600")➊
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: file_chmod("~/.kaggle/kaggle.json", "0600")➊
- en: ➊ **Mark the file readable only by yourself**
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将文件标记为仅自己可读**
- en: 'Install the kaggle package via pip:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 pip 安装 kaggle 包：
- en: reticulate::py_install("kaggle", pip = TRUE)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: reticulate::py_install("kaggle", pip = TRUE)
- en: 'You can now download the data we’re about to use:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以下载我们即将使用的数据：
- en: system('kaggle competitions download -c dogs-vs-cats')
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: system('kaggle competitions download -c dogs-vs-cats')
- en: The first time you try to download the data, you may get a “403 Forbidden” error.
    That’s because you need to accept the terms associated with the dataset before
    you download it—you’ll have to go to [http://www.kaggle.com/c/dogs-vs-cats/rules](http://www.kaggle.com/c/dogs-vs-cats/rules)
    (while logged in to your Kaggle account) and click the I Understand and Accept
    button. You need to do this only once.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次尝试下载数据时，可能会出现“403 Forbidden”错误。这是因为在下载之前，您需要接受与数据集相关的条款——您需要登录 Kaggle 帐户并点击“我理解并接受”按钮，网址为
    [http://www.kaggle.com/c/dogs-vs-cats/rules](http://www.kaggle.com/c/dogs-vs-cats/rules)。您只需要这样做一次。
- en: 'Finally, the data is downloaded as a compressed zip file, dogs-vs-cats.zip.
    That zip file itself contains another compressed zip file, train.zip, which is
    the training data we’ll use. We uncompress train.zip into a new directory, dogs-vs-cats,
    using the zip R package (installable from CRAN with install.packages(“zip”):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据以压缩的 zip 文件 dogs-vs-cats.zip 的形式下载。该 zip 文件本身包含另一个压缩的 zip 文件 train.zip，这是我们将要使用的训练数据。我们使用
    zip R 包（可以通过 install.packages(“zip”) 从 CRAN 安装）将 train.zip 解压缩到一个新目录 dogs-vs-cats
    中：
- en: zip::unzip('dogs-vs-cats.zip', exdir = "dogs-vs-cats", files = "train.zip")
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: zip::unzip('dogs-vs-cats.zip', exdir = "dogs-vs-cats", files = "train.zip")
- en: zip::unzip("dogs-vs-cats/train.zip", exdir = "dogs-vs-cats")
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: zip::unzip("dogs-vs-cats/train.zip", exdir = "dogs-vs-cats")
- en: The pictures in our dataset are medium-resolution color JPEGs. [Figure 8.8](#fig8-8)
    shows some examples.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的图片是中等分辨率的彩色 JPEG。[图 8.8](#fig8-8) 展示了一些示例。
- en: '![Image](../images/f0232-01.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0232-01.jpg)'
- en: '**Figure 8.8 Samples from the Dogs vs. Cats dataset. Sizes weren’t modified:
    The samples come in different sizes, colors, backgrounds, and so on.**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.8 狗与猫数据集的样本。大小未经修改：样本的大小、颜色、背景等各不相同。**'
- en: Unsurprisingly, the original dogs-versus-cats Kaggle competition, all the way
    back in 2013, was won by entrants who used convnets. The best entries achieved
    up to 95% accuracy. In this example, we will get fairly close to this accuracy
    (in the next section), even though we will train our models on less than 10% of
    the data that was available to the competitors.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，最早在 2013 年的狗与猫 Kaggle 竞赛中，获胜者都是使用了卷积神经网络。最好的参赛作品达到了 95% 的准确率。在这个示例中，我们将会在下一节中实现接近这个准确率，尽管我们将只使用竞争对手可用数据的不到
    10% 进行模型训练。
- en: 'This dataset contains 25,000 images of dogs and cats (12,500 from each class)
    and is 543 MB (compressed). After downloading and uncompressing the data, we’ll
    create a new dataset containing three subsets: a training set with 1,000 samples
    of each class, a validation set with 500 samples of each class, and a test set
    with 1,000 samples of each class. Why do this? Because many of the image datasets
    you’ll encounter in your career contain only a few thousand samples, not tens
    of thousands. Having more data available would make the problem easier, so it’s
    good practice to learn with a small dataset.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含 25,000 张狗和猫的图片（每类别 12,500 张），大小为 543 MB（压缩后）。下载并解压缩数据后，我们将创建一个新数据集，其中包含三个子集：一个包含每个类别
    1,000 个样本的训练集，一个包含每个类别 500 个样本的验证集，以及一个包含每个类别 1,000 个样本的测试集。为什么这样做？因为你在职业生涯中遇到的许多图像数据集只包含几千个样本，而不是几万个样本。有更多的数据可用会使问题变得更容易，因此使用小数据集进行学习是一个好的做法。
- en: 'The subsampled dataset we will work with will have the following directory
    structure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的子采样数据集将具有以下目录结构：
- en: cats_vs_dogs_small/
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: cats_vs_dogs_small/
- en: …train/
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: …train/
- en: ……cat/➊
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ……cat/➊
- en: ……dog/➋
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ……dog/➋
- en: …validation/
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: …validation/
- en: ……cat/➌
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ……cat/➌
- en: ……dog/➍
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ……dog/➍
- en: …test/
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: …test/
- en: ……cat/➎
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ……cat/➎
- en: ……dog/➏
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ……dog/➏
- en: ➊ **Contains 1,000 cat images**
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **包含 1,000 张猫的图片**
- en: ➋ **Contains 1,000 dog images**
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **包含 1,000 张狗的图片**
- en: ➌**Contains 500 cat images**
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ➌**包含 500 张猫的图片**
- en: ➍**Contains 500 dog images**
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ➍**包含 500 张狗的图片**
- en: ➎**Contains 1,000 cat images**
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **包含 1,000 张猫的图片**
- en: ➏**Contains 1,000 dog images**
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ➏**包含 1,000 张狗的图片**
- en: Let’s make it happen with a couple calls to {fs} functions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几次调用 {fs} 函数来实现这一点。
- en: Listing 8.6 Copying images to training, validation, and test directories
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.6 将图片复制到训练、验证和测试目录
- en: library(fs)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: library(fs)
- en: original_dir <— path("dogs-vs-cats/train")➊
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: original_dir <— path("dogs-vs-cats/train")➊
- en: new_base_dir <— path("cats_vs_dogs_small")➋
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: new_base_dir <— path("cats_vs_dogs_small")➋
- en: make_subset <— function(subset_name,➌
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: make_subset <— function(subset_name,➌
- en: start_index, end_index) {
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: start_index, end_index) {
- en: for (category in c("dog", "cat")) {
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: for (category in c("dog", "cat")) {
- en: file_name <— glue::glue("{category}.{ start_index:end_index }.jpg")
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: file_name <— glue::glue("{category}.{ start_index:end_index }.jpg")
- en: dir_create(new_base_dir / subset_name / category)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: dir_create(new_base_dir / subset_name / category)
- en: file_copy(original_dir / file_name,
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: file_copy(original_dir / file_name,
- en: new_base_dir / subset_name / category / file_name)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: new_base_dir / subset_name / category / file_name)
- en: '}'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: make_subset("train", start_index = 1, end_index = 1000)➍
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: make_subset("train", start_index = 1, end_index = 1000)➍
- en: make_subset("validation", start_index = 1001, end_index = 1500)➎
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: make_subset("validation", start_index = 1001, end_index = 1500)➎
- en: make_subset("test", start_index = 1501, end_index = 2500)➏
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: make_subset("test", start_index = 1501, end_index = 2500)➏
- en: ➊ **Path to the directory where the original dataset was uncompressed**
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **原始数据集解压缩后的目录路径**
- en: ➋ **Directory where we will store our smaller dataset**
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们将存储较小数据集的目录**
- en: ➌ **Utility function that copies cat and dog images between indexes start_index
    and end_index to the subdirectory new_base_dir/ {subset_name}/cat (and /dog).
    The "subset_name" will be either "train", "validation", or "test".**
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将猫和狗图片在开始索引和结束索引之间复制到子目录 new_base_dir/{subset_name}/cat（和/dog）的实用函数。"subset_name"
    将是 "train"、"validation" 或 "test" 中的一个。**
- en: ➍ **Create the training subset with the first 1,000 images of each category.**
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **创建训练子集，包含每类别的前 1,000 张图片。**
- en: ➎ **Create the validation subset with the next 500 images of each category.**
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **创建验证子集，包含每类别的接下来 500 张图片。**
- en: ➏ **Create the test subset with the next 1,000 images of each category.**
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **用每个类别的接下来的 1,000 张图像创建测试子集。**
- en: 'We now have 2,000 training images, 1,000 validation images, and 2,000 test
    images. Each split contains the same number of samples from each class: this is
    a balanced binary-classification problem, which means classification accuracy
    will be an appropriate measure of success.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有 2,000 张训练图像，1,000 张验证图像和 2,000 张测试图像。每个数据集包含相同数量的来自每个类别的样本：这是一个平衡的二元分类问题，这意味着分类准确度将是一个适当的成功度量。
- en: 8.2.3 Building the model
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 构建模型
- en: 'We will reuse the same general model structure you saw in the first example:
    the convnet will be a stack of alternated layer_conv_2d() (with relu activation)
    and layer_ max_pooling_2d() layers.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复使用你在第一个示例中看到的相同的通用模型结构：卷积网络将是交替的 layer_conv_2d()（使用 relu 激活）和 layer_ max_pooling_2d()
    层的堆叠。
- en: 'But because we’re dealing with bigger images and a more complex problem, we’ll
    make our model larger, accordingly: it will have two more layer_conv_2d() and
    layer_max_pooling_2d() stages. This serves both to augment the capacity of the
    model and to further reduce the size of the feature maps so they aren’t overly
    large when we reach the layer_flatten(). Here, because we start from inputs of
    size 180 pixels × 180 pixels (a somewhat arbitrary choice), we end up with feature
    maps of size 7 × 7 just before the layer_flatten().'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 但因为我们处理的是更大的图像和更复杂的问题，我们将相应地使我们的模型更大：它将具有两个更多的 layer_conv_2d() 和 layer_max_pooling_2d()
    阶段。这既增加了模型的容量，又进一步减小了特征图的大小，使得当我们到达 layer_flatten() 时它们不会过大。在这里，因为我们从尺寸为 180 像素
    × 180 像素的输入开始（一个相对随意的选择），所以在 layer_flatten() 之前我们得到大小为 7 × 7 的特征图。
- en: The depth of the feature maps progressively increases in the model (from 32
    to 256), whereas the size of the feature maps decreases (from 180 × 180 to 7 ×
    7). This is a pattern you’ll see in almost all convnets.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 特征图的深度在模型中逐渐增加（从 32 增加到 256），而特征图的大小逐渐减小（从 180 × 180 减小到 7 × 7）。这是你几乎在所有卷积网络中都会看到的模式。
- en: Because we’re looking at a binary-classification problem, we’ll end the model
    with a single unit (a layer_dense() of size 1) and a sigmoid activation. This
    unit will encode the probability that the model is looking at one class or the
    other.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在处理一个二元分类问题，所以我们将模型结束于一个单元（大小为 1 的 layer_dense()）和一个 sigmoid 激活。这个单元将编码模型正在观察的一个类别或另一个类别的概率。
- en: 'One last small difference: we will start the model with a layer_rescaling(),
    which will rescale image inputs (whose values are originally in the [0, 255] range)
    to the [0, 1] range.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个小差异：我们将以一个 layer_rescaling() 开始模型，它将重新缩放图像输入（其值最初在 [0, 255] 范围内）到 [0, 1]
    范围内。
- en: Listing 8.7 Instantiating a small convnet for dogs vs. cats classification
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.7 实例化用于狗与猫分类的小型卷积网络
- en: inputs <— layer_input(shape = c(180, 180, 3))➊
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <— layer_input(shape = c(180, 180, 3))➊
- en: outputs <— inputs %>%
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <— inputs %>%
- en: layer_rescaling(1 / 255) %>%➋
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: layer_rescaling(1 / 255) %>%➋
- en: layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>%
- en: layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>%
- en: layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>%
- en: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>%
- en: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>% layer_flatten()
    %>%
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>% layer_flatten()
    %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <— keras_model(inputs, outputs)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: model <— keras_model(inputs, outputs)
- en: ➊ **The model expects RGB images of size 180 × 180.**
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **该模型期望的是尺寸为 180 × 180 的 RGB 图像。**
- en: ➋ **Rescale inputs to the [0, 1] range by dividing them by 255.**
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **通过将它们除以 255 来将输入重新缩放到 [0, 1] 范围内。**
- en: 'Let’s look at how the dimensions of the feature maps change with every successive
    layer:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看随着每一层的连续变化，特征图的维度如何改变：
- en: model
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0235-01.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0235-01.jpg)'
- en: For the compilation step, we’ll go with the RMSprop optimizer, as usual. Because
    we ended the model with a single sigmoid unit, we’ll use binary cross-entropy
    as the loss (as a reminder, check out table 6.1 in chapter 6 for a cheat sheet
    on which loss function to use in various situations).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编译步骤，我们将继续使用 RMSprop 优化器，因为通常情况下我们会以单个 sigmoid 单元结束模型，所以我们将使用二元交叉熵作为损失函数（作为提醒，在第
    6 章的表 6.1 中可以查看在各种情况下使用哪个损失函数的速查表）。
- en: Listing 8.8 Configuring the model for training
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.8 配置模型进行训练
- en: model %>% compile(loss = "binary_crossentropy",
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% 编译(损失 = "binary_crossentropy",
- en: optimizer = "rmsprop",
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器 = "rmsprop"
- en: metrics = "accuracy")
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "准确率")
- en: 8.2.4 Data preprocessing
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.4 数据预处理
- en: 'As you know by now, data should be formatted into appropriately preprocessed
    floating-point tensors before being fed into the model. Currently, the data sits
    on a drive as JPEG files, so the steps for getting it into the model are roughly
    as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在所知道的，数据在进入模型之前应该被格式化为适当预处理的浮点张量。目前，数据作为 JPEG 文件存在于驱动器上，因此将其输入模型的步骤大致如下：
- en: '**1** Read the picture files.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 读取图片文件。'
- en: '**2** Decode the JPEG content to RGB grids of pixels.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 将 JPEG 内容解码为 RGB 像素网格。'
- en: '**3** Convert these into floating-point tensors.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 将这些转换为浮点张量。'
- en: '**4** Resize them to a shared size (we’ll use 180 × 180).'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4** 调整它们为共享大小（我们将使用 180 × 180）。'
- en: '**5** Pack them into batches (we’ll use batches of 32 images).'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**5** 将它们打包成批次（我们将使用 32 张图像的批次）。'
- en: It may seem a bit daunting, but fortunately Keras has utilities to take care
    of these steps automatically. In particular, Keras features the utility function
    image_dataset_ from_directory(), which lets you quickly set up a data pipeline
    that can automatically turn image files on disk into batches of preprocessed tensors.
    This is what we’ll use here.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点令人生畏，但幸运的是，Keras 提供了自动处理这些步骤的实用工具。特别是，Keras 具有实用函数 image_dataset_ from_directory()，它让你快速设置一个数据管道，可以自动将磁盘上的图像文件转换为预处理张量的批次。这是我们将在这里使用的方法。
- en: Calling image_dataset_from_directory(directory) will first list the subdirectories
    of directory and assume each one contains images from one of our classes. It will
    then index the image files in each subdirectory. Finally, it will create and return
    a TF Dataset object configured to read these files, shuffle them, decode them
    to tensors, resize them to a shared size, and pack them into batches.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 image_dataset_from_directory(directory) 首先会列出目录的子目录，并假定每个子目录都包含一个类别的图像。然后，它将索引每个子目录中的图像文件。最后，它将创建并返回一个
    TF 数据集对象，配置为读取这些文件、对它们进行洗牌、将它们解码为张量、将它们调整为共享大小并将它们打包成批次。
- en: Listing 8.9 Using image_dataset_from_directory to read images
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 使用 image_dataset_from_directory 读取图像
- en: train_dataset <—
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集 <—
- en: image_dataset_from_directory(new_base_dir / "train",
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 从目录创建图像数据集(new_base_dir / "train")
- en: image_size = c(180, 180),
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图像大小 = c(180, 180)
- en: batch_size = 32)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小 = 32)
- en: validation_dataset <—
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据集 <—
- en: image_dataset_from_directory(new_base_dir / "validation",
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从目录创建图像数据集(new_base_dir / "validation")
- en: image_size = c(180, 180),
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图像大小 = c(180, 180)
- en: batch_size = 32)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小 = 32)
- en: test_dataset <—
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集 <—
- en: image_dataset_from_directory(new_base_dir / "test",
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从目录创建图像数据集(new_base_dir / "test")
- en: image_size = c(180, 180),
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图像大小 = c(180, 180)
- en: batch_size = 32)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小 = 32)
- en: '**Understanding tfdatasets**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解 tfdatasets**'
- en: The tfdatasets package can be used to create efficient input pipelines for machine
    learning models. Its core object type is the TF Dataset.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: tfdatasets 包可用于为机器学习模型创建高效的输入管道。其核心对象类型是 TF 数据集。
- en: 'A TF Dataset object is an iterable: you can call as_iterator() on it to produce
    an iterator, and then repeatedly call iter_next() on the iterator to generate
    sequences of data. You will typically use TF Dataset objects to produce batches
    of input data and labels. You can pass a TF Dataset object directly to the fit()
    method of a Keras model.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: TF 数据集对象是可迭代的：你可以在它上面调用 as_iterator() 来生成一个迭代器，然后在迭代器上重复调用 iter_next() 来生成数据序列。通常，你会使用
    TF 数据集对象来生成输入数据和标签的批次。你可以直接将 TF 数据集对象传递给 Keras 模型的 fit() 方法。
- en: The TF Dataset object handles many key features that would otherwise be cumbersome
    to implement yourself—in particular, asynchronous data prefetching (preprocessing
    the next batch of data while the previous one is being handled by the model, which
    keeps execution flowing without interruptions).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: TF 数据集对象处理了许多关键特性，否则你自己实现起来可能会很麻烦，特别是异步数据预取（在模型处理上一个批次数据的同时预处理下一个批次数据，这样可以保持执行流畅，没有中断）。
- en: 'The tfdatasets package provides a functional-style API for modifying Datasets.
    Here’s a quick example: let’s create a TF Dataset instance from a R array of an
    integer sequence. We’ll consider 100 samples, where each sample is a vector of
    size 6 (in other words, our starting R array is a matrix with shape (100, 6)):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: tfdatasets 包提供了一个函数式 API，用于修改数据集。这里是一个快速示例：让我们从一个整数序列的 R 数组创建一个 TF Dataset 实例。我们将考虑
    100 个样本，其中每个样本是一个大小为 6 的向量（换句话说，我们的起始 R 数组是一个形状为 (100, 6) 的矩阵）：
- en: library(tfdatasets)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: example_array <— array(seq(100*6), c(100, 6))
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: example_array <— array(seq(100*6), c(100, 6))
- en: head(example_array)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: head(example_array)
- en: '![Image](../images/f0237-01.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0237-01.jpg)'
- en: dataset <— tensor_slices_dataset(example_array)➊
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: dataset <— tensor_slices_dataset(example_array)➊
- en: ➊ The tensor_slices_dataset() function can be used to create a TF Dataset from
    an R array, or a list (optionally named) of R arrays.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ tensor_slices_dataset() 函数可以用来从一个 R 数组或一个（可选命名的）R 数组列表创建 TF Dataset。
- en: 'At first, our dataset just yields single samples:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们的数据集只产生单个样本：
- en: dataset_iterator <— as_iterator(dataset)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_iterator <— as_iterator(dataset)
- en: for(i in 1:3) {
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in 1:3) {
- en: element <— iter_next(dataset_iterator)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: element <— iter_next(dataset_iterator)
- en: print(element)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 打印(element)
- en: '}'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: tf.Tensor([ 1 101 201 301 401 501], shape=(6), dtype=int32)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([ 1 101 201 301 401 501], shape=(6), dtype=int32)
- en: tf.Tensor([ 2 102 202 302 402 502], shape=(6), dtype=int32)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([ 2 102 202 302 402 502], shape=(6), dtype=int32)
- en: tf.Tensor([ 3 103 203 303 403 503], shape=(6), dtype=int32)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([ 3 103 203 303 403 503], shape=(6), dtype=int32)
- en: 'Note that the TF Dataset iterator yields Tensorflow Tensors by default. This
    is typically what you want, and the most appropriate type for methods like fit().
    In some situations, however, you may prefer if the iterator yields batches of
    R arrays instead; in that situation, you can call as_array_iterator() instead
    of as_iterator():'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，默认情况下 TF Dataset 迭代器会产生 Tensorflow 张量。这通常是你想要的，也是 fit() 方法的最合适类型。然而，在某些情况下，你可能更喜欢迭代器产生
    R 数组的批次；在这种情况下，你可以调用 as_array_iterator() 而不是 as_iterator()：
- en: dataset_array_iterator <— as_array_iterator(dataset)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_array_iterator <— as_array_iterator(dataset)
- en: for(i in 1:3) {
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in 1:3) {
- en: element <— iter_next(dataset_array_iterator)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: element <— iter_next(dataset_array_iterator)
- en: str(element)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: str(element)
- en: '}'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: int [1:6(1d)] 1 101 201 301 401 501
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: int [1:6(1d)] 1 101 201 301 401 501
- en: int [1:6(1d)] 2 102 202 302 402 502
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: int [1:6(1d)] 2 102 202 302 402 502
- en: int [1:6(1d)] 3 103 203 303 403 503
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: int [1:6(1d)] 3 103 203 303 403 503
- en: 'We can use the dataset_batch() to batch the data:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 dataset_batch() 来对数据进行分批处理：
- en: batched_dataset <— dataset %>%
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: batched_dataset <— dataset %>%
- en: dataset_batch(3)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_batch(3)
- en: batched_dataset_iterator <— as_iterator(batched_dataset)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: batched_dataset_iterator <— as_iterator(batched_dataset)
- en: for(i in 1:3) {
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in 1:3) {
- en: element <— iter_next(batched_dataset_iterator)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: element <— iter_next(batched_dataset_iterator)
- en: print(element)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 打印(element)
- en: '}'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: tf.Tensor(
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[ [ 1 101 201 301 401 501]'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[ [ 1 101 201 301 401 501]'
- en: '[ 2 102 202 302 402 502]'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 2 102 202 302 402 502]'
- en: '[ 3 103 203 303 403 503]], shape=(3, 6), dtype=int32)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 3 103 203 303 403 503]], shape=(3, 6), dtype=int32)'
- en: tf.Tensor(
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[ [ 4 104 204 304 404 504]'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[ [ 4 104 204 304 404 504]'
- en: '[ 5 105 205 305 405 505]'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 5 105 205 305 405 505]'
- en: '[ 6 106 206 306 406 506]], shape=(3, 6), dtype=int32)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 6 106 206 306 406 506]], shape=(3, 6), dtype=int32)'
- en: tf.Tensor(
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[ [ 7 107 207 307 407 507]'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[ [ 7 107 207 307 407 507]'
- en: '[ 8 108 208 308 408 508]'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 8 108 208 308 408 508]'
- en: '[ 9 109 209 309 409 509]], shape=(3, 6), dtype=int32)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 9 109 209 309 409 509]], shape=(3, 6), dtype=int32)'
- en: More broadly, we have access to a range of useful dataset methods, such as
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，我们可以访问一系列有用的数据集方法，比如
- en: dataset_shuffle(buffer_size)—Shuffles elements within a buffer
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dataset_shuffle(buffer_size)—在缓冲区内对元素进行洗牌
- en: dataset_prefetch(buffer_size)—Prefetches a buffer of elements in GPU memory
    to achieve better device utilization
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dataset_prefetch(buffer_size)—预取 GPU 内存中的元素缓冲，以实现更好的设备利用率
- en: dataset_map(fn)—Applies an arbitrary transformation to each element of the dataset
    (the function fn, which expects to take as input a single element yielded by the
    dataset)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dataset_map(fn)—对数据集的每个元素应用任意转换（函数 fn，它期望接受数据集产生的单个元素作为输入）
- en: 'The dataset_map() method, in particular, is one that you will use often. Here’s
    an example. We’ll use it to reshape the elements in our toy dataset from shape
    (6) to shape (2, 3):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map() 方法特别常用。这里是一个例子。我们将使用它来将我们的玩具数据集中的元素重新塑形，从形状 (6) 到形状 (2, 3)：
- en: reshaped_dataset <— dataset %>%
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: reshaped_dataset <— dataset %>%
- en: dataset_map(function(element) tf$reshape(element, shape(2, 3)))➊
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(function(element) tf$reshape(element, shape(2, 3)))➊
- en: reshaped_dataset_iterator <— as_iterator(reshaped_dataset)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: reshaped_dataset_iterator <— as_iterator(reshaped_dataset)
- en: for(i in 1:3) {
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in 1:3) {
- en: element <— iter_next(reshaped_dataset_iterator)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: element <— iter_next(reshaped_dataset_iterator)
- en: print(element)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 打印(element)
- en: '}'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: tf.Tensor(
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[ 1 101 201]'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 1 101 201]'
- en: '[301 401 501]], shape=(2, 3), dtype=int32)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[301 401 501]], 形状=(2, 3), 数据类型=int32)'
- en: tf.Tensor(
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[ 2 102 202]'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 2 102 202]'
- en: '[302 402 502]], shape=(2, 3), dtype=int32)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[302 402 502]], 形状=(2, 3), 数据类型=int32)'
- en: tf.Tensor(
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[ 3 103 203]'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 3 103 203]'
- en: '[303 403 503]],'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[303 403 503]],'
- en: shape=(2, 3), dtype=int32)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 形状=(2, 3), 数据类型=int32)
- en: ➊ **Note that tf$reshape() reshapes using C style (row-major) semantics.**
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **请注意，tf$reshape() 使用 C 风格（行主序）语义进行重塑。**
- en: You’re about to see more dataset_map() action in this chapter.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将看到更多 dataset_map() 操作。
- en: 'Let’s look at the output of one of these Dataset objects: it yields batches
    of 180 × 180 RGB images (shape (32, 180, 180, 3)) and integer labels (shape (32)).
    There are 32 samples in each batch (the batch size).'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些 Dataset 对象之一的输出：它产生 180 × 180 的 RGB 图像批次（形状为 (32, 180, 180, 3)）和整数标签（形状为
    (32)）。每个批次中有 32 个样本（批量大小）。
- en: Listing 8.10 Displaying the shapes of the data and labels yielded by the Dataset
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.10 显示由 Dataset 产生的数据和标签的形状
- en: c (data_batch, labels_batch) %<—% iter_next(as_iterator(train_dataset)) data_batch$shape
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: c (data_batch, labels_batch) %<—% iter_next(as_iterator(train_dataset)) data_batch$形状
- en: TensorShape([32, 180, 180, 3])
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([32, 180, 180, 3])
- en: labels_batch$shape
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 标签批次$形状
- en: TensorShape([32])
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([32])
- en: Let’s fit the model on our dataset. We’ll use the validation_data argument in
    fit() to monitor validation metrics on a separate TF Dataset object.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的数据集上拟合模型。我们将使用 fit() 中的 validation_data 参数在单独的 TF Dataset 对象上监视验证指标。
- en: 'Note that we’ll also use a callback_model_checkpoint() to save the model after
    each epoch. We’ll configure it with the path specifying where to save the file,
    as well as the arguments save_best_only = TRUE and monitor = “val_loss”: they
    tell the call-back to only save a new file (overwriting any previous one) when
    the current value of the val_loss metric is lower than at any previous time during
    training. This guarantees that your saved file will always contain the state of
    the model corresponding to its best-performing training epoch, in terms of its
    performance on the validation data. As a result, we won’t have to retrain a new
    model for a lower number of epochs if we start overfitting: we can just reload
    our saved file.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还将使用 callback_model_checkpoint() 在每个时期之后保存模型。我们将配置它，指定保存文件的路径，以及参数 save_best_only
    = TRUE 和 monitor = “val_loss”：它们告诉回调函数仅在当前 val_loss 度量值低于训练期间任何以前时间的度量值时才保存新文件（覆盖任何先前的文件）。这确保了您保存的文件始终包含模型的状态，对应于其在验证数据上的性能最佳的训练时期。因此，如果我们开始过度拟合，我们不必重新训练一个新模型以进行更少数量的时期：我们可以重新加载我们保存的文件。
- en: Listing 8.11 Fitting the model using a TensorFlow Dataset
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.11 使用 TensorFlow Dataset 拟合模型
- en: callbacks <— list(
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数 <— 列表(
- en: callback_model_checkpoint(
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint(
- en: filepath = "convnet_from_scratch.keras",
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 文件路径 = "convnet_from_scratch.keras",
- en: save_best_only = TRUE,
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE,
- en: monitor = "val_loss"
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: monitor = "val_loss"
- en: )
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: )
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: history <— model %>%
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 历史 <— 模型 %>%
- en: fit(
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: fit(
- en: train_dataset,
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 30,
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 时期 = 30,
- en: validation_data = validation_dataset,
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据 = 验证_dataset,
- en: callbacks = callbacks
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数 = 回调函数
- en: )
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Let’s plot the loss and accuracy of the model over the training and validation
    data during training (see [figure 8.9](#fig8-9)).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制模型在训练和验证数据上的损失和准确性，以便在训练过程中进行对比（见 [图 8.9](#fig8-9)）。
- en: Listing 8.12 Displaying curves of loss and accuracy during training
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.12 显示训练期间的损失和准确性曲线
- en: plot(history)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制(历史)
- en: '![Image](../images/f0240-01.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0240-01.jpg)'
- en: '**Figure 8.9 Training and validation metrics for a simple convnet.**'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.9 简单卷积网络的训练和验证指标。**'
- en: These plots are characteristic of overfitting. The training accuracy increases
    linearly over time, until it reaches nearly 100%, whereas the validation accuracy
    peaks at 75%. The validation loss reaches its minimum after only 10 epochs and
    then increases, whereas the training loss keeps decreasing linearly as training
    proceeds.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表特征是过度拟合的特征。训练准确度随时间线性增加，直到接近 100%，而验证准确度在 75% 处达到峰值。验证损失在仅 10 个时期后达到最小值，然后增加，而训练损失在训练过程中保持线性减少。
- en: Let’s check the test accuracy. We’ll reload the model from its saved file to
    evaluate it as it was before it started overfitting.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确度。我们将重新加载模型，以评估它在开始过度拟合之前的状态。
- en: Listing 8.13 Evaluating the model on the test set
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.13 在测试集上评估模型
- en: test_model <— load_model_tf("convnet_from_scratch.keras")
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型 <— load_model_tf("convnet_from_scratch.keras")
- en: result <— evaluate(test_model, test_dataset)
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: result <— evaluate(test_model, test_dataset)
- en: 'cat(sprintf("Test accuracy: %.3f\n", result["accuracy"]))'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("测试准确度：%.3f\n", result["accuracy"]))
- en: 'Test accuracy: 0.740'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确度：0.740
- en: We get a test accuracy of 74%. (Due to the randomness of neural network initializations,
    you may get slightly different numbers.)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 74% 的测试准确度。（由于神经网络初始化的随机性，你可能会得到略有不同的数字。）
- en: 'Because we have relatively few training samples (2,000), overfitting will be
    our number one concern. You already know about a number of techniques that can
    help mitigate overfitting, such as dropout and weight decay (L2 regularization).
    We’re now going to work with a new one, specific to computer vision and used almost
    universally when processing images with deep learning models: *data augmentation*.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的训练样本相对较少（2,000），过拟合将是我们关注的首要问题。你已经了解到一些可以帮助缓解过拟合的技术，例如随机失活和权重衰减（L2 正则化）。现在我们要使用一种新的技术，针对计算机视觉的，几乎在使用深度学习模型处理图像时通用的一种技术：*数据增强*。
- en: 8.2.5 Using data augmentation
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.5 使用数据增强
- en: 'Overfitting is caused by having too few samples to learn from, rendering you
    unable to train a model that can generalize to new data. Given infinite data,
    your model would be exposed to every possible aspect of the data distribution
    at hand: you would never overfit. Data augmentation takes the approach of generating
    more training data from existing training samples by *augmenting* the samples
    via a number of random transformations that yield believable-looking images. The
    goal is that, at training time, your model will never see the exact same picture
    twice. This helps expose the model to more aspects of the data so it can generalize
    better.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是由于样本数量太少而导致的，使得您无法训练出能够泛化到新数据的模型。如果有无限的数据，您的模型将接触到手头数据分布的每一个可能的方面：您永远不会过拟合。数据增强采取的方法是通过对现有训练样本进行一系列随机变换来生成更多的训练数据，从而*增强*样本。目标是，在训练时，您的模型永远不会看到完全相同的图片两次。这有助于使模型接触到数据的更多方面，从而更好地泛化。
- en: 'In Keras, this can be done by adding a number of *data augmentation layers*
    at the start of your model. Let’s get started with an example: the following keras_model_
    sequential() chains several random image transformations. In our model, we’d include
    it right before the layer_rescaling().'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，可以通过在模型开头添加一些 *数据增强层* 来完成这个任务。让我们从一个示例开始：以下的 keras_model_sequential()
    链接了几个随机图像转换。在我们的模型中，我们会在 layer_rescaling() 之前包含它。
- en: Listing 8.14 Defining a data augmentation stage to add to an image model
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.14 定义要添加到图像模型中的数据增强阶段
- en: data_augmentation <— keras_model_sequential() %>%
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: data_augmentation <— keras_model_sequential() %>%
- en: layer_random_flip("horizontal") %>%
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_flip("horizontal") %>%
- en: layer_random_rotation(0.1) %>%
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_rotation(0.1) %>%
- en: layer_random_zoom(0.2)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_zoom(0.2)
- en: 'These are just a few of the layers available (for more, see the Keras documentation).
    Let’s quickly go over this code:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是可用的几个层之一（更多请参阅 Keras 文档）。让我们快速浏览一下这段代码：
- en: layer_random_flip(“horizontal”)—Applies horizontal flipping to a random 50%
    of the images that go through it
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: layer_random_flip(“horizontal”)—将通过它的随机 50% 的图像进行水平翻转
- en: layer_random_rotation(0.1)—Rotates the input images by a random value in the
    range [-10%, +10%] (these are fractions of a full circle—in degrees, the range
    would be [-36 degrees, +36 degrees])
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: layer_random_rotation(0.1)—将输入图像随机旋转一个范围为 [-10%，+10%] 的值（这些是完整圆的一部分——以角度表示，范围将是
    [-36 度，+36 度]）
- en: layer_random_zoom(0.2)—Zooms in or out of the image by a random factor in the
    range [-20%, +20%]
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: layer_random_zoom(0.2)—将图像放大或缩小一个范围在 [-20%，+20%] 内的随机因子
- en: Let’s look at the augmented images (see [figure 8.10](#fig8-10)).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看增强后的图片（参见 [图 8.10](#fig8-10)）。
- en: Listing 8.15 Displaying some randomly augmented training images
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.15 显示一些随机增强的训练图像
- en: library(tfdatasets)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: batch <— train_dataset %>%
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: batch <— train_dataset %>%
- en: as_iterator() %>%
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: as_iterator() %>%
- en: iter_next()
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: iter_next()
- en: c(images, labels) %<—% batch
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: c(images, labels) %<—% batch
- en: par(mfrow = c(3, 3), mar = rep(.5, 4))➊
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: par(mfrow = c(3, 3), mar = rep(.5, 4))➊
- en: image <— images[1, , , ]
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: image <— images[1, , , ]
- en: plot(as.raster(as.array(image), max = 255))➋
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: plot(as.raster(as.array(image), max = 255))➋
- en: for (i in 2:9) {
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 2:9) {
- en: augmented_images <— data_augmentation(images)➌
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: augmented_images <— data_augmentation(images)➌
- en: augmented_image <— augmented_images[1, , , ]
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: augmented_image <— augmented_images[1, , , ]
- en: plot(as.raster(as.array(augmented_image), max = 255)➍
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: plot(as.raster(as.array(augmented_image), max = 255)➍
- en: '}'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Prepare the graphics device for nine images.**
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **准备用于九张图片的图形设备。**
- en: ➋ **Plot the first image of the batch, without augmentation.**
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **绘制批次的第一张图片，不进行增强。**
- en: ➌ **Apply the augmentation stage to the batch of images.**
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将增强阶段应用于图像批次。**
- en: ➍ **Display the first image in the output batch. For each of the eight iterations,
    this is a different augmentation of the same image.**
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **显示输出批次中的第一张图像。对于每个八次迭代，这是同一图像的不同增强。**
- en: '![Image](../images/f0242-01.jpg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0242-01.jpg)'
- en: '**Figure 8.10 Generating variations of a very good boy via random data augmentation**'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.10 通过随机数据增强生成非常好的狗的变化**'
- en: If we train a new model using this data-augmentation configuration, the model
    will never see the same input twice. But the inputs it sees are still heavily
    intercorrelated because they come from a small number of original images—we can’t
    produce new information; we can only remix existing information. As such, this
    may not be enough to completely get rid of overfitting. To further fight overfitting,
    we’ll also add a layer_ dropout() to our model right before the densely connected
    classifier.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用这个数据增强配置来训练一个新模型，那么模型将永远不会看到相同的输入两次。但是它看到的输入仍然存在很高的相关性，因为它们来自少量原始图像——我们无法产生新的信息；我们只能重新组合现有的信息。因此，这可能不足以完全消除过拟合。为了进一步对抗过拟合，我们还会在密集连接分类器之前向我们的模型添加一个`dropout()`层。
- en: 'One last thing you should know about random image augmentation layers: just
    like layer_dropout(), they’re inactive during inference (when we call predict()
    or evaluate()). During evaluation, our model will behave just the same as when
    it did not include data augmentation and dropout.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机图像增强层，你应该知道的最后一件事：就像`layer_dropout()`一样，在推理期间（当我们调用`predict()`或`evaluate()`时），它们是不活跃的。在评估期间，我们的模型的行为与不包括数据增强和
    dropout 时完全相同。
- en: Listing 8.16 Defining a new convnet that includes image augmentation and dropout
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.16 定义一个包括图像增强和 dropout 的新卷积神经网络
- en: inputs <— layer_input(shape = c(180, 180, 3))
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <— layer_input(shape = c(180, 180, 3))
- en: outputs <— inputs %>%
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <— inputs %>%
- en: data_augmentation() %>%
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: data_augmentation() %>%
- en: layer_rescaling(1 / 255) %>%
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: layer_rescaling(1 / 255) %>%
- en: layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>% layer_conv_2d(filters = 64, kernel_size
    = 3, activation = "relu") %>%
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>% layer_conv_2d(filters = 64, kernel_size
    = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>%
- en: layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>%
- en: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
- en: layer_max_pooling_2d(pool_size = 2) %>%
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: layer_max_pooling_2d(pool_size = 2) %>%
- en: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: layer_conv_2d(filters = 256, kernel_size = 3, activation = "relu") %>%
- en: layer_flatten() %>%
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: layer_flatten() %>%
- en: layer_dropout(0.5) %>%
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <— keras_model(inputs, outputs)
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: model <— keras_model(inputs, outputs)
- en: model %>% compile(loss = "binary_crossentropy",
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(loss = "binary_crossentropy",
- en: optimizer = "rmsprop",
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: metrics = "accuracy")
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: Let’s train the model using data augmentation and dropout. Because we expect
    over-fitting to occur much later during training, we will train for three times
    as many epochs—one hundred.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用数据增强和 dropout 训练模型。因为我们预计过拟合将在训练期间晚得多，所以我们将训练三倍的轮次——一百轮。
- en: Listing 8.17 Training the regularized convnet
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.17 训练正则化的卷积神经网络
- en: callbacks <— list(
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <— list(
- en: callback_model_checkpoint(
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint(
- en: filepath = "convnet_from_scratch_with_augmentation.keras",
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = "convnet_from_scratch_with_augmentation.keras",
- en: save_best_only = TRUE,
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE,
- en: monitor = "val_loss"
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: monitor = "val_loss"
- en: )
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: )
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: history <— model %>% fit(
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: history <— model %>% fit(
- en: train_dataset,
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset,
- en: epochs = 100,
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 100,
- en: validation_data = validation_dataset,
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = validation_dataset,
- en: callbacks = callbacks
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Let’s plot the results again: see [figure 8.11](#fig8-11). Thanks to data augmentation
    and dropout, we start overfitting much later, around epochs 60–70 (compared to
    epoch 10 for the original model). The validation accuracy ends up consistently
    in the 80–85% range—a big improvement over our first try:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次绘制结果：参见[图 8.11](#fig8-11)。由于数据增强和 dropout，我们开始在很晚的时候过拟合，大约在 60–70 轮之后（对比原始模型的
    10 轮）。验证准确率最终保持在 80–85% 的范围内，这是我们第一次尝试的显著改进：
- en: '![Image](../images/f0244-01.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0244-01.jpg)'
- en: '**Figure 8.11 Training and validation metrics with data augmentation**'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.11 使用数据增强的训练和验证指标**'
- en: Let’s check the test accuracy.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查测试准确率。
- en: Listing 8.18 Evaluating the model on the test set
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.18 在测试集上评估模型
- en: test_model <— load_model_tf("convnet_from_scratch_with_augmentation.keras")
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: test_model <— load_model_tf("convnet_from_scratch_with_augmentation.keras")
- en: result <— evaluate(test_model, test_dataset)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 <— evaluate(test_model, test_dataset)
- en: 'cat(sprintf("Test accuracy: %.3f\n", result["accuracy"]))'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("测试准确率：%.3f\n", result["accuracy"]))
- en: 'Test accuracy: 0.814'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率：0.814
- en: We get a test accuracy of 81.4%. It’s starting to look good! If you’re running
    the code, make sure to keep the saved file (convnet_from_scratch_with_augmentation.keras),
    because we will use it for some experiments in the next chapter.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了81.4%的测试准确率。看起来不错！如果您正在运行代码，请确保保留保存的文件（convnet_from_scratch_with_augmentation.keras），因为我们将在下一章中用到它进行一些实验。
- en: By further tuning the model’s configuration (such as the number of filters per
    convolution layer, or the number of layers in the model), we might be able to
    get an even better accuracy, likely up to 90%. But it would prove difficult to
    go any higher just by training our own convnet from scratch, because we have so
    little data to work with. As a next step to improve our accuracy on this problem,
    we’ll have to use a pretrained model, which is the focus of the next two sections.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 通过进一步调整模型的配置（例如每个卷积层的滤波器数量，或者模型中的层数），我们可能会获得更高的准确率，可能高达90%。但是仅通过从头训练我们自己的卷积网络，要达到更高的准确率将会很困难，因为我们的数据量太少。为了提高这个问题上的准确率，我们的下一步将是使用预训练模型，这是接下来两节的重点。
- en: 8.3 Leveraging a pretrained model
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 利用预训练模型
- en: A common and highly effective approach to deep learning on small image datasets
    is to use a pretrained model. A *pretrained model* is a model that was previously
    trained on a large dataset, typically on a large-scale image-classification task.
    If this original data-set is large enough and general enough, the spatial hierarchy
    of features learned by the pretrained model can effectively act as a generic model
    of the visual world, and hence, its features can prove useful for many different
    computer vision problems, even though these new problems may involve completely
    different classes than those of the original task. For instance, you might train
    a model on ImageNet (where classes are mostly animals and everyday objects) and
    then repurpose this trained model for something as remote as identifying furniture
    items in images. Such portability of learned features across different problems
    is a key advantage of deep learning compared to many older, shallow learning approaches,
    and it makes deep learning very effective for small-data problems.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在小图像数据集上进行深度学习的一种常见且非常有效的方法是使用预训练模型。*预训练模型*是以前在大型数据集上训练过的模型，通常是在大规模图像分类任务上。如果这个原始数据集足够大且足够通用，那么预训练模型学到的特征的空间层次结构可以有效地充当视觉世界的通用模型，因此，它的特征可以对许多不同的计算机视觉问题提供有用的信息，即使这些新问题可能涉及与原始任务完全不同的类别。例如，您可以在ImageNet上训练模型（其中大多数类别是动物和日常物品），然后将这个训练好的模型重新用于识别图像中的家具项目等完全不同的目标。深度学习与许多较老的、浅层的学习方法相比的一个关键优势是，它学到的特征在不同问题之间的可移植性，这使得深度学习对于小数据问题非常有效。
- en: In this case, let’s consider a large convnet trained on the ImageNet dataset
    (1.4 million labeled images and 1,000 different classes). ImageNet contains many
    animal classes, including different species of cats and dogs, and you can thus
    expect it to perform well on the dogs-versus-cats classification problem.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，让我们考虑一个在ImageNet数据集上训练的大型卷积网络（140万张带标签的图像和1000个不同的类）。ImageNet包含许多动物类别，包括不同种类的猫和狗，因此您可以期望它在猫狗分类问题上表现良好。
- en: We’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman
    in 2014.^([1](#Rendnote1)) Although it’s an older model, far from the current
    state of the art and somewhat heavier than many other recent models, I chose it
    because its architecture is similar to what you’re already familiar with, and
    it’s easy to understand without introducing any new concepts. This may be your
    first encounter with one of these cutesy model names—VGG, ResNet, Inception, Xception,
    and so on; you’ll get used to them because they will come up frequently if you
    keep doing deep learning for computer vision.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由Karen Simonyan和Andrew Zisserman于2014年开发的VGG16架构^([1](#Rendnote1))。尽管这是一个较老的模型，远非当前技术水平的最新状态，并且比许多其他近期模型更加笨重，但我选择它是因为它的架构与你已经熟悉的相似，并且不需要引入任何新概念就能轻松理解。这可能是你第一次遇到这些可爱的模型名称之一——VGG、ResNet、Inception、Xception等等；如果你继续进行计算机视觉的深度学习，你将经常遇到它们。
- en: 'There are two ways to use a pretrained model: *feature extraction* and *fine-tuning*.
    We’ll cover both of them. Let’s start with feature extraction.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种使用预训练模型的方式：*特征提取*和*微调*。我们将会涵盖这两种方法。让我们从特征提取开始。
- en: 8.3.1 Feature extraction with a pretrained model
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 使用预训练模型进行特征提取
- en: Feature extraction consists of using the representations learned by a previously
    trained model to extract interesting features from new samples. These features
    are then run through a new classifier, which is trained from scratch.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取包括使用先前训练好的模型学到的表示从新样本中提取有趣的特征。然后这些特征通过一个从头开始训练的新分类器。
- en: 'As you saw previously, convnets used for image classification comprise two
    parts: they start with a series of pooling and convolution layers, and they end
    with a densely connected classifier. The first part is called the *convolutional
    base* of the model. In the case of convnets, feature extraction consists of taking
    the convolutional base of a previously trained network, running the new data through
    it, and training a new classifier on top of the output (see [figure 8.12](#fig8-12)).'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前所看到的，用于图像分类的卷积神经网络包括两部分：它们首先是一系列的池化和卷积层，然后是一个全连接的分类器。第一部分被称为模型的*卷积基*。在卷积神经网络中，特征提取是指利用先前训练好的网络的卷积基对新数据进行传递，并在其之上训练一个新的分类器（参见[图8.12](#fig8-12)）。
- en: '![Image](../images/f0246-01.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0246-01.jpg)'
- en: '**Figure 8.12 Swapping classifiers while keeping the same convolutional base**'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.12 保持相同的卷积基进行分类器交换**'
- en: 'Why reuse only the convolutional base? Could we reuse the densely connected
    classifier as well? In general, doing so should be avoided. The reason is that
    the representations learned by the convolutional base are likely to be more generic
    and, therefore, more reusable: the feature maps of a convnet are presence maps
    of generic concepts over a picture, which are likely to be useful regardless of
    the computer vision problem at hand. But the representations learned by the classifier
    will necessarily be specific to the set of classes on which the model was trained—they
    will contain only information about the presence probability of this or that class
    in the entire picture. Additionally, representations found in densely connected
    layers no longer contain any information about where objects are located in the
    input image; these layers get rid of the notion of space, whereas the object location
    is still described by convolutional feature maps. For problems where object location
    matters, densely connected features are largely useless.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么仅重用卷积基？我们能否也重用全连接的分类器？一般来说，应该避免这样做。原因是卷积基学到的表示可能更通用，因此更易重用：卷积神经网络的特征图是图片中通用概念的存在图，无论当前的计算机视觉问题是什么，该特征图都可能是有用的。但是分类器学到的表示将必然特定于模型训练的类别集，它们只包含关于这个或那个类别在整个图片中出现概率的信息。此外，在全连接层中发现的表示不再包含有关物体在输入图像中位置的任何信息；这些层丢弃了空间概念，而卷积特征图仍然描述着物体的位置。对于物体位置很重要的问题，全连接特征基本上是无用的。
- en: Note that the level of generality (and, therefore, reusability) of the representations
    extracted by specific convolution layers depends on the depth of the layer in
    the model. Layers that come earlier in the model extract local, highly generic
    feature maps (such as visual edges, colors, and textures), whereas layers that
    are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”).
    So if your new dataset differs a lot from the dataset on which the original model
    was trained, you may be better off using only the first few layers of the model
    to do feature extraction, rather than the entire convolutional base.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，特定卷积层提取的表示的一般性（因此也是可重用性）取决于模型中的层的深度。在模型中更早的层提取局部的高度通用的特征图（例如视觉边缘，颜色和纹理），而更高的层提取更抽象的概念（如“猫耳”或“狗眼”）。因此，如果你的新数据集与原始模型训练的数据集差异很大，你可能最好只使用模型的前几层进行特征提取，而不是整个卷积基。
- en: In this case, because the ImageNet class set contains multiple dog and cat classes,
    it’s likely to be beneficial to reuse the information contained in the densely
    connected layers of the original model. But we’ll choose not to, to cover the
    more general case where the class set of the new problem doesn’t overlap the class
    set of the original model. Let’s put this into practice by using the convolutional
    base of the VGG16 network, trained on ImageNet, to extract interesting features
    from cat and dog images and then train a dogs-versus-cats classifier on top of
    these features.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于 ImageNet 类集包含多个狗和猫类别，重用原始模型的密集连接层中包含的信息可能是有益的。但我们选择不这样做，以涵盖新问题的类别集不重叠于原始模型的类别集的更普遍情况。让我们通过使用在
    ImageNet 上训练的 VGG16 网络的卷积基础，从猫和狗图像中提取有趣的特征，然后在这些特征的顶部训练一个猫狗分类器来将其付诸实践。
- en: 'The VGG16 model, among others, comes prepackaged with Keras. They all are exported
    as functions with the prefix application_. Many other image-classification models
    (all pretrained on the ImageNet dataset) are available as part of the Keras applications:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 模型等等，都预先在 Keras 中打包。它们都作为以 application_ 前缀开头的函数导出。许多其他图像分类模型（都是在 ImageNet
    数据集上预训练的）都作为 Keras 应用程序的一部分提供：
- en: Xception
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xception
- en: Mobilenet
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mobilenet
- en: DenseNet
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet
- en: ResNet
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: EfficientNet
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EfficientNet
- en: And so on
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等
- en: Let’s instantiate the VGG16 model.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化 VGG16 模型。
- en: Listing 8.19 Instantiating the VGG16 convolutional base
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.19 实例化 VGG16 卷积基础
- en: conv_base <— application_vgg16(
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: conv_base <— application_vgg16(
- en: weights = "imagenet",
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: weights = "imagenet",
- en: include_top = FALSE,
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: include_top = FALSE,
- en: input_shape = c(180, 180, 3)
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape = c(180, 180, 3)
- en: )
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'We pass three arguments to the application function:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向应用函数传递了三个参数：
- en: weights specifies the weight checkpoint from which to initialize the model.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: weights 指定初始化模型的权重检查点。
- en: 'include_top refers to including (or not) the densely connected classifier on
    top of the network. By default, this densely connected classifier corresponds
    to the 1,000 classes from ImageNet. Because we intend to use our own densely connected
    classifier (with only two classes: cat and dog), we don’t need to include it.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: include_top 指的是是否在网络顶部包含（或不包含）密集连接分类器。默认情况下，这个密集连接分类器对应于 ImageNet 的 1,000 个类别。因为我们打算使用自己的密集连接分类器（只有两个类别：猫和狗），所以我们不需要包含它。
- en: 'input_shape is the shape of the image tensors that we’ll feed to the network.
    This argument is purely optional: if we don’t pass it, the network will be able
    to process inputs of any size. Here we pass it so that we can visualize (in the
    following summary) how the size of the feature maps shrinks with each new convolution
    and pooling layer.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: input_shape 是我们将要馈送到网络中的图像张量的形状。这个参数是可选的：如果我们不传递它，网络将能够处理任何大小的输入。在这里，我们传递它，以便我们可以可视化（在以下摘要中）随着每个新的卷积和池化层的形状如何收缩。
- en: 'Here’s the detail of the architecture of the VGG16 convolutional base. It’s
    similar to the simple convnets you’re already familiar with:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 VGG16 卷积基础架构的详细信息。它与你已经熟悉的简单卷积网络类似：
- en: conv_base
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: conv_base
- en: '![Image](../images/f0248-01.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0248-01.jpg)'
- en: The final feature map has shape (5, 5, 512). That’s the feature map on top of
    which we’ll stick a densely connected classifier.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的特征图形状为 (5, 5, 512)。这就是我们将要在其顶部添加一个密集连接分类器的特征图。
- en: 'At this point, we could proceed in two ways:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以采取两种方式：
- en: Run the convolutional base over our dataset, record its output (arrays) to a
    file array on disk, and then use this data as input to a standalone, densely connected
    classifier similar to those you saw in chapter 4 of this book. This solution is
    fast and cheap to run, because it requires running the convolutional base only
    once for every input image, and the convolutional base is by far the most expensive
    part of the pipeline. But for the same reason, this technique won’t allow us to
    use data augmentation.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的数据集上运行卷积基础，将其输出（数组）记录到磁盘上的文件数组中，然后使用这些数据作为章节 4 中所见的独立、密集连接分类器的输入。这种解决方案运行速度快，成本低，因为它只需要对每个输入图像运行一次卷积基础，而卷积基础是流水线中成本最高的部分。但出于同样的原因，这种技术不允许我们使用数据增强。
- en: Extend the model we have (conv_base) by adding Dense layers on top, and run
    the whole thing from end to end on the input data. This will allow us to use data
    augmentation, because every input image goes through the convolutional base every
    time it’s seen by the model. But for the same reason, this technique is far more
    expensive than the first
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展我们已有的模型（conv_base），在其顶部添加密集层，并在输入数据上端到端地运行整个模型。这将允许我们使用数据增强，因为每个输入图像在被模型看到时都会经过卷积基。但出于同样的原因，这种技术比第一个技术要昂贵得多。
- en: 'We’ll cover both techniques. Let’s walk through the code required to set up
    the first one: recording the output of conv_base on our data and using these outputs
    as inputs to a new model.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖这两种技术。让我们逐步介绍设置第一种技术所需的代码：记录 conv_base 在我们的数据上的输出，并将这些输出用作新模型的输入。
- en: '**FAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION**'
  id: totrans-466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**无数据增强的快速特征提取**'
- en: We’ll start by extracting features as R arrays by calling the predict() method
    of the conv_base model on our training, validation, and testing datasets.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过调用 conv_base 模型的 predict() 方法在我们的训练、验证和测试数据集上提取特征作为 R 数组。
- en: Let’s iterate over our datasets to extract the VGG16 features.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们遍历我们的数据集以提取 VGG16 特征。
- en: '**Listing 8.20 Extracting the VGG16 features and corresponding labels**'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 8.20 提取 VGG16 特征和相应的标签**'
- en: get_features_and_labels <- function(dataset) {
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: get_features_and_labels <- function(dataset) {
- en: n_batches <- length(dataset)
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: n_batches <- length(dataset)
- en: all_features <- vector("list", n_batches)
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: all_features <- vector("list", n_batches)
- en: all_labels <- vector("list", n_batches)
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: all_labels <- vector("list", n_batches)
- en: iterator <- as_array_iterator(dataset)
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: iterator <- as_array_iterator(dataset)
- en: for (i in 1:n_batches) {
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:n_batches) {
- en: c(images, labels) %<-% iter_next(iterator)
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: c(images, labels) %<-% iter_next(iterator)
- en: preprocessed_images <- imagenet_preprocess_input(images)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: preprocessed_images <- imagenet_preprocess_input(images)
- en: features <- conv_base %>% predict(preprocessed_images)
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: features <- conv_base %>% predict(preprocessed_images)
- en: all_labels[[i]] <- labels
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: all_labels[[i]] <- labels
- en: all_features[[i]] <- features
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: all_features[[i]] <- features
- en: '}'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: all_features <- listarrays::bind_on_rows(all_features)
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: all_features <- listarrays::bind_on_rows(all_features)
- en: all_labels <- listarrays::bind_on_rows(all_labels)➊
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: all_labels <- listarrays::bind_on_rows(all_labels)➊
- en: list(all_features, all_labels)
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: list(all_features, all_labels)
- en: '}'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: c(train_features, train_labels) %<-% get_features_and_labels(train_dataset)
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: c(train_features, train_labels) %<-% get_features_and_labels(train_dataset)
- en: c(val_features, val_labels) %<-% get_features_and_labels(validation_dataset)
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: c(val_features, val_labels) %<-% get_features_and_labels(validation_dataset)
- en: c(test_features, test_labels) %<-% get_features_and_labels(test_dataset)
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: c(test_features, test_labels) %<-% get_features_and_labels(test_dataset)
- en: ➊**Combine a list of R arrays along the first axis, the batch dimension.**
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ➊**将一系列R数组沿着第一个轴（批处理维度）组合在一起。**
- en: 'Importantly, predict() expects only images, not labels, but our current dataset
    yields batches that contain both images and their labels. Moreover, the VGG16
    model expects inputs that are preprocessed with the function imagenet_preprocess_input(),
    which scales pixel values to an appropriate range. The extracted features are
    currently of shape (samples, 5, 5, 512):'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，predict() 仅期望图像，而不是标签，但我们当前的数据集生成的批次包含图像及其标签。此外，VGG16 模型期望经过函数 imagenet_preprocess_input()
    预处理的输入，该函数将像素值缩放到合适的范围。提取的特征目前的形状为（样本数，5，5，512）：
- en: dim(train_features)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: dim(train_features)
- en: '[1] 2000 5 5 512'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 2000 5 5 512'
- en: At this point, we can define our densely connected classifier (note the use
    of dropout for regularization) and train it on the data and labels that we just
    recorded.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以定义我们的密集连接的分类器（请注意使用了 dropout 进行正则化），并在我们刚刚记录的数据和标签上对其进行训练。
- en: '**Listing 8.21 Defining and training the densely connected classifier**'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 8.21 定义和训练密集连接的分类器**'
- en: inputs <- layer_input(shape = c(5, 5, 512))
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(5, 5, 512))
- en: outputs <- inputs %>%
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_flatten() %>%➊
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: layer_flatten() %>%➊
- en: layer_dense(256) %>%
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(256) %>%
- en: layer_dropout(.5) %>%
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(loss = "binary_crossentropy",
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(loss = "binary_crossentropy",
- en: optimizer = "rmsprop",
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: metrics = "accuracy")
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: callbacks <- list(
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(
- en: callback_model_checkpoint(
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint(
- en: filepath = "feature_extraction.keras",
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = "feature_extraction.keras",
- en: save_best_only = TRUE,
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE,
- en: monitor = "val_loss"
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: monitor = "val_loss"
- en: )
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: )
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: history <- model %>% fit(
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_features, train_labels,
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: train_features, train_labels,
- en: epochs = 20,
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20,
- en: validation_data = list(val_features, val_labels),
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(val_features, val_labels),
- en: callbacks = callbacks
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊**Note the use of the layer_flatten() before passing the features to a layer_dense().**
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ➊**注意在传递特征给 `layer_dense()` 之前使用了 `layer_flatten()`。**
- en: Training is very fast because we have to deal with only two dense layers—an
    epoch takes less than one second, even on a CPU.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非常快，因为我们只需处理两个密集层 - 一个时代少于一秒，甚至在 CPU 上也是如此。
- en: Let’s look at the loss and accuracy curves during training (see [figure 8.13](#fig8-13)).
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在训练过程中查看损失和准确率曲线（见 [图 8.13](#fig8-13)）。
- en: '**Listing 8.22 Plotting the results**'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 8.22 绘制结果**'
- en: plot(history)
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制（history）
- en: '![Image](../images/f0250-01.jpg)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0250-01.jpg)'
- en: '**Figure 8.13 Training and validation metrics for plain feature extraction**'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.13 普通特征提取的训练和验证指标**'
- en: We reach a validation accuracy of about 97%—much better than we achieved in
    the previous section with the small model trained from scratch. This is a bit
    of an unfair comparison, however, because ImageNet contains many dog and cat instances,
    which means that our pretrained model already has the exact knowledge required
    for the task at hand. This won’t always be the case when you use pretrained features.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 我们达到了约 97% 的验证准确率 - 比我们在上一节使用从头开始训练的小模型取得的结果要好得多。然而，这有点不公平的比较，因为 ImageNet 包含许多狗和猫实例，这意味着我们的预训练模型已经具有了所需任务的确切知识。当您使用预训练特征时，情况并非总是如此。
- en: However, the plots also indicate that we’re overfitting almost from the start,
    despite using dropout with a fairly large rate. That’s because this technique
    doesn’t use data augmentation, which is essential for preventing overfitting with
    small image datasets.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图表也显示，尽管使用了相当大的丢弃率，但我们几乎从一开始就出现了过拟合。这是因为这种技术不使用数据增强，而数据增强对于防止小图像数据集过拟合是至关重要的。
- en: FEATURE EXTRACTION TOGETHER WITH DATA AUGMENTATION
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征提取与数据增强
- en: 'Now let’s review the second technique I mentioned for doing feature extraction,
    which is much slower and more expensive, but which allows us to use data augmentation
    during training: creating a model that chains the conv_base with a new dense classifier,
    and training it end to end on the inputs.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾我提到的第二种做特征提取的技术，它要慢得多，更昂贵，但允许我们在训练过程中使用数据增强：创建一个将卷积基与新的密集分类器链在一起的模型，并在输入上端到端地训练它。
- en: To do this, we will first *freeze the convolutional base. Freezing* a layer
    or set of layers means preventing their weights from being updated during training.
    If we don’t do this, the representations that were previously learned by the convolutional
    base will be modified during training. Because the dense layers on top are randomly
    initialized, very large weight updates would be propagated through the network,
    effectively destroying the representations previously learned. In Keras, we freeze
    a layer or model by calling freeze_weights().
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先会 *冻结卷积基*。冻结层或一组层意味着在训练期间阻止它们的权重更新。如果我们不这样做，那么先前由卷积基学到的表示将在训练期间被修改。因为顶部的密集层是随机初始化的，所以会通过网络传播非常大的权重更新，有效地破坏先前学到的表示。在
    Keras 中，我们通过调用 `freeze_weights()` 来冻结层或模型。
- en: '**Listing 8.23 Instantiating and freezing the VGG16 convolutional base**'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 8.23 实例化和冻结 VGG16 卷积基**'
- en: conv_base <- application_vgg16(
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: conv_base <- application_vgg16(
- en: weights = "imagenet",
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 权重 = "imagenet",
- en: include_top = FALSE)
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: include_top = FALSE)
- en: freeze_weights(conv_base)
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: freeze_weights(conv_base)
- en: Calling freeze_weights() empties the list of trainable weights of the layer
    or model.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `freeze_weights()` 会清空层或模型的可训练权重列表。
- en: '**Listing 8.24 Printing the list of trainable weights before and after freezing**'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 8.24 打印在冻结之前和之后的可训练权重列表**'
- en: unfreeze_weights(conv_base)
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: unfreeze_weights(conv_base)
- en: cat("This is the number of trainable weights",
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: cat("这是可训练权重的数量",
- en: '"before freezing the conv base:",'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '"冻结卷积基之前："'
- en: length(conv_base$trainable_weights), "\n")
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: length(conv_base$trainable_weights), "\n")
- en: 'This is the number of trainable weights before freezing the conv base: 26'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在冻结卷积基之前的可训练权重数量：26
- en: freeze_weights(conv_base)
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: freeze_weights(conv_base)
- en: cat("This is the number of trainable weights",
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: cat("这是可训练权重的数量",
- en: '"after freezing the conv base:",'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '"冻结卷积基之后："'
- en: length(conv_base$trainable_weights), "\n")
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: length(conv_base$trainable_weights), "\n")
- en: 'This is the number of trainable weights after freezing the conv base: 0'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结卷积基之后的可训练权重数量为：0
- en: Now we can create a new model that chains together
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个新模型，将特征链接在一起
- en: '**1** A data augmentation stage'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 数据增强阶段'
- en: '**2** Our frozen convolutional base'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 我们冻结的卷积基'
- en: '**3** A dense classifier'
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 一个密集分类器'
- en: data_augmentation <- keras_model_sequential() %>%
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强 <- keras_model_sequential() %>%
- en: layer_random_flip("horizontal") %>%
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_flip("horizontal") %>%
- en: layer_random_rotation(0.1) %>%
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_rotation(0.1) %>%
- en: layer_random_zoom(0.2)
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: layer_random_zoom(0.2)
- en: inputs <- layer_input(shape = c(180, 180, 3))
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 <- layer_input(shape = c(180, 180, 3))
- en: outputs <- inputs %>%
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <- 输入 %>%
- en: data_augmentation() %>%➊
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: data_augmentation() %>%➊
- en: imagenet_preprocess_input() %>%➋
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: imagenet_preprocess_input() %>%➋
- en: conv_base() %>%
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: conv_base() %>%
- en: layer_flatten() %>%
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: layer_flatten() %>%
- en: layer_dense(256) %>%
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(256) %>%
- en: layer_dropout(0.5) %>%
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(loss = "binary_crossentropy",
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(loss = "binary_crossentropy",
- en: optimizer = "rmsprop",
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器 = "rmsprop",
- en: metrics = "accuracy")
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "准确度")
- en: ➊**Apply data augmentation.**
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: ➊**应用数据增强。**
- en: ➋ **Apply input value scaling.**
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **应用输入值缩放。**
- en: 'With this setup, only the weights from the two dense layers that we added will
    be trained. That’s a total of four weight tensors: two per layer (the main weight
    matrix and the bias vector). Note that for these changes to take effect, you must
    first compile the model. If you ever modify weight trainability after compilation,
    you should then recompile the model, or these changes will be ignored.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此设置，只有我们添加的两个密集层的权重将会被训练。总共有四个权重张量：每层两个（主要权重矩阵和偏置向量）。注意，为了使这些更改生效，您必须先编译模型。如果您在编译之后修改了权重的可训练性，那么您应该重新编译模型，否则这些更改将被忽略。
- en: Let’s train our model. Thanks to data augmentation, it will take much longer
    for the model to start overfitting, so we can train for more epochs—let’s do 50.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练我们的模型。由于数据增强，模型要过拟合的时间要长得多，所以我们可以训练更多的epochs——让我们做50个。
- en: 'This technique is expensive enough that you should attempt it only if you have
    access to a GPU—it’s intractable on CPU. If you can’t run your code on GPU, then
    the previous technique is the way to go:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术够昂贵，只有在有GPU的情况下才尝试，CPU上无法操作。如果无法在GPU上运行代码，那么前一项技术就是最佳选择：
- en: callbacks <- list(
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 回调 <- list(
- en: callback_model_checkpoint(
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 回调模型检查点(
- en: filepath = "feature_extraction_with_data_augmentation.keras",
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 文件路径 = "使用数据增强进行特征提取.keras",
- en: save_best_only = TRUE,
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 仅保存最佳结果 = TRUE,
- en: monitor = "val_loss"
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 = "val_loss"
- en: )
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: )
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: history <- model %>% fit(
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_dataset,
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集,
- en: epochs = 50,
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 50,
- en: validation_data = validation_dataset,
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据 = 验证数据集，
- en: callbacks = callbacks
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 回调 = 回调
- en: )
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Let’s plot the results again (see [figure 8.14](#fig8-14)). As you can see,
    we reach a validation accuracy of over 98%. This is a strong improvement over
    the previous model.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次绘制结果（参见[图8.14](#fig8-14)）。如您所见，我们达到了超过98%的验证准确度。这是相对于先前模型的显著提高。
- en: '![Image](../images/f0253-01.jpg)'
  id: totrans-587
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0253-01.jpg)'
- en: '**Figure 8.14 Training and validation metrics for feature extraction with data
    augmentation**'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8.14 使用数据增强进行特征提取的训练和验证指标**'
- en: Let’s check the test accuracy.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下测试准确度。
- en: '**Listing 8.25 Evaluating the model on the test set**'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 8.25 在测试集上评估模型**'
- en: test_model <- load_model_tf(
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型 <- load_model_tf(
- en: '"feature_extraction_with_data_augmentation.keras")'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '"使用数据增强进行特征提取.keras")'
- en: result <- evaluate(test_model, test_dataset)
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: result <- evaluate(test_model, test_dataset)
- en: 'cat(sprintf("Test accuracy: %.3f\n", result["accuracy"]))'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("测试精度：%.3f\n", result["准确度"]))
- en: 'Test accuracy: 0.977'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 测试精度：0.977
- en: We get a test accuracy of 97.7%. This is only a modest improvement compared
    to the previous test accuracy, which is a bit disappointing, given the strong
    results on the validation data. A model’s accuracy always depends on the set of
    samples you evaluate it on. Some sample sets may be more difficult than others,
    and strong results on one set won’t necessarily fully translate to all other sets.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了97.7%的测试准确度。与以前的测试准确度相比，这只是一次适度的提高，这有点令人失望，考虑到在验证数据上的强劲结果。模型的准确度总是取决于您对其进行评估的样本集。有些样本集可能比其他样本集更难，对一个集合的强烈结果未必会完全转化到其他所有集合。
- en: 8.3.2 Fine-tuning a pretrained model
  id: totrans-597
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 微调预训练模型
- en: Another widely used technique for model reuse, complementary to feature extraction,
    is *fine-tuning* (see [figure 8.15](#fig8-15)). Fine-tuning consists of unfreezing
    a few of the top layers of a frozen model base used for feature extraction and
    jointly training both the newly added part of the model (in this case, the fully
    connected classifier) and these top layers. This is called *fine-tuning* because
    it slightly adjusts the more abstract representations of the model being reused
    to make them more relevant for the problem at hand.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步复用模型的一个广泛使用的技巧是微调（fine-tuning），与特征提取互补（见[图 8.15](#fig8-15)）。 微调包含解冻一个用于特征提取的模型基底的顶部几层，同时联合训练这个新添加的部分（在本例中是全连接分类器）和这些顶部层。这称为微调，是因为它会稍微调整正在复用的模型的更抽象表示，使其更相关于手头的问题。
- en: 'I stated earlier that it’s necessary to freeze the convolution base of VGG16
    to be able to train a randomly initialized classifier on top. For the same reason,
    it’s possible to fine-tune the top layers of the convolutional base only once
    the classifier on top has already been trained. If the classifier isn’t already
    trained, the error signal propagating through the network during training will
    be too large, and the representations previously learned by the layers being fine-tuned
    will be destroyed. Thus the steps for fine-tuning a network are as follows:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前说过，必须冻结 VGG16 的卷积基，才能够在其上训练随机初始化的分类器。出于同样的原因，只有在距离分类器先前已经训练好的情况下，才能微调卷积基的顶部层。如果分类器还没有进行训练，训练过程中传播的错误信号会过大，并且微调的层之前学习到的表示将被破坏。因此微调网络的步骤如下：
- en: '**1** Add our custom network on top of an already-trained base network.'
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 在已经训练好的基础网络上添加自定义网络。'
- en: '**2** Freeze the base network.'
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 冻结基础网络。'
- en: '**3** Train the part we added.'
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 训练我们添加的部分。'
- en: '**4** Unfreeze some layers in the base network. (Note that you should not unfreeze
    “batch normalization” layers, which are not relevant here because there are no
    such layers in VGG16\. Batch normalization and its impact on fine-tuning is explained
    in the next chapter.)'
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**4** 解冻基础网络中的一些层。（请注意，不应解冻“批量标准化”层，因为在 VGG16 中没有这样的层。批量标准化及其对微调的影响将在下一章中解释。）'
- en: '**5** Jointly train both these layers and the part we added.'
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**5** 联合训练这两个层和我们添加的部分。'
- en: 'You already completed the first three steps when doing feature extraction.
    Let’s proceed with step 4: we’ll unfreeze our conv_base and then freeze individual
    layers inside it.'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 进行特征提取时，前三个步骤已经完成，现在进行第四个步骤：解冻 conv_base，然后冻结其中的各个层。
- en: '![Image](../images/f0254-01.jpg)'
  id: totrans-606
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0254-01.jpg)'
- en: '**Figure 8.15 Fine-tuning the last convolutional block of the VGG16 network**'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8.15  调整 VGG16 网络的最后一个卷积块**'
- en: 'As a reminder, this is what our convolutional base looks like:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，这就是我们的卷积基长这样子：
- en: conv_base
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: conv_base
- en: '![Image](../images/f0255-01.jpg)'
  id: totrans-610
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0255-01.jpg)'
- en: We’ll fine-tune the last three convolutional layers, which means all layers
    up to block4_pool should be frozen, and the layers block5_conv1, block5_conv2,
    and block5_conv3 should be trainable.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调整最后三个卷积层，这意味着所有层，包括 block4_pool 之前的层都应该保持冻结状态，而 block5_conv1、block5_conv2
    和 block5_conv3 这三个层则应该可训练。
- en: 'Why not fine-tune more layers? Why not fine-tune the entire convolutional base?
    You could. But you need to consider the following:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不能调整更多层？为什么不能调整整个卷积基？其实可以，但要考虑以下几点：
- en: Earlier layers in the convolutional base encode more generic, reusable features,
    whereas layers higher up encode more specialized features. It’s more useful to
    fine-tune the more specialized features, because these are the ones that need
    to be repurposed on your new problem. There would be fast-decreasing returns in
    fine-tuning lower layers.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积基中越往前的层编码的是更加通用、可重复使用的特征，而越往后的层编码的是更加专业化的特征。 fine-tune 更加专业化的特征更有用，因为这些是需要在新问题上转化的。对低层进行调整，收益会逐渐变小。
- en: The more parameters you’re training, the more you’re at risk of overfitting
    The convolutional base has 15 million parameters, so it would be risky to attempt
    to train it on your small dataset.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果要训练的参数越多，过度拟合的风险就越大。卷积基有1500万个参数，因此在小数据集上尝试训练它是很冒险的。
- en: Thus, in this situation, it’s a good strategy to fine-tune only the top two
    or three layers in the convolutional base. Let’s set this up, starting from where
    we left off in the previous example.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，只微调卷积基准模型的前两层或三层是一个不错的策略。让我们从前一个例子结束的地方开始设定这个。
- en: Listing 8.26 Freezing all layers until the fourth from the last
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.26 冻结直到倒数第四层的所有层
- en: unfreeze_weights(conv_base, from = -4) conv_base➊
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: unfreeze_weights(conv_base, from = -4) conv_base➊
- en: '![Image](../images/f0256-01.jpg)'
  id: totrans-618
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0256-01.jpg)'
- en: ➊**from = -4 is shorthand for length(conv_base$layers) + 1 - 4**
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: ➊**from = -4 是 length(conv_base$layers) + 1 - 4 的简写**
- en: Now we can begin fine-tuning the model. We’ll do this with the RMSprop optimizer,
    using a very low learning rate. The reason for using a low learning rate is that
    we want to limit the magnitude of the modifications we make to the representations
    of the three layers we’re fine-tuning. Updates that are too large may harm these
    representations.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始微调模型了。我们将使用 RMSprop 优化器，并使用非常低的学习率。之所以使用低学习率，是因为我们希望限制对三层表示所做修改的幅度。太大的更新可能会损害这些表示。
- en: '**Listing 8.27 Fine-tuning the model**'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 8.27 微调模型**'
- en: model %>% compile(
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(
- en: loss = "binary_crossentropy",
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: optimizer = optimizer_rmsprop(learning_rate = 1e-5),
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = optimizer_rmsprop(learning_rate = 1e-5),
- en: metrics = "accuracy"
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy"
- en: )
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: callbacks <- list(
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(
- en: callback_model_checkpoint(
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint(
- en: filepath = "fine_tuning.keras",
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = "fine_tuning.keras",
- en: save_best_only = TRUE,
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE，
- en: monitor = "val_loss"
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: monitor = "val_loss"
- en: )
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: )
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: history <- model %>% fit(
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: train_dataset,
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: train_dataset，
- en: epochs = 30,
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 30,
- en: validation_data = validation_dataset,
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = validation_dataset,
- en: callbacks = callbacks
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'We can finally evaluate this model on the test data:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以对测试数据集评估这个模型：
- en: model <- load_model_tf("fine_tuning.keras")
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("fine_tuning.keras")
- en: result <- evaluate(model, test_dataset)
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: result <- evaluate(model, test_dataset)
- en: 'cat(sprintf("Test accuracy: %.3f\n", result["accuracy"]))'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("测试准确率：%.3f\n", result["accuracy"]))
- en: 'Test accuracy: 0.985'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率：0.985
- en: Here, we get a test accuracy of 98.5% (again, your own results may be within
    one percentage point). In the original Kaggle competition around this dataset,
    this would have been one of the top results. It’s not quite a fair comparison,
    however, because we used pretrained features that already contained prior knowledge
    about cats and dogs, which competitors couldn’t use at the time.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们获得了98.5%的测试准确率（再次强调，你自己的结果可能和这个相差不超过一个百分点）。在原始的 Kaggle 竞赛中，这将是顶级结果之一。但是这并不是一个公平的比较，因为我们使用了预训练的特征，这些特征已经包含了关于猫和狗的先前知识，而竞争对手当时无法使用。
- en: On the positive side, by leveraging modern deep learning techniques, we managed
    to reach this result using only a small fraction of the training data that was
    available for the competition (about 10%). There is a huge difference between
    being able to train on 20,000 samples compared to 2,000 samples!
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 从正面来说，借助现代深度学习技术，我们仅使用了竞赛数据中可用的一小部分训练数据（约占总量的 10%）就达到了这一结果。在可以训练 2,000 个样本和
    20,000 个样本之间，存在巨大的差距！
- en: Now you have a solid set of tools for dealing with image-classification problems—in
    particular, with small datasets.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经掌握了一套处理图像分类问题的工具，特别是处理小数据集时。
- en: Summary
  id: totrans-648
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Convnets are the best type of machine learning models for computer vision tasks.
    It’s possible to train one from scratch, even on a very small dataset, with decent
    results.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络是计算机视觉任务中最好的机器学习模型类型。即使在非常小的数据集上，也可以训练一个具有不错结果的模型。
- en: Convnets work by learning a hierarchy of modular patterns and concepts to represent
    the visual world.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络通过学习一系列模块化的模式和概念来表示视觉世界。
- en: On a small dataset, overfitting will be the main issue. Data augmentation is
    a powerful way to fight overfitting when you’re working with image data.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在小数据集上，过拟合是主要问题。数据增强是处理图像数据时对抗过拟合的有效手段。
- en: It’s easy to reuse an existing convnet on a new dataset via feature extraction.
    This is a valuable technique for working with small image datasets.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征提取，可以很容易地在新数据集上重用现有的卷积网络。这是处理小型图像数据集的一项有价值的技术。
- en: As a complement to feature extraction, you can use fine-tuning, which adapts
    to a new problem some of the representations previously learned by an existing
    model. This pushes performance a bit further.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为特征提取的补充，你可以使用微调（fine-tuning），它可以根据现有模型先前学到的某些表示来适应一个新的问题。这会进一步提高性能。
- en: ^([1](#endnote1)) Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional
    Networks for Large-Scale Image Recognition,” arXiv (2014), [https://arxiv.org/abs/1409.1556](https://www.arxiv.org/abs/1409.1556).
  id: totrans-654
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([1](#endnote1)) Karen Simonyan 和 Andrew Zisserman，《非常深的卷积网络用于大规模图像识别》，arXiv（2014），[https://arxiv.org/abs/1409.1556](https://www.arxiv.org/abs/1409.1556)。
