- en: '7 Working with Keras: A deep dive'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用Keras：深入研究
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容*'
- en: Creating Keras models with keras_model_ sequential(), the Functional API, and
    model subclassing
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用keras_model_sequential()、功能API和模型子类化创建Keras模型
- en: Using built-in Keras training and evaluation loops
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置的Keras训练和评估循环
- en: Using Keras callbacks to customize training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras回调自定义培训
- en: Using TensorBoard to monitor training and evaluation metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorBoard监控培训和评估指标
- en: Writing training and evaluation loops from scratch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '从头开始编写训练和评估循环 '
- en: You’ve now got some experience with Keras—you’re familiar with the Sequential
    model, dense layers, and built-in APIs for training, evaluation, and inference—
    compile(), fit(), evaluate(), and predict(). You even learned in chapter 3 how
    to use new_layer_class() to create custom layers, and how to use the TensorFlow
    GradientTape() to implement a step-by-step training loop.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经具备了一些使用Keras的经验——您熟悉序列模型、密集层和用于训练、评估和推理的内置API——compile()、fit()、evaluate()和predict()。您甚至在第3章中学习了如何使用new_layer_class()创建自定义层，以及如何使用
    TensorFlow GradientTape() 实现逐步训练循环。
- en: 'In the coming chapters, we’ll dig into computer vision, time-series forecasting,
    natural language processing, and generative deep learning. These complex applications
    will require much more than a keras_model_sequential() architecture and the default
    fit() loop. So, let’s first turn you into a Keras expert! In this chapter, you’ll
    get a complete overview of the key ways to work with Keras APIs: everything you’re
    going to need to handle the advanced deep learning use cases you’ll encounter
    next.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究计算机视觉、时间序列预测、自然语言处理和生成深度学习等复杂应用。这些复杂的应用需要多于一个keras_model_sequential()架构和默认的fit()循环。因此，让我们先将你变成一个Keras专家！在本章中，您将获得有关使用Keras
    API的关键方法的完整概述：这是您需要处理接下来会遇到的高级深度学习用例的所有内容。
- en: 7.1 A spectrum of workflows
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 工作流的多样性
- en: 'The design of the Keras API is guided by the principle of *progressive disclosure
    of complexity*: make it easy to get started, yet make it possible to handle high-complexity
    use cases, requiring only incremental learning at each step. Simple use cases
    should be easy and approachable, and arbitrarily advanced workflows should be
    *possible*: no matter how niche and complex the thing you want to do, there should
    be a clear path to it—a path that builds upon the various things you’ve learned
    from simpler workflows. This means that you can grow from beginner to expert and
    still use the same tools, only in different ways.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Keras API 的设计以*逐渐揭示复杂性*的原则为指导：让开始变得容易，但让处理高复杂度的用例成为可能，每个步骤只需要进行渐进式的学习。简单的用例应该易于接近，并且任意高级工作流程都应该是可能的：无论您想要做什么多么具有特色和复杂，都应该有一条明确的路径，它建立在您从较简单的工作流程中学到的各种事情之上。这意味着您可以从初学者成长为专家，仍然使用相同的工具，只是使用不同的方式。
- en: As such, there’s not a single “true” way of using Keras. Rather, Keras offers
    a *spectrum of workflows*, from the very simple to the very flexible. There are
    different ways to build Keras models, and different ways to train them, answering
    different needs. Because all these workflows are based on shared APIs, such as
    Layer and Model, components from any workflow can be used in any other workflow—they
    can all talk to each other.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Keras没有单一的“真正”使用方式。相反，Keras提供了一系列工作流，从非常简单到非常灵活。有不同的方法来构建Keras模型，以及不同的训练方法，以满足不同的需求。因为所有这些工作流都基于共享的API，例如层和模型，所以任何工作流的组件都可以在任何其他工作流中使用，它们都可以相互通信。
- en: 7.2 Different ways to build Keras models
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 构建 Keras 模型的不同方式
- en: 'Three APIs exist for building models in Keras (see [figure 7.1](#fig7-1)):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，有三个 API 可以用于构建模型（见[图7.1](#fig7-1)）：
- en: The *Sequential model*, the most approachable API—it’s basically a list. As
    such, it’s limited to simple stacks of layers.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sequential模型*是最易接近的API，它基本上是一个列表。因此，它仅限于简单的层堆栈。'
- en: The *Functional API*, which focuses on graph-like model architectures. It represents
    a nice midpoint between usability and flexibility, and as such, it’s the most
    commonly used model-building API.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*功能API*注重于类似于图形的模型架构。它代表了易用性和灵活性之间的不错平衡，因此是最常用的模型构建API。'
- en: '*Model subclassing*, a low-level option where you write everything yourself
    from scratch. This is ideal if you want full control over every little thing.
    However, you won’t get access to many built-in Keras features, and you will be
    more at risk of making mistakes'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型子类化* 是一个低级选项，你需要从头开始自己编写一切。这是理想的选择，如果你想要对每一件事情都有完全的控制。然而，你将无法访问许多内置的 Keras
    功能，并且更容易犯错误。'
- en: '![Image](../images/f0186-01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0186-01.jpg)'
- en: '**Figure 7.1 Progressive disclosure of complexity for model building**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.1 逐渐展示模型构建的复杂性**'
- en: 7.2.1 The Sequential model
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 Sequential 模型
- en: The simplest way to build a Keras model is to use keras_model_sequential(),
    which you already know about.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 Keras 模型的最简单方式是使用 keras_model_sequential()，这是你已经了解的。
- en: Listing 7.1 keras_model_sequential()
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.1 keras_model_sequential()
- en: library(keras)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: model <- keras_model_sequential() %>%
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: Note that it’s possible to build the same model incrementally with %>%.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，可以使用 %>% 逐步构建相同的模型。
- en: Listing 7.2 Incrementally building a Sequential model
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.2 逐步构建 Sequential 模型
- en: model <- keras_model_sequential()
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential()
- en: model %>% layer_dense(64, activation = "relu")
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% layer_dense(64, activation = "relu")
- en: model %>% layer_dense(10, activation = "softmax")
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% layer_dense(10, activation = "softmax")
- en: 'You saw in chapter 4 that layers get built (which is to say, create their weights)
    only when they are called for the first time. That’s because the shape of the
    layers’ weights depends on the shape of their input: until the input shape is
    known, they can’t be created.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四章中，你看到层是在第一次调用时构建的（也就是说，创建它们的权重）。这是因为层的权重形状取决于它们的输入形状：直到输入形状被知道，它们才能被创建。
- en: As such, the preceding Sequential model does not have any weights (listing 7.3)
    until you actually call it on some data, or call its build() method with an input
    shape (listing 7.4).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上述的 Sequential 模型在你实际上对其使用一些数据，或者使用其 build() 方法并指定输入形状之前，是没有任何权重的（清单 7.3）。
- en: Listing 7.3 Models that aren’t yet built have no weights
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.3 尚未构建的模型没有权重
- en: model$weights➊
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: model$weights➊
- en: 'Error in py_get_attr_impl(x, name, silent):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'Error in py_get_attr_impl(x, name, silent):'
- en: 'ValueError: Weights for model sequential_1 have not yet been created. Weights
    are created when the Model is first called on inputs or `build()` is called with
    an `input_shape`.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'ValueError: 模型 sequential_1 的权重尚未创建。当模型首次根据输入进行调用或调用 `build()` 并指定 `input_shape`
    时，才会创建权重。'
- en: ➊ **At that point, the model isn't built yet.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **此时，模型尚未构建。**
- en: Listing 7.4 Calling a model for the first time to build it
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.4 第一次调用模型以构建它
- en: model$build(input_shape = shape(NA, 3))➊
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: model$build(input_shape = shape(NA, 3))➊
- en: str(model$weights)➋
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: str(model$weights)➋
- en: List of 4
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 长度为 4 的列表
- en: $ :<tf.Variable 'dense_2/kernel:0' shape=(3, 64) dtype=float32, numpy=…>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<tf.Variable 'dense_2/kernel:0' shape=(3, 64) dtype=float32, numpy=…>
- en: $ :<tf.Variable 'dense_2/bias:0' shape=(64) dtype=float32, numpy=…>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<tf.Variable 'dense_2/bias:0' shape=(64) dtype=float32, numpy=…>
- en: $ :<tf.Variable 'dense_3/kernel:0' shape=(64, 10) dtype=float32, numpy=…>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<tf.Variable 'dense_3/kernel:0' shape=(64, 10) dtype=float32, numpy=…>
- en: $ :<tf.Variable 'dense_3/bias:0' shape=(10) dtype=float32, numpy=…>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<tf.Variable 'dense_3/bias:0' shape=(10) dtype=float32, numpy=…>
- en: ➊ **Build the model—now the model will expect samples of shape (3). The NA in
    the input shape signals that the batch size could be anything.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **构建模型——现在模型将期望形状为 (3) 的样本。输入形状中的 NA 表示批量大小可以是任何值。**
- en: ➋ **Now you can retrieve the model's weights.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **现在你可以检索模型的权重。**
- en: After the model is built, you can display its contents via the print() method,
    which comes in handy for debugging.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建完成后，你可以通过 print() 方法显示其内容，这对调试很有用。
- en: Listing 7.5 The print() method
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.5 print() 方法
- en: model
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0188-01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0188-01.jpg)'
- en: As you can see, this model happens to be named “sequential_1.” You can give
    names to everything in Keras—every model, every layer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这个模型恰好被命名为“sequential_1”。你可以为 Keras 中的每一个东西都起名字——每一个模型，每一个层。
- en: Listing 7.6 Naming models and layers with the name argument
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.6 使用 name 参数为模型和层命名
- en: model <- keras_model_sequential(name = "my_example_model")
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential(name = "my_example_model")
- en: model %>% layer_dense(64, activation = "relu", name = "my_first_layer")
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% layer_dense(64, activation = "relu", name = "my_first_layer")
- en: model %>% layer_dense(10, activation = "softmax", name = "my_last_layer")
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% layer_dense(10, activation = "softmax", name = "my_last_layer")
- en: model$build(shape(NA, 3))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: model$build(shape(NA, 3))
- en: model
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0188-02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0188-02.jpg)'
- en: 'When building a Sequential model incrementally, it’s useful to be able to print
    a summary of what the current model looks like after you add each layer. But you
    can’t print a summary until the model is built! There’s actually a way to have
    your Sequential model built on the fly: just declare the shape of the model’s
    inputs in advance. You can do this by passing input_shape to keras_model_sequential().'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在逐步构建 Sequential 模型时，可以在每添加一层后打印当前模型的摘要非常有用。但是直到模型构建完成之前，您无法打印摘要！事实上，有一种方法可以动态构建您的
    Sequential 模型：只需提前声明模型输入的形状即可。您可以通过将 input_shape 传递给 keras_model_sequential()
    来实现此目的。
- en: Listing 7.7 Specifying the input shape of your model in advance
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.7 预先指定您模型的输入形状
- en: model <
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`model <`'
- en: keras_model_sequential(input_shape = c(3)) %>%➊
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `keras_model_sequential(input_shape = c(3)) %>%`➋
- en: layer_dense(64, activation = "relu")
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_dense(64, activation = "relu")`'
- en: ➊ **Supply input_shape to declare the shape of the inputs. Note that the shape
    argument must be the shape of each sample, not the shape of one batch.**
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**➊ 提供 input_shape 以声明输入的形状。请注意，shape 参数必须是每个样本的形状，而不是一个批次的形状。**'
- en: 'Now you can use print() to follow how the output shape of your model changes
    as you add more layers:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用 print() 跟踪随着您添加更多层，模型输出形状如何变化：
- en: model
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`'
- en: '![Image](../images/f0189-01.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0189-01.jpg)'
- en: model %>% layer_dense(10, activation = "softmax")
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`model %>% layer_dense(10, activation = "softmax")`'
- en: model
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`'
- en: '![Image](../images/f0189-02.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0189-02.jpg)'
- en: This is a pretty common debugging workflow when dealing with layers that transform
    their inputs in complex ways, such as the convolutional layers you’ll learn about
    in chapter 8.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在处理以复杂方式转换其输入的层时的一种非常常见的调试工作流程，例如您将在第 8 章中了解的卷积层。
- en: 7.2.2 The Functional API
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 Functional API
- en: 'The Sequential model is easy to use, but its applicability is extremely limited:
    it can only express models with a single input and a single output, applying one
    layer after the other in a sequential fashion. In practice, it’s pretty common
    to encounter models with multiple inputs (say, an image and its metadata), multiple
    outputs (different things you want to predict about the data), or a nonlinear
    topology.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Sequential 模型易于使用，但其适用性极为有限：它只能表示具有单个输入和单个输出的模型，并以顺序方式一个接一个地应用层。在实践中，遇到具有多个输入（例如，图像及其元数据）、多个输出（关于数据您想要预测的不同事物）或非线性拓扑的模型是非常常见的。
- en: In such cases, you’d build your model using the Functional API. This is what
    most Keras models you’ll encounter in the wild use. It’s fun and powerful—it feels
    like playing with LEGO bricks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您将使用 Functional API 构建模型。这是您在野外遇到的大多数 Keras 模型使用的。这很有趣和强大——感觉就像玩乐高积木一样。
- en: A SIMPLE EXAMPLE
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个简单的例子
- en: 'Let’s start with something simple: the stack of two layers we used in the previous
    section. Its Functional API version looks like the following listing.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的东西开始：我们在上一节中使用的两层堆叠。其 Functional API 版本如下所示。
- en: Listing 7.8 A simple Functional model with two Dense layers
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.8 具有两个 Dense 层的简单 Functional 模型
- en: inputs <- layer_input(shape = c(3), name = "my_input")
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputs <- layer_input(shape = c(3), name = "my_input")`'
- en: features <- inputs %>% layer_dense(64, activation = "relu")
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`features <- inputs %>% layer_dense(64, activation = "relu")`'
- en: outputs <- features %>% layer_dense(10, activation = "softmax")
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`outputs <- features %>% layer_dense(10, activation = "softmax")`'
- en: model <- keras_model(inputs = inputs, outputs = outputs)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`model <- keras_model(inputs = inputs, outputs = outputs)`'
- en: 'Let’s go over this step by step. We started by declaring a layer_input() (note
    that you can also give names to these input objects, like everything else):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步来。我们首先声明了一个 layer_input()（请注意，您也可以给这些输入对象命名，就像其他所有东西一样）：
- en: inputs <- layer_input(shape = c(3), name = "my_input")
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputs <- layer_input(shape = c(3), name = "my_input")`'
- en: 'This inputs object holds information about the shape and dtype of the data
    that the model will process:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此 inputs 对象保存有关模型将处理的数据的形状和 dtype 的信息：
- en: inputs$shape➊
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputs$shape➊`'
- en: TensorShape([None, 3])
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([None, 3])
- en: inputs$dtype➋
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputs$dtype➋`'
- en: tf.float32
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.float32`'
- en: ➊ **The model will process batches where each sample has shape (3). The number
    of samples per batch is variable (indicated by the None batch size).**
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**➊ 该模型将处理每个样本形状为 (3) 的批次。每个批次中的样本数量是可变的（由 None 批次大小指示）。**'
- en: ➋ **These batches will have dtype float32.**
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**这些批次的 dtype 将为 float32。**'
- en: We call such an object a *symbolic tensor*. It doesn’t contain any actual data,
    but it encodes the specifications of the actual tensors of data that the model
    will see when you use it. It *stands for* future tensors of data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这样的对象为*符号张量*。它不包含任何实际数据，但它编码了模型在使用时将看到的实际数据张量的规格。它*代表*未来的数据张量。
- en: 'Next, we create a layer and compose with the input:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个层，并与输入组合：
- en: features <- inputs %>% layer_dense(64, activation = "relu")
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: features <- inputs %>% layer_dense(64, activation = "relu")
- en: 'In the Functional API, piping a symbolic tensor into a layer constructor invokes
    the layer’s call() method. In essence, this is what is happening:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Functional API 中，将符号张量传送到层构造函数会调用该层的 call() 方法。实质上，这就是发生的事情：
- en: layer_instance <- layer_dense(units = 64, activation = "relu") features <- layer_instance(inputs)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: layer_instance <- layer_dense(units = 64, activation = "relu") features <- layer_instance(inputs)
- en: 'This is different from the Sequential API, where composing a layer with a model
    (model %>% layer_dense()) means this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 Sequential API 不同，其中将层与模型组合（model %>% layer_dense()）意味着这个：
- en: layer_instance <- layer_dense(units = 64, activation = "relu") model$add(layer_instance)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: layer_instance <- layer_dense(units = 64, activation = "relu") model$add(layer_instance)
- en: 'All Keras layers can be called both on real tensors of data and on these symbolic
    tensors. In the latter case, they return a new symbolic tensor, with updated shape
    and dtype information:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Keras 层既可以在实际数据张量上调用，也可以在这些符号张量上调用。在后一种情况下，它们返回一个新的符号张量，具有更新的形状和 dtype 信息：
- en: features$shape
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: features$shape
- en: TensorShape([None, 64])
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([None, 64])
- en: 'Note that symbolic tensors work with almost all the same R generic methods
    as eager tensors. This means that you can also do something like this to get the
    shape as an R integer vector:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，符号张量几乎可以与所有相同的 R 通用方法一起使用，如 eager 张量。这意味着你也可以像这样获取形状作为 R 整数向量：
- en: dim(features)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: dim(features)
- en: '[1] NA 64'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] NA 64'
- en: 'After obtaining the final outputs, we instantiated the model by specifying
    its inputs and outputs in the keras_model() constructor:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得最终输出后，我们通过在 keras_model() 构造函数中指定其输入和输出来实例化模型：
- en: outputs <- layer_dense(features, 10, activation = "softmax")
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- layer_dense(features, 10, activation = "softmax")
- en: model <- keras_model(inputs = inputs, outputs = outputs)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs = inputs, outputs = outputs)
- en: 'Here’s the summary of our model:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们模型的概要：
- en: model
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0191-01.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0191-01.jpg)'
- en: MULTI-INPUT, MULTI-OUTPUT MODELS
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多输入，多输出模型
- en: Unlike this toy model, most deep learning models don’t look like lists—they
    look like graphs. They may, for instance, have multiple inputs or multiple outputs.
    It’s for this kind of model that the Functional API really shines.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个玩具模型不同，大多数深度学习模型看起来不像列表，而是像图。例如，它们可能具有多个输入或多个输出。正是对于这种模型，Functional API 才真正发挥作用。
- en: 'Let’s say you’re building a system to rank customer support tickets by priority
    and route them to the appropriate department. Your model has three inputs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在构建一个系统，根据优先级对客户支持票进行排名，并将其路由到适当的部门。你的模型有三个输入：
- en: The title of the ticket (text input)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票的标题（文本输入）
- en: The text body of the ticket (text input)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票的文本正文（文本输入）
- en: Any tags added by the user (categorical input, assumed here to be one-hot encoded)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户添加的任何标签（假定这里是 one-hot 编码的分类输入）
- en: 'We can encode the text inputs as arrays of ones and zeros of size vocabulary_size
    (see chapter 11 for detailed information about text encoding techniques). Your
    model also has two outputs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将文本输入编码为大小为 vocabulary_size 的 1 和 0 数组（有关文本编码技术的详细信息，请参见第 11 章）。你的模型还有两个输出：
- en: The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票的优先级分数，介于 0 和 1 之间的标量（sigmoid 输出）
- en: The department that should handle the ticket (a softmax over the set of departments
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应处理该票的部门（对部门集合进行 softmax）
- en: You can build this model in a few lines with the Functional API.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用几行代码使用 Functional API 构建此模型。
- en: Listing 7.9 A multi-input, multi-output Functional model
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.9 多输入，多输出的 Functional 模型
- en: vocabulary_size <- 10000
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: vocabulary_size <- 10000
- en: num_tags <- 100
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: num_tags <- 100
- en: num_departments <- 4
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: num_departments <- 4
- en: title <- layer_input(shape = c(vocabulary_size), name = "title")➊
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: title <- layer_input(shape = c(vocabulary_size), name = "title")➊
- en: text_body <- layer_input(shape = c(vocabulary_size), name = "text_body")➊
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: text_body <- layer_input(shape = c(vocabulary_size), name = "text_body")➊
- en: tags <- layer_input(shape = c(num_tags), name = "tags")➊
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: tags <- layer_input(shape = c(num_tags), name = "tags")➊
- en: features <-
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: features <-
- en: layer_concatenate(list(title, text_body, tags)) %>%➋
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: layer_concatenate(list(title, text_body, tags)) %>%➋
- en: layer_dense(64, activation = "relu")➌
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu")➌
- en: priority <- features %>%➍
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: priority <- features %>%➍
- en: layer_dense(1, activation = "sigmoid", name = "priority")
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid", name = "priority")
- en: department <- features %>%➍
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: department <- features %>%➍
- en: layer_dense(num_departments, activation = "softmax", name = "department")
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(num_departments, activation = "softmax", name = "department")
- en: model <- keras_model(➎
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(➎
- en: inputs = list(title, text_body, tags),
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: inputs = list(title, text_body, tags),
- en: outputs = list(priority, department)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: outputs = list(priority, department)
- en: )
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Define the model inputs.**
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **定义模型的输入。**
- en: ➋ **Combine the input features into a single tensor by concatenating them.**
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将输入特征合并成单个张量，通过连接它们。**
- en: ➌ **Apply an intermediate layer to recombine input features into richer representations.**
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **应用中间层将输入特征重新组合成更丰富的表示。**
- en: ➍ **Define the model outputs.**
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **定义模型的输出。**
- en: ➎ **Create the model by specifying its inputs and outputs.**
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **通过指定其输入和输出来创建模型。**
- en: The Functional API is a simple, LEGO-like, yet very flexible way to define arbitrary
    graphs of layers like these.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式API是一种简单、类似于LEGO但非常灵活的方式，可以定义这样的任意层级图。
- en: TRAINING A MULTI-INPUT, MULTI-OUTPUT MODEL
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练多输入、多输出模型
- en: You can train your model in much the same way as you would train a Sequential
    model, by calling fit() with lists of input and output data. These lists of data
    should be in the same order as the inputs you passed to the keras_model() constructor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用fit()并传递输入和输出数据的列表来训练模型，这与训练Sequential模型的方式非常相似。这些数据列表应与您传递给keras_model()构造函数的输入的顺序相同。
- en: Listing 7.10 Training a model by providing lists of input and target arrays
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 7.10 通过提供输入和目标数组列表来训练模型
- en: num_samples <- 1280
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: num_samples <- 1280
- en: random_uniform_array <- function(dim)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: random_uniform_array <- function(dim)
- en: array(runif(prod(dim)), dim)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: array(runif(prod(dim)), dim)
- en: random_vectorized_array <- function(dim)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: random_vectorized_array <- function(dim)
- en: array(sample(0:1, prod(dim), replace = TRUE), dim)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: array(sample(0:1, prod(dim), replace = TRUE), dim)
- en: title_data <- random_vectorized_array(c(num_samples, vocabulary_size))➊
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: title_data <- random_vectorized_array(c(num_samples, vocabulary_size))➊
- en: text_body_data <- random_vectorized_array(c(num_samples, vocabulary_size))➊
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: text_body_data <- random_vectorized_array(c(num_samples, vocabulary_size))➊
- en: tags_data <- random_vectorized_array(c(num_samples, num_tags))➊
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: tags_data <- random_vectorized_array(c(num_samples, num_tags))➊
- en: priority_data <- random_vectorized_array(c(num_samples, 1))➋
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: priority_data <- random_vectorized_array(c(num_samples, 1))➋
- en: department_data <- random_vectorized_array(c(num_samples, num_departments))➋
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: department_data <- random_vectorized_array(c(num_samples, num_departments))➋
- en: model %>% compile(
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(
- en: optimizer = "rmsprop",
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: loss = c("mean_squared_error", "categorical_crossentropy"),
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: loss = c("mean_squared_error", "categorical_crossentropy"),
- en: metrics = c("mean_absolute_error", "accuracy")
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = c("mean_absolute_error", "accuracy")
- en: )
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>% fit(
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(
- en: x = list(title_data, text_body_data, tags_data),
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: x = list(title_data, text_body_data, tags_data),
- en: y = list(priority_data, department_data),
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: y = list(priority_data, department_data),
- en: epochs = 1
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 1
- en: )
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>% evaluate(x = list(title_data, text_body_data, tags_data),
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% evaluate(x = list(title_data, text_body_data, tags_data),
- en: y = list(priority_data, department_data))
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: y = list(priority_data, department_data))
- en: '![Image](../images/f0193-01.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0193-01.jpg)'
- en: c(priority_preds, department_preds) %<-% {➌
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: c(priority_preds, department_preds) %<-% {➌
- en: model %>% predict(list(title_data, text_body_data, tags_data))
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% predict(list(title_data, text_body_data, tags_data))
- en: '}'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Dummy input data**
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **虚拟的输入数据**
- en: ➋ **Dummy target data**
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **虚拟的目标数据**
- en: ➌ **To use %<-% and %>% in the same expression, you need to wrap the pipe sequence
    with {} or () to override the default operator precedence.**
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **要在同一表达式中使用%<-%和%>%，您需要用{}或（）将管道序列包装起来，以覆盖默认的运算符优先级。**
- en: If you don’t want to rely on input order (for instance, because you have many
    inputs or outputs), you can also leverage the names you gave to the input_shape
    and the output layers, and pass data via a named list.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想依赖输入顺序（例如，因为有多个输入或输出），您也可以利用您给输入形状和输出层指定的名称，并通过命名列表传递数据。
- en: IMPORTANT When using named lists, the order of the list is not guaranteed to
    be preserved. Be sure to keep track of items *either* by position *or* by name,
    but not a combination of both.
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 重要提示：使用命名列表时，列表的顺序不能保证保留下来。请务必通过位置或名称跟踪项目，但不能混合使用两者。
- en: Listing 7.11 Training a model by providing named lists of input and target arrays
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 7.11 通过提供命名输入和目标数组列表来训练模型
- en: model %>%
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = c(priority = "mean_squared_error",
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: loss = c(priority = "mean_squared_error",
- en: department = "categorical_crossentropy"),
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: department = "categorical_crossentropy"),
- en: metrics = c(priority = "mean_absolute_error",
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = c(priority = "mean_absolute_error",
- en: department = "accuracy"))
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: department = "accuracy"))
- en: model %>%
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: fit(list(title = title_data,
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: fit(list(title = title_data,
- en: text_body = text_body_data,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: text_body = text_body_data,
- en: tags = tags_data),
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 标签 = tags_data),
- en: list(priority = priority_data,
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: list(priority = priority_data,
- en: department = department_data), epochs = 1)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: department = department_data), epochs = 1)
- en: model %>%
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: evaluate(list(title = title_data,
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 评估(list(title = title_data,
- en: text_body = text_body_data,
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: text_body = text_body_data,
- en: tags = tags_data),
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 标签 = tags_data),
- en: list(priority = priority_data,
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: list(priority = priority_data,
- en: department = department_data))
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: department = department_data))
- en: '![Image](../images/f0194-02.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0194-02.jpg)'
- en: c(priority_preds, department_preds) %<-%
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: c(priority_preds, department_preds) %<-%
- en: predict(model, list(title = title_data,
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 预测(model, list(title = title_data,
- en: text_body = text_body_data,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: text_body = text_body_data,
- en: tags = tags_data))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 标签 = tags_data))
- en: 'THE POWER OF THE FUNCTIONAL API: ACCESS TO LAYER CONNECTIVITY'
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数 API 的威力：访问层连接性
- en: 'A Functional model is an explicit graph data structure. This makes it possible
    to inspect how layers are connected and reuse previous graph nodes (which are
    layer outputs) as part of new models. It also nicely fits the “mental model” that
    most researchers use when thinking about a deep neural network: a graph of layers.
    This enables two important use cases: model visualization and feature extraction.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 功能模型是一个明确的图形数据结构。这使得可以检查层如何连接，并重复使用以前的图形节点（它们是层输出）作为新模型的一部分。它还很好地适应了大多数研究人员在思考深度神经网络时使用的“心理模型”：层的图形。这使两个重要用例成为可能：模型可视化和特征提取。
- en: 'Let’s visualize the connectivity of the model we just defined (the *topology*
    of the model). You can plot a Functional model as a graph with the plot() method
    (see [figure 7.2](#fig7-2)):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化刚刚定义的模型的连接性（模型的 *拓扑*）。您可以使用 plot() 方法将功能模型绘制为图形（参见[图 7.2](#fig7-2)）：
- en: plot(model)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: plot(model)
- en: '![Image](../images/f0194-01.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0194-01.jpg)'
- en: '**Figure 7.2 Plot generated by plot(model) on our ticket classifier model**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.2 由我们的票务分类器模型 plot(model) 生成的图表**'
- en: 'You can add to this plot the input and output shapes of each layer in the model,
    which can be helpful during debugging (see [figure 7.3](#fig7-3)):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在模型的每一层中添加此图表的输入和输出形状，这在调试期间可能会有所帮助（参见[图 7.3](#fig7-3)）：
- en: plot(model, show_shapes = TRUE)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: plot(model, show_shapes = TRUE)
- en: '![Image](../images/f0195-01.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0195-01.jpg)'
- en: '**Figure 7.3 Model plot with shape information added**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.3 添加形状信息的模型图**'
- en: 'The “None” in the tensor shapes represents the batch size: this model allows
    batches of any size.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 张量形状中的“None”表示批处理大小：该模型允许任意大小的批处理。
- en: Access to layer connectivity also means that you can inspect and reuse individual
    nodes (layer calls) in the graph. The model$layers model property provides the
    list of layers that make up the model, and for each layer you can query layer$input
    and layer$output.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 访问层连接性也意味着您可以检查和重复使用图中的单个节点（层调用）。模型$layers 模型属性提供了组成模型的层的列表，对于每个层，您可以查询 layer$input
    和 layer$output。
- en: Listing 7.12 Retrieving the inputs or outputs of a layer in a Functional model
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 在功能模型中检索层的输入或输出
- en: str(model$layers)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: str(model$layers)
- en: List of 7
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 7 的列表
- en: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da63a0>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da63a0>
- en: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da6430>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da6430>
- en: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da68e0>
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<keras.engine.input_layer.InputLayer object at 0x7fc962da68e0>
- en: $ :<keras.layers.merge.Concatenate object at 0x7fc962d2e130>
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<keras.layers.merge.Concatenate object at 0x7fc962d2e130>
- en: $ :<keras.layers.core.dense.Dense object at 0x7fc962da6c40>
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<keras.layers.core.dense.Dense object at 0x7fc962da6c40>
- en: $ :<keras.layers.core.dense.Dense object at 0x7fc962da6340>
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<keras.layers.core.dense.Dense object at 0x7fc962da6340>
- en: $ :<keras.layers.core.dense.Dense object at 0x7fc962d331f0>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: $ :<keras.layers.core.dense.Dense object at 0x7fc962d331f0>
- en: str(model$layers[[4]]$input)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: str(model$layers[[4]]$input)
- en: List of 3
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的列表
- en: '$ :<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '$ :<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer'
- en: '![Image](../images/common01.jpg) ''title'')>'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../images/common01.jpg) ''title'')>'
- en: '$ :<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '$ :<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer'
- en: '![Image](../images/common01.jpg) ''text_body'')>'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../images/common01.jpg) ''text_body'')>'
- en: '$ :<KerasTensor: shape=(None, 100) dtype=float32 (created by layer ''tags'')>'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '$ :<KerasTensor: shape=(None, 100) dtype=float32 (created by layer ''tags'')>'
- en: str(model$layers[[4]]$output)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: str(model$layers[[4]]$output)
- en: '<KerasTensor: shape=(None, 20100) dtype=float32 (created by layer'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '<KerasTensor: shape=(None, 20100) dtype=float32 (created by layer'
- en: '![Image](../images/common01.jpg) ''concatenate'')>'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../images/common01.jpg) ''concatenate'')>'
- en: This enables you to do *feature extraction*, creating models that reuse intermediate
    features from another model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您能够进行 *特征提取*，创建重用另一个模型的中间特征的模型。
- en: 'Let’s say you want to add another output to the previous model—you want to
    estimate how long a given issue ticket will take to resolve, a kind of difficulty
    rating. You could do this via a classification layer over three categories: “quick,”
    “medium,” and “difficult.” You don’t need to recreate and retrain a model from
    scratch. You can start from the intermediate features of your previous model,
    because you have access to them, like this.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要在前一个模型中添加另一个输出——你想要估计给定问题票的解决时间，一种困难评级。你可以通过三个类别的分类层来实现这一点：“快速”，“中等”和“困难”。你不需要从头开始重新创建和重新训练一个模型。你可以从你以前模型的中间特征开始，因为你可以像这样访问它们。
- en: Listing 7.13 Creating a new model by reusing intermediate layer outputs
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 7.13 通过重用中间层输出创建一个新模型
- en: features <- model$layers[[5]]$output➊
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: features <- model$layers[[5]]$output➊
- en: difficulty <- features %>%
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: difficulty <- features %>%
- en: layer_dense(3, activation = "softmax", name = "difficulty")
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(3, activation = "softmax", name = "difficulty")
- en: new_model <- keras_model(
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: new_model <- keras_model(
- en: inputs = list(title, text_body, tags),
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: inputs = list(title, text_body, tags),
- en: outputs = list(priority, department, difficulty)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: outputs = list(priority, department, difficulty)
- en: )
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **layer[[5]] is our intermediate dense layer. You can also retrieve a layer
    by name with get_layer().**
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **layer[[5]] 是我们的中间密集层。您也可以使用 get_layer() 按名称检索层。**
- en: 'Let’s plot our new model (see [figure 7.4](#fig7-4)):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制我们的新模型（见[图 7.4](#fig7-4)）：
- en: plot(new_model, show_shapes = TRUE)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: plot(new_model, show_shapes = TRUE)
- en: '![Image](../images/f0196-01.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0196-01.jpg)'
- en: '**Figure 7.4 Plot of our new model: Updated ticket classifier**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.4 我们的新模型绘图：更新的工单分类器**'
- en: 7.2.3 Subclassing the Model class
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 子类化 Model 类
- en: 'The last model-building pattern you should know about is the most advanced
    one: Model subclassing. You learned in chapter 3 how to use new_layer_class()
    to subclass the Layer class and create custom layers. Using new_model_class()
    to subclass Model class is pretty similar:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该知道的最后一个模型构建模式是最高级的模型子类化。你在第3章学习过如何使用 new_layer_class() 子类化 Layer 类并创建自定义层。使用
    new_model_class() 子类化 Model 类非常相似：
- en: In the initialize() method, define the layers the model will use.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 initialize() 方法中，定义模型将使用的层。
- en: In the call() method, define the forward pass of the model, reusing the layers
    previously created.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 call() 方法中，定义模型的前向传播，重用先前创建的层。
- en: Instantiate your subclass, and call it on data to create its weights
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化你的子类，并在数据上调用它以创建它的权重
- en: REWRITING OUR PREVIOUS EXAMPLE AS A SUBCLASSED MODEL
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重写我们之前的示例作为子类模型
- en: 'Let’s take a look at a simple example: we will reimplement the customer support
    ticket management model using new_model_class() to define a Model subclass.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子：我们将使用 new_model_class() 重新实现客户支持票务管理模型来定义一个 Model 子类。
- en: Listing 7.14 A simple subclassed model
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 7.14 一个简单的子类模型
- en: CustomerTicketModel <- new_model_class(
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: CustomerTicketModel <- new_model_class(
- en: classname = "CustomerTicketModel",
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "CustomerTicketModel",
- en: initialize = function(num_departments) {
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(num_departments) {
- en: super$initialize()➊
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize()➊
- en: self$concat_layer <- layer_concatenate()
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: self$concat_layer <- layer_concatenate()
- en: self$mixing_layer <-➋
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: self$mixing_layer <-➋
- en: layer_dense(units = 64, activation = "relu")
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 64, activation = "relu")
- en: self$priority_scorer <-
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: self$priority_scorer <-
- en: layer_dense(units = 1, activation = "sigmoid")
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 1, activation = "sigmoid")
- en: self$department_classifier <-
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: self$department_classifier <-
- en: layer_dense(units = num_departments, activation = "softmax")
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = num_departments, activation = "softmax")
- en: '},'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: call = function(inputs) {➌
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs) {➌
- en: title <- inputs$title➍
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: title <- inputs$title➍
- en: text_body <- inputs$text_body
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: text_body <- inputs$text_body
- en: tags <- inputs$tags
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: tags <- inputs$tags
- en: features <- list(title, text_body, tags) %>%
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: features <- list(title, text_body, tags) %>%
- en: self$concat_layer() %>%
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: self$concat_layer() %>%
- en: self$mixing_layer()
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: self$mixing_layer()
- en: priority <- self$priority_scorer(features)
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: priority <- self$priority_scorer(features)
- en: department <- self$department_classifier(features)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: department <- self$department_classifier(features)
- en: list(priority, department)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: list(priority, department)
- en: '}'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Don't forget to call the super$initialize()!**
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **别忘了调用 super$initialize()！**
- en: ➋ **Define the sublayers in the constructor. Note that we're specifying the
    units argument name here, so that we get back a layer instance.**
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在构造函数中定义子层。请注意，我们在这里指定了 units 参数名称，以便我们得到一个层实例。**
- en: ➌ **Define the forward pass in the call() method.**
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **在 call() 方法中定义前向传播。**
- en: ➍ **For inputs, we'll provide the model with a named list.**
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **对于输入，我们将提供一个带有名称的列表给模型。**
- en: We implemented a bare-bones version of Model in chapter 3\. The key thing to
    be aware of is that we’re defining a custom class, our model, that subclasses
    Model. Model provides many methods and capabilities that you can opt-in to, as
    you’ll see in the coming sections.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第3章中实现了Model的最简版本。要注意的主要事项是，我们正在定义一个自定义类，即我们的模型，它是Model子类。正如您将在接下来的章节中看到的那样，Model提供了许多方法和功能，您可以选择加入其中。
- en: 'Once you’ve defined the model, you can instantiate it. Note that it will create
    its weights only the first time you call it on some data, much like Layer subclasses:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型之后，您可以实例化它。请注意，它只会在首次在某些数据上调用它时创建其权重，就像层子类一样：
- en: model <- CustomerTicketModel(num_departments = 4)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: model <- CustomerTicketModel(num_departments = 4)
- en: c(priority, department) %<-% model(list(title = title_data,
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: c(priority, department) %<-% model(list(title = title_data,
- en: text_body = text_body_data,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: text_body = text_body_data,
- en: tags = tags_data))
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: tags = tags_data))
- en: 'Models are, fundamentally, a type of Layer. That means you can easily add the
    ability for a model to compose nicely with %>% by using create_layer_wrapper(),
    like this:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 模型本质上是一种层（Layer）。这意味着，使用create_layer_wrapper()就可以轻松地使模型具有与%>%很好地组合的能力，就像这样：
- en: inputs <- list(title = title_data,
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- list(title = title_data,
- en: text_body = text_body_data,
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: text_body = text_body_data,
- en: tags = tags_data)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: tags = tags_data)
- en: layer_customer_ticket_model <- create_layer_wrapper(CustomerTicketModel)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: layer_customer_ticket_model <- create_layer_wrapper(CustomerTicketModel)
- en: outputs <- inputs %>%
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_customer_ticket_model(num_departments = 4)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: layer_customer_ticket_model(num_departments = 4)
- en: c(priority, department) %<-% outputs
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: c(priority, department) %<-% outputs
- en: 'So far, everything looks very similar to Layer subclassing, a workflow you
    encountered in chapter 3\. What, then, is the difference between a Layer subclass
    and a Model sub-class? It’s simple: a “layer” is a building block you use to create
    models, and a “model” is the top-level object that you will actually train, export
    for inference, and so on. In short, a Model has fit(), evaluate(), and predict()
    methods. Layers don’t. Other than that, the two classes are virtually identical.
    (Another difference is that you can *save* a model to a file on disk, which we
    will cover in a few sections.) You can compile and train a Model subclass just
    like a Sequential or Functional model:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有内容看起来与Layer子类化非常相似，这是您在第3章中遇到的工作流程。那么，“层”的子类和“模型”的子类之间有什么区别呢？答案很简单：一个“层”是用来创建模型的构建块，而一个“模型”是您将实际培训、导出推理等的最高级对象。换句话说，模型具有fit()、evaluate()和predict()方法，而层则没有。除此之外，这两个类几乎完全相同。（另一个区别是您可以将模型保存在磁盘上的文件中，我们将在几个章节中介绍这个功能。）您可以编译和训练Model子类，就像Sequential模型或Functional模型一样：
- en: model %>%
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = c("mean_squared_error",
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: loss = c("mean_squared_error",
- en: '"categorical_crossentropy"),➊'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '"categorical_crossentropy"),➊'
- en: metrics = c("mean_absolute_error", "accuracy"))
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = c("mean_absolute_error", "accuracy"))
- en: x <- list(title = title_data,➋
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: x <- list(title = title_data,➋
- en: text_body = text_body_data,
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: text_body = text_body_data,
- en: tags = tags_data)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: tags = tags_data)
- en: y <- list(priority_data, department_data)➌
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: y <- list(priority_data, department_data)➌
- en: model %>% fit(x, y, epochs = 1)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(x, y, epochs = 1)
- en: model %>% evaluate(x, y)
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% evaluate(x, y)
- en: '![Image](../images/f0198-01.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0198-01.jpg)'
- en: c(priority_preds, department_preds) %<-% {
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: c(priority_preds, department_preds) %<-% {
- en: model %>% predict(x)
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% predict(x)
- en: '}'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **The structure of what you pass as the loss and metrics arguments must match
    exactly what gets returned by call()—here, a lists of two elements.**
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **您作为损失和指标参数传递的内容的结构必须严格匹配call()方法返回的内容，即两个元素的列表。**
- en: ➋ **The structure of the input data must match exactly what is expected by the
    call() method— here, a named list with entries title, text_body, and tags. (Remember,
    list order is ignored when matching by names!)**
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **输入数据的结构必须严格匹配call()方法所期望的内容，即一个命名列表，其中包括标题（title）、正文（text_body）和标签（tags）。当按名称匹配时，列表顺序将被忽略！**
- en: ➌ **The structure of the target data must match exactly what is returned by
    the call() method—here, a list of two elements.**
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **目标数据的结构必须严格匹配call()方法返回的内容，即两个元素的列表。**
- en: The Model subclassing workflow is the most flexible way to build a model. It
    enables you to build models that cannot be expressed as directed acyclic graphs
    of layers—imagine, for instance, a model where the call() method uses layers inside
    a for loop, or even calls them recursively. Anything is possible—you’re in charge.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Model子类化流程是构建模型最灵活的方法。它使您能够构建无法表示为层的有向无环图的模型，例如，模型的call()方法在for循环内使用层，甚至会递归调用它们。任何事情都可能发生——您有权决定。
- en: 'BEWARE: WHAT SUBCLASSED MODELS DON’T SUPPORT'
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 警告：子类化模型不支持的内容
- en: 'This freedom comes at a cost: with subclassed models, you are responsible for
    more of the model logic, which means your potential error surface is much larger.
    As a result, you will have more debugging work to do. You are developing a new
    class object, not just snapping together LEGO bricks.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自由是有代价的：通过子类模型，您需要负责更多的模型逻辑，这意味着您的潜在错误范围要大得多。因此，您将需要更多的调试工作。您正在开发一个新的类对象，而不仅仅是将乐高积木拼凑在一起。
- en: Functional and subclassed models are also substantially different in nature.
    A Functional model is an explicit data structure—a graph of layers, which you
    can view, inspect, and modify. A subclassed model is a collection of R code—a
    class with a call() method that is an R function. This is the source of the subclassing
    workflow’s flexibility—you can code up whatever functionality you like—but it
    introduces new limitations.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Functional 和子类模型在性质上也有很大不同。Functional 模型是一个显式数据结构——层的图形，您可以查看、检查和修改。而子类模型是一组
    R 代码——一个具有 call() 方法的类，该方法是一个 R 函数。这就是子类化工作流程的灵活性来源——您可以编写任何功能性代码——但它也引入了新的限制。
- en: For instance, because the way layers are connected to each other is hidden inside
    the body of the call() method, you cannot access that information. Calling summary()
    will not display layer connectivity, and you cannot plot the model topology via
    plot(). Likewise, if you have a subclassed model, you cannot access the nodes
    of the graph of layers to do feature extraction because there is simply no graph.
    Once the model is instantiated, its forward pass becomes a complete black box.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，因为层之间连接的方式被隐藏在 call() 方法的主体内部，您无法访问该信息。调用 summary() 将不显示层连接性，并且您无法通过 plot()
    绘制模型拓扑。同样，如果您有一个子类模型，您无法访问层图的节点来进行特征提取，因为简单地没有图。一旦模型被实例化，其正向传递就变成了一个完全的黑盒。
- en: 7.2.4 Mixing and matching different components
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 混合和匹配不同的组件
- en: Crucially, choosing one of these patterns—the Sequential model, the Functional
    API, or Model subclassing—does not lock you out of the others. All models in the
    Keras API can smoothly interoperate with each other, whether they’re Sequential
    models, Functional models, or subclassed models written from scratch. They’re
    all part of the same spectrum of workflows. For instance, you can use a subclassed
    layer or model in a Functional model.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，选择这些模式之一——Sequential 模型、Functional API 或 Model 子类化——不会将您排除在其他模式之外。Keras
    API 中的所有模型都可以与彼此平滑地互操作，无论它们是 Sequential 模型、Functional 模型还是从头编写的子类模型。它们都是同一工作流谱系的一部分。例如，您可以在
    Functional 模型中使用一个子类化的层或模型。
- en: Listing 7.15 Creating a Functional model that includes a subclassed model
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.15 创建一个包含子类化模型的 Functional 模型
- en: ClassifierModel <- new_model_class(
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ClassifierModel <- new_model_class(
- en: classname = "Classifier",
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "Classifier",
- en: initialize = function(num_classes = 2) {
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(num_classes = 2) {
- en: super$initialize()
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize()
- en: if (num_classes == 2) {
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: if (num_classes == 2) {
- en: num_units <- 1
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: num_units <- 1
- en: activation <- "sigmoid"
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: activation <- "sigmoid"
- en: '} else {'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '} else {'
- en: num_units <- num_classes
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: num_units <- num_classes
- en: activation <- "softmax"
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: activation <- "softmax"
- en: '}'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self$dense <- layer_dense(units = num_units, activation = activation)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense <- layer_dense(units = num_units, activation = activation)
- en: '},'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: call = function(inputs)
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs)
- en: self$dense(inputs)
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense(inputs)
- en: )
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: inputs <- layer_input(shape = c(3))
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(3))
- en: classifier <- ClassifierModel(num_classes = 10)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: classifier <- ClassifierModel(num_classes = 10)
- en: outputs <- inputs %>%
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: classifier()
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: classifier()
- en: model <- keras_model(inputs = inputs, outputs = outputs)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs = inputs, outputs = outputs)
- en: Inversely, you can use a Functional model as part of a subclassed layer or model.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 相反地，您可以在子类化层或模型中使用 Functional 模型的一部分。
- en: Listing 7.16 Creating a subclassed model that includes a Functional model
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.16 创建一个包含 Functional 模型的子类模型
- en: inputs <- layer_input(shape = c(64))
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(64))
- en: outputs <- inputs %>% layer_dense(1, activation = "sigmoid")
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>% layer_dense(1, activation = "sigmoid")
- en: binary_classifier <- keras_model(inputs = inputs, outputs = outputs)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: binary_classifier <- keras_model(inputs = inputs, outputs = outputs)
- en: MyModel <- new_model_class(
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: MyModel <- new_model_class(
- en: classname = "MyModel",
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "MyModel",
- en: initialize = function(num_classes = 2) {
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(num_classes = 2) {
- en: super$initialize()
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize()
- en: self$dense <- layer_dense(units = 64, activation = "relu")
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense <- layer_dense(units = 64, activation = "relu")
- en: self$classifier <- binary_classifier
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: self$classifier <- binary_classifier
- en: '},'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: call = function(inputs) {
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs) {
- en: inputs %>%
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: inputs %>%
- en: self$dense() %>%
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense() %>%
- en: self$classifier()
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: self$classifier()
- en: '}'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- MyModel()
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: model <- MyModel()
- en: '7.2.5 Remember: Use the right tool for the job'
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 记住：用对的工具做对的事情
- en: You’ve learned about the spectrum of workflows for building Keras models, from
    the simplest workflow, the Sequential model, to the most advanced one, model subclassing.
    When should you use one over the other? Each one has its pros and cons—pick the
    one most suitable for the job at hand.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了一系列构建 Keras 模型的工作流程，从最简单的序列模型到最高级的模型子类化。什么时候应该使用其中一种？每种都有其优缺点，选择最适合当前工作的一种。
- en: In general, the Functional API provides you with a pretty good tradeoff between
    ease of use and flexibility. It also gives you direct access to layer connectivity,
    which is very powerful for use cases such as model plotting or feature extraction.
    If you *can* use the Functional API—that is, if your model can be expressed as
    a directed acyclic graph of layers—I recommend using it over model subclassing.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，功能 API 为您提供了易于使用和灵活性的很好的平衡。它还直接给您访问层连接性的能力，对于模型绘制或特征提取等用例非常有用。如果您*可以*使用功能
    API，即，如果您的模型可以表示为层的有向无环图，那么我建议您使用功能 API 而不是模型子类化。
- en: 'Going forward, all examples in this book will use the Functional API, simply
    because all the models we will work with are expressible as graphs of layers.
    We will, however, make frequent use of subclassed layers (using new_layer_class()).
    In general, using Functional models that include subclassed layers provides the
    best of both worlds: high development flexibility while retaining the advantages
    of the Functional API.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，本书中的所有示例都将使用函数式 API，仅因为我们将使用的所有模型都可以表示为层图。我们还将经常使用子类化层（使用 new_layer_class()）。总的来说，在包括子类化层的功能模型中使用，既具有高度的开发灵活性，同时又保留了功能
    API 的优势，是最佳选择。
- en: 7.3 Using built-in training and evaluation loops
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 使用内置的训练和评估循环
- en: The principle of progressive disclosure of complexity—access to a spectrum of
    work-flows that go from dead easy to arbitrarily flexible, one step at a time—also
    applies to model training. Keras provides you with different workflows for training
    models. They can be as simple as calling fit() on your data or as advanced as
    writing a new training algorithm from scratch.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步透明化复杂性的原则——从极简单到任意灵活的不同工作流的访问，一步一步——也适用于模型训练。Keras 为您提供了不同的模型训练工作流。它们可以简单到在数据上调用
    fit()，也可以高级到从头编写新的训练算法。
- en: You are already familiar with the compile(), fit(), evaluate(), predict() work-flow.
    As a reminder, take a look at the following listing.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经熟悉了 compile()、fit()、evaluate()、predict() 的工作流程。作为提醒，请看下面的清单。
- en: 'Listing 7.17 The standard workflow: compile(), fit(), evaluate(), predict()'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 7.17 标准工作流程：compile()、fit()、evaluate()、predict()
- en: get_mnist_model <- function() {➊
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: get_mnist_model <- function() {➊
- en: inputs <- layer_input(shape = c(28 * 28))
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(28 * 28))
- en: outputs <- inputs %>%
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_dense(512, activation = "relu") %>%
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(512, activation = "relu") %>%
- en: layer_dropout(0.5) %>%
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: keras_model(inputs, outputs)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: keras_model(inputs, outputs)
- en: '}'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: c(c(images, labels), c(test_images, test_labels)) %<-%➋
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: c(c(images, labels), c(test_images, test_labels)) %<-%➋
- en: dataset_mnist()
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_mnist()
- en: images <- array_reshape(images, c(-1, 28 * 28)) / 255
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: images <- array_reshape(images, c(-1, 28 * 28)) / 255
- en: test_images <- array_reshape(test_images, c(-1, 28 * 28)) / 255
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: test_images <- array_reshape(test_images, c(-1, 28 * 28)) / 255
- en: val_idx <- seq(10000)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: val_idx <- seq(10000)
- en: val_images <- images[val_idx, ]
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: val_images <- images[val_idx, ]
- en: val_labels <- labels[val_idx]
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: val_labels <- labels[val_idx]
- en: train_images <- images[-val_idx, ]
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- images[-val_idx, ]
- en: train_labels <- labels[-val_idx]
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: train_labels <- labels[-val_idx]
- en: model <- get_mnist_model()
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_mnist_model()
- en: model %>% compile(optimizer = "rmsprop",➌
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",➌
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model %>% fit(train_images, train_labels,➍
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(train_images, train_labels,➍
- en: epochs = 3,
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 3,
- en: validation_data = list(val_images, val_labels))
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(val_images, val_labels))
- en: test_metrics <- model %>% evaluate(test_images, test_labels)➎
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: test_metrics <- model %>% evaluate(test_images, test_labels)➎
- en: predictions <- model %>% predict(test_images)➏
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model %>% predict(test_images)➏
- en: ➊ **Create a model (we factor this into a separate function so as to reuse it
    later).**
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **创建模型（我们将其分解成一个单独的函数，以便以后重用）。**
- en: ➋ **Load your data, reserving some for validation.**
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **预留一部分数据作为验证，加载你的数据。**
- en: ➌ **Compile the model by specifying its optimizer, the loss function to minimize,
    and the metrics to monitor.**
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **通过指定优化器、最小化的损失函数和要监控的指标来编译模型。**
- en: ➍ **Use fit() to train the model, optionally providing validation data to monitor
    performance on unseen data.**
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **使用fit()训练模型，可以选择提供验证数据以监控在看不见的数据上的性能。**
- en: ➎ **Use evaluate() to compute the loss and metrics on new data.**
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **使用evaluate()计算新数据的损失和指标。**
- en: ➏ **Use predict() to compute classification probabilities on new data.**
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **使用predict()计算新数据的分类概率。**
- en: 'There are a couple of ways you can customize this simple workflow:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以自定义这个简单的工作流程：
- en: Provide your own custom metrics.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供你自己的自定义指标。
- en: Pass *callbacks* to the fit() method to schedule actions to be taken at specific
    points during training.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在fit()方法中传递*callbacks*以安排在训练过程中特定时间点执行的操作。
- en: Let’s take a look at these.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这些。
- en: 7.3.1 Writing your own metrics
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 编写自己的指标
- en: Metrics are key to measuring the performance of your model, in particular, to
    measuring the difference between its performance on the training data and its
    performance on the test data. Commonly used metrics for classification and regression
    are already part of the keras package, all starting with the prefix metric_, and
    most of the time that’s what you will use. But if you’re doing anything out of
    the ordinary, you will need to be able to write your own metrics. It’s simple!
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 指标是衡量模型性能的关键，尤其是衡量其在训练数据和测试数据上性能差异的指标。分类和回归常用的指标已经是keras包的一部分，都以metric_前缀开头，大多数情况下你会使用它们。但是，如果你要做一些超出寻常的事情，就需要编写自己的指标。这很简单！
- en: A Keras metric is a subclass of the Keras Metric class. Like layers, a metric
    has an internal state stored in TensorFlow variables. Unlike layers, these variables
    aren’t updated via backpropagation, so you have to write the state-update logic
    yourself, which happens in the update_state() method. For example, here’s a simple
    custom metric that measures the root mean squared error (RMSE).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Keras指标是Keras Metric类的子类。和层一样，指标有一个存储在TensorFlow变量中的内部状态。与层不同的是，这些变量不是通过反向传播更新的，所以必须自己编写状态更新逻辑，这发生在update_state()方法中。例如，这是一个简单的自定义指标，用于测量均方根误差（RMSE）。
- en: Listing 7.18 Implementing a custom metric by subclassing the Metric class
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 使用Metric类的子类实现自定义指标
- en: library(tensorflow)➊
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)➊
- en: metric_root_mean_squared_error <- new_metric_class( classname➋
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: metric_root_mean_squared_error <- new_metric_class( classname➋
- en: = "RootMeanSquaredError",
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: = "RootMeanSquaredError",
- en: initialize = function(name = "rmse", …) {➌
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(name = "rmse", …) {➌
- en: super$initialize(name = name, …)
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize(name = name, …)
- en: self$mse_sum <- self$add_weight(name = "mse_sum",
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: self$mse_sum <- self$add_weight(name = "mse_sum",
- en: initializer = "zeros",
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: initializer = "zeros",
- en: dtype = "float32")
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: dtype = "float32")
- en: self$total_samples <- self$add_weight(name = "total_samples",
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: self$total_samples <- self$add_weight(name = "total_samples",
- en: initializer = "zeros",
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: initializer = "zeros",
- en: dtype = "int32")
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: dtype = "int32")
- en: '},'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: update_state = function(y_true, y_pred, sample_weight = NULL) {➍
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: update_state = function(y_true, y_pred, sample_weight = NULL) {➍
- en: num_samples <- tf$shape(y_pred)[1]
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: num_samples <- tf$shape(y_pred)[1]
- en: num_features <- tf$shape(y_pred)[2]
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: num_features <- tf$shape(y_pred)[2]
- en: y_true <- tf$one_hot➎(y_true, depth = num_features)➏
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: y_true <- tf$one_hot➎(y_true, depth = num_features)➏
- en: mse <- sum((y_true - y_pred) ^ 2)➐
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: mse <- sum((y_true - y_pred) ^ 2)➐
- en: self$mse_sum$assign_add(mse)
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: self$mse_sum$assign_add(mse)
- en: self$total_samples$assign_add(num_samples)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: self$total_samples$assign_add(num_samples)
- en: '},'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: result = function() {
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: result = function() {
- en: sqrt(self$mse_sum /
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: sqrt(self$mse_sum /
- en: tf$cast(self$total_samples, "float32"))➑
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: tf$cast(self$total_samples, "float32"))➑
- en: '},'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: reset_state = function() {
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: reset_state = function() {
- en: self$mse_sum$assign(0)
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: self$mse_sum$assign(0)
- en: self$total_samples$assign(0L)
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: self$total_samples$assign(0L)
- en: '}'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **We will be using tf module functions here.**
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们将使用tf模块函数。**
- en: ➋ **Define a new class that subclasses the Metric base class.**
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **定义一个新的类，它是Metric基类的子类。**
- en: ➌ **Define the state variables in the constructor. Like for layers, you have
    access to the add_weight() method.**
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **在构造函数中定义状态变量。像层一样，你可以访问add_weight()方法。**
- en: ➍ **Implement the state update logic in update_state(). The y_true argument
    is the targets (or labels) for one batch, and y_pred represents the corresponding
    predictions from the model. You can ignore the sample_weight argument—we won't
    use it here.**
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **在update_state()中实现状态更新逻辑。y_true参数是一个batch的目标（或标签），而y_pred表示模型的相应预测。你可以忽略sample_weight参数——我们这里不会使用它。**
- en: ➎ **Remember, tf module functions use 0-based counting conventions. A value
    of 0 in y_true places the 1 in the first position of the one-hot vector.**
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **记住，tf模块函数使用基于0的计数惯例。y_true中的值为0在one-hot向量的第一个位置上放置1。**
- en: ➏ **To match our MNIST model, we expect categorical predictions and integer
    labels.**
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **为了匹配我们的 MNIST 模型，我们期望分类预测和整数标签。**
- en: ➐ **We could also write this as tf$reduce_sum (tf$square(tf$subtract(y_true,
    y_pred))).**
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **我们也可以将此写为 tf$reduce_sum (tf$square(tf$subtract(y_true, y_pred)))。**
- en: ➑ **Cast total_samples to match the dtype of mse_sum.**
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **将 total_samples 强制转换为与 mse_sum 相匹配的 dtype。**
- en: Note that in update_state() we use tf$shape(y_pred) instead of y_pred$shape.
    tf$shape() returns the shape as a tf.Tensor, instead of a tf.TensorShape like
    y_pred$shape would. tf$shape() allows tf_function() to compile a function that
    can operate on tensors with undefined shapes, like our inputs here which have
    a undefined batch dimension. We learn more about tf_function() soon.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 update_state() 中我们使用 tf$shape(y_pred) 而不是 y_pred$shape。tf$shape() 返回一个
    tf.Tensor 形式的形状，而不是像 y_pred$shape 那样返回 tf.TensorShape。tf$shape() 允许 tf_function()
    编译一个可以操作具有未定义形状的张量的函数，例如我们这里的输入具有未定义的批量维度。我们很快就会了解更多关于 tf_function() 的知识。
- en: 'You use the result() method to return the current value of the metric:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 result() 方法返回指标的当前值：
- en: result = function()
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: result = function()
- en: sqrt(self$mse_sum /
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: sqrt(self$mse_sum /
- en: tf$cast(self$total_samples, "float32"))
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: tf$cast(self$total_samples, "float32"))
- en: 'Meanwhile, you also need to expose a way to reset the metric state without
    having to reinstantiate it—this enables the same metric objects to be used across
    different epochs of training or across both training and evaluation. You do this
    with the reset_state() method:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，您还需要暴露一种方法来重置指标状态，而无需重新实例化它——这使得可以在不同的训练周期或在训练和评估期间使用相同的指标对象。您可以使用 reset_state()
    方法来实现这一点：
- en: reset_state = function() {
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: reset_state = function() {
- en: self$mse_sum$assign(0)
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: self$mse_sum$assign(0)
- en: self$total_samples$assign(0L)➊
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: self$total_samples$assign(0L)➊
- en: '}'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Note that we pass an integer because total_samples has an integer dtype.**
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **注意我们传递的是一个整数，因为 total_samples 具有整数类型。**
- en: 'Custom metrics can be used just like built-in ones. Let’s test-drive our own
    metric:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义指标可以像内置指标一样使用。让我们试驾我们自己的指标：
- en: model <- get_mnist_model()
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_mnist_model()
- en: model %>%
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = list("accuracy", metric_root_mean_squared_error()))
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = list("accuracy", metric_root_mean_squared_error()))
- en: model %>%
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: fit(train_images, train_labels,
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_images, train_labels,
- en: epochs = 3,
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 3,
- en: validation_data = list(val_images, val_labels))
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(val_images, val_labels))
- en: test_metrics <- model %>% evaluate(test_images, test_labels)
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: test_metrics <- model %>% evaluate(test_images, test_labels)
- en: You can now see the fit() progress bar displaying the RMSE of your model.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以看到 fit() 进度条显示您模型的 RMSE。
- en: 7.3.2 Using callbacks
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用回调函数
- en: 'Launching a training run on a large dataset for tens of epochs using the model
    fit() method can be a bit like launching a paper airplane: past the initial impulse,
    you don’t have any control over its trajectory or its landing spot. If you want
    to avoid bad outcomes (and thus wasted paper airplanes), it’s smarter to use not
    a paper plane but a drone that can sense its environment, send data back to its
    operator, and automatically make steering decisions based on its current state.
    The Keras *callbacks* API will help you transform your call to fit(model) from
    a paper airplane into a smart, autonomous drone that can self-introspect and dynamically
    take action.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 fit() 方法对大型数据集进行数十个时期的训练运行有点像发射一架纸飞机：过了最初的冲动，您就无法控制其轨迹或着陆点。如果您想避免不良结果（从而浪费纸飞机），最明智的做法不是使用纸飞机，而是使用一个可以感知环境、将数据发送回其操作者并根据当前状态自动做出转向决策的无人机。Keras
    的 *回调* API 将帮助您将对 fit(model) 的调用从纸飞机变成一个聪明的、自主的无人机，它可以自我检查并根据当前状态动态采取行动。
- en: 'A callback is an object (a class instance implementing specific methods) that
    is passed to the model in the call to fit() and that is called by the model at
    various points during training. It has access to all the available data about
    the state of the model and its performance, and it can take action: interrupt
    training, save a model, load a different weight set, or otherwise alter the state
    of the model. Here are some examples of ways you can use callbacks:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 回调是一个对象（实现特定方法的类实例），它在调用 fit() 方法时被传递给模型，并在训练过程中的各个时刻被模型调用。它可以访问模型及其性能的所有可用数据，并且可以采取行动：中断训练，保存模型，加载不同的权重集，或者以其他方式更改模型的状态。以下是您可以使用回调的一些示例：
- en: '*Model checkpointing*—Saving the current state of the model at different points
    during training.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型检查点*—在训练过程中的不同时间点保存模型的当前状态。'
- en: '*Early stopping*—Interrupting training when the validation loss is no longer
    improving (and, of course, saving the best model obtained during training).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提前停止训练*——当验证损失不再改善时中断训练（当然，同时保存训练过程中表现最佳的模型）。'
- en: '*Dynamically adjusting the value of certain parameters during training*—Such
    as the learning rate of the optimizer.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动态调整训练过程中特定参数的值*——比如优化器的学习率。'
- en: '*Logging training and validation metrics during training, or visualizing the
    representations learned by the model as they’re updated*—The fit() progress bar
    that you’re familiar with is in fact a callback!'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*记录训练和验证指标，或者可视化模型学习到的表示的更新过程*——fit() 的进度条实际上就是一个回调！'
- en: 'The keras package includes a number of built-in callbacks (this is not an exhaustive
    list):'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: keras 包中包含了许多内置回调（这不是一个详尽列表）：
- en: callback_model_checkpoint()
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint()
- en: callback_early_stopping()
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: callback_early_stopping()
- en: callback_learning_rate_scheduler()
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: callback_learning_rate_scheduler()
- en: callback_reduce_lr_on_plateau()
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: callback_reduce_lr_on_plateau()
- en: callback_csv_logger()
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: callback_csv_logger()
- en: …
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Let’s review two of them to give you an idea of how to use them: callback_early_stopping()
    and callback_model_checkpoint().'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下其中的两个示例——callback_early_stopping() 和 callback_model_checkpoint()，以便了解它们的使用方法。
- en: THE EARLY STOPPING AND MODEL CHECKPOINT CALLBACKS
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提前停止和模型检查点回调
- en: When you’re training a model, there are many things you can’t predict from the
    start. In particular, you can’t tell how many epochs will be needed to get to
    an optimal validation loss. Our examples so far have adopted the strategy of training
    for enough epochs that you begin overfitting, using the first run to figure out
    the proper number of epochs to train for, and then finally launching a new training
    run from scratch using this optimal number. Of course, this approach is wasteful.
    A much better way to handle this is to stop training when you measure that the
    validation loss is no longer improving. This can be achieved using callback_early_stopping().
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，有很多事情一开始无法预测。特别是，你无法确定要达到最佳验证损失需要多少个 epoch。我们迄今为止的示例采用的策略是训练足够多的 epoch，直到开始出现过拟合，利用第一次运行来确定需要训练的适当
    epoch 数，然后最终从头开始启动新的训练。当然，这种方法是浪费的。更好的处理方法是在测量到验证损失不再改善时停止训练。这可以使用 callback_early_stopping()
    实现。
- en: 'The early stopping callback interrupts training once a target metric being
    monitored has stopped improving for a fixed number of epochs. For instance, this
    callback allows you to interrupt training as soon as you start overfitting, thus
    avoiding having to retrain your model for a smaller number of epochs. This callback
    is typically used in combination with callback_model_checkpoint(), which lets
    you continually save the model during training (and, optionally, save only the
    current best model so far: the version of the model that achieved the best performance
    at the end of an epoch).'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止回调一旦自定监控的指标连续固定轮不再改善，就中断训练。例如，该回调可让你一旦开始出现过拟合，就中断训练，从而避免必须减少训练轮数重新训练模型。此回调通常与
    callback_model_checkpoint() 结合使用，后者让你在训练期间不断保存模型（可选地，仅保存到目前为止表现最佳的模型，即在一个 epoch
    结束时获得最佳性能的模型版本）。
- en: Listing 7.19 Using the callbacks argument in the fit() method
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 7.19 在 fit() 方法中使用 callbacks 参数 '
- en: callbacks_list <- list(
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks_list <- list(
- en: callback_early_stopping(
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: callback_early_stopping(
- en: monitor = "val_accuracy", patience = 2),➊
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: monitor = "val_accuracy", patience = 2),➊
- en: callback_model_checkpoint(➋
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint(➋
- en: filepath = "checkpoint_path.keras",➌
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = "checkpoint_path.keras",➌
- en: monitor = "val_loss", save_best_only = TRUE)➍
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: monitor = "val_loss", save_best_only = TRUE)➍
- en: )
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- get_mnist_model()
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_mnist_model()
- en: model %>% compile(
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(
- en: optimizer = "rmsprop",
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")➎
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")➎
- en: model %>% fit(
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(
- en: train_images, train_labels,
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: train_images, train_labels,
- en: epochs = 10,
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: callbacks = callbacks_list,➏
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks_list,➏
- en: validation_data = list(val_images, val_labels))➐
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(val_images, val_labels))➐
- en: ➊ **Interrupt training when validation accuracy has stopped improving for two
    epochs.**
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **当验证准确率连续两轮停止改善时中断训练。**
- en: ➋ **Save the current weights after every epoch.**
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在每个 epoch 后保存当前的权重。**
- en: ➌ **Path to the destination model file**
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **目标模型文件的保存路径**
- en: ➍ **These two arguments mean you won't overwrite the model file unless val_loss
    has improved, which allows you to keep the best model seen during training.**
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **这两个参数意味着您只有在val_loss有所改善时才会重写模型文件，这样您就可以保留训练过程中见过的最佳模型。**
- en: ➎ **You monitor accuracy, so it should be part of the model's metrics.**
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **您监控准确率，所以它应该是模型的度量值之一。**
- en: ➏ **Callbacks are passed to the model via the callbacks argument in fit(), which
    takes a list of callbacks. You can pass any number of callbacks.**
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **回调函数通过`fit()`中的callbacks参数传递给模型，该参数接受一个回调函数列表。您可以传递任意数量的回调函数。**
- en: ➐ **Note that because the callback will monitor validation loss and validation
    accuracy, you need to pass validation_data to the call to fit().**
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **请注意，由于回调函数会监控val_loss和val_accuracy，您需要将validation_data传递给fit()函数的调用。**
- en: 'Note that you can always save models manually after training as well—just call
    save_ model_tf(model, ‘my_checkpoint_path’). To reload the model you’ve saved,
    just use the following:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您也可以在训练后手动保存模型，只需调用save_model_tf(model，'my_checkpoint_path')。要重新加载保存的模型，只需使用以下命令：
- en: model <- load_model_tf("checkpoint_path.keras")
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("checkpoint_path.keras")
- en: 7.3.3 Writing your own callbacks
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 编写自己的回调函数
- en: 'If you need to take a specific action during training that isn’t covered by
    one of the built-in callbacks, you can write your own callback. Callbacks are
    implemented by subclassing the Keras Callback class with new_callback_class().
    You can then implement any number of the following transparently named methods,
    which are called at various points during training:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在训练过程中执行某个特定的操作，而这个操作不包含在内置的回调函数中，您可以编写自己的回调函数。回调函数通过子类化Keras Callback类并使用new_callback_class()来实现。然后，您可以实现以下任意数量的透明命名方法，在训练的不同阶段调用：
- en: on_epoch_begin(epoch, logs)➊
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: on_epoch_begin(epoch, logs)➊
- en: on_epoch_end(epoch, logs)➋
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: on_epoch_end(epoch, logs)➋
- en: on_batch_begin(batch, logs)➌
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: on_batch_begin(batch, logs)➌
- en: on_batch_end(batch, logs)➍
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个batch结束时调用➍
- en: on_train_begin(logs)➎
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: on_train_begin(logs)➎
- en: on_train_end(logs)➏
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: on_train_end(logs)➏
- en: ➊ **Called at the start of every epoch**
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在每个epoch开始时调用**
- en: ➋ **Called at the end of every epoch**
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在每个epoch结束时调用**
- en: ➌ **Called right before processing each batch**
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **在处理每个batch之前调用**
- en: ➍ **Called right after processing each batch**
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **在处理每个batch之后调用**
- en: ➎ **Called at the start of training**
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **在训练开始时调用**
- en: ➏ **Called at the end of training**
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **在训练结束时调用**
- en: These methods are all called with a logs argument, which is a named list containing
    information about the previous batch, epoch, or training run—training and validation
    metrics, and so on. The on_epoch_* and on_batch_* methods also take the epoch
    or batch index as their first argument (an integer).
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法都带有logs参数，它是一个带有关于上一批次、批次或训练运行的信息的有命名的列表，包括训练和验证的指标等。on_epoch_*和on_batch_*方法还将epoch或batch索引作为它们的第一个参数（一个整数）。
- en: Here’s a simple example that saves a list of per-batch loss values during training
    and saves a graph of these values at the end of each epoch.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的示例，它在训练期间保存了每个batch的损失值列表，并在每个epoch结束时保存了这些值的图形。
- en: '**Listing 7.20 Creating a custom callback by subclassing the Callback class**'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 7.20：通过子类化Callback类创建自定义回调函数**'
- en: callback_plot_per_batch_loss_history <- new_callback_class(
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: callback_plot_per_batch_loss_history <- new_callback_class(
- en: classname = "PlotPerBatchLossHistory",
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "PlotPerBatchLossHistory",
- en: initialize = function(file = "training_loss.pdf") {
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(file = "training_loss.pdf") {
- en: private$outfile <- file
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: private$outfile <- file
- en: '},'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: on_train_begin = function(logs = NULL) {
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: on_train_begin = function(logs = NULL) {
- en: private$plots_dir <- tempfile()
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: private$plots_dir <- tempfile()
- en: dir.create(private$plots_dir)
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: dir.create(private$plots_dir)
- en: private$per_batch_losses <-
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: private$per_batch_losses <-
- en: fastmap::faststack(init = self$params$steps)
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: fastmap::faststack(init = self$params$steps)
- en: '},'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: on_epoch_begin = function(epoch, logs = NULL) {
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: on_epoch_begin = function(epoch, logs = NULL) {
- en: private$per_batch_losses$reset()
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: private$per_batch_losses$reset()
- en: '},'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: on_batch_end = function(batch, logs = NULL) {
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: on_batch_end = function(batch, logs = NULL) {
- en: private$per_batch_losses$push(logs$loss)
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: private$per_batch_losses$push(logs$loss)
- en: '},'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: on_epoch_end = function(epoch, logs = NULL) {
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: on_epoch_end = function(epoch, logs = NULL) {
- en: losses <- as.numeric(private$per_batch_losses$as_list())
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: losses <- as.numeric(private$per_batch_losses$as_list())
- en: filename <- sprintf("epoch_%04i.pdf", epoch)
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: filename <- sprintf("epoch_%04i.pdf", epoch)
- en: filepath <- file.path(private$plots_dir, filename)
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: filepath <- file.path(private$plots_dir, filename)
- en: pdf(filepath, width = 7, height = 5)
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: pdf(filepath, width = 7, height = 5)
- en: on.exit(dev.off())
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: on.exit(dev.off())
- en: plot(losses, type = "o",
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: plot(losses, type = "o",
- en: ylim = c(0, max(losses)),
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: ylim = c(0, max(losses)),
- en: panel.first = grid(),
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: panel.first = grid(),
- en: main = sprintf("Training Loss for Each Batch\n(Epoch %i)", epoch),
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '`main = sprintf("每批次的训练损失\n(第 %i 个周期)", epoch),`'
- en: xlab = "Batch", ylab = "Loss")
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '`xlab = "Batch", ylab = "Loss")`'
- en: '},'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`,'
- en: on_train_end = function(logs) {
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '`on_train_end = function(logs) {`'
- en: private$per_batch_losses <- NULL
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '`private$per_batch_losses <- NULL`'
- en: plots <- sort(list.files(private$plots_dir, full.names = TRUE))
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '`plots <- sort(list.files(private$plots_dir, full.names = TRUE))`'
- en: qpdf::pdf_combine(plots, private$outfile)
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '`qpdf::pdf_combine(plots, private$outfile)`'
- en: unlink(private$plots_dir, recursive = TRUE)
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '`unlink(private$plots_dir, recursive = TRUE)`'
- en: '}'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: )
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '`)`'
- en: Growing R objects with fastmap::faststack()
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `fastmap::faststack()` 增长 R 对象
- en: Growing R vectors with c() or [[<- is typically slow and best avoided. In this
    example, we’re using fastmap::faststack() instead to collect the per-batch losses
    more efficiently.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `c()` 或 `[[<-` 来增长 R 向量通常很慢，最好避免。在这个示例中，我们使用 `fastmap::faststack()` 来更有效地收集每个批次的损失。
- en: private and self in custom class methods
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 在自定义类方法中的 `private` 和 `self`
- en: In all the previous examples, we’ve used self to keep track of instance properties,
    but in this callback example we used private. What’s the difference? Any property
    like self$foo is also accessible directly from the class instance at instance$foo.
    Properties of private, however, are accessible only from inside class methods.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有之前的例子中，我们使用 `self` 来跟踪实例属性，但在这个回调示例中，我们使用了 `private`。有什么区别呢？像 `self$foo`
    这样的属性也可以直接从类实例 `instance$foo` 访问。然而，`private` 的属性只能从类方法内部访问。
- en: Another important difference is that Keras automatically converts everything
    assigned to self to a Keras native format. This helps Keras automatically find,
    for example, all the tf.Variables associated with a custom Layer. This automatic
    conversion, however, can sometimes have a performance impact, or even fail for
    certain types of R objects (like faststack()). private, on the other hand, is
    a plain R environment that is left untouched by Keras. Only the class methods
    you write will directly interact with private properties.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的区别是，Keras 自动将分配给 `self` 的所有内容转换为 Keras 本地格式。这有助于 Keras 自动查找例如与自定义层关联的所有
    `tf.Variables`。然而，这种自动转换有时会对性能产生影响，甚至对某些类型的 R 对象（如 `faststack()`）失败。另一方面，`private`
    是一个纯 R 环境，Keras 不会对其进行任何更改。只有您编写的类方法才会直接与私有属性交互。
- en: model <- get_mnist_model()
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '`model <- get_mnist_model()`'
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '`model %>% compile(optimizer = "rmsprop",`'
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss = "sparse_categorical_crossentropy",`'
- en: metrics = "accuracy")
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '`metrics = "accuracy")`'
- en: model %>% fit(train_images, train_labels,
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '`model %>% fit(train_images, train_labels,`'
- en: epochs = 10,
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '`epochs = 10`,'
- en: callbacks = list(callback_plot_per_batch_loss_history()),
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '`callbacks = list(callback_plot_per_batch_loss_history()),`'
- en: validation_data = list(val_images, val_labels))
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '`validation_data = list(val_images, val_labels))`'
- en: We get plots that look like [figure 7.5](#fig7-5).
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的图像看起来像 [图 7.5](#fig7-5)。
- en: '![Image](../images/f0208-01.jpg)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0208-01.jpg)'
- en: '**Figure 7.5 The output of our custom history plotting callback**'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.5 我们自定义的历史绘图回调的输出**'
- en: 7.3.4 Monitoring and visualization with TensorBoard
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 使用 TensorBoard 进行监控和可视化
- en: 'To do good research or develop good models, you need rich, frequent feedback
    about what’s going on inside your models during your experiments. That’s the point
    of running experiments: to get information about how well a model performs—as
    much information as possible. Making progress is an iterative process, a loop:
    you start with an idea and express it as an experiment, attempting to validate
    or invalidate your idea. You run this experiment and process the information it
    generates. This inspires your next idea. The more iterations of this loop you’re
    able to run, the more refined and powerful your ideas become. Keras helps you
    go from idea to experiment in the least possible time, and fast GPUs can help
    you get from experiment to result as quickly as possible. But what about processing
    the experiment’s results? That’s where TensorBoard comes in (see [figure 7.6](#fig7-6)).'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行良好的研究或开发良好的模型，您需要关于实验过程中模型内部情况的丰富而频繁的反馈。这就是进行实验的目的：尽可能多地获取有关模型性能的信息。取得进展是一个迭代过程，一个循环：您从一个想法开始，将其表达为一个实验，试图验证或无效化您的想法。您运行此实验并处理它生成的信息。这激发了您的下一个想法。您能够运行此循环的迭代次数越多，您的想法就会变得越精炼和强大。Keras
    帮助您在尽可能短的时间内从想法到实验，快速的 GPU 可以帮助您尽快从实验到结果。但是处理实验结果呢？这就是 TensorBoard 的用武之地（见[图 7.6](#fig7-6)）。
- en: '![Image](../images/f0208-02.jpg)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0208-02.jpg)'
- en: '**Figure 7.6 The loop of progress**'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.6 进展的循环**'
- en: TensorBoard ([http://www.tensorflow.org/tensorboard](http://www.tensorflow.org/tensorboard))
    is a browser-based application that you can run locally. It’s the best way to
    monitor everything that goes on inside your model during training. With TensorBoard,
    you can
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard ([http://www.tensorflow.org/tensorboard](http://www.tensorflow.org/tensorboard))
    是一个可以在本地运行的基于浏览器的应用程序。这是在训练过程中监视模型内部发生的一切的最佳方式。通过 TensorBoard，您可以
- en: Visually monitor metrics during training
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中可视化监控指标
- en: Visualize your model architecture
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化您的模型架构
- en: Visualize histograms of activations and gradients
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化激活和梯度的直方图
- en: Explore embeddings in 3
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 3 维嵌入
- en: 'If you’re monitoring more information than just the model’s final loss, you
    can develop a clearer vision of what the model does and doesn’t do, and you can
    make progress more quickly. The easiest way to use TensorBoard with a Keras model
    and the fit() method is to use callback_tensorboard(). In the simplest case, just
    specify where you want the callback to write logs, and you’re good to go:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要监视的信息不仅仅是模型的最终损失，您可以更清晰地了解模型的工作情况，并更快地取得进展。使用 Keras 模型和 fit() 方法与 TensorBoard
    最简单的方法是使用 callback_tensorboard()。在最简单的情况下，只需指定回调应写入日志的位置，就可以开始了：
- en: model <- get_mnist_model()
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_mnist_model()
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy", metrics = "accuracy")
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy", metrics = "accuracy")
- en: model %>% fit(train_images, train_labels,
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(train_images, train_labels,
- en: epochs = 10,
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: validation_data = list(val_images, val_labels),
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(val_images, val_labels),
- en: callbacks = callback_tensorboard(log_dir = "logs/"))➊
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callback_tensorboard(log_dir = "logs/"))➊
- en: ➊ **Path to your log dir**
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **日志目录的路径**
- en: 'Once the model starts running, it will write logs at the target location. You
    can then view the logs by calling tensorboard(); this will launch a browser with
    tensorboard running:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型开始运行，它将在目标位置写入日志。然后，您可以通过调用 tensorboard() 查看日志；这将启动一个带有 tensorboard 运行的浏览器：
- en: tensorboard(log_dir = "logs/")➊
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: tensorboard(log_dir = "logs/")➊
- en: ➊ **Launches a browser with TensorBoard**
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **启动带有 TensorBoard 的浏览器**
- en: In the TensorBoard interface, you will be able to monitor live graphs of your
    training and evaluation metrics (see [figure 7.7](#fig7-7)).
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorBoard 界面中，您将能够监视您的训练和评估指标的实时图表（参见[图 7.7](#fig7-7)）。
- en: '![Image](../images/f0209-01.jpg)'
  id: totrans-612
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0209-01.jpg)'
- en: '**Figure 7.7 TensorBoard can be used for easy monitoring of training and evaluation
    metrics.**'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.7 TensorBoard 可用于轻松监控训练和评估指标。**'
- en: 7.4 Writing your own training and evaluation loops
  id: totrans-614
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 编写您自己的训练和评估循环
- en: The fit() workflow strikes a nice balance between ease of use and flexibility.
    It’s what you will use most of the time. However, it isn’t meant to support everything
    a deep learning researcher may want to do, even with custom metrics, custom losses,
    and custom callbacks.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: fit() 工作流在易用性和灵活性之间取得了良好的平衡。这是你大部分时间将要使用的内容。然而，它并不意味着支持深度学习研究人员可能想做的一切，即使是使用自定义指标、自定义损失和自定义回调。
- en: 'After all, the built-in fit() workflow is solely focused on *supervised learning*:
    a setup where there are known *targets* (also called *labels* or *annotations*)
    associated with your input data, and where you compute your loss as a function
    of these targets and the model’s predictions. However, not every form of machine
    learning falls into this category. There are other setups where no explicit targets
    are present, such as *generative learning* (which we will discuss in chapter 12),
    *self-supervised learning* (where targets are obtained from the inputs), and *reinforcement
    learning* (where learning is driven by occasional “rewards,” much like training
    a dog). Even if you’re doing regular supervised learning, as a researcher, you
    may want to add some novel bells and whistles that require low-level flexibility.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，内置的 fit() 工作流仅专注于 *监督学习*：一种设置，其中已知 *目标*（也称为 *标签* 或 *注释*）与您的输入数据相关联，并且您计算您的损失作为这些目标和模型预测的函数。然而，并非所有形式的机器学习都属于此类别。还有其他设置，其中没有明确的目标存在，例如
    *生成式学习*（我们将在第 12 章中讨论）、*自监督学习*（其中目标从输入中获取）和 *强化学习*（学习受偶尔“奖励”驱动，就像训练一只狗一样）。即使您正在进行常规的监督学习，作为研究人员，您可能也希望添加一些需要低级别灵活性的新颖功能。
- en: 'Whenever you find yourself in a situation where the built-in fit() is not enough,
    you will need to write your own custom training logic. You already saw simple
    examples of low-level training loops in chapters 2 and 3\. As a reminder, the
    contents of a typical training loop look like this:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 每当您发现内置的 fit() 不足以应对某种情况时，您将需要编写自己的自定义训练逻辑。在第 2 章和第 3 章中，您已经看到了低级别训练循环的简单示例。作为提醒，典型训练循环的内容如下：
- en: '**1** Run the forward pass (compute the model’s output) inside a gradient tape
    to obtain a loss value for the current batch of data.'
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 在梯度磁带中运行前向传播（计算模型的输出）以获取当前数据批次的损失值。'
- en: '**2** Retrieve the gradients of the loss with regard to the model’s weights.'
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 获取损失相对于模型权重的梯度。'
- en: '**3** Update the model’s weights so as to lower the loss value on the current
    batch of data.'
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 更新模型的权重以降低当前数据批次的损失值。'
- en: These steps are repeated for as many batches as necessary. This is essentially
    what fit() does under the hood. In this section, you will learn to reimplement
    fit() from scratch, which will give you all the knowledge you need to write any
    training algorithm you may come up with. Let’s go over the details.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将根据需要重复执行多个批次。这本质上是 fit() 在内部执行的操作。在本节中，您将学习如何从头开始重新实现 fit()，这将为您提供编写任何可能出现的训练算法所需的所有知识。让我们详细看一下。
- en: 7.4.1 Training vs. inference
  id: totrans-622
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 训练与推断
- en: In the low-level training loop examples you’ve seen so far, step 1 (the forward
    pass) was done via predictions <- model(inputs), and step 2 (retrieving the gradients
    computed by the gradient tape) was done via gradients <- tape$gradient(loss, model$weights).
    In the general case, there are actually two subtleties you need to take into account.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了低级别训练循环示例，其中步骤 1（前向传播）通过 predictions <- model(inputs) 完成，步骤 2（获取梯度磁带计算的梯度）通过
    gradients <- tape$gradient(loss, model$weights) 完成。在一般情况下，实际上有两个您需要考虑的细微之处。
- en: Some Keras layers, such as layer_dropout(), have different behaviors during
    *training* and during *inference* (when you use them to generate predictions).
    Such layers expose a training Boolean argument in their call() method. Calling
    dropout (inputs, training = TRUE) will drop some activation entries, whereas calling
    dropout (inputs, training = FALSE) does nothing. By extension, Functional and
    Sequential models also expose this training argument in their call() methods.
    Remember to pass training = TRUE when you call a Keras model during the forward
    pass! Our forward pass thus becomes predictions <- model(inputs, training = TRUE).
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 某些 Keras 层，如 layer_dropout()，在*训练*和*推断*（用于生成预测时）期间有不同的行为。这些层在其 call() 方法中公开了一个
    training 布尔参数。调用 dropout(inputs, training = TRUE) 将会丢弃一些激活项，而调用 dropout(inputs,
    training = FALSE) 则不会做任何操作。顺延而言，Functional 和 Sequential 模型也在它们的 call() 方法中公开了这个
    training 参数。记得在前向传播时传递 training = TRUE！因此，我们的前向传播变成了 predictions <- model(inputs,
    training = TRUE)。
- en: 'In addition, note that when you retrieve the gradients of the weights of your
    model, you should not use tape$gradients(loss, model$weights), but rather tape$gradients(loss,
    model$trainable_weights). Indeed, layers and models own two kinds of weights:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，当您检索模型权重的梯度时，您不应该使用 tape$gradients(loss, model$weights)，而应该使用 tape$gradients(loss,
    model$trainable_weights)。实际上，层和模型拥有两种权重：
- en: '*Trainable weights*—These are meant to be updated via backpropagation to minimize
    the loss of the model, such as the kernel and bias of a Dense layer.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可训练的权重* —— 这些权重应通过反向传播来最小化模型的损失，例如 Dense 层的核和偏置。'
- en: '*Nontrainable weights*—These are meant to be updated during the forward pass
    by the layers that own them. For instance, if you wanted a custom layer to keep
    a counter of how many batches it has processed so far, that information would
    be stored in a nontrainable weight, and at each batch, your layer would increment
    the counter by one.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不可训练的权重* —— 这些权重应由拥有它们的层在前向传播中更新。例如，如果您想要一个自定义层来保存到目前为止已处理了多少批次的计数器信息，那么该信息将存储在不可训练的权重中，并且在每个批次中，您的层将计数器递增一次。'
- en: 'Among Keras built-in layers, the only layer that features nontrainable weights
    is layer_ batch_normalization(), which we will discuss in chapter 9\. The batch
    normalization layer needs nontrainable weights to track information about the
    mean and standard deviation of the data that passes through it, so as to perform
    an online approximation of *feature normalization* (a concept you learned about
    in chapter 6). Taking into account these two details, a supervised-learning training
    step ends up looking like this:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras内置层中，唯一具有不可训练权重的层是layer_ batch_normalization()，我们将在第9章中讨论。批量归一化层需要不可训练权重来跟踪通过它的数据的平均值和标准差的信息，以便执行*特征归一化*的在线近似（这是您在第6章学到的概念）。考虑到这两个细节，监督学习的训练步骤最终看起来像这样：
- en: library(tensorflow)
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: train_step <- function(inputs, targets) {
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: train_step <- function(inputs, targets) {
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: predictions <- model(inputs, training = TRUE)
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model(inputs, training = TRUE)
- en: loss <- loss_fn(targets, predictions)
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- loss_fn(targets, predictions)
- en: '})'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradients <- tape$gradients(loss, model$trainable_weights)
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度 <- tape$gradients(loss, model$trainable_weights)
- en: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
- en: '}'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **We introduced zip_lists() in chapter 2.**
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们在第2章介绍了zip_lists()。**
- en: 7.4.2 Low-level usage of metrics
  id: totrans-639
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 度量的低级用法
- en: 'In a low-level training loop, you will probably want to leverage Keras metrics
    (whether custom ones or the built-in ones). You’ve already learned about the metrics
    API: simply call update_state(y_true, y_pred) for each batch of targets and predictions,
    and then use result() to query the current metric value:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在低级训练循环中，您可能希望利用Keras度量（无论是自定义的还是内置的）。您已经了解了度量API：只需为每个目标和预测的批次调用update_state(y_true,
    y_pred)，然后使用result()来查询当前度量值：
- en: metric <- metric_sparse_categorical_accuracy()
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: metric <- metric_sparse_categorical_accuracy()
- en: targets <- c(0, 1, 2)
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: targets <- c(0, 1, 2)
- en: predictions <- rbind(c(1, 0, 0),
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- rbind(c(1, 0, 0),
- en: c(0, 1, 0),
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: c(0, 1, 0),
- en: c(0, 0, 1))
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: c(0, 0, 1))
- en: metric$update_state(targets, predictions)
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: metric$update_state(targets, predictions)
- en: current_result <- metric$result()
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: current_result <- metric$result()
- en: 'sprintf("result: %.2f", as.array(current_result))➊'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("result: %.2f", as.array(current_result))➊'
- en: '[1] "result: 1.00"'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "result: 1.00"'
- en: ➊ **as.array() to convert Tensor to R value**
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **as.array()将Tensor转换为R值**
- en: 'You may also need to track the average of a scalar value, such as the model’s
    loss. You can do this via metric_mean():'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还需要跟踪标量值的平均值，例如模型的损失。您可以通过metric_mean()来实现：
- en: values <- c(0, 1, 2, 3, 4)
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: values <- c(0, 1, 2, 3, 4)
- en: mean_tracker <- metric_mean()
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: mean_tracker <- metric_mean()
- en: for (value in values)
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: for (value in values)
- en: mean_tracker$update_state(value)
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: mean_tracker$update_state(value)
- en: 'sprintf("Mean of values: %.2f", as.array(mean_tracker$result()))'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("Mean of values: %.2f", as.array(mean_tracker$result()))'
- en: '[1] "Mean of values: 2.00"'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "Mean of values: 2.00"'
- en: Remember to use metric$reset_state() when you want to reset the current results
    (at the start of a training epoch or at the start of evaluation).
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 在想要重置当前结果时，请记得使用metric$reset_state()（在训练时期的开始或评估的开始时）。
- en: 7.4.3 A complete training and evaluation loop
  id: totrans-659
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 完整的训练和评估循环
- en: Let’s combine the forward pass, backward pass, and metrics tracking into a fit()-like
    training step function that takes a batch of data and targets and returns the
    logs that would get displayed by the fit() progress bar.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前向传播、反向传播和指标跟踪结合到一个类似fit()的训练步骤函数中，该函数接受一批数据和目标，并返回fit()进度条将显示的日志。
- en: '**Listing 7.21 Writing a step-by-step training loop: The training step function**'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 7.21 编写逐步训练循环：训练步骤函数**'
- en: model <- get_mnist_model()
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_mnist_model()
- en: loss_fn <- loss_sparse_categorical_crossentropy()➊
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: loss_fn <- loss_sparse_categorical_crossentropy()➊
- en: optimizer <- optimizer_rmsprop()➋
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer <- optimizer_rmsprop()➋
- en: metrics <- list(metric_sparse_categorical_accuracy())➌
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: metrics <- list(metric_sparse_categorical_accuracy())➌
- en: loss_tracking_metric <- metric_mean()➍
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: loss_tracking_metric <- metric_mean()➍
- en: train_step <- function(inputs, targets) {
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: train_step <- function(inputs, targets) {
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: predictions <- model(inputs, training = TRUE)➎
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model(inputs, training = TRUE)➎
- en: loss <- loss_fn(targets, predictions)
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- loss_fn(targets, predictions)
- en: '})'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradients <- tape$gradient(loss,➏
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: gradients <- tape$gradient(loss,➏
- en: model$trainable_weights)➏
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: model$trainable_weights)➏
- en: optimizer$apply_gradients(zip_lists(gradients,
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer$apply_gradients(zip_lists(gradients,
- en: model$trainable_weights))
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: model$trainable_weights))
- en: logs <- list()
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: logs <- list()
- en: for (metric in metrics) {➐
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: for (metric in metrics) {➐
- en: metric$update_state(targets, predictions)
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: metric$update_state(targets, predictions)
- en: logs[[metric$name]] <- metric$result()
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: logs[[metric$name]] <- metric$result()
- en: '}'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: loss_tracking_metric$update_state(loss)➑
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: loss_tracking_metric$update_state(loss)➑
- en: logs$loss <- loss_tracking_metric$result()
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: logs$loss <- loss_tracking_metric$result()
- en: logs➒
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 日志➒
- en: '}'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Prepare the loss function.**
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **准备损失函数。**
- en: ➋ **Prepare the optimizer.**
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **准备优化器。**
- en: ➌ **Prepare the list of metrics to monitor.**
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **准备要监视的指标列表。**
- en: ➍ **Prepare a metric_mean() tracker to keep track of the loss average.**
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **准备一个metric_mean()跟踪器来记录平均损失。**
- en: ➎ **Run the forward pass. Note that we pass training = TRUE.**
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **运行前向传播。请注意，我们传递training = TRUE。**
- en: ➏ **Run the backward pass. Note that we use model$trainable_weights.**
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **运行反向传播。请注意，我们使用model$trainable_weights。**
- en: ➐ **Keep track of metrics.**
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **跟踪指标。**
- en: ➑ **Keep track of the loss average.**
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **跟踪损失平均值。**
- en: ➒ **Return the current values of the metrics and the loss.**
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **返回指标和损失的当前值。**
- en: We will need to reset the state of our metrics at the start of each epoch and
    before running evaluation. Here’s a utility function to do it.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在每个迭代开始时和运行评估之前重置指标的状态。下面是一个实用函数来完成这项工作。
- en: '**Listing 7.22 Writing a step-by-step training loop: Resetting the metrics**'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 7.22 逐步编写训练循环：重置指标**'
- en: reset_metrics <- function() {
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: reset_metrics <- function() {
- en: for (metric in metrics)
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: for (指标 in 指标)
- en: metric$reset_state()
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: metric$reset_state()
- en: loss_tracking_metric$reset_state()
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: loss_tracking_metric$reset_state()
- en: '}'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: We can now lay out our complete training loop. Note that we use a TensorFlow
    Data-set object from the tfdatasets package to turn our R array data into an iterator
    that iterates over the data in batches of size 32\. The mechanics are identical
    to the dataset iterator we implemented in chapter 2, just the names are different
    now. We build the TensorFlow Dataset instance from our R arrays with tensor_slices_dataset(),
    convert it to an iterator with as_iterator(), and then repeatedly call iter_next()
    on the iterator to get the next batch. One difference between what we saw in chapter
    2 and now is that iter_next() returns Tensor objects, not R arrays. We cover much
    more about tfdatasets in chapter 8.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以布置完整的训练循环了。请注意，我们使用来自tfdatasets包的TensorFlow数据集对象，将我们的R数组数据转换为以大小为32的批次迭代的迭代器。机制与我们在第2章实施的数据集迭代器是相同的，只是现在的名称不同了。我们用tensor_slices_dataset()从R数组构建TensorFlow数据集实例，用as_iterator()将其转换为迭代器，然后重复调用iter_next()来获取下一个批次。在第2章看到的不同之一是，iter_next()返回的是Tensor对象，而不是R数组。我们将在第8章更详细地介绍tfdatasets。
- en: '**Listing 7.23 Writing a step-by-step training loop: The loop itself**'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 7.23 逐步编写训练循环：循环本身**'
- en: library(tfdatasets)
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: training_dataset <-
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: training_dataset <-
- en: list(inputs = train_images, targets = train_labels) %>%
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: list(inputs = train_images, targets = train_labels) %>%
- en: tensor_slices_dataset() %>%
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: tensor_slices_dataset() %>%
- en: dataset_batch(32)
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_batch(32)
- en: epochs <- 3
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代周期 <- 3
- en: for (epoch in seq(epochs)) {
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: for (迭代周期 in seq(epochs)) {
- en: reset_metrics()
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: reset_metrics()
- en: training_dataset_iterator <- as_iterator(training_dataset)
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: training_dataset_iterator <- as_iterator(training_dataset)
- en: repeat {
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 重复 {
- en: batch <- iter_next(training_dataset_iterator)
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: batch <- iter_next(training_dataset_iterator)
- en: if (is.null(batch))➊
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: if (is.null(batch))➊
- en: break
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 中断
- en: logs <- train_step(batch$inputs, batch$targets)
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: logs <- train_step(batch$inputs, batch$targets)
- en: '}'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: writeLines(c(
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: writeLines(c(
- en: sprintf("Results at the end of epoch %s", epoch),
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: sprintf("第 %s 次迭代结束时的结果", 迭代周期),
- en: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
- en: ))
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: '}'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Results at the end of epoch 1
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 第1次迭代结束时的结果
- en: '…sparse_categorical_accuracy: 0.9156'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: …稀疏分类准确率：0.9156
- en: '…loss: 0.2687'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: …损失：0.2687
- en: Results at the end of epoch 2
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 第2次迭代结束时的结果
- en: '…sparse_categorical_accuracy: 0.9539'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: …稀疏分类准确率：0.9539
- en: '…loss: 0.1659'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: …损失：0.1659
- en: Results at the end of epoch 3
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 第3次迭代结束时的结果
- en: '…sparse_categorical_accuracy: 0.9630'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: …稀疏分类准确率：0.9630
- en: '…loss: 0.1371'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: …损失：0.1371
- en: ➊ **iterator exhausted**
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **迭代器已耗尽**
- en: 'And here’s the evaluation loop: a simple for loop that repeatedly calls a test_step()
    function, which processes a single batch of data. The test_step() function is
    just a subset of the logic of train_step(). It omits the code that deals with
    updating the weights of the model—that is to say, everything involving the GradientTape()
    and the optimizer.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 这是评估循环：一个简单的for循环，它不断调用test_step()函数，该函数处理单个数据批次。test_step()函数只是train_step()逻辑的一个子集。它省略了处理更新模型权重的代码——也就是说，所有涉及GradientTape()和优化器的代码。
- en: '**Listing 7.24 Writing a step-by-step evaluation loop**'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 7.24 逐步编写评估循环**'
- en: test_step <- function(inputs, targets) {
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: test_step <- function(inputs, targets) {
- en: predictions <- model(inputs, training = FALSE)➊
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model(inputs, training = FALSE)➊
- en: loss <- loss_fn(targets, predictions)
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- loss_fn(targets, predictions)
- en: logs <- list()
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: logs <- list()
- en: for (metric in metrics) {
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: for (metric in metrics) {
- en: metric$update_state(targets, predictions)
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: metric$update_state(targets, predictions)
- en: logs[[paste0("val_", metric$name)]] <- metric$result()
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: logs[[paste0("val_", metric$name)]] <- metric$result()
- en: '}'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: loss_tracking_metric$update_state(loss)
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: loss_tracking_metric$update_state(loss)
- en: logs[["val_loss"]] <- loss_tracking_metric$result()
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: logs[["val_loss"]] <- loss_tracking_metric$result()
- en: logs
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: logs
- en: '}'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: val_dataset <- list(val_images, val_labels) %>%
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: val_dataset <- list(val_images, val_labels) %>%
- en: tensor_slices_dataset() %>%
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: tensor_slices_dataset() %>%
- en: dataset_batch(32)
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_batch(32)
- en: reset_metrics()
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: reset_metrics()
- en: val_dataset_iterator <- as_iterator(val_dataset)
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: val_dataset_iterator <- as_iterator(val_dataset)
- en: repeat {
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: repeat {
- en: batch <- iter_next(val_dataset_iterator)
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: batch <- iter_next(val_dataset_iterator)
- en: if(is.null(batch)) break➋
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: if(is.null(batch)) break➋
- en: c(inputs_batch, targets_batch) %<-% batch
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs_batch, targets_batch) %<-% batch
- en: logs <- test_step(inputs_batch, targets_batch)
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: logs <- test_step(inputs_batch, targets_batch)
- en: '}'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: writeLines(c(
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: writeLines(c(
- en: '"Evaluation results:",'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: '"评估结果：",'
- en: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
- en: ))
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: 'Evaluation results:'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果：
- en: '…val_sparse_categorical_accuracy: 0.9461'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '…val_sparse_categorical_accuracy: 0.9461'
- en: '…val_loss: 0.1871'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '…val_loss: 0.1871'
- en: ➊ **Note that we pass training = FALSE.**
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **请注意我们将 training 参数设置为 FALSE。**
- en: ➋ **iter_next() returns NULL once the dataset batch iterator is exhausted.**
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **一旦数据集批处理迭代器耗尽，iter_next() 将返回 NULL。**
- en: 'Congrats—you’ve just reimplemented fit() and evaluate()! Or almost: fit() and
    evaluate() support many more features, including large-scale distributed computation,
    which requires a bit more work. It also includes several key performance optimizations.
    Let’s take a look at one of these optimizations: TensorFlow function compilation.'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你刚刚重新实现了 fit() 和 evaluate()！或者几乎实现了：fit() 和 evaluate() 支持更多功能，包括大规模分布式计算，这需要更多的工作。它还包括几个关键的性能优化。让我们看看其中一个优化：TensorFlow
    函数编译。
- en: 7.4.4 Make it fast with tf_function()
  id: totrans-768
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 使用 tf_function() 提高性能
- en: You may have noticed that your custom loops are running significantly slower
    than the built-in fit() and evaluate(), despite implementing essentially the same
    logic. That’s because, by default, TensorFlow code is executed line by line, *eagerly*,
    much like regular R code using R arrays. Eager execution makes it easier to debug
    your code, but it is far from optimal from a performance standpoint.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，尽管实现了基本相同的逻辑，但自定义循环的运行速度明显比内置的 fit() 和 evaluate() 慢得多。这是因为，默认情况下，TensorFlow
    代码是逐行执行的，*即时执行*，就像使用 R 数组的常规 R 代码一样。即时执行使得调试代码更容易，但从性能的角度来看远非最佳。
- en: 'It’s more performant to *compile* your TensorFlow code into a *computation
    graph* that can be globally optimized in a way that code interpreted line by line
    cannot. The syntax to do this is very simple: just call tf_function() on any function
    you want to compile before executing, as shown in the following listing.'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的 TensorFlow 代码编译成一个可以进行全局优化的 *计算图* 比逐行解释代码更高效。要做到这一点的语法非常简单：只需在执行之前对你想要编译的任何函数调用
    tf_function()，就像下面的示例中所示。
- en: '**Listing 7.25 Using tf_function() with our evaluation-step function**'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 7.25 使用 tf_function() 与我们的评估步骤函数**'
- en: tf_test_step <- tf_function(test_step)➊
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: tf_test_step <- tf_function(test_step)➊
- en: val_dataset_iterator <- as_iterator(val_dataset)➋
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: val_dataset_iterator <- as_iterator(val_dataset)➋
- en: reset_metrics()
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: reset_metrics()
- en: while(!is.null(iter_next(val_dataset_iterator) -> batch)) {
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: while(!is.null(iter_next(val_dataset_iterator) -> batch)) {
- en: c(inputs_batch, targets_batch) %<-% batch
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs_batch, targets_batch) %<-% batch
- en: logs <- tf_test_step(inputs_batch, targets_batch)➌
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: logs <- tf_test_step(inputs_batch, targets_batch)➌
- en: '}'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: writeLines(c(
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: writeLines(c(
- en: '"Evaluation results:",'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '"评估结果：",'
- en: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("…%s: %.4f", names(logs), sapply(logs, as.numeric))'
- en: ))
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: 'Evaluation results:'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果：
- en: '…val_sparse_categorical_accuracy: 0.5190'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '…val_sparse_categorical_accuracy: 0.5190'
- en: '…val_loss: 1.6764'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '…val_loss: 1.6764'
- en: ➊ **Pass the test_step we defined previously to tf_function().**
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将我们之前定义的 test_step 传递给 tf_function()。**
- en: ➋ **Reuse the same TF Dataset defined in the previous example, but make a new
    iterator.**
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **重用前一个示例中定义的相同 TF 数据集，但创建一个新的迭代器。**
- en: ➌ **Use the compiled test step function this time.**
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **这次使用编译后的测试步骤函数。**
- en: On my machine we go from taking 2.4 seconds to run the evaluation loop to only
    0.6 seconds. Much faster!
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的机器上，我们的评估循环运行时间从 2.4 秒缩短到只有 0.6 秒。快得多！
- en: 'The speedup is even greater when the TF Dataset iteration loop is also compiled
    as a graph operation. You can use tf_function() to compile the full evaluation
    loop like this:'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 当 TF 数据集迭代循环也被编译为图操作时，速度提升甚至更大。你可以像这样使用 tf_function() 编译整个评估循环：
- en: my_evaluate <- tf_function(function(model, dataset) {
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: my_evaluate <- tf_function(function(model, dataset) {
- en: reset_metrics()
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: reset_metrics()
- en: for (batch in dataset) {
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: for (batch in dataset) {
- en: c(inputs_batch, targets_batch) %<-% batch
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs_batch, targets_batch) %<-% batch
- en: logs <- test_step(inputs_batch, targets_batch)
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: logs <- test_step(inputs_batch, targets_batch)
- en: '}'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: logs
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: logs
- en: '})'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: system.time(my_evaluate(model, val_dataset))["elapsed"]
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: system.time(my_evaluate(model, val_dataset))["elapsed"]
- en: elapsed
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: elapsed
- en: '0.283'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '0.283'
- en: This cuts the evaluation time by more than half again!
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 这进一步缩短了评估时间！
- en: Remember, while you are debugging your code, prefer running it eagerly, without
    any calls to tf_function(). It’s easier to track bugs this way. Once your code
    is working and you want to make it fast, add a tf_function() decorator to your
    training step and your evaluation step or any other performance-critical function.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，当您调试代码时，最好不要调用tf_function()，而是急切地运行它。这样更容易跟踪错误。一旦您的代码可行并且想要使其快速，就可以向您的训练步骤和评估步骤或任何其他性能关键的函数添加tf_function()修饰符。
- en: 7.4.5 Leveraging fit() with a custom training loop
  id: totrans-804
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.5 利用fit()进行自定义训练循环
- en: In the previous sections, we were writing our own training loop entirely from
    scratch. Doing so provides you with the most flexibility, but you end up writing
    a lot of code while simultaneously missing out on many convenient features of
    fit(), such as call-backs or built-in support for distributed training.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们完全从头编写了自己的训练循环。这样做为您提供了最大的灵活性，但同时您需要编写大量的代码，同时错过了许多方便的fit()功能，例如回调或分布式训练的内置支持。
- en: 'What if you need a custom training algorithm, but you still want to leverage
    the power of the built-in Keras training logic? There’s actually a middle ground
    between fit() and a training loop written from scratch: you can provide a custom
    training step function and let the framework do the rest.'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要自定义训练算法，但仍想利用内置的Keras训练逻辑的优势，那么在fit()和从头编写训练循环之间实际上有一种中间状态：你可以提供自定义训练步骤功能，让框架完成其他工作。
- en: 'You can do this by overriding the train_step() method of the Model class. This
    is the function that is called by fit() for every batch of data. You will then
    be able to call fit() as usual, and it will be running your own learning algorithm
    under the hood. Here’s a simple example:'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过覆盖Model类的train_step()方法来实现这一点。这是由fit()用于每个数据批次调用的函数。然后您将能够像通常一样调用fit()，它将在底层运行您自己的学习算法。下面是一个简单的例子：
- en: We create a new class that subclasses Model by calling new_model_class().
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过调用new_model_class()来创建一个子类Model的新类。
- en: We override the method train_step( data). Its contents are nearly identical
    to what we used in the previous section. It returns a named list mapping metric
    names (including the loss) to their current values.
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们覆盖train_step(data)方法。它的内容几乎与我们在前一节中使用的内容相同。它返回将指标名称（包括损失）映射到其当前值的命名列表。
- en: We implement a metrics active property that tracks the model’s Metric instances.
    This enables the model to automatically call reset_state() on the model’s metrics
    at the start of each epoch and at the start of a call to evaluate(), so you don’t
    have to do it by hand
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实现了一个跟踪模型的Metric实例的metrics活动属性(active property)。这使得模型能够在每个纪元(epoch)和在调用evaluate()时自动调用模型的metrics的reset_state()，因此不必手动执行。
- en: loss_fn <- loss_sparse_categorical_crossentropy()
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: loss_fn <- loss_sparse_categorical_crossentropy()
- en: loss_tracker <- metric_mean(name = "loss")➊
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: loss_tracker <- metric_mean(name = "loss")➊
- en: CustomModel <- new_model_class(
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: CustomModel <- new_model_class(
- en: classname = "CustomModel",
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "CustomModel",
- en: train_step = function(data) {➋
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: train_step = function(data) {➋
- en: c(inputs, targets) %<-% data
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs, targets) %<-% data
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: predictions <- self(inputs, training = TRUE)➌
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- self(inputs, training = TRUE)➌
- en: loss <- loss_fn(targets, predictions)
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- loss_fn(targets, predictions)
- en: '})'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradients <- tape$gradient(loss, model$trainable_weights)
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: gradients <- tape$gradient(loss, model$trainable_weights)
- en: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
- en: loss_tracker$update_state(loss)➍
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: loss_tracker$update_state(loss)➍
- en: list(loss = loss_tracker$result())➎
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: list(loss = loss_tracker$result())➎
- en: '},'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: metrics = mark_active(function() list(loss_tracker))➏
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = mark_active(function() list(loss_tracker))➏
- en: )
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **This metric object will be used to track the average of per-batch losses
    during training and evaluation.**
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **该度量对象将用于跟踪训练和评估期间每个批次损失的平均值。**
- en: ➋ **We override the train_step method.**
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们覆盖train_step方法。**
- en: ➌ **We use self(inputs, training = TRUE) instead of model(inputs, training =
    TRUE), because our model is the class itself.**
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **我们使用self(inputs, training = TRUE)而不是model(inputs, training = TRUE)，因为我们的模型是类本身。**
- en: ➍ **We update the loss tracker metric that tracks the average of the loss.**
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **我们更新损失跟踪器指标，该指标跟踪损失的平均值。**
- en: ➎ **We return the average loss so far by querying the loss tracker metric.**
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **通过查询损失跟踪器指标返回到目前为止的平均损失。**
- en: ➏ **Any metric you would like to reset across epochs should be listed here.**
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **您想在跨时期重置的任何指标都应在此处列出。**
- en: 'We can now instantiate our custom model, compile it (we pass only the optimizer,
    because the loss is already defined outside of the model), and train it using
    fit() as usual:'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实例化我们的自定义模型，编译它（我们仅传递优化器，因为损失已在模型外部定义），然后像往常一样使用 fit() 进行训练：
- en: inputs <- layer_input(shape = c(28 * 28))
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(28 * 28))
- en: features <- inputs %>%
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: features <- inputs %>%
- en: layer_dense(512, activation = "relu") %>%
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(512, activation = "relu") %>%
- en: layer_dropout(0.5)
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5)
- en: outputs <- features %>%
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- features %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: model <- CustomModel(inputs = inputs, outputs = outputs)➊
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: model <- CustomModel(inputs = inputs, outputs = outputs)➊
- en: model %>% compile(optimizer = optimizer_rmsprop())
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = optimizer_rmsprop())
- en: model %>% fit(train_images, train_labels, epochs = 3)
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(train_images, train_labels, epochs = 3)
- en: '➊ **Because we didn''t provide an initialize() method, the same signature as
    keras_model() is used: inputs, outputs, and optionally name.**'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **因为我们没有提供 initialize() 方法，所以使用与 keras_model() 相同的签名：inputs、outputs 和可选的 name。**
- en: 'There are a couple of points to note:'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个要点需要注意：
- en: This pattern does not prevent you from building models with the Functiona API.
    You can do this whether you’re building Sequential models, Functional API models,
    or subclassed models.
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种模式不会阻止您使用功能 API 构建模型。 无论您是构建序贯模型、功能 API 模型还是子类模型，都可以这样做。
- en: You don’t need to call tf_function() when you override train_step—the framework
    does it for you.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您重写 train_step 时，您不需要调用 tf_function() ——框架会为您执行此操作。
- en: 'Now, what about metrics, and what about configuring the loss via compile()?
    After you’ve called compile(), you get access to the following:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，指标呢？ 以及如何通过 compile() 配置损失？ 在调用 compile() 之后，您可以访问以下内容：
- en: self$compiled_loss—The loss function you passed to compile().
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: self$compiled_loss—您传递给 compile() 的损失函数。
- en: self$compiled_metrics—A wrapper for the list of metrics you passed, which allows
    you to call self$compiled_metrics$update_state() to update all of your metrics
    at once.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: self$compiled_metrics—一个包装器，用于您传递的指标列表，它允许您调用 self$compiled_metrics$update_state()
    一次更新所有指标。
- en: self$metrics—The actual list of metrics you passed to compile(). Note that it
    also includes a metric that tracks the loss, similar to what we did manually with
    our loss_tracking_metric earlier.
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: self$metrics—您传递给 compile() 的实际指标列表。 请注意，它还包括一个跟踪损失的指标，类似于我们之前手动进行的 loss_tracking_metric。
- en: 'We can thus write:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以这样写：
- en: CustomModel <- new_model_class(
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: CustomModel <- new_model_class(
- en: classname = "CustomModel",
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "CustomModel",
- en: train_step = function(data) {
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: train_step = function(data) {
- en: c(inputs, targets) %<-% data
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs, targets) %<-% data
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: predictions <- self(inputs, training = TRUE)
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 预测 <- self(inputs, training = TRUE)
- en: loss <- self$compiled_loss(targets, predictions)➊
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- self$compiled_loss(targets, predictions)➊
- en: '})'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradients <- tape$gradient(loss, model$trainable_weights)
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度 <- tape$gradient(loss, model$trainable_weights)
- en: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))
- en: self$compiled_metrics$update_state(
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: self$compiled_metrics$update_state(
- en: targets, predictions)➋
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: targets, predictions)➋
- en: results <- list()
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: results <- list()
- en: for(metric in self$metrics)
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: for(metric in self$metrics)
- en: results[[metric$name]] <- metric$result()
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: results[[metric$name]] <- metric$result()
- en: results➌
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: results➌
- en: '}'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Compute the loss via self$compiled_loss.**
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **通过 self$compiled_loss 计算损失。**
- en: ➋ **Update the model's metrics via self$compiled_metrics.**
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **通过 self$compiled_metrics 更新模型的指标。**
- en: ➌ **Return a named list mapping metric names to their current value.**
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **返回一个命名列表，将指标名称映射到其当前值。**
- en: 'Let’s try it:'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试：
- en: inputs <- layer_input(shape = c(28 * 28))
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(28 * 28))
- en: features <- inputs %>%
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 <- inputs %>%
- en: layer_dense(512, activation = "relu") %>%
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(512, activation = "relu") %>%
- en: layer_dropout(0.5)
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5)
- en: outputs <- features %>% layer_dense(10, activation = "softmax")
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- features %>% layer_dense(10, activation = "softmax")
- en: model <- CustomModel(inputs = inputs, outputs = outputs)
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: model <- CustomModel(inputs = inputs, outputs = outputs)
- en: model %>% compile(optimizer = optimizer_rmsprop(),
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = optimizer_rmsprop(),
- en: loss = loss_sparse_categorical_crossentropy(),
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: loss = loss_sparse_categorical_crossentropy(),
- en: metrics = metric_sparse_categorical_accuracy())
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = metric_sparse_categorical_accuracy())
- en: model %>% fit(train_images, train_labels, epochs = 3)
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(train_images, train_labels, epochs = 3)
- en: That was a lot of information, but you now know enough to use Keras to do almost
    anything.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 那是大量的信息，但现在你已经了解足够的内容，可以使用Keras几乎做任何事情。
- en: Summary
  id: totrans-886
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Keras offers a spectrum of different workflows, based on the principle of *progressive
    disclosure of complexity*. They all smoothly operate together.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras提供了一系列不同的工作流程，基于*逐步透露复杂性*的原则。它们都能够平稳地协同运作。
- en: You can build models via the Sequential API with keras_model_sequential(), via
    the Functional API with keras_model(), or by subclassing the Model class with
    new_model_class(). Most of the time, you’ll be using the Functional API.
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过Sequential API的keras_model_sequential()，通过Functional API的keras_model()，或者通过子类化Model类的new_model_class()来构建模型。大多数情况下，你将使用Functional
    API。
- en: The simplest way to train and evaluate a model is via the default fit() and
    evaluate() methods.
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估模型的最简单方法是通过默认的fit()和evaluate()方法。
- en: Keras callbacks provide a simple way to monitor models during your call to fit()
    and automatically take action based on the state of the model.
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras回调提供了一种简单的方法，在调用fit()期间监视模型并根据模型状态自动采取行动。
- en: You can also fully take control of what fit() does by overriding the train_
    step() method.
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可以通过覆盖train_ step()方法完全控制fit()的行为。
- en: Beyond fit(), you can also write your own training loops entirely from scratch.
    This is useful for researchers implementing brand-new training algorithms.
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了fit()之外，你还可以完全从头开始编写自己的训练循环。这对于实现全新训练算法的研究人员非常有用。
