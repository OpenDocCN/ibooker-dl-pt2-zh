- en: Chapter 7\. Creating Text Embedding Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章\. 创建文本嵌入模型
- en: Text embedding models lie at the foundation of many powerful natural language
    processing applications. They lay the groundwork for empowering already impressive
    technologies such as text generation models. We have already used embedding models
    throughout this book in a number of applications, such as supervised classification,
    unsupervised classification, semantic search, and even giving memory to text generation
    models like ChatGPT.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入模型是许多强大自然语言处理应用的基础。它们为赋能已经令人印象深刻的技术（如文本生成模型）奠定了基础。在本书中，我们已经在多种应用中使用了嵌入模型，如监督分类、无监督分类、语义搜索，甚至为像ChatGPT这样的文本生成模型提供记忆。
- en: It is nearly impossible to overstate the importance of embedding models in the
    field as they are the driving power behind so many applications. As such, in this
    chapter, we will discuss a variety of ways that we can create and fine-tune an
    embedding model to increase its representative and semantic power.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型在该领域的重要性几乎无法过分强调，因为它们是许多应用背后的驱动力。因此，在本章中，我们将讨论多种创建和微调嵌入模型的方法，以增强其代表性和语义能力。
- en: Let’s start by discovering what embedding models are and how they generally
    work.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始探索嵌入模型是什么以及它们通常如何工作。
- en: Embedding Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: Embeddings and embedding models have already been discussed in quite a number
    of chapters before (Chapters X, X, and X) thereby demonstrating their usefulness.
    Before going into training such a model, let’s recap what we have learned with
    embedding models before.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入和嵌入模型已经在之前的多个章节中讨论过（章节 X、X 和 X），这证明了它们的实用性。在训练这样的模型之前，让我们回顾一下之前对嵌入模型的学习。
- en: Unstructured textual data by itself is often quite hard to process. They are
    not values we can directly process, visualize and create actionable results from.
    We first have to convert this textual data to something that we can easily process,
    numeric representations. This process is often referred to as **embedding** the
    input to output usable vectors, namely **embeddings** as shown in [Figure 7-1](#fig_1_we_use_an_embedding_model_to_convert_textual_input).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化文本数据本身通常很难处理。它们不是我们可以直接处理、可视化并创建可操作结果的值。我们首先必须将这些文本数据转换为我们可以轻松处理的内容，即数值表示。这一过程通常被称为**嵌入**输入以输出可用向量，称为**嵌入**，如[图
    7-1](#fig_1_we_use_an_embedding_model_to_convert_textual_input)所示。
- en: '![We use an embedding model to convert textual input  such as documents  sentences  and
    phrases to numerical representations  called embeddings. ](assets/creating_text_embedding_models_249519_01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![我们使用嵌入模型将文本输入（如文档、句子和短语）转换为数值表示，称为嵌入。](assets/creating_text_embedding_models_249519_01.png)'
- en: Figure 7-1\. We use an embedding model to convert textual input, such as documents,
    sentences, and phrases to numerical representations, called embeddings.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 我们使用嵌入模型将文本输入（如文档、句子和短语）转换为数值表示，称为嵌入。
- en: This process of embedding the input is typically performed by an LLM, which
    we refer to as an *embedding model*. The main purpose of such a model is to be
    as accurate as possible in representing the textual data as an embedding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入输入的过程通常由一个大型语言模型（LLM）执行，我们称之为*嵌入模型*。这种模型的主要目的是尽可能准确地表示文本数据为嵌入。
- en: However, what does it mean to be accurate in representation? Typically, we want
    to capture the *semantic nature*, the meaning, of documents. If we can capture
    the core of what the document communicates, we hope to have captured what the
    document is about. In practice, this means that we expect vectors of documents
    that are similar to one another to be similar, whereas the embeddings of documents
    that each discuss something entirely different should be dissimilar. This idea
    of semantic similarity is visualized in [Figure 7-2](#fig_2_the_idea_of_semantic_similarity_is_that_we_expect).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，准确表示意味着什么呢？通常，我们希望捕捉文档的*语义特征*，即其含义。如果我们能够捕捉到文档所传达的核心内容，我们希望能捕捉到文档的主题。在实际操作中，这意味着我们希望相似文档的向量也要相似，而每个讨论完全不同主题的文档的嵌入则应不相似。这种语义相似性的理念在[图
    7-2](#fig_2_the_idea_of_semantic_similarity_is_that_we_expect)中得以可视化。
- en: '![The idea of semantic similarity is that we expect textual data that have
    similar meaning are also closer to each other in n dimensional space. As an example  it
    is illustrated here in 2 dimensional space. Do note that this is a simplified
    example. While 2 dimensional visualization helps illustrate the proximity and
    similarity of embeddings  these embeddings typically reside in high dimensional
    spaces. ](assets/creating_text_embedding_models_249519_02.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![语义相似性的概念是，我们期望具有相似意义的文本数据在n维空间中也彼此更接近。作为一个例子，这里在二维空间中进行了说明。请注意，这是一个简化的例子。虽然二维可视化有助于说明嵌入的接近性和相似性，但这些嵌入通常位于高维空间中。](assets/creating_text_embedding_models_249519_02.png)'
- en: Figure 7-2\. The idea of semantic similarity is that we expect textual data
    that have similar meaning are also closer to each other in n-dimensional space.
    As an example, it is illustrated here in 2-dimensional space. Do note that this
    is a simplified example. While 2-dimensional visualization helps illustrate the
    proximity and similarity of embeddings, these embeddings typically reside in high-dimensional
    spaces.
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 语义相似性的概念是，我们期望具有相似意义的文本数据在n维空间中也彼此更接近。作为一个例子，这里在二维空间中进行了说明。请注意，这是一个简化的例子。虽然二维可视化有助于说明嵌入的接近性和相似性，但这些嵌入通常位于高维空间中。
- en: An embedding model, however, can be trained for a number of purposes. For example,
    when we are building a sentiment classifier, we are more interested in the sentiment
    of texts than their semantic similarity. As illustrated in [Figure 7-3](#fig_3_similarity_can_be_expressed_as_more_than_just_sema),
    we can fine-tune the model such that documents are closer based on their sentiment
    than their semantic nature.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，嵌入模型可以出于多种目的进行训练。例如，在构建情感分类器时，我们对文本的情感比其语义相似性更感兴趣。如[图7-3](#fig_3_similarity_can_be_expressed_as_more_than_just_sema)所示，我们可以微调模型，使得文档根据其情感而不是语义特征更接近。
- en: Either way, an embedding model aims to learn what makes certain documents similar
    to one another and we can guide this process. By presenting the model with enough
    examples of semantically similar documents, we can steer towards semantics whereas
    using examples of sentiment would steer it in that direction.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，嵌入模型的目标是学习某些文档彼此相似的原因，我们可以引导这一过程。通过向模型提供足够多的语义相似文档示例，我们可以朝着语义的方向引导，而使用情感示例则会引导它朝那个方向。
- en: '![Similarity can be expressed as more than just semantically. An embedding
    model can be trained to focus on sentiment similarity  the idea that documents
    with similar sentiments are closer to each other in n dimensional space than documents
    with dissimilar sentiments. In this figure  negative reviews  red  are close to
    one another and dissimilar to positive reviews  green .](assets/creating_text_embedding_models_249519_03.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![相似性不仅可以通过语义来表达。可以训练一个嵌入模型，专注于情感相似性，即具有相似情感的文档在n维空间中彼此更接近，而与情感不同的文档则相距较远。在该图中，负面评价（红色）彼此接近，与正面评价（绿色）不同。](assets/creating_text_embedding_models_249519_03.png)'
- en: Figure 7-3\. Similarity can be expressed as more than just semantically. An
    embedding model can be trained to focus on sentiment similarity, the idea that
    documents with similar sentiments are closer to each other in n-dimensional space
    than documents with dissimilar sentiments. In this figure, negative reviews (red)
    are close to one another and dissimilar to positive reviews (green).
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. 相似性不仅仅可以通过语义来表达。可以训练一个嵌入模型，专注于情感相似性，即具有相似情感的文档在n维空间中彼此更接近，而与情感不同的文档则相距较远。在该图中，负面评价（红色）彼此接近，与正面评价（绿色）不同。
- en: There are many ways in which we can train, fine-tune, and guide embedding models
    but one of the strongest and widely-used techniques is called contrastive learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以训练、微调和引导嵌入模型的方式有很多，但最强大且广泛使用的技术之一称为对比学习。
- en: What is Contrastive Learning?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是对比学习？
- en: One major technique for both training and fine-tuning text embedding models
    is called contrastive learning. Contrastive learning is a technique that aims
    to train an embedding model such that similar documents are closer in vector space
    whilst dissimilar documents are further apart. We have seen this notion previously
    in Figures 13-X and Figure 13-X.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和微调文本嵌入模型的一项主要技术称为对比学习。对比学习是一种旨在训练嵌入模型的技术，使得相似文档在向量空间中更接近，而不同文档则更远离。我们在图13-X和图13-X中之前见过这个概念。
- en: The underlying idea of contrastive learning is that the best way to learn and
    model similarity/dissimilarity between documents is by feeding a model examples
    of similar and dissimilar pairs. In order to accurately capture the semantic nature
    of a document, it often needs to be contrasted with another document for a model
    to learn what makes it different or similar. This contrasting procedure is quite
    powerful and relates to the context in which documents are written. This high-level
    procedure is demonstrated in [Figure 7-4](#fig_4_contrastive_learning_aims_to_teach_an_embedding_mo).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习的基本思想是，学习和建模文档之间相似性/不相似性的最佳方法是向模型提供相似和不相似对的示例。为了准确捕捉文档的语义性质，通常需要将其与另一文档进行对比，以便模型学习什么使其不同或相似。这个对比过程非常强大，并与文档撰写的背景相关。这个高级过程在[图7-4](#fig_4_contrastive_learning_aims_to_teach_an_embedding_mo)中得到了展示。
- en: '![Contrastive learning aims to teach an embedding model whether documents are
    similar or dissimilar. Contrastive learning does so by presenting groups of documents
    to a model that are similar or dissimilar to a certain degree. ](assets/creating_text_embedding_models_249519_04.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![对比学习旨在教导嵌入模型文档是否相似或不相似。对比学习通过向模型展示一定程度上相似或不相似的文档组来实现。](assets/creating_text_embedding_models_249519_04.png)'
- en: Figure 7-4\. Contrastive learning aims to teach an embedding model whether documents
    are similar or dissimilar. Contrastive learning does so by presenting groups of
    documents to a model that are similar or dissimilar to a certain degree.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 对比学习旨在教导嵌入模型文档是否相似或不相似。对比学习通过向模型展示一定程度上相似或不相似的文档组来实现。
- en: 'Another way to look at contrastive learning is through the nature of explanations.
    A nice example of this is an anecdotal story of a reporter asking a robber “Why
    did you rob a bank”, to which he answers “Because that is where the money is.”^([1](ch07.html#id298))
    Although a factually correct answer, the intent of the question was not why he
    robs banks specifically but why he robs at all. This is called contrastive explanation
    and refers to understanding a particular case, “Why P” in contrast to alternatives,
    “Why P and not Q?“^([2](ch07.html#id299)) In the example, the question could be
    interpreted in a number of ways and may be best modeled by providing an alternative:
    “Why did you rob a bank (P) instead of obeying the law (Q)?”.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个看待对比学习的方法是通过解释的性质。一个很好的例子是一个记者问一个抢劫犯“你为什么抢银行”，他回答“因为那里有钱。”^([1](ch07.html#id298))
    尽管这个回答在事实上是正确的，但问题的意图并不是问他为什么特定地抢银行，而是问他为什么会抢劫。这被称为对比解释，指的是理解一个特定案例，“为什么P”与其他选择相比，“为什么P而不是Q？”^([2](ch07.html#id299))
    在这个例子中，问题可以有多种解读，可能最好的方式是提供一个替代选择：“你为什么抢银行（P）而不是遵守法律（Q）？”。
- en: The importance of alternatives to the understanding of a question also applies
    to how an embedding learns through contrastive learning. By showing a model similar
    and dissimilar pairs of documents, it starts to learn what makes something similar/dissimilar
    and more importantly, why.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 理解问题的替代方案的重要性同样适用于嵌入如何通过对比学习来学习。通过向模型展示相似和不相似的文档对，它开始学习什么使得事物相似/不相似，更重要的是，为什么。
- en: For example, you could learn a model to understand what a dog is by letting
    it find features such as “tail”, “nose”, “four legs”, etc. This learning process
    can be quite difficult since features are often not well-defined and can be interpreted
    in a number of ways. A being with a “ tail”, “nose”, and “four legs” can also
    be a cat. To help the model steer toward what we are interested in, we essentially
    ask it “Why is this a dog and not a cat?”. By providing the contrast between two
    concepts, it starts to learn the features that define the concept but also the
    features that are not related. We further illustrate this concept of contrastive
    explanation in [Figure 7-5](#fig_5_explanations_are_typically_grounded_by_the_contras).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以让一个模型通过寻找“尾巴”、“鼻子”、“四条腿”等特征来理解什么是狗。这个学习过程可能相当困难，因为特征往往没有明确的定义，并且可以有多种解读。一个具有“尾巴”、“鼻子”和“四条腿”的生物也可能是一只猫。为了帮助模型朝着我们感兴趣的方向发展，我们基本上问它“为什么这是狗而不是猫？”通过提供两个概念之间的对比，它开始学习定义概念的特征，以及与之无关的特征。我们在[图7-5](#fig_5_explanations_are_typically_grounded_by_the_contras)中进一步说明了这一对比解释的概念。
- en: '![Explanations are typically grounded by the contrast of other possibilities.
    As such  we get more information when we frame a question as a contrast. The same
    applies to an embedding model. When we feed it with different contrasts  degrees
    of similarity   it starts to learn what makes things different from one another
    and thereby the distinctive characteristics of concepts.](assets/creating_text_embedding_models_249519_05.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![解释通常通过其他可能性的对比来进行。因此，当我们将问题框架设为对比时，我们获得更多信息。嵌入模型也是如此。当我们提供不同的对比（相似度程度）时，它开始学习事物之间的差异，从而理解概念的独特特征。](assets/creating_text_embedding_models_249519_05.png)'
- en: Figure 7-5\. Explanations are typically grounded by the contrast of other possibilities.
    As such, we get more information when we frame a question as a contrast. The same
    applies to an embedding model. When we feed it with different contrasts (degrees
    of similarity), it starts to learn what makes things different from one another
    and thereby the distinctive characteristics of concepts.
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5\. 解释通常通过其他可能性的对比来进行。因此，当我们将问题框架设为对比时，我们获得更多信息。嵌入模型也是如此。当我们提供不同的对比（相似度程度）时，它开始学习事物之间的差异，从而理解概念的独特特征。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One of the earliest and most popular examples of contrastive learning in NLP
    is actually Word2Vec. The model learns word representations by training on individual
    words in a sentence. A word close to a target word in a sentence will be constructed
    as a positive pair whereas randomly sampled words constitute dissimilar pairs.
    In other words, positive examples of neighboring words are contrasted with randomly
    selected words that are not neighbors. Although not widely known, it is one of
    the first major breakthroughs in NLP that leverages contrastive learning with
    neural networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习在自然语言处理中的一个早期且最受欢迎的例子实际上是Word2Vec。该模型通过在句子中的单个词上进行训练来学习词表示。接近目标词的词将被构建为正样本，而随机抽样的词构成不相似的样本。换句话说，相邻词的正例与随机选择的非邻近词进行了对比。尽管不广为人知，但这是利用对比学习与神经网络相结合的首个重大突破之一。
- en: There are many ways we can apply contrastive learning to create text embedding
    models but the most well-known technique and framework is [sentence-transformers](https://github.com/UKPLab/sentence-transformers).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用对比学习创建文本嵌入模型的方式有很多，但最著名的技术和框架是[sentence-transformers](https://github.com/UKPLab/sentence-transformers)。
- en: SBERT
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SBERT
- en: Although there are many forms of contrastive learning, one framework that has
    popularized the technique within the Natural Language Processing community, is
    sentence-transformers. Its approach fixes a major problem with the original BERT
    implementation for creating sentence embeddings, namely its computational overhead.
    Before sentence-transformers, sentence embeddings were often used with BERT using
    an architectural structure called cross-encoders.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对比学习有很多形式，但在自然语言处理社区中流行化该技术的一个框架是sentence-transformers。它的做法解决了创建句子嵌入的原始BERT实现的一个主要问题，即计算开销。在sentence-transformers之前，句子嵌入通常与BERT一起使用，采用一种称为交叉编码器的架构结构。
- en: Cross-encoders allow two sentences to be passed to the transformer network simultaneously
    to predict the extent to which the two sentences are similar. It does so by adding
    a classification head to the original architecture that can output a similarity
    score. However, the number of computations rises quickly when you want to find
    the highest pair in a collection of 10,000 sentences. That would require n·(n−1)/2
    = 49 995 000 inference computations and therefore generates significant overhead.
    Moreover, a cross-encoder generally does not generate embeddings, as shown in
    [Figure 7-6](#fig_6_the_architecture_of_a_cross_encoder_both_sentence).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉编码器允许同时将两个句子传递给变换器网络，以预测这两个句子的相似程度。它通过在原始架构中添加一个分类头来实现，能够输出相似度分数。然而，当你想在10,000个句子的集合中找到最高配对时，计算数量迅速增加。这将需要
    n·(n−1)/2 = 49 995 000 次推理计算，从而产生显著的开销。此外，交叉编码器通常不生成嵌入，如[图7-6](#fig_6_the_architecture_of_a_cross_encoder_both_sentence)所示。
- en: A solution to this overhead is by generating embeddings from a BERT model by
    averaging its output layer or using the [CLS] token. This, however, has shown
    to be worse than simply averaging word vectors, like GloVe.^([3](ch07.html#id300))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个开销的方法是通过平均其输出层或使用[CLS]标记从BERT模型生成嵌入。然而，这显示出其效果不如简单地平均词向量，例如GloVe.^([3](ch07.html#id300))
- en: '![The architecture of a cross encoder. Both sentences are concatenated  separated
    with a  SEP  token  and fed to the model simultaneously. Instead of outputting
    embeddings  it outputs a similarity score between the input sentences. ](assets/creating_text_embedding_models_249519_06.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![交叉编码器的架构。两个句子被拼接，用 SEP 标记分隔，并同时输入模型。它输出的是输入句子之间的相似度分数，而不是嵌入。](assets/creating_text_embedding_models_249519_06.png)'
- en: Figure 7-6\. The architecture of a cross-encoder. Both sentences are concatenated,
    separated with a <SEP> token, and fed to the model simultaneously. Instead of
    outputting embeddings, it outputs a similarity score between the input sentences.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 交叉编码器的架构。两个句子被拼接，用 <SEP> 标记分隔，并同时输入模型。它输出的是输入句子之间的相似度分数，而不是嵌入。
- en: Instead, the authors of sentence-transformers approached the problem differently
    and searched for a method that is fast and creates embeddings that can be compared
    semantically. The result is an elegant alternative to the original cross-encoder
    architecture. Unlike a cross-encoder, in sentence-transformers, the classification
    head is dropped, and instead mean pooling is used on the final output layer to
    generate an embedding.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，sentence-transformers 的作者以不同的方式处理这个问题，寻找一种快速且能够进行语义比较的嵌入方法。结果是对原始交叉编码器架构的一种优雅替代方案。在
    sentence-transformers 中，与交叉编码器不同，分类头被省略，而是对最终输出层使用均值池化来生成嵌入。
- en: Sentence-transformers are trained using a siamese architecture. In this architecture,
    as visualized in [Figure 7-7](#fig_7_the_architecture_of_the_original_sentence_transfor),
    we have two identical BERT models that share the same weights and neural architecture.
    Since the weights are identical for both BERT models, we can use a single model
    and feed it the sentences one after the other.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Sentence-transformers 采用孪生架构进行训练。在这个架构中，如 [图 7-7](#fig_7_the_architecture_of_the_original_sentence_transfor)
    所示，我们有两个相同的 BERT 模型，它们共享相同的权重和神经架构。由于两个 BERT 模型的权重是相同的，我们可以使用单个模型，并依次输入句子。
- en: '![The architecture of the original sentence transformers model which leverages
    a siamese network  also called a bi encoder. BERT models with tied weights are
    fed the sentences from which embeddings are generated through the pooling of token
    embeddings. Then  models are optimized through the similarity of the sentence
    embeddings. ](assets/creating_text_embedding_models_249519_07.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![原始 sentence transformers 模型的架构，利用了一个叫做双编码器的孪生网络。共享权重的 BERT 模型接收句子，通过对标记嵌入的池化生成嵌入。然后，模型通过句子嵌入的相似性进行优化。](assets/creating_text_embedding_models_249519_07.png)'
- en: Figure 7-7\. The architecture of the original sentence-transformers model which
    leverages a siamese network, also called a bi-encoder. BERT models with tied weights
    are fed the sentences from which embeddings are generated through the pooling
    of token embeddings. Then, models are optimized through the similarity of the
    sentence embeddings.
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 原始 sentence-transformers 模型的架构，利用了一个叫做双编码器的孪生网络。共享权重的 BERT 模型接收句子，通过对标记嵌入的池化生成嵌入。然后，模型通过句子嵌入的相似性进行优化。
- en: The optimization process of these pairs of sentences is done through the loss
    functions which can have a major impact on the model’s performance. During training,
    the embeddings for each sentence are concatenated together with the difference
    between the embeddings. Then, this resulting embedding is optimized through a
    softmax classifier.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些句子对的优化过程通过损失函数完成，这对模型的性能有重大影响。在训练过程中，每个句子的嵌入与其差异被拼接在一起。然后，这个结果嵌入通过 softmax
    分类器进行优化。
- en: The resulting architecture is also referred to as a bi-encoder or SBERT for
    sentence-BERT. Although a bi-encoder is quite fast and creates accurate sentence
    representations, cross-encoders generally achieve better performance than a bi-encoder
    but do not generate embeddings.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果架构也被称为双编码器或 SBERT（句子-BERT）。尽管双编码器非常快速并生成准确的句子表示，但交叉编码器通常比双编码器实现更好的性能，但不生成嵌入。
- en: The bi-encoder, like a cross-encoder, leverages contrastive learning; by optimizing
    the (dis)similarity between pairs of sentences, the model will eventually learn
    the things that make the sentences what they are.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 双编码器与交叉编码器一样，利用对比学习；通过优化句子对之间的（不）相似性，模型最终会学习使句子成为其本身的特征。
- en: To perform contrastive learning, we need two things. First, we need data that
    constitute similar/dissimilar pairs. Second, we will need to define how the model
    defines and optimizes similarity.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行对比学习，我们需要两个东西。首先，我们需要构成相似/不相似对的数据。其次，我们需要定义模型如何定义和优化相似性。
- en: Creating an Embedding Model
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建嵌入模型
- en: There are many methods through which an embedding model can be created but generally,
    we look towards contrastive learning. This is an important aspect of many embedding
    models as the process allows it to efficiently learn semantic representations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 创建嵌入模型的方法有很多，但通常我们倾向于对比学习。这是许多嵌入模型的重要方面，因为这个过程使其能够高效地学习语义表示。
- en: However, this is not a free process. We will need to understand how to generate
    contrastive examples, how to train the model, and how to properly evaluate it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这不是一个免费的过程。我们需要了解如何生成对比示例，如何训练模型，以及如何正确评估它。
- en: Generating contrastive examples
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对比示例
- en: When pre-training your embedding model, you will often see data being used from
    Natural Language Inference (NLI) datasets. As we described in Chapter 2, NLI refers
    to the task of investigating whether, for a given premise, it entails the hypothesis
    (entailment), contradicts it (contradiction), or neither (neutral).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练嵌入模型时，你通常会看到使用自然语言推理（NLI）数据中的数据。正如我们在第二章中描述的，NLI是指调查给定前提是否蕴含假设（蕴含）、与之矛盾（矛盾）或两者皆不是（中立）的任务。
- en: For example, when the premise is “He is in the cinema watching *Coco*” and the
    hypothesis “He is watching *Frozen* at home”, then these statements are contradictions.
    In contrast, when the premise is “He is in the cinema watching *Coco*” and the
    hypothesis “In the movie theater he is watching the Disney movie *Coco*”, then
    these statements are considered entailment. This principle is illustrated in [Figure 7-8](#fig_8_we_can_leverage_the_structure_of_nli_datasets_to_g).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当前提是“他在电影院观看*寻梦环游记*”而假设是“他在家观看*冰雪奇缘*”时，这些陈述是矛盾的。相比之下，当前提是“他在电影院观看*寻梦环游记*”而假设是“在电影院里他正在观看迪士尼电影*寻梦环游记*”时，这些陈述被视为蕴含。这个原则在[图7-8](#fig_8_we_can_leverage_the_structure_of_nli_datasets_to_g)中得到了说明。
- en: '![We can leverage the structure of NLI datasets to generate negative examples  contradiction  and
    positive examples  entailments  for contrastive learning. ](assets/creating_text_embedding_models_249519_08.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![我们可以利用NLI数据集的结构生成负例（矛盾）和正例（蕴含）用于对比学习。](assets/creating_text_embedding_models_249519_08.png)'
- en: Figure 7-8\. We can leverage the structure of NLI datasets to generate negative
    examples (contradiction) and positive examples (entailments) for contrastive learning.
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8。我们可以利用NLI数据集的结构生成负例（矛盾）和正例（蕴含）用于对比学习。
- en: If you look closely at entailment and contradiction, then they describe the
    extent to which two inputs are similar to one another. As such, we can use NLI
    datasets to generate negative examples (contradictions) and positive examples
    (entailments) for contrastive learning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察蕴含和矛盾，那么它们描述的是两个输入之间的相似程度。因此，我们可以使用NLI数据集生成负例（矛盾）和正例（蕴含）用于对比学习。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Contrastive examples can also be generated if you have labeled data. In Chapter
    2, we used SetFit to perform few-shot classification using sentence-transformers.
    In SetFit, contrastive examples were generated by comparing sentences within classes
    (positive examples) and between classes (negative examples).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有标记的数据，也可以生成对比示例。在第二章中，我们使用SetFit通过句子变换器进行少量分类。在SetFit中，通过比较同类句子（正例）和不同类句子（负例）生成了对比示例。
- en: The data that we are going to be using throughout creating and fine-tuning embedding
    models are derived from the General Language Understanding Evaluation benchmark
    ([GLUE](https://gluebenchmark.com/)). This GLUE benchmark consists of nine language
    understanding tasks to evaluate and analyze model performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在创建和微调嵌入模型时将使用的数据来自通用语言理解评估基准（[GLUE](https://gluebenchmark.com/)）。这个GLUE基准包含九个语言理解任务，用于评估和分析模型性能。
- en: 'One of these tasks is the Multi-Genre Natural Language Inference (MNLI) corpus
    which is a collection of sentence pairs annotated with entailment (contradiction,
    neutral, entailment). We will be using this data to train our text embedding model.
    Let’s load the dataset using the `datasets` package:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务之一是多体裁自然语言推理（MNLI）语料库，它是一个包含带有蕴含关系（矛盾、中立、蕴含）的句子对的集合。我们将使用这些数据来训练我们的文本嵌入模型。让我们使用`datasets`包加载数据集：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we take a look at an example of entailment:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看一个蕴含的示例：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After having loaded the dataset, we will need to process it in such a way that
    it can be read with sentence-transformers:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，我们需要以某种方式处理它，以便可以用sentence-transformers读取：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Train model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now that we have our data loader with training examples, we will need to create
    our embedding model. We typically choose an existing sentence-transformer model
    and fine-tune that model but in this example, we are going to train an embedding
    from scratch.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了包含训练示例的数据加载器，我们需要创建我们的嵌入模型。我们通常选择一个现有的sentence-transformer模型并进行微调，但在这个例子中，我们将从头开始训练一个嵌入模型。
- en: This means that we will have to define two things. First, a pre-trained transformer
    model that serves as embedding individual words. As we have seen in Chapter 2,
    the `“bert-base-uncased”` model is often used for tutorials. However, many others
    exist that also have been evaluated using [sentence-transformers](https://www.sbert.net/docs/training/overview.html#best-transformer-model).
    Most notably, `“microsoft/mpnet-base”` often gives good results when used as word
    embedding models. Second, we will need to define the pooling strategy. Averaging
    the word embeddings is typically used throughout most embedding models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要定义两个东西。首先，一个预训练的变换器模型，用于嵌入单个词。正如我们在第二章中看到的，`“bert-base-uncased”`模型通常用于教程。然而，还有许多其他模型也经过[句子变换器](https://www.sbert.net/docs/training/overview.html#best-transformer-model)评估。特别是，`“microsoft/mpnet-base”`通常在用作词嵌入模型时表现良好。其次，我们需要定义池化策略。对词嵌入进行平均通常是在大多数嵌入模型中使用。
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: By default, all layers of an LLM in sentence-transformers are trainable. Although
    it is possible to freeze certain layers, it is generally not advised since the
    performance is often better when unfreezing all layers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，sentence-transformers中的LLM的所有层都是可训练的。虽然可以冻结某些层，但通常不建议这样做，因为解冻所有层时性能通常更好。
- en: 'Next, we will need to define a loss function over which we will optimize the
    model. As mentioned at the beginning of this section, one of the first instances
    of sentence-transformers uses soft-max loss. For illustrative purposes, we are
    going to be using that for now but we will go into more performant losses later
    on:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义一个损失函数，以便优化模型。正如本节开头提到的，sentence-transformers的第一个实例使用soft-max损失。为了说明，我们将暂时使用它，但稍后我们将讨论更高效的损失：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have defined our data, embedding model, and loss we can start training
    our model. We can do that using the `fit` function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义了数据、嵌入模型和损失，可以开始训练我们的模型。我们可以使用`fit`函数进行训练：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We train our model for a single epoch which takes roughly an hour or so on a
    V100 GPU. And… that’s it! We have now trained our own embedding model from scratch.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型进行单个周期的训练，大约需要一个小时左右，在V100 GPU上。就这样！我们现在从头开始训练了自己的嵌入模型。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The sentence-transformers framework allows for multi-task learning. The `train_objectives`
    parameter accepts a list of tuples which makes it possible to give it different
    datasets each with their own objective to optimize for. This means that we could
    give it the entire GLUE benchmark to train on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: sentence-transformers框架允许多任务学习。`train_objectives`参数接受一个元组列表，这使得可以为其提供不同的数据集，每个数据集都有自己的优化目标。这意味着我们可以将整个GLUE基准提供给它进行训练。
- en: We can perform a quick evaluation of the performance of our model. There are
    many tasks for doing, which we will go in-depth later, but a good one to start
    with is the Semantic Textual Similarity Benchmark (STSB) that is found in the
    GLUE dataset as we have seen before.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速评估模型的性能。可以进行很多任务，稍后我们将深入讨论，但一个好的起点是语义文本相似性基准（STSB），它在GLUE数据集中，如之前所见。
- en: It is a collection of sentence pairs labeled, through human annotation, with
    similarity scores between 1 and 5.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个通过人工标注，带有相似性评分（1到5）的句子对集合。
- en: 'We can leverage this dataset to see how well our model scores on a semantic
    similarity task. First, we will need to process the STSB dataset:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这个数据集来观察我们的模型在语义相似性任务上的评分表现。首先，我们需要处理STSB数据集：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can use these samples to generate an evaluator using sentence-transformers:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些样本生成一个使用句子转换器的评估器：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This evaluator allows us to evaluate any model, so let’s compare our trained
    model with its untrained variant:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个评估器允许我们评估任何模型，因此我们来比较一下训练过的模型与其未训练的变体：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This training procedure improved the baseline score from 0.61 to 0.74!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练过程将基线分数从0.61提高到了0.74！
- en: In-depth Evaluation
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入评估
- en: A good embedding model is more than just a good score on the STSB benchmark!
    As we have seen before, the GLUE benchmark has a number of tasks for which we
    can evaluate our embedding model. However, there exist many more benchmarks that
    allow for the evaluation of embedding models. To unify this evaluation procedure,
    the Massive Text Embedding Benchmark (MTEB)^([4](ch07.html#id301)) was developed.
    This MTEB spans 8 embedding tasks that cover 58 datasets and 112 languages.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的嵌入模型不仅仅是STSB基准测试上的好成绩！正如我们之前看到的，GLUE基准测试有许多任务可以评估我们的嵌入模型。然而，还有许多其他基准可以评估嵌入模型。为了统一这个评估过程，开发了大规模文本嵌入基准（MTEB）^([4](ch07.html#id301))。这个MTEB涵盖了8个嵌入任务，涉及58个数据集和112种语言。
- en: To publicly compare the state-of-the-art embedding models, a [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    was created with the scores of each embedding model across all tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公开比较最新的嵌入模型，创建了一个[排行榜](https://huggingface.co/spaces/mteb/leaderboard)，列出了各个嵌入模型在所有任务中的得分。
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When we inspect the results, we can see a number of evaluation metrics for
    this task:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查结果时，可以看到这个任务的一些评估指标：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The great thing about this evaluation benchmark is not only the diversity of
    the tasks and languages but that even the evaluation time is saved. Although many
    embedding models exist, we typically want those that are both accurate and have
    low latency. The tasks for which embedding models are used, like semantic search,
    often benefit from and require to have fast inference.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个评估基准的伟大之处不仅在于任务和语言的多样性，还在于评估时间也得到了节省。虽然存在许多嵌入模型，但我们通常希望那些既准确又低延迟的模型。用于语义搜索等任务的嵌入模型，通常会受益于快速推理。
- en: Loss Functions
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: We trained [our model](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli)
    using SoftMaxLoss to illustrate how one of the first sentence-transformers models
    was trained. However, not only is there a large variety of loss functions to choose
    from, SoftMaxLoss is generally not advised as there are more performant losses.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SoftMaxLoss训练了[我们的模型](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli)，以说明如何训练第一个句子转换器模型之一。然而，虽然有很多损失函数可供选择，通常不建议使用SoftMaxLoss，因为还有其他性能更优的损失。
- en: 'Instead of going through every single loss function out there, there are two
    loss functions that are typically used and seem to perform generally well, namely:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不必逐一了解所有损失函数，通常有两个损失函数被广泛使用，并且表现良好，即：
- en: Cosine Similarity
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Multiple Negatives Ranking Loss
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多重负样本排名损失
- en: Note
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are many more loss functions to choose from than just those discussed
    here. For example, a loss like MarginMSE works great for training or fine-tuning
    a cross-encoder. There are a number of interesting [loss functions](https://www.sbert.net/docs/package_reference/losses.html)
    implemented in the sentence-transformers framework.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可供选择的损失函数远不止于此。例如，MarginMSE损失在训练或微调交叉编码器时表现出色。在句子转换器框架中实现了许多有趣的[损失函数](https://www.sbert.net/docs/package_reference/losses.html)。
- en: Cosine Similarity
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: The Cosine Similarity Loss is an intuitive and easy-to-use loss that works across
    many different use cases and datasets. However, this loss is typically used in
    semantic textual similarity tasks. In these tasks, a similarity score is assigned
    to the pairs of texts over which we optimize the model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度损失是一种直观且易于使用的损失，适用于许多不同的用例和数据集。然而，这种损失通常用于语义文本相似性任务。在这些任务中，文本对会被分配一个相似度评分，以优化模型。
- en: Instead of having strictly positive or negative pairs of sentences, we assume
    to have pairs of sentences that are similar or dissimilar to a certain degree.
    Typically, this value lies between 0 and 1 to indicate dissimilarity and similarity
    respectively ([Figure 7-9](#fig_9_the_cosine_similarity_loss_aims_to_minimize_the_co)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设有句子对，它们在某种程度上是相似或不相似，而不是严格的正或负句对。通常，这个值在0和1之间，以分别表示不相似和相似（[图7-9](#fig_9_the_cosine_similarity_loss_aims_to_minimize_the_co)）。
- en: '![The Cosine Similarity Loss aims to minimize the cosine distance between semantically
    similar sentences and to maximize the distance between semantically dissimilar
    sentences. ](assets/creating_text_embedding_models_249519_09.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![余弦相似度损失旨在最小化语义相似句子之间的余弦距离，并最大化语义不相似句子之间的距离。](assets/creating_text_embedding_models_249519_09.png)'
- en: Figure 7-9\. The Cosine Similarity Loss aims to minimize the cosine distance
    between semantically similar sentences and to maximize the distance between semantically
    dissimilar sentences.
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. 余弦相似度损失旨在最小化语义相似句子之间的余弦距离，并最大化语义不相似句子之间的距离。
- en: The Cosine Similarity Loss is straightforward–it calculates the cosine similarity
    between the two embeddings of the two texts and compares that to the labeled similarity
    score. The model will learn to recognize the degree of similarity between sentences.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度损失很简单——它计算两个文本的两个嵌入之间的余弦相似度，并将其与标记的相似度分数进行比较。模型将学习识别句子之间的相似度程度。
- en: 'A minimal example of training with Cosine Similarity Loss would be:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用余弦相似度损失的一个最小示例是：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We are using the STSB dataset for this example. As we have seen before, they
    are pairs of sentences with annotated similarity scores that lend themselves naturally
    to the Cosine Similarity Loss.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个例子中使用STSB数据集。如我们之前所见，它们是带有注释相似度分数的句子对，自然适用于余弦相似度损失。
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A score of 0.848 is a big improvement compared to the SoftMaxLoss example. However,
    since our training and evaluation are both on the same tasks in contrast to the
    SoftMaxLoss example, comparing them would not be fair.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 得分0.848相比于SoftMaxLoss示例有了很大改善。然而，由于我们的训练和评估都在与SoftMaxLoss示例相同的任务上，比较它们是不公平的。
- en: Multiple Negatives Ranking Loss
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多重负面排名损失
- en: Multiple Negatives Ranking (MNR^([5](ch07.html#id302))) loss, often referred
    to as InfoNCE^([6](ch07.html#id303)) or NTXentLoss,^([7](ch07.html#id304)) is
    a loss that, in principle, only uses positive pairs of sentences.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 多重负面排名（MNR^([5](ch07.html#id302)))损失，通常被称为InfoNCE^([6](ch07.html#id303))或NTXentLoss^([7](ch07.html#id304))，是一种原则上仅使用正句对的损失。
- en: For example, you might have pairs of question/answer, image/image caption, paper
    title/paper abstract, etc. The great thing about these pairs is that we can be
    confident they are hard positive pairs. In MNR Loss ([Figure 7-10](#fig_10_multiple_negatives_ranking_loss_aims_to_minimize_t)),
    negative pairs are constructed by mixing a positive pair with another positive
    pair. In the example of a paper title and abstract, you would generate a negative
    pair by combining the title of a paper with a completely different abstract. These
    negatives are called in-batch negatives.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能会有问题/答案、图像/图像标题、论文标题/论文摘要等的对。这些对的优点在于我们可以确信它们是难得的正对。在MNR损失中（[图7-10](#fig_10_multiple_negatives_ranking_loss_aims_to_minimize_t)），负对是通过将一个正对与另一个正对混合而构造的。在论文标题和摘要的例子中，你会通过将论文的标题与完全不同的摘要组合来生成负对。这些负对称为批内负对。
- en: '![Multiple Negatives Ranking Loss aims to minimize the distance between related
    pairs of text  such as questions and answers  and maximize the distance between
    unrelated pairs  such as questions and unrelated answers. ](assets/creating_text_embedding_models_249519_10.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![多重负面排名损失旨在最小化相关文本对之间的距离，例如问题和答案，并最大化无关文本对之间的距离。](assets/creating_text_embedding_models_249519_10.png)'
- en: Figure 7-10\. Multiple Negatives Ranking Loss aims to minimize the distance
    between related pairs of text, such as questions and answers, and maximize the
    distance between unrelated pairs, such as questions and unrelated answers.
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10\. 多重负面排名损失旨在最小化相关文本对之间的距离，例如问题和答案，并最大化无关文本对之间的距离，例如问题和无关答案。
- en: After having generated these positive and negative pairs, we calculate their
    embeddings and apply cosine similarity. These similarity scores are then used
    to answer the question, are these pairs negative or positive? In other words,
    it is treated as a classification task and we can use cross-entropy loss to optimize
    the model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成这些正负对之后，我们计算它们的嵌入并应用余弦相似度。这些相似度得分随后用于回答问题，即这些对是负的还是正的？换句话说，这被视为分类任务，我们可以使用交叉熵损失来优化模型。
- en: 'The following is a minimal example of training with MNR loss:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用MNR损失进行训练的一个最小示例：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that we used the same data as in our SoftMaxLoss example. Let’s compare
    the performance of this model with that of our previously trained models:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的数据与我们的SoftMaxLoss示例相同。让我们比较这个模型与我们之前训练的模型的性能：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Compared to our previously trained model with SoftMaxLoss (0.74), our model
    with MNR Loss (0.82) seems to be much more accurate!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前使用SoftMaxLoss（0.74）训练的模型相比，我们使用MNR损失（0.82）的模型似乎准确得多！
- en: Tip
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Larger batch sizes tend to be better with MNR Loss as a larger batch makes the
    task more difficult. The reason for this is that the model needs to find the best
    matching sentence from a larger set of potential pairs of sentences. You can adapt
    the code to try out different batch sizes and get a feeling of its effects.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的批量大小在MNR损失中通常效果更好，因为更大的批量使任务变得更加困难。原因在于模型需要从更大的一组潜在句子对中找到最佳匹配句子。您可以调整代码以尝试不同的批量大小，感受其效果。
- en: There is a downside to how we used this loss function. Since negatives are sampled
    from other question/answer pairs, these in-batch or “easy” negatives that we used
    could potentially be completely unrelated to the question. As a result, the embedding
    model’s task of then finding the right answer to a question becomes quite easy.
    Instead, we would like to have negatives that are very related to the question
    but not the right answer. These negatives are called hard negatives. Since this
    makes the task more difficult for the embedding model as it has to learn more
    nuanced representations, the embedding model’s performance generally improves
    quite a bit.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这种损失函数有一个缺点。由于负例是从其他问答对中抽样的，这些批内或“简单”负例可能与问题完全无关。因此，嵌入模型找到问题正确答案的任务变得相对简单。相反，我们希望有与问题非常相关但不是正确答案的负例。这些负例称为困难负例。由于这使得嵌入模型的任务更加困难，因为它必须学习更细致的表示，嵌入模型的性能通常会有显著提升。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In MNR Loss, cosine similarity is often used but could be replaced with the
    dot product instead. An advantage of using the dot product is that it tends to
    work better for longer texts as the dot product will increase. A disadvantage,
    however, is that it generally works less well for clustering tasks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNR损失中，通常使用余弦相似度，但也可以用点积来替代。使用点积的一个优点是它通常对较长文本效果更好，因为点积会增加。然而，缺点是它通常在聚类任务中表现较差。
- en: 'A good example of a hard negative is the following. Let’s assume we have the
    following question: “How many people live in Amsterdam?”. A related answer to
    this question would be: “Almost a million people live in Amsterdam”. To generate
    a good hard negative, we ideally want the answer to contain something about Amsterdam
    and the number of people living in this city. For example: “More than a million
    people live in Utrecht, which is more than Amsterdam.” This answer is unrelated
    to the question but very similar, so this would be a good hard negative. [Figure 7-11](#fig_11__negatives_png_an_easy_negative_is_typically_unre)
    illustrates the differences between easy and hard negatives.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的困难负例的例子如下。假设我们有这样一个问题：“阿姆斯特丹有多少人居住？”与这个问题相关的答案是：“阿姆斯特丹几乎有一百万人。”为了生成一个好的困难负例，我们理想上希望答案中包含一些关于阿姆斯特丹及其居住人数的信息。例如：“乌特勒支有超过一百万人居住，这比阿姆斯特丹还多。”这个答案与问题无关，但非常相似，因此这是一个好的困难负例。[图 7-11](#fig_11__negatives_png_an_easy_negative_is_typically_unre)展示了简单负例和困难负例之间的区别。
- en: '![ negatives.png  An easy negative is typically unrelated to both the question
    and answer. A semi hard negative has some similarities to the topic of the question
    and answer but is somewhat unrelated. A hard negative is very similar to the question
    but is generally the wrong answer. ](assets/creating_text_embedding_models_249519_11.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![ negatives.png  简单负例通常与问题和答案都无关。半困难负例与问题和答案的主题有一些相似性，但在某种程度上是无关的。困难负例与问题非常相似，但通常是错误的答案。](assets/creating_text_embedding_models_249519_11.png)'
- en: Figure 7-11\. [negatives.png] An easy negative is typically unrelated to both
    the question and answer. A semi-hard negative has some similarities to the topic
    of the question and answer but is somewhat unrelated. A hard negative is very
    similar to the question but is generally the wrong answer.
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-11\. [negatives.png] 一个简单的负样本通常与问题和答案都没有关系。一个半困难的负样本与问题和答案的主题有些相似，但又稍微不相关。一个困难的负样本与问题非常相似，但通常是错误的答案。
- en: Using hard negatives with this loss is rather straightforward, instead of passing
    two related texts to the `InputExample`, we pass three texts, two related texts,
    and the last text a hard-negative.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用困难负样本与此损失函数是相当简单的， вместо передачи двух связанных текстов в `InputExample`，我们传递三个文本，其中两个是相关文本，最后一个是困难负样本。
- en: Fine-tuning an Embedding Model
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调嵌入模型
- en: In the previous section, we went through the basics of training an embedding
    model from scratch and saw how we could leverage loss functions to further optimize
    its performance. This method, although quite powerful, requires creating an embedding
    model from scratch. This process can be quite costly and time-consuming.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们了解了从头开始训练嵌入模型的基础知识，并看到了如何利用损失函数进一步优化其性能。这种方法虽然相当强大，但需要从头开始创建嵌入模型。这个过程可能相当昂贵且耗时。
- en: Instead, sentence-transformers have a number of pre-trained embedding models
    that we can use as a base for fine-tuning. They are trained on large amounts of
    data and already generate great performance out-of-the-box.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，句子变换器有多个预训练的嵌入模型，我们可以将其用作微调的基础。它们是在大量数据上训练的，并且开箱即用时就能产生良好的性能。
- en: There are a number of ways to fine-tune your model, depending on the data availability
    and domain. We will go through a number of them and demonstrate the strength of
    leveraging pre-trained embedding models.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型有多种方法，具体取决于数据的可用性和领域。我们将逐一介绍其中的一些方法，并展示利用预训练嵌入模型的优势。
- en: Supervised
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有监督
- en: The most straightforward way to fine-tune an embedding model is to repeat the
    process of training our model as we did before, but replace the `'bert-base-uncased'`
    with a pre-trained sentence-transformers model. There are many to choose from
    but generally, the `'all-mpnet-base-v2'` ([https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html))
    performs well across many use cases (see [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 微调嵌入模型的最简单方法是重复之前训练模型的过程，但将 `'bert-base-uncased'` 替换为预训练的句子变换器模型。有许多可供选择，但通常
    `'all-mpnet-base-v2'` ([https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html))
    在许多用例中表现良好（见 [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)）。
- en: 'In practice, we would only need to run the following to fine-tune our model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们只需运行以下命令即可微调我们的模型：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we use the same data as we used to train our model in the Multiple Negatives
    Ranking Loss example. The code for fine-tuning your model is rather straightforward
    thanks to the incredibly well-built sentence-transformers package.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用与在多负样本排名损失示例中训练模型时相同的数据。由于句子变换器包构建得相当出色，因此微调模型的代码相对简单。
- en: Tip
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of using a pre-trained BERT model like `'``bert``-base-uncased'` or
    a possible out-of-domain model like `'all-mpnet-base-v2'`, you can also perform
    Masked Language Modeling on the pre-trained BERT model to first adapt it to your
    domain. Then, you can use this fine-tuned BERT model as the base for training
    your embedding model. This is a form of domain adaptation. You can find more about
    Masked Language Modeling in Chapter X.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与其使用预训练的 BERT 模型，如 `'``bert``-base-uncased'` 或可能是领域外模型如 `'all-mpnet-base-v2'`，你还可以对预训练的
    BERT 模型进行掩码语言建模，首先将其适应到你的领域。然后，可以使用这个微调后的 BERT 模型作为训练嵌入模型的基础。这是一种领域适应。关于掩码语言建模的更多信息，请参见第
    X 章。
- en: The main difficulty of fine-tuning/training your model is finding the right
    data. With these models, we not only want to have very large datasets, the data
    in itself needs to be of high quality. Developing positive pairs is generally
    straightforward but adding hard negative pairs significantly increases the difficulty
    of creating quality data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 微调/训练模型的主要困难在于找到合适的数据。对于这些模型，我们不仅希望拥有非常大的数据集，数据本身也需要高质量。开发正样本对通常比较简单，但添加困难负样本对则显著增加了创建高质量数据的难度。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Training on the data, as shown above, is a bit redundant since the model was
    already trained on a very similar NLI dataset. However, the procedure remains
    the same for fine-tuning it on your domain-specific data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，对数据进行训练有些多余，因为该模型已经在一个非常相似的NLI数据集上进行了训练。然而，对于特定领域的数据微调过程仍然是相同的。
- en: Augmented SBERT
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强型SBERT
- en: A disadvantage of training or fine-tuning these embedding models is that they
    often require substantial training data. Many of these models are trained with
    more than a billion sentence pairs. Extracting such a high number of sentence
    pairs for your use case is generally not possible as in many cases, there is only
    little data available.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 训练或微调这些嵌入模型的一个缺点是，它们通常需要大量的训练数据。这些模型中的许多在超过十亿对句子上进行训练。提取如此多的句子对以适应你的用例通常是不可行的，因为在许多情况下，只有很少的数据可用。
- en: Fortunately, there is a way to augment your data such that an embedding model
    can be fine-tuned when there is only a little labeled data available. This procedure
    is referred to as Augmented SBERT.^([8](ch07.html#id305))
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种方法可以增强你的数据，以便在只有少量标注数据可用时，可以对嵌入模型进行微调。这个过程称为增强型SBERT。^([8](ch07.html#id305))
- en: In this procedure, we aim to augment the small amount of labeled data such that
    they can be used for regular training. It makes use of the slow and more accurate
    cross-encoder architecture (BERT) to augment and label a larger set of input pairs.
    These newly labeled pairs are then used for fine-tuning a bi-encoder (SBERT).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们旨在增强少量标注数据，使其可以用于常规训练。它利用慢而更准确的交叉编码器架构（BERT）来增强和标注更大数量的输入对。这些新标注的对随后用于微调双编码器（SBERT）。
- en: 'As shown in [Figure 7-12](#fig_12_augmented_sbert_works_through_training_a_high_perf),
    Augmented SBERT involves the following steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图7-12](#fig_12_augmented_sbert_works_through_training_a_high_perf)所示，增强型SBERT涉及以下步骤：
- en: Fine-tune a cross-encoder (BERT) using a small annotated dataset (gold dataset)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用小型标注数据集（黄金数据集）对交叉编码器（BERT）进行微调
- en: Create new sentence pairs
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建新的句子对
- en: Label new sentence pairs with the fine-tuned cross-encoder (silver dataset)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用微调后的交叉编码器标注新的句子对（银色数据集）
- en: Train a bi-encoder (SBERT) on the extended dataset (gold + silver dataset)
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在扩展数据集（黄金+银色数据集）上训练双编码器（SBERT）
- en: Here, a gold dataset is a small but fully annotated dataset that holds the ground
    truth. A silver dataset is also fully annotated but is not necessarily the ground
    truth as it was generated through predictions of the cross-encoder.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此处，黄金数据集是一个小型但完全标注的数据集，包含真实的基础真相。银色数据集也完全标注，但不一定是基础真相，因为它是通过交叉编码器的预测生成的。
- en: '![Augmented SBERT works through training a high performant cross encoder on
    a small gold dataset. Then  the trained cross encoder can be used to label an
    unlabeled dataset to generate the silver dataset which is much bigger than the
    gold dataset. Finally  both the gold and silver datasets are used to train the
    bi encoder.](assets/creating_text_embedding_models_249519_12.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![增强型SBERT通过在小型黄金数据集上训练高性能交叉编码器来工作。然后，训练好的交叉编码器可以用来标注未标注的数据集，以生成比黄金数据集大得多的银色数据集。最后，黄金和银色数据集都用于训练双编码器。](assets/creating_text_embedding_models_249519_12.png)'
- en: Figure 7-12\. Augmented SBERT works through training a high-performant cross-encoder
    on a small gold dataset. Then, the trained cross-encoder can be used to label
    an unlabeled dataset to generate the silver dataset which is much bigger than
    the gold dataset. Finally, both the gold and silver datasets are used to train
    the bi-encoder.
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12\. 增强型SBERT通过在小型黄金数据集上训练高性能的交叉编码器来工作。然后，训练好的交叉编码器可以用来标注未标注的数据集，以生成比黄金数据集大得多的银色数据集。最后，黄金和银色数据集都用于训练双编码器。
- en: 'Before we get into the steps above, let us first prepare the data:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入上述步骤之前，首先准备数据：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We use the train split of the STSB corpus as our gold dataset and use it to
    train our cross-encoder (step 1):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用STSB语料库的训练集作为我们的黄金数据集，并用它来训练我们的交叉编码器（步骤1）：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After having trained our cross-encoder, we can generate new candidate sentence
    pairs by simply random sampling 10 sentences for each input sentence (step 2):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练好我们的交叉编码器后，我们可以通过对每个输入句子随机抽取10个句子，生成新的候选句子对（步骤2）：
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Tip
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of randomly sampling the silver sentence pairs, we can also use pre-trained
    sentence-transformers. By retrieving the top-k sentences from the dataset using
    semantic search, the silver sentence pairs that we create tend to be more accurate.
    Although the sentence pairs are still chosen based on an approximation it is much
    better than random sampling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用预训练的句子转换器，而不是随机抽样银句子对。通过使用语义搜索从数据集中检索前k个句子，我们创建的银句子对往往更准确。虽然句子对仍然是基于近似选择的，但比随机抽样要好得多。
- en: 'The cross-encoder that we trained in step 1 can then be used to label the candidate
    sentence pairs generated in the previous step to build up the silver dataset:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一步训练的交叉编码器可以用于给之前生成的候选句子对标记，以建立银数据集：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have a silver and gold dataset, we simply combine them and train
    our embedding model as we did before:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了银和金数据集，我们只需将它们结合起来，像之前一样训练我们的嵌入模型：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can run the above again with only the gold dataset and see how adding this
    silver dataset influences the performance. Training on only the gold dataset results
    in a performance of 0.804 whereas adding the silver dataset ups the performance
    to 0.830!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次仅使用金数据集进行上述操作，看看添加这个银数据集如何影响性能。仅在金数据集上的训练性能为0.804，而添加银数据集后性能提升至0.830！
- en: This method allows for increasing the datasets that you already have available
    without the need to manually label hundreds of thousands of sentence pairs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许在不需要手动标记数十万句子对的情况下增加您现有的数据集。
- en: Unsupervised Learning
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: To create an embedding model, we typically need labeled data. However, not all
    real-world datasets come with a nice set of labels that we can use. We would instead
    look for techniques to train the model without any pre-determined labels–unsupervised
    learning.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建嵌入模型，通常需要标记数据。然而，并不是所有现实世界的数据集都带有我们可以使用的完整标签。我们会寻找无需预先确定标签的技术进行模型训练——无监督学习。
- en: Unsupervised techniques for creating or fine-tuning an embedding model generally
    perform worse than their supervised alter-egos. Many approaches exist, like Simple
    Contrastive Learning of Sentence Embeddings (SimCSE)^([9](ch07.html#id306)), Contrastive
    Tension (CT)^([10](ch07.html#id307)), Tranformer-based Denoising AutoEncoder (TSDAE)^([11](ch07.html#id308)),
    and Generative Pseudo-Labeling (GPL)^([12](ch07.html#id309)) .
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 创建或微调嵌入模型的无监督技术通常表现不如其监督版本。存在许多方法，如简单对比学习句子嵌入（SimCSE）^([9](ch07.html#id306))、对比张力（CT）^([10](ch07.html#id307))、基于变换器的去噪自编码器（TSDAE）^([11](ch07.html#id308))以及生成伪标记（GPL）^([12](ch07.html#id309))。
- en: We will be going through two methods, TSDAE and GPL which we can even combine
    later on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论两种方法，TSDAE和GPL，甚至可以在后续进行组合。
- en: Transformer-based Denoising AutoEncoder
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于变换器的去噪自编码器
- en: TSDAE is a very elegant approach to creating an embedding model with unsupervised
    learning. The method assumes that we have no labeled data at all and does not
    require us to artificially create labels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TSDAE是一种非常优雅的方法，用于创建无监督学习的嵌入模型。该方法假设我们完全没有标记数据，也不需要人为创建标签。
- en: The underlying idea of TSDAE is that we add noise to the input sentence by removing
    a certain percentage of words from it. This “damaged” sentence is put through
    an encoder, with a pooling layer on top of it, to map it to a sentence embedding.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: TSDAE的基本思路是通过从输入句子中去除一定比例的词来添加噪声。这句“受损”的句子经过编码器处理，并在其上方加上一个池化层，以将其映射到句子嵌入。
- en: From this sentence embedding, a decoder tries to reconstruct the original sentence
    from the “damaged” sentence but without the artificial noise.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个句子嵌入中，解码器尝试从“受损”句子中重建原始句子，但不包含人为噪声。
- en: This method is very similar to Masked Language Modeling, where we try to reconstruct
    and learn certain masked words. Here, instead of reconstructing masked words,
    we try to reconstruct the entire sentence.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与掩蔽语言建模非常相似，我们试图重建和学习某些被掩蔽的词。这里，我们试图重建的是整个句子，而不是掩蔽的词。
- en: After training, we can use the encoder to generate embeddings from text since
    the decoder is only used for judging whether the embeddings can accurately reconstruct
    the original sentence ([Figure 7-13](#fig_13_tsdae_randomly_removes_words_from_an_input_sentenc)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以使用编码器从文本中生成嵌入，因为解码器仅用于判断嵌入是否能准确重建原始句子（[图7-13](#fig_13_tsdae_randomly_removes_words_from_an_input_sentenc)）。
- en: '![TSDAE randomly removes words from an input sentence which is passed through
    an encoder to generate a sentence embedding. From this sentence embedding  the
    original sentence is reconstructed. The main idea here is that the more accurate
    the sentence embedding is  the more accurate the reconstructed sentence will be.](assets/creating_text_embedding_models_249519_13.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![TSDAE随机删除输入句子中的单词，这些句子通过编码器传递以生成句子嵌入。通过这个句子嵌入，重建原始句子。这里的主要思想是，句子嵌入越准确，重建的句子也会越准确。](assets/creating_text_embedding_models_249519_13.png)'
- en: Figure 7-13\. TSDAE randomly removes words from an input sentence which is passed
    through an encoder to generate a sentence embedding. From this sentence embedding,
    the original sentence is reconstructed. The main idea here is that the more accurate
    the sentence embedding is, the more accurate the reconstructed sentence will be.
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-13\. TSDAE随机删除输入句子中的单词，这些句子通过编码器传递以生成句子嵌入。通过这个句子嵌入，重建原始句子。这里的主要思想是，句子嵌入越准确，重建的句子也会越准确。
- en: 'Since we only need a bunch of sentences without any labels, training this model
    is straightforward. We start by defining our model as we did before:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只需要一堆没有标签的句子，因此训练这个模型非常简单。我们开始定义模型，方式与之前相同：
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we are using CLS pooling instead of mean pooling. The authors found
    that there was little difference between them and since mean pooling loses positional
    information, they choose CLS pooling.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是CLS池化而不是平均池化。作者发现二者之间几乎没有区别，并且由于平均池化会丢失位置信息，因此选择了CLS池化。
- en: 'Next, we create a bunch of sentences to be used for our model using the STSB
    dataset that we used before:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前的STSB数据集创建一堆句子来用于我们的模型：
- en: '[PRE22]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The main difference between this and how we performed supervised modeling before
    is that we pass our data to the `DenoisingAutoEncoderDataset`. This will generate
    noise to the input during training. We also defined our encoder loss using the
    same base model as we used for our embedding model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们之前进行监督建模的主要区别在于，我们将数据传递给`DenoisingAutoEncoderDataset`。这将在训练期间为输入生成噪声。我们还使用与嵌入模型相同的基础模型定义了编码器损失。
- en: 'Lastly, we only need to fit and evaluate our model:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需拟合和评估我们的模型：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: After fitting our model, we get a score of 0.74 which is quite impressive considering
    we did all this training with unlabeled data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合我们的模型后，我们得到了0.74的分数，这在考虑到我们使用无标签数据进行所有训练的情况下相当令人印象深刻。
- en: Domain Adaptation
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域适应
- en: When you have very little or no labeled data available, you typically use unsupervised
    learning to create your text embedding model. However, unsupervised techniques
    are generally outperformed by supervised techniques and have difficulty learning
    domain-specific concepts.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当可用的标记数据非常少或没有时，通常使用无监督学习来创建文本嵌入模型。然而，无监督技术通常不如监督技术表现良好，并且在学习特定领域的概念时存在困难。
- en: This is where *domain adaptation* comes in. Its goal is to update existing embedding
    models to a specific textual domain that contains different subjects from the
    source domain. [Figure 7-14](#fig_14_in_domain_adaptation_the_aim_is_to_create_and_gen)
    demonstrates how domains can differ in content.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*领域适应*的作用所在。它的目标是更新现有的嵌入模型，以适应包含与源领域不同主题的特定文本领域。[图 7-14](#fig_14_in_domain_adaptation_the_aim_is_to_create_and_gen)展示了不同领域内容的差异。
- en: '![In domain adaptation  the aim is to create and generalize an embedding model
    from one domain to another. The target domain  or out domain  generally contains
    words and subjects that were not found in the source domain or in domain. ](assets/creating_text_embedding_models_249519_14.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![在领域适应中，目标是从一个领域创建并泛化一个嵌入模型到另一个领域。目标领域或外部领域通常包含在源领域或内部领域中找不到的单词和主题。](assets/creating_text_embedding_models_249519_14.png)'
- en: Figure 7-14\. In domain adaptation, the aim is to create and generalize an embedding
    model from one domain to another. The target domain, or out-domain, generally
    contains words and subjects that were not found in the source domain or in-domain.
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-14\. 在领域适应中，目标是从一个领域创建并泛化一个嵌入模型到另一个领域。目标领域或外部领域通常包含在源领域或内部领域中找不到的单词和主题。
- en: One method for domain adaptation is called *adaptive pre-training*. You start
    by pre-training your domain-specific corpus using an unsupervised technique, such
    as the previously discussed TSDAE or even Masked Language Modeling. Then, as illustrated
    in [Figure 7-15](#fig_15_domain_adaptation_can_be_performed_with_adaptive_p),
    you fine-tune that model using a training dataset in your target domain.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应的一种方法称为*自适应预训练*。你首先使用无监督技术（例如之前讨论的TSDAE或掩蔽语言模型）对领域特定语料库进行预训练。然后，如[图7-15](#fig_15_domain_adaptation_can_be_performed_with_adaptive_p)所示，你使用目标领域的训练数据集微调该模型。
- en: This procedure leverages a pipeline that we have seen before, first using TSDAE
    to fine-tune an LLM or existing embedding model which is further fine-tuned using
    supervised or even augmented SBERT training.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程利用了我们之前看到的管道，首先使用TSDAE微调LLM或现有嵌入模型，然后使用监督或增强的SBERT训练进一步微调。
- en: This, however, can be computationally expensive since we must first pre-train
    our data on a large corpus and then use supervised learning with a labeled dataset.
    Typically, the labeled datasets need to be large and can require millions of training
    pairs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能在计算上是昂贵的，因为我们必须首先在大型语料库上对数据进行预训练，然后使用带标记的数据集进行监督学习。通常，带标记的数据集需要很大，并可能需要数百万对训练样本。
- en: '![Domain adaptation can be performed with adaptive pre training and adaptive
    fine tuning. With adaptive pre training  we first apply unsupervised tuning of
    an LLM on the target domain after which we fine tune that model on labeled non
    domain specific data. With adaptive fine tuning  we take any existing pre training
    embedding model and fine tune it using unlabeled domain specific data.](assets/creating_text_embedding_models_249519_15.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![领域适应可以通过自适应预训练和自适应微调来实现。通过自适应预训练，我们首先对目标领域进行无监督的LLM调优，然后在标记的非领域特定数据上微调该模型。通过自适应微调，我们可以使用任何现有的预训练嵌入模型，并使用未标记的领域特定数据进行微调。](assets/creating_text_embedding_models_249519_15.png)'
- en: Figure 7-15\. Domain adaptation can be performed with adaptive pre-training
    and adaptive fine-tuning. With adaptive pre-training, we first apply unsupervised
    tuning of an LLM on the target domain after which we fine-tune that model on labeled
    non-domain specific data. With adaptive fine-tuning, we take any existing pre-training
    embedding model and fine-tune it using unlabeled domain-specific data.
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-15\. 领域适应可以通过自适应预训练和自适应微调来实现。通过自适应预训练，我们首先对目标领域进行无监督的LLM调优，然后在标记的非领域特定数据上微调该模型。通过自适应微调，我们可以使用任何现有的预训练嵌入模型，并使用未标记的领域特定数据进行微调。
- en: Instead, we are going to be using a method that can be run on top of a pre-trained
    embedding model instead, namely *Generative Pseudo-Labeling*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将使用一种可以在预训练嵌入模型之上运行的方法，即*生成伪标注*。
- en: Generative Pseudo-Labeling
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成伪标注
- en: Generative Pseudo-Labeling assumes that although we have data, none of it is
    labeled. It consists of three steps for generating labeled data that we can use
    for training an embedding model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 生成伪标注假设尽管我们有数据，但这些数据都没有标记。它包括三个步骤，以生成我们可以用于训练嵌入模型的标记数据。
- en: First, for each unlabeled text that you have in your domain-specific data, we
    use a generative model, like T5, to generate a number of queries. These queries
    are generally questions that can be answered with part(s) of the input text. For
    example, when your text is “Coco is a movie produced by Pixar”, the model might
    generate a query like “Who produced the movie Coco?”. These are the positive examples
    that we generate as we illustrate in [Figure 7-16](#fig_16_in_the_first_step_of_gpl_s_pipeline_queries_are_g).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于你在特定领域数据中拥有的每一条未标记文本，我们使用生成模型，如T5，生成若干查询。这些查询通常是可以用输入文本的部分回答的问题。例如，当你的文本是“《可可夜总会》是由皮克斯制作的”时，模型可能会生成类似“谁制作了电影《可可夜总会》？”的查询。这些是我们生成的正例，如[图7-16](#fig_16_in_the_first_step_of_gpl_s_pipeline_queries_are_g)所示。
- en: '![In the first step of GPL s pipeline  queries are generated with a generative
    model  like T5  for each unlabeled input text. These queries can be used as sentence
    pairs for labeling later on in the GPL pipeline.](assets/creating_text_embedding_models_249519_16.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![在GPL管道的第一步中，针对每条未标记输入文本，使用生成模型（如T5）生成查询。这些查询可以作为后续在GPL管道中标记的句子对。](assets/creating_text_embedding_models_249519_16.png)'
- en: Figure 7-16\. In the first step of GPL’s pipeline, queries are generated with
    a generative model, like T5, for each unlabeled input text. These queries can
    be used as sentence pairs for labeling later on in the GPL pipeline.
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-16\. 在GPL管道的第一步中，使用生成模型（如T5）为每个未标记的输入文本生成查询。这些查询可以用作后续GPL管道中的句子对进行标记。
- en: Second, we will also need to have negative examples for our model to learn.
    Preferably, a negative example is related to the query but is not the relevant
    answer. For example, a negative example for the query “Who produced the movie
    *Coco*?” would be “*The Lion King* was produced by Disney”.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们还需要负样本，以便模型进行学习。理想情况下，负样本与查询相关，但不是相关答案。例如，查询“谁制作了电影*可可*？”的负样本可能是“*狮子王*是迪士尼制作的”。
- en: To extract these negative examples, we use a pre-training embedding model to
    retrieve all texts that are relevant to the query. In [Figure 7-17](#fig_17_in_the_second_step_of_gpl_s_pipeline_negatives_ar),
    this second step shows how relevant texts are mined.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取这些负样本，我们使用预训练的嵌入模型来检索与查询相关的所有文本。在[图7-17](#fig_17_in_the_second_step_of_gpl_s_pipeline_negatives_ar)中，第二步展示了如何挖掘相关文本。
- en: '![In the second step of GPL s pipeline  negatives are mined using a pre trained
    embedding model. This model generates embeddings for the input query and the corpus
    and finds those that relate to the input query. ](assets/creating_text_embedding_models_249519_17.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![在GPL管道的第二步中，使用预训练的嵌入模型挖掘负样本。该模型为输入查询和语料库生成嵌入，并找到与输入查询相关的样本。](assets/creating_text_embedding_models_249519_17.png)'
- en: Figure 7-17\. In the second step of GPL’s pipeline, negatives are mined using
    a pre-trained embedding model. This model generates embeddings for the input query
    and the corpus and finds those that relate to the input query.
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-17\. 在GPL管道的第二步中，使用预训练的嵌入模型挖掘负样本。该模型为输入查询和语料库生成嵌入，并找到与输入查询相关的样本。
- en: Third, after we generate the negative examples, we will need to score them.
    Some of the negative examples might end up being the actual answer or they might
    not be relevant, both of which we want to prevent. As demonstrated in [Figure 7-18](#fig_18_in_the_third_step_of_gpl_s_pipeline_we_use_a_cros),
    we can use a pre-trained cross-encoder to score the query/passage pairs that we
    have created through the previous two steps.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，在生成负样本后，我们需要对其进行评分。一些负样本可能最终成为实际答案，或者可能与之不相关，而这两者都是我们希望避免的。正如[图7-18](#fig_18_in_the_third_step_of_gpl_s_pipeline_we_use_a_cros)所示，我们可以使用预训练的交叉编码器对通过前两个步骤创建的查询/段落对进行评分。
- en: '![In the third step of GPL s pipeline  we use a cross encoder to score all
    query passage pairs. The goal of this procedure is to filter out any false negatives.
    Some negatives might accidentally turn out to be positives. Instead of a negative
    vs. positive labeling procedure  using a scoring procedure allows for more nuance
    in the representations.](assets/creating_text_embedding_models_249519_18.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![在GPL管道的第三步中，我们使用交叉编码器对所有查询-段落对进行评分。此过程的目标是过滤掉任何假阴性。一些负样本可能会意外变成正样本。与负样本和正样本的标记过程相比，使用评分过程可以在表示中提供更多的细微差别。](assets/creating_text_embedding_models_249519_18.png)'
- en: Figure 7-18\. In the third step of GPL’s pipeline, we use a cross-encoder to
    score all query/passage pairs. The goal of this procedure is to filter out any
    false negatives. Some negatives might accidentally turn out to be positives. Instead
    of a negative vs. positive labeling procedure, using a scoring procedure allows
    for more nuance in the representations.
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-18\. 在GPL管道的第三步中，我们使用交叉编码器对所有查询/段落对进行评分。此过程的目标是过滤掉任何假阴性。一些负样本可能会意外变成正样本。与负样本和正样本的标记过程相比，使用评分过程可以在表示中提供更多的细微差别。
- en: After this final step, for each query, we now have a positive example (step
    1) and a mined negative (step 2). In other words, we have triplets that we can
    use for our training procedure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步后，对于每个查询，我们现在有一个正样本（步骤1）和一个挖掘出的负样本（步骤2）。换句话说，我们有可以用于训练过程的三元组。
- en: Running each step individually is quite a hassle but fortunately, there is a
    GPL package that abstracts that difficulty away. However, we first need to format
    our data such that the package can read it.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 单独运行每一步确实很麻烦，但幸运的是，有一个GPL包可以抽象掉这种困难。然而，我们首先需要格式化我们的数据，以便该包能够读取。
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We are again using the STSB dataset and we extract just a small portion of the
    data for training. The reason for doing so is that training can still be quite
    expensive. The authors of GPL had to train for roughly a day on a V100 GPU, so
    trying out with a smaller dataset is advised before scaling up.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用STSB数据集，仅提取少量数据进行训练。这样做的原因是训练仍然可能相当昂贵。GPL的作者在V100 GPU上训练了大约一天，因此建议在扩大规模之前先尝试较小的数据集。
- en: 'Next, we can import GPL and only need to run `train` to start:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以导入GPL，只需运行 `train` 即可开始：
- en: '[PRE25]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: There are a few sub-models worth noting that describe the steps as we have seen
    them before
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些值得注意的子模型，描述了我们之前所看到的步骤。
- en: '`generator` refers to *step 1* where we generate queries for our data'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` 指的是 *步骤 1*，在此我们为我们的数据生成查询。'
- en: '`retriever` refers to *step 2* where we generate queries for our data'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retriever` 指的是 *步骤 2*，在此我们为我们的数据生成查询。'
- en: '`cross_encoder` refers to *step 3* where we generate queries for our data'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_encoder` 指的是 *步骤 3*，在此我们为我们的数据生成查询。'
- en: '`base_ckpt` refers to the final step of training our embedding model'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base_ckpt` 指的是训练我们嵌入模型的最后一步。'
- en: 'After training, we can load and evaluate the model as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以按如下方式加载和评估模型：
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With GPL, we managed to get a score of 0.82 without any labeled data at all!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 借助GPL，我们在没有任何标注数据的情况下取得了0.82的分数！
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have taken a look at creating and fine-tuning embedding
    models through contrastive learning, one of the most important components of training
    such models. Through both unsupervised and supervised techniques, we were able
    to create embedding models tuned to our datasets.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了通过对比学习创建和微调嵌入模型，这是训练此类模型最重要的组成部分之一。通过无监督和有监督技术，我们能够创建针对我们数据集调优的嵌入模型。
- en: '^([1](ch07.html#id298-marker)) Alan Garfinkel. “Forms of Explanation: Rethinking
    the Questions in Social Theory.” (1981).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#id298-marker)) Alan Garfinkel. “解释的形式：重新思考社会理论中的问题。” (1981)。
- en: '^([2](ch07.html#id299-marker)) Tim Miller. “Contrastive explanation: A structural-model
    approach”.*The Knowledge Engineering Review* 36 (2021): e14.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch07.html#id299-marker)) Tim Miller. “对比解释：结构模型方法”。*知识工程评论* 36 (2021):
    e14。'
- en: '^([3](ch07.html#id300-marker)) Jeffrey Pennington, Richard, Socher, and Christopher
    D, Manning. “Glove: Global vectors for word representation.” In *Proceedings of
    the 2014 conference on empirical methods in natural language processing (EMNLP)* (pp.
    1532–1543). 2014.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch07.html#id300-marker)) Jeffrey Pennington, Richard, Socher, 和 Christopher
    D, Manning. “Glove: 全局词向量表示。” 收录于 *2014年自然语言处理实证方法会议论文集 (EMNLP)* (pp. 1532–1543).
    2014。'
- en: '^([4](ch07.html#id301-marker)) Muennighoff, Niklas, Nouamane, Tazi, Loïc, Magne,
    and Nils, Reimers. “MTEB: Massive text embedding benchmark”.*arXiv preprint arXiv:2210.07316* (2022).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch07.html#id301-marker)) Muennighoff, Niklas, Nouamane, Tazi, Loïc, Magne,
    和 Nils, Reimers. “MTEB: 大规模文本嵌入基准”。*arXiv 预印本 arXiv:2210.07316* (2022)。'
- en: ^([5](ch07.html#id302-marker)) Matthew Henderson, Rami, Al-Rfou, Brian, Strope,
    Yun-Hsuan, Sung, Lászl\'o, Lukacs, Ruiqi, Guo, Sanjiv, Kumar, Balint, Miklos,
    and Ray, Kurzweil. “Efficient natural language response suggestion for smart reply.”
    *arXiv preprint arXiv:1705.00652* (2017).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.html#id302-marker)) Matthew Henderson, Rami, Al-Rfou, Brian, Strope,
    Yun-Hsuan, Sung, László, Lukacs, Ruiqi, Guo, Sanjiv, Kumar, Balint, Miklos, 和
    Ray, Kurzweil. “智能回复的高效自然语言响应建议。” *arXiv 预印本 arXiv:1705.00652* (2017)。
- en: ^([6](ch07.html#id303-marker)) Oord, Aaron van den, Yazhe, Li, and Oriol, Vinyals.
    “Representation learning with contrastive predictive coding”.*arXiv preprint arXiv:1807.03748* (2018).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.html#id303-marker)) Oord, Aaron van den, Yazhe, Li, 和 Oriol, Vinyals.
    “使用对比预测编码的表示学习”。*arXiv 预印本 arXiv:1807.03748* (2018)。
- en: ^([7](ch07.html#id304-marker)) Ting Chn, Simon, Kornblith, Mohammad, Norouzi,
    and Geoffrey, Hinton. “A simple framework for contrastive learning of visual representations.”
    . In *International conference on machine learning* (pp. 1597–1607).2020.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.html#id304-marker)) Ting Chn, Simon, Kornblith, Mohammad, Norouzi,
    和 Geoffrey, Hinton. “用于对比学习视觉表示的简单框架。” 收录于 *国际机器学习会议* (pp. 1597–1607). 2020。
- en: '^([8](ch07.html#id305-marker)) Thakur, Nandan, Nils, Reimers, Johannes, Daxenberger,
    and Iryna, Gurevych. “Augmented sbert: Data augmentation method for improving
    bi-encoders for pairwise sentence scoring tasks”.*arXiv preprint arXiv:2010.08240* (2020).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch07.html#id305-marker)) Thakur, Nandan, Nils, Reimers, Johannes, Daxenberger,
    和 Iryna, Gurevych. “增强的sbert: 提升双编码器用于成对句子评分任务的数据增强方法”。*arXiv 预印本 arXiv:2010.08240* (2020)。'
- en: '^([9](ch07.html#id306-marker)) Gao, Tianyu, Xingcheng, Yao, and Danqi, Chen.
    “Simcse: Simple contrastive learning of sentence embeddings”.*arXiv preprint arXiv:2104.08821* (2021).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch07.html#id306-marker)) Gao, Tianyu, Xingcheng Yao 和 Danqi Chen. “Simcse:
    简单对比学习句子嵌入”。*arXiv 预印本 arXiv:2104.08821* (2021)。'
- en: ^([10](ch07.html#id307-marker)) Janson, Sverker, Evangelina, Gogoulou, Erik,
    Ylipäa, Amaru, Cuba Gyllensten, and Magnus, Sahlgren. “Semantic re-tuning with
    contrastive tension.” In *International Conference on Learning Representations,
    2021*.2021.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch07.html#id307-marker)) Janson, Sverker, Evangelina Gogoulou, Erik Ylipäa,
    Amaru Cuba Gyllensten 和 Magnus Sahlgren. “语义重调与对比张力。” 在 *国际学习表征会议, 2021*。2021。
- en: '^([11](ch07.html#id308-marker)) Kexin Wang, Nils, Reimers, and Iryna, Gurevych.
    “Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised
    sentence embedding learning”.*arXiv preprint arXiv:2104.06979* (2021).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '^([11](ch07.html#id308-marker)) Kexin Wang, Nils Reimers 和 Iryna Gurevych.
    “Tsdae: 使用基于变换器的顺序去噪自编码器进行无监督句子嵌入学习”。*arXiv 预印本 arXiv:2104.06979* (2021)。'
- en: '^([12](ch07.html#id309-marker)) Kexin Wang, Nandan, Thakur, Nils, Reimers,
    and Iryna, Gurevych. “Gpl: Generative pseudo labeling for unsupervised domain
    adaptation of dense retrieval”.*arXiv preprint arXiv:2112.07577* (2021).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '^([12](ch07.html#id309-marker)) Kexin Wang, Nandan Thakur, Nils Reimers 和 Iryna
    Gurevych. “Gpl: 生成伪标签用于无监督领域适应的密集检索”。*arXiv 预印本 arXiv:2112.07577* (2021)。'
- en: About the Authors
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者介绍
- en: '**Jay Alammar** is Director and Engineering Fellow at Cohere (pioneering provider
    of large language models as an API). In this role, he advises and educates enterprises
    and the developer community on using language models for practical use cases).
    Through his popular AI/ML blog, Jay has helped millions of researchers and engineers
    visually understand machine learning tools and concepts from the basic (ending
    up in the documentation of packages like NumPy and pandas) to the cutting-edge
    (Transformers, BERT, GPT-3, Stable Diffusion). Jay is also a co-creator of popular
    machine learning and natural language processing courses on Deeplearning.ai and
    Udacity.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jay Alammar** 是 Cohere 的总监和工程院士（领先的语言模型API提供商）。在这个角色中，他为企业和开发者社区提供建议和教育，指导他们将语言模型应用于实际案例。通过他受欢迎的
    AI/ML 博客，Jay 帮助数百万研究人员和工程师从基础（最终进入 NumPy 和 pandas 等包的文档）到前沿（变换器、BERT、GPT-3、Stable
    Diffusion）直观理解机器学习工具和概念。Jay 还是 Deeplearning.ai 和 Udacity 上受欢迎的机器学习和自然语言处理课程的共同创作者。'
- en: '**Maarten Grootendorst** is a Senior Clinical Data Scientist at IKNL (Netherlands
    Comprehensive Cancer Organization). He holds master’s degrees in organizational
    psychology, clinical psychology, and data science which he leverages to communicate
    complex Machine Learning concepts to a wide audience. With his popular blogs,
    he has reached millions of readers by explaining the fundamentals of Artificial
    Intelligence–often from a psychological point of view. He is the author and maintainer
    of several open-source packages that rely on the strength of Large Language Models,
    such as BERTopic, PolyFuzz, and KeyBERT. His packages are downloaded millions
    of times and used by data professionals and organizations worldwide.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**Maarten Grootendorst** 是 IKNL（荷兰综合癌症组织）的高级临床数据科学家。他拥有组织心理学、临床心理学和数据科学的硕士学位，利用这些知识向广泛受众传达复杂的机器学习概念。通过他受欢迎的博客，他通过心理学视角解释人工智能的基本原理，触及了数百万读者。他是多个开源包的作者和维护者，这些包依赖于大型语言模型的强大功能，如
    BERTopic、PolyFuzz 和 KeyBERT。他的包被全球的数据专业人士和组织下载数百万次。'
