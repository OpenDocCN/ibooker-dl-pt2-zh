- en: 3 *Introduction to Keras and TensorFlow*
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 *Keras 和 TensorFlow 入门*
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章包括*'
- en: A closer look at TensorFlow, Keras, and their relationship
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 TensorFlow、Keras 及其关系的更深入了解
- en: Setting up a deep learning workspace
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个深度学习工作空间
- en: An overview of how core deep learning concepts translate to Keras and TensorFlow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习核心概念如何转换为 Keras 和 TensorFlow 的概述
- en: This chapter is meant to give you everything you need to start doing deep learning
    in practice. I’ll give you a quick presentation of Keras ([https://keras.rstudio.com](https://www.keras.rstudio.com))
    and TensorFlow ([https://tensorflow.rstudio.com](https://www.tensorflow.rstudio.com)),
    the R-based deep learning tools that we’ll use throughout the book. You’ll find
    out how to set up a deep learning workspace with TensorFlow, Keras, and GPU support.
    Finally, building on top of the first contact you had with Keras and TensorFlow
    in chapter 2, we’ll review the core components of neural networks and how they
    translate to the Keras and Tensor-Flow APIs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为您提供开始在实践中进行深度学习所需的一切。我将简要介绍 Keras（[https://keras.rstudio.com](https://www.keras.rstudio.com)）和
    TensorFlow（[https://tensorflow.rstudio.com](https://www.tensorflow.rstudio.com)），这是我们将在本书中使用的基于
    R 的深度学习工具。您将了解如何使用 TensorFlow、Keras 和 GPU 支持设置深度学习工作空间。最后，建立在您在第2章中对 Keras 和 TensorFlow
    的初次接触之上，我们将回顾神经网络的核心组件以及它们如何转换为 Keras 和 Tensor-Flow 的 API。
- en: By the end of this chapter, you’ll be ready to move on to practical, real-world
    applications, which will start with chapter 4.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将准备好进入实际的、真实世界的应用程序，这将从第四章开始。
- en: 3.1 What’s TensorFlow?
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 TensorFlow 是什么？
- en: 'TensorFlow is a free, open source machine learning platform, developed primarily
    by Google. Much like R itself, the primary purpose of TensorFlow is to enable
    scientists, engineers, and researchers to manipulate mathematical expressions
    over numerical tensors. But TensorFlow brings to R the following new capabilities:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个免费、开源的机器学习平台，主要由 Google 开发。与 R 本身类似，TensorFlow 的主要目的是让科学家、工程师和研究人员能够在数值张量上操作数学表达式。但是
    TensorFlow 为 R 带来了以下新功能：
- en: It can automatically compute the gradient of any differentiable expression (as
    you saw in chapter 2), making it highly suitable for machine learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以自动计算任何可微分表达式的梯度（正如你在第2章中看到的那样），这使其非常适合机器学习。
- en: It can run not only on CPUs but also on GPUs and TPUs, which are highly parallel
    hardware accelerators.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不仅可以在 CPU 上运行，还可以在 GPU 和 TPU 上运行，后者是高度并行的硬件加速器。
- en: Computation defined in TensorFlow can be easily distributed across many machines.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义的计算可以轻松分布到许多机器上。
- en: TensorFlow programs can be exported to other runtimes, such as C++, Java-Script
    (for browser-based applications), or TensorFlow Lite (for applications running
    on mobile devices or embedded devices). This makes TensorFlow applications easy
    to deploy in practical settings
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 程序可以导出到其他运行时，比如 C++、Java-Script（用于基于浏览器的应用程序）或 TensorFlow Lite（用于在移动设备或嵌入式设备上运行的应用程序）。这使得
    TensorFlow 应用在实际环境中容易部署。
- en: It’s important to keep in mind that TensorFlow is much more than a single library.
    It’s really a platform, home to a vast ecosystem of components, some developed
    by Google and some developed by third parties. For instance, there’s TF-Agents
    for reinforcement-learning research, TFX for industry-strength machine learning
    workflow management, TensorFlow Serving for production deployment, and the TensorFlow
    Hub repository of pretrained models. Together, these components cover a very wide
    range of use cases, from cutting-edge research to large-scale production applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，TensorFlow 不仅仅是一个单一的库。它实际上是一个平台，是许多组件的家园，其中一些由 Google 开发，一些由第三方开发。例如，有
    TF-Agents 用于强化学习研究，TFX 用于工业强度的机器学习工作流管理，TensorFlow Serving 用于生产部署，以及 TensorFlow
    Hub 预训练模型的存储库。总之，这些组件涵盖了非常广泛的用例，从前沿研究到大规模生产应用。
- en: 'TensorFlow scales fairly well: for instance, scientists from Oak Ridge National
    Lab have used it to train a 1.1 exaflops extreme weather forecasting model on
    the 27,000 GPUs of the IBM Summit supercomputer. Likewise, Google has used TensorFlow
    to develop very compute-intensive deep learning applications, such as the chess-playing
    and Go-playing agent AlphaZero. For your own models, if you have the budget, you
    can realistically hope to scale to around 10 petaflops on a small TPU pod or a
    large cluster of GPUs rented on Google Cloud or AWS. That would still be around
    1% of the peak compute power of the top supercomputer in 2019!'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的扩展性相当不错：例如，来自奥克岭国家实验室的科学家们已经利用它在 IBM Summit 超级计算机的 27,000 个 GPU
    上训练了一个 1.1 艾克斯弗洛普的极端天气预测模型。同样，谷歌已经利用 TensorFlow 开发了计算密集型的深度学习应用程序，比如下棋和围棋代理 AlphaZero。对于你自己的模型，如果你有预算，你可以实际上希望在一个小型
    TPU 机架或一个大型的在 Google Cloud 或 AWS 上租用的 GPU 集群上实现大约 10 百万亿次浮点运算的规模。这仍然只是 2019 年顶级超级计算机峰值计算能力的大约
    1%！
- en: 3.2 What’s Keras?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 什么是 Keras？
- en: Keras is a deep learning API, built on top of TensorFlow, that provides a convenient
    way to define and train any kind of deep learning model. Keras was initially developed
    for research, with the aim of enabling fast deep learning experimentation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个深度学习 API，建立在 TensorFlow 之上，提供了一种方便的方式来定义和训练任何类型的深度学习模型。Keras 最初是为研究而开发的，旨在实现快速的深度学习实验。
- en: Through TensorFlow, Keras can run on top of different types of hardware (see
    [figure 3.1](#fig3-1))—GPU, TPU, or plain CPU—and can be seamlessly scaled to
    thousands of machines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 TensorFlow，Keras 可以在不同类型的硬件（见 [图 3.1](#fig3-1)）上运行——GPU、TPU 或普通 CPU——并且可以无缝地扩展到数千台机器。
- en: 'Keras is known for prioritizing the developer experience. It’s an API for human
    beings, not machines. It follows best practices for reducing cognitive load: it
    offers consistent and simple workflows, it minimizes the number of actions required
    for common use cases, and it provides clear and actionable feedback upon user
    error. This makes Keras easy to learn for a beginner and highly productive to
    use for an expert.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 以优化开发者体验而闻名。它是一个面向人类而不是机器的 API。它遵循减少认知负荷的最佳实践：它提供一致和简单的工作流程，它最小化了常见用例所需的操作数量，并且在用户出错时提供清晰和可操作的反馈。这使得
    Keras 对于初学者来说易于学习，对于专家来说使用起来高效。
- en: '![Image](../images/f0070-01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0070-01.jpg)'
- en: '**Figure 3.1 Keras and TensorFlow: TensorFlow is a low-level tensor computing
    platform, and Keras is a high-level deep learning API.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3.1 Keras 和 TensorFlow：TensorFlow 是一个低级张量计算平台，而 Keras 是一个高级深度学习 API。**'
- en: Keras has well over a million users as of late 2021, ranging from academic researchers,
    engineers, and data scientists at both startups and large companies to graduate
    students and hobbyists. Keras is used at Google, Netflix, Uber, CERN, NASA, Yelp,
    Instacart, Square, and hundreds of startups working on a wide range of problems
    across every industry. Your YouTube recommendations originate from Keras models.
    The Waymo self-driving cars are developed with Keras models. Keras is also a popular
    framework on Kaggle, the machine learning competition website, where most deep
    learning competitions have been won using Keras.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2021 年末，Keras 已经有超过一百万的用户，包括学术研究人员、工程师、数据科学家，以及初创公司和大公司的研究生和爱好者。Keras 在谷歌、Netflix、Uber、CERN、NASA、Yelp、Instacart、Square，以及数百家在各行各业解决各种问题的初创公司中被使用。你的
    YouTube 推荐来自 Keras 模型。Waymo 自动驾驶汽车是用 Keras 模型开发的。Keras 也是 Kaggle 的一个流行框架，这是一个机器学习竞赛网站，大多数深度学习竞赛都是使用
    Keras 赢得的。
- en: Because Keras has a large and diverse user base, it doesn’t force you to follow
    a single “true” way of building and training models. Rather, it enables a wide
    range of different workflows, from the very high level to the very low level,
    corresponding to different user profiles. For instance, you have a multitude of
    ways to build and train models, each representing a certain tradeoff between usability
    and flexibility. In chapter 5, we’ll review in detail a good fraction of this
    spectrum of workflows. You could be using Keras like you would use most other
    high-level frameworks—just calling fit() and letting the framework do its thing—or
    you could be using it like you can base R, by taking full control of every little
    detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Keras 拥有大量且多样化的用户基础，它不会强制要求你遵循单一的“正确”方式来构建和训练模型。相反，它提供了多种不同的工作流程，从非常高级到非常低级，适用于不同的用户群体。例如，你有多种方式来构建和训练模型，每一种都代表了可用性和灵活性之间的某种权衡。在第
    5 章中，我们将详细回顾这一系列工作流程中的大部分内容。你可以像使用大多数其他高级框架一样使用 Keras——只需调用 `fit()` 然后让框架执行它的工作；或者你可以像使用基本的
    R 一样，完全掌控每一个细节。
- en: This means that everything you’re learning now as you’re getting started will
    still be relevant once you’ve become an expert. You can get started easily and
    then gradually dive into workflows where you’re writing more and more logic from
    scratch. You won’t have to switch to an entirely different framework as you go
    from student to researcher, or from data scientist to deep learning engineer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，当你刚开始学习的内容，即使在你成为专家之后仍然适用。你可以轻松开始，然后逐渐深入到更复杂的工作流程，其中你需要从头开始编写越来越多的逻辑。你不需要在从学生到研究员，或者从数据科学家到深度学习工程师的过程中，切换到完全不同的框架。
- en: 'This philosophy is not unlike that of R itself! Some languages only offer one
    way to write programs—for instance, object-oriented programming or functional
    programming. Meanwhile, R is a multiparadigm language: it offers an array of possible
    usage patterns that all work nicely together. This makes R suitable to a wide
    range of very different use cases: data science, machine learning engineering,
    web development… or just learning how to program. Likewise, you can think of Keras
    as the R of deep learning: a user-friendly deep learning language that offers
    a variety of workflows to different user profiles.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种哲学与 R 本身的哲学非常相似！有些语言只提供一种编程方式——例如，面向对象编程或函数式编程。而 R 是一种多范式语言：它提供了各种可能的用法模式，这些模式都可以很好地协同工作。这使得
    R 适用于非常不同的用例：数据科学、机器学习工程、网页开发……或者只是学习编程。同样，你可以将 Keras 视为深度学习领域的 R：一种用户友好的深度学习语言，它为不同的用户群体提供了多种工作流程。
- en: '3.3 Keras and TensorFlow: A brief history'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 Keras 和 TensorFlow：简史
- en: Keras predates TensorFlow by eight months. It was released in March 2015, and
    TensorFlow was released in November 2015\. You may ask, if Keras is built on top
    of TensorFlow, how it could exist before TensorFlow was released? Keras was originally
    built on top of Theano, another tensor-manipulation library that provided automatic
    differentiation and GPU support—the earliest of its kind. Theano, developed at
    the Montréal Institute for Learning Algorithms (MILA) at the Université de Montréal,
    was in many ways a precursor of TensorFlow. It pioneered the idea of using static
    computation graphs for automatic differentiation and for compiling code to both
    CPU and GPU.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 比 TensorFlow 早八个月。Keras 于 2015 年 3 月发布，而 TensorFlow 于 2015 年 11 月发布。你可能会问，如果
    Keras 是建立在 TensorFlow 之上的，它是如何在 TensorFlow 发布之前就存在的？Keras 最初是建立在 Theano 之上的，这是另一个张量操作库，它提供了自动微分和
    GPU 支持——这是同类中的最早期的。Theano 由蒙特利尔大学（Université de Montréal）的蒙特利尔学习算法研究所（MILA）开发，在许多方面是
    TensorFlow 的前身。它开创了使用静态计算图进行自动微分并将代码编译到 CPU 和 GPU 的理念。
- en: 'In late 2015, after the release of TensorFlow, Keras was refactored to a multibackend
    architecture: it became possible to use Keras with either Theano or TensorFlow,
    and switching between the two was as easy as changing an environment variable.
    By September 2016, TensorFlow had reached a level of technical maturity where
    it became possible to make it the default backend option for Keras. In 2017, two
    new additional backend options were added to Keras: CNTK (developed by Microsoft)
    and MXNet (developed by Amazon). Nowadays, both Theano and CNTK are out of development,
    and MXNet is not widely used outside of Amazon. Keras is back to being a single-backend
    API—on top of TensorFlow.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年底，TensorFlow发布后，Keras被重构为一个多后端架构：可以使用Theano或TensorFlow来使用Keras，并且在两者之间切换只需更改一个环境变量。到2016年9月，TensorFlow已经达到了技术成熟的水平，使其成为Keras的默认后端选项是可行的。2017年，Keras增加了两个新的后端选项：由Microsoft开发的CNTK和由Amazon开发的MXNet。现在，Theano和CNTK已停止开发，MXNet在Amazon以外的使用不是很广泛。Keras又恢复了成为基于TensorFlow的单后端API。
- en: Keras and TensorFlow have had a symbiotic relationship for many years. Throughout
    2016 and 2017, Keras became well known as the user-friendly way to develop TensorFlow
    applications, funneling new users into the TensorFlow ecosystem. By late 2017,
    a majority of TensorFlow users were using it through Keras or in combination with
    Keras. In 2018, the TensorFlow leadership picked Keras as TensorFlow’s official
    high-level API. As a result, the Keras API is front and center in TensorFlow 2.0,
    released in September 2019—an extensive redesign of TensorFlow and Keras that
    takes into account more than four years of user feedback and technical progress.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Keras和TensorFlow多年来一直保持着一种共生关系。在2016年和2017年期间，Keras成为了开发TensorFlow应用的用户友好方式，将新用户引入TensorFlow生态系统。到2017年底，大多数TensorFlow用户使用Keras或与Keras相结合。2018年，TensorFlow领导团队选择Keras作为TensorFlow官方的高级API。因此，Keras
    API是在2019年9月发布的TensorFlow 2.0中的重点，这是TensorFlow和Keras的广泛重新设计，考虑了四年的用户反馈和技术进步。
- en: '3.4 Python and R interfaces: A brief history'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 Python和R接口：简史
- en: The R interfaces to TensorFlow and Keras were made available in late 2016 and
    early 2017, respectively. They are principally developed and maintained by RStudio.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和Keras的R接口分别于2016年底和2017年初发布，并由RStudio进行主要开发和维护。
- en: The R interfaces to Keras and TensorFlow are built on top of the reticulate
    package, which embeds a full Python process in R. For the majority of users, this
    is merely an implementation detail. However, as you progress on your journey,
    this setup will turn out to be a great boon, because it means that you have full
    access to everything available in *both* Python and R.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Keras和TensorFlow的R接口是基于reticulate包构建的，该包在R中嵌入了完整的Python过程。对于大多数用户来说，这只是一个实现细节。但是，随着你的学习进展，这将成为一个巨大的优点，因为这意味着你可以完全访问*Python和R*中可用的所有内容。
- en: Throughout the book we use the R interface to Keras that works well with R idioms.
    However, in chapter 13, we show how you can directly use a Python library from
    R, even if no R interface is conveniently available for it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用R接口来使用Keras，这个接口可以很好地与R网格协作。然而，在第13章中，我们展示了如何直接从R中使用Python库，即使没有方便的R界面可用。
- en: By this point, you must be eager to start running Keras and TensorFlow code
    in practice. Let’s get started!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 到了这个点，你一定渴望开始实际运行Keras和TensorFlow的代码。让我们开始吧！
- en: 3.5 Setting up a deep learning workspace
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 设置深度学习工作区
- en: Before you can get started developing deep learning applications, you need to
    set up your development environment. It’s highly recommended, although not strictly
    necessary, that you run deep learning code on a modern NVIDIA GPU rather than
    your computer’s CPU. Some applications—in particular, image processing with convolutional
    networks—will be excruciatingly slow on CPU, even a fast multicore CPU. And even
    for applications that can realistically be run on CPU, you’ll generally see the
    speed increase by a factor of 5 or 10 by using a recent GPU.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始开发深度学习应用程序之前，需要设置开发环境。强烈建议您使用现代NVIDIA GPU而不是计算机的CPU运行深度学习代码，虽然这不是严格必需的。一些应用程序，特别是使用卷积网络进行图像处理的应用程序，甚至在快速的多核CPU上都会变得非常缓慢。即使是可以实际在CPU上运行的应用程序，使用最近的GPU通常也会使速度提高5倍或10倍。
- en: 'To do deep learning on a GPU, you have the following three options:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上进行深度学习，你有以下三种选择：
- en: Buy and install a physical NVIDIA GPU on your workstation.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的工作站上购买并安装一个物理NVIDIA GPU。
- en: Use GPU instances on Google Cloud or Amazon EC2.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 或 Amazon EC2 上使用 GPU 实例。
- en: Use the free GPU runtime from Kaggle, Colaboratory, or similar providers
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kaggle、Colaboratory 或类似提供商的免费 GPU 运行时
- en: Free online providers like Colaboratory or Kaggle are the easiest way to get
    started, because they require no hardware purchase and no software installation—just
    open a tab in your browser and start coding. However, the free version of these
    services is suitable only for small workloads. If you want to scale up, you’ll
    have to use the first or second option.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 豆瓣云、Kaggle 等免费在线服务提供商是入门的最简单途径，因为它们无需购买硬件，也不需要安装软件——只需打开浏览器的一个标签页并开始编码。然而，这些服务的免费版本只适用于小工作负载。如果你想扩大规模，你必须使用第一或第二个选项。
- en: If you don’t already have a GPU that you can use for deep learning (a recent,
    high-end NVIDIA GPU), then running deep learning experiments in the cloud is a
    simple, low-cost way for you to move to larger workloads without having to buy
    any additional hardware.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有可用于深度学习的 GPU（最近的、高端的 NVIDIA GPU），那么在云中运行深度学习实验是一种简单、低成本的方法，让你无需购买任何额外的硬件就可以移动到更大的工作负载。
- en: 'If you’re a heavy user of deep learning, however, this setup isn’t sustainable
    in the long term—or even for more than a few months. Cloud instances aren’t cheap:
    you’d pay $2.48 per hour for a V100 GPU on Google Cloud in mid-2021\. Meanwhile,
    a solid consumer-class GPU will cost you somewhere between $1,500 and $2,500—a
    price that has been fairly stable over time, even as the specs of these GPUs keep
    improving. If you’re a heavy user of deep learning, consider setting up a local
    workstation with one or more GPUs.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你是深度学习的重度用户，这种设置在长期内甚至超过几个月都不可持续。云实例不便宜：在 2021 年中期，你将支付 2.48 美元每小时的 Google
    Cloud V100 GPU。同时，一个稳定的消费级 GPU 的成本将在 1,500-2,500 美元之间——尽管这些 GPU 的规格不断得到改进，但这个价格已经相当稳定。如果你是深度学习的重度用户，请考虑在本地工作站上安装一个或多个
    GPU。
- en: Additionally, whether you’re running locally or in the cloud, it’s better to
    be using a Unix workstation. Although it’s technically possible to run Keras on
    Windows directly, we don’t recommend it. If you’re a Windows user and you want
    to do deep learning on your own workstation, the simplest solution to get everything
    running is to set up an Ubuntu dual boot on your machine, or to leverage Windows
    Subsystem for Linux (WSL), a compatibility layer that enables you to run Linux
    applications from Windows. It may seem like a hassle, but it will save you a lot
    of time and trouble in the long run.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，无论是本地运行还是在云中运行，最好使用 Unix 工作站。虽然在 Windows 上直接运行 Keras 在技术上是可能的，但我们不建议这样做。如果你是
    Windows 用户，并且想在自己的工作站上进行深度学习，最简单的解决方案是在机器上设置 Ubuntu 双启动，或者使用 Windows 子系统用于 Linux（WSL），这是一种兼容层，使你能够从
    Windows 运行 Linux 应用程序。这可能看起来麻烦，但它将在长期内为你节省大量时间和麻烦。
- en: 3.5.1 Installing Keras and TensorFlow
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 安装 Keras 和 TensorFlow
- en: 'Installing Keras and TensorFlow on R on your local machine is straightforward:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地计算机上安装 Keras 和 TensorFlow 很简单：
- en: '**1** Make sure you have R installed. The latest instructions for doing so
    are always available at [https://cloud.r-project.org](https://www.cloud.r-project.org).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 确保已安装 R。最新的安装说明始终可在[https://cloud.r-project.org](https://www.cloud.r-project.org)找到。'
- en: '**2** Install RStudio, available for download at [http://mng.bz/v6JM](http://mng.bz/v6JM).
    (You can safely skip this step if you prefer to use R from another environment.)'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 安装 RStudio，可在[http://mng.bz/v6JM](http://mng.bz/v6JM)下载。（如果你希望使用其他环境的
    R，则可以跳过此步骤。）'
- en: '**3** From the R console, run the following commands:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 从 R 控制台中运行以下命令：'
- en: install.packages("keras")➊
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: install.packages("keras")➊
- en: library(reticulate)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: library(reticulate)
- en: virtualenv_create("r-reticulate", python = install_python())➋
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: virtualenv_create("r-reticulate", python = install_python())➋
- en: library(keras)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: install_keras(envname = "r-reticulate")➌
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: install_keras(envname = "r-reticulate")➌
- en: ➊ **This also pulls in all R dependencies, like reticulate.**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **这也会安装所有 R 依赖项，如 reticulate。**
- en: ➋ **Set up R (reticulate) with a Python installation it can use.**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **设置 R（reticulate）与可用的 Python 安装。**
- en: ➌ **Install TensorFlow and Keras (the Python modules).**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **安装 TensorFlow 和 Keras（Python 模块）。**
- en: And that’s it! You now have a working Keras and TensorFlow installation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！你现在拥有一个可用的 Keras 和 TensorFlow 安装文件。
- en: '**INSTALLING CUDA**'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**安装 CUDA**'
- en: Note that if have a NVIDIA GPU on your machine and you want TensorFlow to use
    it, you will also need to download and install CUDA, cuDNN, and GPU drivers, all
    available for download from [https://developer.nvidia.com/cuda-downloads](https://www.developer.nvidia.com/cuda-downloads)
    and [https://developer.nvidia.com/cudnn](https://www.developer.nvidia.com/cudnn).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您的机器上有 NVIDIA GPU 并且希望 TensorFlow 使用它，则还需要下载并安装 CUDA、cuDNN 和 GPU 驱动程序，可从[https://developer.nvidia.com/cuda-downloads](https://www.developer.nvidia.com/cuda-downloads)和[https://developer.nvidia.com/cudnn](https://www.developer.nvidia.com/cudnn)进行下载。
- en: Each version of TensorFlow requires a specific version of CUDA and cuDNN, and
    it’s rarely the case that the latest CUDA version works with the latest TensorFlow
    version. Typically, you will need to identify the specific CUDA version required
    by Tensor-Flow and then install it from the CUDA toolkit archive at [https://developer.nvidia.com/cuda-toolkit-archive](https://www.developer.nvidia.com/cuda-toolkit-archive).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 TensorFlow 版本都需要特定版本的 CUDA 和 cuDNN，很少情况下最新的 CUDA 版本与最新的 TensorFlow 版本兼容。通常，您需要确定
    Tensor-Flow 需要的特定 CUDA 版本，然后从 CUDA 工具包存档中安装，地址为 [https://developer.nvidia.com/cuda-toolkit-archive](https://www.developer.nvidia.com/cuda-toolkit-archive)。
- en: 'You can find the CUDA version required by the current TensorFlow release version
    by consulting [http://mng.bz/44pV](http://mng.bz/44pV). If you are running an
    older version of TensorFlow, then you can consult the “Tested Build Configurations”
    table at [https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu)
    to find the entry corresponding to your TensorFlow version. You can find out the
    TensorFlow version installed on your machine with:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查阅[http://mng.bz/44pV](http://mng.bz/44pV)来找到当前 TensorFlow 版本所需的 CUDA 版本。如果您正在运行较旧版本的
    TensorFlow，则可以在[https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu)的“Tested
    Build Configurations”表中找到对应于您的 TensorFlow 版本的条目。要查找安装在您的机器上的 TensorFlow 版本，请使用：
- en: tensorflow::tf_config()
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: tensorflow::tf_config()
- en: TensorFlow v2.8.0
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow v2.8.0
- en: '![Image](../images/common01.jpg) (~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common01.jpg) (~/.virtualenvs/r-reticulate/lib/python3.9/site-packages/tensorflow)'
- en: Python v3.9 (~/.virtualenvs/r-reticulate/bin/python)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Python v3.9 (~/.virtualenvs/r-reticulate/bin/python)
- en: At this writing, the latest release of TensorFlow 2.8 requires CUDA 11.2 and
    cuDNN 8.1.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文写作时，最新的 TensorFlow 2.8 版本需要 CUDA 11.2 和 cuDNN 8.1。
- en: Note that the shelf life of specific incantations you can run at the terminal
    to install all the CUDA drivers is very short (not to mention specific to each
    OS). We don’t include any such incantations in the book because they would likely
    be outdated before the book was even printed. Instead, you can always find the
    latest instructions at [https://tensorflow.rstudio.com/installation/](https://www.tensorflow.rstudio.com/installation/).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以在终端上运行特定命令来安装所有 CUDA 驱动程序，但这些命令的有效期非常短（更不用说针对每个操作系统特定）。我们在书中不包含任何这样的命令，因为它们可能会在书印刷之前就过时了。相反，您可以始终在
    [https://tensorflow.rstudio.com/installation/](https://www.tensorflow.rstudio.com/installation/)
    找到最新的安装说明。
- en: You now have a way to start running Keras code in practice. Next, let’s see
    how the key ideas you learned about in chapter 2 translate to Keras and TensorFlow
    code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您有一种实践运行 Keras 代码的方法。接下来，让我们看看您在第 2 章学到的关键思想如何转化为 Keras 和 TensorFlow 代码。
- en: 3.6 First steps with TensorFlow
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 TensorFlow 的初始步骤
- en: 'As you saw in the previous chapters, training a neural network revolves around
    the following concepts:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前几章中所看到的，训练神经网络围绕以下概念展开：
- en: 'First, low-level tensor manipulation—the infrastructure that underlies all
    modern machine learning. This translates to TensorFlow APIs:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先是低级张量操作——现代机器学习的基础设施。这对应 TensorFlow 的 API：
- en: '*Tensors*, including special tensors that store the network’s state (*variables*)'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*张量*（tensors），包括存储网络状态的特殊张量（*变量*）'
- en: '*Tensor operations* such as addition, relu, matmul'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如加法、relu、matmul等*张量操作*
- en: '*Backpropagation*, a way to compute the gradient of mathematical expression
    (handled in TensorFlow via the GradientTape object)'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向传播*（Backpropagation），一种计算数学表达式梯度的方法（在 TensorFlow 中通过 GradientTape 对象处理）'
- en: Second, high-level deep learning concepts. This translates to Keras APIs
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，是高级深度学习概念。这对应于 Keras 的 API
- en: '*Layers*, which are combined into a *model*'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层*（layers），它们被组合成一个*模型*（model）'
- en: A *loss function*, which defines the feedback signal used for learning
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数*（loss function）定义了用于学习的反馈信号。'
- en: An *optimizer*, which determines how learning proceeds
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定学习过程的*优化器*（optimizer）
- en: '*Metrics* to evaluate model performance, such as accuracy'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*度量指标*（metrics）用于评估模型的性能，如准确率。'
- en: A *training loop* that performs mini-batch stochastic gradient descent
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行小批量随机梯度下降的 *训练循环*
- en: In the previous chapter, you already had a quick look at some of the corresponding
    TensorFlow and Keras APIs. You’ve briefly used TensorFlow’s Variable class, the
    matmul operation, and the GradientTape. You’ve instantiated Keras dense layers,
    packed them into a sequential model, and trained that model with the fit() method.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你已经简要了解了一些对应的 TensorFlow 和 Keras API。你已经简要使用了 TensorFlow 的 Variable 类、matmul
    操作和 GradientTape。你已经实例化了 Keras 的 dense 层，将它们打包成了一个 sequential 模型，并使用 fit() 方法对该模型进行了训练。
- en: Now let’s take a deeper dive into how all of these different concepts can be
    approached in practice using TensorFlow and Keras.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更深入地了解如何使用 TensorFlow 和 Keras 在实践中处理所有这些不同的概念。
- en: 3.6.1 TensorFlow tensors
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 TensorFlow 张量
- en: To do anything in TensorFlow, we’re going to need some tensors. In the previous
    chapter, we introduced some tensor concepts and terminology, and used something
    you may already be familiar with, R arrays, as an example implementation. Here,
    we move beyond the concepts and introduce the specific implementation of tensors
    used by TensorFlow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 TensorFlow 中做任何事情，我们都需要一些张量。在前一章中，我们介绍了一些张量的概念和术语，并使用了你可能已经熟悉的 R 数组作为一个示例实现。在这里，我们超越了概念，介绍了
    TensorFlow 使用的张量的具体实现。
- en: 'TensorFlow tensors are very much like R arrays; they are a container for data
    that also has some metadata, like shape and type. You can convert an R array to
    a Tensor-Flow tensor with as_tensor():'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 张量非常类似于 R 数组；它们是数据的容器，还具有一些元数据，如形状和类型。你可以使用 as_tensor() 将 R 数组转换为
    TensorFlow 张量：
- en: r_array <- array(1:6, c(2, 3))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: r_array <- array(1:6, c(2, 3))
- en: tf_tensor <- as_tensor(r_array)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor <- as_tensor(r_array)
- en: tf_tensor
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor
- en: tf.Tensor(
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[1  3  5]'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1  3  5]'
- en: '[2  4  6]], shape=(2, 3), dtype=int32)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[2  4  6]], shape=(2, 3), dtype=int32)'
- en: 'Like R arrays, tensors work with many of the same tensor operations you are
    already familiar with: functions like dim(), length(), built-in math generics
    like + and log(), and so on:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与 R 数组类似，张量可以使用许多你已经熟悉的相同张量操作：如 dim()、length()、内置数学泛型如 + 和 log() 等等：
- en: dim(tf_tensor)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: dim(tf_tensor)
- en: '[1]  2  3'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]  2  3'
- en: tf_tensor + tf_tensor
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor + tf_tensor
- en: tf.Tensor(
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[ 2  6  10]'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 2  6  10]'
- en: '[ 4  8  12]], shape=(2, 3), dtype=int32)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 4  8  12]], shape=(2, 3), dtype=int32)'
- en: 'The set of R generics that work with tensors is extensive:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于张量的 R 泛型集合非常广泛：
- en: methods(class = "tensorflow.tensor")
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: methods(class = "tensorflow.tensor")
- en: '![Image](../images/f0075-01.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0075-01.jpg)'
- en: This means that you can often write the same code for TensorFlow tensors as
    you would for R arrays.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你通常可以为 TensorFlow 张量编写与 R 数组相同的代码。
- en: 3.7 Tensor attributes
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 张量属性
- en: 'Unlike R arrays, tensors have some attributes you can access with $:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与 R 数组不同，张量具有一些可以使用 $ 访问的属性：
- en: tf_tensor$ndim➊
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor$ndim➊
- en: '[1]  2'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]  2'
- en: ➊**ndim returns a scalar integer, the rank of the tensor, equivalent to length(dim(x)).**
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ➊**ndim 返回一个标量整数，张量的秩，相当于 length(dim(x))。**
- en: 'Length 1 R vectors are automatically converted to rank 0 tensors, whereas R
    vectors of length > 1 are converted to rank 1 tensors:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 长度为 1 的 R 向量会自动转换为秩为 0 的张量，而长度大于 1 的 R 向量会转换为秩为 1 的张量：
- en: as_tensor(1)$ndim
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(1)$ndim
- en: '[1] 0'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 0'
- en: as_tensor(1:2)$ndim
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(1:2)$ndim
- en: '[1]  1'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]  1'
- en: tf_tensor$shape
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor$shape
- en: TensorShape([2, 3])
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([2, 3])
- en: 'tf_tensor$shape returns a tf.TensorShape object. This a class object with support
    for undefined or unspecified dimensions, and a variety of methods and properties:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor$shape 返回一个 tf.TensorShape 对象。这是一个支持未定义或未指定维度的类对象，并且具有各种方法和属性：
- en: methods(class = class(shape())[1])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: methods(class = class(shape())[1])
- en: '![Image](../images/f0076-01.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0076-01.jpg)'
- en: 'For now, all you need to know is that you can convert a TensorShape to an integer
    vector with as.integer() (dim(x) is shorthand for as.integer(x$shape)), and you
    can construct a TensorShape object manually with the shape() function:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你只需要知道你可以使用 as.integer() 将 TensorShape 转换为整数向量（dim(x) 是 as.integer(x$shape)
    的缩写），并且你可以使用 shape() 函数手动构造一个 TensorShape 对象：
- en: shape(2, 3)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: shape(2, 3)
- en: TensorShape([2, 3])
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([2, 3])
- en: tf_tensor$dtype
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor$dtype
- en: tf.int32
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: tf.int32
- en: tf_tensor$dtype returns the data type of the array. TensorFlow provides support
    for many more data types than base R. For example, base R has one integer type,
    whereas TensorFlow provides support for 13! The R integer type corresponds to
    int32. Different data types make different tradeoffs between how much memory they
    can consume and the range of values they can represent. For example, a tensor
    with a int8 dtype takes only one-quarter the space in memory as one with dtype
    int32, but it can only represent integers between –128 and 127, as opposed to
    –2147483648 to 2147483647.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: tf_tensor$dtype 返回数组的数据类型。TensorFlow 支持的数据类型比基本的 R 更多。例如，基本的 R 只有一个整数类型，而 TensorFlow
    提供了 13 种整数类型！R 的整数类型对应于 int32。不同的数据类型在内存消耗和可以表示的值的范围之间进行不同的权衡。例如，具有 int8 数据类型的张量在内存中只占用
    int32 数据类型的四分之一的空间，但它只能表示 -128 到 127 之间的整数，而不是 -2147483648 到 2147483647。
- en: 'We’ll also be dealing with floating-point data throughout the book. In R, the
    default floating numeric datatype, double, is converted to tf.float64:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还将处理浮点数据。在 R 中，默认的浮点数数据类型 double 会转换为 tf.float64：
- en: r_array <- array(1)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: r_array <- array(1)
- en: typeof(r_array)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: typeof(r_array)
- en: '[1] "double"'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "double"'
- en: as_tensor(r_array)$dtype
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(r_array)$dtype
- en: tf.float64
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: tf.float64
- en: 'For the majority of the book, we’ll be using the smaller float32 as the default
    floating point datatype, trading some accuracy for a smaller memory footprint
    and faster computation speed:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们将使用更小的 float32 作为默认的浮点数数据类型，以换取一些精度，以获得更小的内存占用和更快的计算速度：
- en: as_tensor(r_array, dtype = "float32")
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(r_array, dtype = "float32")
- en: tf.Tensor([1.], shape=(1), dtype=float32)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([1.], shape=(1), dtype=float32)
- en: 3.7.1 Tensor shape and reshaping
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.1 张量的形状和重塑
- en: 'as_tensor() can also optionally take a shape argument, which you can use to
    expand a scalar or reshape a tensor. For example, to make an array of zeros, you
    could write:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor() 还可以选择接受一个形状参数，你可以用它来扩展一个标量或重塑一个张量。例如，要创建一个零数组，你可以写成：
- en: as_tensor(0, shape = c(2, 3))
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(0, shape = c(2, 3))
- en: tf.Tensor(
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[0\. 0\. 0.]'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[[0\. 0\. 0.]'
- en: '[0\. 0\. 0.]], shape=(2, 3), dtype=float32)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 0\. 0.]], shape=(2, 3), dtype=float32)'
- en: 'For R vectors that are not scalars (length(x) > 1), you can also reshape the
    tensor, so long as the overall size of the array stays the same:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不是标量（length(x) > 1）的 R 向量，你也可以重塑张量，只要数组的总大小保持不变：
- en: as_tensor(1:6, shape = c(2, 3))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(1:6, shape = c(2, 3))
- en: tf.Tensor(
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[1 2 3]'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 2 3]'
- en: '[4 5 6]], shape=(2, 3), dtype=int32)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[4 5 6]], shape=(2, 3), dtype=int32)'
- en: 'Note that the tensor was filled row-wise. This is different from R, which fills
    arrays column-wise:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，张量是按行填充的。这与 R 不同，R 是按列填充数组的：
- en: array(1:6, dim = c(2, 3))
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: array(1:6, dim = c(2, 3))
- en: '[,1]   [,2]   [,3]'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1]   [,2]   [,3]'
- en: '[1,]      1      3       5'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]      1      3       5'
- en: '[2,]      2      4       6'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]      2      4       6'
- en: This difference between row-major and column-major ordering (also known as C
    and Fortran ordering, respectively) is one of the things to be on the lookout
    for when converting between R arrays and tensors. R arrays are always Fortran-ordered,
    and Tensor-Flow tensors are always C-ordered, and the distinction becomes important
    anytime you are reshaping an array.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 行主要和列主要顺序之间的差异（也称为 C 和 Fortran 顺序）是在转换 R 数组和张量时要注意的事项之一。R 数组始终是 Fortran 顺序的，而
    TensorFlow 张量始终是 C 顺序的，这种区别在任何时候重塑数组时都变得重要。
- en: 'When you are working with tensors, reshaping will use C-style ordering. Anytime
    you are handling R arrays, you can use array_reshape() if you want to be explicit
    about the reshaping behavior you want:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理张量时，重塑将使用 C 风格的顺序。每当你处理 R 数组时，如果你想明确指定重塑行为，你可以使用 array_reshape()：
- en: array_reshape(1:6, c(2, 3), order = "C")
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: array_reshape(1:6, c(2, 3), order = "C")
- en: '[,1]   [,2]   [,3]'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1]   [,2]   [,3]'
- en: '[1,]      1      2       3'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]      1      2       3'
- en: '[2,]      4      5       6'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]      4      5       6'
- en: array_reshape(1:6, c(2, 3), order = "F")
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: array_reshape(1:6, c(2, 3), order = "F")
- en: '[,1]   [,2]   [,3]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1]   [,2]   [,3]'
- en: '[1,]      1      3       5'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]      1      3       5'
- en: '[2,]      2      4       6'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]      2      4       6'
- en: 'Finally, array_reshape() and as_tensor() allow you to leave the size of one
    of the axes unspecified, and it will be automatically inferred using the size
    of the array and the size of the remaining axes. You can pass -1 or NA for the
    axis you want inferred:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，array_reshape() 和 as_tensor() 还允许你保留一个轴的大小未指定，它将自动推断使用数组的大小和剩余轴的大小。你可以为你想要推断的轴传递
    -1 或 NA：
- en: array_reshape(1:6, c(-1, 3))
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: array_reshape(1:6, c(-1, 3))
- en: '[,1]   [,2]   [,3]'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1]   [,2]   [,3]'
- en: '[1,]      1      2       3'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,]      1      2       3'
- en: '[2,]      4      5       6'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]      4      5       6'
- en: as_tensor(1:6, shape = c(NA, 3))
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(1:6, shape = c(NA, 3))
- en: tf.Tensor(
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[1 2 3]'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 2 3]'
- en: '[4 5 6]], shape=(2, 3), dtype=int32)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[4 5 6]], 形状=(2, 3), 数据类型=int32)'
- en: 3.7.2 Tensor slicing
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.2 张量切片
- en: Subsetting tensors is similar to subsetting R arrays, but not identical. Slicing
    tensors offers some conveniences that R arrays don’t and vice versa.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 子集张量类似于子集R数组，但不完全相同。切片张量提供了一些R数组没有的便利，反之亦然。
- en: 'Tensors allow you to slice with a missing value supplied to one end of a slice
    range, which implicitly means “the rest of the tensor in that direction” (R arrays
    don’t offer this slicing convenience). For example, revisiting the example in
    chapter 2 where we want to slice out a crop of the MNIST images, we could have
    provided an NA to the slice instead:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '张量允许您使用提供给切片范围的一个端点的缺失值进行切片，这意味着“该方向上的张量的其余部分”(R数组不提供此切片便利) (R数组不提供此切片便利)。例如，重新访问第2章中的示例，我们要切片MNIST图像的一部分，我们可以提供一个NA给切片:'
- en: train_images <- as_tensor(dataset_mnist()$train$x)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- as_tensor(dataset_mnist()$train$x)
- en: my_slice <- train_images[, 15:NA, 15:NA]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: my_slice <- train_images[, 15:NA, 15:NA]
- en: Be aware that the expression 15:NA will produce an R error in other contexts;
    it works only in the brackets of a tensor slicing operation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，表达式15:NA将在其他上下文中产生R错误;它只在张量切片操作的括号中起作用。
- en: 'It’s also possible to use negative indices. Note that unlike R arrays, negative
    indices do not drop elements; instead, they indicate the index position relative
    to the end of the current axis. (Because this is a change from standard R subsetting
    behavior, a warning is issued the first time a negative slice index is encountered.)
    To crop the images to patches of 14 × 14 pixels centered in the middle, you could
    do this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '还可以使用负索引。请注意，与R数组不同，负索引不会丢弃元素;相反，它们指示相对于当前轴末端的索引位置。(因为这是与标准R子集行为的改变，所以在第一次遇到负切片索引时会发出警告。)要将图像裁剪为14
    × 14像素的补丁居中，你可以这样做:'
- en: my_slice <- train_images[, 8:-8, 8:-8]
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: my_slice <- train_images[, 8:-8, 8:-8]
- en: 'Warning:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '警告:'
- en: Negative numbers are interpreted python-style
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 负数采用Python风格解释
- en: '![Image](../images/common01.jpg) when subsetting tensorflow tensors.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../images/common01.jpg) 在子集张量的情况下。'
- en: See ?`[.tensorflow.tensor` for details.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 详细信息请参阅?[.tensorflow.tensor`。
- en: To turn off this warning,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭此警告，
- en: '![Image](../images/common01.jpg) set `options(tensorflow.extract.warn_negatives_pythonic
    = FALSE)`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../images/common01.jpg) 设置`options(tensorflow.extract.warn_negatives_pythonic
    = FALSE)`'
- en: You can also use the special all_dims() object anytime you want to implicitly
    capture remaining dimensions, without supplying the exact number of commas (,)
    required in the call to [. For example, say you want to take the first 100 images
    only, you can write
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以随时使用特殊的all_dims()对象来隐式捕获剩余维度，而不必提供调用时所需的逗号(,)的确切数量。例如，假设你只想取前100个图像，你可以写成
- en: my_slice <- train_images[1:100, all_dims()]
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: my_slice <- train_images[1:100, all_dims()]
- en: instead of
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是
- en: my_slice <- train_images[1:100, , ]
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: my_slice <- train_images[1:100, , ]
- en: This comes in handy for writing code that can work with tensors of different
    ranks, for example, taking matching slices of model inputs and targets along the
    batch dimension.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这在编写可以处理不同秩的张量的代码时非常方便，例如，在批量维度上取匹配的模型输入和目标的切片。
- en: 3.7.3 Tensor broadcasting
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.3 张量广播
- en: 'We introduced broadcasting in chapter 2\. *Broadcasting* is performed when
    we have an operation on two different-sized tensors, and we want the smaller tensor
    to be *broadcast* to match the shape of the larger tensor. Broadcasting consists
    of the following two steps:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第2章介绍了广播。当我们对两个不同大小的张量进行操作，并且我们希望较小的张量*广播*以匹配较大张量的形状时，就会执行广播。广播包括以下两个步骤:'
- en: '**1** Axes (called *broadcast axes*) are added to the smaller tensor to match
    the ndim of the larger tensor.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 轴(称为*广播轴*)被添加到较小的张量中，以匹配较大张量的ndim。'
- en: '**2** The smaller tensor is repeated alongside these new axes to match the
    full shape of the larger tensor.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 较小的张量沿着这些新轴重复，以匹配较大张量的完整形状。'
- en: With broadcasting, you can generally perform element-wise operations that take
    two input tensors if one tensor has shape (a, b, … n, n + 1, … m) and the other
    has shape (n, n + 1, … m). The broadcasting will then automatically happen for
    axes a through n - 1.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用广播，如果一个张量的形状为(a, b, ... n, n + 1, ... m)，另一个张量的形状为(n, n + 1, ... m)，通常可以执行元素级别操作。然后广播将自动发生在轴a到n
    - 1上。
- en: 'The following example applies the element-wise + operation to two tensors of
    different shapes via broadcasting:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '以下示例通过广播将两个不同形状的张量应用于元素级别+操作:'
- en: x <- as_tensor(1, shape = c(64, 3, 32, 10))
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: x <- as_tensor(1, shape = c(64, 3, 32, 10))
- en: y <- as_tensor(2, shape = c(32, 10))
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: y <- as_tensor(2, shape = c(32, 10))
- en: z <- x + y➊
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: z <- x + y➊
- en: ➊ **The output z has shape (64, 3, 32, 10) like x.**
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输出 z 的形状与 x 相同，为 (64, 3, 32, 10)。**
- en: 'Anytime you want to be explicit about broadcasting semantics, you can use a
    tf$newaxis to insert a size 1 dimension in a tensor:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 每当您希望明确广播语义时，可以使用 tf$newaxis 在张量中插入大小为 1 的维度：
- en: z <- x + y[tf$newaxis, tf$newaxis, , ]
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: z <- x + y[tf$newaxis, tf$newaxis, , ]
- en: 3.7.4 The tf module
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.4 The tf module
- en: Tensors need to be created with some initial value. You can generally stick
    to as_ tensor() for creating tensors, but the tf module also contains many functions
    for creating tensors. For instance, you could create all-ones or all-zeros tensors,
    or tensors of values drawn from a random distribution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 张量需要用一些初始值创建。通常可以使用 as_tensor() 来创建张量，但是 tf 模块还包含许多用于创建张量的函数。例如，您可以创建全 1 或全
    0 的张量，或者从随机分布中绘制值的张量。
- en: library(tensorflow)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: tf$ones(shape(1, 3))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: tf$ones(shape(1, 3))
- en: tf.Tensor([[1\. 1\. 1.]], shape=(1, 3), dtype=float32)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([[1\. 1\. 1.]], shape=(1, 3), dtype=float32)
- en: tf$zeros(shape(1, 3))
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: tf$zeros(shape(1, 3))
- en: tf.Tensor([[0\. 0\. 0.]], shape=(1, 3), dtype=float32)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([[0\. 0\. 0.]], shape=(1, 3), dtype=float32)
- en: tf$random$normal(shape(1, 3), mean = 0, stddev = 1)➊
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: tf$random$normal(shape(1, 3), mean = 0, stddev = 1)➊
- en: tf.Tensor([[ 0.79165614
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([[ 0.79165614
- en: '![Image](../images/common01.jpg) -0.35886717 0.13686056]], shape=(1, 3), dtype=float32)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common01.jpg) -0.35886717 0.13686056]], shape=(1, 3), dtype=float32)'
- en: tf$random$uniform(shape(1, 3))➋
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: tf$random$uniform(shape(1, 3))➋
- en: tf.Tensor([[0.93715847 0.67879045 0.60081327]], shape=(1, 3), dtype=float32)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([[0.93715847 0.67879045 0.60081327]], shape=(1, 3), dtype=float32)
- en: ➊ **Tensor of random values drawn from a normal distribution with mean 0 and
    standard deviation 1. Equivalent to array(rnorm(3 * 1, mean = 0, sd = 1), dim
    = c(1, 3).**
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **从均值为 0、标准差为 1 的正态分布中绘制的随机值张量。等同于 array(rnorm(3 * 1, mean = 0, sd = 1), dim
    = c(1, 3)。**
- en: ➋ **Tensor of random values drawn from a uniform distribution between 0 and
    1. Equivalent to array(runif(3 * 1, min = 0, max = 1), dim = c(1, 3)).**
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **从 0 到 1 之间的均匀分布中绘制的随机值张量。等同于 array(runif(3 * 1, min = 0, max = 1), dim =
    c(1, 3))。**
- en: Note that the tf module exposes the full Python TensorFlow API. One thing to
    be aware of is that the Python API frequently expects integers, whereas a bare
    R numeric literal like 2 produces a double instead of an integer. In R, we can
    specify an integer literal by appending an L, as in 2L.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，tf 模块公开了完整的 Python TensorFlow API。需要注意的一点是，Python API 经常期望整数，而像 2 这样的裸 R
    数字文字会产生 double 而不是整数。在 R 中，我们可以通过添加 L 来指定整数文字，例如 2L。
- en: tf$ones(c(2, 1))➊
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: tf$ones(c(2, 1))➊
- en: 'Error in py_call_impl(callable, dots$args, dots$keywords):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 'Error in py_call_impl(callable, dots$args, dots$keywords):'
- en: 'TypeError: Cannot convert [2.0, 1.0] to EagerTensor of dtype int32'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 'TypeError: Cannot convert [2.0, 1.0] to EagerTensor of dtype int32'
- en: tf$ones(c(2L, 1L))➋
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: tf$ones(c(2L, 1L))➋
- en: tf.Tensor(
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[1.]'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1.]'
- en: '[1.]], shape=(2, 1), dtype=float32)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.]], shape=(2, 1), dtype=float32)'
- en: ➊ **Providing R doubles here gives an error.**
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在这里提供 R doubles 会产生错误。**
- en: ➋ **Provide integer literals to avoid the error.**
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **提供整数文字以避免错误。**
- en: When dealing with the tf module, we will often write literal integers with an
    L suffix where the Python API requires it.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理 tf 模块时，我们经常会在需要 Python API 时使用带有 L 后缀的文字整数。
- en: 'Another thing to be aware of is that functions in the tf module use a 0-based
    index counting convention, that is, the first element of a list is element 0\.
    For example, if you want to take the mean along the first axis of a 2D array (in
    other words, the column means of a matrix), you would do so like this:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的另一件事是 tf 模块中的函数使用基于 0 的索引计数约定，即列表的第一个元素是元素 0。例如，如果您想沿着 2D 数组的第一个轴（换句话说，矩阵的列均值）取均值，可以这样做：
- en: m <- as_tensor(1:12, shape = c(3, 4))
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: m <- as_tensor(1:12, shape = c(3, 4))
- en: tf$reduce_mean(m, axis = 0L, keepdims = TRUE)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: tf$reduce_mean(m, axis = 0L, keepdims = TRUE)
- en: tf.Tensor([[5 6 7 8]], shape=(1, 4), dtype=int32)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([[5 6 7 8]], shape=(1, 4), dtype=int32)
- en: 'The corresponding R functions, however, use a 1-based counting convention:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的 R 函数，但是使用基于 1 的计数约定：
- en: mean(m, axis = 1, keepdims = TRUE)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: mean(m, axis = 1, keepdims = TRUE)
- en: tf.Tensor([[5 6 7 8]], shape=(1, 4), dtype=int32)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([[5 6 7 8]], shape=(1, 4), dtype=int32)
- en: You can easily access the help for functions in the tf module, right from the
    RStudio IDE. Press F1 while your cursor is over a function in the tf module to
    open a webpage with the corresponding documentation at [www.tensorflow.org](http://www.tensorflow.org).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松地从 RStudio IDE 访问 tf 模块中函数的帮助。在 tf 模块的函数上将光标悬停在 F1 上，即可打开带有相应文档的网页，网址为
    [www.tensorflow.org](http://www.tensorflow.org)。
- en: 3.7.5 Constant tensors and variables
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.5 常量张量和变量
- en: 'A significant difference between R arrays and TensorFlow tensors is that TensorFlow
    tensors aren’t modifiable: they’re constant. For instance, in R, you can do the
    following.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: R 数组和 TensorFlow 张量之间的一个重要区别是 TensorFlow 张量是不可修改的：它们是常量。例如，在 R 中，你可以这样做。
- en: '**Listing 3.1 R arrays are assignable**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.1 R 数组可赋值**'
- en: x <- array(1, dim = c(2, 2))
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: x <- array(1, dim = c(2, 2))
- en: x[1, 1] <- 0
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: x[1, 1] <- 0
- en: 'Try to do the same thing in TensorFlow, and you will get an error: “EagerTensor
    object does not support item assignment.”'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中尝试做同样的事情，你将会得到一个错误：“EagerTensor 对象不支持项目赋值。”
- en: '**Listing 3.2 TensorFlow tensors are not assignable**'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.2 TensorFlow 张量不可赋值**'
- en: x <- as_tensor(1, shape = c(2, 2))
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: x <- as_tensor(1, shape = c(2, 2))
- en: x[1, 1] <- 0➊
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: x[1, 1] <- 0➊
- en: 'Error in `[<-.tensorflow.tensor`(`*tmp*`, 1, 1, value = 0):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'Error in `[<-.tensorflow.tensor`(`*tmp*`, 1, 1, value = 0):'
- en: 'TypeError: ''tensorflow.python.framework.ops.EagerTensor'''
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'TypeError: ''tensorflow.python.framework.ops.EagerTensor'''
- en: object does not support item assignment
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对象不支持项目赋值
- en: ➊ **This will fail, because a tensor isn't modifiable.**
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **这将失败，因为张量不可修改。**
- en: To train a model, we’ll need to update its state, which is a set of tensors.
    If tensors aren’t modifiable, how do we do this? That’s where *variables* come
    in. tf$Variable is the class meant to manage modifiable state in TensorFlow. You’ve
    already briefly seen it in action in the training loop implementation at the end
    of chapter 2.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个模型，我们需要更新它的状态，这是一组张量。如果张量是不可修改的，那么我们该怎么做呢？这就是*变量*发挥作用的地方。tf$Variable 是
    TensorFlow 中用来管理可修改状态的类。你已经在第二章结束时的训练循环实现中简要地看到了它的作用。
- en: To create a variable, you need to provide some initial value, such as a random
    tensor.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个变量，你需要提供一些初始值，比如一个随机张量。
- en: '**Listing 3.3 Creating a TensorFlow variable**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.3 创建 TensorFlow 变量**'
- en: v <- tf$Variable(initial_value = tf$random$normal(shape(3, 1)))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: v <- tf$Variable(initial_value = tf$random$normal(shape(3, 1)))
- en: v
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: v
- en: <tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: <tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=
- en: array([[-1.1629326 ],
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: array([[-1.1629326 ],
- en: '[ 0.53641343],'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.53641343],'
- en: '[ 1.4736737 ]], dtype=float32)>'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 1.4736737 ]], dtype=float32)>'
- en: The state of a variable can be modified in place via its assign method, as follows.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过其 assign 方法就地修改变量的状态，如下所示。
- en: '**Listing 3.4 Assigning a value to a TensorFlow variable**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.4 给 TensorFlow 变量赋值**'
- en: v$assign(tf$ones(shape(3, 1)))
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: v$assign(tf$ones(shape(3, 1)))
- en: <tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: <tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
- en: array([[1.],
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: array([[1.],
- en: '[1.],'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.],'
- en: '[1.]], dtype=float32)>'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.]], dtype=float32)>'
- en: It also works for a subset of the coefficients.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 它也适用于一部分系数。
- en: '**Listing 3.5 Assigning a value to a subset of a TensorFlow variable**'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.5 给 TensorFlow 变量的子集赋值**'
- en: v[1, 1]$assign(3)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: v[1, 1]$assign(3)
- en: <tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: <tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
- en: array([[3.],
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: array([[3.],
- en: '[1.],'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.],'
- en: '[1.]], dtype=float32)>'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.]], dtype=float32)>'
- en: Similarly, assign_add() and assign_sub() are efficient equivalents of x <- x
    + value and x <- x - value.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，assign_add() 和 assign_sub() 是 x <- x + value 和 x <- x - value 的高效等效方法。
- en: '**Listing 3.6 Using assign_add()**'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.6 使用 assign_add()**'
- en: v$assign_add(tf$ones(shape(3, 1)))
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: v$assign_add(tf$ones(shape(3, 1)))
- en: <tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: <tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
- en: array([[4.],
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: array([[4.],
- en: '[2.],'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.],'
- en: '[2.]], dtype=float32)>'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.]], dtype=float32)>'
- en: '3.7.6 Tensor operations: Doing math in TensorFlow'
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.6 张量运算：在 TensorFlow 中进行数学运算
- en: TensorFlow offers a large collection of tensor operations to express mathematical
    formulas. Here are a few examples.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了大量的张量操作来表达数学公式。以下是一些例子。
- en: '**Listing 3.7 A few basic math operations**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.7 几个基本的数学运算**'
- en: a <- tf$ones(c(2L, 2L))
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: a <- tf$ones(c(2L, 2L))
- en: b <- tf$square(a)➊
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: b <- tf$square(a)➊
- en: c <- tf$sqrt(a)➋
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: c <- tf$sqrt(a)➋
- en: d <- b + c➌
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: d <- b + c➌
- en: e <- tf$matmul(a, b)➍
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: e <- tf$matmul(a, b)➍
- en: e <- e * d➎
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: e <- e * d➎
- en: ➊ **Take the square.**
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **进行平方运算。**
- en: ➋ **Take the square root.**
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **进行平方根运算。**
- en: ➌ **Add two tensors (element-wise).**
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **对两个张量进行加法（逐元素）。**
- en: ➍ **Take the product of two tensors (as discussed in chapter 2).**
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **对两个张量进行乘法（如第 2 章中所讨论的）。**
- en: ➎ **Multiply two tensors (element-wise).**
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **对两个张量进行乘法（逐元素）。**
- en: Note that some of these operations are invoked by their corresponding R generics.
    For example, calling sqrt(x) will call tf$sqrt(x) if x is a tensor.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，其中一些操作是通过它们对应的 R 泛型调用的。例如，调用 sqrt(x) 如果 x 是一个张量，就会调用 tf$sqrt(x)。
- en: 'Importantly, each of the preceding operations is executed on the fly: at any
    point, you can print the current result, just like regular R code. We call this
    *eager execution*.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，前面的每个操作都是即时执行的：在任何时候，你都可以打印当前结果，就像普通的 R 代码一样。我们称之为*eager execution*（急切执行）。
- en: 3.7.7 A second look at the GradientTape API
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.7 对 GradientTape API 的第二次查看
- en: 'So far, TensorFlow seems to look a lot like base R, just with different names
    for functions and some different tensor capabilities. But here’s something R can’t
    easily do: retrieve the gradient of any differentiable expression with respect
    to any of its inputs. Just open a tf$GradientTape() scope using with(), apply
    some computation to one or several input tensors, and retrieve the gradient of
    the result with respect to the inputs.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，TensorFlow 看起来很像基本 R，只是函数的名称不同，并且具有一些不同的张量功能。但是，这里有一件 R 不能轻易做到的事情：检索任何可微表达式相对于其输入的梯度。只需使用
    with() 打开一个 tf$GradientTape() 作用域，对一个或多个输入张量应用一些计算，并检索结果相对于输入的梯度。
- en: '**Listing 3.8 Using the GradientTape**'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 3.8 使用 GradientTape**'
- en: input_var <- tf$Variable(initial_value = 3)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: input_var <- tf$Variable(initial_value = 3)
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: result <- tf$square(input_var)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: result <- tf$square(input_var)
- en: '})'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradient <- tape$gradient(result, input_var)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: gradient <- tape$gradient(result, input_var)
- en: 'This is most commonly used to retrieve the gradients of the loss of a model
    with respect to its weights: gradients <- tape$gradient(loss, weights). You saw
    this in action in chapter 2.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于检索模型损失相对于其权重的梯度：gradients <- tape$gradient(loss, weights)。你在第二章中见过这种情况。
- en: So far, you’ve only seen the case where the input tensors in tape$gradient()
    were TensorFlow variables. It’s actually possible for these inputs to be any arbitrary
    tensor. However, only *trainable variables* are tracked by default. With a constant
    tensor, you’d have to manually mark it as being tracked by calling tape$watch()
    on it.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只看到了 tape$gradient() 中输入张量是 TensorFlow 变量的情况。实际上，这些输入可以是任意的张量。但是，默认只跟踪*可训练变量*。对于常数张量，你必须手动调用
    tape$watch() 来标记它为被跟踪的。
- en: '**Listing 3.9 Using GradientTape with constant tensor inputs**'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 3.9 使用带有常数张量输入的 GradientTape**'
- en: input_const <- as_tensor(3)
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: input_const <- as_tensor(3)
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: tape$watch(input_const)
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: tape$watch(input_const)
- en: result <- tf$square(input_const)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: result <- tf$square(input_const)
- en: '})'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: gradient <- tape$gradient(result, input_const)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: gradient <- tape$gradient(result, input_const)
- en: Why is this necessary? Because it would be too expensive to preemptively store
    the information required to compute the gradient of anything with respect to anything.
    To avoid wasting resources, the tape needs to know what to watch. Trainable variables
    are watched by default because computing the gradient of a loss with regard to
    a list of trainable variables is the most common use of the gradient tape.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是必要的呢？因为预先存储与任何内容的梯度计算相关的信息将会太昂贵。为了避免浪费资源，记录必须知道要观察什么。可训练变量默认被观察，因为计算损失相对于可训练变量列表的梯度是梯度记录最常见的用途。
- en: The gradient tape is a powerful utility, even capable of computing *second-order
    gradients*, that is to say, the gradient of a gradient. For instance, the gradient
    of the position of an object with regard to time is the speed of that object,
    and the second-order gradient is its acceleration.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度记录是一个强大的实用工具，甚至可以计算*二阶梯度*，也就是说，梯度的梯度。例如，物体位置相对于时间的梯度是物体的速度，而二阶梯度是它的加速度。
- en: If you measure the position of a falling apple along a vertical axis over time
    and find that it verifies position(time) = 4.9 * time ^ 2, what is its acceleration?
    Let’s use two nested gradient tapes to find out.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你测量一个垂直轴上一个下落的苹果的位置随时间变化，并发现它验证了 position(time) = 4.9 * time ^ 2，那么它的加速度是多少？让我们使用两个嵌套的梯度记录来找出答案。
- en: '**Listing 3.10 Using nested gradient tapes to compute second-order gradients**'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 3.10 使用嵌套梯度记录计算二阶梯度**'
- en: time <- tf$Variable(0)
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: time <- tf$Variable(0)
- en: with(tf$GradientTape() %as% outer_tape, {➊
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% outer_tape, {➊
- en: with(tf$GradientTape() %as% inner_tape, {
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% inner_tape, {
- en: position <- 4.9 * time ^ 2
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: position <- 4.9 * time ^ 2
- en: '})'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: speed <- inner_tape$gradient(position, time)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: speed <- inner_tape$gradient(position, time)
- en: '})'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: acceleration <- outer_tape$gradient(speed, time)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: acceleration <- outer_tape$gradient(speed, time)
- en: acceleration
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 加速度
- en: tf.Tensor(9.8, shape=(), dtype=float32)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(9.8, shape=(), dtype=float32)
- en: ➊ **We use the outer tape to compute the gradient of the gradient from the inner
    tape. Naturally, the answer is 4.9 * 2 = 9.8.**
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们使用外部磁带计算内部磁带的梯度。自然，答案是4.9 * 2 = 9.8。**
- en: '3.7.8 An end-to-end example: A linear classifier in pure TensorFlow'
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.8 一个端到端的例子：一个纯TensorFlow线性分类器
- en: You know about tensors, variables, and tensor operations, and you know how to
    compute gradients. That’s enough to build any machine learning model based on
    gradient descent. And you’re only at chapter 3!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解张量、变量和张量操作，以及如何计算梯度。这足以基于梯度下降构建任何机器学习模型。而你只到第3章！
- en: 'In a machine learning job interview, you may be asked to implement a linear
    classifier from scratch in TensorFlow: a very simple task that serves as a filter
    between candidates who have some minimal machine learning background and those
    who don’t. Let’s get you past that filter and use your newfound knowledge of TensorFlow
    to implement such a linear classifier.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的面试中，可能会要求你在 TensorFlow 中从头开始实现一个线性分类器：这是一个非常简单的任务，为了筛选出那些具有一些最小机器学习背景的候选人和那些没有背景的候选人。让我们通过这个筛选并利用你对
    TensorFlow 的新知识实现这样一个线性分类器。
- en: 'First, let’s come up with some nicely linearly separable synthetic data to
    work with: two classes of points in a 2D plane. We’ll generate each class of points
    by drawing their coordinates from a random distribution with a specific covariance
    matrix and a specific mean. Intuitively, the covariance matrix describes the shape
    of the point cloud, and the mean describes its position in the plane. We’ll reuse
    the same covariance matrix for both point clouds, but we’ll use two different
    mean values—the point clouds will have the same shape, but different positions.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构造一些漂亮的线性可分的合成数据：在2D平面上的两类点。我们将通过从具有特定协方差矩阵和特定均值的随机分布中绘制它们的坐标来生成每个点类。直观地说，协方差矩阵描述了点云的形状，而均值描述了其在平面中的位置。我们将为两个点云重用相同的协方差矩阵，但使用两个不同的均值值——点云将具有相同的形状，但不同的位置。
- en: '**Listing 3.11 Generating two classes of random points in a 2D plane**'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单3.11 在2D平面中生成两类随机点**'
- en: num_samples_per_class <- 1000
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: num_samples_per_class <- 1000
- en: Sigma <- rbind(c(1, 0.5),
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: Sigma <- rbind(c(1, 0.5),
- en: c(0.5, 1))
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: c(0.5, 1))
- en: negative_samples <-
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: negative_samples <-
- en: MASS::mvrnorm(n = num_samples_per_class,➊
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: MASS::mvrnorm(n = num_samples_per_class,➊
- en: mu = c(0, 3), Sigma = Sigma)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: mu = c(0, 3), Sigma = Sigma)
- en: positive_samples <-
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: positive_samples <-
- en: MASS::mvrnorm(n = num_samples_per_class,➋
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: MASS::mvrnorm(n = num_samples_per_class,➋
- en: mu = c(3, 0), Sigma = Sigma)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: mu = c(3, 0), Sigma = Sigma)
- en: '➊ **Generate the first class of points: 1,000 random 2D points. Sigma corresponds
    to an oval-like point cloud oriented from bottom left to top right.**'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **生成第一类点：1000个随机2D点。Sigma对应于从左下角到右上角定向的椭圆形点云。**
- en: ➋ **Generate the other class of points with a different mean and the same covariance
    matrix.**
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **使用不同的平均值和相同的协方差矩阵生成另一类点。**
- en: In the preceding code, negative_samples and positive_samples are both arrays
    with shape (1000, 2). Let’s stack them into a single array with shape (2000, 2).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，负样本和正样本都是形状为（1000，2）的数组。让我们将它们堆叠成一个形状为（2000，2）的单个数组。
- en: '**Listing 3.12 Stacking the two classes into an array with shape (2000, 2)**'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单3.12 将两个类堆叠成形状为（2000,2）的数组**'
- en: inputs <- rbind(negative_samples, positive_samples)
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- rbind(negative_samples, positive_samples)
- en: Let’s generate the corresponding target labels, an array of zeros and ones of
    shape (2000, 1), where targets[i, 1] is 0 if inputs[i] belongs to class 1 (and
    inversely).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成相应的目标标签，一个形状为（2000，1）的零和一的数组，其中当inputs[i]属于类1（反之亦然）时，targets[i，1]为0。
- en: '**Listing 3.13 Generating the corresponding targets (0 and 1)**'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单3.13 生成相应的目标（0和1）**'
- en: targets <- rbind(array(0, dim = c(num_samples_per_class, 1)),
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: targets <- rbind(array(0, dim = c(num_samples_per_class, 1)),
- en: array(1, dim = c(num_samples_per_class, 1)))
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: array(1, dim = c(num_samples_per_class, 1)))
- en: Next, let’s plot our data.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们绘制我们的数据。
- en: '**Listing 3.14 Plotting the two point classes**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单3.14 绘制两个点类**'
- en: plot(x = inputs[, 1], y = inputs[, 2],
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: plot(x = inputs[, 1], y = inputs[, 2],
- en: col = ifelse(targets[, 1] == 0, "purple", "green"))
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: col = ifelse(targets[, 1] == 0, "purple", "green"))
- en: '![Image](../images/f0085-01.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0085-01.jpg)'
- en: Now let’s create a linear classifier that can learn to separate these two blobs.
    A linear classifier is an affine transformation (prediction = W • input + b) trained
    to minimize the square of the difference between predictions and the targets.
    As you’ll see, it’s actually a much simpler example than the end-to-end example
    of a toy two-layer neural network you saw at the end of chapter 2\. However, this
    time you should be able to understand everything about the code, line by line.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个线性分类器，它可以学习将这两个斑块分开。线性分类器是一个仿射变换（预测 = W • 输入 + b），训练目标是最小化预测值与目标值之间的差值的平方。正如您将看到的那样，这实际上比您在第2章末尾看到的玩具两层神经网络的端到端示例要简单得多。但是，这一次您应该能够逐行理解代码的每一部分。
- en: Let’s create our variables, W and b, initialized with random values and with
    zeros, respectively.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的变量W和b，分别初始化为随机值和零值。
- en: '**Listing 3.15 Creating the linear classifier variables**'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.15 创建线性分类器变量**'
- en: input_dim <- 2➊
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: input_dim <- 2➊
- en: output_dim <- 1➋
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: output_dim <- 1➋
- en: W <- tf$Variable(initial_value =
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: W <- tf$Variable(initial_value =
- en: tf$random$uniform(shape(input_dim, output_dim)))
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: tf$random$uniform(shape(input_dim, output_dim)))
- en: b <- tf$Variable(initial_value = tf$zeros(shape(output_dim)))
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: b <- tf$Variable(initial_value = tf$zeros(shape(output_dim)))
- en: ➊ **The inputs will be 2D points.**
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入将是2D点。**
- en: ➋ **The output predictions will be a single score per sample (close to 0 if
    the sample is predicted to be in class 0, and close to 1 if the sample is predicted
    to be in class 1).**
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **输出预测将是每个样本的单个评分（如果样本被预测为类0，则接近0，如果样本被预测为类1，则接近1）。**
- en: Here’s our forward pass function.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的前向传播函数。
- en: '**Listing 3.16 The forward pass function**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.16 前向传播函数**'
- en: model <- function(inputs)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: model <- function(inputs)
- en: tf$matmul(inputs, W) + b
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: tf$matmul(inputs, W) + b
- en: 'Because our linear classifier operates on 2D inputs, W is really just two scalar
    coefficients, w1 and w2: W = [[w1], [w2]]. Meanwhile, b is a single scalar coefficient.
    As such, for a given input point [x, y], its prediction value is prediction =
    [[w1], [w2]] • [x, y] + b = w1 * x + w2 * y + b.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的线性分类器是在2D输入上运行的，W实际上只是两个标量系数，w1和w2：W = [[w1], [w2]]。与此同时，b是一个单一的标量系数。因此，对于给定的输入点[x,
    y]，其预测值为prediction = [[w1], [w2]] • [x, y] + b = w1 * x + w2 * y + b。
- en: The following listing shows our loss function.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 以下清单显示了我们的损失函数。
- en: '**Listing 3.17 The mean squared error loss function**'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.17 均方误差损失函数**'
- en: square_loss <- function(targets, predictions) {
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: square_loss <- function(targets, predictions) {
- en: per_sample_losses <- (targets - predictions)^2➊
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: per_sample_losses <- (targets - predictions)^2➊
- en: mean(per_sample_losses)➋
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: mean(per_sample_losses)➋
- en: '}'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **per_sample_losses will be a tensor with the same shape as targets and predictions,
    containing per-sample loss scores.**
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **per_sample_losses将是一个与targets和predictions相同形状的张量，包含每个样本的损失评分。**
- en: '➋ **We need to average these per-sample loss scores into a single scalar loss
    value: this is what mean() does.**'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **我们需要将每个样本的损失评分平均为单个标量损失值：这就是mean()的作用。**
- en: 'Note that in square_loss(), both targets and predictions can be tensors, but
    they don’t have to be. This is one of the niceties of the R interface—generics
    like mean(), ^, and - allow you to write the same code for tensors as you would
    for R arrays. When targets and predictions are tensors, the generics will dispatch
    to functions in the tf module. We can also write the equivalent square_loss using
    functions from the tf module directly:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在square_loss()中，targets和predictions都可以是张量，但不一定是。这是R接口的一个好处——像mean()、^和-这样的通用函数使您可以为张量写与R数组相同的代码。当targets和predictions是张量时，通用函数将派发到tf模块中的函数。我们也可以直接使用tf模块的函数编写等效的square_loss：
- en: square_loss <- function(targets, predictions) {
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: square_loss <- function(targets, predictions) {
- en: per_sample_losses <- tf$square(tf$subtract(targets, predictions))
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: per_sample_losses <- tf$square(tf$subtract(targets, predictions))
- en: tf$reduce_mean(per_sample_losses)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: tf$reduce_mean(per_sample_losses)
- en: '}'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Next is the training step, which receives some training data and updates the
    weights W and b so as to minimize the loss on the data.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是��练步骤，它接收一些训练数据，并更新权重W和b，以使数据上的损失最小化。
- en: '**Listing 3.18 The training step function**'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.18 训练步骤函数**'
- en: learning_rate <- 0.1
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 <- 0.1
- en: training_step <- function(inputs, targets) {
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: training_step <- function(inputs, targets) {
- en: with(tf$GradientTape() %as% tape, {
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: with(tf$GradientTape() %as% tape, {
- en: predictions <- model(inputs)➊
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model(inputs)➊
- en: loss <- square_loss(predictions, targets)
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- square_loss(predictions, targets)
- en: '})'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: grad_loss_wrt <- tape$gradient(loss, list(W = W, b = b))➋
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: grad_loss_wrt <- tape$gradient(loss, list(W = W, b = b))➋
- en: W$assign_sub(grad_loss_wrt$W * learning_rate)➌
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: W$assign_sub(grad_loss_wrt$W * 学习率)➌
- en: b$assign_sub(grad_loss_wrt$b * learning_rate)➌
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: b$assign_sub(grad_loss_wrt$b * learning_rate)➌
- en: loss
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 损失
- en: '}'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Forward pass, inside a gradient tape scope**
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在梯度带范围内进行前向传播**
- en: ➋ **Retrieve the gradient of the loss with regard to weights.**
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **检索损失相对于权重的梯度。**
- en: ➌ **Update the weights.**
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **更新权重。**
- en: 'For simplicity, we’ll do *batch training* instead of *mini-batch training*:
    we’ll run each training step (gradient computation and weight update) for all
    the data, rather than iterate over the data in small batches. On one hand, this
    means that each training step will take much longer to run, because we’ll compute
    the forward pass and the gradients for 2,000 samples at once. On the other hand,
    each gradient update will be much more effective at reducing the loss on the training
    data, because it will encompass information from all training samples instead
    of, say, only 128 random samples. As a result, we will need many fewer steps of
    training, and we should use a larger learning rate than we would typically use
    for mini-batch training (we’ll use learning_rate = 0.1, defined in listing 3.18).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将进行*批量训练*而不是*小批量训练*：我们将对所有数据运行每个训练步骤（梯度计算和权重更新），而不是在小批量中迭代数据。一方面，这意味着每个训练步骤将花费更长的时间运行，因为我们将一次计算2000个样本的前向传播和梯度。另一方面，每个梯度更新将更有效地减少训练数据的损失，因为它将涵盖所有训练样本的信息，而不是仅仅128个随机样本。因此，我们将需要更少的训练步骤，并且应该使用比我们通常用于小批量训练的学习率更大的学习率（我们将使用在列表3.18中定义的learning_rate
    = 0.1）。
- en: '**Listing 3.19 The batch training loop**'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表3.19 批量训练循环**'
- en: inputs <- as_tensor(inputs, dtype = "float32")
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- as_tensor(inputs, dtype = "float32")
- en: for (step in seq(40)) {
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: for (step in seq(40)) {
- en: loss <- training_step(inputs, targets)
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: loss <- training_step(inputs, targets)
- en: 'cat(sprintf("Loss at step %s: %.4f\n", step, loss))'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("第%s步的损失：%.4f\n", step, loss))
- en: '}'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Loss at step 1: 0.7263'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步的损失：0.7263
- en: 'Loss at step 2: 0.0911'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步的损失：0.0911
- en: …
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Loss at step 39: 0.0271'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 第39步的损失：0.0271
- en: 'Loss at step 40: 0.0269'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 第40步的损失：0.0269
- en: 'After 40 steps, the training loss seems to have stabilized around 0.025\. Let’s
    plot how our linear model classifies the training data points. Because our targets
    are zeros and ones, a given input point will be classified as 0 if its prediction
    value is below 0.5, and as 1 if it is above 0.5:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 经过40个步骤，训练损失似乎在0.025左右稳定下来。让我们绘制一下我们的线性模型是如何对训练数据点进行分类的。因为我们的目标是零和一，所以如果一个给定的输入点的预测值低于0.5，它将被分类为0，如果高于0.5则被分类为1：
- en: predictions <- model(inputs)➊
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model(inputs)➊
- en: inputs <- as.array(inputs)➊
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- as.array(inputs)➊
- en: predictions <- as.array(predictions)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- as.array(predictions)
- en: plot(inputs[, 1], inputs[, 2],
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制(inputs[, 1], inputs[, 2],
- en: col = ifelse(predictions[, 1] <= 0.5, "purple", "green"))
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: col = ifelse(predictions[, 1] <= 0.5, "purple", "green"))
- en: ➊ Convert tensors to R arrays for plotting.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 将张量转换为R数组以进行绘图。
- en: '![Image](../images/f0088-01.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0088-01.jpg)'
- en: 'Recall that the prediction value for a given point [x, y] is simply prediction
    == [[w1], [w2]] • [x, y] + b == w1 * x + w2 * y + b. Thus, class 1 is defined
    as (w1 * x + w2 * y + b) < 0.5, and class 2 is defined as (w1 * x + w2 * y + b)
    > 0.5. You’ll notice that what you’re looking at is really the equation of a line
    in the 2D plane: w1 * x + w2 * y + b = 0.5. Above the line is class 1, and below
    the line is class 0\. You may be used to seeing line equations in the format y
    = a * x + b; in the same format, our line becomes y = - w1 / w2 * x + (0.5 - b)
    / w2.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，给定点[x, y]的预测值简单地是预测 == [[w1], [w2]] • [x, y] + b == w1 * x + w2 * y + b。因此，类1被定义为(w1
    * x + w2 * y + b) < 0.5，类2被定义为(w1 * x + w2 * y + b) > 0.5。您会注意到，您所看到的实际上是二维平面上的一条线的方程：w1
    * x + w2 * y + b = 0.5。在线的上方是类1，在线的下方是类0。您可能习惯了以y = a * x + b的形式看到线方程；在相同的格式中，我们的线变成了y
    = - w1 / w2 * x + (0.5 - b) / w2。
- en: 'Let’s plot this line:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这条线：
- en: plot(x = inputs[, 1], y = inputs[, 2],
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制(x = inputs[, 1], y = inputs[, 2],
- en: col = ifelse(predictions[, 1] <= 0.5, "purple", "green"))➊
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: col = ifelse(predictions[, 1] <= 0.5, "purple", "green"))➊
- en: slope <- -W[1, ] / W[2, ]➋
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率 <- -W[1, ] / W[2, ]➋
- en: intercept <- (0.5 - b) / W[2, ]➋
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 截距 <- (0.5 - b) / W[2, ]➋
- en: abline(as.array(intercept), as.array(slope), col = "red")➌
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: abline(as.array(intercept), as.array(slope), col = "red")➌
- en: ➊ **Plot our model's predictions.**
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **绘制我们模型的预测。**
- en: ➋ **These are our line's equation values.**
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **这些是我们线的方程值。**
- en: ➌ **Plot our line.**
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **绘制我们的线。**
- en: '![Image](../images/f0089-01.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0089-01.jpg)'
- en: 'This is really what a linear classifier is all about: finding the parameters
    of a line (or, in higher-dimensional spaces, a hyperplane) neatly separating two
    classes of data.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的是线性分类器的全部意义所在：找到一个线（或者在更高维空间中，一个超平面）的参数，将两类数据整齐地分开。
- en: '3.8 Anatomy of a neural network: Understanding core Keras APIs'
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.8 神经网络概览：理解核心 Keras APIs
- en: 'At this point, you know the basics of TensorFlow, and you can use it to implement
    a toy model from scratch, such as the batch linear classifier in the previous
    section, or the toy neural network at the end of chapter 2\. That’s a solid foundation
    to build upon. It’s now time to move on to a more productive, more robust path
    to deep learning: the Keras API.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了 TensorFlow 的基础知识，并可以使用它从头开始实现一个玩具模型，例如前一节中的批量线性分类器，或第 2 章末尾的玩具神经网络。这为你打下了坚实的基础。现在是时候转向更高效、更可靠的深度学习路径了：Keras
    API。
- en: '3.8.1 Layers: The building blocks of deep learning'
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '3.8.1 Layers: Deep learning 的构建模块'
- en: 'The fundamental data structure in neural networks is the *layer*, to which
    you were introduced in chapter 2\. A layer is a data-processing module that takes
    as input one or more tensors and that outputs one or more tensors. Some layers
    are stateless, but more frequently layers have a state: the layer’s *weights*,
    one or several tensors learned with stochastic gradient descent, which together
    contain the network’s *knowledge*.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的基本数据结构是*层*，你在第 2 章中已经介绍过它。层是一个数据处理模块，它接受一个或多个张量作为输入，并输出一个或多个张量。有些层是无状态的，但更常见的是层具有状态：层的*权重*，一个或多个通过随机梯度下降学到的张量，它们一起包含了网络的*知识*。
- en: Different types of layers are appropriate for different tensor formats and different
    types of data processing. For instance, simple vector data, stored in rank 2 tensors
    of shape (samples, features), is often processed by *densely connected* layers,
    also called *fully connected* or *dense* layers (built by the layer_dense() function
    in Keras). Sequence data, stored in rank 3 tensors of shape (samples, timesteps,
    features), is typically processed by *recurrent* layers, such as an LSTM layer
    (layer_lstm()), or 1D convolution layers (layer_conv_1d()). Image data, stored
    in rank 4 tensors, is usually processed by 2D convolution layers (layer_conv_2d()).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的层适用于不同的张量格式和不同类型的数据处理。例如，简单的向量数据，以 (samples, features) 形状的 rank 2 张量存储，通常通过*密集连接*层（由
    Keras 中的 layer_dense() 函数构建）进行处理。序列数据，以 (samples, timesteps, features) 形状的 rank
    3 张量存储，通常通过*循环*层进行处理，例如 LSTM 层（layer_lstm()）或 1D 卷积层（layer_conv_1d()）。图像数据以 rank
    4 张量存储，通常通过二维卷积层（layer_conv_2d()）进行处理。
- en: You can think of layers as the LEGO bricks of deep learning, a metaphor that
    is made explicit by Keras. Building deep learning models in Keras is done by clipping
    together compatible layers to form useful data-transformation pipelines.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将层想象为深度学习中的乐高积木，这个比喻在 Keras 中得到了明确的体现。在 Keras 中构建深度学习模型是通过将兼容的层裁剪在一起形成有用的数据转换流程。
- en: '**THE LAYER CLASS IN KERAS**'
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**KERAS 中的 LAYER CLASS**'
- en: A simple API should have a single abstraction around which everything is centered.
    In Keras, that’s the Layer class. Everything in Keras is either a Layer or something
    that closely interacts with a Layer.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的 API 应该围绕一个抽象概念展开。在 Keras 中，这个抽象概念就是 Layer 类。在 Keras 中，每个东西都是一个 Layer，或者与
    Layer 密切交互的东西。
- en: A Layer is an object that encapsulates some state (weights) and some computation
    (a forward pass). The weights are typically defined in a build() method (although
    they could also be created in the initialize() method), and the computation is
    defined in the call() method.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: Layer 是一个封装了一些状态（权重）和计算操作（前向传播）的对象。权重通常在 build() 方法中定义（虽然它们也可以在 initialize()
    方法中创建），计算操作在 call() 方法中定义。
- en: In the previous chapter, we implemented a layer_naive_dense() that contained
    two weights, W and b, and applied the computation output = activation(dot(input,
    W) + b). This is what the same layer would look like in Keras.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们实现了一个包含两个权重 W 和 b，并应用计算 output = activation(dot(input, W) + b) 的 layer_naive_dense()。这就是在
    Keras 中同一层的样子。
- en: '**Listing 3.20 Implementing a dense layer as a Keras Layer class**'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '**列出 3.20 以一个 Keras Layer 类实现密集层**'
- en: layer_simple_dense <- new_layer_class(
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense <- new_layer_class(
- en: classname = "SimpleDense",
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "SimpleDense",
- en: initialize = function(units, activation = NULL) {
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(units, activation = NULL) {
- en: super$initialize()
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize()
- en: self$units <- as.integer(units)
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: self$units <- as.integer(units)
- en: self$activation <- activation
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: self$activation <- activation
- en: '},'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: build = function(input_shape) {➊
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: build = function(input_shape) {➊
- en: input_dim <- input_shape[length(input_shape)]➋
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: input_dim <- input_shape[length(input_shape)]➋
- en: self$W <- self$add_weight(
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: self$W <- self$add_weight(
- en: shape = c(input_dim, self$units),➌
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: shape = c(input_dim, self$units),➌
- en: initializer = "random_normal")
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: initializer = "random_normal")
- en: self$b <- self$add_weight(
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: self$b <- self$add_weight(
- en: shape = c(self$units),
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: shape = c(self$units),
- en: initializer = "zeros")
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: initializer = "zeros")
- en: '},'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: call = function(inputs) {➍
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs) {➍
- en: y <- tf$matmul(inputs, self$W) + self$b
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: y <- tf$matmul(inputs, self$W) + self$b
- en: if (!is.null(self$activation))
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: if (!is.null(self$activation))
- en: y <- self$activation(y)
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: y <- self$activation(y)
- en: y
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: y
- en: '}'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Weight creation takes place in the build() method.**
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **权重的创建发生在 build() 方法中。**
- en: ➋ **Take the last dim.**
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **取最后一个维度。**
- en: '➌ **add_weight() is a shortcut method for creating weights. It is also possible
    to create standalone variables and assign them as layer attributes, like this:
    self$W < - tf$Variable(tf$random$normal(w_shape)).**'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **add_weight() 是创建权重的快捷方法。也可以创建独立的变量，并将它们分配为层属性，就像这样：self$W < - tf$Variable(tf$random$normal(w_shape)).**
- en: ➍ **We define the forward pass computation in the call() method.**
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **我们在 call() 方法中定义了前向传播计算。**
- en: This time, instead of building up an empty R environment, we use the new_layer_
    class() function provided by Keras. new_layer_class() returns a layer instance
    generator, just like layer_naive_dense() in chapter 2, but it also provides some
    additional convenient features for us (like composability with %>%, which we’ll
    cover in a moment).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们不是构建一个空的 R 环境，而是使用 Keras 提供的 new_layer_class() 函数。new_layer_class() 返回一个层实例生成器，就像第二章中的
    layer_naive_dense() 一样，但它还为我们提供了一些额外的方便功能（比如与 %>%(管道操作符) 的组合，我们稍后会介绍）。
- en: In the next section, we’ll cover in detail the purpose of these build() and
    call() methods. Don’t worry if you don’t understand everything just yet!
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将详细介绍这些 build() 和 call() 方法的目的。如果你现在还不理解一切，不用担心！
- en: 'Layers can be instantiated simply by calling a Keras function that starts with
    the layer_ prefix. Then, once instantiated, a layer instance can be used just
    like a function, taking as input a TensorFlow tensor:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 层可以通过调用以 layer_ 前缀开头的 Keras 函数来实例化。然后，一旦实例化，层实例就可以像函数一样使用，以 TensorFlow 张量作为输入：
- en: my_dense <- layer_simple_dense(units = 32,➊
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: my_dense <- layer_simple_dense(units = 32,➊
- en: activation = tf$nn$relu)
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: activation = tf$nn$relu)
- en: input_tensor <- as_tensor(1, shape = c(2, 784))➋
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: input_tensor <- as_tensor(1, shape = c(2, 784))➋
- en: output_tensor <- my_dense(input_tensor)➌
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: output_tensor <- my_dense(input_tensor)➌
- en: output_tensor$shape
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: output_tensor$shape
- en: TensorShape([2, 32])
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: TensorShape([2, 32])
- en: ➊ **Instantiate our layer, defined previously.**
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **实例化我们之前定义的层。**
- en: ➋ **Create some test inputs.**
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **创建一些测试输入。**
- en: ➌ **Call the layer on the inputs, just like a function.**
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **在输入上调用该层，就像调用函数一样。**
- en: You’re probably wondering, why did we have to implement call() and build(),
    because we ended up using our layer by plainly calling it? It’s because we want
    to be able to create the state just in time. Let’s see how that works.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们要实现 call() 和 build() 方法，因为我们最终是通过直接调用它来使用我们的层的？这是因为我们想要能够及时创建状态。让我们看看它是如何工作的。
- en: '**AUTOMATIC SHAPE INFERENCE: BUILDING LAYERS ON THE FLY**'
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自动形状推断：即时构建层**'
- en: 'Just like with LEGO bricks, you can only “clip” together layers that are compatible.
    The notion of *layer compatibility* here refers specifically to the fact that
    every layer will accept only input tensors of a certain shape and will return
    output tensors of a certain shape. Consider the following example:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 就像玩乐高积木一样，你只能“clip”（夹合）那些兼容的层在一起。这里的 *层兼容性* 概念特指每个层只接受特定形状的输入张量，并返回特定形状的输出张量。考虑以下示例：
- en: layer <- layer_dense(units = 32, activation = "relu")➊
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: layer <- layer_dense(units = 32, activation = "relu")➊
- en: ➊ **A dense layer with 32 output units**
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **具有 32 个输出单元的密集层**
- en: This layer will return a tensor where the first dimension has been transformed
    to be 32\. It can only be connected to a downstream layer that expects 32-dimensional
    vectors as its input.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 此层将返回一个张量，其中第一个维度已被转换为 32。它只能连接到期望其输入为 32 维向量的下游层。
- en: 'When using Keras, you don’t have to worry about size compatibility most of
    the time, because the layers you add to your models are dynamically built to match
    the shape of the incoming layer. For instance, suppose you write the following:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras 时，大多数情况下你不必担心大小兼容性，因为你添加到模型中的层会动态构建以匹配传入层的形状。例如，假设你写了以下代码：
- en: model <- keras_model_sequential(list(
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential(list(
- en: layer_dense(units = 32, activation = "relu"),
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 32, activation = "relu"),
- en: layer_dense(units = 32)
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 32)
- en: ))
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: 'The layers didn’t receive any information about the shape of their inputs—instead,
    they automatically inferred their input shape as being the shape of the first
    inputs they see. In the toy version of the dense layer we implemented in chapter
    2 (which we named layer_naive_dense()), we had to pass the layer’s input size
    explicitly to the constructor to be able to create its weights. That’s not ideal,
    because it would lead to models that look like the following code snippet, where
    each new layer needs to be made aware of the shape of the layer before it:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图层没有收到任何关于其输入形状的信息——相反，它们自动推断其输入形状为它们看到的第一个输入的形状。在我们在第 2 章中实现的密集层的玩具版本（我们命名为
    layer_naive_dense()）中，我们必须显式地将层的输入大小传递给构造函数，以便能够创建其权重。这不是理想的，因为它会导致模型看起来像下面的代码片段，其中每个新图层都需要知道其之前图层的形状：
- en: model <- model_naive_sequential(list(
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: model <- model_naive_sequential(list(
- en: layer_naive_dense(input_size = 784, output_size = 32,
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: layer_naive_dense(input_size = 784, output_size = 32,
- en: activation = "relu"),
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: activation = "relu"),
- en: layer_naive_dense(input_size = 32, output_size = 64,
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: layer_naive_dense(input_size = 32, output_size = 64,
- en: activation = "relu"),
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: activation = "relu"),
- en: layer_naive_dense(input_size = 64, output_size = 32,
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: layer_naive_dense(input_size = 64, output_size = 32，
- en: activation = "relu"),
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: activation = "relu"),
- en: layer_naive_dense(input_size = 32, output_size = 10,
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: layer_naive_dense(input_size = 32, output_size = 10,
- en: activation = "softmax") ))
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: activation = "softmax") ))
- en: It would be even worse if the rules used by a layer to produce its output shape
    are complex. For instance, what if our layer returned outputs of shape if (input_size
    %% 2 == 0) c(batch, input_size * 2) else c(input_size * 3)?
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个图层用于生成其输出形状的规则很复杂，情况会更糟。例如，如果我们的图层返回形状为 if (input_size %% 2 == 0) c(batch,
    input_size * 2) else c(input_size * 3) 的输出，会更糟。
- en: If we were to reimplement layer_naive_dense() as a Keras layer capable of automatic
    shape inference, it would look like the previous layer_simple_dense() layer (see
    listing 3.20), with its build() and call() methods.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要将 layer_naive_dense() 重新实现为一个能够进行自动形状推断的 Keras 图层，它会看起来像之前的 layer_simple_dense()
    图层（见清单 3.20），具有其 build() 和 call() 方法。
- en: In layer_simple_dense(), we no longer create weights in the constructor like
    in the layer_naive_dense() example; instead, we create them in a dedicated state-creation
    method, build(), which receives as an argument the first input shape seen by the
    layer. The build() method is called automatically the first time the layer is
    called. In fact, the function that’s actually called when you call a layer is
    not call() directly but something that optionally first calls build() before calling
    call().
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在 layer_simple_dense() 中，我们不再像 layer_naive_dense() 示例中在构造函数中创建权重；相反，我们在一个专门的状态创建方法
    build() 中创建它们，该方法接收图层看到的第一个输入形状作为参数。第一次调用图层时，build() 方法会自动调用。实际上，在调用图层时实际调用的函数不是直接调用
    call()，而是可选地首先调用 build() 然后调用 call()。
- en: 'The function that’s called when you call a layer schematically looks like this:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 调用图层时实际调用的函数原理上看起来像这样：
- en: layer <- function(inputs) {
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: layer <- function(inputs) {
- en: if(!self$built) {
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: if(!self$built) {
- en: self$build(inputs$shape)
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: self$build(inputs$shape)
- en: self$built <- TRUE
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: self$built <- TRUE
- en: '}'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self$call(inputs)
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: self$call(inputs)
- en: '}'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'With automatic shape inference, our previous example becomes simple and neat:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动形状推断，我们之前的例子变得简单而整洁：
- en: model <- keras_model_sequential(list(
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential(list(
- en: layer_simple_dense(units = 32, activation = "relu"),
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(units = 32, activation = "relu"),
- en: layer_simple_dense(units = 64, activation = "relu"),
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(units = 64, activation = "relu"),
- en: layer_simple_dense(units = 32, activation = "relu"),
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(units = 32, activation = "relu"),
- en: layer_simple_dense(units = 10, activation = "softmax")
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(units = 10, activation = "softmax")
- en: ))
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: 'Note that automatic shape inference is not the only thing that the layer class
    handles. It takes care of many more things, in particular routing between *eager*
    and *graph* execution (a concept you’ll learn about in chapter 7) and input masking
    (which we’ll cover in chapter 11). For now, just remember: when implementing your
    own layers, put the forward pass in the call() method.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，自动形状推断并不是图层类处理的唯一事项。它处理更多事情，特别是 *eager* 和 *graph* 执行之间的路由（你将在第 7 章学到的概念）以及输入掩码（我们将在第
    11 章介绍）。现在，只需记住：在实现自己的图层时，将正向传播放在 call() 方法中。
- en: '**Composing layers with %>% (the pipe operator)**'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 %>%（管道操作符）组合图层**'
- en: Although you can create layer instances directly and manipulate them, most often,
    all you will want to do is to compose the new layer instance with something, like
    a sequential model. For this reason, the first argument to all the layer_ generator
    functions is object. If object is supplied, then a new layer instance is created
    and then immediately composed with object.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以直接创建图层实例并对其进行操作，但大多数情况下，你只需要做的是将新的图层实例与某些东西组合在一起，比如一个序贯模型。因此，所有图层生成函数的第一个参数都是对象。如果提供了对象，则会创建一个新的图层实例，然后立即与对象组合在一起。
- en: 'Previously we built the keras_model_sequential() by passing it a list of layers,
    but we can also build up a model by adding one layer at a time:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们通过传递图层列表来构建keras_model_sequential()，但我们也可以逐层添加图层来构建模型：
- en: model <- keras_model_sequential()
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential()
- en: layer_simple_dense(model, 32, activation = "relu")
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(model, 32, activation = "relu")
- en: layer_simple_dense(model, 64, activation = "relu")
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(model, 64, activation = "relu")
- en: layer_simple_dense(model, 32, activation = "relu")
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(model, 32, activation = "relu")
- en: layer_simple_dense(model, 10, activation = "softmax")
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(model, 10, activation = "softmax")
- en: 'Here, because model is supplied as the first argument to layer_simple_dense(),
    the layer is constructed and then composed with the model (by calling model$add(layer)).
    Note that model is modified in place—we don’t need to save the output of our calls
    to layer_simple_dense() when composing layers this way:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，由于model被作为第一个参数提供给layer_simple_dense()，该图层被构建然后与模型组合在一起（通过调用model$add(layer)）。注意，模型会被就地修改——在以这种方式组合图层时，我们不需要保存对layer_simple_dense()的调用的输出：
- en: length(model$layers)
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: model$layers的长度
- en: '[1] 4'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 4'
- en: One subtlety is that when the layer constructor composes a layer with object,
    it returns the result of the composition, not the layer instance. Thus, if a keras_model_
    sequential() is supplied as the first argument, the same model is also returned,
    except now with one additional layer.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 一个微妙之处在于，当图层构造函数将图层与对象合成时，它会返回合成的结果，而不是图层实例。���此，如果把一个keras_model_sequential()作为第一个参数提供，也会返回相同的模型，除了现在多了一个图层。
- en: This means you can use the pipe (%>%) operator to add layers to a sequential
    model. This operator comes from the magrittr package; it’s shorthand for passing
    the value on its left as the first argument to the function on its right.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你可以使用管道（%>%）运算符将图层添加到序贯模型中。这个操作符来自magrittr包；它是将左侧的值作为右侧函数的第一个参数传递的简写。
- en: 'You can use %>% with Keras like this:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样在Keras中使用%>%：
- en: model <- keras_model_sequential() %>%
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_simple_dense(32, activation = "relu") %>%
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(32, activation = "relu") %>%
- en: layer_simple_dense(64, activation = "relu") %>%
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(64, activation = "relu") %>%
- en: layer_simple_dense(32, activation = "relu") %>%
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(32, activation = "relu") %>%
- en: layer_simple_dense(10, activation = "softmax")
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: layer_simple_dense(10, activation = "softmax")
- en: What’s the difference between this, and calling keras_model_sequential() with
    a list of layers? There is none—with both approaches you end up with the same
    model.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 这和用一个图层列表调用keras_model_sequential()有什么区别？没有——用两种方法都会得到相同的模型。
- en: Using %>% results in code that’s more readable and compact, so we’ll use this
    form throughout the book. If you’re using RStudio, you can insert %>% using the
    Ctrl-Shift-M keyboard shortcut. To learn more about the pipe operator, see [http://r4ds.had.co.nz/pipes.html](http://r4ds.had.co.nz/pipes.html).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 使用%>%会导致更易读和更简洁的代码，所以我们将在整本书中采用这种形式。如果你使用RStudio，可以使用Ctrl-Shift-M键盘快捷键插入%>%。要了解更多关于管道运算符的信息，请参见[http://r4ds.had.co.nz/pipes.html](http://r4ds.had.co.nz/pipes.html)。
- en: 3.8.2 From layers to models
  id: totrans-538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.2 从图层到模型
- en: 'A deep learning model is a graph of layers. In Keras, that’s the Model type.
    Until now, you’ve only seen sequential models, which are simple stacks of layers,
    mapping a single input to a single output. But as you move forward, you’ll be
    exposed to a much broader variety of network topologies. These are some common
    ones:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型是图层的图形。在Keras中，这是Model类型。到目前为止，你只看到过序贯模型，它们是简单的图层堆叠，将单个输入映射到单个输出。但随着你向前迈进，你将接触到更广泛的网络拓扑结构。这些是一些常见的拓扑结构：
- en: Two-branch networks
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个分支网络
- en: Multihead network
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头网络
- en: Residual connection
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差连接
- en: Network topology can get quite involved. For instance, [figure 3.2](#fig3-2)
    shows the topology of the graph of layers of a Transformer, a common architecture
    designed to process text data.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑结构可能变得相当复杂。例如，[图3.2](#fig3-2)显示了Transformer的图层图形的拓扑结构，它是一种用于处理文本数据的常见架构。
- en: '![Image](../images/f0094-01.jpg)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0094-01.jpg)'
- en: '**Figure 3.2 The Transformer architecture (covered in chapter 11). There’s
    a lot going on here. Throughout the next few chapters, you’ll climb your way up
    to understanding it.**'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3.2 transformers架构（在第11章中介绍）。这里面有很多内容。在接下来的几章中，您将逐步理解它。**'
- en: 'There are generally two ways of building such models in Keras: you could directly
    define a new_model_class(), or you could use the Functional API, which lets you
    do more with less code. We’ll cover both approaches in chapter 7.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中通常有两种构建这种模型的方法：您可以直接定义一个新的`model_class()`，或者您可以使用功能 API，这样您就可以用更少的代码做更多的事情。我们将在第7章中涵盖这两种方法。
- en: The topology of a model defines a *hypothesis space*. You may remember that
    in chapter 1 we described machine learning as searching for useful representations
    of some input data, within a predefined *space of possibilities*, using guidance
    from a feedback signal. By choosing a network topology, you constrain your space
    of possibilities (hypothesis space) to a specific series of tensor operations,
    mapping input data to output data. What you’ll then be searching for is a good
    set of values for the weight tensors involved in these tensor operations.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的拓扑结构定义了一个*假设空间*。您可能还记得，在第1章中我们将机器学习描述为在预定义的*可能性空间*内寻找一些输入数据的有用表示，使用来自反馈信号的指导。通过选择网络拓扑结构，您将您的可能性空间（假设空间）限制为一系列将输入数据映射到输出数据的特定张量操作。接下来您将要搜索的是这些张量操作中涉及的权重张量的一组良好值。
- en: To learn from data, you have to make assumptions about it. These assumptions
    define what can be learned. As such, the structure of your hypothesis space—the
    architecture of your model—is extremely important. It encodes the assumptions
    you make about your problem—the prior knowledge that the model starts with. For
    instance, if you’re working on a two-class classification problem with a model
    made of a single layer_dense() with no activation (a pure affine transformation),
    you are assuming that your two classes are linearly separable.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 要从数据中学习，您必须对其进行假设。这些假设定义了可以学到什么。因此，您的假设空间的结构——您模型的架构——非常重要。它编码了您对问题的假设——模型开始时具有的先验知识。例如，如果您正在处理一个由单个无激活函数（纯仿射变换）的`layer_dense()`组成的模型的二分类问题，则您假设您的两个类是线性可分的。
- en: Picking the right network architecture is more an art than a science, and although
    you can rely on some best practices and principles, only practice can help you
    become a proper neural network architect. The next few chapters will both teach
    you explicit principles for building neural networks and help you develop intuition
    as to what works or doesn’t work for specific problems. You’ll build a solid intuition
    about what type of model architectures work for different kinds of problems, how
    to build these networks in practice, how to pick the right learning configuration,
    and how to tweak a model until it yields the results you want to see.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的网络架构更多地是一门艺术而不是科学，尽管您可以依靠一些最佳实践和原则，但只有实践才能帮助您成为一名合格的神经网络架构师。接下来的几章将教给您构建神经网络的显式原则，并帮助您培养对于什么对于特定问题有效或无效的直觉。您将建立对于不同种类问题适用的模型架构的坚实直觉，以及如何在实践中构建这些网络、如何选择正确的学习配置，以及如何调整模型直至产生您想要看到的结果。
- en: '3.8.3 The “compile” step: Configuring the learning process'
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.3 “编译”步骤：配置学习过程
- en: 'Once the model architecture is defined, you still have to choose three more
    things:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型架构被定义，您仍然需要选择另外三个东西：
- en: '*Loss function (objective function)*—The quantity that will be minimized during
    training. It represents a measure of success for the task at hand.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数（目标函数）* — 训练过程中将被最小化的数量。它代表了当前任务的成功度量。'
- en: '*Optimizer*—Determines how the network will be updated based on the loss function.
    It implements a specific variant of stochastic gradient descent (SGD).'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器* — 根据损失函数确定网络将如何更新。它实现了随机梯度下降（SGD）的特定变体。'
- en: '*Metrics*—The measures of success you want to monitor during training and validation,
    such as classification accuracy. Unlike the loss, training will not optimize directly
    for these metrics. As such, metrics don’t need to be differentiable.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*度量* — 训练和验证过程中要监视的成功指标，例如分类准确率。与损失不同，训练不会直接为这些度量优化。因此，度量不需要可微分。'
- en: Once you’ve picked your loss, optimizer, and metrics, you can use the compile()
    and fit() methods to start training your model. Alternatively, you could also
    write your own custom training loops—we’ll cover how to do this in chapter 7\.
    It’s a lot more work! For now, let’s take a look at compile() and fit().
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 选择了损失函数、优化器和指标之后，您可以使用 compile() 和 fit() 方法开始训练模型。或者，您也可以编写自己的自定义训练循环——我们将在第七章中介绍如何做到这一点。这要花费更多的工作！现在，让我们来看看
    compile() 和 fit()。
- en: 'The compile() method configures the training process—you’ve already been introduced
    to it in your very first neural network example in chapter 2\. It takes the arguments
    optimizer, loss, and metrics:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: compile() 方法配置训练过程——在第二章的第一个神经网络示例中，您已经了解了它。它接受优化器、损失函数和指标作为参数：
- en: model <- keras_model_sequential() %>% layer_dense(1)➊
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>% layer_dense(1)➊
- en: model %>% compile(optimizer = "rmsprop",➋
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",➋
- en: loss = "mean_squared_error",➌
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mean_squared_error",➌
- en: metrics = "accuracy")➍
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")➍
- en: ➊ **Define a linear classifier.**
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **定义线性分类器。**
- en: '➋ **Specify the optimizer by name: RMSprop (case-insensitive).**'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **使用名称指定优化器：RMSprop（不区分大小写）。**
- en: '➌ **Specify the loss by name: mean squared error.**'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **使用名称指定损失函数：均方误差。**
- en: '➍ **Specify (potentially multiple) metrics: in this case, only accuracy.**'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **指定（可能是多个）指标：在这种情况下，仅为准确性。**
- en: '**In-place modification of models**'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型的就地修改**'
- en: 'We’re using the %>% operator to call compile(). We could have written the network
    compilation step:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 %>% 操作符调用 compile()。我们也可以写网络编译步骤：
- en: compile(model,
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: compile(model,
- en: optimizer = "rmsprop",
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: loss = "mean_squared_error",
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mean_squared_error",
- en: metrics = "accuracy")
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: 'Using %>% for compile is less about compactness and more about providing a
    syntactic reminder of an important characteristic of Keras models: unlike most
    objects you work with in R, Keras models are modified in place. This is because
    Keras models are directed acyclic graphs of layers whose state is updated during
    training. You don’t operate on network and then return a new network object. Rather,
    you do *something* to the network object. Placing the network to the left of %>%
    and not saving the results to a new variable signals to the reader that you’re
    modifying in place.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 %>% 进行编译不仅仅是为了紧凑，而是为了提供语法上的提示，以提醒您 Keras 模型的一个重要特点：与您在 R 中使用的大多数对象不同，Keras
    模型是在原地修改的。这是因为 Keras 模型是在训练期间状态更新的层的有向无环图。您不是在网络上操作，而是对网络对象进行某些操作。将网络放在 %>% 左侧，而不将结果保存到一个新变量中，向读者发出您正在就地修改的信号。
- en: 'In the preceding call to compile(), we passed the optimizer, loss, and metrics
    as strings (such as “rmsprop”). These strings are actually shortcuts that are
    converted to R objects. For instance, “rmsprop” becomes optimizer_rmsprop(). Importantly,
    it’s also possible to specify these arguments as object instances:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 compile() 中，我们将优化器、损失函数和指标作为字符串（例如“rmsprop”）传递。这些字符串实际上是快捷方式，将转换为 R 对象。例如，“rmsprop”变成
    optimizer_rmsprop()。重要的是，也可以将这些参数指定为对象实例：
- en: model %>% compile(
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(
- en: optimizer = optimizer_rmsprop(),
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = optimizer_rmsprop(),
- en: loss = loss_mean_squared_error(),
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: loss = loss_mean_squared_error(),
- en: metrics = metric_binary_accuracy()
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = metric_binary_accuracy()
- en: )
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'This is useful if you want to pass your own custom losses or metrics, or if
    you want to further configure the objects you’re using—for instance, by passing
    a learning_rate argument to the optimizer:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想传递自己的自定义损失或指标，或者如果您想进一步配置正在使用的对象，例如通过将 learning_rate 参数传递给优化器，则这非常有用：
- en: model %>% compile(
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(
- en: optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = optimizer_rmsprop(learning_rate = 1e-4),
- en: loss = my_custom_loss,
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: loss = my_custom_loss,
- en: metrics = c(my_custom_metric_1, my_custom_metric_2)
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = c(my_custom_metric_1, my_custom_metric_2)
- en: )
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: In chapter 7, we’ll cover how to create custom losses and metrics. In general,
    you won’t have to create your own losses, metrics, or optimizers from scratch,
    because Keras offers a wide range of built-in options that is likely to include
    what you need.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在第七章，我们将介绍如何创建自定义的损失函数和指标。一般来说，您不必从头开始创建自己的损失函数、指标或优化器，因为 Keras 提供了广泛的内置选项，很可能包括您需要的内容。
- en: 'Optimizers:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器：
- en: ls(pattern = "^optimizer_", "package:keras")
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: ls(pattern = "^optimizer_", "package:keras")
- en: '[1] "optimizer_adadelta" "optimizer_adagrad" "optimizer_adam"'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "optimizer_adadelta" "optimizer_adagrad" "optimizer_adam"'
- en: '[4] "optimizer_adamax" "optimizer_nadam" "optimizer_rmsprop"'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] "optimizer_adamax" "optimizer_nadam" "optimizer_rmsprop"'
- en: '[7] "optimizer_sgd"'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] "optimizer_sgd"'
- en: 'Losses:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数：
- en: ls(pattern = "^loss_", "package:keras")
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: ls(pattern = "^loss_", "package:keras")
- en: '[1] "loss_binary_crossentropy"'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "loss_binary_crossentropy"'
- en: '[2] "loss_categorical_crossentropy"'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] "loss_categorical_crossentropy"'
- en: '[3] "loss_categorical_hinge"'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] "loss_categorical_hinge"'
- en: '[4] "loss_cosine_proximity"'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] "loss_cosine_proximity"'
- en: '[5] "loss_cosine_similarity"'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] "loss_cosine_similarity"'
- en: '[6] "loss_hinge"'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] "loss_hinge"'
- en: '[7] "loss_huber"'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] "loss_huber"'
- en: '[8] "loss_kl_divergence"'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] "loss_kl_divergence"'
- en: '[9] "loss_kullback_leibler_divergence"'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] "loss_kullback_leibler_divergence"'
- en: '[10] "loss_logcosh"'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] "loss_logcosh"'
- en: '[11] "loss_mean_absolute_error"'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] "loss_mean_absolute_error"'
- en: '[12] "loss_mean_absolute_percentage_error"'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] "loss_mean_absolute_percentage_error"'
- en: '[13] "loss_mean_squared_error"'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] "loss_mean_squared_error"'
- en: '[14] "loss_mean_squared_logarithmic_error"'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] "loss_mean_squared_logarithmic_error"'
- en: '[15] "loss_poisson"'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] "loss_poisson"'
- en: '[16] "loss_sparse_categorical_crossentropy"'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] "loss_sparse_categorical_crossentropy"'
- en: '[17] "loss_squared_hinge"'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] "loss_squared_hinge"'
- en: 'Metrics:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 指标：
- en: ls(pattern = "^metric_", "package:keras")
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: ls(pattern = "^metric_", "package:keras")
- en: '[1] "metric_accuracy"'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "metric_accuracy"'
- en: '[2] "metric_auc"'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] "metric_auc"'
- en: '[3] "metric_binary_accuracy"'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] "metric_binary_accuracy"'
- en: '[4] "metric_binary_crossentropy"'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] "metric_binary_crossentropy"'
- en: '[5] "metric_categorical_accuracy"'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] "metric_categorical_accuracy"'
- en: '[6] "metric_categorical_crossentropy"'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] "metric_categorical_crossentropy"'
- en: '[7] "metric_categorical_hinge"'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] "metric_categorical_hinge"'
- en: '[8] "metric_cosine_proximity"'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] "metric_cosine_proximity"'
- en: '[9] "metric_cosine_similarity"'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] "metric_cosine_similarity"'
- en: '[10] "metric_false_negatives"'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] "metric_false_negatives"'
- en: '[11] "metric_false_positives"'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] "metric_false_positives"'
- en: '[12] "metric_hinge"'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] "metric_hinge"'
- en: '[13] "metric_kullback_leibler_divergence"'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] "metric_kullback_leibler_divergence"'
- en: '[14] "metric_logcosh_error"'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] "metric_logcosh_error"'
- en: '[15] "metric_mean"'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] "metric_mean"'
- en: '[16] "metric_mean_absolute_error"'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] "metric_mean_absolute_error"'
- en: '[17] "metric_mean_absolute_percentage_error"'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] "metric_mean_absolute_percentage_error"'
- en: '[18] "metric_mean_iou"'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] "metric_mean_iou"'
- en: '[19] "metric_mean_relative_error"'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] "metric_mean_relative_error"'
- en: '[20] "metric_mean_squared_error"'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] "metric_mean_squared_error"'
- en: '[21] "metric_mean_squared_logarithmic_error"'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] "metric_mean_squared_logarithmic_error"'
- en: '[22] "metric_mean_tensor"'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] "metric_mean_tensor"'
- en: '[23] "metric_mean_wrapper"'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '[23] "metric_mean_wrapper"'
- en: '[24] "metric_poisson"'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '[24] "metric_poisson"'
- en: '[25] "metric_precision"'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '[25] "metric_precision"'
- en: '[26] "metric_precision_at_recall"'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '[26] "metric_precision_at_recall"'
- en: '[27] "metric_recall"'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '[27] "metric_recall"'
- en: '[28] "metric_recall_at_precision"'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '[28] "metric_recall_at_precision"'
- en: '[29] "metric_root_mean_squared_error"'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '[29] "metric_root_mean_squared_error"'
- en: '[30] "metric_sensitivity_at_specificity"'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '[30] "metric_sensitivity_at_specificity"'
- en: '[31] "metric_sparse_categorical_accuracy"'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '[31] "metric_sparse_categorical_accuracy"'
- en: '[32] "metric_sparse_categorical_crossentropy"'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '[32] "metric_sparse_categorical_crossentropy"'
- en: '[33] "metric_sparse_top_k_categorical_accuracy"'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '[33] "metric_sparse_top_k_categorical_accuracy"'
- en: '[34] "metric_specificity_at_sensitivity"'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '[34] "metric_specificity_at_sensitivity"'
- en: '[35] "metric_squared_hinge"'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '[35] "metric_squared_hinge"'
- en: '[36] "metric_sum"'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] "metric_sum"'
- en: '[37] "metric_top_k_categorical_accuracy"'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '[37] "metric_top_k_categorical_accuracy"'
- en: '[38] "metric_true_negatives"'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '[38] "metric_true_negatives"'
- en: '[39] "metric_true_positives"'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '[39] "metric_true_positives"'
- en: Throughout this book, you’ll see concrete applications of many of these options.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你将看到许多这些选项的具体应用。
- en: 3.8.4 Picking a loss function
  id: totrans-651
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.4 选择损失函数
- en: 'Choosing the right loss function for the right problem is extremely important:
    your network will take any shortcut it can to minimize the loss, so if the objective
    doesn’t fully correlate with success for the task at hand, your network will end
    up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained
    via SGD with this poorly chosen objective function: “maximizing the average well-being
    of all humans alive.” To make its job easier, this AI might choose to kill all
    humans except a few and focus on the well-being of the remaining ones, because
    average well-being isn’t affected by how many humans are left. That might not
    be what you intended! Just remember that all neural networks you build will be
    just as ruthless in lowering their loss function, so choose the objective wisely,
    or you’ll have to face unintended side effects.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的损失函数非常重要：你的网络会尽其所能采取任何捷径来最小化损失，因此，如果目标与手头任务的成功不完全相关，你的网络最终会做出你可能不希望的事情。想象一下，通过
    SGD 训练的愚蠢的全能 AI，其目标函数选择不当：“最大化所有活着的人的平均幸福感。”为了简化其工作，这个 AI 可能选择杀死除了少数人之外的所有人，并专注于剩下人的幸福感，因为平均幸福感不受剩余人数的影响。这可能不是你想要的结果！请记住，你构建的所有神经网络在降低损失函数方面都会同样无情，所以明智地选择目标，否则你将不得不面对意想不到的副作用。
- en: Fortunately, when it comes to common problems such as classification, regression,
    and sequence prediction, you can follow simple guidelines to choose the correct
    loss. For instance, you’ll use binary cross-entropy for a two-class classification
    problem, categorical cross-entropy for a many-class classification problem, and
    so on. Only when you’re working on truly new research problems will you have to
    develop your own loss functions. In the next few chapters, we’ll detail explicitly
    which loss functions to choose for a wide range of common tasks.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于常见问题，如分类、回归和序列预测，您可以遵循简单的指南来选择正确的损失。例如，对于两类分类问题，您将使用二元交叉熵，对于多类分类问题，您将使用分类交叉熵，依此类推。只有当您处理真正的新研究问题时，您才需要开发自己的损失函数。在接下来的几章中，我们将明确详细说明选择哪些损失函数适用于各种常见任务。
- en: 3.8.5 Understanding the fit() method
  id: totrans-654
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.5 理解fit()方法
- en: 'After compile() comes fit(). The fit() method implements the training loop
    itself. These are its key arguments:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 编译()之后是fit()。fit()方法实现了训练循环本身。以下是其关键参数：
- en: The *data* (inputs and targets) to train on. It will typically be passed either
    in the form of R arrays, tensors, or a TensorFlow Dataset object. You’ll learn
    more about the tfdatasets API in the next chapters.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练所需的*数据*（输入和目标）。通常以R数组、张量或TensorFlow数据集对象的形式传递。您将在接下来的章节中更多了解tfdatasets API。
- en: 'The number of *epochs* to train for: how many times the training loop should
    iterate over the data passed.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的*周期*数：训练循环应该遍历传递的数据的次数。
- en: 'The batch size to use within each epoch of mini-batch gradient descent: the
    number of training examples considered to compute the gradients for one weight
    update step'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个迷你批次梯度下降的周期中使用的批次大小：用于计算一个权重更新步骤的梯度的训练示例数
- en: '**Listing 3.21 Calling fit() with R arrays**'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.21 使用R数组调用fit()**'
- en: history <- model %>%
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>%
- en: fit(inputs,➊
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: fit(inputs,➊
- en: targets,➋
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 目标,➋
- en: epochs = 5,➌
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 5,➌
- en: batch_size = 128)➍
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128)➍
- en: ➊ **The input examples, as an R array**
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入示例，作为R数组**
- en: ➋ **The corresponding training targets, as an R array**
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **相应的训练目标，作为R数组**
- en: ➌ **The training loop will iterate over the data five times.**
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **训练循环将遍历数据五次。**
- en: ➍ **The training loop will iterate over the data in batches of 128 examples.**
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **训练循环将遍历128个示例的批次数据。**
- en: 'The call to fit() returns a history object. This object contains a metrics
    property, which is a named list of their per-epoch values for “loss” and specific
    metric names:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 对fit()的调用返回一个历史对象。此对象包含一个指标属性，它是每个周期的“损失”和特定指标名称的值的命名列表：
- en: str(history$metrics)
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: str(history$metrics)
- en: List of 2
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 长度为2的列表
- en: '$ loss             : num [1:5] 14.2 13.6 13.1 12.6 12.1'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '$ 损失             : num [1:5] 14.2 13.6 13.1 12.6 12.1'
- en: '$ binary_accuracy: num [1:5] 0.55 0.552 0.554 0.557 0.559'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '$ 二元准确度: num [1:5] 0.55 0.552 0.554 0.557 0.559'
- en: 3.8.6 Monitoring loss and metrics on validation data
  id: totrans-674
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.6 监控验证数据上的损失和指标
- en: The goal of machine learning is not to obtain models that perform well on the
    training data, which is easy—all you have to do is follow the gradient. The goal
    is to obtain models that perform well in general, and particularly on data points
    that the model has never encountered before. Just because a model performs well
    on its training data doesn’t mean it will perform well on data it has never seen!
    For instance, it’s possible that your model could end up merely *memorizing* a
    mapping between your training samples and their targets, which would be useless
    for the task of predicting targets for data the model has never seen before. We’ll
    go over this point in much more detail in chapter 5.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的目标不是获得在训练数据上表现良好的模型，这很容易——你只需跟随梯度。目标是获得在一般情况下表现良好的模型，特别是在模型以前从未遇到过的数据点上表现良好。仅仅因为一个模型在训练数据上表现良好，并不意味着它在以前从未见过的数据上也会表现良好！例如，您的模型可能仅仅*记忆*了您的训练样本和它们的目标之间的映射，这对于预测模型以前从未见过的数据的目标将是无用的。在第五章中，我们将更详细地讨论这一点。
- en: 'To keep an eye on how the model does on new data, it’s standard practice to
    reserve a subset of the training data as *validation data*: you won’t be training
    the model on this data, but you will use it to compute a loss value and metrics
    value. You do this by using the validation_data argument in fit(). Like the training
    data, the validation data could be passed as R arrays or as a TensorFlow Dataset
    object.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 为了密切关注模型在新数据上的表现，将一部分训练数据作为*验证数据*是标准做法：您不会在这些数据上训练模型，但会使用它来计算损失值和指标值。您可以通过在
    fit() 中使用 validation_data 参数来实现此目的。像训练数据一样，验证数据可以作为 R 数组或 TensorFlow Dataset 对象传递。
- en: '**Listing 3.22 Using the validation_data argument**'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 3.22 使用 validation_data 参数**'
- en: model <- keras_model_sequential() %>%
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(1)
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model %>% compile(optimizer_rmsprop(learning_rate = 0.1),
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer_rmsprop(learning_rate = 0.1),
- en: loss = loss_mean_squared_error(),
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: loss = loss_mean_squared_error(),
- en: metrics = metric_binary_accuracy())
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = metric_binary_accuracy())
- en: n_cases <- dim(inputs)[1]
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: n_cases <- dim(inputs)[1]
- en: num_validation_samples <- round(0.3 * n_cases)➊
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: num_validation_samples <- round(0.3 * n_cases)➊
- en: val_indices <-
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: val_indices <-
- en: sample.int(n_cases, num_validation_samples)➋
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: sample.int(n_cases, num_validation_samples)➋
- en: val_inputs <- inputs[val_indices, ]
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: val_inputs <- inputs[val_indices, ]
- en: val_targets <- targets[val_indices, , drop = FALSE]➌
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: val_targets <- targets[val_indices, , drop = FALSE]➌
- en: training_inputs <- inputs[-val_indices, ]
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: training_inputs <- inputs[-val_indices, ]
- en: training_targets <-
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: training_targets <-
- en: targets[-val_indices, , drop = FALSE]➌
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: targets[-val_indices, , drop = FALSE]➌
- en: model %>% fit(
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(
- en: training_inputs,
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: training_inputs,
- en: training_targets,➍
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: training_targets,➍
- en: epochs = 5,
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 5,
- en: batch_size = 16,
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 16,
- en: validation_data = list(val_inputs, val_targets)➎
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(val_inputs, val_targets)➎
- en: )
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Reserve 30% of the training inputs and targets for validation (we'll exclude
    these samples from training and reserve them to compute the validation loss and
    metrics).**
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将 30% 的训练输入和目标保留用于验证（我们将排除这些样本进行训练，并保留它们以计算验证损失和指标）。**
- en: ➋ **Generate num_validation_samples random integers, in the range of [1, n_cases].**
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **生成 num_validation_samples 个随机整数，范围在 [1, n_cases]。**
- en: ➌ **Pass drop = FALSE to prevent the R array [ method from dropping the size-1
    dimension, and instead return an array with shape (num_validation_ samples, 1).**
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **传递 drop = FALSE 以防止 R 数组 [ 方法删除大小为 1 的维度，而是返回形状为 (num_validation_samples,
    1) 的数组。**
- en: ➍ **Training data, used to update the weights of the model**
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **训练数据，用于更新模型的权重**
- en: ➎ **Validation data, used only to monitor the validation loss and metrics**
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **验证数据，仅用于监控验证损失和指标**
- en: 'The value of the loss on the validation data is called the *validation loss*,
    to distinguish it from the *training loss*. Note that it’s essential to keep the
    training data and validation data strictly separate: the purpose of validation
    is to monitor whether what the model is learning is actually useful on new data.
    If any of the validation data has been seen by the model during training, your
    validation loss and metrics will be flawed.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证数据上的损失值称为*验证损失*，以区别于*训练损失*。请注意，严格保持训练数据和验证数据分开是至关重要的：验证的目的是监控模型学习的内容是否实际上对新数据有用。如果任何验证数据在训练期间被模型看到，您的验证损失和指标将是错误的。
- en: 'Note that if you want to compute the validation loss and metrics after the
    training is complete, you can call the evaluate() method:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您想在训练完成后计算验证损失和指标，您可以调用 evaluate() 方法：
- en: loss_and_metrics <- evaluate(model, val_inputs, val_targets,
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: loss_and_metrics <- evaluate(model, val_inputs, val_targets,
- en: batch_size = 128)
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128)
- en: evaluate() will iterate in batches (of size batch_size) over the data passed
    and return numeric vector, where the first entry is the validation loss and the
    following entries are the validation metrics. If the model has no metrics, only
    the validation loss is returned (an R vector of length 1).
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: evaluate() 将在传递的数据上以批次（批量大小）迭代，并返回数字向量，其中第一个条目是验证损失，后续条目是验证指标。如果模型没有指标，则仅返回验证损失（长度为
    1 的 R 向量）。
- en: '3.8.7 Inference: Using a model after training'
  id: totrans-709
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8.7 推断：在训练后使用模型
- en: 'Once you’ve trained your model, you’re going to want to use it to make predictions
    on new data. This is called *inference*. To do this, a naive approach would simply
    be to call the model:'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您训练好了模型，您会想要将其用于在新数据上进行预测。这被称为*推断*。为了做到这一点，一个朴素的方法是简单地调用模型：
- en: predictions <- model(new_inputs)➊
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model(new_inputs)➊
- en: ➊ **Take an R array or TensorFlow tensor and returns a TensorFlow tensor.**
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **接受一个 R 数组或 TensorFlow 张量，并返回一个 TensorFlow 张量。**
- en: However, this will process all inputs in new_inputs at once, which may not be
    feasible if you’re looking at a lot of data (in particular, it may require more
    memory than your GPU has).
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这将一次性处理new_inputs中的所有输入，如果你要处理大量数据可能不可行（特别是可能需要比你的GPU拥有的内存更多）。
- en: 'A better way to do inference is to use the predict() method. It will iterate
    over the data in small batches and return an R array of predictions. And unlike
    calling the model, it can also process TensorFlow Dataset objects:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 进行推断的更好方法是使用predict()方法。它将在小批量数据上迭代，并返回一个R数组的预测。与调用模型不同的是，它还可以处理TensorFlow Dataset对象：
- en: predictions <- model %>%
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model %>%
- en: predict(new_inputs, batch_size = 128)➊
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: predict(new_inputs, batch_size = 128)➊
- en: ➊ **Take an R array or a TF Dataset and return an R array.**
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **接受一个R数组或TF Dataset并返回一个R数组。**
- en: 'For instance, if we use predict() on some of our validation data with the linear
    model we trained earlier, we get scalar scores that correspond to the model’s
    prediction for each input sample:'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们对先前训练的线性模型使用predict()在一些验证数据上，我们会得到对应于模型对每个输入样本的预测的标量分数：
- en: predictions <- model %>%
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model %>%
- en: predict(val_inputs, batch_size = 128)
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: predict(val_inputs, batch_size = 128)
- en: head(predictions, 10)
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: head(predictions, 10)
- en: '[,1]'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1]'
- en: '[1,] -0.11416233'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,] -0.11416233'
- en: '[2,]  0.43776459'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,]  0.43776459'
- en: '[3,] -0.02436411'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '[3,] -0.02436411'
- en: '[4,] -0.19723934'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: '[4,] -0.19723934'
- en: '[5,] -0.24584538'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '[5,] -0.24584538'
- en: '[6,] -0.18628466'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '[6,] -0.18628466'
- en: '[7,] -0.06967193'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '[7,] -0.06967193'
- en: '[8,]  0.19761485'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: '[8,]  0.19761485'
- en: '[9,] -0.28266442'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '[9,] -0.28266442'
- en: '[10,]  0.43299851'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '[10,]  0.43299851'
- en: For now, this is all you need to know about Keras models. You are ready to move
    on to solving real-world machine learning problems with Keras in the next chapter.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这就是你需要了解的关于Keras模型的全部内容。你已经准备好在下一章中使用Keras解决真实世界的机器学习问题了。
- en: Summary
  id: totrans-734
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow is an industry-strength numerical computing framework that can run
    on CPU, GPU, or TPU. It can automatically compute the gradient of any differentiable
    expression, it can be distributed to many devices, and it can export programs
    to various external runtimes—even JavaScript.
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow是一个工业强度的数值计算框架，可以在CPU、GPU或TPU上运行。它可以自动计算任何可微表达式的梯度，可以分布到多个设备，并且可以导出程序到各种外部运行时——甚至JavaScript。
- en: Keras is the standard API for doing deep learning with TensorFlow. It’s what
    we’ll use throughout this book
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras是使用TensorFlow进行深度学习的标准API。这是本书中我们将使用的内容
- en: Key TensorFlow objects include tensors, variables, tensor operations, and the
    gradient tape.
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow的关键对象包括张量、变量、张量操作和梯度带。
- en: The central type in Keras is the Layer. A *layer* encapsulates some weights
    and some computation. Layers are assembled into *models*.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras中的核心类型是Layer。一个*层*封装了一些权重和一些计算。层被组装成*模型*。
- en: Before you start training a model, you need to pick an *optimizer*, a *loss*,
    and some *metrics*, which you specify via the model %>% compile() method.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，你需要选择一个*优化器*，一个*损失*，和一些*度量标准*，你可以通过model %>% compile()方法来指定。
- en: To train a model, you can use the fit() method, which runs mini-batch gradient
    descent for you. You can also use it to monitor your loss and metrics on *validation
    data*, a set of inputs that the model doesn’t see during training.
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要训练一个模型，你可以使用fit()方法，它为你运行小批量梯度下降。你还可以用它来监视你在*验证数据*上的损失和指标，验证数据是模型在训练过程中不见的一组输入。
- en: Once your model is trained, you use the model %>% predict() method to generate
    predictions on new inputs.
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦你的模型训练好了，你就可以使用model %>% predict()方法在新输入上生成预测。
