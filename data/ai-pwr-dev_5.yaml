- en: 5 Managing Data with GitHub Copilot and Copilot Chat
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 通过 GitHub Copilot 和 Copilot Chat 管理数据
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容涵盖
- en: Persisting our data into a relational database
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的数据持久化到关系型数据库
- en: Streaming our data using Apache Kafka
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Kafka 进行数据流式传输
- en: Incorporating event-driven principles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融合事件驱动原则
- en: Analyzing our data to monitor the location using Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark 分析我们的数据以监控位置
- en: 'The last chapter laid the foundation for our Information Technology Asset Management
    system. However, this application will not fulfill our requirements without data.
    Data is the life’s blood of every application. That is what this chapter is all
    about: the various ways that we can use Generative AIs to create data, stream
    data, transform data, react to data, and learn from data.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章为我们的信息技术资产管理系统奠定了基础。然而，没有数据，这个应用程序将无法满足我们的要求。数据是每个应用程序的命脉。这正是本章的主题：我们可以使用生成式
    AI 来创建数据、流式传输数据、转换数据、对数据做出反应并从数据中学习的各种方式。
- en: Perceptive individuals might have noticed in the last chapter that our data
    access pattern would not have worked as it was incomplete. The opening section
    of this chapter will address this. After that, we will set up our database, fix
    the classes which access this data, and load some sample data for us to use in
    the rest of the chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 敏锐的人可能已经注意到在上一章中，我们的数据访问模式是无法正常工作的，因为它是不完整的。本章的开头部分将解决这个问题。之后，我们将设置我们的数据库，修复访问这些数据的类，并加载一些示例数据，以便在本章的其余部分使用。
- en: 5.1 Amassing our data set
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 积累我们的数据集
- en: 'Our first task will be to construct a substantial corpus of data to assist
    our experimentation in the remainder of the chapter. First, we will use GitHub
    Copilot to generate one thousand rows of asset information. We will soon find,
    however, that this may not be the tool most suited to this task. One key driver
    behind using these tools is the idea of discovery: Testing their boundaries, pushing
    against them, and occasionally, pushing back. But the journey is often where the
    joy is found. Once we have found this edge, we will be introduced to a new, previously
    unseen tool: GitHub Copilot Chat. Finally, once we have created our list of assets,
    we will add location information for those assets, again using GitHub Copilot
    Chat.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个任务将是构建一个大量的数据集，以帮助我们在本章剩余的实验中。首先，我们将使用 GitHub Copilot 生成一千行资产信息。然而，我们很快会发现，这可能不是最适合这项任务的工具。使用这些工具的一个关键驱动因素是发现的概念：测试它们的边界，推动它们，有时候，反击它们。但旅程往往是快乐的源泉。一旦我们找到了这个边缘，我们将被介绍一个新的、以前从未见过的工具：GitHub
    Copilot Chat。最后，一旦我们创建了我们的资产列表，我们将再次使用 GitHub Copilot Chat 为这些资产添加位置信息。
- en: We need to get our database running before building our initial dataset. Docker
    makes this task trivial, allowing us to quickly spin up an empty Postgres (or
    other RDBMS/NoSQL server) with minimal effort. Have you forgotten the command
    to do this? No worries, we can ask Copilot. Open a new file called data/initial_data_laod.sql.
    and enter the following prompt at the top of your newly minted SQL file.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的初始数据集之前，我们需要让数据库运行起来。Docker 让这个任务变得微不足道，让我们能够快速启动一个空的 Postgres（或其他 RDBMS/NoSQL
    服务器），几乎不费吹灰之力。你忘记了执行此操作的命令吗？别担心，我们可以问 Copilot。打开一个名为 data/initial_data_laod.sql
    的新文件，并在你的新生成的 SQL 文件顶部输入以下提示。
- en: Snippet 5.1 A prompt to have GitHub Copilot provide us with a Docker command
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 片段 5.1 提示 GitHub Copilot 为我们提供一个 Docker 命令
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Copilot will slowly reveal the Docker command: `--Answer: docker run --name
    itam_db -e POSTGRES_PASSWORD=postgres -d -p 5432:5432 postgres.` Once you run
    this command at your terminal or command line, we can build out our dataset. You
    should be able to connect to the locally running database. You should notice that
    there is a database called itam_db running in it. However, this database has no
    schema, tables, or data. Let’s first set up a new schema.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'Copilot 将逐渐揭示 Docker 命令：`--Answer: docker run --name itam_db -e POSTGRES_PASSWORD=postgres
    -d -p 5432:5432 postgres.` 一旦你在终端或命令行运行这个命令，我们就可以构建出我们的数据集。你应该能够连接到本地运行的数据库。你应该注意到里面有一个名为
    itam_db 的数据库在运行。然而，这个数据库没有模式、表或数据。让我们首先设置一个新的模式。'
- en: In our initial_data_file.sql, we will add a prompt to have Copilot draft the
    schema creation command. The following prompt (and response from Copilot) will
    allow you to create a new schema called itam if executed from within your database
    client application (e.g., DataGrip, SQuirreL, pdAdmin, or even using the Docker
    exec command `docker exec -i itam_db psql -U postgres -c "create schema itam"`)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 initial_data_file.sql 中，我们将添加一个提示，让 Copilot 起草模式创建命令。执行此命令将允许您从数据库客户端应用程序（例如
    DataGrip、SQuirreL、pdAdmin，甚至使用 Docker exec 命令 `docker exec -i itam_db psql -U
    postgres -c "create schema itam"`）中创建名为 itam 的新模式。
- en: Snippet 5.2 A prompt to have Copilot create a new schema
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示让 Copilot 创建新模式
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we should add a user for use within our application. This user will be
    able to perform *CRUD* (Create, Read, Update, Delete) operations on our data but
    will not be able to affect the structure of the database tables or procedures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该为应用程序中使用的用户添加一个用户。此用户将能够对我们的数据执行*CRUD*（创建、读取、更新、删除）操作，但将无法影响数据库表或存储过程的结构。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The lines that start with double dashes are comments in SQL. Commenting out
    these lines are optional from Copilot’s perspective, as it will generate solutions
    without the comments; it makes it easier to copy and paste the code directly into
    our database tool of choice.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以双破折号开头的行是 SQL 中的注释。从 Copilot 的角度来看，注释这些行是可选的，因为它将生成没有注释的解决方案；这样做可以更轻松地将代码直接复制粘贴到我们选择的数据库工具中。
- en: While we are at it, we will also add an administrative account to perform the
    operations that our read-write users cannot, such as creating or dropping tables.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在此期间，我们还将添加一个管理帐户，以执行我们的读写用户无法执行的操作，例如创建或删除表。
- en: Listing 5.1 A prompt to create new users
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示创建新用户
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will grant ownership of this schema to the itam_admin account. Transfering
    this ownership will ensure that only this account can change the table structure:
    the data definition.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把此模式的所有权授予 itam_admin 帐户。转移此所有权将确保只有此帐户可以更改表结构：数据定义。
- en: Snippet 5.3 A prompt to transfer schema ownership to the admin account
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将模式所有权转移给管理员帐户的提示
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With the set-up, account creation, and worship of the system out of the way,
    we can start to focus on the data. We will begin by adding the reference data,
    the data that supports the assets: the depreciation strategies. This data is more
    static in nature; it changes less frequently, if at all. Next, we will define
    and store these strategies.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 配置完成、帐户创建和系统的崇拜已经完成，我们可以开始专注于数据。我们将从添加参考数据开始，即支持资产的数据：折旧策略。这些数据的性质更加静态；它们的变化频率较低，甚至根本不变。接下来，我们将定义并存储这些策略。
- en: Listing 5.2 A prompt to create the depreciation_strategy table
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示创建折旧策略表
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will use a sequence as this table's primary key. While this would not be
    strictly necessary for a table that will not be very large and with known values
    that we could and will manually enter, adding this sequence will allow us to work
    with Copilot more and have it make some suggestions. Moreover, it is amusing to
    ask Copilot questions and have Copilot answer within a text file.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用序列作为此表的主键。虽然对于一个不会很大并且我们可以手动输入已知值的表而言，这并不是严格必要的，但是添加此序列将允许我们与 Copilot 更多地合作并让它提出一些建议。此外，询问
    Copilot 并在文本文件中获得 Copilot 的回答是有趣的。
- en: Listing 5.3 A prompt to create a sequence for use as the primary key of the
    depreciation_strategy table
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示为折旧策略表的主键创建序列
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Naturally, with the sequence in our proverbial hand, we need to know how to
    associate the sequence with the primary key column of the `depreciation_stategy`
    table. Luckily, Copilot has the answer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，有了我们手中的序列，我们需要知道如何将序列与`depreciation_stategy`表的主键列关联起来。幸运的是，Copilot 有答案。
- en: Listing 5.4 Asking Copilot how to associate the sequence with the primary key
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 询问 Copilot 如何将序列与主键关联
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we shall complete this table by inserting the following static entries
    into the table. We will only use two depreciation strategies for now: straight-line
    and double declining balance.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过将以下静态条目插入表格来完成此表格。目前我们只使用两种折旧策略：直线法和双倍余额递减法。
- en: Lising 5.5 Adding the static entries to the depreciation_strategy table
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将静态条目添加到折旧策略表
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we shall move on the funding_details table. This information tells us
    how we financed our equipment, the resale value, and instructions for what should
    be done with our asset once its useful life is over. The sequence of steps will
    be identical for what we did for the depreciation strategies, with the exception
    that we will not be added static entries, as this data is directly related to
    an individual asset. We will define the table, create the sequence, and apply
    said sequence to the table, functioning as the primary key.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向资金细节表。这些信息告诉我们如何为我们的设备进行融资，再销售价值，并对资产在其有用生命周期结束后应采取的措施进行说明。我们在折旧策略中所做的步骤顺序将与此相同，唯一的区别是我们不会添加静态条目，因为这些数据直接与个体资产相关。我们将定义表，创建序列，并将该序列应用于表，作为主键的功能。
- en: Listing 5.6 Complete code listing for the funding_details table
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.6 资金详情表的完整代码列表
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The final information that we will define and generate are the assets themselves.
    This listing, too, is redundant but included for completeness. Finally, we create
    the table, make the sequence, and use it as the primary key.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义和生成的最后信息是资产本身。这个列表也是冗余的，但出于完整性考虑已包括在内。最后，我们创建表，创建序列，并将其用作主键。
- en: Listing 5.7 Complete code listing for the assets table
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.7 资产表的完整代码列表
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With the tables defined and created, we will now focus on creating the data.
    In our text file, we instruct Copilot with parameters for the dataset we are looking
    for. Copilot will likely attempt to assist you in outlining the attributes surrounding
    your new dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义和创建表之后，我们现在将专注于创建数据。在我们的文本文件中，我们使用参数指示Copilot我们正在寻找的数据集。Copilot可能会尝试帮助您概述围绕新数据集的属性。
- en: Listing 5.8 Creating a dataset for the assets table
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.8 为资产表创建数据集
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The solution that Copilot provides is novel. It builds a large series using
    a Postgres built-in function, meaning that this would not be a portable solution.
    However, given that this is the database we will use, this is an appropriate enough
    solution. The resultant dataset is refined. We would have gotten better results
    if we had used Python and asked for Copilot’s assistance in coding a script to
    generate a file to load into Postgres. However, given, that this dataset is only
    for playing out with the application, we do not need to be overly concerned with
    the data quality for now. Although, in the real world, data quality is everything.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot提供的解决方案是新颖的。它使用Postgres内置功能构建了一个大系列，这意味着这不是一个可移植的解决方案。然而，考虑到这是我们将要使用的数据库，这是一个足够合适的解决方案。生成的数据集经过了精心制作。如果我们使用Python并要求Copilot帮助编写一个用于加载到Postgres的文件的脚本，我们可能会得到更好的结果。然而，鉴于这个数据集只是用来玩应用程序，目前我们不需要过于担心数据质量。尽管在现实世界中，数据质量是至关重要的。
- en: 'Listing 5.9 Copilot’s response: An insert statement built off of a series'
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.9 Copilot的响应：基于一系列构建的插入语句
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If we switch back to ChatGPT for just minute, we can get a second opinion as
    to how to create such a dataset. ChatGPT suggests the Python library `faker`.
    The `faker` package is used to generate fake data, like common English first names.
    `numpy` is used to generate the random float values for cost, useful life, and
    salvage value. `pandas` is used to manage the data in a `DataFrame` (the table).
    Additionally, we could save the `DataFrame` to a CSV file, using the method `df.to_csv('assets.csv',
    index=False)`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅仅转向ChatGPT，我们可以得到如何创建这样的数据集的第二意见。ChatGPT建议使用Python库`faker`。`faker`包用于生成虚假数据，例如常见的英文姓名。`numpy`用于生成成本、有用生命和残值的随机浮点值。`pandas`用于在`DataFrame`（表）中管理数据。此外，我们可以使用`df.to_csv('assets.csv',
    index=False)`方法将`DataFrame`保存到CSV文件中。
- en: Listing 5.10 ChatGPT suggests Faker to generate the fake dataset
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.10 ChatGPT建议使用Faker来生成虚假数据集
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For each of these assets, we will need funding details as well; how they were
    financed (purchased in this case), and the depreciation details. Unsurprisingly,
    we get a similar solution from Copilot: generate a series of entries using a similar
    prompt as what we used for the assets. We will need to ensure that for each of
    the asset identifiers (1-1000), we have a corresponding funding details entry.
    Otherwise, we would risk getting null pointers when running out code.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些资产的每一项，我们也需要资金细节；它们是如何融资的（在这种情况下购买），以及折旧细节。毫不奇怪，我们从Copilot得到了类似的解决方案：使用与我们为资产使用的类似提示生成一系列条目。我们需要确保对于每个资产标识符（1-1000），我们都有相应的资金细节条目。否则，在运行我们的代码时会出现空指针的风险。
- en: Listing 5.11 Creating a dataset for the `funding_details` table
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.11 创建`funding_details`表的数据集
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With the dataset generated and stored in the database, we should be able to
    wire up the remainder of our application to store and display assets using the
    REST APIs. However, since we had previously stripped out all of the metadata for
    SQLAlchemy during our build phase (see previous chapter), we need a way to wire
    this metadata with our adapters differently.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有了在数据库中生成和存储的数据集，我们应该能够通过 REST API 连接我们应用程序的其余部分来存储和显示资产。然而，由于我们在构建阶段之前已经剥离了所有的
    SQLAlchemy 元数据（请参阅上一章），我们需要一种不同的方法来将这些元数据与我们的适配器进行连接。
- en: 'With this, we have reached the edge of Copilot’s capabilities. We are perplexed
    by what comes next; how we can solve our most recent dilemma. Tempting as it is,
    we cannot give up and go home. Therefore, it is time to introduce the most recent
    addition to the Copilot product suite: Copilot Chat. Copilot Chat is an embedded
    GPT-4 model in your IDE (only currently supported by Visual Studio Code). We shall
    open the chat dialog and ask how to keep our business model clean but still use
    SQLAlchemy’s ORM (Object Relational Model) features.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们已经达到了 Copilot 的能力边缘。我们对接下来会发生什么感到困惑；我们如何解决我们最近的困境。尽管诱人，我们不能放弃回家。因此，现在是时候介绍
    Copilot 产品套件的最新成员了：Copilot Chat。Copilot Chat 是嵌入式的 GPT-4 模型，可以集成到你的 IDE 中（目前仅支持
    Visual Studio Code）。我们将打开聊天对话框，询问如何保持我们的业务模型清晰，但仍然使用 SQLAlchemy 的 ORM（对象关系模型）功能。
- en: Figure 5.1 GitHub Copilot Chat’s solution for how to solve our most recent quandary
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1 GitHub Copilot Chat 对我们最近困境的解决方案
- en: '![A screenshot of a computer Description automatically generated with low confidence](images/05image002.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![一张低置信度自动生成的计算机截图的描述](images/05image002.png)'
- en: Copilot Chat suggests that we create a separate Data Access Layer. This approach
    maps nicely onto the Ports and Adapters approach we have used thus far. In addition,
    Copilot Chat recommended modeling these classes similarly to the domain classes
    but including the metadata required for ORM functionality to work correctly. The
    resultant code is in Listing 5.12.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot Chat 建议我们创建一个单独的数据访问层。这种方法与我们迄今为止使用的端口和适配器方法非常契合。此外，Copilot Chat 建议将这些类建模得与领域类类似，但包括
    ORM 功能正常工作所需的元数据。生成的代码见清单 5.12。
- en: Listing 5.12 ORM support outside of the domain classes.
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.12 领域类之外的 ORM 支持。
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that the external model classes have been created, we must map these ORM
    instances to our domain model before returning them to the system's core. While
    this might seem like over-engineered code for such a simple application, this
    gives us great flexibility in how our domain model can operate. For example, our
    model can perform complex operations beyond just CRUD. We would be limited to
    these operations if we kept our domain model identity to the model used in the
    Data Access Layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，外部模型类已经创建，我们必须将这些 ORM 实例映射到我们的领域模型，然后返回给系统核心。虽然对于这样一个简单的应用程序来说，这似乎是过度设计的代码，但这给了我们很大的灵活性，使我们的领域模型可以进行复杂的操作。例如，我们的模型可以执行
    CRUD 之外的复杂操作。如果我们将我们的领域模型身份限制在数据访问层中使用的模型上，我们将受到这些操作的限制。
- en: 'Next, we will use Copilot and Copilot Chat to explore incorporating event-driven
    ideas into our application. Event-driven concepts will allow us to track our IT
    assets in real-time: their location, status, and market value, for example.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Copilot 和 Copilot Chat 探索将事件驱动思想纳入我们的应用程序中。事件驱动的概念将允许我们实时跟踪我们的 IT
    资产：它们的位置、状态和市场价值，例如。
- en: 5.2 Monitoring our assets in real time with Kafka
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 使用 Kafka 实时监控我们的资产
- en: We will monitor our assets in real-time to motivate our exploration of using
    Generative AIs in conjunction with event-driven architecture. We shall take it
    as a given that some system external to the Information Security Asset Management
    system fires events as our assets move from one location to another.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实时监控我们的资产，以激励我们探索将生成式人工智能与事件驱动架构结合使用的探索。我们应该认为，信息安全资产管理系统外部的某些系统在我们的资产从一个位置移动到另一个位置时会触发事件。
- en: To delve into ITAM events, we will need to configure a few additional services.
    In this case, we will use Apache Kafka. Apache Kafka is a distributed streaming
    platform that is used for building real-time data pipelines and streaming apps.
    It's designed to handle data streams from multiple sources and deliver them to
    multiple consumers, effectively acting as a middleman for our real-time data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解 ITAM 事件，我们需要配置一些额外的服务。在这种情况下，我们将使用 Apache Kafka。Apache Kafka 是一个分布式流平台，用于构建实时数据管道和流应用程序。它被设计用于处理来自多个来源的数据流，并将它们传送到多个消费者，有效地充当我们实时数据的中间人。
- en: To start we will ask Copilot Chat how to run Kafka locally, using Docker. Apache
    Kafka has an undeserved reputation for being difficult to install and configure.
    Running in Docker will allow us to side-step is this controversy. Using Copilot
    Chat we can produce a docker compose file. However, as is often the case, the
    versions are very old to the point of not supporting some hardware. Listing 5.13
    is an updated listing from Confluent’s (the company that offers commercial support
    for Kafka) official GitHub repository. Notice that the docker-compose file's content
    includes both Kafka and Zookeeper. Zookeeper is a distributed coordination service
    that Kafka uses to manage and coordinate the brokers within the cluster, at least
    for now. Future versions aim to remove dependency on Zookeeper.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将询问 Copilot Chat 如何在本地使用 Docker 运行 Kafka。Apache Kafka 有一个不应该存在的声誉，即安装和配置很困难。在
    Docker 中运行将允许我们回避这个争议。使用 Copilot Chat，我们可以生成一个 docker compose 文件。然而，通常情况下，版本非常旧，甚至不支持一些硬件。图示
    5.13 是从 Confluent（提供 Kafka 商业支持的公司）官方 GitHub 存储库中提取的更新的列表。请注意，docker-compose 文件的内容包括
    Kafka 和 Zookeeper。Zookeeper 是 Kafka 用来管理和协调集群内代理的分布式协调服务，至少目前是这样。未来版本的目标是消除对 Zookeeper
    的依赖。
- en: Listing 5.13 Docker-Compose file to launch Kafka with Zookeeper
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**图示 5.13 Docker-Compose 文件，用于启动带有 Zookeeper 的 Kafka**'
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With our new Kafka instance running, we now need a consumer that will pull the
    updated locations off the topic `asset_location` (which we share create shortly)
    and update the inventory in the database. Again, we can ask Copilot Chat to provide
    us with a suggestion as to how to do this.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的新 Kafka 实例正在运行，我们现在需要一个消费者，它将从主题`asset_location`（我们很快将创建的主题）中拉取更新的位置，并在数据库中更新库存。同样，我们可以请求
    Copilot Chat 为我们提供如何做到这一点的建议。
- en: Listing 5.14 Prompt to Copilot Chat asking how to use Python to subscribe to
    the topic
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**图示 5.14 提示 Copilot Chat 如何使用 Python 订阅主题**'
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The code that Copilot Chat generates creates a consumer, listens to the topic,
    and uses a reference to the `AssetManager` class to update the location of the
    affected asset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot Chat 生成的代码创建一个消费者，监听主题，并使用对`AssetManager`类的引用来更新受影响资产的位置。
- en: Listing 5.15 Copilot Chat code to monitor assets’ locations
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**图示 5.15 Copilot Chat 代码，用于监控资产位置**'
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'However, should the adapter be aware of the `AssetManager`? While the `AssetManager`
    does not directly sit inside the domain model, it is a critical intermediary.
    This coupling level between the `AssetManager` and the adapter could arguably
    violate the hexagon in the hexagonal architecture model. Let’s have Copilot Chat
    weigh in. We can ask it: “Given that this project uses hexagonal architecture,
    is it a good idea to have an adapter aware of the `AssetManger`?”'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，适配器应该知道`AssetManager`吗？虽然`AssetManager`不直接位于领域模型内部，但它是一个至关重要的中介。适配器和`AssetManager`之间的这种耦合程度可能会违反六边形架构模型中的六边形。让我们让
    Copilot Chat 来发表意见。我们可以问它：“考虑到这个项目使用了六边形架构，让适配器知道`AssetManager`是一个好主意吗？”
- en: Figure 5.2 GitHub Copilot Chat’s acceptance that it is, in fact, imperfect
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**图示 5.2 GitHub Copilot Chat 承认它实际上并不完美**'
- en: '![A screenshot of a computer Description automatically generated with medium
    confidence](images/05image003.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![一张计算机的截图 用中度信心自动生成的描述](images/05image003.png)'
- en: 'It would appear to be the case that Copilot Chat agrees that this would indeed
    be considered a breach of the proper contraction responsibility of an adapter.
    We could add a new port to our `AssetManager` class that could use the Kakfa port.
    However, let’s see if Copilot Chat has any other suggestions: “We do not want
    the Kafka consumer to interact directly with the `AssetManager`. Are there any
    ways to accomplish this?” Copilot Chat suggests that we apply the `Mediator` pattern,
    which resembles a port and adapter combination.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 Copilot Chat 认为这确实会被视为适配器的适当承包责任的违约。我们可以向我们的`AssetManager`类添加一个新的端口，它可以使用
    Kakfa 端口。然而，让我们看看 Copilot Chat 是否有其他建议：“我们不希望 Kafka 消费者直接与`AssetManager`进行交互。有什么方法可以实现这一点？”
    Copilot Chat 建议我们应用`Mediator`模式，这类似于一个端口和适配器的组合。
- en: Mediator Pattern
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 中介者模式
- en: The Mediator design pattern is a behavioral pattern that promotes loose coupling
    between objects by encapsulating their interactions within a mediator object.
    The mediator object acts as a central hub that facilitates communication between
    objects without the objects having to know about each other.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Mediator 设计模式是一种行为模式，通过将对象之间的交互封装在一个中介对象中，促进对象之间的松耦合。中介对象充当一个中心枢纽，促进对象之间的通信，而无需对象知道彼此。
- en: Let us first take a look at the Mediator class that Copilot Chat generated.
    The class would sit between the `AssetManager` and the Kafka consumer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下 Copilot Chat 生成的 Mediator 类。该类将位于`AssetManager`和 Kafka 消费者之间。
- en: Listing 5.16 The Mediator class that will mediate the interaction between the
    AssetManager and the Kafka consumer
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.16 Mediator类，它将调和AssetManager和Kafka消费者之间的交互
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This implementation closely resembles a message queue or message publisher,
    which is precisely the point: decoupling.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现与消息队列或消息发布者十分相似，这正是关键所在：解耦。
- en: Note
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Eagle-eyed readers would likely notice that we are playing a little fast and
    loose with the distinction between an Asset and an asset of type Hardware. In
    the original domain model, only Hardware had a location. Generally, one does not
    think of Software as having a location. Of course, you could say that the software
    is installed in its location, but it is arguably how convincing one finds this
    argument. However, as this project continues, the reason for the flattening of
    the domain model is for simplicity’s sake, as polymorphic structures in a persistence
    layer is a distractingly complex topic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有慧眼的读者可能会注意到我们在资产和硬件类型的资产之间的区别上有一点疏忽。在原始领域模型中，只有硬件有位置。通常，人们不会认为软件有位置。当然，您可以说软件安装在它的位置上，但人们是否会认为这个论点说服力有待商榷。不过，随着项目的继续进行，领域模型变平的原因是为了简化，因为在持久性层中的多态结构是一个令人分心的复杂主题。
- en: Now that we have a strategy to decouple the Kafka consumer with the AssetManager,
    we should update the Kafka consumer to take advantage of it. We will need to pass
    the mediator into the class in its constructor. This way the AssetManager and
    the consumer will have access to the same instance, and messages can freely flow
    back and forth; or rather, in this case, the flow will be unidirectional. You
    should note that we intend to read and write JSON on this topic, so we will need
    to have our value deserializer understand this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个将 Kafka 消费者与 AssetManager 解耦的策略，我们应该更新 Kafka 消费者以利用它。我们需要在构造函数中将中介者传递给类。这样，AssetManager和消费者将可以访问相同的实例，并且消息可以自由地来来往往；或者，在这种情况下，流程将是单向的。您应该注意，我们打算在这个主题上读取和写入
    JSON，因此我们需要让我们的值反序列化器理解这一点。
- en: Listing 5.17 Incorporating the mediator into the Kafka consumer class
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.17 将中介者整合到 Kafka 消费者类中
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next, we shall examine the changes that the `AssetManager` class require to
    incorporate the ability to track these locations. You should note that to get
    this project to run in its entirely, you would need to modify the `AssetManager`,
    `SQLAlchemyAssetRepository`, and `Asset` classes, as well as create a new table
    in your database called `itam.asset_locations`. The complete and updated source
    code is available in the GitHub repository for this book. For now, we shall just
    focus on the changes needed to get the events flowing through our system and use
    the repository for reference if the reader so chooses.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查`AssetManager`类需要的更改，以包含跟踪这些位置的功能。您应该注意，要使该项目完全运行起来，您需要修改`AssetManager`、`SQLAlchemyAssetRepository`和`Asset`类，并在数据库中创建一个名为`itam.asset_locations`的新表。完整且更新的源代码可在本书的
    GitHub 存储库中找到。现在，我们将专注于为我们的系统流动的事件所需的更改，并在需要时使用存储库作为参考。
- en: Figure 5.3 AssetManager requires the additional of another constructor parameter
    and a method to handle the updates to its locations objects
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.3 `AssetManager`需要另外一个构造函数参数和一个方法来处理其位置对象的更新
- en: '![A picture containing text, screenshot, font, receipt Description automatically
    generated](images/05image004.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、屏幕截图、字体、收据描述的图片 自动生成](images/05image004.png)'
- en: 'There are two required changes for the `AssetManager` class: First, we need
    to add the `AssetLocationMediator` added to the constructor, registering it to
    handle the `AssetLocationUpdated` event. And secondly, we need to add a method
    that will handle this event. In this case, we call the method `update_asset_location`.
    The abridged code is below.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`AssetManager`类，有两个必需的更改：首先，我们需要将`AssetLocationMediator`添加到构造函数中，并注册它来处理`AssetLocationUpdated`事件。其次，我们需要添加一个处理此事件的方法。在这种情况下，我们称此方法为`update_asset_location`。删节后的代码如下。
- en: Listing 5.18 The updated constructor and an event handler for the `AssetManager`
    class
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.18 更新后的`AssetManager`类的构造函数和事件处理程序
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `add_location` method of the Asset class merely appends the new Location
    to the end of a list of Locations. More sophisticated domain models might include
    a `current_location` attribute, relegating the rest to a list of historical locations;
    however, given that we are trying to get our events flowing through the system,
    it behooves us to keep things simple.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`Asset`类的`add_location`方法仅仅是将新的位置追加到位置列表的末尾。更复杂的领域模型可能会包括一个`current_location`属性，将其余部分归类为历史位置列表；然而，鉴于我们试图使我们的事件在系统中流动，使事情保持简单是明智的。'
- en: 'There is only one final item on our todo list: create the topic. How does one
    create a topic? That is a good question. Thankfully, all the tools we need are
    available in our running Docker container. So, let’s log into our Kafka Docker
    instance. We use the following command (assuming that your Docker instance is
    named kafka): `docker exec -it kafka /bin/bash`.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们待办事项清单上只有一项最终项目：创建主题。如何创建主题呢？这是一个很好的问题。幸运的是，我们运行的 Docker 容器中有我们需要的所有工具。所以，让我们登录到我们的
    Kafka Docker 实例中。我们使用以下命令（假设您的 Docker 实例命名为 kafka）：`docker exec -it kafka /bin/bash`.
- en: 'The first thing that should check is if any topics are already created. We
    can do that with the following command: `kafka-topics --list --bootstrap-server
    localhost:9092.` This command would list all of the existing topics, running on
    this Kafka cluster. As you can see, there aren’t any.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 应该首先检查的是是否已经创建了任何主题。我们可以使用以下命令来执行：`kafka-topics --list --bootstrap-server localhost:9092.`
    此命令将列出在此 Kafka 集群上运行的所有现有主题。正如您所见，没有任何主题。
- en: 'Given the need for a topic, let’s create it. You would use the following command:
    `kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic asset_location.`
    If you re-run the `kafka-topics --list` command again, you will see the new topic.
    The partitions and replication-factor instructions we included in the create topic
    command inform Kafka that we want one partition and a replication factor of one.
    If we were setting this up for production, or any purpose other than testing,
    we would likely want them to be greater than that to ensure availability of data.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个主题，让我们来创建它。您可以使用以下命令：`kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic asset_location.`
    如果您重新运行`kafka-topics --list`命令，您将看到新主题。我们在创建主题命令中包含的分区和复制因子说明告诉 Kafka 我们希望有一个分区和一个复制因子。如果我们是为生产环境或测试以外的任何目的设置这个，我们可能希望它们大于那个数，以确保数据的可用性。
- en: Table 5.1 Summary of Kafka console commands
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 5.1 Kafka 控制台命令摘要
- en: '| Action | Command |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 命令 |'
- en: '| Create |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 创建 |'
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Read |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 读取 |'
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Write |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 写入 |'
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Delete |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 删除 |'
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| List |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 列出 |'
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Now comes the fun part, observing the application in action. Kafka comes with
    a console produce that will allow us to publish messages to Kafka from standard
    in. To do this, we should launch the console producer with the follow command:
    `kafka-console-producer --broker-list localhost:9092 --topic asset_location`.
    You will enter an interactive session allowing you to publish a message with every
    line. Let’s publish a few messages simulating our asset moving around or near
    Chicago.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分，观察应用程序的运行情况。Kafka 自带一个控制台生产者，允许我们从标准输入向 Kafka 发布消息。要做到这一点，我们应该使用以下命令启动控制台生产者：`kafka-console-producer
    --broker-list localhost:9092 --topic asset_location`。您将进入一个交互式会话，可以在每行发布一条消息。让我们发布一些消息，模拟我们的资产在芝加哥附近移动的情况。
- en: Listing 5.19 Entries for the Kafka console producer
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.19 Kafka 控制台生产者的条目
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you enter these messages, you should see the output from your application
    indicating that the location has been updated.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当您输入这些消息时，您应该看到应用程序的输出，指示位置已经更新。
- en: 'For completeness’s sake, there is one more command that you should be aware
    of: you might make a mistake when entering these messages. An invalid message
    could potentially break your consumer. One possible solution is to delete the
    topic. Deleting a topic might sound dramatic, but it will solve the issue. So
    here is that command: `kafka-topics --delete --topic asset_location --bootstrap-server
    localhost:9092.`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整起见，还有一个命令需要您注意：在输入这些消息时，您可能会犯错。一个无效的消息可能会导致您的消费者出现问题。一个可能的解决方案是删除主题。删除主题可能听起来很夸张，但这将解决问题。因此，这就是那个命令：`kafka-topics
    --delete --topic asset_location --bootstrap-server localhost:9092.`
- en: In this section, we have added the ability to see changes in location of our
    `Assets` in real-time tracking using Apache Kafka. In the final section of this
    chapter, we will work with Copilot Chat to extend the capacity by monitoring our
    `Assets` in real-time, attempting to determine if they are where they should be.
    Again, we will explore using Spark and Kafka together to accomplish this analysis.
    Once completed, we will win the thanks for our InfoSec team who fear that too
    much of our core business and intellectual property exists on and within these
    `Assets`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经添加了实时跟踪我们的 `资产` 在 Apache Kafka 中的位置变化的能力。在本章的最后一节中，我们将使用 Copilot Chat
    扩展能力，通过实时监控我们的 `资产`，尝试确定它们是否位于它们应该位的位置。同样，我们将探讨使用 Spark 和 Kafka 一起完成此分析。完成后，我们将感谢我们的信息安全团队，他们担心我们的核心业务和知识产权过多存在于这些
    `资产` 中。
- en: 5.3 Analyzing, Learning, and Tracking with Apache Spark
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 使用 Apache Spark 进行分析、学习和跟踪
- en: Real-time tracking of assets is a business-critical function. Your IT assets
    contain sensitive business data, client lists, sales figures, Profits and Loss
    (PnL) projections, and sales strategies, amongst many other items. A lost asset
    can be an existential event for a company. Therefore, careful management and monitoring
    is priority one for many InfoSec professionals. In this section, we aim to make
    their jobs substantially easier. Modern data platforms make it trivial to track
    your assets in real time and send notifications should questionable conditions
    arise. Let’s get into it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 资产的实时跟踪是一个业务关键功能。您的 IT 资产包含着敏感的商业数据、客户名单、销售数据、利润和损失（PnL）预测以及销售策略，以及许多其他项目。一次丢失的资产可能是公司的生存危机。因此，对于许多信息安全专业人员来说，仔细的管理和监控是首要任务。在本节中，我们旨在使他们的工作变得更加轻松。现代数据平台使实时跟踪资产并在出现可疑情况时发送通知变得微不足道。让我们开始吧。
- en: Apache Spark is a powerful, open-source data processing engine built around
    speed, ease of use, and sophisticated analytics. It was developed to provide an
    improved alternative to MapReduce for processing big data sets and can handle
    batch and real-time analytics. Spark provides APIs for Scala, Java, Python, and
    R and a built-in module for SQL queries. Its core data structure, the Resilient
    Distributed Dataset (RDD), enables fault-tolerant operation and allows data to
    be processed in parallel across a cluster of computers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个强大的、开源的数据处理引擎，以速度、易用性和复杂的分析而闻名。它旨在提供一个比 MapReduce 更好的处理大数据集的替代方案，并且可以处理批处理和实时分析。Spark
    提供了 Scala、Java、Python 和 R 的 API，并且具有用于 SQL 查询的内置模块。其核心数据结构，弹性分布式数据集（RDD），支持容错操作，并允许数据在计算机集群上并行处理。
- en: Spark also includes several libraries to broaden its capabilities, including
    MLlib for machine learning, Spark Streaming for processing live data streams,
    and Spark SQL and DataFrames for processing structured data. These tools make
    it well-suited for tasks ranging from machine learning to real-time data streaming
    and batch processing. Its in-memory processing capabilities make Spark significantly
    faster than its predecessor, making it a popular choice for big data processing.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 还包括几个库来扩展其功能，包括用于机器学习的 MLlib，用于处理实时数据流的 Spark Streaming，以及用于处理结构化数据的 Spark
    SQL 和 DataFrames。这些工具使其非常适合从机器学习到实时数据流和批处理的任务。其内存处理能力使 Spark 比其前身快得多，使其成为大数据处理的热门选择。
- en: First, we will ask Copilot Chat to recommend a strategy for using Apache Spark
    to track our assets.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将要求 Copilot Chat 推荐一种使用 Apache Spark 跟踪我们资产的策略。
- en: Listing 5.20 The prompt to ask Copilot Chat how best to track out `Assets` in
    real-time
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.20 请求 Copilot Chat 如何最好地实时跟踪我们的`资产`
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Copilot Chat generates the class that you should be able to put into a file
    called asset_location_spark_adapter.py in the infrastructure package. Helpfully,
    it also includes comments for each line, so you should find the generated code
    easy to follow. The import statements include the Spark libraries, as well as
    the geopy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot Chat 生成了应该放入 infrastructure 包中名为 asset_location_spark_adapter.py 的文件的类。幸运的是，它还为每一行包括了注释，因此你应该会发现生成的代码很容易跟踪。导入语句包括
    Spark 库以及 geopy。
- en: Listing 5.21 The imports required to run Spark
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.21 运行 Spark 所需的导入项
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The class begins with an overstuffed constructor that defines the schema Spark
    will use when it translates the JSON to a DataFrame.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 课程以一个过于臃肿的构造函数开始，该构造函数定义了 Spark 在将 JSON 转换为 DataFrame 时将使用的模式。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The `AssetLocationSparkAdapter`, as defined, is a blocking process. Therefore,
    your FastAPI application will not “fully” boot until the Spark process has been
    killed. You would want this to be a standalone process, or you would need to introduce
    an asynchronous framework to have these two processes run concomitantly.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如定义的 `AssetLocationSparkAdapter` 是一个阻塞进程。因此，在 Spark 进程被终止之前，你的 FastAPI 应用程序将不会“完全”启动。你希望这是一个独立的进程，或者你需要引入一个异步框架来使这两个进程同时运行。
- en: Next, it will start up a local Spark instance/session that will allow Spark
    to connect to the Kafka topic and continuously stream in the records.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它将启动一个本地 Spark 实例/会话，允许 Spark 连接到 Kafka 主题并持续地流式处理记录。
- en: Listing 5.22 The `AssessLocationSparkAdapter`, which is responsible for processing
    the Kafka topic and generating notifications
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.22 负责处理 Kafka 主题并生成通知的 `AssessLocationSparkAdapter`
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The final section of the `AssetLocationSparkAdapter` class will calculate the
    distance from the asset’s current location to Chicago’s. If the difference is
    greater than 25 miles, then it will send the result set to the console. Additionally,
    it provides a method to start and stop the adapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`AssetLocationSparkAdapter` 类的最后一部分将计算资产当前位置到芝加哥的距离。如果差距大于 25 英里，则将结果集发送到控制台。此外，它还提供了一个方法来启动和停止适配器。'
- en: Listing 5.23 The `AssessLocationSparkAdapter` calculating the distance from
    the current `Asset’s` location from Chicago
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.23 `AssessLocationSparkAdapter` 计算当前`资产`位置与芝加哥的距离
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The calculate_distance method takes the logitude and latitude of the asset’s
    location and determines the distance from Chicaogo using the geopy.distance function.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: calculate_distance 方法接受资产位置的经度和纬度，并使用 geopy.distance 函数确定距离芝加哥的距离。
- en: Listing 5.24 The function that Spark uses to calculate the distance between
    Chi-town and your `Asset`
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.24 Spark 用于计算芝加哥和你的`资产`之间距离的函数
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In this instance, the code that Copilot Chat produced had some issues preventing
    it from running locally. After running it locally, encountering these issues,
    and trolling Stack Overflow, you would have found a solution to the two main issues
    with the code: a missing environmental variable for running locally, and failing
    to register your UDF (User Defined Function). Thankfully, you do not need to do
    the testing and research, as a solution is provided in listing 5.23.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Copilot Chat 生成的代码存在一些问题，阻止其在本地运行。在本地运行并遇到这些问题后，搜索 Stack Overflow，你会发现代码的两个主要问题的解决方案：缺少用于本地运行的环境变量，以及未能注册您的
    UDF（用户定义的函数）。幸运的是，你不需要进行测试和研究，因为清单 5.23 中提供了解决方案。
- en: Listing 5.25 Edits required to run the application locally
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.25 在本地运行应用程序所需的编辑
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally, to run your Spark application, you would update `main.py` with the
    following code in the `main` function.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要运行你的 Spark 应用程序，你需要在 `main.py` 中的 `main` 函数中更新以下代码。
- en: Listing 5.26 Updates to the `main` function
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.26 对 `main` 函数的更新
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you enter locations for your asset into the Kafka console producer that are
    further than twenty-five miles from downtown Chicago, you will notice entries
    get written to the console. It would be trivial to update the class to output
    these results to Twilio’s SMS API or an email service such as SendGrid.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将资产位置输入到 Kafka 控制台生产者中时，如果距离芝加哥市中心超过二十五英里，你会注意到条目被写入到控制台中。更新类以将这些结果输出到 Twilio
    的短信 API 或类似 SendGrid 的电子邮件服务是微不足道的。
- en: Listing 5.27 The streaming output from your asset location
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.27 你的资产位置的流式输出
- en: '[PRE34]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Congratulations, you are tracking your assets in real-time and sending real-time
    alerts should your corporate resources grow legs and walk away.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你正在实时追踪你的资产，并在公司资源离开的时候发送实时警报。
- en: 5.4 Summary
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 总结
- en: GitHub Copilot Chat is an innovative tool that brings together the comprehensive
    language understanding of ChatGPT and the handy features of Copilot. It's a noteworthy
    development in the realm of programming assistance, particularly for providing
    detailed and contextually relevant suggestions in real-time, fostering a more
    efficient coding experience.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub Copilot Chat 是一款创新性工具，结合了 ChatGPT 的全面语言理解和 Copilot 的便捷功能。这是编程辅助领域的一个值得注意的发展，特别是在实时提供详细且上下文相关的建议方面，促进了更高效的编码体验。
- en: The Mediator design pattern is a distinct behavioral pattern that facilitates
    a high level of decoupling between objects, thus enhancing the modularity of your
    code. By encompassing the interactions between objects within a mediator object,
    objects can communicate indirectly, which reduces dependencies and promotes code
    reusability and ease of modification.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中介者设计模式是一种独特的行为模式，它促进了对象之间高度解耦，从而增强了代码的模块化。通过在一个中介者对象中包含对象之间的交互，对象可以间接地进行通信，从而减少了依赖性，促进了代码的可重用性和易修改性。
- en: Apache Kafka serves as a robust, distributed streaming platform engineered for
    creating real-time data pipelines and streaming applications. It can effectively
    handle data streams from a multitude of sources and transmit them to various consumers,
    making it an ideal solution for use cases that require handling substantial volumes
    of real-time or near-real-time data. It's important to remember that Kafka is
    optimized for append-only, immutable data and not for use cases that need record
    updates or deletions, or complex querying.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Kafka 是一个健壮的、分布式的流平台，专为创建实时数据管道和流应用而设计。它可以有效处理来自多种来源的数据流，并将其传输给各种消费者，使其成为处理大量实时或准实时数据的理想解决方案。需要记住的是，Kafka
    优化了追加式、不可变数据，而不适用于需要记录更新或删除，或复杂查询的用例。
- en: Apache Spark stands as a high-performance, distributed data processing engine
    renowned for its speed, user-friendliness, and advanced analytics capabilities.
    It's highly suitable for scenarios necessitating real-time data processing or
    for operations on enormous datasets. However, for simpler tasks such as basic
    analytics or straightforward aggregations, a traditional relational database could
    be a more appropriate choice.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 是一款性能卓越的、分布式的数据处理引擎，以其速度、易用性和高级分析功能而闻名。它非常适用于需要实时数据处理或对大量数据集进行操作的场景。然而，对于诸如基本分析或简单聚合等较简单的任务，传统的关系型数据库可能是更合适的选择。
- en: Generative AI, despite its rapid evolution, is not infallible. It's crucial
    to meticulously review all generated output to ensure it aligns with your specific
    requirements and quality standards. While generative AI is not a substitute for
    deep domain knowledge or coding expertise, it significantly enhances productivity
    by providing valuable insights and reducing the time spent on routine tasks.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管生成式人工智能迅速发展，但并非无懈可击。仔细审查所有生成的输出以确保其符合你的特定要求和质量标准至关重要。虽然生成式人工智能不能替代深入的领域知识或编码专业知识，但它通过提供有价值的见解和减少在例行任务上花费的时间来显著提高生产力。
