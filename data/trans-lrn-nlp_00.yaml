- en: front matter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言部分
- en: preface
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: Over the past couple of years, it has become increasingly difficult to ignore
    the breakneck speed at which the field of natural language processing (NLP) has
    been progressing. Over this period, you have likely been bombarded with news articles
    about trending NLP models such as ELMo, BERT, and more recently GPT-3\. The excitement
    around this technology is warranted, because these models have enabled NLP applications
    we couldn’t imagine would be practical just three years prior, such as writing
    production code from a mere description of it, or the automatic generation of
    believable poetry and blogging.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，很难忽视自然语言处理（NLP）领域的飞速发展。在此期间，您可能已经被关于流行NLP模型（如ELMo、BERT，以及最近的GPT-3）的新闻文章所淹没。这种技术周围的兴奋是有道理的，因为这些模型使我们能够实现三年前我们无法想象的NLP应用，比如仅仅从对代码的描述中编写出生产代码，或者自动生成可信的诗歌和博客。
- en: A large driver behind this advance has been the focus on increasingly sophisticated
    transfer learning techniques for NLP models. Transfer learning is an increasingly
    popular and exciting paradigm in NLP because it enables you to adapt or transfer
    the knowledge acquired from one scenario to a different scenario, such as a different
    language or task. It is a big step forward for the democratization of NLP and,
    more widely, artificial intelligence (AI), allowing knowledge to be reused in
    new settings at a fraction of the previously required resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 推动这一进步的一个重要因素是对自然语言处理模型的越来越复杂的迁移学习技术的关注。迁移学习在自然语言处理中越来越受欢迎和激动人心，因为它使您能够将从一个场景中获得的知识适应或转移到另一个场景，例如不同的语言或任务。这对于自然语言处理的民主化以及更广泛地说人工智能（AI）是一个重大进步，允许知识以前所需资源的一小部分在新环境中得到重复使用。
- en: As a citizen of the West African nation of Ghana, where many budding entrepreneurs
    and inventors do not have access to vast computing resources and where so many
    fundamental NLP problems remain to be solved, this topic is particularly personal
    to me. This paradigm empowers engineers in such settings to build potentially
    life-saving NLP technologies, which would simply not be possible otherwise.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为加纳西非国家的公民，在那里，许多新兴的企业家和发明家无法获得大量的计算资源，并且许多基本的自然语言处理问题仍然有待解决，这个主题对我来说尤为重要。这种范式赋予了这样的环境中的工程师们权力，使他们能够构建潜在的拯救生命的自然语言处理技术，否则这是不可能的。
- en: I first encountered these ideas in 2017, while working on open source automatic
    machine learning technologies within the US Defense Advanced Research Projects
    Agency (DARPA) ecosystem. We used transfer learning to reduce the requirement
    for labeled data by training NLP systems on simulated data first and then transferring
    the model to a small set of real labeled data. The breakthrough model ELMo emerged
    shortly after and inspired me to learn more about the topic and explore how I
    could leverage these ideas further in my software projects.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次接触到这些想法是在2017年，当时我正在美国国防高级研究计划局（DARPA）生态系统内从事开源自动机器学习技术的工作。我们使用迁移学习来减少对标记数据的需求，首先在模拟数据上训练自然语言处理系统，然后将模型转移到少量真实标记数据上。突破性模型ELMo随后出现，激发了我对该主题的进一步学习和探索，以了解如何在我的软件项目中进一步利用这些想法。
- en: Naturally, I discovered that a comprehensive practical introduction to the topic
    did not exist, due to the sheer novelty of these ideas and the speed at which
    the field is moving. When an opportunity to write a practical introduction to
    the topic presented itself in 2019, I didn’t think twice. You are holding in your
    hands the product of approximately two years of effort toward this purpose. This
    book will quickly bring you up to speed on key recent NLP models in the space
    and provide executable code you will be able to modify and reuse directly in your
    own projects. Although it would be impossible to cover every single architecture
    and use case, we strategically cover architectures and examples that we believe
    will arm you with fundamental skills for further exploration and staying up-to-date
    in this burgeoning field on your own.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自然而然地，我发现由于这些想法的绝对新颖性和领域发展速度的快速性，这个主题没有全面的实用介绍。2019年，我有机会撰写这个主题的实用介绍，我没有犹豫。你手里拿着的是我大约两年努力的成果。这本书将快速带领你了解该领域的关键近期自然语言处理模型，并提供可执行代码，您将能够直接修改和重用在自己的项目中。尽管不可能涵盖每一个体系结构和用例，但我们战略性地涵盖了我们认为会装备您基本技能以便在这个新兴领域中进一步探索并保持最新的架构和示例。
- en: You made a good decision when you decided to learn more about this topic. Opportunities
    for novel theories, algorithmic methodologies, and breakthrough applications abound.
    I look forward to hearing about the transformational positive impact you make
    on the society around you with it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当你决定更多了解这个话题时，你做出了一个明智的决定。涌现出机会来探索新理论、算法方法和突破性应用。我期待着听到您在周围社会上所产生的转型积极影响。
- en: acknowledgments
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: I am grateful to members of the NLP Ghana open source community, where I have
    had the privilege to learn more about this important topic. The feedback from
    members of the group and users of our tools has served to underscore my understanding
    of how transformational this technology truly is. This has inspired and motivated
    me to push this book across the finish line.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常感谢加纳自然语言处理（NLP）开源社区的成员，在那里我有幸学习更多关于这一重要主题的知识。该群体成员和我们工具的用户的反馈强调了我对这项技术变革的理解。这激励并激励我将这本书完成。
- en: I would like to thank my Manning development editor, Susan Ethridge, for the
    uncountable hours spent reading the manuscript, providing feedback, and guiding
    me through the many challenges. I am thankful for all the time and effort my technical
    development editor, Al Krinker, put in to help me improve the technical dimension
    of my writing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢我的Manning开发编辑苏珊·埃斯里奇，她花了无数小时阅读手稿，提供反馈，并指导我度过了许多挑战。我感谢我的技术开发编辑艾尔·克林克尔为帮助我改进写作的技术维度所付出的所有时间和精力。
- en: I am grateful to all members of the editorial board, the marketing professionals,
    and other members of the production team that worked hard to make this book a
    reality. In no particular order, these include Rebecca Rinehart, Bert Bates, Nicole
    Butterfield, Rejhana Markanovic, Aleksandar Dragosavljevic´, Melissa Ice, Branko
    Latincic, Christopher Kaufmann, Candace Gillhoolley, Becky Whitney, Pamela Hunt,
    and Radmila Ercegovac.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我感谢所有编辑委员会成员、市场营销专业人员和其他努力使这本书成为现实的制作团队成员。这些人包括丽贝卡·赖恩哈特、伯特·贝茨、尼科尔·巴特菲尔德、雷哈娜·马尔卡诺维奇、亚历山大·德拉戈萨夫列维奇、梅丽莎·艾斯、布兰科·拉廷西奇、克里斯托弗·考夫曼、坎迪斯·吉尔霍利、贝基·惠特尼、帕梅拉·亨特和拉德米拉·埃尔塞戈瓦克，排名不分先后。
- en: The technical peer reviewers provided invaluable feedback at several junctures
    during this project, and the book would not be nearly as good without them. I
    am very grateful for their input. These include Andres Sacco, Angelo Simone Scotto,
    Ariel Gamino, Austin Poor, Clifford Thurber, Diego Casella, Jaume López, Manuel
    R. Ciosici, Marc-Anthony Taylor, Mathijs Affourtit, Matthew Sarmiento, Michael
    Wall, Nikos Kanakaris, Ninoslav Cerkez, Or Golan, Rani Sharim, Sayak Paul, Sebastián
    Palma, Sergio Govoni, Todd Cook, and Vamsi Sistla. I am thankful to the technical
    proofreader, Ariel Gamiño, for catching many typos and other errors during the
    proofreading process. I am grateful to all the excellent comments from book forum
    participants that further helped improve the book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目的几个关键时刻，技术同行审阅者提供了宝贵的反馈，没有他们，这本书就不会那么好。我非常感谢他们的意见。其中包括安德烈斯·萨科、安吉洛·西蒙尼·斯科托、艾瑞尔·加米诺、奥斯汀·普尔、克利福德·瑟伯、迭戈·卡塞拉、豪梅·洛佩兹、曼努埃尔·R·西奥西奇、马克·安东尼·泰勒、马西耶·阿弗尔蒂特、马修·萨尔门托、迈克尔·沃尔、尼科斯·卡纳卡里斯、尼诺斯拉夫·切尔克斯、奥尔·戈兰、拉尼·夏利姆、萨亚克·保罗、塞巴斯蒂安·帕尔马、塞尔吉奥·戈沃尼、托德·库克和万斯·西斯特拉。我感谢技术校对者艾瑞尔·加米诺在校对过程中捕捉到的许多拼写错误和其他错误。我感谢所有书籍论坛参与者的优秀评论，这进一步帮助改进了本书。
- en: I am extremely grateful to my wife, Diana, for supporting and encouraging this
    work. I am grateful to my Mom and my siblings—Richard, Gideon, and Gifty—for continuing
    to motivate me.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常感谢我的妻子戴安娜对这项工作的支持和鼓励。我感激我的母亲和我的兄弟姐妹——理查德、吉迪恩和吉夫蒂——对我继续激励我。
- en: about this book
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于这本书
- en: This book is an attempt to produce a comprehensive practical introduction to
    the important topic of transfer learning for NLP. Rather than focusing on theory,
    we stress building intuition via representative code and examples. Our code is
    written to facilitate quickly modifying and repurposing it to solve your own practical
    problems and challenges.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书试图为自然语言处理中的迁移学习这一重要主题提供全面实用的介绍。我们强调通过代表性代码和示例建立直观理解，而不是专注于理论。我们的代码编写旨在便于快速修改和重新利用，以解决您自己的实际问题和挑战。
- en: Who should read this book?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谁应该读这本书？
- en: To get the most out of this book, you should have some experience with Python,
    as well as some intermediate machine learning skills, such as an understanding
    of basic classification and regression concepts. It would also help to have some
    basic data manipulation and preprocessing skills with libraries such as Pandas
    and NumPy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用本书，您应具备一些Python经验，以及一些中级机器学习技能，如对基本分类和回归概念的理解。具备一些基本的数据处理和预处理技能，如使用Pandas和NumPy等库，也会有所帮助。
- en: That said, I wrote the book in a way that allows you to pick up these skills
    with a bit of extra work. The first three chapters will rapidly bring you up to
    speed on everything you need to know to grasp the transfer learning for NLP concepts
    sufficiently to apply in your own projects. Subsequently, following the included
    curated references on your own will solidify your prerequisite background skills,
    if that is something you feel that you need.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我写这本书的方式使你可以通过一点额外的工作掌握这些技能。前三章将迅速带你了解你需要掌握的一切，以充分理解迁移学习NLP的概念，并应用于你自己的项目中。随后，通过自行查阅包含的精选参考资料，你将巩固你的先修背景技能，如果你觉得有必要的话。
- en: Road map
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路线图
- en: The book is divided into three parts. You will get the most out of it by progressing
    through them in the order of appearance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为三个部分。按照它们的出现顺序逐步学习将让您收获最多。
- en: 'Part 1 reviews key concepts in machine learning, presents a historical overview
    of advances in machine learning that have enabled the recent progress in transfer
    learning for NLP, and provides the motivation for studying the subject. It also
    walks through a pair of examples that serve to both review your knowledge of more
    traditional NLP methods and get your hands dirty with some key modern transfer
    learning for NLP approaches. A chapter-level breakdown of covered concepts in
    this part of the book follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分回顾了机器学习的关键概念，提供了使最近的迁移学习NLP进展成为可能的机器学习进步的历史概述，并提供了研究该主题的动机。它还通过一对示例来回顾更传统的NLP方法的知识，并让您亲自动手使用一些关键的现代迁移学习NLP方法。本书此部分涵盖的概念章节级别的分解如下：
- en: Chapter 1 covers what exactly transfer learning is, both generally in AI and
    in the context of NLP. It also looks at the historical progression of technological
    advances that enabled it.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1章介绍了迁移学习的确切含义，包括在人工智能领域和自然语言处理（NLP）的背景下。它还探讨了促成迁移学习的技术进步的历史发展。
- en: Chapter 2 introduces a pair of representative example natural language processing
    (NLP) problems and shows how to obtain and preprocess data for them. It also establishes
    baselines for them using the traditional linear machine learning methods of logistic
    regression and support vector machines.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2章介绍了一对代表性的自然语言处理（NLP）问题，并展示了如何获取和预处理数据。它还使用传统的线性机器学习方法——逻辑回归和支持向量机——为它们建立了基准。
- en: Chapter 3 continues baselining the pair of problems from chapter 2 with the
    traditional tree-based machine learning methods—random forests and gradient boosting
    machines. It also baselines them using key modern transfer learning techniques,
    ELMo and BERT.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3章继续通过传统的基于树的机器学习方法——随机森林和梯度提升机——对第2章中的一对问题进行基准测试。它还使用关键的现代迁移学习技术ELMo和BERT对它们进行基准测试。
- en: 'Part 2 dives deeper into some important transfer learning NLP approaches based
    on shallow neural networks, that is, neural networks with relatively few layers.
    It also begins to explore deep transfer learning in more detail via representative
    techniques, such as ELMo, that employ recurrent neural networks (RNNs) for key
    functions. A chapter-level breakdown of covered concepts in this part of the book
    follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分深入探讨了基于浅层神经网络的一些重要的迁移学习NLP方法，即层次相对较少的神经网络。它还通过代表性技术（如ELMo）更详细地探讨了深度迁移学习，这些技术利用循环神经网络（RNN）进行关键功能。本书此部分涵盖的概念章节级别的分解如下：
- en: Chapter 4 applies shallow word and sentence embedding techniques, such as word2vec
    and sent2vec, to further explore some of our illustrative examples from part 1
    of the book. It also introduces the important transfer learning concepts of domain
    adaptation and multitask learning.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4章应用了浅层词和句子嵌入技术，如word2vec和sent2vec，进一步探索了本书第1部分的一些示例。它还介绍了领域自适应和多任务学习等重要的迁移学习概念。
- en: Chapter 5 introduces a set of deep transfer learning NLP methods that rely on
    RNNs, as well as a fresh pair of illustrative example datasets that will be used
    to study them.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 5 章介绍了一组依赖于 RNN 的深度迁移学习 NLP 方法，以及一对新的例子数据集，这些数据集将用于研究这些方法。
- en: Chapter 6 discusses the methods introduced in chapter 5 in more detail and applies
    them to the datasets introduced in the same chapter.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 6 章更详细地讨论了第 5 章介绍的方法，并将其应用于同一章节中介绍的数据集。
- en: 'Part 3 covers arguably the most important subfield in this space, namely, deep
    transfer learning techniques relying on transformer neural networks for key functions,
    such as BERT and GPT. This model architecture class is proving to be the most
    influential on recent applications, partly due to better scalability on parallel
    computing architectures than equivalent prior methods. This part also digs deeper
    into various adaptation strategies for making the transfer learning process more
    efficient. A chapter-level breakdown of covered concepts in this part of the book
    follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 部分涵盖了这一领域中可能最重要的子领域，即依赖于transformers神经网络进行关键功能的深度迁移学习技术，例如 BERT 和 GPT。这种模型架构类别正在证明在最近的应用中最具影响力，部分原因是在并行计算架构上比等效的先前方法具有更好的可扩展性。本部分还深入探讨了各种使迁移学习过程更有效的适应策略。书中此部分涵盖的概念的章节级细分如下：
- en: Chapter 7 describes the fundamental transformer architecture and uses an important
    variant of it—GPT—for some text generation and a basic chatbot.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 7 章介绍了基本的transformers架构，并使用其重要变体之一—GPT—进行了一些文本生成和基本聊天机器人。
- en: Chapter 8 covers the important transformer architecture BERT and applies it
    to a number of use cases, including question answering, filling in the blanks,
    and cross-lingual transfer to a low-resource language.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 8 章介绍了重要的transformers架构 BERT，并将其应用于多种用例，包括问答、填空以及向低资源语言的跨语言转移。
- en: Chapter 9 introduces some adaptation strategies meant to make the transfer learning
    process more efficient. This includes the strategies of discriminative fine-tuning
    and gradual unfreezing from the method ULMFiT, as well as knowledge distillation.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 9 章介绍了一些旨在使迁移学习过程更有效的适应策略。其中包括区分性微调和逐步解冻（来自方法 ULMFiT 的方法）以及知识蒸馏。
- en: Chapter 10 introduces additional adaptation strategies, including embedding
    factorization and parameter sharing—strategies behind the ALBERT method. The chapter
    also covers adapters and sequential multitask adaptation.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 10 章介绍了额外的适应策略，包括嵌入因子分解和参数共享—这些是 ALBERT 方法背后的策略。该章还涵盖了适配器和顺序多任务适应。
- en: Chapter 11 concludes the book by reviewing important topics and briefly discussing
    emerging research topics and directions, such as the need to think about and mitigate
    potential negative impacts of the technology. These include biased predictions
    on different parts of the population and the environmental impact of training
    these large models.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 11 章通过回顾重要主题并简要讨论新兴的研究主题和方向来结束本书，例如需要考虑和减轻技术可能产生的潜在负面影响。这些包括对不同人群的偏见预测以及训练这些大型模型的环境影响。
- en: Software requirements
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件要求
- en: Kaggle notebooks are the recommended way of executing these methods, because
    they allow you to get moving right away without any setup delays. Moreover, the
    free GPU resources provided by this service at the time of writing expand the
    accessibility of all these methods to people who may not have access to powerful
    GPUs locally, which is consistent with the “democratization of AI” agenda that
    excites so many people about NLP transfer learning. Appendix A provides a Kaggle
    quick start guide and a number of the author’s personal tips on how to maximize
    the platform’s usefulness. However, we anticipate that most readers should find
    it pretty self-explanatory to get started. We have hosted all notebooks publicly
    on Kaggle with all required data attached to enable you to start executing code
    in a few clicks. However, please remember to “copy and edit” (fork) notebooks—instead
    of copying and pasting into a new Kaggle notebook—because this will ensure that
    the resulting libraries in the environment match those that we wrote the code
    for.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 笔记本是执行这些方法的推荐方式，因为它们可以让您立即开始，无需进行任何设置延迟。此外，在编写本文时，此服务提供的免费 GPU 资源扩大了所有这些方法的可访问性，使得那些可能没有本地强大
    GPU 访问权限的人也能够使用，这与“AI 民主化”议程一致，激发了许多人对 NLP 迁移学习的兴趣。附录 A 提供了 Kaggle 快速入门指南和作者个人关于如何最大程度地发挥平台作用的一些建议。但是，我们预计大多数读者应该会发现开始使用是相当简单的。我们已经在
    Kaggle 上公开托管了所有笔记本，并附上了所有必需的数据，以便您只需点击几下即可开始执行代码。但是，请记住“复制并编辑”（fork）笔记本——而不是将其复制并粘贴到新的
    Kaggle 笔记本中——因为这样可以确保结果库在环境中与我们为代码编写的库匹配。
- en: About the code
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于代码
- en: This book contains many examples of source code both in numbered listings and
    in line with normal text. In both cases, source code is formatted in a `fixed-width
    font like this` to separate it from ordinary text. Sometimes code is also `**in
    bold**` to highlight code that has changed from previous steps in the chapter,
    such as when a new feature adds to an existing line of code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书包含了许多源代码示例，既有编号列表中的，也有与普通文本一起的。在这两种情况下，源代码都以`像这样的等宽字体`格式化，以便与普通文本分开。有时代码也会以**粗体**显示，以突出显示章节中已更改的代码，例如当新功能添加到现有代码行时。
- en: In many cases, the original source code has been reformatted; we’ve added line
    breaks and reworked indentation to accommodate the available page space in the
    book. In rare cases, even this was not enough, and listings include line-continuation
    markers (➥). Additionally, comments in the source code have often been removed
    from the listings when the code is described in the text. Code annotations accompany
    many of the listings, highlighting important concepts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，原始源代码已经重新格式化；我们添加了换行符并重新排列了缩进以适应书中可用的页面空间。在极少数情况下，即使这样还不够，列表中还包括了行继续标记（➥）。此外，在文本中描述代码时，源代码中的注释通常已从列表中删除。代码注释伴随许多列表，突出显示重要概念。
- en: The code for the examples in this book is available for download from the Manning
    website at [http://www.manning.com/downloads/2116](http://www.manning.com/downloads/2116)and
    from GitHub at [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本书示例中的代码可从 Manning 网站下载，网址为[http://www.manning.com/downloads/2116](http://www.manning.com/downloads/2116)，也可以从
    GitHub 上下载，网址为[https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)。
- en: liveBook discussion forum
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: liveBook 讨论论坛
- en: Purchase of *Transfer Learning for Natural Language Processing* includes free
    access to a private web forum run by Manning Publications where you can make comments
    about the book, ask technical questions, and receive help from the author and
    from other users. To access the forum, go to [https://livebook.manning.com/#!/book/transfer-learning-for-natural-language-processing/discussion](https://livebook.manning.com/#!/book/transfer-learning-for-natural-language-processing/discussion)
    . You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/
    discussion](https://livebook.manning.com/#!/discussion).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 购买《自然语言处理的迁移学习》包括免费访问由 Manning Publications 运营的私人网络论坛，在该论坛上，您可以对该书发表评论，提出技术问题，并从作者和其他用户那里获得帮助。要访问论坛，请转到[https://livebook.manning.com/#!/book/transfer-learning-for-natural-language-processing/discussion](https://livebook.manning.com/#!/book/transfer-learning-for-natural-language-processing/discussion)。您还可以在[https://livebook.manning.com/#!/discussion](https://livebook.manning.com/#!/discussion)了解有关
    Manning 论坛及行为规范的更多信息。
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking the author some challenging questions lest his interest
    stray! The forum and the archives of previous discussions will be accessible from
    the publisher’s website as long as the book is in print.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Manning 致力于为我们的读者提供一个场所，让个人读者之间以及读者与作者之间进行有意义的对话。这不是对作者参与的特定数量的承诺，作者对论坛的贡献仍然是自愿的（未付酬）。我们建议您尝试向作者提出一些具有挑战性的问题，以免他失去兴趣！
    只要这本书还在印刷，您都可以从出版商的网站访问论坛和以前的讨论档案。
- en: about the author
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者简介
- en: '**Paul Azunre** holds a PhD in Computer Science from MIT and has served as
    a principal investigator on several DARPA research programs. He founded Algorine
    Inc., a research lab dedicated to advancing AI/ML and identifying scenarios where
    they can have a significant social impact. Paul also co-founded Ghana NLP, an
    open source initiative focused on using NLP and Transfer Learning with Ghanaian
    and other low-resource languages.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**Paul Azunre** 持有麻省理工学院计算机科学博士学位，并曾担任多个DARPA研究项目的主要研究员。他创立了Algorine Inc.，一个致力于推动人工智能/机器学习发展并确定其可能产生重大社会影响的研究实验室。Paul
    还共同创立了 Ghana NLP，一个专注于使用自然语言处理和迁移学习处理加纳语和其他资源稀缺语言的开源倡议。'
- en: about the cover illustration
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 封面插图说明
- en: The figure on the cover of *Transfer Learning for Natural Language Processing*
    is captioned “Moluquoise,” or Moluccan woman. The illustration is taken from a
    collection of dress costumes from various countries by Jacques Grasset de Saint-Sauveur
    (1757-1810), titled *Costumes civils actuels de tous les peuples connus*, published
    in France in 1788\. Each illustration is finely drawn and colored by hand. The
    rich variety of Grasset de Saint-Sauveur’s collection reminds us vividly of how
    culturally apart the world’s towns and regions were just 200 years ago. Isolated
    from each other, people spoke different dialects and languages. In the streets
    or in the countryside, it was easy to identify where they lived and what their
    trade or station in life was just by their dress.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理的迁移学习* 封面上的图案标题为“Moluquoise”，或者说是马鲁古妇女。 这幅插图取自法国1788年出版的雅克·格拉塞·德·圣索维尔（Jacques
    Grasset de Saint-Sauveur，1757-1810）的 *所有已知民族的现代民族服饰* 系列，每个插图都经过精细手绘和上色。格拉塞·德·圣索维尔（Grasset
    de Saint-Sauveur）收集的丰富多样性生动地提醒我们，就在200年前，世界的城镇和地区文化迥然不同。 人们相互隔离，说着不同的方言和语言。 在街上或乡间，仅通过他们的服装就可以轻松辨认出他们住在哪里以及他们的贸易或生活状况。'
- en: The way we dress has changed since then and the diversity by region, so rich
    at the time, has faded away. It is now hard to tell apart the inhabitants of different
    continents, let alone different towns, regions, or countries. Perhaps we have
    traded cultural diversity for a more varied personal life—certainly for a more
    varied and fast-paced technological life.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时起，我们的着装方式已经发生了变化，地区的多样性也在消失。 现在很难区分不同大陆的居民，更不用说不同的城镇、地区或国家了。 或许我们已经用文化多样性换取了更丰富的个人生活，肯定是为了更丰富和快节奏的技术生活。
- en: At a time when it is hard to tell one computer book from another, Manning celebrates
    the inventiveness and initiative of the computer business with book covers based
    on the rich diversity of regional life of two centuries ago, brought back to life
    by Grasset de Saint-Sauveur’s pictures.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在很难将一本计算机书籍与另一本区分开来的时候，Manning 通过基于两个世纪前地区生活丰富多样性的丰富多样的书籍封面，庆祝了计算机业的创造力和主动性，这些封面是格拉塞·德·圣索维尔的图片重新呈现的。
