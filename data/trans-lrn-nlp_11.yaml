- en: 8 Deep transfer learning for NLP with BERT and multilingual BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用BERT和多语言BERT的NLP深度迁移学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Using pretrained Bidirectional Encoder Representations from Transformers (BERT)
    architecture to perform some interesting tasks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的双向编码器表示来自变换器（BERT）架构来执行一些有趣的任务
- en: Using the BERT architecture for cross-lingual transfer learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BERT架构进行跨语言迁移学习
- en: 'In this chapter and the previous chapter, our goal is to cover some representative
    deep transfer learning modeling architectures for natural language processing
    (NLP) that rely on a recently popularized neural architecture—*the transformer*[¹](#pgfId-1107268)—for
    key functions. This is arguably the most important architecture for NLP today.
    Specifically, our goal has to look at modeling frameworks such as the generative
    pretrained transformer (GPT),[²](#pgfId-1107272) Bidirectional Encoder Representations
    from Transformers (BERT),[³](#pgfId-1107276) and multilingual BERT (mBERT).[⁴](#pgfId-1107281)
    These methods employ neural networks with even more parameters than the deep convolutional
    and recurrent neural network models that we looked at previously. Despite their
    larger size, they have exploded in popularity because they scale comparatively
    more effectively on parallel computing architecture. This enables even larger
    and more sophisticated models to be developed in practice. To make the content
    more digestible, we split the coverage of these models into two chapters/parts:
    we covered the transformer and GPT neural network architectures in the previous
    chapter, and in this next chapter, we focus on BERT and mBERT.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章和上一章，我们的目标是介绍一些代表性的深度迁移学习建模架构，这些架构依赖于最近流行的神经架构——*transformer*[¹](#pgfId-1107268)——来进行关键功能的自然语言处理（NLP）。这可以说是当今NLP中最重要的架构。具体来说，我们的目标是研究一些建模框架，例如生成式预训练变换器（GPT），[²](#pgfId-1107272)
    双向编码器表示来自变换器（BERT），[³](#pgfId-1107276) 和多语言BERT（mBERT）。[⁴](#pgfId-1107281) 这些方法使用的神经网络的参数比我们之前介绍的深度卷积和循环神经网络模型更多。尽管它们体积更大，但由于它们在并行计算架构上的比较效率更高，它们的流行度急剧上升。这使得实际上可以开发出更大更复杂的模型。为了使内容更易理解，我们将这些模型的覆盖分成两章/部分：我们在上一章中介绍了变换器和GPT神经网络架构，而在接下来的这章中，我们将专注于BERT和mBERT。
- en: As a reminder, BERT is a transformer-based model that we encountered briefly
    in chapters 3 and 7\. It was trained with the *masked modeling objective* to fill
    in the blanks. Additionally, it was trained with the *next sentence prediction*
    task, to determine whether a given sentence is a plausible following sentence
    after a target sentence. mBERT, which stands for “multilingual BERT,” is effectively
    BERT pretrained on over 100 languages simultaneously. Naturally, this model is
    particularly well-suited for cross-lingual transfer learning. We will show how
    the multilingual pretrained weights checkpoint can facilitate creating BERT embeddings
    for languages that were not even originally included in the multilingual training
    corpus. Both BERT and mBERT were created at Google.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，BERT是基于transformer的模型，我们在第3章和第7章中简要介绍过。它是使用*masked modeling objective*进行训练来填补空白。此外，它还经过了“下一个句子预测”任务的训练，以确定给定句子是否是目标句子后的合理跟随句子。mBERT，即“多语言BERT”，实际上是针对100多种语言同时预训练的BERT。自然地，这个模型特别适用于跨语言迁移学习。我们将展示多语言预训练权重检查点如何促进为初始未包含在多语言训练语料库中的语言创建BERT嵌入。BERT和mBERT均由Google创建。
- en: The first section of this chapter dives deeper into BERT, and we apply it to
    the important question-answering application as a representative example in a
    standalone section. The chapter concludes with an experiment showing the transfer
    of pretrained knowledge from mBERT pretrained weights to a BERT embedding for
    a new language. This new language was not initially included in the multilingual
    corpus used to generate the pretrained mBERT weights. We use the Ghanaian language
    Twi as the illustrative language in this case.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一节深入探讨了BERT，并将其应用于重要的问答应用作为一个独立的示例。该章节通过实验展示了预训练知识从mBERT预训练权重转移到新语言的BERT嵌入的迁移。这种新语言最初并未包含在用于生成预训练mBERT权重的多语料库中。在这种情况下，我们使用加纳语Twi作为示例语言。
- en: Let’s proceed to analyzing BERT further in the next section.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节继续分析BERT。
- en: 8.1 Bidirectional Encoder Representations from Transformers (BERT)
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 双向编码器表示来自变换器（BERT）
- en: In this section we present arguably the most popular and influential transformer-based
    neural network architecture for NLP transfer learning—the Bidirectional Encoder
    Representations from Transformers (BERT) model—which, as we previously mentioned,
    was also named after a popular *Sesame Street* character as a nod to the trend
    started by ELMo. Recall that ELMo does essentially what the transformer does but
    with recurrent neural networks. We encountered both of these models first in chapter
    1 during our overview of NLP transfer learning history. We also used them for
    a pair of classification problems, using TensorFlow Hub and Keras, in chapter
    3\. If you do not recall these exercises, it may be beneficial to review them
    before you continue with this section. Coupled with the previous chapter, these
    previews of the model have brought you to a good place to understand in more detail
    how the model functions, which is our goal in this section.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了可能是最受欢迎和最具影响力的基于Transformer的神经网络架构，用于自然语言处理的迁移学习——双向编码器表示的Transformer（BERT）模型，正如我们之前提到的，它也是以流行的*Sesame
    Street*角色命名的，向ELMo开创的潮流致敬。回想一下ELMo本质上就是transformers做的事情，但是使用的是循环神经网络。我们在第1章首次遇到了这两种模型，在我们对自然语言处理迁移学习历史的概述中。我们还在第3章中使用了它们进行了一对分类问题，使用了TensorFlow
    Hub和Keras。如果您不记得这些练习，可能有必要在继续本节之前进行复习。结合上一章，这些模型的预览使您对了解模型的更详细功能处于一个很好的位置，这是本节的目标。
- en: BERT is an early pretrained language model that was developed after ELMo and
    GPT but which outperformed both on most tasks in the General Language-Understanding
    Evaluation (GLUE) dataset because it is *bidirectionally trained*. We discussed
    in chapter 6 how ELMo combined left-to-right and right-to-left LSTMs to achieve
    bidirectional context. In the previous chapter, we also discussed how the masked
    self-attention of the GPT model, by virtue of its stacking of the transformer
    decoders, makes it better suited for causal text generation. Unlike these models,
    BERT achieves bidirectional context for every input token *at the same time* by
    stacking transformer encoders rather than decoders. Recall from our discussion
    of self-attention in every BERT layer in section 7.2 that the computation for
    every token considers every other token in both directions. Whereas ELMo does
    achieve bidirectionality by putting together the two directions, GPT is a causal
    unidirectional model. Simultaneous bidirectionality in every layer of BERT appears
    to give it a deeper sense of language context.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是早期预训练语言模型，开发于ELMo和GPT之后，但在普通语言理解评估（GLUE）数据集的大多数任务中表现出色，因为它是*双向训练*的。我们在第6章讨论了ELMo如何将从左到右和从右到左的LSTM组合起来实现双向上下文。在上一章中，我们还讨论了GPT模型的掩码自注意力如何通过堆叠transformers解码器更适合因果文本生成。与这些模型不同，BERT通过堆叠transformers编码器而不是解码器，为每个输入标记*同时*实现双向上下文。回顾我们在第7.2节中对BERT每个层中的自注意力的讨论，每个标记的计算都考虑了两个方向上的每个其他标记。而ELMo通过将两个方向放在一起实现了双向性，GPT是一种因果单向模型。BERT每一层的同时双向性似乎给了它更深层次的语言上下文感。
- en: BERT was trained with the masked language modeling (MLM) fill-in-the-blanks
    type of prediction objective. Tokens in the training text are randomly masked,
    and the model is tasked with predicting the masked tokens. For illustration, consider
    again a slightly modified version of our example sentence, “He didn’t want to
    talk about cells on the cell phone, a subject he considered very boring.” To use
    MLM, we may transform it into “He didn’t want to talk about cells on the cell
    phone, a [MASK] he considered very boring.” Here [MASK] is a special token indicating
    which words have been dropped. We then ask the model to predict the dropped word
    based on all the text it has observed during training up to that point. A trained
    model might predict that the masked word is “conversation” 40% of the time, “subject”
    35% percent of the time, and “topic” the remaining 25% of the time. Repeating
    this for billions of English examples during training builds up the model’s knowledge
    of the English language.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是通过掩码语言建模（MLM）填空预测目标进行训练的。在训练文本中，标记被随机掩码，模型的任务是预测掩码的标记。为了说明，再次考虑我们示例句子的略微修改版本，“他不想在手机上谈论细胞，他认为这个话题很无聊。”
    为了使用MLM，我们可以将其转换为“他不想在手机上谈论细胞，一个[MASK]，他认为这个话题很无聊。” 这里的[MASK]是一个特殊标记，指示哪些词已被省略。然后，我们要求模型根据其在此之前观察到的所有文本来预测省略的词。经过训练的模型可能会预测掩码词40%的时间是“conversation”，35%的时间是“subject”，其余25%的时间是“topic”。在训练期间重复执行这个过程，建立了模型对英语语言的知识。
- en: 'Additionally, a next-sentence prediction (NSP) objective was used to train
    BERT. Here, some sentences in the training text are replaced with random sentences,
    and the model is asked to predict whether a sentence B following a sentence A
    is a plausible follow-up. For illustration, let’s split our example sentence into
    two sentences: “He didn’t want to talk about cells on the cell phone. He considered
    the subject very boring.” We then might drop the second sentence and replace it
    with the somewhat random sentence, “Soccer is a fun sport.” A properly trained
    model would need to be able to detect the former as a potential plausible completion
    and the latter as implausible. We address both MLM and NSP objectives in this
    section via concrete coding exercise examples to aid your understanding of these
    concepts.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，BERT 的训练还使用了下一句预测（NSP）目标。在这里，训练文本中的一些句子被随机替换为其他句子，并要求模型预测句子 B 是否是句子 A 的合理续篇。为了说明，让我们将我们的示例句子分成两个句子：“他不想谈论手机上的细胞。他认为这个话题很无聊。”
    然后我们可能删除第二个句子，并用略微随机的句子替换它，“足球是一项有趣的运动。” 一个经过适当训练的模型需要能够检测前者作为潜在的合理完成，而将后者视为不合理的。我们通过具体的编码练习示例来讨论MLM和NSP目标，以帮助您理解这些概念。
- en: In the next subsection, we briefly describe the key aspects of the BERT architecture.
    We follow that with an application of the pipelines API concept in the transformers
    library to the task of question answering with a pretrained BERT model. We follow
    that up by example executions of the fill-in-the-blanks MLM task and the NSP task.
    For the NSP task, we use the transformers API directly to build your familiarity
    with it. Like in the previous chapter, we do not explicitly refine the pretrained
    BERT model on more-specific target data here. However, we do so in the last section
    of the chapter, where we will fine-tune a multilingual BERT model on monolingual
    Twi data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们简要描述了BERT架构的关键方面。我们接着介绍了将transformers库中的管道API概念应用于使用预训练BERT模型进行问答任务。我们随后通过示例执行填空MLM任务和NSP任务。对于NSP任务，我们直接使用transformers
    API来帮助您熟悉它。与上一章节类似，我们在这里没有明确地在更具体的目标数据上对预训练的BERT模型进行调优。然而，在本章的最后一节中，我们将在单语Twi数据上微调多语言BERT模型。
- en: 8.1.1 Model architecture
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 模型架构
- en: You may recall from section 7.1.1, where we visualized BERT self-attention,
    that BERT is essentially a stacked set of encoders of the original encoder-decoder
    transformer architecture in figure 7.1\. The BERT model architecture is shown
    in figure 8.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得第7.1.1节中我们可视化了BERT自注意力时，BERT本质上是图7.1中原始编码器-解码器变换器架构的一组叠加编码器。BERT模型架构如图8.1所示。
- en: '![08_01](../Images/08_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![08_01](../Images/08_01.png)'
- en: Figure 8.1 A high-level representation of the BERT architecture, showing stacked
    encoders, input embeddings, and positional encodings. The output from the top
    is used for both next-sentence prediction and fill-in-blanks masked language modeling
    objectives during training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 BERT架构的高级表示，显示堆叠的编码器、输入嵌入和位置编码。顶部的输出在训练期间用于下一句预测和填空遮蔽语言建模目标。
- en: 'As we discussed in the introduction, and as shown in the figure, during training
    we use the next-sentence prediction (NSP) and masked language modeling (MSM) objectives.
    BERT was originally presented in two flavors, BASE and LARGE. As shown in figure
    8.1, BASE stacks 12 encoders whereas LARGE stacks 24 encoders. As before—in both
    the GPT and the original transformer—the input is converted into vectors via an
    input embedding, and a positional encoding is added to them to give a sense of
    the position of every token in the input sequence. To account for the next-sentence-prediction
    task, for which the input is a pair of sentences A and B, an extra segment encoding
    step is added. The segment embeddings indicate which sentence a given token belongs
    to and are added to the input and positional encodings to yield the output that
    is fed to the encoder stack. This entire input transformation is visualized in
    figure 8.2 for our example sentence pair: “He didn’t want to talk about cells
    on the cell phone. He considered the subject very boring.”'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在介绍中讨论的，并且如图所示，在训练期间，我们使用下一句预测（NSP）和遮蔽语言建模（MSM）目标。BERT最初以两种风味呈现，BASE和LARGE。如图8.1所示，BASE堆叠了12个编码器，而LARGE堆叠了24个编码器。与之前一样——在GPT和原始Transformer中——通过输入嵌入将输入转换为向量，并向它们添加位置编码，以给出输入序列中每个标记的位置感。为了考虑下一句预测任务，其中输入是句子A和B的一对，添加了额外的段编码步骤。段嵌入指示给定标记属于哪个句子，并添加到输入和位置编码中，以产生输入到编码器堆栈的输出。我们的示例句对的整个输入转换在图8.2中可视化：“他不想在手机上谈论细胞。他认为这个主题非常无聊。”
- en: '![08_02](../Images/08_02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![08_02](../Images/08_02.png)'
- en: Figure 8.2 BERT input transformation visualization
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 BERT输入转换可视化
- en: A brief note about the `[CLS]` and `[SEP]` special tokens is worth bringing
    up at this point. Recall that the `[SEP]` token separates sentences and ends them,
    as discussed in previous sections. The `[CLS]` special token, on the other hand,
    is added to the beginning of every *input example*. Input example is the terminology
    used within the BERT framework to refer to the tokenized input text, as illustrated
    in figure 8.2\. The final hidden state of the `[CLS]` token is used as the aggregate
    sequence representation for classification tasks, such as entailment or sentiment
    analysis. `[CLS]` stands for “classification.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此时提到`[CLS]`和`[SEP]`特殊标记的简要说明值得一提。回想一下，`[SEP]`标记分隔句子并结束它们，如前几节所讨论的。另一方面，`[CLS]`特殊标记被添加到每个*输入示例*的开头。输入示例是BERT框架内部用来指代标记化的输入文本的术语，如图8.2所示。`[CLS]`标记的最终隐藏状态用作分类任务的聚合序列表示，例如蕴涵或情感分析。`[CLS]`代表“分类”。
- en: Before proceeding to the concrete examples using some of these concepts in the
    following subsections, recall that when we first encountered the BERT model in
    chapter 3, we converted input first into input examples and then into a special
    triplet form. These were *input IDs*, *input masks*, and *segment IDs*. We replicate
    listing 3.8 here to help you remember, because at the time, these terms had not
    been yet introduced.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续查看以下小节中使用一些这些概念的具体示例之前，请记得，在第三章中首次遇到BERT模型时，我们将输入首先转换为输入示例，然后转换为特殊的三元组形式。这些是*输入ID*，*输入掩码*和*段ID*。我们在这里复制了列表3.8以帮助你记忆，因为当时这些术语尚未被介绍。
- en: Listing 3.8 (Duplicated from ch. 3) Converting data to form expected by BERT,
    training
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8（从第3章复制）将数据转换为BERT期望的形式，训练
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Function for building the model
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于构建模型的函数
- en: ❷ We do not retrain any BERT layers but rather use the pretrained model as an
    embedding and retrain some new layers on top of it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们不重新训练任何BERT层，而是将预训练模型用作嵌入，并在其上重新训练一些新层。
- en: ❸ Vanilla TensorFlow initialization calls
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Vanilla TensorFlow初始化调用
- en: ❹ Creates a compatible tokenizer using the function in the BERT source repository
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用BERT源代码库中的函数创建兼容的分词器
- en: ❺ Converts data to InputExample format using the function in the BERT source
    repository
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用BERT源代码库中的函数将数据转换为InputExample格式
- en: ❻ Converts the InputExample format into triplet final BERT input format, using
    the function in the BERT source repository
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将 InputExample 格式转换为三元 BERT 输入格式，使用 BERT 源存储库中的函数
- en: ❼ Builds the model
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 构建模型
- en: ❽ Instantiates the variables
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 实例化变量
- en: ❾ Trains the model
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 训练模型
- en: Input IDs, as discussed in the previous chapter, are simply integer IDs of the
    corresponding token in the vocabulary—for the WordPiece tokenization used by BERT,
    the vocabulary size is 30,000\. Because the input to the transformer is of fixed
    length, which is defined by the hyperparameter `max_seq_length` in listing 3.8,
    shorter inputs need to be padded and longer inputs need to be truncated. Inputs
    masks are simply binary vectors of the same length, with 0s corresponding to pad
    tokens (`[PAD]`) and 1s corresponding to the actual input. Segment IDs are the
    same as described in figure 8.2\. The positional encodings and input embeddings,
    on the other hand, are handled internally by the TensorFlow Hub model and were
    not exposed to the user. It may be beneficial for you to work through chapter
    3 again to fully grasp this comparison.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章节所述，输入 ID 只是词汇表中对应标记的整数 ID——对于 BERT 使用的 WordPiece 分词，词汇表大小为 30,000。由于变换器的输入长度是由列表
    3.8 中的超参数 `max_seq_length` 定义的，因此需要对较短的输入进行填充，对较长的输入进行截断。输入掩码只是相同长度的二进制向量，其中 0
    对应填充标记 (`[PAD]`)，1 对应实际输入。段 ID 与图 8.2 中描述的相同。另一方面，位置编码和输入嵌入由 TensorFlow Hub 模型在内部处理，用户无法访问。可能需要再次仔细阅读第
    3 章才能充分理解这种比较。
- en: Although TensorFlow and Keras remain a critical component of any NLP engineer’s
    toolbox—with unmatched flexibility and efficiency—the *transformers* library has
    arguably made these models a lot more approachable and easier to use for many
    engineers and applications. In the following subsections, we apply BERT from this
    library instead to the critical applications of question answering, filling in
    the blanks, and next-sentence prediction.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 TensorFlow 和 Keras 仍然是任何自然语言处理工程师工具箱中至关重要的组件——具有无与伦比的灵活性和效率——但 *transformers*
    库无疑使这些模型对许多工程师和应用更加易于接近和使用。在接下来的小节中，我们将使用该库中的 BERT 应用于问题回答、填空和下一个句子预测等关键应用。
- en: 8.1.2 Application to question answering
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 问题回答的应用
- en: Question answering has captured the imaginations of computer scientists since
    the inception of the NLP field. It concerns having a computer automatically provide
    answers to questions posed by a human, given some specified context. Potential
    applications are limited only by the imagination. Prominent examples include medical
    diagnosis, fact checking, and chatbots for customer service. In fact, anytime
    you search on Google for something like “Who won the Super Bowl in 2010?” or “Who
    won the FIFA World Cup in 2006?” you are using question answering.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理领域的开端以来，问题回答一直吸引着计算机科学家的想象力。它涉及让计算机在给定某些指定上下文的情况下自动回答人类提出的问题。潜在的应用场景仅受想象力限制。突出的例子包括医学诊断、事实检查和客户服务的聊天机器人。事实上，每当你在谷歌上搜索像“2010年超级碗冠军是谁？”或“2006年谁赢得了FIFA世界杯？”这样的问题时，你正在使用问题回答。
- en: 'Let’s define question answering a bit more carefully. More specifically, we
    will consider *extractive question answering*, defined as follows: given a context
    paragraph p and question q, the task of question answering is concerned with producing
    the start and end integer indices in p where the answer is located. If no plausible
    answer exists in p, the system needs to be able to indicate this as well. Jumping
    directly into trying a simple example, as we do next using a pretrained BERT model
    with the transformers pipelines API, will help you make this much more concrete.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更加仔细地定义问题回答。更具体地说，我们将考虑 *抽取式问题回答*，定义如下：给定上下文段落 p 和问题 q，问题回答的任务是产生 p 中答案所在的起始和结束整数索引。如果
    p 中不存在合理的答案，系统也需要能够指示这一点。直接尝试一个简单的例子，如我们接下来使用预训练的 BERT 模型和 transformers pipelines
    API 做的，将帮助你更好地具体了解这一点。
- en: We pick an article from the World Economic Forum[⁵](#pgfId-1107395) about the
    effectiveness of masks and other lockdown policies on the COVID-19 pandemic in
    the United States. We pick the article summary as the context paragraph. Note
    that if no article summary were available, we could quickly generate one using
    a summarization pipeline from the same library. The initialization of the question-answering
    pipeline and context is carried out by the following code. Note that we are using
    BERT LARGE in this case, which has been fine-tuned on the Stanford Question-Answering
    Dataset (SQuAD),[⁶](#pgfId-1107400) the most extensive question-answering dataset
    to date. Note also that this is the default model transformers uses for this task,
    and we did not need to specify it explicitly. However, we do so for transparency.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从世界经济论坛[⁵](#pgfId-1107395)中选择了一篇有关口罩和其他封锁政策对美国COVID-19大流行的有效性的文章。我们选择文章摘要作为上下文段落。请注意，如果没有文章摘要可用，我们可以使用相同库中的摘要流水线快速生成一个。以下代码初始化了问答流水线和上下文。请注意，这种情况下我们使用了BERT
    LARGE，它已经在斯坦福问答数据集（SQuAD）[⁶](#pgfId-1107400)上进行了微调，这是迄今为止最广泛的问答数据集。还请注意，这是transformers默认使用的任务，默认模型，我们不需要显式指定。但是，我们为了透明度而这样做。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ These models would have been loaded by default, but we make it explicit for
    transparency. It is important to use a model that has been fine-tuned on SQuAD;
    otherwise, results will be poor.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这些模型通常会被默认加载，但我们明确指出以保持透明度。使用已在SQuAD上进行了微调的模型非常重要；否则，结果将很差。
- en: 'Having initialized the pipeline, let’s first see if we can automatically extract
    the essence of the article by asking what it is about. We do that with the following
    code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化了流水线之后，让我们首先看看是否能够通过询问文章的主题来自动提取文章的精髓。我们用以下代码来实现：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following output, which we can probably agree is a plausible
    response:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出，我们可能会认为这是一个合理的回答：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the relatively low score of 0.47 is indicative that the answer is
    missing some context. Something like “Effect of containment policies on COVID-19”
    is probably a better response, but because we are doing extractive question answering
    and this sentence is not in the context paragraph, this is the best the model
    can do. The low score can help flag this response for double-checking and/or improvement
    by a human.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，0.47相对较低的分数表明答案缺少一些上下文。类似“遏制政策对COVID-19的影响”可能是更好的回答，但因为我们正在进行提取式问答，而这个句子不在上下文段落中，所以这是模型能做到的最好的。低分数可以帮助标记此回答进行人工双重检查和/或改进。
- en: 'Why not ask some more questions? Let us see if the model knows what country
    is described in the article, using the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不问一些更多的问题？让我们看看模型是否知道文章中描述的是哪个国家，使用以下代码：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This produces the following output, which is exactly right, as indicated by
    the higher-than-before score of approximately 0.8:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出，正如以前的分数约为0.8所示，完全正确：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How about the disease being discussed?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的是哪种疾病？
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is spot-on, and the confidence is even higher than before at 0.98,
    as shown here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出完全正确，信心甚至比之前更高，达到了0.98，如下所示：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How about the time period?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那时间段呢？
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The low score of 0.22 associated with the output is indicative of the poor
    quality of the result, because a time range of April to June is discussed in the
    article but never in a contiguous chunk of text that can be extracted for a high-quality
    answer, as shown next:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与输出相关联的0.22的低分数表明结果质量差，因为文章中讨论了4月至6月的时间范围，但从未在连续的文本块中讨论，可以为高质量答案提取，如下所示：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: However, the ability to pick out just one end point of the range is arguably
    already a useful outcome. The low score here can alert a human to double-check
    this result. In an automated system, the goal is for such lower-quality answers
    to be the minority, requiring little human intervention overall.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅选择一个范围的端点能力已经是一个有用的结果。这里的低分数可以提醒人工检查此结果。在自动化系统中，目标是这样的较低质量答案成为少数，总体上需要很少的人工干预。
- en: Having introduced question answering, in the next subsection we address the
    BERT training tasks of filling in the blanks and next-sentence prediction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了问答之后，在下一小节中，我们将解决BERT训练任务的填空和下一句预测。
- en: 8.1.3 Application to fill in the blanks and next-sentence prediction tasks
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 应用于填空和下一句预测任务
- en: 'We use the article from the previous subsection for the exercises in this one.
    Let’s immediately proceed to defining a pipeline for filling in the blanks using
    the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一节的练习中使用了上一小节的文章。让我们立即开始编写一个用于填写空白的流程，使用以下代码：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we are using the BERT BASE model here. These tasks are fundamental
    to the training of any BERT model, so this is a reasonable choice, and no special
    fine-tuned models are needed. Having initialized the appropriate pipeline, we
    can now apply it to the first sentence of the article in the previous subsection.
    We drop the word “cases” by replacing it with the appropriate masking token, `[MASK]`,
    and ask the model to predict the dropped word, using the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里我们使用的是BERT BASE模型。这些任务对任何BERT模型的训练来说都是基本的，所以这是一个合理的选择，不需要特殊的微调模型。初始化适当的流程后，我们现在可以将它应用于上一小节中文章的第一句话。我们通过用适当的掩码标记`[MASK]`来删除“cases”这个词，并使用以下代码向模型提供已省略的词进行预测：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output shows with the top one being “deaths” and being an arguably plausible
    completion. Even the remaining suggestions could work in different contexts!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，最高的是“deaths”，这是一个可能合理的完成。即使剩下的建议也可以在不同的情境下起作用！
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We encourage you to play around by dropping various words from various sentences
    to convince yourself that this almost always works quite well. Our companion notebook
    does this for several more sentences, but we do not print those results here in
    the interest of brevity.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你尝试从各种句子中省略各种单词，以确信这几乎总是非常有效的。在节省篇幅的情况下，我们的附带笔记本会为几个更多的句子做到这一点，但我们不在这里打印这些结果。
- en: 'Let’s then proceed to the next-sentence prediction (NSP) task. At the time
    of writing, this task is not included in the pipelines API. We will thus use the
    transformers API directly, which will also give you more experience with it. The
    first thing we will need to do is make sure that a version of transformers greater
    than 3.0.0 is installed, because this task was included only in the library at
    that stage. We do this using the following code; Kaggle comes installed with an
    earlier version by default, at the time of writing:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续进行下一个句子预测（NSP）任务。在写作本文时，此任务尚未包含在pipelines API中。因此，我们将直接使用transformers
    API，这也将让您更加熟悉它。我们首先需要确保已安装transformers 3.0.0以上的版本，因为该任务仅在该阶段的库中包含。我们使用以下代码实现这一点；在写作本文时，Kaggle默认安装了较早的版本：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the version upgraded, we can load an NSP-specific BERT using the following
    code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 升级版本后，我们可以使用以下代码加载一个NSP-specific BERT：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ NSP-specific BERT
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ NSP-specific BERT
- en: ❷ Computes the final probabilities from raw outputs
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算原始输出的最终概率
- en: ❸ PyTorch models are trainable by default. For cheaper inference and execution
    repeatability, set to “eval” mode as shown here. Set back to “train” mode via
    model.train(). Not applicable to TensorFlow models!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PyTorch模型默认是可训练的。为了更便宜的推断和可执行重复性，将其设置为“eval”模式，如此处所示。通过model.train()将其设置回“train”模式。对于TensorFlow模型不适用！
- en: 'First, as a sanity check, we determine whether the first and second sentences
    are plausible completions as far as the model is concerned. We do that using the
    following code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作为健全性检查，首先我们要确定第一句和第二句是否从模型的角度来看是合理的完成。我们使用以下代码进行检查：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ The output is a tuple; the first item describes the relationship between the
    two sentences we are after.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输出是一个元组；第一项描述了我们追求的两个句子之间的关系。
- en: ❷ Computes the probability from raw numbers
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从原始数字计算概率
- en: 'Note the term `logits` in the code. This is a term for the raw input to the
    softmax. Passing `logits` through the softmax yields probabilities. The output
    from the code confirms that the correct relationship was found, as shown here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意代码中的术语`logits`。这是softmax函数的原始输入。通过softmax将`logits`传递，可以得到概率。代码的输出确认找到了正确的关系，如下所示：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let’s replace the second sentence with a somewhat random “Cats are independent.”
    This produces the following outcome:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将第二个句子替换为一个有点随机的“Cats are independent.” 这将产生以下结果：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It appears that things work as expected!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来一切都如预期的那样工作！
- en: You should now have a very good sense of which tasks BERT is solving during
    training. Note that so far in this chapter we have not fine-tuned BERT on any
    new domain or task-specific data. This was done on purpose to help you understand
    the model architecture without any distractions. In the following section, we
    demonstrate how fine-tuning can be carried out, by working on a cross-lingual
    transfer learning experiment. Transfer learning for all the other tasks we have
    already presented can be carried out analogously, and by completing the exercise
    in the next section, you will be in a good position to do so on your own.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该已经非常清楚 BERT 在训练中解决哪些任务了。需要注意的是，本章我们还没有将 BERT 调整到任何新域或任务特定的数据上进行微调。这是有意为之的，以帮助你在没有任何干扰的情况下了解模型架构。在下一节中，我们会演示如何进行微调，通过进行跨语言迁移学习实验。对于我们已经介绍过的所有其他任务，都可以采用类似的迁移学习方式进行，通过完成下一节练习，您将有很好的发挥空间去自己实践。
- en: 8.2 Cross-lingual learning with multilingual BERT (mBERT)
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 基于多语言 BERT（mBERT）的跨语言学习
- en: In this section, we carry out the second overall, and the first major, cross-lingual
    experiment of the book. More specifically, we are working on a transfer learning
    experiment that involves transferring knowledge from a multilingual BERT model
    to a language it was not originally trained to include. As before, the language
    we are using for our experiments will be Twi, a language considered “low-resource”
    due to a relative lack of availability of quality training data for a variety
    of tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行本书中第二个整体和第一个主要的跨语言实验。更具体地说，我们正在进行一个迁移学习实验，该实验涉及从多语言 BERT 模型中转移知识到其原始训练中不包含的语言。与之前一样，我们在实验中使用的语言将是
    Twi 语，这是一种被认为是“低资源”的语言，因为缺乏多种任务的高质量训练数据。
- en: Multilingual BERT (mBERT) is essentially BERT, as described in the previous
    section, applied to a multilingual corpus of about 100 concatenated language Wikipedias.[⁷](#pgfId-1107531)
    The language set was initially the top 100 largest Wikipedias and has been expanded
    to the top 104 languages. The language set does not include Twi but does include
    a handful of African languages such as Swahili and Yoruba. Because the sizes of
    the various language corpora differ widely, an “exponential smoothing” procedure
    is applied to undersample high-resource languages such as English and oversample
    low-resource languages such as Yoruba. As before, WordPiece tokenization is used.
    For our purposes, it suffices to remind you that this tokenization procedure is
    subword, as we saw in the previous sections. The only exceptions are Chinese,
    Japanese kanji, and Korean hanja, which are converted into effective character-tokenization
    by surrounding every character with whitespace. Moreover, the vocabulary is reduced
    by eliminating accents—a trade-off choice between accuracy and a more efficient
    model made by the mBERT authors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言 BERT（mBERT）本质上是指应用前一节中所描述的 BERT，并将其应用于约 100 个连接在一起的语言维基百科[⁷](#pgfId-1107531)
    语料库。最初的语言集合是前 100 大维基百科，现已扩展到前 104 种语言。该语言集合不包括 Twi，但包括一些非洲语言，如斯瓦希里语和约鲁巴语。由于各种语言语料库的大小差异很大，因此会应用一种“指数平滑”过程来对高资源语言（如英语）进行欠采样，对低资源语言（如约鲁巴语）进行过采样。与之前一样，使用了
    WordPiece 分词。对于我们而言，它足以提醒你，这种分词过程是子词级别的，正如我们在之前的章节中所看到的。唯一的例外是中文、日文的汉字和韩文汉字，它们通过在每个字符周围加上空格的方式被转换为有效的字符分词。此外，为了在精度和模型效率之间做出权衡选择，mBERT
    作者消除了重音词汇。
- en: We can intuitively believe that a BERT model trained on over 100 languages contains
    knowledge that could transfer to a language that was not originally included in
    the training set. Simply put, such a model is likely to learn common features
    of the languages that are common across all of them. One simple example of such
    a common feature is the concept of words and verb-noun relationships. If we frame
    the proposed experiment as a multitask learning problem, as we discussed in chapter
    4, we expect improved generalizability to new previously unseen scenarios. In
    this section, we will essentially prove that this is the case. We first transfer
    from mBERT to monolingual Twi data using the pretrained tokenizer. We then repeat
    the experiment by training the same mBERT/BERT architecture from scratch and training
    a suitable tokenizer as well. Comparing these two experiments will allow us to
    qualitatively evaluate the effectiveness of the multilingual transfer. We use
    a Twi subset of the JW300 dataset[⁸](#pgfId-1107536) for our purposes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直观地认为，一个在100多种语言上训练的BERT模型包含了可以转移到原始训练集中未包含的语言的知识。简单来说，这样的模型很可能会学习到所有语言中共同的特征。这种共同特征的一个简单例子是单词和动词-名词关系的概念。如果我们将提出的实验框架设定为多任务学习问题，正如我们在第4章中讨论的那样，我们期望对以前未见过的新场景的泛化性能得到改善。在本节中，我们将基本证明这一点。我们首先使用预训练的分词器将mBERT转移到单语Twi数据上。然后，我们通过从头开始训练相同的mBERT/BERT架构以及训练适当的分词器来重复实验。比较这两个实验将允许我们定性地评估多语言转移的有效性。我们为此目的使用JW300数据集的Twi子集[⁸](#pgfId-1107536)。
- en: The exercise in this section has implications reaching beyond multilingual transfer
    for your skill set. This exercise will teach you how to train your own tokenizer
    and transformer-based model from scratch. It will also demonstrate how to transfer
    from one checkpoint to new domain/language data for such a model. The previous
    sections and a bit of adventure/imagination will arm you with transformer-based
    transfer learning superpowers, be it for domain adaptation, cross-lingual transfer,
    or multitask learning.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的练习对于你的技能集具有超越多语言转移的影响。这个练习将教会你如何从头开始训练你自己的分词器和基于transformer的模型。它还将演示如何将一个检查点转移到这样一个模型的新领域/语言数据。之前的章节和一点冒险/想象力将为你提供基于transformer的迁移学习超能力，无论是用于领域自适应、跨语言转移还是多任务学习。
- en: In the next subsection, we briefly overview the JW300 dataset, followed by subsections
    performing cross-lingual transfer and then training from scratch.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们简要概述了JW300数据集，然后是执行跨语言转移和从头开始训练的小节。
- en: 8.2.1 Brief JW300 dataset overview
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 JW300数据集简介
- en: The JW300 dataset is a wide-coverage parallel corpus for low-resource languages.
    As previously mentioned, it is an arguably biased sample, being composed of religious
    text translated by the Jehovah’s Witnesses. However, for a lot of low-resource
    language research, it serves as a starting point and is often the only open source
    of parallel data available. It is important to remember the bias, though, and
    couple any training on this corpus with a second stage during which the model
    from the first stage can be transferred to a less biased and more representative
    sample of the language and/or task at hand.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: JW300数据集是一个面向低资源语言的广泛覆盖的平行语料库。正如之前提到的，它是一个可能具有偏见的样本，由耶和华见证人翻译的宗教文本组成。然而，对于许多低资源语言研究而言，它是一个起点，通常是唯一可用的平行数据的开放来源。然而，重要的是要记住这种偏见，并在这个语料库上进行任何训练时配备第二阶段，该阶段可以将第一阶段的模型转移到一个更少偏见和更具代表性的语言和/或任务样本。
- en: Although it is inherently a parallel corpus, we need only the monolingual corpus
    of Twi data for our experiments. The Python package opustools-pkg can be used
    to obtain a parallel corpus for a given language pair. To make things easier for
    you, we already did this for the English-Twi language pair and hosted it on Kaggle.[⁹](#pgfId-1107548)
    To repeat our experiment for some other low-resource language, you will need to
    tinker a bit with *the opustools-pkg* and obtain an equivalent corpus (please
    share with the community if you do). We use only the Twi part of the parallel
    corpus for our experiments and ignore the English part.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它本质上是一个平行语料库，但我们只需要Twi数据的单语语料库进行我们的实验。Python包opustools-pkg可以用于获取给定语言对的平行语料库。为了让您的工作更容易，我们已经为英语-Twi语对进行了这项工作，并将其托管在Kaggle上。[⁹](#pgfId-1107548)要为其他低资源语言重复我们的实验，您需要稍微调整一下*opustools-pkg*并获取一个等价的语料库（如果您这样做，请与社区分享）。我们只使用平行语料库的Twi部分进行我们的实验，并忽略英语部分。
- en: Let’s proceed to transferring mBERT to the monolingual low-resource language
    corpus.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续将 mBERT 转移到单语低资源语言语料库。
- en: 8.2.2 Transfer mBERT to monolingual Twi data with the pretrained tokenizer
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 将 mBERT 转移到单语 Twi 数据与预训练的标记器
- en: 'The first thing to do is to initialize a BERT tokenizer to the pretrained checkpoint
    from one of the mBERT models. We use the cased version this time, as shown by
    the following code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是初始化一个 BERT 标记器到来自 mBERT 模型中的预训练检查点。这次我们使用的是大小写版本，如下代码所示：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ This is just a faster version of BertTokenizer, which you could use instead.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这只是 BertTokenizer 的一个更快的版本，你可以用这个替代它。
- en: ❷ Uses the pretrained mBERT tokenizer
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用了预训练的 mBERT 标记器
- en: 'Having prepared the tokenizer, let’s load the mBERT checkpoint into a BERT
    masked language model, and display the number of parameters as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了标记器后，让我们按以下方法将 mBERT 检查点加载到 BERT 遮蔽语言模型中，并显示参数数量：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Uses masked language modeling
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用了遮蔽语言建模
- en: ❷ Initializes to the mBERT checkpoint
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化到了 mBERT 检查点
- en: The output indicates that the model has 178.6 million parameters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表明模型有1.786亿个参数。
- en: 'Next, we build the dataset using the tokenizer from the monolingual Twi text,
    using the convenient `LineByLineTextDataset` method included with transformers
    as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 transformers 附带的方便的 `LineByLineTextDataset` 方法，使用单语 Twi 文本的标记器来构建数据集，如下所示：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Indicates how many lines to read at a time
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指示一次读取多少行
- en: 'As shown in the following code, we will next need to define a “data collator”—a
    helper method that creates a special object out of a batch of sample data lines
    (of length `block_size`). This special object is consummable by PyTorch for neural
    network training:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如下代码所示，接下来我们需要定义一个“data collator” —— 一个帮助方法，通过一批样本数据行（长度为`block_size`）创建一个特殊对象。
    这个特殊对象适用于 PyTorch 进行神经网络训练：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Uses masked language modeling, and masks words with a probability of 0.15
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用了遮蔽语言建模，并以0.15的概率遮蔽单词
- en: Here we used masked language modeling, as described in the previous section.
    In our input data, 15% of words are randomly masked, and the model is asked to
    predict them during training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了遮蔽语言建模，就像前一节所描述的一样。在我们的输入数据中，有 15% 的单词被随机遮蔽，模型在训练期间被要求对它们进行预测。
- en: 'Define standard training arguments, such as output directory and training batch
    size, as shown next:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 定义标准的训练参数，比如输出目录和训练批量大小，如下所示：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Then use training arguments with the previously defined dataset and collator
    to define a “trainer” for one training epoch across the data as follows. Note
    that the data contains over 600,000 lines, so one pass across all of it is a significant
    amount of training!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用先前定义的数据集和数据收集器定义一个“训练器”来进行数据上的一个训练周期。注意，数据包含了超过 600,000 行，因此一次遍历所有数据是相当大量的训练！
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Train and time how long training takes, as shown here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练并计算训练时间，如下所示：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The model takes about three hours to complete the epoch at the hyperparameters
    shown and reaches a loss of about 0.77.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在所示的超参数下大约需要三个小时才能完成一个周期，并且损失大约为0.77。
- en: 'Save the model as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下进行模型保存：
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we take the following sentence from the corpus—“Eyi de *ɔ*haw k*ɛ*se
    baa sukuu h*ɔ*”—which translates to “This presented a big problem at school.”
    We mask one word, sukuu (which means “school” in Twi), and then apply the pipelines
    API to predict the dropped word as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从语料库中取出以下句子 —— “Eyi de *ɔ*haw k*ɛ*se baa sukuu h*ɔ*” —— 它的翻译是 “这在学校中提出了一个大问题。”
    我们遮蔽了一个单词，sukuu（在 Twi 中意思是“学校”），然后应用 pipelines API 来预测遗漏的单词，如下所示：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Defines the fill-in-the-blanks pipeline
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了填空管道
- en: ❷ Predicts the masked token
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测被遮蔽的标记
- en: 'This yields the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You can immediately see the religious bias in the outcome. “Israel” and “Eden”
    are suggested as among the top five completions. That said, these are somewhat
    plausible completions—for one thing, they are nouns. Overall, the performance
    is arguably decent.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你立刻就能看到结果中的宗教偏见。“以色列”和“伊甸园”被提议为前五个完成之一。话虽如此，它们算是比较有说服力的完成 —— 因为它们都是名词。总的来说，表现可能还算不错。
- en: If you do not speak the language, do not worry. In the next section, we will
    train BERT from scratch and compare the loss value to the one we obtained here
    to confirm the efficacy of the transfer learning experiment we just performed.
    We hope you get to try the steps outlined here on other low-resource languages
    you are interested in.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不会说这种语言，不用担心。在下一节中，我们将从头开始训练BERT，并将损失值与我们在这里获得的值进行比较，以确认我们刚刚执行的转移学习实验的功效。我们希望您能尝试在其他您感兴趣的低资源语言上尝试这里概述的步骤。
- en: 8.2.3 mBERT and tokenizer trained from scratch on monolingual Twi data
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 在单语Twi数据上从零开始训练的mBERT和分词器
- en: To train BERT from scratch, we first need to train a tokenizer. We can initialize,
    train, and save our own tokenizer to disk using the code in the next listing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要从头开始训练BERT，我们首先需要训练一个分词器。我们可以使用下一节代码中的代码初始化、训练和保存自己的分词器到磁盘。
- en: Listing 8.1 Initialize, train, and save our own Twi tokenizer from scratch
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单8.1 从头初始化、训练和保存我们自己的Twi分词器
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Initializes the tokenizer
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化分词器
- en: ❷ Customizes the training, and carries it out
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 自定义训练，并进行训练
- en: ❸ Standard BERT special tokens
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 标准BERT特殊标记
- en: ❹ Saves the tokenizer to disk
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将分词器保存到磁盘
- en: 'To load the tokenizer from what we just saved, we just need to execute the
    following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要从刚刚保存的分词器中加载分词器，我们只需要执行以下操作：
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Uses the language-specific tokenizer we just trained with max_len=512, to
    be consistent with previous subsection
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用我们刚刚训练的语言特定的分词器，max_len=512，以保持与上一小节一致
- en: Note that we use a maximum sequence length of 512 to be consistent with the
    previous subsection—this is what the pretrained mBERT uses as well. Also note
    that saving the tokenizer creates the vocabulary file vocab.txt file in the specified
    folder.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用最大序列长度为512，以保持与上一小节一致——这也是预训练的mBERT使用的长度。还要注意，保存分词器将在指定文件夹中创建词汇文件vocab.txt文件。
- en: 'From here we just need to initialize a fresh BERT model for masked language
    modeling as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们只需初始化一个全新的BERT模型来进行掩码语言建模，如下所示：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Don’t initialize to pretrained; create a fresh one.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不要初始化为预训练的；创建一个全新的。
- en: Otherwise, the steps are the same as the previous subsection, and we do not
    repeat the code here. Repeating the same steps yields a loss of about 2.8 in about
    1.5 hours after one epoch, and 2.5 in about 3 hours after two epochs. This is
    clearly not as good a loss as the previous subsection value of 0.77 after one
    epoch, confirming the efficacy of the transfer learning in that case. Note that
    this experiment took a shorter time per epoch because the tokenizer we built is
    fully focused on Twi and so has a smaller vocabulary than the 104-language pretrained
    mBERT vocabulary.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，步骤与上一小节相同，我们不在此处重复代码。重复相同的步骤在一个时代后大约1.5小时产生大约2.8的损失，并在两个时代后的大约3小时产生2.5的损失。这显然不如前一小节的0.77损失值好，证实了在那种情况下转移学习的功效。请注意，这次实验每个时代的时间较短，因为我们构建的分词器完全专注于Twi，因此其词汇量比104种语言的预训练mBERT词汇表小。
- en: Go forth and transform!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 去吧，改变未来！
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The transformer architecture uses self-attention to build bidirectional context
    for understanding text. This has enabled it to become a dominant language model
    in NLP recently.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformer架构使用自注意力机制来构建文本的双向上下文以理解文本。这使得它最近在NLP中成为主要的语言模型。
- en: The transformer allows tokens in a sequence to be processed independently of
    each other. This achieves greater parallelizability than bi-LSTMs, which process
    tokens in order.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformer允许序列中的标记独立于彼此进行处理。这比按顺序处理标记的bi-LSTM实现了更高的可并行性。
- en: The transformer is a good choice for translation applications.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: transformer是翻译应用的一个不错选择。
- en: BERT is a transformer-based architecture that is a good choice for other tasks,
    such as classification.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT是一种基于transformer的架构，对于其他任务，如分类，是一个不错的选择。
- en: BERT can be trained on multiple languages at once, producing the multilingual
    model mBERT. This model captures knowledge transferable even to languages not
    originally included in the training.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT可以同时在多种语言上进行训练，生成多语言模型mBERT。该模型捕获的知识可转移到原本未包含在训练中的语言。
- en: 1. A. Vaswani et al., “Attention Is All You Need,” NeurIPS (2017).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 1. A. Vaswani等人，“注意力就是一切”，NeurIPS (2017)。
- en: 2. A. Radford et al., “Improving Language Understanding by Generative Pre-Training,”
    arXiv (2018).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 2. A. Radford等人，“通过生成式预训练改善语言理解”，arXiv (2018)。
- en: '3. M. E. Peters et al., “BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding,” Proc. of NAACL-HLT (2019): 4171-86.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '3. M. E. Peters et al., “BERT: Pre-Training of Deep Bidirectional Transformers
    for Language Understanding,” Proc. of NAACL-HLT (2019): 4171-86.'
- en: 4. [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 4. [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)
- en: 5. [https://www.weforum.org/agenda/2020/07/mask-mandates-and-other-lockdown-policies-reduced-the-spread-of-covid-19-in-the-us](https://www.weforum.org/agenda/2020/07/mask-mandates-and-other-lockdown-policies-reduced-the-spread-of-covid-19-in-the-us).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 5. [https://www.weforum.org/agenda/2020/07/口罩命令和其他封锁政策减少了在美国的COVID-19传播](https://www.weforum.org/agenda/2020/07/mask-mandates-and-other-lockdown-policies-reduced-the-spread-of-covid-19-in-the-us).
- en: '6. P. Rajpurkar et al., “SQuAD: 100,000+ Questions for Machine Comprehension
    of Text,” arXiv (2016).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '6. P. Rajpurkar et al., “SQuAD: 100,000+ Questions for Machine Comprehension
    of Text,” arXiv (2016).'
- en: 7. [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 7. [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md)
- en: 8. [http://opus.nlpl.eu/JW300.php](http://opus.nlpl.eu/JW300.php)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 8. [http://opus.nlpl.eu/JW300.php](http://opus.nlpl.eu/JW300.php)
- en: 9. [https://www.kaggle.com/azunre/jw300entw](https://www.kaggle.com/azunre/jw300entw)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 9. [https://www.kaggle.com/azunre/jw300entw](https://www.kaggle.com/azunre/jw300entw)
