- en: 7 Unsupervised learning for text data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 无监督学习用于文本数据
- en: “Everybody smiles in the same language – George Carlin”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “每个人用相同的语言微笑 - 乔治·卡林”
- en: Our world has so many languages. These languages are the most common medium
    of communication to express our thoughts and emotions to each other. This ability
    to express our thoughts in words is unique to humans. These words are a source
    of information to us. These words can be written into text. In this chapter, we
    are going to explore the analysis we can do on text data. Text data falls under
    unstructured data and carries a lot of useful information and hence is a useful
    source of insights for the business. We use natural language processing or NLP
    to analyse the text data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的世界有这么多语言。这些语言是我们表达思想和情感的最常见媒介。用文字表达我们的想法的能力是人类独有的。这些词语对我们来说是信息的来源。这些词语可以被写成文本。在本章中，我们将探讨我们可以对文本数据进行的分析。文本数据属于非结构化数据，并携带着大量有用信息，因此是业务洞察的有用来源。我们使用自然语言处理或NLP来分析文本数据。
- en: At the same time, to analyse text data, we have to make the data analysis ready.
    Or in very simple terms, since our algorithms and processors can only understand
    numbers, we have to represent the text data in numbers or *vectors*. We are exploring
    all such steps in this chapter. Text data holds the key to quite a few important
    use cases like sentiment analysis, document categorization, language translation
    etc. to name a few. We will cover the use cases using a case study and develop
    Python solution on the same.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，为了分析文本数据，我们必须准备好数据分析。简单来说，由于我们的算法和处理器只能理解数字，我们必须用数字或*向量*表示文本数据。我们在本章中探讨了所有这些步骤。文本数据包含了一些重要用例的关键，如情感分析、文档分类、语言翻译等等。我们将通过案例研究来涵盖这些用例，并在此基础上开发Python解决方案。
- en: The chapter starts with defining text data, sources of text data and various
    use cases of text data. We will then move to the steps and processes to clean
    and handle the text data. We will cover the concepts of NLP, mathematical foundation
    and methods to represent text data into vectors. We will create Python codes for
    the use cases. And at the end, we are sharing case study on text data. In this
    book, we are providing that level of mathematical support without going in too
    much depth – an optimal mix of practical world and mathematical concepts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从定义文本数据、文本数据的来源和文本数据的各种用例开始。然后，我们将转向清理和处理文本数据的步骤和过程。我们将涵盖NLP的概念、数学基础和将文本数据表示为向量的方法。我们将为用例创建Python代码。最后，我们分享了关于文本数据的案例研究。在这本书中，我们提供了适当深度的数学支持，而不会深入探讨过多
    - 这是实际世界和数学概念的最佳结合。
- en: 'In this seventh chapter of the book, we are going to cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第七章中，我们将涵盖以下主题：
- en: Text data and various use cases of Text data analysis
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本数据和文本数据分析的各种用例
- en: Challenges we face with text data
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在处理文本数据时面临的挑战
- en: Pre-processing of text data and data cleaning
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本数据的预处理和数据清理
- en: Methods to represent text data in vectors
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本数据表示为向量的方法
- en: Sentiment analysis using Python – a case study
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Python进行情感分析 - 一个案例研究
- en: Text clustering using Python
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Python进行文本聚类
- en: Welcome to the seventh chapter and all the very best!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第七章，祝你一切顺利！
- en: 7.1 Technical toolkit
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 技术工具包
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at this location.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用到目前为止所用的相同版本的Python和Jupyter笔记本。本章中使用的代码和数据集已经上传到了这个位置。
- en: You would need to install a few Python libraries in this chapter which are –
    XXXX. Along with this we will need numpy and pandas. Using libraries, we can implement
    the algorithms very quickly. Otherwise, coding these algorithms is quite a time-consuming
    and painstaking task.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要安装一些Python库，它们是 - XXXX。除此之外，我们还需要numpy和pandas。使用库，我们可以非常快速地实现算法。否则，编写这些算法是非常耗时且繁琐的任务。
- en: Here we are dealing with text data, perhaps you will find it very interesting
    and useful.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在处理文本数据，也许你会发现它非常有趣和有用。
- en: Let’s get started with Chapter 7 of the book!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始书的第7章吧！
- en: 7.2 Text data is everywhere
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 文本数据无处不在
- en: 'Recall in the very first chapter of the book, we explored structured and unstructured
    datasets. Unstructured data can be text, audio, image or a video. The examples
    of unstructured data and their respective sources are given in (Figure 7-1) below,
    where we explain the primary types of unstructured data: text, images, audio and
    video along with their examples. The focus for this chapter is on text data.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆一下书的第一章，我们探讨了结构化和非结构化数据集。非结构化数据可以是文本、音频、图像或视频。下面给出了非结构化数据及其来源的示例（请参见图7-1），我们解释了非结构化数据的主要类型：文本、图像、音频和视频以及它们的示例。本章重点是文本数据。
- en: Figure 7-1 Unstructured data can be text, images, audio, video. We are dealing
    with text data in this chapter
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7-1 非结构化数据可以是文本、图像、音频、视频。本章我们处理文本数据
- en: '![A screenshot of a cell phone Description automatically generated](images/07_img_0001.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![一个手机的截图 自动产生描述](images/07_img_0001.png)'
- en: Language is indeed a gift to the mankind. We indulge in speaking, messaging,
    writing, listening to convey our thoughts. And this is done using text data which
    is generated using blogs and social media posts, tweets, comments, stories, reviews,
    chats and comments to name a few. Text data is generally much more direct, and
    emotionally expressive. It is imperative that business unlocks the potential of
    text data and derives insights from it. We can understand our customers better,
    explore the business processes and gauge the quality of services offered. We generate
    text data is the form of news, Facebook comments, tweets, Instagram posts, customer
    reviews, feedback, blogs, articles, literature, stories etc. This data represents
    a wide range of emotions and expressions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 语言确实是赐予人类的礼物。我们沉迷于说话、发短信、写作、倾听来传达我们的想法。而这是通过生成的文本数据来完成的，这些数据是由博客、社交媒体帖子、推文、评论、故事、评价、对话等产生的。文本数据通常更加直接，情感更具表达力。必须解锁文本数据的潜力，并从中得出见解。我们可以更好地了解我们的客户，探索业务流程，并衡量提供的服务质量。我们生成的文本数据以新闻、Facebook评论、推文、Instagram帖子、客户评价、反馈、博客、文章、文学、故事等形式存在。这些数据代表了各种情绪和表达。
- en: Have you even reviewed a product or a service on Amazon? You award stars to
    a product; at the same time, you can also input free text. Go to Amazon and look
    at some of the reviews. You might find some reviews have good amount of text as
    the feedback. This text is useful for the product/service providers to enhance
    their offerings. Also, you might have participated in a few surveys which ask
    you to share your feedback. Moreover, with the advent of Alexa, Siri, Cortona
    etc. the voice command is acting as an interface between humans and machines –
    which is again a rich source of data. Even the customer calls we make to a call
    centre can be source of text data. These calls can be recorded and using speech-to-text
    conversion, we can generate huge amount of text data. Massive dataset, right!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否在亚马逊上评论过产品或服务？你给产品评星级，同时也可以输入自由文本。去亚马逊看一些评论吧。你可能会发现一些评论有很多文字作为反馈。这些文本对产品/服务提供商来说是很有用的，可以帮助他们改进他们的产品/服务。此外，你可能参与了一些调查，要求你分享反馈。另外，随着Alexa、Siri、Cortana等技术的出现，语音指令正在充当人类和机器之间的接口
    - 这又是一个丰富的数据来源。甚至我们打给客服中心的电话也可以成为文本数据的来源。这些通话可以被记录，并通过语音转文字转换，我们可以生成大量的文本数据。海量数据集，对吧！
- en: We are discussing some of the important use cases for text data in the following
    section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节讨论一些文本数据的重要用例。
- en: 7.3 Use cases of text data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 文本数据的用例
- en: Text data is indeed super useful. It is really a rich source of insights for
    the business. There are some of the use cases which are of much interest, which
    we are listing below. The list is not exhaustive. At the same time, not all the
    use cases given implement unsupervised learning. Some of the use cases require
    supervised learning too. Nevertheless, for your knowledge we are sharing both
    types of use cases – based on supervised learning and unsupervised learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据确实非常有用。对于业务而言，这真是一个见解丰富的来源。下面列出了一些令人感兴趣的用例。列表并不是详尽无遗的。同时，并非所有给出的用例都实施无监督学习。有些用例也需要监督学习。尽管如此，为了让你了解，我们分享了基于监督学习和无监督学习的两种用例
    - 基于监督学习和无监督学习的用例。
- en: '**Sentiment analysis:** You might have participated in surveys or given your
    feedback of products/surveys. These survey generate tons of text data for us.
    That text data can be analyzed and we can determine whether the sentiment in the
    review is positive or negative. In simple words, sentiment analysis is gauging
    what is the positiveness or negativity in the text data. And hence, what is the
    sentiment about a product or service in the minds of the customers. We can use
    both supervised and unsupervised learning for sentiment analysis.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**情感分析：**你可能参与过调查或针对产品/调查提供了反馈。这些调查为我们生成了大量文本数据。可以分析这些文本数据，我们可以确定评论中的情感是积极的还是消极的。简单来说，情感分析就是评估文本数据中的积极性或消极性。因此，客户对产品或服务的情感是什么。我们可以使用监督式和非监督式学习进行情感分析。'
- en: '**News categorization or document categorization:** Look at the Google News
    webpage, you will find that each news item has been categorized to sports, politics,
    science, business or any other category. Incoming news will be classified based
    on the content of the news which is the actual text. Similarly, imagine we have
    some documents with us and we might want to segregate them based on their categories
    of based on the domain of study. For example, medical, economics, history, arts,
    physics etc. Such a use case will save a lot of time and man power indeed.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**新闻分类或文档分类：**看看Google新闻网页，你会发现每个新闻都被分类为体育、政治、科学、商业或任何其他类别。传入的新闻将根据实际文本的内容进行分类。同样，想象一下，我们手头有一些文档，我们可能想根据它们的类别或研究领域进行区分。例如，医学、经济学、历史、艺术、物理等。这样的用例确实会节省大量时间和人力。'
- en: '**Language Translation:** Translation of text from one language to another
    is a very interesting use case. Using natural language processing we can translate
    between languages. Language translation is very tricky as different languages
    have different grammatical rules. Generally, deep learning based solutions are
    the best fit for language translation.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语言翻译：**从一种语言到另一种语言的文本翻译是一个非常有趣的用例。使用自然语言处理可以在各种语言之间进行翻译。语言翻译非常棘手，因为不同的语言有不同的语法规则。通常，基于深度学习的解决方案最适合语言翻译。'
- en: '**Spam filtering:** email spam filter can be composed using NLP and supervised
    machine learning. A supervised learning algorithm can analyze incoming mail parameters
    and can give a prediction if that email belongs to a spam folder or not. The prediction
    can be based on the various parameters like sender email-id, subject line, body
    of the mail, attachments, time of mail etc. Generally, supervised learning algorithms
    are used here.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**垃圾邮件过滤：**可以使用自然语言处理和监督式机器学习来构建电子邮件垃圾邮件过滤器。监督式学习算法可以分析传入邮件的参数，并可以预测该邮件是否属于垃圾邮件文件夹。预测可以基于诸如发件人电子邮件ID、主题行、邮件正文、附件、邮件时间等各种参数。通常，在这里使用监督式学习算法。'
- en: '**Part of speech tagging** or POS tagging is one of the popular use cases.
    It means that we are able to distinguish the noun, adjectives, verbs, adverbs
    etc. in a sentence. **Named-entity recognition** or NER is also one of the famous
    applications of NLP. It involves identifying a person, place, organization, time,
    number in a sentence. For example, John lives in London and works for Google.
    NER can generate understanding like [John][Person] lives in [London][Location]
    and works for [Google][organization].'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词性标注**或POS标注是其中一个受欢迎的用例。这意味着我们能够区分句子中的名词、形容词、动词、副词等。**命名实体识别**或NER也是自然语言处理的著名应用之一。它涉及在句子中识别人物、地点、组织、时间、数字。例如，John住在London并为Google工作。NER能够生成类似[John][Person]住在[London][Location]并为[Google][organization]工作的理解。'
- en: Sentence generation, captioning the images, speech-to-text or text-to-speech
    tasks, handwriting recognition are a few other significant and popular use cases.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成句子，为图像加标题，语音转文本或文本转语音任务，手写识别是其他一些重要和受欢迎的用例。
- en: The use cases listed above are not exhaustive. There are tons of other use cases
    which can be implemented using NLP. NLP is a very popular research field too.
    We are sharing some significant papers at the end of the chapter for your perusal.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列出的用例并不是穷尽的。还有大量其他用例可以使用自然语言处理实现。NLP也是一个非常受欢迎的研究领域。我们在本章末尾分享了一些重要的论文供您参考。
- en: Whilst, text data is of much importance, at the same time is quite a difficult
    dataset to analyze. To be noted is, our computers and processors understand only
    numbers. So, the text still needs to be represented as numbers so that we can
    perform mathematical and statistical calculations on them. But before diving into
    the preparation of text data, we will cover some of the challenges we face while
    working on text dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文本数据非常重要，但同时也是一种难以分析的数据集。需要注意的是，我们的计算机和处理器只能理解数字。因此，文本仍然需要表示为数字，以便我们可以对其进行数学和统计计算。但在深入研究文本数据准备之前，我们将介绍在处理文本数据集时面临的一些挑战。
- en: 7.4 Challenges with text data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 文本数据的挑战
- en: Text is perhaps the most difficult data to work with. There are a large number
    of permutations to express the same thought. For example, if I may ask, “Hey buddy,
    what is your age?” and “Hey buddy, may I know how old are you?” mean the same,
    right! The answer to both the questions is same, and it is quite easy for humans
    to decipher. But can be an equally daunting task for a machine.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可能是最难处理的数据。有很多种方式可以表达同样的想法。例如，如果我问：“嘿，伙计，你几岁了？”和“嘿，伙计，我可以知道你多大了吗？”意思相同，对吧！对于人类来说，回答这两个问题是相同的，并且很容易解释。但对于机器来说可能是同样艰巨的任务。
- en: 'The most common challenges we face are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的最常见的挑战有：
- en: Text data can be complex to handle. There can be a lot of junk characters like
    $^%*& present in the text.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理文本数据可能很复杂。文本中可能有很多垃圾字符，比如$^%*&之类的。
- en: With the advent of modern communications, we have started to use short forms
    of words like “u” can be used for “you”, “brb” for “be right back” and so on.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随着现代通信的发展，我们开始使用缩写词，比如“u”可以代表“you”，“brb”可以代表“be right back”等。
- en: Language is changing, unbounding and ever evolving. It changes every day and
    new words are added to the language.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语言在变化，解除了限制，不断发展。它每天都在变化，新词汇不断被加入到语言中。
- en: If you do a simple Google search, you will find that quite a few words are added
    to the dictionary each year.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你做一个简单的谷歌搜索，你会发现每年都有相当多的词汇被添加到词典中。
- en: The world has close to 6500 languages, and each and every one carries their
    uniqueness. Each and every one complete our world. For example, Arabic, Chinese,
    English, French, German, Hindi , Italian, Japanese, Spanish etc. Each language
    follows its own rules and grammar which are unique in usage and pattern. Even
    the writing can be different - some are written left to right; some might be right
    to left or may be vertically! The same emotion, might take lesser or a greater
    number of words in different languages.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 世界上有近6500种语言，每一种语言都具有其独特性。每一种语言都构成了我们的世界。例如，阿拉伯语、中文、英语、法语、德语、印地语、意大利语、日语、西班牙语等。每种语言都有自己独特的用法和语法规则。甚至写作方式也可能不同
    - 有些从左到右书写；有些可能从右到左书写，或者垂直书写！相同的情感，在不同语言中可能需要更少或更多的词汇来表达。
- en: 'The meaning of a word is dependent on the context. A word can be an adjective
    and can be a noun too depending on the context. Look at these examples below:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个词的意义取决于上下文。一个词在不同的上下文中可以是形容词，也可以是名词。看看下面这些例子：
- en: “This book is a must read” and “Please book a room for me”.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “这本书是必读的”和“请为我预订一个房间”。
- en: “Tommy” can be a name but when used as “Tommy Hilfiger” its usage is completely
    changed.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “汤米”可以是一个名字，但当用作“汤米·希尔菲格”时，它的用法完全改变了。
- en: “Apple” is a fruit while “Apple” is a company producing Macintosh, iPhones etc.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “苹果”是一种水果，而“Apple”是一家生产Macintosh、iPhone等产品的公司。
- en: “April” is a month and can be a name too.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “四月”是一个月份，也可以是一个名字。
- en: Look at one more example - “Mark travelled from the UK to France and is working
    with John over there. He misses his friends”. The humans can easily understand
    that “he” in the second sentence is Mark and not John, which might not be that
    simple for a machine.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再看一个例子 - “马克从英国去了法国，正在那里和约翰一起工作。他想念他的朋友”。人类很容易理解第二句中的“他”是指马克而不是约翰，这对于机器来说可能不那么简单。
- en: There can be many synonyms for the same word, like “good” can be replaced by
    positive, wonderful, superb, exceptional in different scenarios. Or, words like
    “studying”, “studying”, “studies”, “studies” are related to the same root word
    “study”.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同一个词可能有很多同义词，比如“好”在不同情境中可以用“积极的”、“精彩的”、“出色的”、“异常的”来替换。或者，像“studying”、“studying”、“studies”、“studies”这样的词都与同一个词根“study”相关。
- en: And the size of text data can be daunting too. Managing a text dataset, storing
    it, cleaning it and refreshing it is a herculean task in itself.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本数据的大小也可能令人生畏。管理文本数据集、存储它、清理它并更新它本身就是一项艰巨的任务。
- en: Like any other machine learning project, text analytics follow the principles
    of machine learning albeit the precise process is slightly different. Recall in
    chapter 1, we examined the process of a machine learning project as shown in Figure
    7-2\. You are advised to refresh the process from Chapter 1.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何其他的机器学习项目一样，文本分析遵循机器学习的原则，尽管确切的过程略有不同。回想一下第 1 章，我们在图 7-2 中展示了机器学习项目的流程。建议您从第
    1 章刷新一下流程。
- en: Figure 7-2 Overall steps in data science project are same for text data too.
    The pre-processing of text data is very different from the structured dataset.
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7-2 数据科学项目的整体步骤对文本数据也是一样的。文本数据的预处理与结构化数据集非常不同。
- en: '![A screenshot of a cell phone Description automatically generated](images/07_img_0002.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![一个手机说明的截图 Description automatically generated](images/07_img_0002.png)'
- en: Defining the business problem, data collection and monitoring etc. remain the
    same. The major difference is in processing of the text, which involves data cleaning,
    creation of features, representation of text data etc. We are covering it now.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 定义业务问题、数据收集和监控等工作保持不变。主要的差异在于文本的处理，包括数据清理、特征创建、文本数据的表示等。我们现在来介绍它。
- en: '![](../Images/tgt.png)'
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_IMG
  zh: '![](../Images/tgt.png)'
- en: POP QUIZ – answer these question to check your understanding.. Answers at the
    end of the book
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 弹出测验 – 回答这些问题来检查你的理解。答案在书的末尾。
- en: (1)      Note the three most impactful use cases for the text data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 文本数据的三个最有影响力的用例是什么？
- en: (2)      Why is working on text data so tedious?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 为什么处理文本数据如此繁琐？
- en: 7.5 Preprocessing of the text data
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 文本数据的预处理
- en: Text data, like any other data source can be messy and noisy. We clean some
    of it in the data discovery phase and a lot of it in the pre-processing phase.
    At the same time, we have to extract the features from our dataset. This cleaning
    process is sometimes and can be implemented on most of the text datasets. Some
    text datasets might require a customized approach. We will start with cleaning
    the raw text data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据，像任何其他数据源一样，可能会杂乱无章。我们在数据发现阶段清理了一部分数据，而在预处理阶段清理了大部分数据。同时，我们需要从数据集中提取特征。这个清理过程有时是相似的，并且可以应用于大多数文本数据集。一些文本数据集可能需要定制的处理方法。我们将从清理原始文本数据开始。
- en: 7.5.1 Data cleaning
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 数据清理
- en: There is no second thought about the importance of data quality. The cleaner
    the text data is, the better the analysis will be. At the same time, the pre-processing
    is not a straight-forward task. It is complex and time-consuming task.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量的重要性不容置疑。文本数据越干净，分析结果就会越好。与此同时，预处理并不是一项简单的任务。这是一个复杂且耗时的任务。
- en: 'Text data is to be cleaned as it contains a lot of junk characters, irrelevant
    words, noise and punctuations, URLs etc. The primary ways of cleaning the text
    data are:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据需要进行清理，因为它包含了大量的垃圾字符、无关的词语、噪音和标点符号、URL 等。清理文本数据的主要方法有：
- en: '**Stop words removal:** out of all the words that are used in any language,
    there are some words which are most common. Stop words are the most common words
    in a vocabulary which carry less importance than the key words. For example, “is”,
    “an”, “the”, “a”, “be”, “has”, “had”, “it” etc. Once we remove the stop words
    from the text, the dimensions of the data are reduced and hence complexity of
    the solution is reduced.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**停用词去除：** 在任何语言中，有一些词是最常见的。停用词是词汇中最常见的单词，比关键词的重要性低。例如，“是”，“一个”，“这”，“有”，“已经”，“它”等。一旦我们从文本中去除了停用词，数据的维度就被减少了，因此解决方案的复杂性也减少了。'
- en: At the same time, it is imperative that we understand the context very well
    while removing the stop words. For example, if we ask a question “is it raining?”.
    Then the answer “it is” is a complete answer in itself.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与此同时，在去除停用词时，我们必须非常了解上下文。例如，如果我们问一个问题“下雨了吗？”。那么答案“下雨了”就是一个完整的答案。
- en: To remove the stop words, we can define a customized list of stop words and
    remove them. Else there are standard libraries are to remove the stop words.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要去除停用词，可以定义一个定制的停用词列表并去除它们。否则，也可以使用标准库去除停用词。
- en: When we are working with solutions where contextual information is important,
    we do not remove stop words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在处理需要上下文信息很重要的解决方案时，我们不会去除停用词。
- en: '**Frequency based removal of words:** Sometimes, you might wish to remove the
    words which are most common in your text or very unique. The process is to get
    the frequency of the words in the text and then set a threshold of frequency.
    We can remove the most common ones. Or maybe you might wish to remove the ones
    which have occurred only once/twice in the entire dataset. Based on the requirement,
    you will decide.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于频率的词汇移除:** 有时，您可能希望移除文本中最常见或非常独特的词。这个过程是获取文本中词语的频率，然后设置一个频率阈值。我们可以移除最常见的词。或者也许您希望移除整个数据集中只出现一次/两次的词。根据需求，您将决定。'
- en: '**Library based cleaning** is done when we wish to clean the data using pre-defined
    and customized library. We can create a repository of words which we do not want
    in our text and can iteratively remove from the text data. This approach allows
    us flexibility to implement the cleaning of our own choice.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于库的清洗**是在我们希望使用预定义和自定义库来清理数据时进行的。我们可以创建一个词库，其中包含我们不希望在文本中出现的词，然后可以迭代地从文本数据中移除它们。这种方法允许我们以自己的选择实施清理。'
- en: '**Junk or unwanted characters:** A text data particularly tweets, comments
    etc. might contain a lot of URLs, hashtags, numbers, punctuations, social media
    mentions, special characters etc. We might need to clean them from the text. At
    the same time, we have to be careful as some words which are not important for
    one domain might be quite required for a different domain. If data has been scraped
    from websites or HTML/XML sources, we need to get rid of all the HTML entities,
    punctuations, non-alphabets and so on.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**垃圾或不需要的字符:** 文本数据，特别是推文、评论等，可能包含许多URL、标签、数字、标点符号、社交媒体提及、特殊字符等。我们可能需要从文本中清除它们。与此同时，我们必须小心，因为对于一个领域不重要的一些词对于另一个领域可能是非常重要的。如果数据被从网站或HTML/XML来源中爬取，我们需要摆脱所有HTML实体、标点符号、非字母等。'
- en: Always keep business context in mind while cleaning the text data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 清理文本数据时，始终要牢记业务背景。
- en: As we know that a lot of new type of expressions have entered the language.
    For example, lol, hahahaha, brb, rofl etc. These expressions have to be converted
    to their original meanings. Even emojis like :-), ;-) etc. have to be converted
    to their original meanings.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道很多新类型的表达进入了语言。例如，lol，hahahaha，brb，rofl等。这些表达必须被转换为它们的原始含义。甚至像:-)，;-)等表情符号也必须被转换为它们的原始含义。
- en: '**Data encoding:** There are a number of data encodings available like ISO/IEC,
    UTF-8 etc. Generally, UTF-8 is the most popular one. But it is not a hard and
    fast rule to always use UTF-8 only.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据编码:** 有许多数据编码可用，如ISO/IEC，UTF-8等。通常，UTF-8是最流行的。但不是硬性规定一定要使用UTF-8。'
- en: '**Lexicon normalization:** Depending on the context and usage, the same word
    might get represented in different manners. During lexicon normalization we clean
    such ambiguities. The basic idea is to reduce the word to its root form. Hence,
    words which are derived from each other can be mapped to the central word provided
    they have the same core meaning.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词典归一化:** 根据上下文和使用情况，同一个词可能以不同的方式被表示。在词典归一化期间，我们清理这样的歧义。基本思想是将单词缩减到其根形式。因此，从彼此派生出来的词可以映射到中心词，只要它们有相同的核心含义。'
- en: Look at Figure 7-2 wherein we have shown that the same word “eat”, has been
    used in various forms. The root word is “eat” but these different forms are so
    many different representations for eat.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看一下图7-2，在这里我们展示了相同的单词“eat”，已经被用在各种形式中。根词是“eat”，但这些不同的形式是eat的许多不同表达。
- en: Figure 7-3 Ate, eaten, eats, eating all have the same root word – eat. Stemming
    and lemmatization can be used to get the root word.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7-3 Ate，eaten，eats，eating都有相同的根词——eat。词干提取和词形恢复可用于获取根词。
- en: '![](images/07_img_0003.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0003.png)'
- en: 'We wish to map all of these words like eating, eaten etc. to their central
    word “eat”, as they have the same core meaning. There are two primary methods
    to work on this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将所有这些词，如eating，eaten等，映射到它们的中心词“eat”，因为它们具有相同的核心含义。在这方面有两种主要方法可以处理：
- en: '**Stemming**: *Stemming* is a basic rule-based approach of mapping a word to
    its core word. It removes “es”, “ing”, “ly”, “ed” etc. from the end of the word.
    For example, studies will become studi and studying will become study. As visible
    being a rule-based approach, the output spellings might not always be accurate.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词干提取**: *词干提取*是一种基本的基于规则的方法，将一个单词映射到它的核心词。它会去掉单词末尾的“es”，“ing”，“ly”，“ed”等。例如，studies会变成studi，studying会变成study。显然，作为一种基于规则的方法，输出的拼写可能并不总是准确的。'
- en: '**Lemmatization**: is an organized approach which reduces words to their dictionary
    form. *Lemma* of a word is its dictionary or canonical form. For example, eats,
    eating, eaten etc. all have the same root word eat. Lemmatization provides better
    results than stemming but it takes more time then stemming.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词形还原**：是一种有组织的方法，将单词缩减为它们的词典形式。单词的*Lemma*是其词典或规范形式。例如，eats, eating, eaten等所有单词都有相同的根词eat。词形还原提供比词干提取更好的结果，但它需要比词干提取更多的时间。'
- en: These are only some of the methods to clean the text data. These techniques
    will help to a large extent. But still business acumen is required to further
    make sense to the dataset. We will clean the text data using these approaches
    by developing Python solution.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是清洗文本数据的一些方法。这些技术在很大程度上会有所帮助。但是，仍然需要商业眼光来进一步理解数据集。我们将通过开发Python解决方案来使用这些方法清洁文本数据。
- en: Once the data is cleaned, we have to start representation of data so that it
    can be processed by machine learning algorithms – which is our next topic.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被清洗，我们就必须开始表示数据，以便机器学习算法可以处理它 - 这是我们的下一个主题。
- en: 7.5.2 Extracting features from the text dataset
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 从文本数据集中提取特征
- en: Text data, like any other data source can be messy and noisy. We explored the
    concepts and techniques to clean it in the last section. Now we have cleaned the
    data and it is ready to be used. The next step is to represent this data in a
    format which can be understood by our algorithms. As we know that our algorithms
    can only understand numbers. Text data in its purest form cannot be understood
    by algorithms. So, everything needs to be converted to numbers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据，就像任何其他数据源一样可能会混乱和嘈杂。我们在上一节中探讨了清理它的概念和技术。现在我们已经清理了数据，准备好使用。下一步是以算法可以理解的格式表示这些数据。我们知道我们的算法只能理解数字。文本数据在其最纯粹的形式下无法被算法理解。因此，一切都需要转换为数字。
- en: A very simple technique can be to simply perform *one-hot encoding* on our words
    and represent them in a matrix.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的技术可以是简单地对我们的单词进行*独热编码*，并将它们表示为矩阵。
- en: We have covered one hot encoding in the previous chapters of the book
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书的前几章已经介绍了独热编码
- en: If we describe the steps, the words can be first converted to lowercase and
    then sorted in an alphabetical order. And then a numeric label can be assigned
    to them. And finally, words are converted to binary vectors. Let us understand
    using an example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们描述这些步骤，那么单词可以首先转换为小写，然后按字母顺序排序。然后可以为它们分配一个数字标签。最后，单词将被转换为二进制向量。让我们通过一个例子来理解。
- en: 'For example, the text is “It is raining heavily”. We will use the steps below:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，文本是“It is raining heavily”。我们将使用以下步骤：
- en: Lowercase the words so the output will be “it is raining heavily”
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小写单词，所以输出将会是“it is raining heavily”
- en: We will now arrange them in alphabetical order. The result is – heavily, is,
    it, raining.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将它们按字母顺序排列。结果是 - heavily, is, it, raining。
- en: We can now assign place values to each word as heavily:0, is:1, it:2, raining:3.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以为每个单词分配位置值，如 heavily:0, is:1, it:2, raining:3。
- en: Finally, we can transform them to binary vectors as shown below
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将它们转换为如下所示的二进制向量
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Though this approach is quite intuitive and simple to comprehend, it is pragmatically
    not possible due to the massive size of the corpus and the vocabulary.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法非常直观和简单易懂，但由于语料库和词汇的庞大规模，实际上是不可能的。
- en: Corpus refers to a collection of texts. It is Latin for body. It can be a body
    of written words or spoken words, which will be used to perform a linguistic analysis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库指的是一系列文本。它是拉丁文的意思。它可以是一组书面文字或口头文字，用于进行语言分析。
- en: Moreover, handling massive data size with so many dimensions will be computationally
    very expensive. The resulting matrix thus created will be very sparse too. Hence,
    we look at other means and ways to represent our text data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，处理如此多维的大数据集将在计算上非常昂贵。因此创建的矩阵也将非常稀疏。因此，我们需要寻找其他方法来表示我们的文本数据。
- en: There are better alternatives available to one-hot encoding. These techniques
    focus on the frequency of the word or the context in which the word is being used.
    This scientific method of text representation is much more accurate, robust and
    explanatory. It generates better results too. There are multiple such techniques
    like tf-idf, bag-of-words approach etc. We are discussing a few of these techniques
    in the next sections. But we will examine an important concept of tokenization
    first!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有更好的替代方案可用于一热编码。这些技术侧重于单词的频率或单词的使用上下文。这种科学方法的文本表示更准确、更健壮、更具解释性。它也产生更好的结果。有多种这样的技术，如
    tf-idf、词袋模型等。我们将在接下来的章节中讨论其中一些技术。但我们首先会考察标记化的一个重要概念！
- en: Tokenization
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Tokenization（标记化）
- en: Tokenization is simply breaking a text or a set of text into individual tokens.
    It is the building block of NLP. Look at the example in Figure 7-3, where we have
    created individual tokens for each of the word of the sentence. Tokenization is
    an important step as it allows us to assign unique identifiers or tokens to each
    of the words. Once we have allocated each word a specific token, the analysis
    become less complex.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Tokenization（标记化）就是简单地将文本或一组文本分解成单个标记。它是自然语言处理的基本构件。看一下图7-3中的示例，我们为句子中的每个单词创建了单独的标记。标记化是一个重要的步骤，因为它允许我们为每个单词分配唯一的标识符或标记。一旦我们为每个单词分配了特定的标记，分析就变得不那么复杂了。
- en: Figure 7-3 Tokenization can be used to break a sentence into different tokens
    of words.
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7-3 标记化可以将句子分解为不同的单词标记。
- en: '![](images/07_img_0004.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图7-3 Tokenization 可以将句子分解为不同的单词标记。](images/07_img_0004.png)'
- en: Tokens are usually used on individual words, but it is not always necessary.
    We are allowed to tokenize a word or the sub-words or characters in a word. In
    the case of sub-words, the same sentence can have sub-word tokens as rain-ing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 标记通常用于单个单词，但并不总是必要的。我们可以将一个单词或单词的子词或字符进行标记化。在子词的情况下，同一句子可以有子词标记，如 rain-ing。
- en: If we wish to perform tokenization at a character level, it can be r-a-i-n-i-n-g.
    In fact, in the one-hot encoding approach discussed in the last section as a first
    step, tokenization was done on the words.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望在字符级别执行标记化，那么可以是 r-a-i-n-i-n-g。实际上，在上一节讨论的一热编码方法中，标记化是在单词上进行的。
- en: Tokenization is the building blocks for Natural Language Processing solutions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化是自然语言处理解决方案的基石。
- en: Once we have obtained the tokens, then the tokens can be used to prepare a vocabulary.
    Vocabulary is the set of unique tokens in the corpus.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了标记，那么这些标记就可以用来准备一个词汇表。词汇表是语料库中唯一标记的集合。
- en: There are multiple libraries for tokenization. *Regexp* tokenization uses the
    given patterns arguments to match the tokens or separators between the tokens.
    *Whitespace* tokenization uses by treating any sequence of whitespace characters
    as a separator. Then we have *blankline* which use sequence of blank lines as
    a separator. And *wordpunct* tokenizes by matching sequence of alphabetic characters
    and sequence of non-alphabetic and non-whitespace characters. We will perform
    tokenization when we create Python solutions for our text data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个用于标记化的库。*Regexp* 标记化使用给定的模式参数来匹配标记或标记之间的分隔符。*Whitespace* 标记化通过将任何空白字符序列视为分隔符来使用。然后我们有
    *blankline*，它使用空白行的序列作为分隔符。*Wordpunct* 标记化通过匹配字母字符序列和非字母非空白字符序列来进行标记化。当我们为文本数据创建
    Python 解决方案时，我们将执行标记化。
- en: Now, we will explore more methods to represent text data. The first such method
    is Bag of Words.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨更多表示文本数据的方法。第一个这样的方法是词袋模型。
- en: Bag of words approach
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词袋模型方法
- en: As the name suggests, all the words in the corpus are used. In bag of words
    approach, or BOW, the text data is tokenized for each word in the corpus and then
    the respective frequency of each token is calculated. During this process, we
    disregard the grammar, or the order or the context of the word. We simply focus
    on the simplicity. Hence, we will represent each text (sentence or a document)
    as a *bag of its own words*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，语料库中的所有单词都会被使用。在词袋模型方法中，或者说 BOW 方法中，文本数据被标记化为语料库中的每个单词，然后计算每个标记的相应频率。在此过程中，我们忽略语法、单词的顺序或上下文。我们只专注于简单性。因此，我们将每个文本（句子或文档）表示为*它自己的单词袋*。
- en: In the BOW approach for the entire document, we define the vocabulary of the
    corpus as all of the unique words present in the corpus. Please note we use all
    the unique words in the corpus. If we want, we can also set a threshold i.e.,
    the upper and lower limit for the frequency of the words to be selected. Once
    we have got the unique words, then each of the sentence can be represented by
    a vector of the same dimension as of the base vocabulary vector. This vector representation
    contains the frequency of each word of the sentence in the vocabulary. It might
    sound complicated to understand, but it is actually a straightforward approach.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个文档的 BOW 方法中，我们将语料库的词汇表定义为语料库中存在的所有唯一单词。请注意，我们使用语料库中的所有唯一单词。如果我们愿意，我们也可以设置一个阈值，即单词被选中的频率的上限和下限。一旦我们获得了唯一的单词，那么每个句子都可以用与基础词汇向量相同维度的向量来表示。这个向量表示包含了句子中每个单词在词汇表中的频率。这听起来可能很复杂，但实际上这是一种直接的方法。
- en: Let us understand this approach with an example. Let’s say that we have two
    sentences – It is raining heavily and We should eat fruits.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这种方法。假设我们有两个句子——It is raining heavily 和 We should eat fruits。
- en: To represent these two sentences, we will calculate the frequency of each of
    the word in these sentences as shown in Figure 7-4.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要表示这两个句子，我们将计算这些句子中每个单词的频率，如图 7-4 所示。
- en: Figure 7-4 The frequency of each word has been calculated. In this example,
    we have two sentences.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7-4 每个单词的频率已经计算出来了。在这个例子中，我们有两个句子。
- en: '![](images/07_img_0005.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0005.png)'
- en: Now, if we assume that only these two words represent the entire vocabulary,
    we can represent the first sentence as shown in Figure 7-5\. Note that the table
    contains all the words, but the words which are not present in the sentence have
    received value as 0.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们假设只有这两个单词代表整个词汇表，我们可以将第一个句子表示如图 7-5 所示。请注意，该表格包含了所有单词，但是不在句子中的单词的值为 0。
- en: Figure 7-5 The first sentence if represented for all the words in the vocabulary,
    we are assuming that in the vocabulary only two sentences are present
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7-5 第一个句子对于词汇表中的所有单词进行了表示，我们假设词汇表中只有两个句子。
- en: '![](images/07_img_0006.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0006.png)'
- en: In this example, we examined how BOW approach has been used to represent a sentence
    as a vector. But BOW approach has not considered the order of the words or the
    context. It focuses only on the frequency of the word. Hence, it is a very fast
    approach to represent the data and computationally less expensive as compared
    to its peers. Since it is frequency based, it is commonly used for document classifications.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们讨论了如何使用 BOW 方法将句子表示为向量。但 BOW 方法没有考虑单词的顺序或上下文。它只关注单词的频率。因此，它是一种非常快速的方法来表示数据，并且与其同行相比计算成本较低。由于它是基于频率的，所以它通常用于文档分类。
- en: But, due to its pure frequency-based calculation and representation, the solution
    accuracy can take a hit. In language, the context of the word plays a significant
    role. As we have seen earlier, apple is both a fruit as well as a well-known brand
    and organization. And that is why we have other advanced methods which consider
    more parameters than frequency alone. One of such methods is tf-idf or term frequency-inverse
    document frequency, which we are studying next.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于其纯粹基于频率的计算和表示，解决方案的准确性可能会受到影响。在语言中，单词的上下文起着重要作用。正如我们之前所看到的，苹果既是一种水果，也是一个著名的品牌和组织。这就是为什么我们有其他考虑比仅仅是频率更多参数的高级方法。下面我们将学习一种这样的方法，即
    tf-idf 或词项频率-逆文档频率。
- en: '![](../Images/tgt.png)'
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_IMG
  zh: '![](../Images/tgt.png)'
- en: POP QUIZ – answer these question to check your understanding.. Answers at the
    end of the book
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 弹出测验——回答这些问题以检查你的理解。答案在本书的末尾。
- en: (1)      Explain tokenization in simple language as if you are explaining to
    a person who does not know NLP.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 用简单的语言解释标记化，就好像你在向一个不懂 NLP 的人解释一样。
- en: (2)      Bag of words approach the context of the words and not frequency alone.
    True or False.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (2) Bag of words 方法关注单词的上下文而不仅仅是频率。True or False.
- en: (3)      Lemmatization is less rigorous approach then stemming. True or False.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (3) Lemmatization is less rigorous approach then stemming. True or False.
- en: tf-idf (Term frequency and inverse document frequency)
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: tf-idf（词项频率和逆文档频率）
- en: We studied Bag of words approach in the last section. In the BOW approach, we
    gave importance to the frequency of a word only. The idea is that the words which
    have higher frequency might not offer meaningful information as compared to words
    which are rare but carry more importance. For example, if we have a collection
    of medical documents, and we wish to compare two words “disease” and “diabetes”.
    Since the corpus consists of medical documents, the word disease is bound to be
    more frequent whilst the word “diabetes” will be less frequent but more important
    to identify the documents which deal with diabetes. The approach tf-idf allow
    us to resolve this issue and extract information on the more important words.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节学习了词袋方法。在词袋方法中，我们只重视单词的频率。这个想法是，频率较高的词可能不像频率较低但更重要的词那样提供有意义的信息。例如，如果我们有一套医学文件，我们想要比较两个词“疾病”和“糖尿病”。由于语料库包含医学文件，单词疾病必然会更频繁，而“糖尿病”这个词会更少，但更重要，以便识别处理糖尿病的文件。tf-idf方法使我们能够解决这个问题，并提取更重要的词的信息。
- en: 'In term-frequency and inverse-document-frequency (tf-idf), we consider the
    relative importance of the word. TF-idf means term frequency and idf means inverse
    document frequency. We can define tf-idf as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在术语频率和逆文档频率（tf-idf）中，我们考虑单词的相对重要性。TF-idf表示术语频率，idf表示逆文档频率。我们可以定义tf-idf如下：
- en: '**Term frequency (t** is the count of a term in the entire document. For example,
    the count of the word “a” in the document “D”.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**术语频率（t**是整个文档中术语的计数。例如，文档“D”中单词“a”的计数。'
- en: '**Inverse document frequency (id** is the log of the ratio of total documents
    (N) in the entire corpus and number of documents(df) which contain the word “a”.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逆文档频率（id**是整个语料库中总文档数（N）与包含单词“a”的文档数(df)的比率的对数。'
- en: So, the tf-idf formula will give us the relative importance of a word in the
    entire corpus. The mathematical formula is the multiplication of tf and idf and
    is given by
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，tf-idf公式将为我们提供单词在整个语料库中的相对重要性。数学公式是tf和idf的乘积，表示为
- en: w[i,j] = tf[i,j] * log (N/df[i]) (Equation 7-1)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: w[i,j] = tf[i,j] * log (N/df[i]) (方程式 7-1)
- en: 'where N: total number of documents in the corpus'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中N：语料库中的文档总数
- en: tf[i,j] is the frequency of the word in the document
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: tf[i,j]是文档中单词的频率
- en: df[i] is the number of documents in the corpus which contain that word.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: df[i]是包含该词的语料库中的文档数量。
- en: The concept might sound complex to comprehend. Let’s understand this with an
    example.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念听起来可能很复杂。让我们通过一个例子来理解。
- en: Consider we have a collection of 1 million sports journals. These sports journals
    contain have many articles of various lengths. We also assume that all the articles
    are in English language only. So, let’s say, in these documents, we want to calculate
    tf-idf value for the word “ground” and “backhand”.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一百万本运动期刊的集合。这些运动期刊包含了许多不同长度的文章。我们还假设所有文章都只有英语。所以，假设在这些文件中，我们想要计算单词“ground”和“backhand”的tf-idf值。
- en: Let’s assume that there is a document of 100 words having “ground” word five
    times and backhand only twice. So tf for ground is 5/100 = 0.05 and for backhand
    is 2/100 = 0.02.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个包含100个词的文档，其中“ground”出现了五次，而“backhand”只出现了两次。所以ground的tf为5/100 = 0.05，而backhand的tf为2/100
    = 0.02。
- en: We understand that the word “ground” is quite a common word in sports, while
    the word “backhand” will have lesser number of usage. Now, we assume that “ground”
    appears in 100,000 documents out of 1 million documents while “backhand” appears
    only in 10\. So, idf for “ground” is log (1,000,000/100,000) = log (10) = 1\.
    For “backhand” it will be log (1,000,000/10) = log (100,000) = 5.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，“ground”这个词在体育中是相当常见的词，而“backhand”这个词的使用次数会较少。现在，我们假设“ground”出现在100,000个文档中，而“backhand”只出现在10个文档中。所以，“ground”的idf为log
    (1,000,000/100,000) = log (10) = 1。对于“backhand”，它将是log (1,000,000/10) = log (100,000)
    = 5。
- en: To get the final values for “ground” we will multiply tf and idf = 0.05 x 1
    = 0.05.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到“ground”的最终值，我们将tf和idf相乘= 0.05 x 1 = 0.05。
- en: To get the final values for “backhand” we will multiply tf and idf = 0.02 x
    5 = 0.1.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到“backhand”的最终值，我们将tf和idf相乘= 0.02 x 5 = 0.1。
- en: We can observe that the relative importance of “backhand” is more than the relative
    importance of the word “ground”. This is the advantage of tf-idf over frequency-based
    BOW approach. But tf-idf takes more time to compute as compared to BOW since all
    the tf and idf have to be calculated. Nevertheless, tf-idf offer a better and
    more mature solution as compared to BOW approach.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，“背手”这个词的相对重要性要比单词“地面”的相对重要性更高。这就是 tf-idf 相对于基于频率的 BOW 方法的优势。但是，与 BOW
    相比，tf-idf 计算时间更长，因为必须计算所有的 tf 和 idf。尽管如此，与 BOW 方法相比，tf-idf 提供了一个更好、更成熟的解决方案。
- en: We will now cover language models in the next section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将在下一节中介绍语言模型。
- en: Language models
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言模型
- en: So far, we have studied the bag of words approach and tf-idf. Now we are focusing
    on language models.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了词袋模型方法和tf-idf。现在我们将专注于语言模型。
- en: Language models assign probabilities to the sequence of words. N-grams are the
    simplest in language models. We know that to analyze the text data they have to
    be converted to feature vectors. N-gram models create the feature vectors so that
    text can be represented in a format which can be analyzed further.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型分配概率给词的序列。N-grams 是语言模型中最简单的模型。我们知道，为了分析文本数据，它们必须被转换为特征向量。N-gram 模型创建了特征向量，使得文本可以以进一步分析的格式来表示。
- en: n-gram is a probabilistic language model. In a n-gram model we calculate the
    probability of the N^(th) word given the sequence of (N-1) words. If we go more
    specific, an n-gram model will predict the next word x[i] based on the words x[i-(n-1)],
    x[i-(n-2)]…x[i-1]. If we wish to use the probability terms, we can represent as
    conditional probability of x[i] given the previous words which can be represented
    as P(x[i] | x[i-(n-1)], x[i-(n-2)]…x[i-1]). The probability is calculated by using
    the relative frequency of the sequence occurring in the text corpus.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram 是一个概率语言模型。在 n-gram 模型中，我们计算第 N^(th) 个词在给定(N-1)个词的序列的情况下出现的概率。更具体地说，n-gram
    模型将基于词 x[i-(n-1)], x[i-(n-2)]…x[i-1] 预测下一个词 x[i]。若我们希望使用概率术语，可以表示为给定前面的词 x[i-(n-1)],
    x[i-(n-2)]…x[i-1] 的条件概率 P(x[i] | x[i-(n-1)], x[i-(n-2)]…x[i-1])。该概率通过文本语料库中序列的相对频率计算得出。
- en: If the items are words, n-grams may be referred to as *shingles*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 若条目是单词，n-grams 可能被称为*shingles*。
- en: Let’s study this using an example.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来学习这个。
- en: Consider we have a sentence It is raining heavily. We have shown the respective
    representations using different values of n. You should note that how the sequence
    of words and their respective combinations is getting changed for different values
    of n. If we wish to use n=1 or a single word to make a prediction, the representation
    will be as shown in Figure 7-6\. Note that each word is used separately here.
    They are referred to as *unigrams*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个句子 It is raining heavily。我们已经使用不同的 n 值展示了相应的表示方式。您应该注意到，对于不同的 n 值，单词的序列以及它们的组合方式是如何变化的。如果我们希望使用
    n=1 或单个词来进行预测，表示将如图 7-6 所示。请注意，此处每个词单独使用。称为*unigrams*。
- en: If we wish to use n=2, the number of words now used will become two. They are
    referred to as *bigrams* and the process will continue.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望使用 n=2，那么现在使用的词数量将变为两个。它们被称为*bigrams*，这个过程将继续下去。
- en: Figure 7-6 Unigrams, bigrams, trigrams can be used to represent the same sentence.
    The concept can be extended to n-grams too.
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7-6 Unigrams, bigrams, trigrams 可以用来表示相同的句子。这个概念也可以扩展到 n-grams。
- en: '![](images/07_img_0007.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0007.png)'
- en: Hence, if we are having a unigram, it is a sequence of one word, for two words
    it is bi-gram, for three words it is tri-gram and so on. So, a tri-gram model
    will approximate the probability of a word given all the previous words by using
    the conditional probability of only the preceding two words. Whereas a bi-gram
    will do the same by considering only the preceding word. This is a very strong
    assumption indeed that the probability of a word will depend only on the preceding
    word and is referred to as *Markov* assumption. Generally, N > 1 is considered
    to be much more informative than unigrams. But obviously the computation time
    will increase too.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们有一个 unigram，那么它是一个词的序列；对于两个词的序列，称为 bigram；对于三个词的序列，称为 trigram，依此类推。因此，trigram
    模型将使用仅考虑前两个词的条件概率来逼近后一个词的概率，而 bigram 则仅考虑前一个词的条件概率。这确实是一个非常强的假设，即一个词的概率只取决于之前的一个词，被称为*马尔可夫*假设。一般来说，N
    > 1 被认为较 unigrams 更加信息丰富。但显然计算时间也会增加。
- en: n-gram approach is very sensitive to the choice of n. It also depends significantly
    on the training corpus which have been used, which makes the probabilities heavily
    dependent on the training corpus. So, if an unknown word is encountered, it will
    be difficult for the model to work on that new word.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram 方法对 n 的选择非常敏感。它还在很大程度上取决于所使用的训练语料库，这使得概率非常依赖于训练语料库。因此，如果遇到一个未知单词，模型将很难处理该新单词。
- en: We will create a Python example now. We will show a few examples of text cleaning
    using Python.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个Python示例。我们将展示一些使用Python进行文本清洗的示例。
- en: Text cleaning using Python
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 Python 进行文本清洗
- en: We will clean the text data using Python now. There are a few libraries which
    you may need to install. We will show a number of small code snippets. You are
    advised to use them as per the requirement. We are also pasting the respective
    screenshot of the code snippets and their results.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将用Python清洗文本数据。有一些库可能需要安装。我们将展示一些小的代码片段。你可以根据需要使用它们。我们还附上了代码片段及其结果的相应截图。
- en: 'Code 1: Remove the blank spaces in the text. We import the library `re`, it
    is called `Regex` expression. The text is It is raining outside with a lot of
    blank spaces in between.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 1：删除文本中的空格。我们导入库`re`，它被称为`正则表达式`。文本是 It is raining outside with a lot of
    blank spaces in between.
- en: '[PRE1]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](images/07_img_0008.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0008.png)'
- en: 'Code 2: Now we will remove the punctuations in the text data.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 2：现在我们将从文本数据中删除标点符号。
- en: '[PRE2]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](images/07_img_0009.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0009.png)'
- en: 'Code 3: This is one more method to remove the punctuation.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 3：这是另一种去除标点符号的方法。
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](images/07_img_0010.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0010.png)'
- en: 'Code 4: We will now remove the punctutions as well as convert the text to lowercase.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 4：我们现在将删除标点符号，并将文本转换为小写。
- en: '[PRE4]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](images/07_img_0011.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0011.png)'
- en: 'Code 5: We will now use a standard `nltk` library. Tokenization will be done
    here using NLTK library. The output is also pasted below.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 5：我们现在将使用标准的`nltk`库。标记化将在这里使用NLTK库完成。输出也附在下面。
- en: '[PRE5]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](images/07_img_0012.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0012.png)'
- en: Note that in the output of the code, we have all the words including the punctuation
    marks as different tokens. If you wish to exclude the punctuations, you can clean
    the punctuations using the code snippets shared earlier.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在代码的输出中，我们有包括标点符号的所有单词作为不同的标记。如果你希望排除标点符号，可以使用之前共享的代码片段来清理标点符号。
- en: 'Code 6: Next comes the stop words. We will remove the stopwords using nltk
    library. Post that, we are tokenizing the words.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 6：接下来是停用词。我们将使用nltk库删除停用词。然后，我们将对单词进行标记。
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](images/07_img_0013.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0013.png)'
- en: 'Code 7: We will now perform stemming on a text example. We use NLTK library
    for it. The words are first tokenized and then we apply stemming on them.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 7：我们现在将在文本示例上执行词干提取。我们使用NLTK库进行词干提取。首先对单词进行标记，然后我们对其应用词干提取。
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](images/07_img_0014.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0014.png)'
- en: 'Code 8: We will now perform lemmatization on a text example. We use NLTK library
    for it. The words are first tokenized and then we apply lemmatization on them.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 8：我们现在将在文本示例上执行词形还原。我们使用NLTK库进行词形还原。首先对单词进行标记，然后我们对其应用词形还原。
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](images/07_img_0015.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0015.png)'
- en: Observe and compare the difference between the two outputs of stemming and lemmatization.
    For studies and studying, stemming generated the output at studi while lemmatization
    generated correct output as study.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 观察并比较词干提取和词形还原的两个输出之间的差异。对于 studies 和 studying，词干提取生成的输出为 studi，而词形还原生成了正确的输出
    study。
- en: We covered bag-of-words, tf-idf and N-gram approaches so far. But in all of
    these techniques, the relationship between words has been neglected which is used
    in *word embeddings* – our next topic.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了词袋模型、tf-idf 和 N-gram 方法。但在所有这些技术中，忽略了单词之间的关系，而这些关系在*词嵌入*中被使用 -
    我们的下一个主题。
- en: Word Embeddings
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词嵌入
- en: '"A word is characterized by the company it keeps” – John Rupert Firth.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '"一个单词的特点在于它周围的公司" - 约翰·鲁珀特·费斯。'
- en: So far, we studied a number of approaches, but all the techniques ignore the
    contextual relationship between words. Let’s study by an example.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们研究了许多方法，但所有的技术都忽略了单词之间的上下文关系。让我们通过一个例子来学习。
- en: Imagine we have 100,000 words in our vocabulary – starting from aa to zoom.
    Now, if we perform one-hot encoding we studied in the last section, all of these
    words can be represented in a vector form. Each word will have a unique vector.
    For example, if the position of the word king in 21000, the vector will have shape
    like the vector below which has 1 at the 21000 position and rest of the values
    as 0.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的词汇表中有 100,000 个单词，从 aa 到 zoom。现在，如果我们执行上一节学习的 one-hot 编码，所有这些单词都可以以向量形式表示。每个单词将有一个唯一的向量。例如，如果单词
    king 的位置在 21000，那么向量的形状将如下所示，其中在 21000 位置上有 1，其余位置为 0。
- en: '[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…………………1, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…………………1, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]'
- en: 'There are a few glaring issues with this approach:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在一些明显的问题：
- en: The number of dimensions is very high to compute and complex.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 维度的数量非常高，计算复杂。
- en: The data is very sparse in nature.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据在本质上非常稀疏。
- en: If n new words have to be entered, the vocabulary increases by n and hence each
    vector dimensionality increases by n.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果要输入 n 个新单词，则词汇量增加 n，因此每个向量的维度增加 n。
- en: This approach ignores the relationship between words. We know that ruler, king,
    monarch is sometimes used interchangeably. In the one-hot-encoding approach, any
    such relationships are ignored.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种方法忽略了单词之间的关系。我们知道，ruler（统治者）、king（国王）、monarch（君主）有时可以互换使用。在 one-hot 编码方法中，任何这种关系都被忽略了。
- en: If we wish to perform language translation, or generate a chat bot, we need
    to pass such knowledge to the machine learning solution. Word embeddings provide
    a solution to the problem. They convert the high-dimensional word features into
    lower dimensions while maintaining the contextual relationship. Word-embeddings
    allow us to create much more generalized models. We can understand the meaning
    by looking at an example.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望进行语言翻译或生成聊天机器人，我们需要将这样的知识传递给机器学习解决方案。单词嵌入为这个问题提供了解决方案。它们将高维度的单词特征转换为较低的维度，同时保持上下文关系。单词嵌入允许我们创建更加泛化的模型。我们可以通过示例来理解含义。
- en: In the example shown below in Figure 7-7, the relation of “man” to “woman” is
    similar to “king” to “queen”, “eat” to “eating” is similar as “study” to “studying”
    or “UK” to “London” is similar to “Japan” to “Tokyo”.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 7-7 所示的示例中，"man" 到 "woman" 的关系类似于 "king" 到 "queen"，"eat" 到 "eating" 类似于 "study"
    到 "studying" 或 "UK" 到 "London" 类似于 "Japan" 到 "Tokyo"。
- en: Figure 7-7 Word embeddings can be used to represent the relationships between
    words. For example, there is a relation from men to women which is similar to
    king to queen.
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7-7 单词嵌入可以用来表示单词之间的关系。例如，从 men（男人）到 women（女人）之间存在一种关系，类似于 king（国王）到 queen（女王）之间的关系。
- en: '![](images/07_img_0016.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/07_img_0016.png)'
- en: In simple terms, using word embeddings we can represent the words similarly
    which have similar meaning. Word embeddings can be thought as a class of techniques
    where we represent each of the individual words in a predefined vector space.
    Each of the word in the corpus is mapped to one vector. The distributed representation
    is understood based on the word’s usage. Hence, words which can be used similarly
    have similar representations. This allows the solution to capture the underlying
    meaning of the words and their relationships. Hence, the meaning of the word plays
    a significant role. This representation hence is more intelligent as compared
    to bag of words approach where each word is treated differently, irrespective
    of their usage. Also, the number of dimensions is lesser as compared to one-hot
    encoding. Each word is represented by 10 or 100s of dimensions which is significantly
    less than one-hot encoding approach where more than 1000s of dimensions are used
    for representation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用单词嵌入，我们可以将具有相似含义的单词表示为类似的形式。单词嵌入可以被认为是一类技术，其中我们将每个单词在预定义的向量空间中表示出来。语料库中的每个单词都被映射到一个向量上。根据单词的使用情况来理解分布式表示。因此，可以使用类似的单词具有相似的表示。这使得解决方案能够捕捉单词及其关系的潜在含义。因此，单词的含义起着重要作用。相比于词袋方法，这种表示更加智能，词袋方法中的每个单词都是独立处理的，不考虑它们的使用情况。而且，维度的数量较少，相比于
    one-hot 编码，每个单词由 10 或 100 维的向量表示，这比 one-hot 编码方法中使用的超过 1000 维的向量表示要少得多。
- en: We will cover two most popular techniques Word2Vec and GloVe in the next section.
    The section provides an understanding of Word2Vec and GloVe. The mathematical
    foundation for Word2Vec and GloVe are beyond the scope of this book. We are providing
    un understanding on the working mechanism of the solutions and then developing
    Python code using Word2Vec and GloVe. There are a few terms which we have not
    discussed in the book so far, so the next section on Word2Vec and GloVe might
    be quite tedious to understand. If you are interested only in the application
    of the solutions, you can skip the next section.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节介绍两种最流行的技术 Word2Vec 和 GloVe。这一节提供了对 Word2Vec 和 GloVe 的理解。Word2Vec 和 GloVe
    的数学基础超出了本书的范围。我们将理解解决方案的工作机制，然后使用 Word2Vec 和 GloVe 开发 Python 代码。到目前为止，书中还有一些术语我们没有讨论过，所以下一节关于
    Word2Vec 和 GloVe 可能会很繁琐。如果你只对解决方案的应用感兴趣，可以跳过下一节。
- en: Word2Vec and GloVe
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Word2Vec 和 GloVe
- en: Word2Vec was first published in 2013\. It was developed by Tomas Mikolov, et
    al. at Google. We are sharing the link to the paper at the end of the chapter.
    You are advised to study the paper thoroughly.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 首次发表于 2013 年。它是由 Google 的 Tomas Mikolov 等人开发的。我们会在章节结束时分享论文的链接。建议你彻底研究这篇论文。
- en: Word2Vec is a group of models used to produce word embeddings. The input is
    a large corpus of text. The output is a vector space, with a very large number
    of dimensions. In this output, each of the word in the corpus is assigned a unique
    and corresponding vector. The most important point is that the words which have
    similar or common context in the corpus, are located similar in the vector space
    produced.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 是一组用于生成词嵌入的模型。输入是一个大型文本语料库。输出是一个向量空间，具有非常多的维度。在这个输出中，语料库中的每个单词被分配了一个唯一且对应的向量。最重要的一点是，在语料库中具有相似或共同上下文的单词，在生成的向量空间中位置相似。
- en: 'In Word2Vec, the researchers introduced two different learning models – Continuous
    Bag of Words and Continuous Skip-gram model, which we are covering briefly:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Word2Vec 中，研究人员介绍了两种不同的学习模型 - 连续词袋模型和连续跳字模型，我们简要介绍如下：
- en: 'Continuous bag of words or CBOW: in CBOW, the model makes a prediction of the
    current word from a window of surrounding context words. So, the CBOW learns Recall
    that in bag of words approach, the order of the words does not play any part.
    Similarly, in CBOW, the order of the words is insignificant.'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续词袋模型或 CBOW：在 CBOW 中，模型从周围上下文单词的窗口中预测当前单词。因此，CBOW 学习了在词袋方法中，单词的顺序不起作用。同样，在
    CBOW 中，单词的顺序是无关紧要的。
- en: 'Continuous skip-gram model: it uses the current word to predict the surrounding
    window of context words. While doing so, it allocates more weight to the neighboring
    words as compared to the distant words.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续跳字模型：它使用当前单词来预测周围窗口的上下文单词。在这样做时，它给邻近单词分配比远离单词更多的权重。
- en: GloVe or Global vectors for Word Representation is an unsupervised learning
    algorithm for generating vector representation for words. It was developed by
    Pennington, et al. at Stanford and launched in 2014\. It is a combination of two
    techniques – matrix factorization techniques and local context-based learning
    used in Word2Vec. GloVe can be used to find relationships like zip codes and cities,
    synonyms etc. It generated a single set of vectors for the words having the same
    morphological structure.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 或全局词向量是用于生成词向量表示的无监督学习算法。它由斯坦福的 Pennington 等人于 2014 年开发，并在 2014 年推出。它是两种技术的组合
    - 矩阵分解技术和 Word2Vec 中使用的基于本地上下文的学习。GloVe 可用于找到像邮政编码和城市、同义词等关系。它为具有相同形态结构的单词生成了一组单一的向量。
- en: Both the models (Word2Vec and GloVe) learn and understand vector representation
    of their words from the co-occurrence information. Co-occurrence means how frequently
    the words are appearing together in a large corpus. The prime difference is that
    word2vec is a prediction-based model, while GloVe is a frequency-based model.
    Word2Vec predicts the context given a word while GloVe learns the context by creating
    a co-occurrence matrix on how frequent a word appears in a given context.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型（Word2Vec 和 GloVe）都从共现信息中学习和理解它们的单词的向量表示。共现意味着单词在大型语料库中一起出现的频率。主要区别在于 Word2Vec
    是基于预测的模型，而 GloVe 是基于频率的模型。Word2Vec 预测给定单词的上下文，而 GloVe 通过创建一个共现矩阵来学习单词在给定上下文中出现的频率。
- en: '![](../Images/tgt.png)'
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_IMG
  zh: '![](../Images/tgt.png)'
- en: POP QUIZ – answer these question to check your understanding.. Answers at the
    end of the book
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 弹出测验 - 回答这些问题以检查您的理解。书的末尾有答案
- en: (1)      BOW is more rigorous than tf-idf approach. True or False.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: （1）BOW比tf-idf方法更严格。真或假。
- en: (2)      Differentiate between Word2Vec and GloVe.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: （2）Word2Vec和GloVe之间的区别。
- en: We will now move to the Case Study and Python implementation in the next section.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在下一节中转到案例研究和Python实现。
- en: Sentiment analysis case study with Python implementation
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用Python实现的情感分析案例研究
- en: We have so far discussed a lot of concepts on NLP and Text data. In this section,
    we are first going to explore a business case and then develop Python solution
    on the same. We are working on Sentiment analysis.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了许多关于NLP和文本数据的概念。在本节中，我们首先将探讨一个业务案例，然后在同一案例上开发Python解决方案。我们正在进行情感分析。
- en: Product reviews are a rich source of information – both to the customers and
    the organizations. Whenever we wish to buy any new product or services, we tend
    to look at the reviews by fellow customers. You might have reviewed products and
    services yourself. These reviews are available at Amazon, blogs, surveys etc.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 产品评论是信息的丰富来源 - 对客户和组织都是如此。每当我们希望购买任何新产品或服务时，我们倾向于查看其他客户的评论。您可能自己也曾评论过产品和服务。这些评论可以在亚马逊、博客、调查等处找到。
- en: 'Let’s consider a case. A water utilities provider receives complaints from
    its customers, reviews about the supply and comments about the overall experience.
    The streams can be – product quality, pricing, onboarding experience, ease of
    registration, payment process, supply reviews, power reviews etc. We want to determine
    the general context of the review – whether it is positive, negative or neutral.
    The reviews have the number of stars allocated, actual text reviews, pros and
    cons about the product/service, attributes etc. But at the same time, there are
    a few business problems like:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个案例。一个供水公用事业提供商收到了客户的投诉，对供水的评论以及对整体体验的评论。可能的流是 - 产品质量、定价、注册体验、注册流程的简易性、付款流程、供水评论、电力评论等等。我们想要确定评论的一般背景
    - 它是积极的、消极的还是中立的。评论有分配的星级数量、实际文本评论、关于产品/服务的优点和缺点、属性等。但与此同时，也存在一些业务问题，如：
- en: Many times, it is observed that the numbers of stars received ay a product/service
    is very high, while the actual reviews are quite negative.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多时候，观察到产品/服务收到的星级数量非常高，而实际评论却相当负面。
- en: The organizations and the product owners, need to know which features are appreciated
    by the customers and which features are disliked by the customers. The team can
    then work on improving the features which are disliked by the customers.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组织和产品所有者需要知道客户喜欢哪些功能，哪些功能不受客户喜欢。然后团队可以着手改进客户不喜欢的功能。
- en: There is also a need to gauge and keep an eye on the competition! The organizations
    need to know, the attributes of the popular products of their competitors.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还需要评估并密切关注竞争！组织需要知道竞争对手的热门产品的属性。
- en: The product owners can better plan for the upcoming features they wish to release
    in the future.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产品所有者可以更好地计划他们希望在未来发布的功能。
- en: 'So, the business teams will be able to answer these two most important questions:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，业务团队将能够回答这两个最重要的问题：
- en: What is our customer’s satisfaction levels for the products and services?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们客户对产品和服务的满意度水平是多少？
- en: What are the major pain points and dissatisfactions of the customers, what drives
    the customers engagement, which services are complex and time-consuming, and which
    are the most liked services/products?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户的主要痛点和不满，驱动客户参与的因素，哪些服务是复杂且耗时的，哪些服务/产品最受欢迎？
- en: 'This business use case will drive the following business benefits:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这个商业用例将带来以下业务利益：
- en: The products and services which are most satisfactory and are most liked ones
    should be continued.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最令人满意且最受欢迎的产品和服务应该继续。
- en: The ones which are not liked and receiving a negative score have to be improved
    and challenges have to be mitigated.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那些不受欢迎且得到负面评分的功能必须得到改进，挑战也必须得到缓解。
- en: The respective teams like finance, operations, complaints, CRM etc. can be notified
    and they can work individually to improve the customer experience.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 各自的团队，如财务、运营、投诉、CRM等，可以被通知，并且他们可以分别工作以改善客户体验。
- en: The precise reasons of liking or disliking the services will be useful for the
    respective teams to work in the correct direction.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 喜欢或不喜欢服务的精确原因将对相应的团队有助于朝正确的方向努力。
- en: Overall, it will provide a benchmark to measure the Net Promoter Score (NPS)
    score for the customer base. The business can strive to enhance the overall customer
    experience.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总的来说，它将为测量客户基础的净推荐得分（NPS）提供一个基准。企业可以努力提升整体客户体验。
- en: We might like to represent these findings by means of a dashboard. This dashboard
    will be refreshed at a regular cycle like monthly or quarterly refresh.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可能希望通过仪表板来表示这些发现。这个仪表板将定期刷新，比如每月或每季度刷新一次。
- en: To solve this business problem, the teams can collect relevant data from websites,
    surveys, Amazon, blogs etc. And then an analysis can be done on that dataset.
    It is relatively easy to analyze the structured data. In this example we are going
    to work on text data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个业务问题，团队可以从网站、调查、亚马逊、博客等收集相关数据。然后对该数据集进行分析。分析结构化数据相对容易。在这个例子中，我们将处理文本数据。
- en: The Python Jupyter notebook is checkedin at the Github location. You are advised
    to use the Jupyter notebook from the GitHub location as it contains more steps.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Python Jupyter 笔记本在 GitHub 地址上进行了检入。建议您使用 GitHub 地址上的 Jupyter 笔记本，因为它包含更多步骤。
- en: 'Step 1: We import all the libraries here.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步：我们在这里导入所有库。
- en: '[PRE9]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Step 2: We will define the tags here. These tags are used to get the attributes
    of the product from the reviews.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步：我们在这里定义标签。这些标签用于从评论中获取产品的属性。
- en: '[PRE10]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Step 3: We are now making everything ready to extract the data. We are creating
    a dataframe to store the customer reviews. Then we are iterating through all the
    reviews and then extracting the information.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步：我们现在正在准备好提取数据。我们正在创建一个数据框来存储客户评论。然后我们迭代所有评论，然后提取信息。
- en: '[PRE11]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Step 4: Let’s iterate through the reviews and then fill the details.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 步：让我们迭代通过评论然后填写详细信息。
- en: '[PRE12]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Step 5: Let’s have a look at the output we generated.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 步：让我们看看我们生成的输出。
- en: '[PRE13]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Step 6: we will now save the output to a path. You can give your own path.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 第 6 步：现在我们将输出保存到一个路径。您可以提供自己的路径。
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 7: Load the data and analyze it.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 第 7 步：加载数据并分析。
- en: '[PRE15]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Step 8: We will now look at the basic information about the dataset'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 第 8 步：我们现在将查看数据集的基本信息。
- en: '[PRE16]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Step 9: We will now look at the distribution of the stars given in the reviews.
    This will allow us to understand the reviews given by the customers.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9 步：我们现在将查看评论中给出的星级分布。这将帮助我们理解客户给出的评论。
- en: '[PRE17]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Step 10: Make the text as lowercase, and then we remove the stop words and
    the words which have highest frequency.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 第 10 步：将文本转换为小写，并删除停用词和频率最高的单词。
- en: '[PRE18]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Step 11: tokenize the data now.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 第 11 步：现在对数据进行分词。
- en: '[PRE19]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Step 12: we are performing lemmatization now.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 第 12 步：我们现在正在执行词形还原。
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 13: Now we are appending all the reviews to the string.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 第 13 步：现在我们正在将所有评论附加到字符串上。
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Step 14: the sentiment analysis is done here. From TextBlob we are taking the
    sentiment method. It generates polarity and subjectivity for a sentiment.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 第 14 步：情感分析在这里完成。我们从 TextBlob 中获取情感方法。它为情感生成极性和主观性。
- en: '[PRE22]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Step 15: save the sentiment to a csv file.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 第 15 步：将情感保存到 csv 文件中。
- en: '[PRE23]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Step 15: we will now allocate a meaning or a tag to the sentiment. We are classifying
    each of the score from extremely satisfied to extremely dissatisfied.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 第 15 步：我们现在将给情感分配一个含义或标签。我们正在将每个分数分类为非常满意到非常不满意。
- en: '[PRE24]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Step 16: We will now look at the sentiment scores and plot them too. Finally,
    we will merge them with the main dataset.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 第 16 步：我们现在将查看情感得分并绘制它们。最后，我们将它们与主数据集合并。
- en: '[PRE25]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this case study, you not only scraped the reviews from the website, you also
    analyzed the dataset. If we compare the sentiments, we can see that the stars
    given to a product, do not represent a true picture.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，您不仅从网站上抓取了评论，还分析了数据集。如果我们比较情感，我们可以看到给产品的星级并不代表真实情况。
- en: In (), we are comparing the actual stars and the output from sentiment analysis.
    We can observe that 73% have given 5 stars and 7% have given 4 stars, while in
    the sentiment analysis most of the reviews have been classified as neutral. This
    is the real power of sentiment analysis!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在 () 中，我们正在比较实际的星级和情感分析的输出。我们可以观察到，73% 的人给出了 5 星，7% 的人给出了 4 星，而在情感分析中，大多数评论被分类为中性。这就是情感分析的真正力量！
- en: Figure 7-8 Compare the original distribution of number of stars on the left
    side, observe the real results from the sentiment analysis
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7-8比较左侧的原始星级分布，观察情感分析的实际结果
- en: '![](images/07_img_0017.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0017.png)'
- en: Sentiment analysis is quite an important use case. It is very useful for the
    business and the product teams.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一个非常重要的用例。它对企业和产品团队非常有用。
- en: We will now move to the second case studt on document classification using Python.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转到使用Python进行文档分类的第二个案例研究。
- en: Text clustering using Python
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用Python进行文本聚类
- en: Consider this. You have got a bunch of text datasets or documents. But they
    all are mixed up. We do not know the text belongs to which class. In this case,
    we will assume that we have two types of text datasets with us – one which has
    all the data related to football and second class is travel. We will develop a
    model which can segregate these two classes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下。你有一堆的文本数据集或文档。但是它们都混在一起了。我们不知道文本属于哪个类别。在这种情况下，我们假设我们手头有两类文本数据集 - 一个与足球有关的数据，第二类是旅行。我们将开发一个可以分离这两个类别的模型。
- en: 'Step 1: Import all the libraries'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步：导入所有库
- en: '[PRE26]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Step 2: We are now creating a dummy dataset. This text data has a few sentences
    we have written ourselves. There are two categories -'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步：我们现在正在创建一个虚拟数据集。这段文本数据是我们自己写的几句话。有两个类别 -
- en: '[PRE27]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Step 3: We will use tfidf to vectorize the data.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步：我们将使用tfidf对数据进行向量化处理。
- en: '[PRE28]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Step 4: let’s do the clustering now.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第四步：现在让我们进行聚类。
- en: '[PRE29]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Step 5: Lets represent centroids and print the outputs.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 第五步：让我们表示质心并打印输出。
- en: '[PRE30]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](images/07_img_0019.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](images/07_img_0019.png)'
- en: You can extend this example to other datasets too. Use your own dataset and
    replicate the code in the example above.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将此示例扩展到其他数据集。使用自己的数据集并复制上面示例中的代码。
- en: There is no more Python Jupyter notebook which uses Word2Vec and GloVe. We have
    checked-in the code at the GitHub location of the book. You are advised to use
    it. It is really an important source to represent text data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 不再有使用Word2Vec和GloVe的Python Jupyter笔记本了。我们已经将代码检入到了本书的GitHub位置。建议您使用它。这是一个非常重要的表示文本数据的资源。
- en: With this, we are coming to the end of this exciting chapter. Let’s move to
    the summary now.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们来到了这个令人兴奋的章节的结尾。现在让我们进入总结部分。
- en: 7.6 Summary
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 小结
- en: Text data is one of the most useful datasets. A lot of intelligence is hidden
    in the texts. Logs, blogs, reviews, posts, tweets, complaints, comments, articles
    and so on – the sources of text data are many. Organizations have started to invest
    in setting up the infrastructure for accessing text data and storing it. Analyzing
    text data requires better processing powers and better machines. It requires special
    skill sets and deeper understanding of the concepts. NLP is an evolving field
    and a lot of research is underway. At the same time, we cannot ignore the importance
    of sound business acumen and knowledge.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据是最有用的数据集之一。很多智慧隐藏在文本中。日志、博客、评论、推文、投诉、评论、文章等等 - 文本数据的来源很多。机构已经开始投资建立访问文本数据和存储文本数据的基础设施。分析文本数据需要更好的处理能力和更好的计算机。它需要特殊的技能和更深入的理解概念。NLP是一个不断发展的领域，许多研究正在进行中。与此同时，我们不能忽视商业敏锐度和知识的重要性。
- en: Data analysis and machine learning are not easy. We have to understand a lot
    of concepts around data cleaning, exploration, representation and modelling. But,
    analyzing unstructured data might be even more complex than structured datasets.
    We worked on images dataset in the last chapter. In the current chapter, we worked
    on text data.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析和机器学习并不容易。我们必须理解很多关于数据清洗、数据探索、数据表示和建模的概念。但是，分析非结构化数据可能比结构化数据集更加复杂。在上一章中，我们处理了图像数据集。在当前章节中，我们处理了文本数据。
- en: Text data is one of the most difficult to analyze. There are so many permutations
    and combinations for text data. Cleaning the text data is not easy and is quite
    a complex task. In this chapter we discussed few of those important techniques
    to clean the text data. We also covered few important methods to represent text
    data in vector forms. You are advised to do practice of each of these methods
    and compare the performances by applying each of the techniques.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据是最难分析的数据之一。文本数据有很多排列组合。清洗文本数据并不容易，是一项相当复杂的任务。在本章中，我们讨论了几种清洗文本数据的重要技术。我们还介绍了几种将文本数据表示为向量形式的重要方法。建议您对这些方法进行实践，并通过应用每种技术进行比较性能。
- en: With this, we come to an end to the chapter seven of the book. This also marks
    an end to part two of the book. In the next part of the book, the complexity increases.
    We will be studying even deeper concepts of unsupervised learning algorithms.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一章的结束，我们也结束了书的第二部分。在书的下一部分，复杂性会增加。我们将研究更深层次的无监督学习算法概念。
- en: You can now move to the practice questions.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以转到练习问题。
- en: Practical next steps and suggested readings
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 实用的下一步和建议阅读
- en: Get the datasets from the link below. You will find a lot of text datasets here.
    You are advised to implement clustering and dimensionality reduction solutions.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下面的链接获取数据集。在这里你会找到很多文本数据集。建议你实现聚类和降维解决方案。
- en: '[https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da](blog.cambridgespark.com.html)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da](blog.cambridgespark.com.html)'
- en: This is the second source of text datasets, where you will find a lot of useful
    datasets.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是文本数据集的第二个来源，你会找到许多有用的数据集。
- en: '[https://www.kaggle.com/datasets?search=text](www.kaggle.com.html)'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/datasets?search=text](www.kaggle.com.html)'
- en: You are advised to go through the research paper – Efficient Estimation of Word
    Representations in Vector Space by Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey
    Dean
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你被建议阅读研究论文《在向量空间中高效估计词表示》（Efficient Estimation of Word Representations in Vector
    Space），作者是 Tomas Mikolov、Kai Chen、Greg Corrado、Jeffrey Dean。
- en: '[https://arxiv.org/pdf/1301.3781.pdf](pdf.html)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1301.3781.pdf](pdf.html)'
- en: 'You are advised to go through the research paper – GloVe: Global Vectors for
    Word Representation by Jeffrey Pennington, Richard Socher, Christopher D. Manning'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '你被建议阅读研究论文《GloVe：用于词表示的全局向量》（GloVe: Global Vectors for Word Representation），作者是
    Jeffrey Pennington、Richard Socher、Christopher D. Manning。'
- en: '[https://nlp.stanford.edu/pubs/glove.pdf](pubs.html)'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://nlp.stanford.edu/pubs/glove.pdf](pubs.html)'
- en: There are a few papers which are really quoted widely
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有几篇论文被广泛引用。
- en: 'Avrim Blum and Tom Mitchell: Combining Labeled and Unlabeled Data with Co-Training,
    1998'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Avrim Blum 和 Tom Mitchell：使用共训练结合标记和未标记数据，1998年
- en: 'Kevin Knight: Bayesian Inference with Tears, 2009.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kevin Knight：贝叶斯推断与眼泪，2009年。
- en: 'Thomas Hofmann: Probabilistic Latent Semantic Indexing, SIGIR 1999.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Thomas Hofmann：概率隐语义索引，SIGIR 1999。
- en: Donald Hindle and Mats Rooth. Structural Ambiguity and Lexical Relations, Computational
    Linguistics, 1993.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Donald Hindle 和 Mats Rooth。结构歧义和词汇关系，计算语言学，1993年。
- en: 'Collins and Singer: Unsupervised Models for Named Entity Classification, EMNLP
    1999.'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Collins 和 Singer：命名实体分类的无监督模型，EMNLP 1999。
- en: You are advised to go through the research paper- Using TF-IDF to Determine
    Word Relevance in Document Queries by Juan Ramos
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你被建议阅读研究论文《使用 TF-IDF 确定文档查询中单词相关性》（Using TF-IDF to Determine Word Relevance
    in Document Queries），作者是 Juan Ramos。
- en: '[https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf](viewdoc.html)'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&rep=rep1&type=pdf](viewdoc.html)'
- en: You are advised to go through the research paper – An Improved Text Sentiment
    Classification Model Using TF-IDF and Next Word Negation by Bijoyan das and Sarit
    Chakraborty
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你被建议阅读研究论文《使用 TF-IDF 和下一个词否定改进的文本情感分类模型》（An Improved Text Sentiment Classification
    Model Using TF-IDF and Next Word Negation），作者是 Bijoyan Das 和 Sarit Chakraborty。
- en: '[https://arxiv.org/pdf/1806.06407.pdf](pdf.html)'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1806.06407.pdf](pdf.html)'
