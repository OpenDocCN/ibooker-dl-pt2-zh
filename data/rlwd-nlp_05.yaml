- en: 3 Word and document embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 单词和文档嵌入
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: What word embeddings are and why they are important
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词嵌入是什么以及它们为什么重要
- en: How the Skip-gram model learns word embeddings and how to implement it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-gram 模型如何学习单词嵌入以及如何实现它
- en: What GloVe embeddings are and how to use pretrained vectors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GloVe 嵌入是什么以及如何使用预训练的向量
- en: How to use Doc2Vec and fastText to train more advanced embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Doc2Vec 和 fastText 训练更高级的嵌入
- en: How to visualize word embeddings
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化单词嵌入
- en: In chapter 2, I pointed out that neural networks can deal only with numbers,
    whereas almost everything in natural language is discrete (i.e., separate concepts).
    To use neural networks in your NLP application, you need to convert linguistic
    units to numbers, such as vectors. For example, if you wish to build a sentiment
    analyzer, you need to convert the input sentence (sequence of words) into a sequence
    of vectors. In this chapter, we’ll discuss word embeddings, which are the key
    to achieving this bridging. We’ll also touch upon a couple of fundamental linguistic
    components that are important in understanding embeddings and neural networks
    in general.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，我指出神经网络只能处理数字，而自然语言中几乎所有内容都是离散的（即，分离的概念）。要在自然语言处理应用中使用神经网络，你需要将语言单位转换为数字，例如向量。例如，如果你希望构建一个情感分析器，你需要将输入句子（单词序列）转换为向量序列。在本章中，我们将讨论单词嵌入，这是实现这种桥接的关键。我们还将简要介绍几个在理解嵌入和神经网络的一般性质中重要的基本语言组件。
- en: 3.1 Introducing embeddings
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 引入嵌入
- en: As we discussed in chapter 2, an embedding is a real-valued vector representation
    of something that is usually discrete. In this section, we’ll revisit what embeddings
    are and discuss in detail what roles they play in NLP applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 2 章中讨论的，嵌入是通常离散的事物的实值向量表示。在本节中，我们将重新讨论嵌入是什么，并详细讨论它们在自然语言处理应用中的作用。
- en: 3.1.1 What are embeddings?
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 什么是嵌入？
- en: 'A word embedding is a real-valued vector representation of a word. If you find
    the concept of vectors intimidating, think of them as single-dimensional arrays
    of float numbers, like the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入是一个单词的实值向量表示。如果你觉得向量的概念令人生畏，可以把它们想象成一维的浮点数数组，就像下面这样：
- en: vec("cat") = [0.7, 0.5, 0.1]
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("cat") = [0.7, 0.5, 0.1]
- en: vec("dog") = [0.8, 0.3, 0.1]
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("dog") = [0.8, 0.3, 0.1]
- en: vec("pizza") = [0.1, 0.2, 0.8]
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("pizza") = [0.1, 0.2, 0.8]
- en: Because each array contains three elements, you can plot them as points in a
    3-D space as in figure 3.1\. Notice that semantically-related words (“cat” and
    “dog”) are placed close to each other.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个数组都包含三个元素，你可以将它们绘制为三维空间中的点，如图 3.1 所示。请注意，语义相关的单词（“猫”和“狗”）被放置在彼此附近。
- en: '![CH03_F01_Hagiwara](../Images/CH03_F01_Hagiwara.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F01_Hagiwara](../Images/CH03_F01_Hagiwara.png)'
- en: Figure 3.1 Word embeddings on a 3-D space
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 单词嵌入在三维空间中
- en: NOTE In fact, you can embed (i.e., represent by a list of numbers) not just
    words but also almost anything—characters, sequences of characters, sentences,
    or categories. You can embed any categorical variables using the same method,
    although in this chapter, we’ll focus on two of the most important concepts in
    NLP—words and sentences.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 实际上，你可以嵌入（即，用一系列数字表示）不仅仅是单词，还有几乎任何东西 —— 字符、字符序列、句子或类别。你可以使用相同的方法嵌入任何分类变量，尽管在本章中，我们将专注于自然语言处理中两个最重要的概念
    —— 单词和句子。
- en: 3.1.2 Why are embeddings important?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 嵌入为什么重要？
- en: Why are embeddings important? Well, word embeddings are not just important but
    *essential* for using neural networks to solve NLP tasks. Neural networks are
    pure mathematical computation models that can deal only with numbers. They can’t
    do symbolic operations, such as concatenating two strings or conjugating a verb
    to past tense, unless these items are all represented by numbers and arithmetic
    operations. On the other hand, almost everything in NLP, such as words and labels,
    is symbolic and discrete. This is why you need to bridge these two worlds, and
    using embeddings is a way to do it. See figure 3.2 for an overview on how to use
    word embeddings for an NLP application.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入为什么重要？嗯，单词嵌入不仅重要，而且 *至关重要* 用于使用神经网络解决自然语言处理任务。神经网络是纯数学计算模型，只能处理数字。它们无法进行符号操作，例如连接两个字符串或使动词变为过去时，除非这些项目都用数字和算术操作表示。另一方面，自然语言处理中的几乎所有内容，如单词和标签，都是符号和离散的。这就是为什么你需要连接这两个世界，使用嵌入就是一种方法。请参阅图
    3.2，了解如何在自然语言处理应用中使用单词嵌入的概述。
- en: '![CH03_F02_Hagiwara](../Images/CH03_F02_Hagiwara.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F02_Hagiwara](../Images/CH03_F02_Hagiwara.png)'
- en: Figure 3.2 Using word embeddings with NLP models
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 使用词嵌入与 NLP 模型
- en: 'Word embeddings, just like any other neural network models, can be trained,
    because they are simply a collection of parameters (or “magic constants,” which
    we talked about in the previous chapter). Embeddings are used with your NLP model
    in the following three scenarios:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入，就像任何其他神经网络模型一样，可以进行训练，因为它们只是一组参数（或“魔法常数”，我们在上一章中谈到过）。词嵌入在以下三种情况下与您的 NLP
    模型一起使用：
- en: 'Scenario 1: Train word embeddings and your model at the same time using the
    train set for your task.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情况 1：同时使用任务的训练集训练词嵌入和您的模型。
- en: 'Scenario 2: First, train word embeddings independently using a larger text
    dataset. Alternatively, obtain pretrained word embeddings from somewhere else.
    Then initialize your model using the pretrained word embeddings, and train them
    and your model at the same time using the train set for your task.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情况 2：首先，独立训练词嵌入使用更大的文本数据集。或者，从其他地方获取预训练的词嵌入。然后使用预训练的词嵌入初始化您的模型，并同时使用任务的训练集对它们和您的模型进行训练。
- en: 'Scenario 3: Same as scenario 2, except you fix word embeddings while you train
    your model.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情况 3：与情况 2 相同，除了您在训练模型时固定词嵌入。
- en: In the first scenario, word embeddings are initialized randomly and trained
    in conjunction with your NLP model using the same dataset. This is basically how
    we built the sentiment analyzer in chapter 2\. Using an analogy, this is like
    having a dance teacher teach a baby to walk and dance at the same time. It is
    not an entirely impossible feat (in fact, some babies might end up being better,
    maybe more creative dancers by skipping the walking part, but don’t try this at
    home), but rarely a good idea. Babies would probably have a much better chance
    if they are taught how to stand and walk properly first, and then how to dance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，词嵌入是随机初始化的，并且与您的 NLP 模型一起使用相同的数据集进行训练。这基本上就是我们在第 2 章中构建情感分析器的方式。用一个类比来说，这就像一个舞蹈老师同时教一个婴儿走路和跳舞。这并不是完全不可能的事情（事实上，有些婴儿可能通过跳过走路部分而成为更好、更有创意的舞者，但不要在家里尝试这样做），但很少是一个好主意。如果先教会婴儿正确站立和行走，然后再教会他们如何跳舞，他们可能会有更好的机会。
- en: Similarly, it’s not uncommon to train an NLP model and word embeddings as its
    subcomponent at the same time. But many large-scale, high-performance NLP models
    usually rely on external word embeddings that are pretrained using larger datasets
    (scenarios 2 and 3). Word embeddings can be learned from unlabeled large text
    datasets—that is, a large amount of plain text data (e.g., Wikipedia dumps), which
    are usually more readily available than the train datasets for your task (e.g.,
    the Stanford Sentiment Treebank). By leveraging such large textual data, you can
    teach your model a lot about how natural language works even before it sees a
    single instance from the dataset for your task. Training a machine learning model
    on one task and repurposing it for another task is called *transfer learning*,
    which is becoming increasingly popular in many machine learning domains, NLP included.
    We’ll further discuss transfer learning in chapter 9.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，同时训练 NLP 模型和其子组件词嵌入并不罕见。但是，许多大规模、高性能的 NLP 模型通常依赖于使用更大数据集预训练的外部词嵌入（情况 2 和
    3）。词嵌入可以从未标记的大型文本数据集中学习，即大量的纯文本数据（例如维基百科转储），这通常比用于任务的训练数据集（例如斯坦福情感树库）更容易获得。通过利用这样的大量文本数据，您可以在模型看到任务数据集中的任何实例之前就向其教授关于自然语言的许多知识。在一个任务上训练机器学习模型，然后为另一个任务重新利用它被称为*迁移学习*，这在许多机器学习领域中，包括
    NLP 在内，变得越来越受欢迎。我们将在第 9 章进一步讨论迁移学习。
- en: Using the dancing baby analogy again, most healthy babies figure out how to
    stand and walk themselves. They may get some help from adults, usually from their
    close caregivers such as parents. This form of “help,” however, is usually a lot
    more abundant and cheaper than the “training signal” you get from a hired dance
    teacher, which is why it’s a lot more effective if they learn how to walk first,
    then move on to dancing. Many skills used for walking transfer to dancing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用跳舞婴儿的类比，大多数健康的婴儿都会自己学会站立和行走。他们可能会得到一些成人的帮助，通常来自他们的亲近照顾者，比如父母。然而，这种“帮助”通常比从聘请的舞蹈老师那里得到的“训练信号”丰富得多，也更便宜，这就是为什么如果他们先学会走路，然后再学会跳舞，效果会更好的原因。许多用于行走的技能会转移到跳舞上。
- en: The difference between scenarios 2 and 3 is whether the word embeddings are
    adjusted, or *fine-tuned*, while your NLP model is trained. Whether or not this
    is effective may depend on your task and the dataset. Teaching your toddler ballet
    may have a good effect on how they walk (e.g., by improving their posture), which
    in turn could have a positive effect on how they dance, but scenario 3 doesn’t
    allow this to happen.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 方案2和方案3之间的区别在于，在训练NLP模型时是否调整了词嵌入，或者*精调*了词嵌入。这是否有效可能取决于你的任务和数据集。教你的幼儿芭蕾可能会对他们的步履有好处（例如通过改善他们的姿势），从而可能对他们的舞蹈有积极的影响，但是方案3不允许发生这种情况。
- en: 'The only remaining question you might have is: Where do embeddings come from?
    I mentioned earlier that they can be trained from a large amount of plain text.
    This chapter explains how this is possible and what models are used to achieve
    this.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问的最后一个问题是：嵌入是怎么来的呢？之前我提到过，它们可以从大量的纯文本中进行训练。本章将解释这是如何实现的以及使用了哪些模型。
- en: '3.2 Building blocks of language: Characters, words, and phrases'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 语言的基本单元：字符、词和短语
- en: Before I explain word-embedding models, I’m going to touch upon some basic concepts
    of language, such as characters, words, and phrases. It helps to understand these
    concepts when you design the structure of your NLP application. Figure 3.3 shows
    some examples of those concepts.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释词嵌入模型之前，我会简单介绍一些语言的基本概念，如字符、词和短语。当你设计你的NLP应用程序的结构时，了解这些概念将会有所帮助。图3.3展示了一些例子。
- en: 3.2.1 Characters
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 字符
- en: A *character* (also called a *grapheme* in linguistics) is the smallest unit
    of a writing system. In written English, “a,” “b,” and “z” are characters. Characters
    do not necessarily carry meaning by themselves or represent any fixed sound when
    spoken, although in some languages (e.g., Chinese), most do. A typical character
    in many languages can be represented by a single Unicode codepoint (by string
    literals such as "\uXXXX" in Python), but this is not always the case. Many languages
    use a combination of more than one Unicode codepoint (e.g., accent marks) to represent
    a single character. Punctuation marks, such as “.” (period), “,” (comma), and
    “?” (question mark), are also characters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*字符*（在语言学中也称为*字形*）是书写系统中的最小单位。在英语中，"a"、"b"和"z"都是字符。字符本身并不一定具有意义，也不一定在口语中代表任何固定的声音，尽管在某些语言中（例如中文）大多数字符都有这样的特点。许多语言中的典型字符可以用单个Unicode代码点（通过Python中的字符串文字，如"\uXXXX"）表示，但并非总是如此。许多语言使用多个Unicode代码点的组合（例如重音符号）来表示一个字符。标点符号，如"."（句号）、","（逗号）和"?"（问号），也是字符。'
- en: '![CH03_F03_Hagiwara](../Images/CH03_F03_Hagiwara.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F03_Hagiwara](../Images/CH03_F03_Hagiwara.png)'
- en: Figure 3.3 Building blocks of language used in NLP
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 NLP中使用的语言基本单元
- en: 3.2.2 Words, tokens, morphemes, and phrases
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 词、标记、词素和短语
- en: A *word* is the smallest unit in a language that can be uttered independently
    and that usually carries some meaning. In English, “apple,” “banana,” and “zebra”
    are words. In most written languages that use alphabetic scripts, words are usually
    separated by spaces or punctuation marks. In some languages, like Chinese, Japanese,
    and Thai, however, words are not explicitly delimited by spaces and require a
    preprocessing step called *word segmentation* to identify words in a sentence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*词*是语言中可以独立发音并通常具有一定意义的最小单位。在英语中，"apple"、"banana"和"zebra"都是词。在大多数使用字母脚本的书面语言中，词通常由空格或标点符号分隔。然而，在一些语言（如中文、日文和泰文）中，词并没有明确由空格分隔，并且需要通过预处理步骤（称为*分词*）来识别句子中的词。'
- en: A closely related concept to a word in NLP is a *token*. A token is a string
    of contiguous characters that play a certain role in a written language. Most
    words (“apple,” “banana,” “zebra”) are also tokens when written. Punctuation marks
    such as the exclamation mark (“!”) are tokens but not words, because you can’t
    utter them in isolation. Word and token are often used interchangeably in NLP.
    In fact, when you see “word” in NLP text (including this book), it often means
    “token,” because most NLP tasks deal only with written text that is processed
    in an automatic way. Tokens are the output of a process called *tokenization*,
    which I’ll explain more below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: NLP中与单词相关的一个概念是*标记*。标记是在书面语言中扮演特定角色的连续字符字符串。大多数单词（“苹果”，“香蕉”，“斑马”）在书写时也是标记。标点符号（如感叹号“!”）是标记，但不是单词，因为不能单独发音。在NLP中，“单词”和“标记”通常可以互换使用。实际上，在NLP文本（包括本书）中，当你看到“单词”时，通常指的是“标记”，因为大多数NLP任务只处理以自动方式处理的书面文本。标记是一个称为*标记化*的过程的输出，我将在下面进行详细解释。
- en: Another closely related concept is morpheme. A *morpheme* is the smallest unit
    of meaning in a language. A typical word consists of one or more morphemes. For
    example, “apple” is a word and also a morpheme. “Apples” is a word comprised of
    two morphemes, “apple” and “-s,” which is used to signify the noun is plural.
    English contains many other morphemes, including “-ing,” “-ly,” “-ness,” and “un-.”
    The process for identifying morphemes in a word or a sentence is called *morphological
    analysis*, and it has a wide range of NLP/linguistics applications, but this is
    outside the scope of this book.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关概念是形态素。*形态素*是语言中的最小意义单位。一个典型的单词由一个或多个形态素组成。例如，“苹果”既是一个单词，也是一个形态素。“苹果”是由两个形态素“苹果”和“-s”组成的单词，用来表示名词的复数形式。英语中还包含许多其他形态素，包括“-ing”，“-ly”，“-ness”和“un-”。在单词或句子中识别形态素的过程称为*形态分析*，它在NLP/语言学应用中有广泛的应用，但超出了本书的范围。
- en: A *phrase* is a group of words that play a certain grammatical role. For example,
    “the quick brown fox” is a noun phrase (a group of words that behaves like a noun),
    whereas “jumps over the lazy dog” is a verb phrase. The concept of phrase may
    be used somewhat liberally in NLP to simply mean any group of words. For example,
    in many NLP literatures and tasks, words like “Los Angeles” are treated as phrases,
    although, linguistically speaking, they are closer to a word.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*短语*是一组在语法角色上扮演特定角色的单词。例如，“the quick brown fox”是一个名词短语（像一个名词那样表现的一组词），而“jumps
    over the lazy dog”是一个动词短语。在NLP中，短语的概念可能被宽泛地用来表示任何一组单词。例如，在许多NLP文献和任务中，像“洛杉矶”这样的词被视为短语，虽然在语言学上，它们更接近一个词。'
- en: 3.2.3 N-grams
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 N-grams
- en: Finally, you may encounter the concept of *n-grams* in NLP. An n-gram is a contiguous
    sequence of one or more occurrences of linguistic units, such as characters and
    words. For example, a word n-gram is a contiguous sequence of words, such as “the”
    (one word), “quick brown” (two words), “brown fox jumps” (three words). Similarly,
    a character n-gram is composed of characters, such as “b” (one character), “br”
    (two characters), “row” (three characters), and so on, which are all character
    n-grams made from “brown.” An n-gram of size 1 (when n = 1) is called a *unigram*.
    N-grams of size 2 and 3 are called a *bigram* and a *trigram*, respectively.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在NLP中，你可能会遇到*n-gram*的概念。n-gram是一个或多个语言单位（如字符和单词）的连续序列。例如，一个单词n-gram是一个连续的单词序列，如“the”（一个单词），“quick
    brown”（两个单词），“brown fox jumps”（三个单词）。同样，字符n-gram由字符组成，例如“b”（一个字符），“br”（两个字符），“row”（三个字符）等，它们都是由“brown”组成的字符n-gram。当n
    = 1时，大小为1的n-gram称为*unigram*。大小为2和3的n-gram分别被称为*bigram*和*trigram*。
- en: Word n-grams are often used as proxies for phrases in NLP, because if you enumerate
    all the n-grams of a sentence, they often contain linguistically interesting units
    that correspond to phrases such as “Los Angeles” and “take off.” In a similar
    vein, we use character n-grams when we want to capture subword units that roughly
    correspond to morphemes. In NLP, when you see “n-grams” (without a qualifier),
    they are often *word* n-grams.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，单词n-gram通常被用作短语的代理，因为如果枚举一个句子的所有n-gram，它们通常包含语言上有趣的单元，与短语（例如“洛杉矶”和“起飞”）对应。类似地，当我们想捕捉大致对应于形态素的子词单位时，我们使用字符n-gram。在NLP中，当你看到“n-grams”（没有限定词）时，它们通常是*单词*n-gram。
- en: NOTE Interestingly, in search and information retrieval, n-grams often mean
    *character* n-grams used for indexing documents. Be mindful which type of n-grams
    are implied by the context when you read papers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有趣的是，在搜索和信息检索中，n-grams通常指用于索引文档的*字符* n-grams。当你阅读论文时，要注意上下文暗示的是哪种类型的n-grams。
- en: 3.3 Tokenization, stemming, and lemmatization
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 分词、词干提取和词形还原
- en: We covered some basic linguistic units often encountered in NLP. In this section,
    I introduce some steps where linguistic units are processed in a typical NLP pipeline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了在自然语言处理中经常遇到的一些基本语言单位。在本节中，我将介绍一些典型自然语言处理流水线中处理语言单位的步骤。
- en: 3.3.1 Tokenization
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 分词
- en: '*Tokenization* is a process where the input text is split into smaller units.
    There are two types of tokenization: word and sentence tokenization. *Word tokenization*
    splits a sentence into tokens (rough equivalent to words and punctuation), which
    I mentioned earlier. *Sentence tokenization*, on the other hand, splits a piece
    of text that may include more than one sentence into individual sentences. If
    you say tokenization, it usually means word tokenization in NLP.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*分词* 是将输入文本分割成较小单元的过程。有两种类型的分词：单词分词和句子分词。*单词分词* 将一个句子分割成标记（大致相当于单词和标点符号），我之前提到过。*句子分词*
    则将可能包含多个句子的文本分割成单个句子。如果说分词，通常指的是NLP中的单词分词。'
- en: Many NLP libraries and frameworks support tokenization out of the box, because
    it is one of the most fundamental and widely used preprocessing steps in NLP.
    In what follows, I show you how to do tokenization using two popular NLP libraries—NLTK
    ([https://www.nltk.org/](https://www.nltk.org/)) and spaCy ([https://spacy.io/](https://spacy.io/)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言处理库和框架支持分词，因为它是自然语言处理中最基本且最常用的预处理步骤之一。接下来，我将向你展示如何使用两个流行的自然语言处理库——NLTK
    ([https://www.nltk.org/](https://www.nltk.org/)) 和 spaCy ([https://spacy.io/](https://spacy.io/))
    进行分词。
- en: NOTE Before running the example code in this section, make sure the libraries
    are both installed. In a typical Python environment, they can be installed by
    running pip install nltk and pip install spacy. After installation, you need to
    download the necessary data and models by running python -c "import nltk; nltk.download('punkt')"
    for NLTK, and python -m spacy download en for spaCy from the command line. You
    can also run all the examples in this section via Google Colab ([http://realworldnlpbook.com/ch3.html#tokeni
    zation](http://realworldnlpbook.com/ch3.html#tokenization)) without installing
    any Python environments or dependencies.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在运行本节示例代码之前，请确保两个库都已安装。在典型的Python环境中，可以通过运行 pip install nltk 和 pip install
    spacy 进行安装。安装完成后，你需要通过命令行运行 python -c "import nltk; nltk.download('punkt')" 来下载
    NLTK 所需的数据和模型，以及通过 python -m spacy download en 来下载 spaCy 所需的数据和模型。你还可以通过 Google
    Colab ([http://realworldnlpbook.com/ch3.html#tokeni zation](http://realworldnlpbook.com/ch3.html#tokenization))
    在不安装任何Python环境或依赖项的情况下运行本节中的所有示例。
- en: 'To use the default word and sentence tokenizers from NLTK, you can import them
    from nltk.tokenize package as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用NLTK的默认单词和句子分词器，你可以从nltk.tokenize包中导入它们，如下所示：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can call these methods with a string, and they return a list of words or
    sentences as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用一个字符串调用这些方法，它们会返回一个单词或句子的列表，如下所示：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: NLTK implements a wide range of tokenizers in addition to the default one we
    used here. Its documentation page ([https://www.nltk.org/api/nltk.tokenize.html](https://www.nltk.org/api/nltk.tokenize.html))
    is a good starting point if you are interested in exploring more options.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 实现了除我们在此使用的默认方法之外的各种分词器。如果你有兴趣探索更多选项，可以参考其文档页面 ([https://www.nltk.org/api/nltk.tokenize.html](https://www.nltk.org/api/nltk.tokenize.html))。
- en: 'You can tokenize words and sentences as follows using spaCy:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用spaCy如下进行单词和句子的分词：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the results from NLTK and spaCy are slightly different. For example,
    spaCy’s word tokenizer leaves newlines ('\n') intact. The behavior of tokenizers
    differs from one implementation to another, and there is no single standard solution
    that every NLP practitioner agrees upon. Although standard libraries such as NLTK
    and spaCy give a good baseline, be ready to experiment depending on your task
    and data. Also, if you are dealing with languages other than English, your options
    may vary (and might be quite limited depending on the language). If you are familiar
    with the Java ecosystem, Stanford CoreNLP ([https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/))
    is another good NLP framework worth checking out.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，NLTK和spaCy的结果略有不同。例如，spaCy的单词分词器保留换行符（'\n'）。标记器的行为因实现而异，并且没有每个自然语言处理从业者都同意的单一标准解决方案。尽管标准库（如NLTK和spaCy）提供了一个良好的基线，但根据您的任务和数据，准备好进行实验。此外，如果您处理的是英语以外的语言，则您的选择可能会有所不同（并且可能会根据语言而有所限制）。如果您熟悉Java生态系统，Stanford
    CoreNLP（[https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/)）是另一个值得一试的良好自然语言处理框架。
- en: Finally, an increasingly popular and important tokenization method for neural
    network-based NLP models is *byte-pair encoding* (BPE). Byte-pair encoding is
    a purely statistical technique to split text into sequences of characters in any
    language, relying not on heuristic rules (such as spaces and punctuations) but
    only on character statistics from the dataset. We’ll study byte-pair encoding
    more in depth in chapter 10.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，用于基于神经网络的自然语言处理模型的一个日益流行和重要的标记化方法是*字节对编码*（BPE）。字节对编码是一种纯统计技术，将文本分割成任何语言的字符序列，不依赖于启发式规则（如空格和标点符号），而只依赖于数据集中的字符统计信息。我们将在第10章更深入地学习字节对编码。
- en: 3.3.2 Stemming
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 词干提取
- en: '*Stemming* is a process for identifying word stems. A *word stem* is the main
    part of a word after stripping off its affixes (prefixes and suffixes). For example,
    the word stem of “apples” (plural) is “apple.” The stem of “meets” (with a third-person
    singular s) is “meet.” The word stem of “unbelievable” is “believe.” It is often
    a part that remains unchanged after inflection.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*词干提取*是识别词干的过程。*词干*是在去除其词缀（前缀和后缀）后的单词的主要部分。例如，“apples”（复数）的词干是“apple”。具有第三人称单数s的“meets”的词干是“meet”。“unbelievable”的词干是“believe”。它通常是在屈折后保持不变的一部分。'
- en: Stemming—that is, normalizing words to something closer to their original forms—has
    great benefits in many NLP applications. In search, for example, you can improve
    the chance of retrieving relevant documents if you index documents using word
    stems instead of words. In many feature-based NLP pipelines, you’d be able to
    alleviate the OOV (out-of-vocabulary) problem by dealing with word stems instead.
    For example, even if your dictionary doesn’t have an entry for “apples,” you can
    instead use its stem “apple” as a proxy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取——即将单词规范化为更接近其原始形式的东西——在许多自然语言处理应用中具有巨大的好处。例如，在搜索中，如果使用单词干而不是单词对文档进行索引，可以提高检索相关文档的机会。在许多基于特征的自然语言处理流水线中，您可以通过处理单词干来减轻OOV（词汇外）问题。例如，即使您的字典中没有“apples”的条目，您也可以使用其词干“apple”作为代理。
- en: 'The most popular algorithm used for stemming English words is called the *Porter
    stemming algorithm*, originally written by Martin Porter. It consists of a number
    of rules for rewriting affixes (e.g., if a word ends with “-ization,” change it
    to “-ize”). NLTK implements a version of the algorithm as the PorterStemmer class,
    which can be used as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 用于提取英语单词词干的最流行算法称为*波特词干提取算法*，最初由马丁·波特编写。它由一系列用于重写词缀的规则组成（例如，如果一个词以“-ization”结尾，将其更改为“-ize”）。NLTK实现了该算法的一个版本，称为PorterStemmer类，可以如下使用：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Stemming is not without its limitations. In many cases, it can be too aggressive.
    For example, as you can see from the previous example, the Porter stemming algorithm
    changes both “colonizer” and “colonize” to just “colon.” I can’t imagine many
    applications would be happy to treat those three words as an identical entry.
    Also, many stemming algorithms do not consider the context or even the parts of
    speech. In the previous example, “meetings” is changed to “meet,” but you could
    argue that “meetings” as a plural noun should be stemmed to “meeting,” not “meet.”
    For those reasons, as of today, few NLP applications use stemming.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取并非没有局限性。在许多情况下，它可能过于激进。例如，你可以从上一个例子中看到，波特词干提取算法将“colonizer”和“colonize”都改为了“colon”。我无法想象有多少应用会愿意将这三个词视为相同的条目。此外，许多词干提取算法不考虑上下文甚至词性。在前面的例子中，“meetings”
    被改为 “meet”，但你可以争辩说 “meetings” 作为复数名词应该被提取为 “meeting”，而不是 “meet”。因此，截至今天，很少有自然语言处理应用使用词干提取。
- en: 3.3.3 Lemmatization
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 词形还原
- en: A lemma is the original form of a word that you often find as a head word in
    a dictionary. It is also the base form of the word before inflection. For example,
    the lemma of “meetings” (as a plural noun) is “meeting.” The lemma of “met” (a
    verb past form) is “meet.” Notice that it differs from stemming, which simply
    strips off affixes from a word and cannot deal with such irregular verbs and nouns.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 词元是单词的原始形式，通常在字典中作为首字母出现。它也是词在屈折之前的基本形式。例如，“meetings”（作为复数名词）的词元是“meeting”。
    “met”（动词过去式）的词元是“meet”。注意它与词干提取的区别，词干提取仅仅是从单词中剥离词缀，无法处理此类不规则的动词和名词。
- en: 'It is straightforward to run lemmatization using NLTK, as shown here:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NLTK 运行词形还原很简单，如下所示：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And the spaCy code looks like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的代码看起来像这样：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that lemmatization inherently requires that you know the part of speech
    of the input word, because the lemma depends on it. For example, “meeting” as
    a noun should be lemmatized to “meeting,” whereas the result should be “meet”
    if it’s a verb. WordNetLemmatizer in NLTK treats everything as a noun by default,
    which is why you see many unlemmatized words in the result (“agreed,” “owned,”
    etc.). On the other hand, spaCy automatically infers parts of speech from the
    word form and the context, which is why most of the lemmatized words are correct
    in its result. Lemmatization is more resource-intensive than stemming because
    it requires statistical analysis of the input and/or some form of linguistic resources
    such as dictionaries, but it has a wider range of applications in NLP due to its
    linguistic correctness.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，词形还原固有地要求您知道输入单词的词性，因为词形还原取决于它。例如，“meeting” 作为名词应该被还原为 “meeting”，而如果是动词，结果应该是
    “meet”。NLTK 中的 WordNetLemmatizer 默认将所有内容视为名词，这就是为什么在结果中看到许多未还原的单词（“agreed”，“owned”
    等）。另一方面，spaCy 可以从单词形式和上下文中自动推断词性，这就是为什么大多数还原的单词在其结果中是正确的。词形还原比词干提取更加耗费资源，因为它需要对输入进行统计分析和/或某种形式的语言资源（如字典），但由于其语言正确性，它在自然语言处理中有更广泛的应用。
- en: 3.4 Skip-gram and continuous bag of words (CBOW)
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 Skip-gram 和 连续词袋（CBOW）
- en: In previous sections, I explained what word embeddings are and how they are
    used in NLP applications. In this section, we’ll start exploring how to calculate
    word embeddings from large textual data using two popular algorithms—Skip-gram
    and CBOW.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我解释了词嵌入是什么以及它们如何在自然语言处理应用中使用。在这一节中，我们将开始探索如何使用两种流行的算法——Skip-gram 和 CBOW——从大量文本数据中计算词嵌入。
- en: 3.4.1 Where word embeddings come from
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 词嵌入来自何处
- en: 'In section 3.1, I explained that word embeddings represent each word in the
    vocabulary using a single-dimensional array of float numbers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3.1 节中，我解释了词嵌入是如何用单个浮点数数组表示词汇表中的每个单词的：
- en: vec("cat") = [0.7, 0.5, 0.1]
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("cat") = [0.7, 0.5, 0.1]
- en: vec("dog") = [0.8, 0.3, 0.1]
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("dog") = [0.8, 0.3, 0.1]
- en: vec("pizza") = [0.1, 0.2, 0.8]
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec("pizza") = [0.1, 0.2, 0.8]
- en: Now, there’s one important piece of information missing from the discussion
    so far. Where do those numbers come from? Do we hire a group of experts and have
    them come up with those numbers? It would be virtually impossible to assign them
    by hand. Hundreds of thousands of unique words exist in a typical large corpus,
    and the arrays should be at least around 100-dimensional long to be effective,
    which means you need to tweak more than tens of millions of numbers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到目前为止的讨论中缺少一条重要信息。这些数字从哪里来？我们雇用一群专家让他们提出这些数字吗？这几乎是不可能的手动分配它们。在典型的大型语料库中存在数十万个唯一的词，而数组应该至少长达100维才能有效，这意味着你需要调整超过数千万个数字。
- en: More importantly, what should those numbers look like? How do you determine
    whether you should assign a 0.8 to the first element of the “dog” vector, or 0.7,
    or any other number?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，这些数字应该是什么样子的？你如何确定是否应该为“狗”向量的第一个元素分配0.8，还是0.7，或者其他任何数字？
- en: The answer is that those numbers are also trained using a training dataset and
    a machine learning model like any other model in this book. In what follows, I’ll
    introduce and implement one of the most popular models to train word embeddings—the
    Skip-gram model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，这些数字也是使用训练数据集和像本书中的任何其他模型一样的机器学习模型进行训练的。接下来，我将介绍并实现训练词嵌入最流行的模型之一——Skip-gram模型。
- en: 3.4.2 Using word associations
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 使用词语关联
- en: First, let’s step back and think how humans learn concepts such as “a dog.”
    I don’t think any of you have ever been explicitly taught what a dog is. You knew
    this thing called “dog” since you were a toddler without anyone else telling you,
    “Oh by the way, there’s this thing called ‘dog’ in this world. It’s a four-legged
    animal that barks.” How is this possible? You acquire the concept through a large
    amount of physical (touching and smelling dogs), cognitive (seeing and hearing
    dogs), and linguistic (reading and hearing about dogs) interactions with the external
    world.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们退一步思考人类是如何学习“一只狗”等概念的。我不认为你们中的任何人曾经被明确告知过什么是狗。自从你还是一个蹒跚学步的孩童时，你就知道了这个叫做“狗”的东西，而没有任何其他人告诉你，“哦，顺便说一下，这个世界上有一种叫‘狗’的东西。它是一种四条腿的动物，会叫。”这是怎么可能的呢？你通过大量与外部世界的物理（触摸和闻到狗）、认知（看到和听到狗）和语言（阅读和听到有关狗的信息）的互动获得了这个概念。
- en: Now let’s think about what it takes to teach the concept of “dog” to a computer.
    Can we get a computer to “experience” interactions with the external world related
    to the concept of a dog? Although typical computers cannot move around and have
    interactions with actual dogs (well, not yet, as of this writing), one possible
    way to do this without teaching the computer what “dog” means is to use *association*
    relative to other words. For example, what words tend to appear together with
    the word “dog” if you look at its appearances in a large text corpus? “Pet,” “tail,”
    “smell,” “bark,” “puppy”—there can be countless options. How about “cat”? Maybe
    “pet,” “tail,” “fur,” “meow,” “kitten,” and so on. Because “dog” and “cat” have
    a lot in common conceptually (they are both popular pet animals with a tail, etc.),
    these two sets of context words also have large overlap. In other words, you can
    guess how close two words are to each other by looking at what other words appear
    in the same context. This is called the *distributional hypothesis*, and it has
    a long history in NLP.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们想一想教计算机“狗”这个概念需要什么条件。我们能让计算机“体验”与与“狗”概念相关的外部世界的互动吗？尽管典型的计算机不能四处移动并与实际狗有互动（截至目前为止，写作本文时还不能），但不教计算机“狗”是有可能的一种方法是使用与其他单词的*关联*。例如，如果你查看大型文本语料库中“狗”的出现，哪些词倾向于与之一起出现？“宠物”、“尾巴”、“气味”、“吠叫”、“小狗”——可能有无数个选项。那“猫”呢？也许是“宠物”、“尾巴”、“毛皮”、“喵喵叫”、“小猫”等等。因为“狗”和“猫”在概念上有很多共同之处（它们都是流行的宠物动物，有尾巴等），所以这两组上下文词也有很大的重叠。换句话说，你可以通过查看同一上下文中出现的其他单词来猜测两个单词彼此之间有多接近。这被称为*分布假设*，在自然语言处理中有着悠久的历史。
- en: NOTE There’s a related term used in artificial intelligence—*distributed representations*.
    Distributed representations of words are simply another name for word embeddings.
    Yes, it’s confusing, but both terms are commonly used in NLP.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在人工智能中有一个相关术语——*分布式表示*。单词的分布式表示简单地是词嵌入的另一个名称。是的，这很令人困惑，但这两个术语在自然语言处理中都是常用的。
- en: 'We are now one step closer. If two words have a lot of context words in common,
    we can give similar vectors to those two words. You can think of a word vector
    as a “compressed” representation of its context words. Then the question becomes:
    how can you “decompress” a word vector to obtain its context words? How can you
    even represent a set of context words mathematically? Conceptually, we’d like
    to come up with a model that does something like the one in figure 3.4.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经更近了一步。如果两个单词有很多上下文单词是共同的，我们可以给这两个单词分配相似的向量。你可以将一个单词向量看作是其上下文单词的“压缩”表示。那么问题就变成了：如何“解压缩”一个单词向量以获取其上下文单词？如何甚至在数学上表示一组上下文单词？从概念上讲，我们希望设计一个模型，类似于图
    3.4 中的模型。
- en: '![CH03_F04_Hagiwara](../Images/CH03_F04_Hagiwara.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F04_Hagiwara](../Images/CH03_F04_Hagiwara.png)'
- en: Figure 3.4 Decompressing a word vector
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 解压缩一个单词向量
- en: 'One way to represent a set of words mathematically is to assign a score to
    each word in the vocabulary. Instead of representing context words as a set, we
    can think of them as an associative array (dict in Python) from words to their
    “scores” that correspond to how related each word is to “dog,” as shown next:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表示一组单词的一种数学方法是为词汇表中的每个单词分配一个分数。我们可以将上下文单词表示为一个关联数组（在 Python 中为字典），从单词到它们与“dog”相关程度的“分数”，如下所示：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The only remaining piece of the model is how to come up with those “scores.”
    If you sort this list by word IDs (which may be assigned alphabetically), the
    scores can be conveniently represented by an *N*-dimensional vector, where *N*
    is the size of the entire vocabulary (the number of unique context words we consider),
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的唯一剩下的部分是如何生成这些“分数”。如果你按单词 ID（可能按字母顺序分配）对这个列表进行排序，那么这些分数可以方便地由一个 *N* 维向量表示，其中
    *N* 是整个词汇表的大小（我们考虑的唯一上下文单词的数量），如下所示：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: All the “decompressor” needs to do is expand the word-embedding vector (which
    has three dimensions) to another vector of *N* dimensions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: “解压缩器”唯一需要做的就是将单词嵌入向量（具有三个维度）扩展到另一个 *N* 维向量。
- en: This may sound very familiar to some of you—yes, it’s exactly what linear layers
    (aka fully connected layers) do. I briefly talked about linear layers in section
    2.4.2, but this is a perfect time to go deeper into what they really do.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能对你们中的一些人听起来非常熟悉 — 是的，这正是线性层（也称为全连接层）所做的。我在第 2.4.2 节简要讨论了线性层，但现在正是深入了解它们真正作用的好时机。
- en: 3.4.3 Linear layers
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 线性层
- en: 'Linear layers transform a vector into another vector in a linear fashion, but
    how exactly do they do this? Before talking about vectors, let’s simplify and
    start with numbers. How would you write a function (say, a method in Python) that
    transforms a number into another one in a linear fashion? Remember, being linear
    means the output always changes by a fixed amount (say, w) if you change the input
    by 1, no matter what the value of the input is. For example, 2.0 * x is a linear
    function, because the value always increases by 2.0 if you increase x by 1, no
    matter what value x is. You can write a general version of such a function as
    follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层以线性方式将一个向量转换为另一个向量，但它们究竟是如何做到这一点的呢？在讨论向量之前，让我们简化并从数字开始。你如何编写一个函数（比如说，在 Python
    中的一个方法），以线性方式将一个数字转换为另一个数字？记住，线性意味着如果你将输入增加 1，输出总是以固定量（比如说，w）增加，无论输入的值是多少。例如，2.0
    * x 是一个线性函数，因为如果你将 x 增加 1，值始终增加 2.0，无论 x 的值是多少。你可以写一个这样的函数的一般版本，如下所示：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s now assume the parameters w and b are fixed and defined somewhere else.
    You can confirm that the output (the return value) always changes by w if you
    increase or decrease x by 1\. b is the value of the output when x = 0. It is called
    a *bias* in machine learning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设参数 w 和 b 是固定的，并在其他地方定义。你可以确认输出（返回值）在增加或减少 x 1 时始终会变化 w。当 x = 0 时，b 是输出的值。这在机器学习中称为*偏差*。
- en: 'Now, what if there are two input variables, say, x1 and x2? Can you still write
    a function that transforms two input variables into another number in a linear
    way? Yes, and there’s very little change required to do this, as shown next:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设有两个输入变量，比如，x1 和 x2？你是否仍然可以编写一个函数，将两个输入变量线性转换为另一个数字？是的，这样做所需的改变非常少，如下所示：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can confirm its linearity by checking that the output changes by w1 if you
    change x1 by 1, and the output changes by w2 if you change x2 by 1, regardless
    of the value of the other variable. Bias b is still the value of the output when
    x1 and x2 are both 0.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查输出变量的值来确认其线性性，如果你将x1增加1，输出变量的值将增加w1；如果你将x2增加1，输出变量的值将增加w2，而不管其他变量的值如何。偏差b仍然是当x1和x2都为0时的输出值。
- en: For example, assume we have w1 = 2.0, w2 = -1.0, and b = 1. For an input (1,
    1), the function returns 2\. If you increase x1 by 1 and give (2, 1) as the input,
    you’ll get 4, which is w1 more than 2\. If you increase x2 by 1 and give (1, 2)
    as the input, you’ll get 1, which is 1 less (or w2 more) than 2.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有w1 = 2.0，w2 = -1.0，b = 1。对于输入（1,1），该函数返回2。如果你增加x1 1并将（2,1）作为输入，你将得到4，比2多w1。如果你增加x2
    1并将（1,2）作为输入，你将得到1，比2少1（或比w2多）。
- en: 'At this point, we can start thinking about generalizing this to vectors. What
    if there are two output variables, say, y1 and y2? Can you still write a linear
    function with respect to the two inputs? Yes, you can simply duplicate the linear
    transformation twice, with different weights and biases, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以开始思考将其推广到向量。如果有两个输出变量，比如y1和y2，会怎么样？你能否仍然写出关于这两个输入的线性函数？是的，你可以简单地两次复制线性变换，使用不同的权重和偏差，如下所示：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: OK, it’s getting a little bit complicated, but you effectively wrote a function
    for a linear layer that transforms a two-dimensional vector into another two-dimensional
    vector! If you increase the input dimension (the number of input variables), this
    method would get horizontally long (i.e., more additions per line), whereas if
    you increase the output dimension, this method would get vertically long (i.e.,
    more lines).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，情况有点复杂，但你实际上编写了一个将二维向量转换为另一个二维向量的线性层函数！如果你增加输入维度（输入变量的数量），这个方法将变得水平更长（即每行增加更多的加法），而如果你增加输出维度，这个方法将变得垂直更长（即更多的行）。
- en: In practice, deep learning libraries and frameworks implement linear layers
    in a more efficient, generic way, and often most of the computation happens on
    a GPU. However, knowing how linear layers—the most important, simplest form of
    neural networks—work conceptually should be essential for understanding more complex
    neural network models.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，深度学习库和框架以一种更高效、通用的方式实现线性层，并且通常大部分计算发生在GPU上。然而，从概念上了解线性层——神经网络最重要、最简单的形式，对理解更复杂的神经网络模型应该是至关重要的。
- en: NOTE In AI literature, you may encounter the concept of *perceptrons*. A perceptron
    is a linear layer with only one output variable, applied to a classification problem.
    If you stack multiple linear layers (= perceptrons), you get a *multilayer perceptron*,
    which is basically another name for a feedforward neural network with some specific
    structures.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在人工智能文献中，你可能会遇到*感知器*的概念。感知器是一个只有一个输出变量的线性层，应用于分类问题。如果你堆叠多个线性层（= 感知器），你就得到了*多层感知器*，这基本上是另一个称为具有一些特定结构的前馈神经网络。
- en: Finally, you may be wondering where the constants w and b you saw in this section
    come from. These are exactly the “magic constants” that I talked about in section
    2.4.1\. You adjust these constants so that the output of the linear layer (and
    the neural network as a whole) gets closer to what you want through a process
    called *optimization*. These magic constants are also called *parameters* of a
    machine learning model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能想知道本节中看到的常数w和b是从哪里来的。这些正是我在第2.4.1节中谈到的“魔法常数”。你调整这些常数，使线性层（以及整个神经网络）的输出通过*优化*过程更接近你想要的结果。这些魔法常数也被称为机器学习模型的*参数*。
- en: Putting this all together, the structure we want for the Skip-gram model is
    shown in figure 3.5\. This network is very simple. It takes a word embedding as
    an input and expands it via a linear layer to a set of scores, one for each context
    word. Hopefully this is not as intimidating as many people think!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 把所有这些放在一起，我们希望Skip-gram模型的结构如图3.5所示。这个网络非常简单。它以一个词嵌入作为输入，并通过一个线性层扩展到一组分数，每个上下文词一个。希望这不会像许多人想象的那样令人畏惧！
- en: '![CH03_F05_Hagiwara](../Images/CH03_F05_Hagiwara.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F05_Hagiwara](../Images/CH03_F05_Hagiwara.png)'
- en: Figure 3.5 Skip-gram model structure
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 Skip-gram模型结构
- en: 3.4.4 Softmax
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 Softmax
- en: Now let’s talk about how to “train” the Skip-gram model and learn the word embeddings
    we want. The key here is to turn this into a classification task, where the model
    predicts what words appear in the context. The “context” here simply means a window
    of a fixed size (e.g., 5 + 5 words on both sides) centered around the target word
    (e.g., “dog”). See figure 3.6 for an illustration when the window size is 2\.
    This is actually a “fake” task because we are not interested in the prediction
    of the model per se, but rather in the by-product (word embeddings) produced while
    training the model. In machine learning and NLP, we often make up a fake task
    to train something else as a by-product.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来讨论如何“训练”Skip-gram模型并学习我们想要的单词嵌入。关键在于将这个过程转化为一个分类任务，在这个任务中，模型预测哪些单词出现在上下文中。这里的“上下文”指的只是一个固定大小的窗口（例如，上下各5个单词），窗口以目标单词（例如，“狗”）为中心。当窗口大小为2时，请参见图3.6以便了解。实际上，这是一个“假”的任务，因为我们对模型的预测本身并不感兴趣，而是对训练模型时产生的副产品（单词嵌入）感兴趣。在机器学习和自然语言处理中，我们经常虚构一个假任务来训练另一些东西作为副产品。
- en: '![CH03_F06_Hagiwara](../Images/CH03_F06_Hagiwara.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_Hagiwara](../Images/CH03_F06_Hagiwara.png)'
- en: Figure 3.6 Target word and context words (when window size = 2)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 目标单词和上下文单词（窗口大小=2时）
- en: NOTE This setting of machine learning, where the training labels are automatically
    created from a given dataset, can also be called *self-supervised learning*. Recent
    popular techniques, such as word embeddings and language modeling, all use self-supervision.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 这种机器学习设置，其中训练标签是从给定数据集自动生成的，也可以称为*自监督学习*。最近流行的技术，如单词嵌入和语言建模，都使用了自监督学习。
- en: 'It is relatively easy to make a neural network solve a classification task.
    You need to do the following two things:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使神经网络解决分类任务相对容易。你需要做以下两件事情：
- en: Modify the network so that it produces a probability distribution.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改网络，以便产生概率分布。
- en: Use cross entropy as the loss function (we’ll cover this in detail shortly).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉熵作为损失函数（我们马上会详细介绍这一点）。
- en: You can use something called *softmax* to do the first. Softmax is a function
    that turns a vector of *K* float numbers to a probability distribution, by first
    “squashing” the numbers so that they fit a range between 0.0-1.0, and then normalizing
    them so that the sum equals 1\. If you are not familiar with the concept of probabilities,
    replace them with *confidence*. A probability distribution is a set of confidence
    values that the network places on individual predictions (in this case, context
    words). Softmax does all this while preserving the relative ordering of the input
    float numbers, so large input numbers still have large probability values in the
    output distribution. Figure 3.7 illustrates this conceptually.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用称为*softmax*的东西来进行第一个。Softmax是一个函数，它将*K*个浮点数向量转化为概率分布，首先“压缩”这些数字，使其适合0.0-1.0的范围，然后将它们归一化，使其总和等于1。如果你对概率的概念不熟悉，请将其替换为*置信度*。概率分布是网络对个别预测（在这种情况下，上下文单词）的置信度值的集合。Softmax在保持输入浮点数的相对顺序的同时执行所有这些操作，因此大的输入数字仍然在输出分布中具有大的概率值。图3.7以概念上的方式说明了这一点。
- en: '![CH03_F07_Hagiwara](../Images/CH03_F07_Hagiwara.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_Hagiwara](../Images/CH03_F07_Hagiwara.png)'
- en: Figure 3.7 Softmax
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 软最大值
- en: 'Another component required to turn a neural network into a classifier is *cross
    entropy*. Cross entropy is a loss function used to measure the distance between
    two probability distributions. It returns zero if two distributions match exactly
    and higher values if the two diverge. For classification tasks, we use cross entropy
    to compare the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络转化为分类器所需的另一个组件是*交叉熵*。交叉熵是一种用于衡量两个概率分布之间距离的损失函数。如果两个分布完全匹配，则返回零，如果两个分布不同，则返回较高的值。对于分类任务，我们使用交叉熵来比较以下内容：
- en: Predicted probability distribution produced by the neural network (output of
    softmax)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络产生的预测概率分布（softmax的输出）
- en: Target probability distribution, where the probability of the correct class
    is 1.0 and everything else is 0.0
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标概率分布，其中正确类别的概率为1.0，其他的都是0.0
- en: The predictions made by the Skip-gram model get closer and closer to the actual
    context words, and word embeddings are learned at the same time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram模型的预测逐渐接近实际的上下文词，同时学习单词嵌入。
- en: 3.4.5 Implementing Skip-gram on AllenNLP
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在AllenNLP上实现Skip-gram
- en: 'It is relatively straightforward to turn this model into working code using
    AllenNLP. Note that all the code listed in this section can be executed on the
    Google Colab notebook ([http://realworldnlpbook.com/ch3.html#word2vec-nb](http://realworldnlpbook.com/ch3.html#word2vec-nb)).
    First, you need to implement a dataset reader that reads a plain text corpus and
    turns it into a set of instances that can be consumed by the Skip-gram model.
    The details of the dataset reader are not critical to the discussion here, so
    I’m going to omit the full code listing. You can clone the code repository of
    this book ([https://github.com/mhagiwara/realworldnlp](https://github.com/mhagiwara/realworldnlp))
    and import it as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AllenNLP 将这个模型转化为可工作的代码相对直接。请注意，本节中列出的所有代码都可以在 Google Colab 笔记本上执行（[http://realworldnlpbook.com/ch3.html#word2vec-nb](http://realworldnlpbook.com/ch3.html#word2vec-nb)）。首先，你需要实现一个数据集读取器，该读取器将读取一个纯文本语料库，并将其转换为
    Skip-gram 模型可以使用的一组实例。数据集读取器的详细信息对于本讨论并不关键，因此我将省略完整的代码清单。你可以克隆本书的代码仓库（[https://github.com/mhagiwara/realworldnlp](https://github.com/mhagiwara/realworldnlp)），并按照以下方式导入它：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Alternatively, if you are interested, you can see the full code from [http://realworldnlpbook.com/ch3.html#word2vec](http://realworldnlpbook.com/ch3.html#word2vec).
    You can use the reader as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你有兴趣，你可以从 [http://realworldnlpbook.com/ch3.html#word2vec](http://realworldnlpbook.com/ch3.html#word2vec)
    查看完整的代码。你可以按照以下方式使用读取器：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Also, be sure to import all the necessary modules and define some constants
    in this example, as shown next:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这个示例中，请确保导入所有必要的模块并定义一些常量，如下所示：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are going to use the text8 ([http://mattmahoney.net/dc/textdata](http://mattmahoney.net/dc/textdata))
    dataset in this example. The dataset is an excerpt from Wikipedia and is often
    used for training toy word embedding and language models. You can iterate over
    the instances in the dataset. token_in is the input token to the model, and token_out
    is the output (the context word):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 text8（[http://mattmahoney.net/dc/textdata](http://mattmahoney.net/dc/textdata)）数据集。该数据集是维基百科的一部分，经常用于训练玩具词嵌入和语言模型。你可以迭代数据集中的实例。token_in
    是模型的输入标记，token_out 是输出（上下文词）：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, you can build the vocabulary, as we did in chapter 2, as shown next:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以构建词汇表，就像我们在第 2 章中所做的那样，如下所示：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that we are using min_count, which sets the lower bound on the number
    of occurrences for each token. Also, let’s define the data loader we use for training
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了 min_count 参数，该参数设置了每个 token 出现的最低限制。此外，让我们按照以下方式定义用于训练的数据加载器：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s then define an Embedding object that holds all the word embeddings we’d
    like to learn:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个包含所有要学习的词嵌入的嵌入对象：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, EMBEDDING_DIM is the length of each word vector (number of float numbers).
    A typical NLP application uses word vectors of a couple hundred dimensions long
    (in this example, 256), but this value depends greatly on the task and the datasets.
    It is often suggested that you use longer word vectors as your training data grows.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，EMBEDDING_DIM 是每个单词向量的长度（浮点数的数量）。一个典型的 NLP 应用程序使用几百维的单词向量（在本例中为 256），但该值在任务和数据集上有很大的依赖性。通常建议随着训练数据的增长使用更长的单词向量。
- en: Finally, you need to implement the body of the Skip-gram model, as shown next.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要实现 Skip-gram 模型的主体，如下所示。
- en: Listing 3.1 Skip-gram model implemented in AllenNLP
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 3.1 在 AllenNLP 中实现的 Skip-gram 模型。
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ AllenNLP requires every model to be inherited from Model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ AllenNLP 要求每个模型都必须继承自 Model。
- en: ❷ The embedding object is passed from outside rather than defined inside.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 嵌入对象从外部传入，而不是在内部定义。
- en: ❸ This creates a linear layer (note that we don’t need biases).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这创建了一个线性层（请注意我们不需要偏置）。
- en: ❹ The body of neural network computation is implemented in forward().
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 神经网络计算的主体在 forward() 中实现。
- en: ❺ Converts input tensors (word IDs) to word embeddings
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将输入张量（单词 ID）转换为单词嵌入。
- en: ❻ Applies the linear layer
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 应用线性层。
- en: ❼ Computes the loss
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算损失。
- en: 'A few things to note:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下几点：
- en: AllenNLP requires every model to be inherited from Model, which can be imported
    from allennlp.models.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AllenNLP 要求每个模型都必须继承自 allennlp.models.Model。
- en: Model’s initializer (__init__) takes a Vocabulary instance and any other parameters
    or submodels defined externally. It also defines any internal parameters or models.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的初始化函数（__init__）接受一个 Vocabulary 实例和定义在外部的任何其他参数或子模型。它还定义任何内部的参数或模型。
- en: The main computation of the model is defined in forward(). It takes all the
    fields from instances (in this example, token_in and token_out) as tensors (multidimensional
    arrays) and returns a dict that contains the 'loss' key, which will be used by
    the optimizer to train the model.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的主要计算是在forward()中定义的。它将实例中的所有字段（在这个例子中是token_in和token_out）作为张量（多维数组），并返回一个包含'loss'键的字典，该键将被优化器用于训练模型。
- en: You can train this model using the following code.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码来训练这个模型。
- en: Listing 3.2 Code for training the Skip-gram model
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 训练Skip-gram模型的代码
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Training takes a while, so I recommend truncating the training data first, say,
    by using only the first one million tokens. You can do this by inserting text8
    = list(text8) [:1000000] after reader.read(). After the training is finished,
    you can get related words (words with the same meanings) using the method shown
    in listing 3.3\. This method first obtains the word vector for a given word (token),
    then computes how similar it is to every other word vector in the vocabulary.
    The similarity is calculated using something called the *cosine similarity*. In
    simple terms, the cosine similarity is the opposite of the angle between two vectors.
    If two vectors are identical, the angle between them is zero, and the similarity
    will be 1, which is the largest possible value. If two vectors are perpendicular,
    the angle is 90 degrees, and the cosine will be 0\. If the vectors are in totally
    opposite directions, the cosine will be -1.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 训练需要一段时间，所以我建议首先截取训练数据，比如只使用前一百万个标记。你可以在reader.read()后插入text8 = list(text8)[:1000000]。训练结束后，你可以使用列表3.3中展示的方法获取相关词（具有相同意义的词）。这个方法首先获取给定词（标记）的词向量，然后计算它与词汇表中的每个其他词向量的相似度。相似性是使用所谓的*余弦相似度*来计算的。简单来说，余弦相似度是两个向量之间角度的反义词。如果两个向量相同，那么它们之间的角度就是零，相似度就是1，这是可能的最大值。如果两个向量是垂直的，角度是90度，余弦相似度就是0。如果向量完全相反，余弦相似度就是-1。
- en: Listing 3.3 Method to obtain related words using word embeddings
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 使用词嵌入获取相关词的方法
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you run this for words “one” and “december,” you get the lists of related
    words shown in table 3.1\. Although you can see some words that are not related
    to the query word, overall, the results look good.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对“one”和“december”这两个词运行这个模型，你将得到表3.1中展示的相关词列表。虽然你可能会看到一些与查询词无关的词，但整体上，结果看起来很不错。
- en: Table 3.1 Related words for “one” and “december"
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 “one” 和 “december”的相关词
- en: '| “one” | “december” |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| “一” | “十二月” |'
- en: '| one | december |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 一 | 十二月 |'
- en: '| nine | january |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 九 | 一月 |'
- en: '| eight | nixus |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 八 | 尼克斯 |'
- en: '| six | londini |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 六 | 伦敦 |'
- en: '| five | plantarum |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 五 | 植物 |'
- en: '| seven | june |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 七 | 六月 |'
- en: '| three | smissen |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 三 | 斯密森 |'
- en: '| four | february |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 四 | 二月 |'
- en: '| d | qanuni |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| d | 卡努尼 |'
- en: '| actress | october |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 女演员 | 十月 |'
- en: 'One final note: you need to implement a couple of techniques if you want to
    use Skip-gram to train high-quality word vectors in practice, namely, negative
    sampling and subsampling of high-frequency words. Although they are important
    concepts, they can be a distraction if you are just starting out and would like
    to learn the basics of NLP. If you are interested in learning more, check out
    this blog post that I wrote on this topic: [http://realworldnlpbook.com/ch3.html#word2vec-blog](http://realworldnlpbook.com/ch3.html#word2vec-blog).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点说明：如果你想要在实践中使用Skip-gram来训练高质量的词向量，你需要实现一些技术，即负采样和高频词子采样。虽然它们是重要的概念，但如果你刚开始学习，并想了解自然语言处理的基础知识，它们可能会分散你的注意力。如果你对了解更多感兴趣，请查看我在这个主题上写的这篇博客文章：[http://realworldnlpbook.com/ch3.html#word2vec-blog](http://realworldnlpbook.com/ch3.html#word2vec-blog)。
- en: 3.4.6 Continuous bag of words (CBOW) model
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.6 连续词袋（CBOW）模型
- en: Another word-embedding model that is often mentioned along with the Skip-gram
    model is the *continuous bag of words* (CBOW) *model*. As a close sibling of the
    Skip- gram model, proposed at the same time ([http://realworldnlpbook.com/ch3.html#
    mikolov13](http://realworldnlpbook.com/ch3.html#mikolov13)), the architecture
    of the CBOW model looks similar to that of the Skip-gram model but flipped upside
    down. The “fake” task the model is trying to solve is to predict the target word
    from a set of its context words. This is also similar to fill-in-the-blank type
    of questions. For example, if you see a sentence “I heard a ___ barking in the
    distance,” most of you can probably guess the answer “dog” instantly. Figure 3.8
    shows the structure of this model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通常与 Skip-gram 模型一起提到的另一个词嵌入模型是*连续词袋*（CBOW）*模型*。作为 Skip-gram 模型的近亲，同时提出（[http://realworldnlpbook.com/ch3.html#
    mikolov13](http://realworldnlpbook.com/ch3.html#mikolov13)），CBOW 模型的结构与 Skip-gram
    模型相似，但上下颠倒。该模型试图解决的“假”任务是从一组上下文词预测目标词。这与填空题类型的问题类似。例如，如果你看到一个句子“I heard a ___
    barking in the distance.”，大多数人可能会立即猜到答案“dog”。图 3.8 显示了该模型的结构。
- en: '![CH03_F08_Hagiwara](../Images/CH03_F08_Hagiwara.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Hagiwara](../Images/CH03_F08_Hagiwara.png)'
- en: Figure 3.8 Continuous bag of words (CBOW) model
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 连续词袋 (CBOW) 模型
- en: I’m not going to implement the CBOW model from scratch here for a couple of
    reasons. It should be straightforward to implement if you understand the Skip-gram
    model. Also, the accuracy of the CBOW model measured on word semantic tasks is
    usually slightly lower than that of Skip-gram, and CBOW is less often used in
    NLP than Skip-gram. Both models are implemented in the original Word2vec ([https://code
    .google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/))
    toolkit, if you want to try them yourself, although the vanilla Skip-gram and
    CBOW models are less and less often used nowadays because of the advent of more
    recent, powerful word-embedding models (such as GloVe and fastText) that are covered
    in the rest of this chapter.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我不打算在这里从头实现 CBOW 模型，有几个原因。如果你理解了 Skip-gram 模型，实现 CBOW 模型应该很简单。此外，CBOW 模型在词语语义任务上的准确性通常略低于
    Skip-gram 模型，并且 CBOW 在 NLP 中使用的频率也较低。这两个模型都在原始的 Word2vec ([https://code .google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/))
    工具包中实现，如果你想自己尝试它们，尽管如此，由于更近期、更强大的词嵌入模型的出现（例如 GloVe 和 fastText），这些基本的 Skip-gram
    和 CBOW 模型现在使用得越来越少，这些模型在本章的其余部分中介绍。
- en: 3.5 GloVe
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 GloVe
- en: In the previous section, I implemented Skip-gram and showed how you can train
    your word embeddings using large text data. But what if you wanted to build your
    own NLP applications leveraging high-quality word embeddings while skipping all
    the hassle? What if you couldn’t afford the computation and data required to train
    word embeddings?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我实现了 Skip-gram，并展示了如何利用大量文本数据训练词嵌入。但是如果你想构建自己的 NLP 应用程序，利用高质量的词嵌入，同时避开所有麻烦呢？如果你无法承担训练词嵌入所需的计算和数据呢？
- en: Instead of training word embeddings, you can always download pretrained word
    embeddings published by somebody else, which many NLP practitioners do. In this
    section, I’m going to introduce another popular word-embedding model—*GloVe*,
    named after *Global Vectors*. Pretrained word embeddings generated by GloVe are
    probably the most widely used embeddings in NLP applications today.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练词嵌入相反，你总是可以下载其他人发布的预训练词嵌入，这是许多 NLP 从业者做的。在本节中，我将介绍另一种流行的词嵌入模型——*GloVe*，名为
    *Global Vectors*。由 GloVe 生成的预训练词嵌入可能是当今 NLP 应用中最广泛使用的嵌入。
- en: 3.5.1 How GloVe learns word embeddings
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 GloVe 如何学习词嵌入
- en: The main difference between the two models described earlier and GloVe is that
    the former is *local*. To recap, Skip-gram uses a prediction task where a context
    word (“bark”) is predicted from the target word (“dog”). CBOW basically does the
    opposite of this. This process is repeated as many times as there are word tokens
    in the dataset. It basically scans the entire dataset and asks the question, “Can
    this word be predicted from this other word?” for every single occurrence of words
    in the dataset.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 之前描述的两个模型与 GloVe 的主要区别在于前者是*局部的*。简而言之，Skip-gram 使用预测任务，其中上下文词（“bark”）从目标词（“dog”）预测。CBOW
    基本上做相反的事情。这个过程重复了数据集中的每个单词标记的次数。它基本上扫描整个数据集，并询问：“这个词可以从另一个词预测吗？”对于数据集中每个单词的每次出现都会问这个问题。
- en: Let’s think how efficient this algorithm is. What if there were two or more
    identical sentences in the dataset? Or very similar sentences? In that case, Skip-gram
    would repeat the exact same set of updates multiple times. “Can ‘bark’ be predicted
    from ‘dog’?” you might ask. But chances are you already asked that exact same
    question a couple of hundred sentences ago. If you know that the words “dog” and
    “bark” appear together in the context *N* times in the entire dataset, why repeat
    this *N* times? It’s as if you were adding “1” *N* times to something else (x
    + 1 + 1 + 1 + ... + 1) when you could simply add *N* to it (x + N). Could we somehow
    use this *global* information directly?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下这个算法的效率。如果数据集中有两个或更多相同的句子呢？或者非常相似的句子呢？在这种情况下，Skip-gram 将重复多次相同的一组更新。你可能会问，“‘bark’
    可以从 ‘dog’ 预测吗？” 但很有可能你在几百个句子前已经问过了完全相同的问题。如果你知道 “dog” 和 “bark” 这两个词在整个数据集中共同出现了
    *N* 次，那为什么要重复这 *N* 次呢？这就好像你在把 “1” 加 *N* 次到另一个东西上（x + 1 + 1 + 1 + ... + 1），而你其实可以直接加
    *N* 到它上面（x + N）。我们能否直接利用这个 *全局* 信息呢？
- en: The design of GloVe is motivated by this insight. Instead of using local word
    co-occurrences, it uses aggregated word co-occurrence statistics in the entire
    dataset. Let’s say “dog” and “bark” co-occur *N* times in the dataset. I’m not
    going into the details of the model, but roughly speaking, the GloVe model tries
    to predict this number *N* from the embeddings of both words. Figure 3.9 illustrates
    this prediction task. It still makes some predictions about word relations, but
    notice that it makes one prediction per a combination of word *types*, but Skip-gram
    does so for every combination of word *tokens*!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 的设计受到这一见解的启发。它不是使用局部单词共现，而是使用整个数据集中的聚合单词共现统计信息。假设 “dog” 和 “bark” 在数据集中共同出现了
    *N* 次。我不会深入讨论模型的细节，但粗略地说，GloVe 模型试图从两个单词的嵌入中预测这个数字 *N*。图 3.9 描绘了这个预测任务。它仍然对单词关系进行了一些预测，但请注意，它对于每个单词
    *类型* 的组合进行了一次预测，而 Skip-gram 则对每个单词 *标记* 的组合进行了预测！
- en: '![CH03_F09_Hagiwara](../Images/CH03_F09_Hagiwara.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Hagiwara](../Images/CH03_F09_Hagiwara.png)'
- en: Figure 3.9 GloVe
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 GloVe
- en: Token and Type As mentioned in section 3.3.1, a *token* is an occurrence of
    a word in text. There may be multiple occurrences of the same word in a corpus.
    A *type*, on the other hand, is a distinctive, unique word. For example, in the
    sentence “A rose is a rose is a rose,” there are eight tokens but only three types
    (“a,” “rose,” and “is”). If you are familiar with object-oriented programming,
    they are roughly equivalent to instance and class. There can be multiple instances
    of a class, but there is only one class for a concept.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和类型 如第 3.3.1 节所述，*标记* 是文本中单词的出现。一个语料库中可能会有同一个单词的多次出现。另一方面，*类型* 是一个独特的、唯一的词。例如，在句子
    “A rose is a rose is a rose.” 中，有八个标记但只有三种类型（“a”，“rose”，和 “is”）。如果你熟悉面向对象编程，它们大致相当于实例和类。一个类可以有多个实例，但一个概念只有一个类。
- en: 3.5.2 Using pretrained GloVe vectors
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 使用预训练的 GloVe 向量
- en: In fact, not many NLP practitioners train GloVe vectors from scratch by themselves.
    More often, we download and use word embeddings, which are pretrained using large
    text corpora. This is not only quick but usually beneficial in making your NLP
    applications more accurate, because those pretrained word embeddings (often made
    public by the inventor of word-embedding algorithms) are usually trained using
    larger datasets and more computational power than most of us can afford. By using
    pretrained word embeddings, you can “stand on the shoulders of giants” and quickly
    leverage high-quality linguistic knowledge distilled from large text corpora.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，并不是很多自然语言处理（NLP）从业者自己从头开始训练 GloVe 向量。更常见的是，我们下载并使用预训练的词向量，这些词向量是使用大型文本语料库预训练的。这不仅快捷，而且通常有助于使您的
    NLP 应用程序更准确，因为这些预训练的词向量（通常由词向量算法的发明者公开）通常是使用比我们大多数人负担得起的更大的数据集和更多的计算资源进行训练的。通过使用预训练的词向量，您可以“站在巨人的肩膀上”，并快速利用从大型文本语料库中提炼出的高质量语言知识。
- en: In the rest of this section, let’s see how we can download and search for similar
    words using pretrained GloVe embeddings. First, you need to download the data
    file. The official GloVe website ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/))
    provides multiple word-embedding files trained using different datasets and vector
    sizes. You can pick any one you like (although the file size could be large, depending
    on which one you choose) and unzip it. In what follows, we assume you save it
    under the relative path data/glove/.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，让我们看看如何使用预先训练的GloVe嵌入下载并搜索相似单词。首先，您需要下载数据文件。官方的GloVe网站（[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)）提供了使用不同数据集和向量大小训练的多个词嵌入文件。您可以选择任何一个（尽管取决于您选择的文件大小可能很大）并解压缩它。在接下来的内容中，我们假设您将其保存在相对路径data/glove/下。
- en: 'Most word-embedding files are formatted in a similar way. Each line contains
    a word, followed by a sequence of numbers that correspond to its word vector.
    There are as many numbers as there are dimensions (in the GloVe files distributed
    on the website above, you can tell the dimensionality from the filename suffix
    in the form of xxxd). Each field is delimited by a space. Here is an excerpt from
    one of the GloVe word-embedding files:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数词嵌入文件都以相似的方式格式化。每一行都包含一个单词，后跟一系列与其单词向量对应的数字。数字的数量与维度一样多（在上述网站上分发的GloVe文件中，您可以从文件名后缀中以xxx维的形式了解维度）。每个字段由一个空格分隔。以下是GloVe词嵌入文件的摘录：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As we did in section 3.4.5, what we’d like to do is to take a query word (say,
    “dog”) and find its neighbors in the *N*-dimensional space. One way to do this
    is to calculate the similarity between the query word and every other word in
    the vocabulary and sort the words by their similarities, as shown in listing 3.3\.
    Depending on the size of the vocabulary, this approach could be very slow. It’s
    like linearly scanning an array to find an element instead of using binary search.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第3.4.5节中所做的那样，我们想要做的是接受一个查询词（比如，“狗”）并在*N*维空间中找到它的邻居。一种方法是计算查询词与词汇表中的每个其他词之间的相似性，并按其相似性对单词进行排序，如清单3.3所示。根据词汇表的大小，这种方法可能非常缓慢。这就像线性扫描数组以找到元素而不是使用二进制搜索一样。
- en: Instead, we’ll use approximate nearest neighbor algorithms to quickly search
    for similar words. In a nutshell, these algorithms enable us to quickly retrieve
    nearest neighbors without computing the similarity between every word pair. In
    particular, we’ll use Annoy ([https://github.com/spotify/annoy](https://github.com/spotify/annoy)),
    a library for approximate neighbor search released from Spotify. You can install
    it by running pip install annoy. It implements a popular approximate nearest neighbor
    algorithm called *locally sensitive hashing* (LSH) using random projection.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将使用近似最近邻算法快速搜索相似单词。简而言之，这些算法使我们能够快速检索最近的邻居，而无需计算每个单词对之间的相似性。具体而言，我们将使用Annoy（[https://github.com/spotify/annoy](https://github.com/spotify/annoy)）库，这是来自Spotify的用于近似最近邻搜索的库。您可以通过运行pip
    install annoy来安装它。它使用随机投影实现了一种流行的近似最近邻算法称为*局部敏感哈希*（LSH）。
- en: To use Annoy to search similar words, you first need to build an index, which
    can be done as shown in listing 3.4\. Note that we are also building a dict from
    word indices to words and saving it to a separate file to facilitate the word
    lookup later (listing 3.5).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Annoy搜索相似的单词，您首先需要构建一个索引，可以按照清单3.4所示进行。请注意，我们还正在构建一个从单词索引到单词的字典，并将其保存到单独的文件中，以便以后方便进行单词查找（清单3.5）。
- en: Listing 3.4 Building an Annoy index
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 清单3.4 构建Annoy索引
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Reading a GloVe embedding file and building an Annoy index can be quite slow,
    but once it’s built, accessing it and retrieving similar words can be performed
    very quickly. This configuration is similar to search engines, where an index
    is built to achieve near real-time retrieval of documents. This is suitable for
    applications where retrieval of similar items in real time is required but update
    of the dataset happens less frequently. Examples include search engines and recommendation
    engines.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 读取GloVe嵌入文件并构建Annoy索引可能会相当慢，但一旦构建完成，访问它并检索相似单词的速度可以非常快。这种配置类似于搜索引擎，其中构建索引以实现几乎实时检索文档。这适用于需要实时检索相似项但数据集更新频率较低的应用程序。示例包括搜索引擎和推荐引擎。
- en: Listing 3.5 Using an Annoy index to retrieve similar words
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 清单3.5 使用Annoy索引检索相似单词
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you run this for the words “dog” and “december,” you’ll get the lists of
    the 10 most-related words shown in table 3.2.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个对于单词“狗”和“十二月”，你将得到表3.2中显示的与这两个单词最相关的10个单词列表。
- en: Table 3.2 Related words for “dog” and “december"
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 “狗”和“十二月”的相关词
- en: '| “dog” | “december” |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| “狗” | “十二月” |'
- en: '| dog | december |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 狗 | 十二月 |'
- en: '| puppy | january |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 小狗 | 一月 |'
- en: '| cat | october |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 猫 | 十月 |'
- en: '| cats | november |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 猫 | 十一月 |'
- en: '| horse | september |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 马 | 九月 |'
- en: '| baby | february |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 婴儿 | 二月 |'
- en: '| bull | august |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 公牛 | 八月 |'
- en: '| kid | july |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 小孩 | 七月 |'
- en: '| kids | april |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 孩子 | 四月 |'
- en: '| monkey | march |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 猴子 | 三月 |'
- en: You can see that each list contains many related words to the query word. You
    see the identical words at the top of each list—this is because the cosine similarity
    of two identical vectors is always 1, its maximum possible value.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到每个列表中包含与查询单词相关的许多单词。你会在每个列表的顶部看到相同的单词——这是因为两个相同向量的余弦相似度总是1，它的最大可能值。
- en: 3.6 fastText
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 fastText
- en: In the previous section, we saw how to download pretrained word embeddings and
    retrieve related words. In this section, I’ll explain how to train word embeddings
    using your own text data using fastText, a popular word-embedding toolkit. This
    is handy when your textual data is not in a general domain (e.g., medical, financial,
    legal, and so on) and/or is not in English.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到了如何下载预训练的单词嵌入并检索相关的单词。在本节中，我将解释如何使用自己的文本数据使用 fastText，一种流行的单词嵌入工具包，训练单词嵌入。当你的文本数据不是在普通领域（例如，医疗、金融、法律等）中，和/或者不是英文时，这将非常方便。
- en: 3.6.1 Making use of subword information
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 利用子词信息
- en: All the word-embedding methods we’ve seen so far in this chapter assign a distinct
    word vector for each word. For example, word vectors for “dog” and “cat” are treated
    distinctly and are independently trained at the training time. At first glance,
    there seems to be nothing wrong about this. After all, they *are* separate words.
    But what if the words were, say, “dog” and “doggy?” Because “-y” is an English
    suffix that denotes some familiarity and affection (other examples include “grandma”
    and “granny” and “kitten” and “kitty”), these pairs of words have some semantic
    connection. However, word-embedding algorithms that treat words as distinct cannot
    make this connection. In the eyes of these algorithms, “dog” and “doggy” are nothing
    more than, say, word_823 and word_1719.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中我们看到的所有词嵌入方法都为每个单词分配了一个独特的单词向量。例如，“狗”和“猫”的单词向量被视为独立的，并且在训练时独立训练。乍一看，这似乎没有什么问题。毕竟，它们*确实*是不同的单词。但是，如果单词分别是“狗”和“小狗”呢？因为“-y”是一个表示亲近和喜爱的英语后缀（其他例子包括“奶奶”和“奶奶”、“小猫”和“小猫”），这些词对有一定的语义联系。然而，将单词视为独立的单词嵌入算法无法建立这种联系。在这些算法的眼中，“狗”和“小狗”只不过是word_823和word_1719而已。
- en: This is obviously limiting. In most languages, there’s a strong connection between
    word orthography (how you write words) and word semantics (what they mean). For
    example, words that share the same stems (e.g., “study” and “studied,” “repeat”
    and “repeatedly,” and “legal” and “illegal”) are often related. By treating them
    as separate words, word-embedding algorithms are losing a lot of information.
    How can they leverage word structures and reflect the similarities in the learned
    word embeddings?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是局限性的。在大多数语言中，单词拼写（你如何书写）和单词语义（它们的意思）之间有着强烈的联系。例如，共享相同词根的单词（例如，“study”和“studied”、“repeat”和“repeatedly”以及“legal”和“illegal”）通常是相关的。通过将它们视为独立的单词，单词嵌入算法正在丢失很多信息。它们如何利用单词结构并反映所学单词嵌入中的相似性呢？
- en: '*fastText*, an algorithm and a word-embedding library developed by Facebook,
    is one such model. It uses *subword information*, which means information about
    linguistic units that are smaller than words, to train higher-quality word embeddings.
    Specifically, fastText breaks words down to character n-grams (section 3.2.3)
    and learns embeddings for them. For example, if the target word is “doggy,” it
    first adds special symbols at the beginning and end of the word and learns embeddings
    for <do, dog, ogg, ggy, gy>, when n = 3\. The vector for “doggy” is simply the
    sum of all these vectors. The rest of the architecture is quite similar to that
    of Skip-gram. Figure 3.10 shows the structure of the fastText model.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*fastText*，是Facebook开发的一种算法和词嵌入库，是这样一个模型。它使用*子词信息*，这意味着比单词更小的语言单位的信息，来训练更高质量的词嵌入。具体来说，fastText将单词分解为字符n-gram（第3.2.3节）并为它们学习嵌入。例如，如果目标单词是“doggy”，它首先在单词的开头和结尾添加特殊符号并为<do，dog，ogg，ggy，gy>学习嵌入，当n=3。
    “doggy”的向量只是所有这些向量的总和。其余的架构与Skip-gram的架构非常相似。图3.10显示了fastText模型的结构。'
- en: '![CH03_F10_Hagiwara](../Images/CH03_F10_Hagiwara.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F10_Hagiwara](../Images/CH03_F10_Hagiwara.png)'
- en: Figure 3.10 Architecture of fastText
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 fastText的架构
- en: Another benefit in leveraging subword information is that it can alleviate the
    *out-of-vocabulary* (OOV) problem. Many NLP applications and models assume a fixed
    vocabulary. For example, a typical word-embedding algorithm such as Skip-gram
    learns word vectors only for the words that were encountered in the train set.
    However, if a test set contains words that did not appear in the train set (which
    are called OOV words), the model would be unable to assign any vectors to them.
    For example, if you train Skip-gram word embeddings from books published in the
    1980s and apply them to modern social media text, how would it know what vectors
    to assign to “Instagram”? It won’t. On the other hand, because fastText uses subword
    information (character n-grams), it can assign word vectors to any OOV words,
    as long as they contain character n-grams seen in the training data (which is
    almost always the case). It can potentially guess it’s related to something quick
    (“Insta”) and pictures (“gram”).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 利用子词信息的另一个好处是可以减轻*词汇外*（OOV）问题。许多NLP应用和模型假设一个固定的词汇表。例如，典型的词嵌入算法如Skip-gram只学习在训练集中遇到的单词的词向量。但是，如果测试集包含在训练集中未出现的单词（称为OOV单词），模型将无法为它们分配任何向量。例如，如果您从上世纪80年代出版的书籍中训练Skip-gram词嵌入，并将其应用于现代社交媒体文本，它将如何知道要为“Instagram”分配什么向量？它不会。另一方面，由于fastText使用子词信息（字符n-gram），它可以为任何OOV单词分配词向量，只要它们包含在训练数据中看到的字符n-gram（这几乎总是如此）。它可能猜到它与一些快速相关（“Insta”）和图片（“gram”）有关。
- en: 3.6.2 Using the fastText toolkit
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.2 使用fastText工具包
- en: Facebook provides the open source for the fastText toolkit, a library for training
    the word-embedding model discussed in the previous section. In the remainder of
    this section, let’s see how it feels like to use this library to train word embeddings.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook提供了fastText工具包的开源代码，这是一个用于训练前面章节讨论的词嵌入模型的库。在本节的其余部分，让我们看看使用这个库来训练词嵌入是什么感觉。
- en: 'First, go to their official documentation ([http://realworldnlpbook.com/ch3.html
    #fasttext](http://realworldnlpbook.com/ch3.html#fasttext)) and follow the instruction
    to download and compile the library. It is just a matter of cloning the GitHub
    repository and running make from the command line in most environments. After
    compilation is finished, you can run the following command to train a Skip-gram-based
    fastText model:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，转到官方文档（[http://realworldnlpbook.com/ch3.html #fasttext](http://realworldnlpbook.com/ch3.html#fasttext)）并按照说明下载和编译该库。在大多数环境中，只需克隆GitHub存储库并从命令行运行make即可。编译完成后，您可以运行以下命令来训练基于Skip-gram的fastText模型：'
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We are assuming there’s a text data file under ../data/text8 that you’d like
    to use as the training data, but change this if necessary. This will create a
    model.bin file, which is a binary representation of the trained model. After training
    the model, you can obtain word vectors for any words, even for the ones that you’ve
    never seen in the training data, as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设在../data/text8下有一个文本数据文件，您想要用作训练数据，但如果需要，请更改这个位置。这将创建一个model.bin文件，这是一个训练模型的二进制表示。训练完模型后，您可以获得任何单词的词向量，甚至是在训练数据中从未见过的单词，方法如下：
- en: '[PRE25]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 3.7 Document-level embeddings
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 文档级嵌入
- en: All the models I have described so far learn embeddings for individual words.
    If you are concerned only with word-level tasks such as inferring word relationships,
    or if they are combined with more powerful neural network models such as recurrent
    neural networks (RNNs), they can be very useful tools. However, if you wish to
    solve NLP tasks that are concerned with larger linguistic structures such as sentences
    and documents using word embeddings and traditional machine learning tools such
    as logistic regression and support vector machines (SVMs), word-level embedding
    methods are still limited. How can you represent larger linguistic units such
    as sentences using vector representations? How can you use word embeddings for
    sentiment analysis, for example?
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我描述的所有模型都是为单词学习嵌入。如果您只关注词级任务，比如推断词之间的关系，或者将它们与更强大的神经网络模型（如循环神经网络（RNN））结合使用，它们可以是非常有用的工具。然而，如果您希望使用词嵌入和传统机器学习工具（如逻辑回归和支持向量机（SVM））解决与更大语言结构（如句子和文档）相关的
    NLP 任务，词级嵌入方法仍然是有限的。您如何用向量表示来表示更大的语言单元，比如句子？您如何使用词嵌入进行情感分析，例如？
- en: One way to achieve this is to simply use the average of all word vectors in
    a sentence. You can average vectors by taking the average of first elements, second
    elements, and so on and make a new vector by combining these averaged numbers.
    You can use this new vector as an input to traditional machine learning models.
    Although this method is simple and can be effective, it is also very limiting.
    The biggest issue is that it cannot take word order into consideration. For example,
    both sentences “Mary loves John.” and “John loves Mary.” would have exactly the
    same vectors if you simply averaged word vectors for each word in the sentence.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实现这一目标的方法是简单地使用句子中所有词向量的平均值。您可以通过取第一个元素、第二个元素的平均值等等，然后通过组合这些平均数生成一个新的向量。您可以将这个新向量作为传统机器学习模型的输入。尽管这种方法简单且有效，但它也有很大的局限性。最大的问题是它不能考虑词序。例如，如果您仅仅对句子中的每个单词向量取平均值，那么句子“Mary
    loves John.”和“John loves Mary.”的向量将完全相同。
- en: NLP researchers have proposed models and algorithms that can specifically address
    this issue. One of the most popular is *Doc2Vec*, originally proposed by Le and
    Mikolov in 2014 ([https://cs.stanford.edu/~quocle/paragraph_vector.pdf](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)).
    This model, as its name suggests, learns vector representations for documents.
    In fact, “document” here simply means any variable-length piece of text that contains
    multiple words. Similar models are also called under many similar names such as
    *Sentence2Vec*, *Paragraph2Vec*, *paragraph vectors* (this is what the authors
    of the original paper used), but in essence, they all refer to the variations
    of the same model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 研究人员提出了可以专门解决这个问题的模型和算法。其中最流行的之一是*Doc2Vec*，最初由 Le 和 Mikolov 在 2014 年提出（[https://cs.stanford.edu/~quocle/paragraph_vector.pdf](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)）。这个模型，正如其名称所示，学习文档的向量表示。事实上，“文档”在这里只是指任何包含多个单词的可变长度文本。类似的模型还被称为许多类似的名称，比如*句子2Vec*、*段落2Vec*、*段落向量*（这是原始论文的作者所用的），但本质上，它们都指的是相同模型的变体。
- en: In the rest of this section, I’m going to discuss one of the Doc2Vec models
    called *distributed memory model of paragraph vectors* (PV-DM). The model looks
    very similar to CBOW, which we studied earlier in this chapter, but with one key
    difference—one additional vector, called *paragraph vector*, is added as an input.
    The model predicts the target word from a set of context words *and* the paragraph
    vector. Each paragraph is assigned a distinct paragraph vector. Figure 3.11 shows
    the structure of this PV-DM model. Also, PV-DM uses only context words that come
    before the target word for prediction, but this is a minor difference.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我将讨论一种称为*段落向量分布记忆模型*（PV-DM）的 Doc2Vec 模型之一。该模型与我们在本章前面学习的 CBOW 非常相似，但有一个关键的区别——多了一个向量，称为*段落向量*，作为输入。该模型从一组上下文单词*和*段落向量预测目标词。每个段落都被分配一个不同的段落向量。图
    3.11 展示了 PV-DM 模型的结构。另外，PV-DM 仅使用在目标词之前出现的上下文单词进行预测，但这只是一个微小的差异。
- en: '![CH03_F11_Hagiwara](../Images/CH03_F11_Hagiwara.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F11_Hagiwara](../Images/CH03_F11_Hagiwara.png)'
- en: Figure 3.11 Distributed memory model of paragraph vectors
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 段落向量分布记忆模型
- en: What effect would this paragraph vector have on the prediction task? Now you
    have some extra information from the paragraph vector for predicting the target
    word. As the model tries to maximize the prediction accuracy, you can expect that
    the paragraph vector is updated so that it provides some useful “context” information
    in the sentence that is not collectively captured by the context word vectors.
    As a by-product, the model learns something that reflects the overall meaning
    of each paragraph, along with word vectors.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这段向量会对预测任务有什么影响？现在您从段落向量中获得了一些额外信息来预测目标单词。由于模型试图最大化预测准确性，您可以预期段落向量会更新，以便它提供一些在句子中有用的“上下文”信息，这些信息不能被上下文词向量共同捕获。作为副产品，模型学会了反映每个段落的整体含义，以及词向量。
- en: Several open source libraries and packages support Doc2Vec models, but one of
    the most widely used is Gensim ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)),
    which can be installed by running pip install gensim. Gensim is a popular NLP
    toolkit that supports a wide range of vector and topic models such as TF-IDF (term
    frequency and inverse document frequency), LDA (latent semantic analysis), and
    word embeddings.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 几个开源库和包支持 Doc2Vec 模型，但其中一个最广泛使用的是 Gensim（[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)），可以通过运行
    pip install gensim 来安装。Gensim 是一个流行的自然语言处理工具包，支持广泛的向量和主题模型，例如 TF-IDF（词频和逆文档频率）、LDA（潜在语义分析）和词嵌入。
- en: 'To train a Doc2Vec model using Gensim, you first need to read a dataset and
    convert documents to TaggedDocument. This can be done using the read_corpus()
    method shown here:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Gensim 训练 Doc2Vec 模型，您首先需要读取数据集并将文档转换为 TaggedDocument。可以使用此处显示的 read_corpus()
    方法来完成：
- en: '[PRE26]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We are going to use a small dataset consisting of the first 200,000 English
    sentences taken from the Tatoeba project ([https://tatoeba.org/](https://tatoeba.org/)).
    You can download the dataset from [http://mng.bz/7l0y](http://mng.bz/7l0y). Then
    you can use Gensim’s Doc2Vec class to train the Doc2Vec model and retrieve similar
    documents based on the trained paragraph vectors, as demonstrated next.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个小数据集，其中包含来自 Tatoeba 项目（[https://tatoeba.org/](https://tatoeba.org/)）的前
    200,000 个英文句子。您可以从 [http://mng.bz/7l0y](http://mng.bz/7l0y) 下载数据集。然后，您可以使用 Gensim
    的 Doc2Vec 类来训练 Doc2Vec 模型，并根据训练的段落向量检索相似的文档，如下所示。
- en: Listing 3.6 Training a Doc2Vec model and retrieving similar documents
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 训练 Doc2Vec 模型并检索相似文档
- en: '[PRE27]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will show you a list of documents similar to the input document “I heard
    a dog barking in the distance,” as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示与输入文档“I heard a dog barking in the distance.”相似的文档列表，如下所示：
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Notice that most of the retrieved sentences here are related to hearing sound.
    In fact, an identical sentence is in the list, because I took the query sentence
    from Tatoeba in the first place! Gensim’s Doc2Vec class has a number of hyperparameters
    that you can use to tweak the model. You can read further about the class on their
    reference page ([https://radimrehurek.com/gensim/models/doc2vec.html](https://radimrehurek.com/gensim/models/doc2vec.html)).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里检索到的大多数句子与听到声音有关。事实上，列表中有一个相同的句子，因为我一开始就从 Tatoeba 中获取了查询句子！Gensim 的 Doc2Vec
    类有许多超参数，您可以使用它们来调整模型。您可以在他们的参考页面上进一步了解该类（[https://radimrehurek.com/gensim/models/doc2vec.html](https://radimrehurek.com/gensim/models/doc2vec.html)）。
- en: 3.8 Visualizing embeddings
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.8 可视化嵌入
- en: In the final section of this chapter, we are going to shift our focus on visualizing
    word embeddings. As we’ve done earlier, retrieving similar words given a query
    word is a great way to quickly check if word embeddings are trained correctly.
    But it gets tiring and time-consuming if you need to check a number of words to
    see if the word embeddings are capturing semantic relationships between words
    as a whole.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们将把重点转移到可视化词嵌入上。正如我们之前所做的，给定一个查询词检索相似的词是一个快速检查词嵌入是否正确训练的好方法。但是，如果您需要检查多个词以查看词嵌入是否捕获了单词之间的语义关系，这将变得令人疲倦和耗时。
- en: As mentioned earlier, word embeddings are simply *N*-dimensional vectors, which
    are also “points” in an *N*-dimensional space. We were able to see those points
    visually in a 3-D space in figure 3.1 because *N* was 3\. But *N* is typically
    a couple of hundred in most word embeddings, and we cannot simply plot them on
    an *N*-dimensional space.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，词嵌入简单地是 *N* 维向量，也是 *N* 维空间中的“点”。我们之所以能够在图 3.1 中以 3-D 空间可视化这些点，是因为 *N* 是
    3。但是在大多数词嵌入中，*N* 通常是一百多，我们不能简单地将它们绘制在 *N* 维空间中。
- en: A solution is to reduce the dimension down to something that we can see (two
    or three dimensions) while preserving relative distances between points. This
    technique is called *dimensionality reduction*. We have a number of ways to reduce
    dimensionality, including PCA (principal component analysis) and ICA (independent
    component analysis), but by far the most widely used visualization technique for
    word embeddings is called *t-SNE* (t-distributed Stochastic Neighbor Embedding,
    pronounced “tee-snee). Although the details of t-SNE are outside the scope of
    this book, the algorithm tries to map points to a lower-dimensional space by preserving
    the relative neighboring relationship between points in the original high-dimensional
    space.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方案是将维度降低到我们可以看到的东西（二维或三维），同时保持点之间的相对距离。这种技术称为*降维*。我们有许多降低维度的方法，包括 PCA（主成分分析）和
    ICA（独立成分分析），但迄今为止，用于单词嵌入的最广泛使用的可视化技术是称为*t-SNE*（t-分布随机近邻嵌入，发音为“tee-snee”）的方法。虽然
    t-SNE 的细节超出了本书的范围，但该算法试图通过保持原始高维空间中点之间的相对邻近关系来将点映射到较低维度的空间。
- en: The easiest way to use t-SNE is to use Scikit-Learn ([https://scikit-learn.org/](https://scikit-learn.org/)),
    a popular Python library for machine learning. After installing it (usually just
    a matter of running pip install scikit-learn), you can use it to visualize the
    GloVe vectors read from a file as shown next (we use Matplotlib to draw the plot).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 t-SNE 的最简单方法是使用 Scikit-Learn ([https://scikit-learn.org/](https://scikit-learn.org/))，这是一个流行的用于机器学习的
    Python 库。安装后（通常只需运行 pip install scikit-learn），您可以像下面展示的那样使用它来可视化从文件中读取的 GloVe
    向量（我们使用 Matplotlib 来绘制图表）。
- en: Listing 3.7 Using t-SNE to visualize GloVe embeddings
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 3.7 使用 t-SNE 来可视化 GloVe 嵌入
- en: '[PRE29]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In listing 3.7, I used xlim() and ylim() to limit the plotted range to magnify
    some areas that are of interest to us. You may want to try different values to
    focus on other areas in the plot.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在清单 3.7 中，我使用 xlim() 和 ylim() 将绘制范围限制在我们感兴趣的一些区域，以放大一些区域。您可能想尝试不同的值来聚焦绘图中的其他区域。
- en: 'The code in listing 3.7 generates the plot shown in figure 3.12\. There’s a
    lot of interesting stuff going on here, but at a quick glance, you will notice
    the following clusters of words that are semantically related:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 3.7 中的代码生成了图 3.12 中显示的图。这里有很多有趣的东西，但快速浏览时，您会注意到以下词语聚类，它们在语义上相关：
- en: 'Bottom-left: web-related words (*posts*, *article*, *blog*, *comments*, . .
    . ).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底部左侧：与网络相关的词语（*posts*，*article*，*blog*，*comments*，. . . ）。
- en: 'Upper-left: time-related words (*day*, *week*, *month*, *year*, . . . ).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上方左侧：与时间相关的词语（*day*，*week*，*month*，*year*，. . . ）。
- en: 'Middle: numbers (0, 1, 2, . . . ). Surprisingly, these numbers are lined up
    in an increasing order toward the bottom. GloVe figured out which numbers are
    larger purely from a large amount of textual data.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间：数字（0，1，2，. . . ）。令人惊讶的是，这些数字向底部递增排序。GloVe 仅从大量的文本数据中找出了哪些数字较大。
- en: 'Bottom-right: months (january, february, . . . ) and years (2004, 2005, . .
    . ). Again, the years seem to be lined up in an increasing order, almost in parallel
    with the numbers (0, 1, 2, . . . ).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底部右侧：月份（january，february，. . . ）和年份（2004，2005，. . . ）。同样，年份似乎按照递增顺序排列，几乎与数字（0，1，2，.
    . . ）平行。
- en: '![CH03_F12_Hagiwara](../Images/CH03_F12_Hagiwara.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_Hagiwara](../Images/CH03_F12_Hagiwara.png)'
- en: Figure 3.12 GloVe embeddings visualized by t-SNE
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 由 t-SNE 可视化的 GloVe 嵌入
- en: If you think about it, it’s an incredible feat for a purely mathematical model
    to figure out these relationships among words, all from a large amount of text
    data. Hopefully, now you know how much of an advantage it is if the model knows
    “july” and “june” are closely related compared to needing to figure everything
    out starting from word_823 and word_1719.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细思考一下，一个纯粹的数学模型能够从大量的文本数据中找出这些词语之间的关系，这实在是一项不可思议的成就。希望现在您知道，如果模型知道“july”和“june”之间的关系密切相连，与从
    word_823 和 word_1719 开始逐一解释所有内容相比，这有多么有利。
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Word embeddings are numeric representations of words, and they help convert
    discrete units (words and sentences) to continuous mathematical objects (vectors).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词嵌入是单词的数字表示，它们有助于将离散单位（单词和句子）转换为连续的数学对象（向量）。
- en: The Skip-gram model uses a neural network with a linear layer and softmax to
    learn word embeddings as a by-product of the “fake” word-association task.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-gram 模型使用具有线性层和 softmax 的神经网络来学习单词嵌入，作为“假”词语关联任务的副产品。
- en: GloVe makes use of global statistics of word co-occurrence to train word embeddings
    efficiently.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GloVe 利用单词共现的全局统计信息有效地训练单词嵌入。
- en: Doc2Vec and fastText learn document-level embeddings and word embeddings with
    subword information, respectively.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doc2Vec 和 fastText 分别用于学习文档级别的嵌入和带有子词信息的词嵌入。
- en: You can use t-SNE to visualize word embeddings.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用 t-SNE 来可视化词嵌入。
