- en: '3 Getting started with baselines: Benchmarking and optimization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3 开始基线: 基准测试和优化'
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Analyzing a pair of natural language processing (NLP) problems
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析一对自然语言处理（NLP）问题
- en: Establishing problem baselines using key traditional methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用关键的传统方法建立问题基线
- en: Baselining with representative deep pretrained language models, ELMo and BERT
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用代表性的深度预训练语言模型ELMo和BERT进行基线
- en: In this chapter, we continue our direct dive into solving NLP problems we started
    in the previous chapter. We continue with our goal to establish a set of baselines
    for a pair of concrete NLP problems, which we will later be able to use to measure
    progressive improvements gained from leveraging increasingly sophisticated transfer
    learning approaches. We complete the exercise that we started in chapter 2, where
    we introduced a pair of practical problems, preprocessed the corresponding data,
    and initiated baselining by exploring some generalized linear methods. In particular,
    we introduced the email spam and the Internet Movie Database (IMDB) movie review
    classification examples and used logistic regression and a support vector machine
    (SVM) to baseline them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续直接深入解决NLP问题，这是我们在上一章开始的。我们继续追求建立一套具体NLP问题的基线，稍后我们将能够利用这些基线来衡量从越来越复杂的迁移学习方法中获得的逐渐改进。我们完成了我们在第2章开始的练习，那里我们介绍了一对实际问题，预处理了相应的数据，并通过探索一些广义线性方法开始了基线。特别是，我们介绍了电子邮件垃圾邮件和互联网电影数据库（IMDB）电影评论分类示例，并使用了逻辑回归和支持向量机（SVM）来建立它们的基线。
- en: In this chapter, we explore decision-tree-based and neural-network-based methods.
    The decision-tree-based methods we look at include random forests and gradient-boosting
    machines. With regard to the neural-network-based methods, we apply the simplest
    form of transfer learning to a pair of recently popularized deep pretrained language
    models, ELMo and BERT. This work involves fine-tuning only a handful of the final
    layers of each network on a target dataset. This activity will serve as an applied
    hands-on introduction to the main theme of the book—transfer learning for NLP.
    Additionally, we explore optimizing the performance of models via hyperparameter
    tuning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨基于决策树和基于神经网络的方法。我们研究的基于决策树的方法包括随机森林和梯度提升机。关于基于神经网络的方法，我们将最简单形式的迁移学习应用到了一对最近流行的深度预训练语言模型ELMo和BERT上。这项工作只涉及在目标数据集上对每个网络的最后几层进行微调。这项活动将作为本书主题的应用性实践介绍，即NLP的迁移学习。此外，我们探索通过超参数调优来优化模型的性能。
- en: We begin by exploring decision-tree-based methods in the next section.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中探讨基于决策树的方法。
- en: 3.1 Decision-tree-based models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 基于决策树的模型
- en: A decision tree is a decision support aid that models decisions and their consequences
    as *trees*—a graph where any two nodes are connected by exactly one path. An alternative
    definition of a tree is a flowchart that transforms input values into output categories.
    More details about this type of model can be found in chapter 1.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种决策支持工具，它将决策及其后果建模为*树*——一个图，其中任意两个节点都由一条路径连接。树的另一种定义是将输入值转换为输出类别的流程图。有关这种类型模型的更多详细信息，请参阅第1章。
- en: In this section, we apply two of the most common decision-tree-based methods—random
    forests and gradient-boosting machines—to our two running problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将两种最常见的基于决策树的方法——随机森林和梯度提升机——应用到我们的两个正在运行的问题上。
- en: 3.1.1 Random forests (RFs)
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 随机森林（RFs）
- en: '*Random* *forests* (RFs) provide a practical machine learning method for applying
    decision trees by generating a large number of specialized trees and collecting
    their outputs. RFs are extremely flexible and widely applicable, making them often
    the second algorithm practitioners try after logistic regression for baselining.
    See chapter 1 for more detailed discussions around RFs and their historical context.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林*（RFs）通过生成大量专门的树并收集它们的输出，为应用决策树提供了一种实用的机器学习方法。RFs非常灵活和广泛适用，通常是从逻辑回归后从业者尝试的第二种算法用于建立基线。有关RFs及其历史背景的更详细讨论，请参阅第1章。'
- en: Let’s build our classifier using the popular library scikit-learn, as shown
    next.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用流行的库scikit-learn来构建我们的分类器，如下所示。
- en: Listing 3.1 Training and testing a random forest classifier
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 列出3.1训练和测试随机森林分类器
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Loads scikit's random forest classifier library
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载scikit的随机森林分类器库
- en: ❷ Creates a random forest classifier
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个随机森林分类器
- en: ❸ Trains the classifier to learn how the training features relate to the training
    response variable
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练分类器，以了解训练特征与训练响应变量的关系
- en: Training the RF classifier on the email example data with this code took under
    a second in our experience and achieved an accuracy score of 0.945\. Training
    it on the IMDB example similarly took under a second and achieved an accuracy
    score of 0.665\. This exercise further confirms the initial hunch from the previous
    chapter that the IMDB review problem is harder than the email classification problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用这段代码在我们的实验中，对电子邮件示例数据进行 RF 分类器的训练只需不到一秒钟的时间，并且达到了 0.945 的准确率分数。类似地，在 IMDB 示例上进行训练也只需不到一秒钟，并且达到了
    0.665 的准确率分数。这个练习进一步证实了上一章的最初猜测，即 IMDB 评论问题比电子邮件分类问题更难。
- en: 3.1.2 Gradient-boosting machines (GBMs)
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 梯度提升机（GBMs）
- en: This variant of decision-tree-based machine learning algorithms iteratively
    learns new decision-tree-based models that address the weak points of models from
    the previous iterations. At the time of this writing, they are widely considered
    to be the best class of methods for addressing nonperceptual machine learning
    problems. They do present some disadvantages, unfortunately, including a larger
    model size, a higher risk of overfitting, and less interpretability than some
    other decision tree models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于决策树的机器学习算法的变体迭代地学习新的基于决策树的模型，以解决前次迭代模型的弱点。在撰写本文时，它们被普遍认为是解决非感知机器学习问题的最佳类方法。不幸的是，它们确实存在一些缺点，包括较大的模型大小、过拟合的风险更高以及比其他一些决策树模型更少的可解释性。
- en: The code for training a gradient-boosting machine (GBM) classifier is shown
    in the next listing. Again, we use the implementation of these models in scikit-learn.
    Note that the implementation in the Python library XGBoost is widely considered
    to be more memory-efficient and more readily scalable/parallelizable.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练梯度提升机（GBM）分类器的代码显示在下一个列表中。同样，我们使用 scikit-learn 中这些模型的实现。请注意，Python 库 XGBoost
    中的实现被普遍认为更具有内存效率，并且更容易扩展/并行化。
- en: Listing 3.2 Training/testing a gradient-boosting machine classifier
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 训练/测试梯度提升机分类器
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ GBM algorithm
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ GBM 算法
- en: ❷ Additional sklearn functions
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 附加的 sklearn 函数
- en: ❸ Fits the algorithm on the overall data
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在整体数据上拟合算法
- en: ❹ Predicts the training set
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 预测训练集
- en: ❺ Performs k-fold cross-validation
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行 k 折交叉验证
- en: ❻ Prints the model report
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印模型报告
- en: ❼ Predicts the test data
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 预测测试数据
- en: Note that in listing 3.2, in addition to the usual training accuracy score,
    we report *k-fold cross-validation* and the *area under the receiver operating
    characteristic* (ROC) *curve* to evaluate the model. It’s necessary to do this
    here because GBMs are particularly prone to overfitting, and reporting these metrics
    helps us monitor that risk. Another reason is that the exercise allows you to
    review these concepts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在列表 3.2 中，除了通常的训练准确率分数外，我们还报告了 *k 折交叉验证* 和 *受试者工作特征曲线*（ROC）下的 *曲线下面积* 来评估模型。这是必要的，因为
    GBMs 特别容易过拟合，报告这些指标有助于我们监控这种风险。另一个原因是，这个练习可以让你复习这些概念。
- en: More specifically, k-fold cross-validation (with a default value of k=5 folds)
    randomly splits the training dataset into k partitions, or folds, and trains the
    model on k-1 of them while evaluating/validating performance on the remaining
    kth partition, repeating this process k times, with each partition serving as
    a validation set. It then reports the performance using the statistics of these
    k evaluation iterations. This process allows us to reduce the risk of the model
    overfitting on some parts of the dataset and underperforming on others.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，k 折交叉验证（默认值为 k=5 折）将训练数据集随机分为 k 个分区或折叠，并在 k-1 个分区上训练模型，同时在剩余的第 k 个分区上评估/验证性能，重复这个过程
    k 次，每个分区都作为验证集。然后，它使用这些 k 次评估迭代的统计数据报告性能。这个过程允许我们减少模型在数据集的某些部分过拟合和在其他部分表现不佳的风险。
- en: Note Put simply, overfitting refers to fitting too many parameters to too little
    data. This scenario hurts the model’s ability to generalize to new data and often
    manifests as improving training metrics with no improvement in validation metrics.
    It can be alleviated by collecting more data, simplifying the model to reduce
    the number of training parameters, and other approaches that we will highlight
    throughout the book.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，过拟合是指将太多参数拟合到太少的数据中。这种情况会损害模型对新数据的泛化能力，并且通常表现为改善训练指标但验证指标没有改善。可以通过收集更多数据、简化模型以减少训练参数的数量以及本书中我们将重点介绍的其他方法来减轻这种情况。
- en: 'The following code can be used to call the function and evaluate it on each
    of the two running examples:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可用于调用函数并在两个运行示例中对其进行评估：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For the email spam classification example, this yields the following result:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电子邮件垃圾邮件分类示例，这产生了以下结果：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For the IMDB movie review classification example, this yields the result shown
    here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IMDB电影评论分类示例，这产生了以下结果：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At first glance, looking at these results, one may be tempted to conclude that
    the GBM numerical experiment is more expensive compared to those carried out for
    the prior methods we looked at—taking almost 10 minutes to complete in the case
    of the IMDB example on Kaggle. However, we must take into account the fact that
    when the k-fold cross-validation exercise is carried out, the model is trained
    repeatedly k=5 times to obtain a more reliable estimate of performance. Each training
    thus takes approximately two minutes—not as drastic an increase in training time
    as would be deduced without considering the k-fold cross-validation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，看着这些结果，人们可能会倾向于认为与我们之前看过的先前方法相比，GBM数值实验更昂贵——在Kaggle上IMDB示例完成需要将近10分钟。然而，我们必须考虑到当进行k折交叉验证练习时，模型会被训练k=5次以获得更可靠的性能估计。因此，每次训练大约需要两分钟——这并不像不考虑k折交叉验证而推断出的训练时间增加那么剧烈。
- en: We can see some evidence of overfitting—the testing accuracy is lower than the
    k-fold training accuracy for the first example. Moreover, in the case of the IMDB
    example, the k-fold cross-validation scores are noticeably lower than the training
    score on the overall dataset, underscoring the importance of using the k-fold
    cross-validation approach for tracking overfitting in this model type. We discuss
    some approaches to improving the accuracy of the classifier further in the penultimate
    section of this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些过拟合的证据——第一个示例的测试准确度低于k折训练准确度。此外，在IMDB示例中，k折交叉验证分数明显低于整个数据集的训练分数，强调了在这种模型类型中使用k折交叉验证方法跟踪过拟合的重要性。我们在本章的倒数第二节中讨论了一些进一步提高分类器准确性的方法。
- en: 'So what exactly is the ROC curve? It is the plot of the false positive rate
    (FPR) versus the true positive rate (TPR) and an important characteristic used
    to evaluate and tune classifiers. It shows the trade-off in these important qualities
    of a classifier as the decision threshold—the probability value beyond which a
    predicted confidence begins to be classified as a member of a given class—is varied
    between 0 and 1\. The following code can now be used to plot this curve:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 那么ROC曲线究竟是什么呢？它是假正率（FPR）与真正率（TPR）的曲线图，是用来评估和调整分类器的重要特征。它显示了这些分类器重要特性的权衡，当决策阈值——预测置信度开始被分类为给定类别的成员的概率值——在0和1之间变化时。现在可以使用以下代码来绘制这条曲线：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ We first need to find the maximum probabilities for each example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们首先需要找到每个示例的最大概率。
- en: ❷ Calculates the ROC curve values
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算ROC曲线值
- en: ❸ Generates the labeled ROC curve plot with the matplotlib library
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用matplotlib库生成标记的ROC曲线图
- en: The resulting ROC curve for the email classification example is shown in figure
    3.1\. The straight line with a slope of 1 represents the FPR-versus-TPR trade-off
    corresponding to random chance. The further to the left the ROC curve is from
    this line, the better performing the classifier. As a result, the area under the
    ROC curve can be used as a measure of performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 邮件分类示例的结果ROC曲线如图3.1所示。斜率为1的直线代表FPR与TPR之间的权衡对应于随机机会。ROC曲线越远离此线的左侧，分类器性能越好。因此，ROC曲线下面积可用作性能的度量。
- en: '![03_01](../Images/03_01.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![03_01](../Images/03_01.png)'
- en: Figure 3.1 ROC curve for the email classification example
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 邮件分类示例的ROC曲线
- en: One important property of decision-tree-based methods is that they can provide
    features an importance score, which can be used to detect the most important features
    in a given dataset. We do this by inserting a couple lines of code right before
    the return statement of the function in listing 3.2, as shown in listing 3.3.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树方法的一个重要特性是它们可以提供特征的重要性得分，这可以用来检测给定数据集中最重要的特征。我们通过在列表 3.2 函数的返回语句之前插入几行代码来做到这一点，如列表
    3.3 中所示。
- en: Listing 3.3 Gradient-boosting machine classification code with feature importance
    scores
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 梯度提升机分类代码和特征重要性分数
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ GBM algorithm
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ GBM 算法
- en: ❷ Additional sklearn functions
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 其他 sklearn 函数
- en: ❸ Fits the algorithm on the overall data
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在整体数据上拟合算法
- en: ❹ Predicts the training set
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 预测训练集
- en: ❺ Performs k-fold cross-validation
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行 k 折交叉验证
- en: ❻ Prints the model report
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印模型报告
- en: ❼ Adds new code to compute the importances of features
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 添加新的代码来计算特征的重要性
- en: ❽ Predicts the test data
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 预测测试数据
- en: For the IMDB example, this yields the plot in figure 3.2\. We see that words
    like “worst” and “awful” are very important to the classification decision, which
    makes qualitative sense, because one can imagine a negative critic using these
    words. On the other hand, words like “loved” may be used by a positive reviewer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 IMDB 示例，这产生了图 3.2 的绘图。我们看到像“worst”和“awful”这样的词对分类决策非常重要，这在定性上是有意义的，因为可以想象负面评论家使用这些词。另一方面，“loved”这样的词可能会被积极的评论者使用。
- en: 'A point of caution: Importance scores seem to work well for this example, but
    they should not always be blindly trusted. For instance, it has been widely recognized
    that these importance scores can be biased toward continuous variables, as well
    as high cardinality categorical variables.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：重要性分数在这个示例中似乎效果很好，但不应该一味地相信它们。例如，已经广泛认识到这些重要性分数可能对连续变量以及高基数分类变量有偏见。
- en: We now proceed to applying to our two running examples some neural network models,
    arguably the most important class of models for NLP today.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始应用一些神经网络模型到我们两个运行的示例中，可以说神经网络是当今自然语言处理中最重要的模型类别之一。
- en: 3.2 Neural network models
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 神经网络模型
- en: As we discussed in chapter 1, neural networks are the most important class of
    machine learning algorithms for handling perceptual problems such as computer
    vision and NLP.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 1 章讨论的那样，神经网络是处理感知问题（如计算机视觉和自然语言处理）最重要的机器学习算法类别之一。
- en: In this section, we will train two representative pretrained neural network
    language models on the two example problems we have been baselining in this and
    the previous chapter. We consider the Embeddings from Language Models (ELMo) and
    Bidirectional Encoder Representations from Transformers (BERT).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在本章和上一章中基线化的两个示例问题上训练两个代表性的预训练神经网络语言模型。我们考虑来自语言模型的嵌入 (ELMo) 和来自transformers的双向编码器表示
    (BERT)。
- en: '![03_02](../Images/03_02.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![03_02](../Images/03_02.png)'
- en: Figure 3.2 Importance scores for the various tokens discovered by the gradient-boosting
    machine classifier for the IMDB classification example
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 在 IMDB 分类示例中由梯度提升机分类器发现的各种标记的重要性分数
- en: ELMo includes elements of convolutional and recurrent (specifically *long short-term
    memory* [LSTM]) elements, whereas the appropriately named BERT is transformer-based.
    These terms were introduced in chapter 1 and will be addressed in more detail
    in subsequent chapters. We employ the simplest form of transfer learning fine-tuning,
    where a single dense classification layer is trained on top of the corresponding
    pretrained embedding over our dataset of labels from the previous sections.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo 包含卷积和循环（特别是*长短期记忆* [LSTM]）元素，而合适命名的 BERT 是基于transformers的。这些术语在第 1 章中介绍过，并将在后续章节中更详细地讨论。我们采用了最简单的迁移学习微调形式，在对应的预训练嵌入之上，通过我们前几节的标签数据集训练了一个单独的密集分类层。
- en: 3.2.1 Embeddings from Language Models (ELMo)
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 语言模型的嵌入 (ELMo)
- en: The Embeddings from Language Models (ELMo) model, named after the popular *Sesame
    Street* character, was among the first models to demonstrate the effectiveness
    of transferring pretrained language model knowledge to general NLP tasks. The
    model was trained to predict the next word in a sequence of words, which can be
    done in an unsupervised manner on very large corpora, and showed that the weights
    obtained as a result could generalize to a variety of other NLP tasks. We will
    not discuss the architecture of this model in detail in this section—we’ll get
    to that in a later chapter. Here we focus on building intuition, but it suffices
    to mention that the model employs character-level convolutions to build up preliminary
    embeddings of each word token, followed by bidirectional LSTM layers, which introduce
    the context of surrounding words into the final embeddings produced by the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型嵌入（ELMo）模型以热门的*Sesame Street*角色命名，是最早证明在一般NLP任务中将预训练的语言模型知识转移的有效性的模型之一。该模型被训练以预测一系列单词中的下一个单词，在非监督的大型语料库上可以进行，结果显示得到的权重可以推广到各种其他NLP任务。我们将不会在本节详细讨论该模型的架构--我们将在后面的章节中讨论该问题。在这里，我们专注于建立直觉,但足够提到的是，该模型利用了字符级卷积来构建每个单词标记的初步嵌入，接着是双向LSTM层，将上下文信息引入模型产生的最终嵌入中。
- en: Having briefly introduced ELMo, let’s proceed to train it for each of the two
    running example datasets. The ELMo model is available through the TensorFlow Hub,
    which provides an easy platform for sharing TensorFlow models. We will use Keras
    with a TensorFlow backend to build our model. In order to make the TensorFlow
    Hub model usable by Keras, we need to define a custom Keras layer that instantiates
    it in the right format. This is achieved using the function shown in the next
    listing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 简要介绍了ELMo之后，让我们开始为两个运行示例数据集中的每个数据集训练它。ELMo模型可以通过TensorFlow Hub获得，TensorFlow
    Hub提供了一个简单的平台用于共享TensorFlow模型。我们将使用使用TensorFlow作为后端的Keras来构建我们的模型。为了使TensorFlow
    Hub模型可以被Keras使用，我们需要定义一个自定义的Keras层，以正确的格式实例化它。下面的代码段展示了如何实现这个功能。
- en: Listing 3.4 Instantiating TensorFlow Hub ELMo as a custom Keras layer
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 代码段3.4：将TensorFlow Hub ELMo实例化为自定义的Keras层
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Imports the required dependencies
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入所需的依赖项
- en: ❷ Initializes the session
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化会话
- en: ❸ Creates a custom layer that allows us to update weights
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个自定义层，允许我们更新权重
- en: ❹ Downloads the pretrained ELMo model from TensorFlow Hub
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从TensorFlow Hub下载预训练的ELMo模型
- en: ❺ Extracts the trainable parameters—four weights in the weighted average of
    the ELMo model layers; see the earlier TensorFlow Hub link for more details
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 提取可训练参数--ELMo模型层加权平均值中的四个权重；更多详细信息请参阅之前的TensorFlow Hub链接
- en: ❻ Specifies the shape of the output
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 指定输出的形状
- en: Before using this function to train a model, we will need to adapt our preprocessed
    data a bit for this model architecture. In particular, recall that we assembled
    a bag-of-words representation for the traditional models from the variable `raw_data`,
    which was produced by listing 2.7 and is a NumPy array containing a list of word
    tokens per email. In this case, we instead use the function and code in listing
    3.5 to combine each such list into a single text string. This is the format in
    which the ELMo TensorFlow Hub model expects the input, and we are glad to oblige.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这个函数来训练模型之前，我们需要对我们的预处理数据进行一些调整，以适应这个模型结构。特别是，回想一下，我们为传统模型组装了变量`raw_data`的词袋表示，该变量由第2.7节的代码生成，并且这是一个包含每封电子邮件的单词标记列表的NumPy数组。在这种情况下，我们将使用第3.5节的函数和代码将每个这样的列表合并成一个单一的文本字符串。这是ELMo
    TensorFlow Hub模型期望的输入格式，我们很高兴满足要求。
- en: Note The combined string in this case has stop words removed, a step that is
    often not required in deep learning practice due to the uncanny ability of artificial
    neural networks to figure out what is important and what isn’t—feature engineering,
    automatically. In our case, because we are trying to compare the strengths and
    weaknesses of the different model types for this problem, applying the same kind
    of preprocessing for all algorithms makes sense and is arguably the right approach.
    We note, however, that ELMo was pretrained on a corpus containing stop words,
    as was BERT.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 深度学习实践中，由于人工神经网络极具发现重要性和非重要性的神奇能力，通常不需要去除停用词这一步骤。但在我们的情况下，因为我们试图比较不同模型类型在这个问题上的优点和缺点，对所有算法应用相同的预处理是有意义的，同时也可以说是正确的方法。然而，需要注意的是，ELMo和BERT都是在包含停用词的语料库上进行预训练的。
- en: Listing 3.5 Converting data to the form expected by the ELMo TensorFlow Hub
    model
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 将数据转换为ELMo TensorFlow Hub模型所需的形式
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Converts data into the right format
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据转换为正确的格式
- en: ❷ Concatenates the tokens for each email into a single string
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将每封邮件的标记连接成一个字符串
- en: ❸ Shuffles the raw data first
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 首先对原始数据进行洗牌
- en: ❹ Converts 70% of the data for training
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将70%的数据转换为训练数据
- en: ❺ Converts the remaining 30% of the data for testing
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将剩余的30%的数据转换为测试数据
- en: Having converted the data into the right format, we use the code in the next
    listing to build and train the Keras ELMo TensorFlow Hub model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据转换为正确的格式之后，我们使用下一个列表中的代码构建和训练Keras ELMo TensorFlow Hub模型。
- en: Listing 3.6 Building ELMo Keras model using custom layer defined in listing
    3.4
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.6 使用列表3.4中定义的自定义层构建ELMo Keras模型
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ New layer outputting 256-dimensional feature vectors
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输出256维特征向量的新层
- en: ❷ Classification layer
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分类层
- en: ❸ Loss, metric, and optimizer choices
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 损失、度量和优化器选择
- en: ❹ Shows the model architecture for inspection
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 显示模型架构以进行检查
- en: ❺ Fits the model for five epochs
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对模型进行五个时期的拟合
- en: A few things should be noted here, given that this is our first encounter with
    some detailed aspects of deep learning design. First of all, notice that we have
    added an additional layer on top the pretrained ELMo embedding, producing 256-dimensional
    feature vectors. We have also added a classification layer of output dimension
    1\. The activation function `sigmoid` transforms its input into the interval between
    0 and 1 and is essentially the logistic curve in figure 2.6\. We can interpret
    its output as the probability of the positive class, and when it exceeds some
    prespecified threshold (usually 0.5), we can classify the corresponding input
    to the network as the said positive class.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要注意几点，因为这是我们第一次接触深度学习设计的一些详细方面。首先，注意到我们在预训练的ELMo嵌入之上添加了一个额外的层，产生了256维的特征向量。我们还添加了一个输出维数为1的分类层。激活函数`sigmoid`将其输入转换为0到1之间的区间，并且在本质上是图2.6中的逻辑曲线。我们可以将其输出解释为正类别的概率，并且当它超过某个预先指定的阈值（通常为0.5）时，我们可以将相应的网络输入分类为正类别。
- en: 'The model is fitted for five “major steps,” or epochs, over the whole dataset.
    The Keras code statement `model.summary()` in listing 3.6 prints the model details
    and produces the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在整个数据集上进行了五个“主要步骤”或时期的拟合。列表3.6中的Keras代码语句`model.summary()`打印出模型的详细信息，并产生以下输出：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We note—without delving into too much further detail, because this will be
    addressed in chapter 4—that most of the trainable parameters in this case (approximately
    260 thousand of them) are coming from the layers we added on top of the custom
    ELMo model. This is our first instance of transfer learning: learning a pair of
    new layers on top of the pretrained model shared by ELMo’s creators. It is important
    to use a powerful GPU for most neural network experiments, and the value of the
    `batch_size` parameter, which specifies how much data is fed to the GPU at each
    step, can be extremely important to the speed of convergence. It will vary with
    the GPU being used, or the lack thereof. In practice, one can increase the value
    of this parameter until the speed of convergence of a typical problem instance
    does not benefit from the increase, or whenever the GPU memory is no longer large
    enough for a single data batch to fit on it during an iteration of the algorithm—whichever
    happens first. Additionally, when dealing with a multi-GPU scenario, some evidence
    has been experimentally shown[¹](#pgfId-1086936) that the optimal scaling-up schedule
    of the batch size is linear in the number of GPUs.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到——不深入进一步的细节，因为这将在第4章中讨论——在这种情况下，大多数可训练参数（大约26万个）来自于我们添加在自定义ELMo模型之上的层。这是我们第一个使用迁移学习的实例：在ELMo的创建者共享的预训练模型之上学习一对新的层。对于大多数神经网络实验而言，使用强大的GPU是非常重要的，并且`batch_size`参数的值（指定每一步向GPU输入的数据量）对于收敛速度非常重要。它将根据所使用的GPU或缺少GPU而有所不同。在实践中，可以增加这个参数的值，直到典型问题实例的收敛速度不再因增加而获益，或者GPU的内存在算法的一次迭代中已经不足以容纳单个数据批次为止。此外，在处理多GPU的情况下，已经实验证明[¹](#pgfId-1086936)批大小的最佳扩展计划与GPU的数量呈线性关系。
- en: On a free NVIDIA Tesla K80 GPU via a Kaggle Kernel (see our companion GitHub
    repository[²](#pgfId-1086941) for Kaggle notebook links), we achieve the performance
    on our email dataset for the first five epochs as shown in figure 3.3 for a typical
    run. We found a `batch_size` of 32 to work well for us in that context.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Kaggle Kernel（请看我们的配套GitHub存储库[²](#pgfId-1086941)获得Kaggle笔记本链接）上的一个免费的NVIDIA
    Tesla K80 GPU，在我们的电子邮件数据集上，我们在图3.3中显示了前五个时代的典型运行性能。我们发现`batch_size`为32在那个环境中能很好地工作。
- en: '![03_03](../Images/03_03.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![03_03](../Images/03_03.png)'
- en: Figure 3.3 Convergence of the validation and training accuracy scores for the
    first five epochs of training the ELMo model on the email classification example
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 显示了在电子邮件分类示例中训练ELMo模型的前五个时代的验证和训练准确性比分的趋势。
- en: Each epoch takes approximately 10 seconds to complete—this information is printed
    by our code. We see that a validation accuracy of approximately 97.3% is attained
    at the fourth epoch (meaning the results were reached in under a minute). This
    performance is comparable to the performance of the logistic regression approach,
    which is only slightly better at 97.7% (see also table 3.1). We note that the
    behavior of the algorithm is stochastic—it behaves differently from run to run.
    Thus, your own convergence will vary somewhat, even on architecture similar to
    what we used. It is typical in practice to try the algorithm run a few times and
    pick the best set of parameters among the stochastic and varying results attained.
    Finally, we note that the divergence of training and validation accuracies is
    suggestive of the beginning of overfitting, as indicated in the figure. This lends
    credence to the hypothesis that increasing the amount of signal by increasing
    the length of tokens, as specified by the hyperparameter `maxtokenlen`, and the
    number of tokens per email, as specified by `maxtokens`, may increase performance
    further. Naturally, increasing the number of samples per class by cranking up
    `Nsamp` should also work to improve performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时代需要大约10秒来完成——这些信息是由我们的代码打印的。我们看到第四个时代达到了约97.3%的验证准确性（意味着在不到一分钟的时间内达到了结果）。这个性能与Logistic
    Regression方法的性能相当，后者只稍好一点，为97.7%（也见表3.1）。我们注意到这个算法的行为是随机的——它的运行在每次运行时都表现出不同的行为。因此，即使在与我们使用的类似的架构上，你自己的收敛性也会有所不同。实践中通常尝试几次运行算法，并在随机和不同的结果中选择最佳的参数集。最后，我们注意到训练和验证准确性的背离表明了过拟合的开始，正如图中所示的那样。这证实了将通过增加超参数`maxtokenlen`指定的标记长度以及通过`maxtokens`指定的每封电子邮件的标记数量来增加信号量的数量可能会进一步提高性能的假设。自然地，通过打开`Nsamp`来增加每类样本的数量也应该有助于提高性能。
- en: For the IMDB example, the ELMo model code yields the convergence output shown
    in figure 3.4.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IMDB示例，ELMo模型代码产生了图3.4所示的收敛输出。
- en: '![03_04](../Images/03_04.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![03_04](../Images/03_04.png)'
- en: Figure 3.4 Convergence of the validation and training accuracy scores for the
    first five epochs of training the ELMo model on the IMDB movie review classification
    example
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 显示了在IMDB电影评论分类示例上训练ELMo模型的前五个时代的验证和训练准确性比分的趋势。
- en: Each epoch again takes approximately 10 seconds and achieves a validation accuracy
    of approximately 70% in under a minute at the second epoch. We will see how to
    improve the performances of these models in the next and final sections of this
    chapter. Note that some evidence of overfitting can be observed at the third and
    later epochs, as the training accuracy continues to improve—the fit to the data
    improves, whereas the validation accuracy remains lower.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时代再次需要大约10秒，并且在第二个时代不到一分钟的时间内就实现了约70%左右的验证准确性。我们将看到如何在本章的下一个和最后一个部分提高这些模型的性能。请注意，在第三个以及之后的时代，可以观察到一些过拟合的证据，因为训练准确性继续提高——对数据的拟合改善了，而验证准确性仍然较低。
- en: 3.2.2 Bidirectional Encoder Representations from Transformers (BERT)
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 双向编码器表示来自转换（BERT）
- en: The Bidirectional Encoder Representations from Transformers (BERT) model was
    also named after a popular *Sesame Street* character as a nod to the trend started
    by ELMo. At the time of writing, its variants achieve some of the best performance
    in transferring pretrained language model knowledge to downstream NLP tasks. The
    model was similarly trained to predict words in a sequence of words, although
    the exact *masking* procedure is somewhat different and will be discussed in detail
    later in the book. It can also be done in an unsupervised manner on very large
    corpuses, and the resulting weights similarly generalize to a variety of other
    NLP tasks. Arguably, to familiarize yourself with transfer learning in NLP, it
    is indispensable to also familiarize yourself with BERT.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 双向编码器表示来自变换器（BERT）模型也是以流行的*Sesame Street*角色命名的，以向ELMo开始的趋势致敬。在撰写本文时，其变体在将预训练语言模型知识转移到下游自然语言处理任务方面取得了一些最佳性能。该模型同样被训练来预测词语序列中的词语，尽管确切的*masking*过程略有不同，将在本书后面详细讨论。它也可以在非常大的语料库上以无监督的方式进行，并且生成的权重同样适用于各种其他自然语言处理任务。可以说，要熟悉自然语言处理中的迁移学习，熟悉BERT也是不可或缺的。
- en: Just as we did with ELMo, we will avoid discussing the architecture of this
    deep learning model in complete detail in this section—we will cover that in a
    subsequent chapter. It will suffice to mention here that the model employs character-level
    convolutions to build up preliminary embeddings of word tokens, followed by transformer-based
    encoders with self-attention layers that provide the model with a context of surrounding
    words. The transformer functionally replaced the role of the bidirectional LSTMs
    employed by ELMo. Recalling from chapter 1 that transformers have some advantages
    over LSTMs with respect to training scalability, we see some of the motivation
    behind this model. Again, we will use Keras with a TensorFlow backend to build
    our model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们对ELMo所做的那样，在本节中我们将避免完全详细讨论这个深度学习模型的架构——我们将在后面的章节中涵盖这个话题。在这里提一下，模型利用字符级卷积来构建词元的初步嵌入，然后是基于变换器的编码器，其中包含自注意层，为模型提供周围单词的上下文。变换器在功能上取代了ELMo所采用的双向LSTM的作用。回顾第1章中，变换器相对于LSTM在训练可扩展性方面具有一些优势，我们可以看到这个模型背后的一些动机。同样，我们将使用带有TensorFlow后端的Keras来构建我们的模型。
- en: Having briefly introduced BERT, let’s proceed to train it for each of the two
    running example datasets. The BERT model is also available through the TensorFlow
    Hub. To make the hub model usable by Keras, we similarly define a custom Keras
    layer that instantiates it in the right format, as shown in the next listing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 简要介绍了BERT之后，让我们继续为两个运行示例数据集中的每一个训练它。BERT模型也可以通过TensorFlow Hub获得。为了使hub模型能够被Keras使用，我们同样定义了一个自定义Keras层，以正确的格式实例化它，如下一个清单所示。
- en: Listing 3.7 Instantiating TensorFlow Hub BERT as a custom Keras layer
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用自定义 Keras 层实例化 TensorFlow Hub BERT](https://wiki.example.org/instantiating_tensorflow_hub_bert_as_custom_keras_layer)'
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Default number of top layers to unfreeze for training
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 默认要解冻的顶层数量进行训练
- en: ❷ Choice of regularization type
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 正则化类型的选择
- en: ❸ Pretrained model to use; this is the large, uncased original version of the
    model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 要使用的预训练模型；这是模型的大型、不区分大小写的原始版本。
- en: ❹ BERT embedding dimension, that is, the size of the resulting output semantic
    vectors
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ BERT嵌入维度，即生成的输出语义向量的大小
- en: ❺ Removes unused layers
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 移除未使用的层
- en: ❻ Enforces the number of unfrozen layers to fine-tune
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 强制执行要微调的解冻层的数量
- en: ❼ Trainable weights
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 可训练权重
- en: ❽ Inputs to BERT take a very specific triplet form; we will show how to generate
    it in the next listing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 输入到BERT采用非常特定的三元组形式；我们将在下一个清单中展示如何生成它。
- en: ❾ BERT “masks” some words and then attempts to predict them as learning target.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ BERT“masks”一些词语，然后尝试将它们预测为学习目标。
- en: Similar to what we did for ELMo in the previous subsection, we perform a sequence
    of analogous postprocessing steps on the data from the prior sections to put it
    into the format required by the BERT model. In addition to what we did in listing
    3.5 to concatenate the bag-of-words token representations into a list of strings,
    we subsequently need to convert each concatenated string into three arrays—*input
    IDs*, *input masks**,* and *segment IDs*—prior to feeding them to the BERT model.
    The code for doing this is shown in listing 3.8\. Having converted the data into
    the right format, we use the remaining code in the same listing 3.8 to build and
    train the Keras BERT TensorFlow Hub model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在前一小节为 ELMo 所做的工作，我们对前几节的数据执行一系列类似的后处理步骤，将其放入 BERT 模型所需的格式中。除了我们在列表 3.5
    中所做的将词袋标记表示连接成字符串列表之外，我们随后需要将每个连接的字符串转换为三个数组——*输入 ID*、*输入掩码* 和 *段 ID*——然后再将它们馈送到
    BERT 模型中。这样做的代码在列表 3.8 中显示。将数据转换为正确格式后，我们使用同一列表 3.8 中的剩余代码构建和训练 Keras BERT TensorFlow
    Hub 模型。
- en: Listing 3.8 Converting data to form expected by BERT, building and training
    model
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.8 将数据转换为 BERT 所期望的格式，构建和训练模型
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Function for building model
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于构建模型的函数
- en: ❷ We do not retrain any BERT layers but rather use the pretrained model as an
    embedding and retrain some new layers on top of it.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们不重新训练任何 BERT 层，而是将预训练模型用作嵌入，并在其上重新训练一些新层。
- en: ❸ Vanilla TensorFlow initialization calls
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Vanilla TensorFlow 初始化调用
- en: ❹ Creates a compatible tokenizer using a function in the BERT source repository
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 BERT 源代码库中的函数创建兼容的分词器
- en: ❺ Converts data to the “InputExample” format using a function in the BERT source
    repository
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 BERT 源代码库中的函数将数据转换为“InputExample”格式
- en: ❻ Converts the InputExample format into the triplicate final BERT input format,
    using a function in the BERT source repository
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用 BERT 源代码库中的函数将 InputExample 格式转换为最终的 BERT 输入格式
- en: ❼ Builds the model
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 构建模型
- en: ❽ Instantiates the variables
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 实例化变量
- en: ❾ Trains the model
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 训练模型
- en: Similar to the ELMo model we built in the previous subsection, we put a pair
    of layers on top of the pretrained model and train only those, which amounts to
    about 200 thousand parameters. With hyperparameters set at comparable values with
    all of the prior methods, we achieved validation accuracies of approximately 98.3%
    and 71% for the email and movie review classification problems, respectively (within
    five epochs).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在前一小节中构建的 ELMo 模型，我们在预训练模型之上放置了一对层，并且仅对这些层进行训练，这大约有 20 万个参数。通过将超参数设置为与之前的所有方法相当的值，我们在电子邮件和电影评论分类问题上分别获得了约为
    98.3% 和 71% 的验证准确率（在五个时期内）。
- en: 3.3 Optimizing performance
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 优化性能
- en: In looking at the performance results of the various algorithms from the previous
    sections of the chapter, as well as those from the previous chapter, we might
    be tempted to make conclusions right away about which algorithm is best-performing
    for each problem we looked at. For instance, we may conclude that BERT and logistic
    regression are the best algorithms for the email classification problem, with
    an accuracy of around 98%, whereas ELMo is not that far behind, followed by the
    decision-tree-based methods and SVM in last place. On the other hand, for the
    IMDB movie review classification problem, BERT appears to be the winner with a
    performance of approximately 71%, followed by ELMo, and only then logistic regression.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看本章前几节以及上一章的各种算法的性能结果时，我们可能会立即得出关于我们研究的每个问题中哪种算法表现最佳的结论。例如，我们可能会得出结论，对于电子邮件分类问题，BERT
    和逻辑回归是最佳算法，准确率约为 98%，而 ELMo 的准确率也不远，然后是基于决策树的方法和 SVM 排在最后。另一方面，对于 IMDB 电影评论分类问题，BERT
    看起来是赢家，性能约为 71%，其次是 ELMo，然后才是逻辑回归。
- en: We must remember, however, that we know this to be true for sure only at the
    *hyperparameter* *settings* at which we initially evaluated the algorithms—`Nsamp`
    `=` `1000,` `maxtokens` `=` `50,` `maxtokenlen` `=` `20`—in addition to any algorithm-specific
    default parameter values. To make general statements with confidence, we need
    to explore the space of hyperparameters more thoroughly by evaluating the performance
    of all algorithms at many hyperparameter settings, a process typically referred
    to as *hyperparameter tuning* *or optimization*. It may be that the best performance
    found through this process for each algorithm will change their performance ranking,
    and in general, this will help us achieve better accuracies for our problems of
    interest.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们必须记住，我们只有在最初评估算法时才能确定这一点是真实的——`Nsamp`=`1000`，`maxtokens`=`50`，`maxtokenlen`=`20`——以及任何特定于算法的默认参数值。要有信心地作出一般性的陈述，我们需要更彻底地探索超参数空间，通过在许多超参数设置下评估所有算法的性能，这个过程通常称为*超参数调优*或*优化*。也许通过这个过程找到的每个算法的最佳性能会改变它们的性能排名，而且一般来说，这将帮助我们为我们感兴趣的问题获得更好的准确性。
- en: 3.3.1 Manual hyperparameter tuning
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 手动超参数调整
- en: Hyperparameter tuning is often initially performed manually, driven by intuition.
    We describe such an approach here for the hyperparameters `Nsamp`, `maxtokens`
    `and` `maxtokenlen`, which are general across all the algorithms.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优通常最初是通过直觉进行手动操作的。我们在这里描述了这样一种方法，用于超参数`Nsamp`、`maxtokens`和`maxtokenlen`，这些超参数在所有算法中都是通用的。
- en: Let’s first assume that the initial amount of data trained—with `Nsamp`*=1000*,
    for example—is all the data we have. We hypothesize that if we increase the number
    of tokens in the data for each document—`maxtokens`—and increase the maximum length
    of any such token—`maxtokenlen`—we would be able to increase the amount of signal
    for making the classification decision and, thereby, the resulting accuracy.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先假设初始训练数据量——比如`Nsamp`=1000——就是我们拥有的全部数据。我们假设，如果我们增加每个文档中的数据令牌数量——`maxtokens`——并增加任何此类令牌的最大长度——`maxtokenlen`——我们将能够增加用于做出分类决策的信号量，从而提高结果的准确性。
- en: For the email classification problem, we first increase both of these, from
    values of 50 and 20, respectively, to 100 each. Accuracy results for doing this
    for logistic regression (LR), support vector machines (SVMs), random forests (RFs),
    gradient-boosting machines (GBMs), ELMo, and BERT are shown in second row of table
    3.1\. Furthermore, we increase `maxtokens` to 200 to yield the results in the
    third row of table 3.1.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电子邮件分类问题，我们首先将这两个值从分别为 50 和 20 的值增加到 100。对于逻辑回归（LR）、支持向量机（SVM）、随机森林（RF）、梯度提升机（GBM）、ELMo
    和 BERT 执行此操作的准确性结果显示在表 3.1 的第二行。此外，我们将`maxtokens`增加到 200，以得到表 3.1 的第三行的结果。
- en: Table 3.1 Comparison of algorithm accuracies at different general hyperparameter
    settings explored during the manual tuning process for the email classification
    example
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 比较了手动调整过程中探索的电子邮件分类示例的不同通用超参数设置下算法的准确性
- en: '| General hyperparameter settings | LR | SVM | RF | GBM | ELMo | BERT |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 通用超参数设置 | LR | SVM | RF | GBM | ELMo | BERT |'
- en: '| Nsamp = 1000 maxtokens = 50 maxtokenlen = 20 | 97.7% | 70.2% | 94.5% | 94.2%
    | 97.3% | 98.3% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Nsamp = 1000 maxtokens = 50 maxtokenlen = 20 | 97.7% | 70.2% | 94.5% | 94.2%
    | 97.3% | 98.3% |'
- en: '| Nsamp = 1000 maxtokens = 100 maxtokenlen = 100 | 99.2% | 72.3% | 97.2% |
    97.3% | 98.2% | 98.8% |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Nsamp = 1000 maxtokens = 100 maxtokenlen = 100 | 99.2% | 72.3% | 97.2% |
    97.3% | 98.2% | 98.8% |'
- en: '| Nsamp = 1000, maxtokens = 200, maxtokenlen = 100 | 98.7% | 90.0% | 97.7%
    | 97.2% | 99.7% | 98.8% |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Nsamp = 1000, maxtokens = 200, maxtokenlen = 100 | 98.7% | 90.0% | 97.7%
    | 97.2% | 99.7% | 98.8% |'
- en: We see based on this that, although SVMs is clearly the worst-performing classifier
    for this problem, logistic regression, ELMo, and BERT can achieve nearly perfect
    performance. Note also that ELMo is the clear winner in the presence of more signal—something
    we would have missed without the optimization step. However, the simplicity and
    speed of logistic regression would likely result in it being picked as the classifier
    of choice for this email classification problem in production.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个结果，我们可以看到，虽然 SVM 显然是这个问题中表现最差的分类器，但 logistic 回归、ELMo 和 BERT 几乎可以达到完美的性能。还要注意，ELMo
    在更多信号存在时表现最好——这是我们在没有优化步骤的情况下可能会错过的东西。但是，logistic 回归的简单性和速度可能导致它被选为此电子邮件分类问题的生产中首选的分类器。
- en: We now repeat a similar sequence of hyperparameter testing steps for the IMDB
    movie review classification problem. We first increase `maxtokens` and `maxtokenlen`
    to 100 each and then increase `maxtokens` further to 200\. The resulting algorithm
    performances are listed in table 3.2, along with the performances at the initial
    hyperparameter settings.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对IMDB电影评论分类问题进行了类似的超参数测试步骤。我们首先将`maxtokens`和`maxtokenlen`都增加到100，然后将`maxtokens`进一步增加到200。得到的算法性能列在表3.2中，同时列出了初始超参数设置时的性能。
- en: Table 3.2 Comparison of algorithm accuracies at different general hyperparameter
    settings explored during the manual tuning process for the IMDB movie review classification
    example
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 IMDB电影评论分类示例中手动调整过程中探索的不同通用超参数设置的算法准确度比较
- en: '| General hyperparameter settings | LR | SVM | RF | GBM | ELMo | BERT |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 通用超参数设置 | 逻辑回归 | 支持向量机 | 随机森林 | 梯度提升机 | ELMo | BERT |'
- en: '| Nsamp = 1000 maxtokens = 50 maxtokenlen = 20 | 69.1% | 66.0% | 63.9% | 67.0%
    | 69.7% | 71.0% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Nsamp = 1000 maxtokens = 50 maxtokenlen = 20 | 69.1% | 66.0% | 63.9% | 67.0%
    | 69.7% | 71.0% |'
- en: '| Nsamp = 1000 maxtokens = 100 maxtokenlen = 100 | 74.3% | 72.5% | 70.0% |
    72.0% | 75.2% | 79.1% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Nsamp = 1000 maxtokens = 100 maxtokenlen = 100 | 74.3% | 72.5% | 70.0% |
    72.0% | 75.2% | 79.1% |'
- en: '| Nsamp = 1000 maxtokens = 200 maxtokenlen = 100 | 79.0% | 78.3% | 67.2% |
    77.5% | 77.7% | 81.0% |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Nsamp = 1000 maxtokens = 200 maxtokenlen = 100 | 79.0% | 78.3% | 67.2% |
    77.5% | 77.7% | 81.0% |'
- en: BERT appears to be the best model for this problem across the board, followed
    by ELMo and logistic regression. Observe that this problem has more headroom for
    improvement, consistent with our earlier observation that this problem is more
    difficult than the email classification one. This leads us to hypothesize that
    pretrained knowledge transfer has more of an effect on harder problems, which
    makes intuitive sense. This concept is also consistent with general advice that
    stipulates that neural network models are likely to be preferable to other approaches
    when significant labeled data is available, assuming the problem to be solved
    is complex enough for the additional data to be needed in the first place.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，BERT似乎是最佳模型，其次是ELMo和逻辑回归。注意，这个问题有更多的改进空间，这与我们早期观察到的这个问题比电子邮件分类问题更难的观察一致。这使我们假设，预训练知识转移对更难的问题有更大的影响，这是直观的。这个概念也符合一般建议，即在有大量标记数据可用时，神经网络模型可能优于其他方法，假设要解决的问题足够复杂，需要额外的数据。
- en: 3.3.2 Systematic hyperparameter tuning
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 系统化超参数调整
- en: A number of tools exist for more systematic and exhaustive hyperparameter searches
    on ranges of hyperparameters. These include the Python methods `GridSearchCV`,
    which performs an exhaustive search over a specified parameter grid, and `HyperOpt,`
    which does a random search over parameter ranges. Here, we present code for using
    `GridSearchCV` to tune an algorithm of choice as an illustrative example. Note
    that we tune only some internal algorithm-specific hyperparameters in this exercise,
    with the general ones we tuned in the last subsection fixed, for simplicity of
    illustration.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些工具用于对超参数范围进行更系统化和全面的搜索。这些包括Python方法`GridSearchCV`，它对指定的参数网格执行全面搜索，以及`HyperOpt`，它在参数范围上进行随机搜索。在这里，我们提供了使用`GridSearchCV`来调整所选算法的代码，作为一个说明性示例。请注意，在这个练习中，我们只调整了一些特定于算法的内部超参数，而将上一小节中我们调整的通用超参数固定，以简化说明。
- en: We pick email classification with RF with the initial general hyperparameter
    settings as our illustrative example. The reason for this choice is that it takes
    about a second for each fit of this algorithm on this problem, and because the
    grid search will perform a lot of fits, this example can be executed quickly for
    the greatest learning value for the reader.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用初始通用超参数设置的RF进行电子邮件分类作为我们的说明性示例。之所以做出这个选择，是因为对于这个问题的每次拟合大约需要一秒钟，由于网格搜索将执行大量的拟合，这个例子可以快速执行，以便读者获得最大的学习价值。
- en: 'We first import the required method and check which RF hyperparameters are
    available for tuning as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需方法，并检查RF超参数可用于调整如下：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ GridSearchCV scikit-learn import statement
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ GridSearchCV scikit-learn导入语句
- en: ❷ clf is the RF classifier from listing 2.13.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ clf是列表2.13中的RF分类器。
- en: 'This yields the following output:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We pick three of these hyperparameters to search over and specify three values
    for each of them, as shown next:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了其中三个超参数进行搜索，并为每个参数指定了三个值，如下所示：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then carry out the grid search using the following code, making sure to
    print out final test accuracy and best hyperparameter values:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用以下代码进行网格搜索，确保打印出最终的测试准确性和最佳的超参数值：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Defines the grid search object with a specified hyperparameter grid
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用指定的超参数网格定义网格搜索对象
- en: ❷ Fits the grid search to the data
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将网格搜索适配到数据
- en: ❸ Displays the results
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 显示结果
- en: 'This experiment required training the classifier at 3*3*3=27 points, because
    each of the three hyperparameter grids has three requested points on it. The overall
    experiment took less than five minutes to complete and yielded an accuracy of
    95.7%. This is an improvement of more than 1% over the original score of 94.5%.
    The raw output from the code is shown next, specifying best hyperparameter values:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验需要在3*3*3=27个点上训练分类器，因为每个超参数网格上有三个请求的点。整个实验不到五分钟就完成了，并且准确率达到了95.7%。这比原始得分94.5%提高了超过1%。代码的原始输出如下，指定了最佳的超参数值：
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Indeed, when we performed the tuning across the board on all classifiers, we
    found that we could boost the performance of each by 1-2%, without affecting the
    conclusions on the best classifier for each problem reached in the previous subsection.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，当我们在所有分类器上进行全面调整时，我们发现可以将每个分类器的性能提升1-2%，而不会影响在前一小节中达到的每个问题的最佳分类器的结论。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: It is typical to try a variety of algorithms on any given problem of interest
    to find the best combination of model complexity and performance for your circumstances.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常会尝试多种算法来解决感兴趣的任何问题，以找到模型复杂性和性能的最佳组合，以适应您的情况。
- en: Baselines usually start with the simplest algorithms, such as logistic regression,
    and become increasingly complex until the right performance/complexity trade-off
    is attained.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线通常从最简单的算法开始，例如逻辑回归，然后逐渐变得更复杂，直到达到正确的性能/复杂性折衷。
- en: Important model design choices include metrics for evaluating performance, loss
    functions to guide the training algorithm, and best validation practices, among
    many others, and these can vary by model and problem type.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的模型设计选择包括用于评估性能的指标，用于指导训练算法的损失函数以及最佳验证实践，等等，这些可以根据模型和问题类型而异。
- en: Hyperparameter tuning is an important step of the model-development pipeline,
    because initial hyperparameter settings may severely misrepresent the best attainable
    performance that can be found by tuning it.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整是模型开发流程的重要步骤，因为初始超参数设置可能严重误代表通过调整可以找到的最佳性能。
- en: Simple models tend to work best when the amount of available data isn’t very
    large and/or for easier problems, whereas complex neural network models tend to
    do better, and as such be worth the extra complexity, when more data is available.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单模型在可用数据量不大和/或问题较简单时往往效果最佳，而复杂的神经网络模型在有更多数据可用时往往表现更好，因此值得额外复杂性，当更多数据可用时。
- en: '1. P. Goyal et al., “Accurate, Large Minibatch SGD: Training ImageNet in 1
    Hour,” *arXhiv* (2018).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 1. P. Goyal等人，“准确的、大型小批次SGD：在1小时内训练ImageNet”，*arXhiv*（2018年）。
- en: 2. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 2. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
