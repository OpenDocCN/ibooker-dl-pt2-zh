- en: Appendix A. Introduction to machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A. 机器学习简介
- en: 'When discussing machine learning with newcomers to the field, I find that many
    have picked up on the basics but found the quantity of information about the topic
    and the depth of the mathematics intimidating. I remember that when I was just
    starting out with machine learning, I had a similar experience: it felt like there
    was just too much to learn. This appendix is designed for those who may have attempted
    to understand machine learning from a patchwork of tutorials or a few online courses.
    In the appendix, I organize basic machine learning concepts into a big picture
    and explain how the concepts fit together so that you have enough of a review
    of the basics to attempt the project in this book. Whenever possible I am going
    to present machine learning concepts intuitively, keeping mathematical notation
    to a minimum. My goal is not to replace comprehensive coursework on machine learning
    or deep-dive blog posts; instead, I’d like to show you the most important, most
    salient parts of machine learning needed for practical applications.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在和机器学习领域的新手讨论时，我发现很多人已经掌握了基础知识，但是他们觉得机器学习领域的信息量和数学深度让人望而生畏。我还记得当我刚开始学习机器学习的时候，我有过类似的经历：感觉需要学的东西太多了。这个附录是为那些可能已经通过一些教程或几个在线课程试图理解机器学习的人准备的。在这个附录中，我将基础的机器学习概念整理成一个完整的框架，并解释它们如何组合在一起，以便你对基础有足够的回顾，可以尝试本书中的项目。在可能的情况下，我将直观地介绍机器学习概念，并尽量少使用数学符号。我的目标不是要替代机器学习的全面课程或深度博客文章；相反，我想向你展示机器学习中最重要、最显著的部分，以供实际应用。
- en: Novice students of machine learning often start their educational journey with
    in-depth study of machine learning algorithms. This is a mistake. Machine learning
    algorithms enable solutions to problems, and the full understanding of the problems
    that are suitable for machine learning should come first. As a machine learning
    practitioner (e.g., as a machine learning engineer or a data scientist), you are
    going to be expected to understand your customer’s business problem and decide
    whether it can be recast as a machine learning problem. So, section A.1 through
    section A.3 introduce you to the fundamentals of machine learning and cover the
    most common machine learning use cases for structured data sets. Starting with
    section A.4 and through the conclusion of this appendix, I introduce you to the
    machine learning algorithms that can be used to solve machine learning problems
    as well as to the details about how to apply the algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习初学者往往会从深入研究机器学习算法开始他们的学习之旅，这是错误的。机器学习算法可以用于解决问题，但首先应理解适合机器学习的问题。作为机器学习从业者（例如机器学习工程师或数据科学家），你需要了解客户的业务问题，并决定它是否可以重组为一个机器学习问题。因此，A.1节到A.3节介绍机器学习的基础知识，并涵盖结构化数据集的最常见机器学习用例。从第A.4节开始并通过本附录的结论，我将向你介绍可以用于解决机器学习问题的机器学习算法以及有关如何应用算法的细节。
- en: A.1 Why machine learning?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.1 为什么要学机器学习？
- en: If you are reading this book, chances are that you are at least willing to consider
    machine learning as a topic of study or perhaps even as a solution to your problem.
    However, is machine learning the right technology for you to study or to use?
    When does applying machine learning make sense? Even if you are interested in
    machine learning you may find the barrier to entry (which is substantial) intimidating
    and decide against putting in the effort needed to understand machine learning
    in enough depth to apply the technology. In the past, numerous technologies came
    to market claiming to “change everything” but failed to deliver on the promise.
    Is machine learning destined to capture the headlines for a few years and then
    fade away into obscurity? Or is there something different about machine learning?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这本书，那么你至少愿意考虑把机器学习作为研究课题甚至解决问题的方法。但是，机器学习是你学习或使用的正确技术吗？什么情况下应用机器学习才有意义？即使你对机器学习很感兴趣，你可能会发现，进入门槛（是相当高的）让人望而生畏，决定不付出真正理解机器学习所需的深度努力来应用这项技术。过去，很多技术曾经在市场上声称要“改变一切”，但最终没有兑现承诺。机器学习是否注定成为头条新闻几年后就消失在人们的记忆里？还是说有些东西与众不同？
- en: On the surface, machine learning can appear quite ordinary to the users of contemporary
    computer software and hardware. Machine learning depends on human beings who write
    code, and the code in turn depends on information technology resources such as
    compute, storage, networking, as well as the input and output interfaces. However,
    to gain a perspective on the magnitude of the change brought by machine learning
    to the field of computing, it is useful to revisit the time computing went through
    a transformation at a similar scale.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上看，机器学习对于当代计算机软件和硬件的用户可能看起来非常普通。机器学习依赖于编写代码的人类，并且代码又依赖于信息技术资源，如计算、存储、网络以及输入和输出接口。然而，要了解机器学习为计算机领域带来的变革之巨大，回顾计算机经历了类似规模的转型的时刻是很有用的。
- en: You may find it surprising to read in a machine learning book about a 1940s
    “computer” for mathematical computations. That is, if you don’t know that prior
    to the invention and a broad adoption of electronic and digital computers in the
    1950s, the term *computer* was used to describe a human being who performed mathematical
    computations, often in concert with a group of other “computers.” A photograph
    of a computer team from 1949 is shown in figure A.1.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读关于 1940 年代进行数学计算的“计算机”的机器学习书籍可能会让你感到惊讶。这是因为如果你不知道在 1950 年代电子和数字计算机的发明和广泛采用之前，术语*计算机*被用来描述执行数学计算的人类，通常是与其他“计算机”组合工作。图
    A.1 显示了 1949 年的一个计算机团队的照片。
- en: '![A-01](Images/A-01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![A-01](Images/A-01.png)'
- en: Figure A.1 An office of human computers at work in the Dryden Flight Research
    Center Facilities during the summer of 1949\. (This photograph is in the public
    domain; more information is available from [http://mng.bz/XrBv](http://mng.bz/XrBv).)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.1 1949 年夏季，人类计算机在德雷登飞行研究中心设施的办公室里工作。（此照片属于公共领域；更多信息请访问 [http://mng.bz/XrBv](http://mng.bz/XrBv)。）
- en: At its core, computing is about programs (algorithms) that use data to answer
    questions. Prior to the broad deployment of digital computers in the 1950—60s,
    computers (the human beings) played key roles in answering computational questions.
    For assistance in this job, the human computers often relied on external computing
    tools that ranged from pen and paper to early calculators or punch card-based
    tabulating machines. In this paradigm of computing, the computing instructions,
    the program describing how to compute, remained in the human computers’ minds
    (left side of figure A.2).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，计算是关于使用数据回答问题的程序（算法）。在 1950—60 年代数字计算机广泛部署之前，计算机（人类）在回答计算问题中扮演了关键角色。在这项工作中，人类计算机通常依靠从纸和笔到早期计算器或基于打孔卡的制表机等外部计算工具。在这种计算范式中，计算指令，描述如何计算的程序，仍然存储在人类计算机的脑海中（图
    A.2 的左侧）。
- en: '![A-02](Images/A-02.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![A-02](Images/A-02.png)'
- en: 'Figure A.2 Human computers relied on devices to store the data used in computing.
    The devices ranged from pen and paper to electro-mechanical calculators and even
    punch card—based tabulating machines. However, none of these had an internal memory,
    which was used for the storage and execution of computing instructions, in other
    words, the program code (left side of the figure). In contrast, the von Neumann
    architecture for computing, which used computer device memory for both the data
    and the computer instructions, created a transformative practice of computer programming:
    the transfer of instructions for how to compute (the program) to the memory of
    the computing device for storage and execution (right side of the figure).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.2 人类计算机依靠设备来存储计算中使用的数据。这些设备范围从纸和笔到电机计算器，甚至是基于打孔卡的制表机。然而，这些设备都没有内部存储器，用于存储和执行计算指令，换句话说，程序代码（图的左侧）。相比之下，冯·诺伊曼计算机架构，它将计算机设备内存用于数据和计算机指令，创造了一种革命性的计算机编程实践：将用于计算的指令（程序）传输到计算设备的存储器中以供存储和执行（图的右侧）。
- en: The modern computers transformed this computing paradigm by changing the role
    of the human beings in relation to the computing devices. Instead of storing the
    program with the computing instructions in the human mind, human programmers took
    on the job of entering the programs into the memory of the computing devices in
    the form of code, or machine-readable instructions for how to compute (right side
    of figure A.2).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现代计算机通过改变人类与计算设备的关系角色而转变了这种计算范式。人类程序员不再将计算指令与计算机程序存储在人类思维中，而是将程序以代码或可机读的指令形式输入到计算设备的内存中（图
    A.2 右侧）。
- en: The von Neumann architecture changed the global economy at a scale comparable
    to the transformations brought about by the Industrial Revolution. Nearly every
    contemporary computing device on the planet, from pocket-sized mobile phones to
    massive servers powering cloud-computing data centers, uses the von Neumann architecture.
    The field of artificial intelligence became possible once computation transitioned
    away from the paradigm, where instructions for computing were stored in the biological
    minds of human computers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 冯·诺依曼结构以与工业革命带来的变革相媲美的规模改变了全球经济。几乎地球上每一台当代计算设备，从袖珍式的移动电话到驱动云计算数据中心的大型服务器，都使用冯·诺依曼结构。人工智能领域的出现是一旦计算转移到了从前的范式，即计算指令存储在人类计算机的生物思维中，才变得可能的。
- en: The von Neumann computing paradigm also produced notable breakthroughs for the
    field of artificial intelligence; for example, DeepBlue, the first chess program
    to defeat the human chess champion Garry Kasparov, was built by IBM based on the
    paradigm. Despite this and many other successes, the hard-crafted programs produced
    by human programmers proved too simplistic for many subfields of artificial intelligence,
    including computer vision, speech recognition, natural language understanding,
    and many others. The code programmed by humans to perform tasks such as classifying
    objects in digital images or recognizing speech ended up too inaccurate and too
    brittle for broad adoption.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 冯·诺依曼计算范式还为人工智能领域带来了显著突破；例如，IBM 基于该范式构建的 DeepBlue 是第一个击败人类国际象棋冠军加里·卡斯帕罗夫的国际象棋程序。尽管如此，人类程序员编写的硬编码程序对人工智能的许多子领域（包括计算机视觉、语音识别、自然语言理解等）来说过于简单了。人类编程人员编写的用于执行诸如数字图像中对象分类或语音识别等任务的代码最终过于不准确且脆弱，无法广泛采用。
- en: Contemporary machine learning is changing the relationship of the programmer
    to the modern computing devices at a level that is as fundamental as the transformation
    that happened with computing in the 1950s. Instead of programming a computer,
    a machine learning practitioner trains a machine learning system (using a machine
    learning algorithm) with bespoke data sets to produce a machine learning model
    (figure A.3). Since a machine learning model is just computer code, a machine
    learning algorithm can empower a machine learning practitioner with the ability
    to produce code capable of computing answers to questions that are beyond the
    capacity of human-generated programs. For instance, in the 2010s machine learning
    models were used to classify images with superhuman performance, recognize human
    speech so effectively that many households installed speech-recognizing digital
    assistants (such as Amazon Alexa and Google Home), and defeat Lee Sedol, the human
    champion at the ancient board game of Go.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当代机器学习正在以与上世纪五十年代计算机革命相当的基本程度改变程序员与现代计算设备的关系。机器学习从业者不再编写计算机程序，而是使用定制的数据集训练机器学习系统（使用机器学习算法），以生成机器学习模型（见图
    A.3）。由于机器学习模型只是计算机代码，机器学习算法可以赋予机器学习从业者产生能够计算超出人类编写程序能力的问题答案的代码的能力。例如，在 2010 年代，机器学习模型被用于以超人类的表现分类图像，如此有效地识别人类语音，以至于许多家庭安装了语音识别数字助理（如亚马逊的
    Alexa 和谷歌的 Home），并击败了李世石，古老棋类游戏围棋的人类冠军。
- en: '![A-03](Images/A-03.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![A-03](Images/A-03.png)'
- en: Figure A.3 Machine learning relies on a machine learning practitioner who uses
    a machine learning algorithm to “train” a machine learning model based on a bespoke
    data set. Although a trained machine learning model is just code created by the
    machine learning algorithm, the model can answer questions that are too complex
    for a human to manually program as code. For example, machine learning models
    can classify objects in digital images or recognize human speech in digital audio
    better than handcrafted code developed by human programmers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.3 机器学习依赖于机器学习从业者，后者使用机器学习算法根据定制数据集来“训练”机器学习模型。虽然训练好的机器学习模型只是由机器学习算法创建的代码，但该模型可以回答人类无法手动编程的复杂问题。例如，机器学习模型可以比由人类程序员开发的手工编码更好地对数字图像中的对象进行分类，或者识别数字音频中的人类语音。
- en: This appendix introduces machine learning as a subfield of computer science
    focused on using computers to learn from data. Although this definition is accurate,
    it does not fully communicate the importance and the lasting effect of machine
    learning on transforming the field of computing. At the other end of the spectrum,
    marketing slogans about machine learning as “the new electricity” or the enabler
    of artificial general intelligence sensationalize and obfuscate the field. It
    is clear that machine learning is changing the parts of the computing architecture
    that remained fundamentally static from the 1950s to 2010s. The extent to which
    machine learning will transform computing is unclear. I hope you find the uncertainty
    and the potential role that you can play in this transformation as exciting as
    I do!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录将机器学习介绍为计算机科学的一个子领域，专注于使用计算机从数据中学习。尽管这个定义是准确的，但它并没有完全传达机器学习对计算机领域转型的重要性和持久影响。在另一端，关于机器学习的营销口号，如“新的电力”或人工通用智能的实现者，夸大了和模糊了这个领域。显然，机器学习正在改变从
    1950 年代到 2010 年代基本保持不变的计算架构的部分。机器学习将如何改变计算尚不清楚。我希望您能像我一样对这种变革的不确定性和您可以在其中扮演的潜在角色感到兴奋！
- en: A.2 Machine learning at first glance
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.2 乍一看的机器学习
- en: This section introduces you to the changes brought to traditional computer science
    by the machine learning algorithms, illustrates machine learning with an easy-to-understand
    example, and describes how to implement the machine learning example using the
    Python programming language and the pandas, Numpy, and scikit-learn libraries.
    By the conclusion of this section you should be prepared to explain basic machine
    learning concepts and use the concepts with simple machine learning examples.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了机器学习算法对传统计算机科学带来的变革，用易于理解的例子说明了机器学习，并描述了如何使用 Python 编程语言和 pandas、Numpy
    和 scikit-learn 库来实现机器学习示例。通过本节的介绍，您应该能够解释基本的机器学习概念，并将这些概念与简单的机器学习示例结合使用。
- en: 'Before the advent of machine learning, the traditional computer science algorithms[¹](#pgfId-1011940)
    had focused on computing answers on the basis of what is known: the data. Machine
    learning expanded the field of computer science with algorithms that use data
    to compute answers to questions on the basis of what is possible but is not known:
    the uncertain.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习出现之前，传统计算机科学算法[¹](#pgfId-1011940)主要关注于根据已知数据计算答案。机器学习通过使用数据计算答案来回答基于可能但未知的内容的问题，从而扩展了计算机科学领域。
- en: To illustrate the essence of the change machine learning brought to computer
    science, suppose you are working with the following easy-to-understand data set
    describing Ford Mustangs manufactured from 1971 through 1982 and their fuel efficiency[²](#pgfId-1018783)
    in terms of miles per gallon (mpg) fuel consumption. With the Python programming
    language and the pandas library for structured data,[³](#pgfId-1012332) you can
    prepare this data set for analysis by instantiating it in the computer memory
    as a pandas data structure called a DataFrame.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明机器学习对计算机科学带来的变革的实质，假设您正在处理以下易于理解的数据集，描述了从 1971 年到 1982 年制造的福特野马及其燃油效率[²](#pgfId-1018783)，以每加仑英里数（mpg）的燃油消耗。使用
    Python 编程语言和用于结构化数据的 pandas 库，您可以通过在计算机内存中将其实例化为 pandas 数据结构来准备该数据集以进行分析，称之为 DataFrame。
- en: Listing A.1 Listing A.1 Creating a data set in memory as a pandas DataFrame
    to be ready for analysis
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 A.1 在内存中创建一个准备好进行分析的 pandas DataFrame 数据集](#)'
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Import the pandas library and give it the alias pd.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 pandas 库并将其别名为 pd。
- en: ❷ Import the NumPy library and give it the alias np.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入 NumPy 库并将其别名设为 np。
- en: ❸ The pandas DataFrame for storing and managing structured data can be constructed
    from a list of Python dictionaries such that each row is specified using a dictionary
    instance with the data frame column names as the keys and the rows’ contents as
    the dictionary values.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于存储和管理结构化数据的 pandas DataFrame 可以通过使用包含每行数据的 Python 字典列表构建，其中每行使用字典实例指定，数据框列名为键，行内容为字典值。
- en: ❹ To avoid printing the default, zero-based index for each row, df.to_string(index=False)
    is used instead of print(df).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为了避免列印每行的默认零基索引，使用 df.to_string(index=False) 代替 print(df)。
- en: This produces the output, shown as a table on the left side of figure A.4.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这样会产生一个输出，显示在图 A.4 的左侧作为表格。
- en: '![A-04](Images/A-04.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![A-04](Images/A-04.png)'
- en: 'Figure A.4 Ford Mustang fuel efficiency data set (left) and the corresponding
    scatter plot (right) based on the same data set. (This publicly available data
    set is sourced from University of California Irvine Machine Learning Repository:
    [https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG).)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.4 中福特野马燃油效率数据集（左）和对应的散点图（右），基于同一数据集。（这个公开的数据集是从加州大学尔湾分校机器学习库获取的：[https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG).)
- en: It should come as no surprise that well-known computer science algorithms and
    data structures (e.g., a hash table) can be used to answer questions about what
    is known from this data set, such as the mpg efficiency of a Ford Mustang with
    a weight of 2,905 pounds. With pandas this question can be answered using
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，众所周知，计算机科学中的算法和数据结构（例如哈希表）可以用于回答关于数据集中已知内容的问题，比如一辆重 2,905 磅的福特野马的每加仑油耗效率。使用
    pandas 可以回答这个问题
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'which outputs a NumPy[⁴](#pgfId-1012833) array with a single element corresponding
    to the value of the mpg column and the fourth row of the data set from listing
    A.1:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出一个 NumPy[⁴](#pgfId-1012833) 数组，其中包含了 mpg 列的单个元素，以及列表 A.1 中数据集的第四行的值：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: pandas DataFrame
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: pandas DataFrame
- en: pandas is used throughout this appendix to illustrate common operations on machine
    learning data sets. While a pandas DataFrame is an easy-to-learn and easy-to-use
    data structure, it does not scale to data sets that do not fit into the memory
    of a single node (a computer on a network). Further, pandas was not designed for
    data analysis with distributed computing clusters like those that are available
    in cloud computing environments from major cloud providers. This appendix will
    continue using pandas DataFrames to introduce machine learning; however, the rest
    of the book focuses on SQL tables and PySpark DataFrames that scale to much larger
    data sets than pandas. Many concepts about pandas DataFrames apply directly to
    PySpark and SQL. For example, the descriptions of structured data sets and supervised
    machine learning in section A.3 apply to data sets regardless of whether they
    are managed using pandas, PySpark, or SQL.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录中使用 pandas 来演示对机器学习数据集的常见操作。虽然 pandas DataFrame 是一个易学易用的数据结构，但它无法处理不适合单个节点内存（网络上的计算机）的数据集。此外，pandas
    并不是为那些可以在主要云计算提供商的云计算环境中使用分布式计算集群进行数据分析设计的。本附录将继续使用 pandas DataFrames 来介绍机器学习；然而，本书的其余部分重点关注可以处理比
    pandas 更大数据集的 SQL 表和 PySpark DataFrames 。许多关于 pandas DataFrames 的概念直接适用于 PySpark
    和 SQL。例如，第 A.3 节关于结构化数据集和监督机器学习的描述适用于不管是使用 pandas、PySpark 还是 SQL 管理的数据集。
- en: What about the fuel efficiency of a 3,000-pound Ford Mustang? Answering this
    question was outside the scope of the traditional computer science algorithms
    prior to the advent of machine learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么一辆重 3,000 磅的福特野马的油耗如何？在机器学习出现之前，传统计算机科学算法无法回答这个问题。
- en: 'Nonetheless, as a human being, you can observe the data set (the right side
    of figure A.4) and notice a *pattern* (a recurring rule) across the related Ford
    Mustangs described by the data set: as the weight of the vehicles increases, their
    fuel efficiency decreases. If asked to estimate fuel effiency of a 3,000-pound
    Ford Mustang (which is not specified by the data set), you can apply your mental
    model of the pattern to estimate an answer of roughly 20 miles per gallon.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为一个人类，你可以观察数据集（图 A.4 的右侧）并注意到数据集描述的相关福特野马之间的一个*模式*（一个重复的规律）：随着车辆重量的增加，它们的燃油效率下降。如果要估计一辆重
    3,000 磅的福特野马的燃油效率（数据集并未给出），你可以将这个模式的心理模型应用到估计答案，大约为每加仑 20 英里。
- en: Given the right machine learning algorithm, a computer can learn a software-based
    model (known as a machine learning model, to be defined more precisely in section
    A.3) of the data set, such that the learned (also known as trained) model can
    output estimates much like the mental model you intuited to estimate the fuel
    efficiency of the 3,000-pound Ford Mustang.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定正确的机器学习算法，计算机可以学习数据集的软件模型（称为机器学习模型，将在第 A.3 节中更准确地定义），使得学习到的（也称为训练的）模型可以输出估计值，就像你推断出来的估计
    3,000 磅福特野马燃油效率的心理模型一样。
- en: scikit-learn, a popular machine learning library,[⁵](#pgfId-1012919) includes
    a variety of ready-to-use machine learning algorithms, including several that
    can construct a machine learning model of the pattern you have observed in the
    data set. The steps to create the model using a machine learning algorithm known
    as *linear regression* based on the values just from the weight column are shown.[⁶](#pgfId-1012958)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn，一种流行的机器学习库，[⁵](#pgfId-1012919) 包括各种可供使用的机器学习算法，包括几种可以构建与数据集中观察到的模式相符的机器学习模型的算法。根据仅从重量列中的值构建模型的步骤显示如下。[⁶](#pgfId-1012958)
- en: Listing A.2 A simple model of the Ford Mustang data set
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: A.2 列出了福特野马数据集的简单模型
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Import the linear regression implementation from the scikit-learn library.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 scikit-learn 库导入线性回归实现。
- en: ❷ Create an instance of a linear regression machine learning model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建线性回归机器学习模型的实例。
- en: ❸ Train (fit) the linear regression model using the weight column from the Ford
    Mustang data set as the model input and the mpg values as the model output. The
    reshape function reshapes the NumPy array returned by df['weight'].values to a
    matrix consisting of a single column. The reshape is needed here due to the scikit-learn
    requirement for the model input to be structured as a matrix.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用福特野马数据集中的重量列作为模型输入和 mpg 值作为模型输出来训练（拟合）线性回归模型。reshape 函数将 df['weight'].values
    返回的 NumPy 数组重新塑形为由单列组成的矩阵。由于 scikit-learn 要求模型输入为结构化矩阵，因此在这里需要重新塑形。
- en: The linear regression algorithm, widely considered one of the fundamental algorithms
    in machine learning,[⁷](#pgfId-1013152) *trains* (i.e., “fits”) the machine learning
    model instance based on the data set passed to the fit method of the LinearRegression
    class. Once trained by the fit method, the model instance can answer questions
    such as “What is the estimated fuel efficiency of a 3,000-pound Ford Mustang?”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归算法被广泛认为是机器学习中的基本算法之一，[⁷](#pgfId-1013152) *训练*（即“拟合”）机器学习模型实例基于传递给 LinearRegression
    类的数据集。一旦通过 fit 方法训练，模型实例就可以回答诸如“一辆 3,000 磅的福特野马的预计燃油效率是多少？”这样的问题。
- en: Listing A.3 Using the trained linear regression model to estimate mpg
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: A.3 使用训练过的线性回归模型来估计 mpg
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Reshape an array containing a single value of 3,000 (representing a 3,000-pound
    Ford Mustang) into a matrix with one row and one column and ask the model to estimate
    the output value using the predict method. Since the output of the predict method
    is returned as a NumPy array, retrieve the first element from the array of the
    estimated values using a [0] index.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将包含单个值 3,000（代表一辆 3,000 磅的福特野马）的数组重新塑形为一个具有一行一列的矩阵，并使用预测方法要求模型估计输出值。由于预测方法的输出以
    NumPy 数组的形式返回，因此使用 [0] 索引从估计值的数组中检索第一个元素。
- en: This outputs
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: which represents the estimate of roughly 20.03 miles per gallon (MPG). The machine
    learning model can also produce estimates for other values of the vehicle weight.
    Note that the code that follows is more straightforward and thus easier to understand
    for existing Python developers who are new to machine learning.[⁸](#pgfId-1013369)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表着大约 20.03 英里每加仑（MPG）的估计值。机器学习模型还可以为车辆重量的其他值产生估计值。请注意，随后的代码对于已经熟悉 Python 但对机器学习新手来说更加简单直观，因此更容易理解。[⁸](#pgfId-1013369)
- en: Listing A.4 Estimating MPG for vehicles weighing 2,500, 3,000, and 3,500 pounds
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: A.4 列出了对重量为 2,500、3,000 和 3,500 磅的车辆进行 MPG 估计的情况
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '❶ The Python for expression iterates over weight values from the [2_500, 3_000,
    3_500] list. For each weight in the list, the expression returns a row of a matrix
    consisting of two columns: the left column with the value of mpg predicted by
    the model for the weight, and the right column with the value of the weight itself.
    The resulting matrix is stored in the ds variable.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Python for 表达式遍历来自列表 `[2_500, 3_000, 3_500]` 的重量值。对于列表中的每个重量，表达式返回一个由两列组成的矩阵的行：左列是模型预测的
    mpg 值，右列是重量本身的值。生成的矩阵存储在变量 ds 中。
- en: ❷ The pandas DataFrame is instantiated using the ds matrix and annotated with
    column names mpg_est and weight for the left and right columns, respectively.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 ds 矩阵实例化 pandas DataFrame，并以 mpg_est 和 weight 作为左右列的列名进行注释。
- en: ❸ To avoid printing the default, zero-based index for each row, df.to_string(index=False)
    is used instead of print(df).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为了避免打印每一行的默认从零开始的索引，使用 `df.to_string(index=False)` 替代 `print(df)`。
- en: This outputs the results shown as a pandas DataFrame on the left side of figure
    A.5\. The right side of figure A.5 shows that the model learned by LinearRegression
    from the original data set has a dashed line and can be used to estimate mpg values
    for arbitrary values of weight.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出如图 A.5 左侧显示的作为 pandas DataFrame 的结果。图 A.5 右侧显示了由 LinearRegression 从原始数据集学习的模型具有虚线，可用于估计任意重量值的
    mpg 值。
- en: '![A-05](Images/A-05.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![A-05](Images/A-05.png)'
- en: Figure A.5 A table of the estimated fuel efficiency mpg for hypothetical Ford
    Mustangs with the weight values given by the weight column (left) and the corresponding
    linear model (right) plotted by connecting the data points from the table
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.5 一个估计的 Ford Mustang 虚构燃油效率 mpg 的表格，其中给定的重量值由重量列（左侧）给出，并由连接表中数据点的线性模型（右侧）绘制
- en: In this section, you learned from an example where the machine learning problem,
    the data set, and the machine learning algorithm were prepared for you in advance.
    This meant that you did not have to fully understand the nuances of the problem,
    how to prepare the data set for the problem, or how to choose the right machine
    learning algorithm to solve the problem. In the rest of the appendix, you will
    explore these dimensions of working with machine learning in greater depth and
    prepare to apply your understanding to the machine learning project in this book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你通过一个示例学到了机器学习问题、数据集和机器学习算法是如何事先准备好的。这意味着你不必完全理解问题的细微之处，如何为问题准备数据集，或者如何选择合适的机器学习算法来解决问题。在附录的其余部分，你将更深入地探索与机器学习工作的这些方面，并准备将你的理解应用到本书中的机器学习项目中去。
- en: A.3 Machine learning with structured data sets
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.3 结构化数据集的机器学习
- en: In the previous section, you were introduced to an application of machine learning
    using an example data set describing the fuel efficiency of Ford Mustangs. This
    section teaches the concepts needed to apply machine learning across arbitrary
    structured data sets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，你已经了解了使用描述 Ford Mustang 燃油效率的示例数据集应用机器学习的实例。本节将教授应用机器学习到任意结构化数据集所需的概念。
- en: For the purposes of this appendix, a *structured data set* stores observations[⁹](#pgfId-1013710)
    about
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本附录的目的，*结构化数据集*存储了关于
- en: Related objects, for example cars of different makes and models, publicly traded
    companies in the S&P 500 index, iris flowers of different sub-species, or
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关对象，例如不同品牌和型号的汽车、标准普尔 500 指数中的上市公司、不同亚种的鸢尾花，或
- en: Recurring events, such as won or lost sales opportunities, on-time or late meal
    deliveries, or button clicks on a website
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复事件，例如赢得或失去的销售机会、按时或延迟的餐食送达或网站上的按钮点击
- en: as records (usually rows) in a table, where each *record* consists of numeric
    values organized as at least two but typically three or more columns of the table.
    Note that figure A.6 illustrates a structured data set with N observations (records
    in rows) and M columns, as well as the related terminology explained later in
    this section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为表格中的记录（通常是行），其中每个*记录*由至少两列但通常是三列或更多列的数字值组成。请注意，图 A.6 展示了一个具有 N 个观测值（以行表示的记录）和
    M 列的结构化数据集，以及本节后面解释的相关术语。
- en: '![A-06](Images/A-06.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![A-06](Images/A-06.png)'
- en: Figure A.6 A conceptual representation of a structured data set for machine
    learning. The data set is organized into N rows of observations such that each
    row has an observation label column along with M - 1 (the remaining) columns of
    features. By convention, the first column is often the observation label. In cases
    when observations are about events (e.g., labeling whether a sales opportunity
    was won or lost), the label is sometimes called the *outcome* of the event. When
    using the data set for machine learning, the label is often described as the *truth,
    target*, or *actual* value. Variables y and X are commonly used in machine learning
    to denote the label and the features, respectively.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.6 机器学习用的结构化数据集的概念表示。数据集被组织成N行的观察，每一行都有一个观察的标签列，以及其他M-1列（剩下的）特征列。按照惯例，第一列通常是观察标签。在有关事件的观察中（例如标记销售机会是否成功），标签有时被称为事件的“结果”。在使用数据集进行机器学习时，标签通常被描述为“实际值”、“目标”或“真值”。在机器学习中，变量y和X通常用来表示标签和特征。
- en: A *supervised machine learning algorithm* (e.g., the linear regression algorithm
    used in section A.2) for a structured data set trains (outputs) a machine learning
    model, which can estimate the value for one of the columns in a record (the label)
    using the values from the other columns in the record (the features).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 用于结构化数据集的监督式机器学习算法（例如第A.2节中使用的线性回归算法）训练（输出）一个机器学习模型，该模型可以使用记录中的其他列的值（特征）来估计记录中某列（标签）的值。
- en: In supervised machine learning, a *label* is the numeric value used as the target[^(10)](#pgfId-1013877)
    for the estimates produced by a machine learning model during training. By convention,
    machine learning practitioners often use the first column of a structured machine
    learning data set to store the values for the label (often designating it using
    the variable y) and the remaining columns (using variable X) to store the features.
    The *features* are the numeric values used by a supervised machine learning model
    to estimate the label value. When a collection of records consisting of labels
    and features is used to train a machine learning model, the records are described
    as a *training data set*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督式机器学习中，“标签”是机器学习模型在训练过程中用作估计结果的数值。按照惯例，机器学习从业者通常使用结构化机器学习数据集的第一列来存储标签的值（通常使用变量y进行指示），并使用其余列（使用变量X进行指示）存储特征。特征是监督式机器学习模型用来估计标签值的数值。当使用由标签和特征组成的记录集来训练机器学习模型时，这些记录被描述为“训练数据集”。
- en: The term *label* is ambiguous in machine learning
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “标签”这个术语在机器学习中具有歧义。
- en: Unfortunately, the use of the word *label* isn’t consistent in the machine learning
    community, leading to much confusion for newcomers to machine learning. While
    the use of the word is frequent when the observations in a structured data set
    describe related objects (e.g., Ford Mustangs), the word *outcome* is used synonymously
    with label when the observations in the data set describe events (e.g., sales
    opportunities). Machine learning practitioners with a statistics background often
    resort to describing the label as the *dependent variable* and the features as
    the *independent variables*. Others use *target value* or *actual value* synonymously
    with *label*. This book aims to simplify the terminology and uses the word *label*
    whenever possible.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在机器学习社区中，对于“标签”一词的使用并不一致，这给机器学习的初学者带来了很多困惑。尽管在结构化数据集中，当观察值描述相关对象（例如福特野马）时，该词频繁使用，但当数据集中的观察值描述事件（例如销售机会）时，与“标签”同义的词为“结果”。具有统计学背景的机器学习从业者通常将标签描述为“因变量”，将特征描述为“自变量”。其他人将“目标值”或“实际值”与“标签”同义使用。本书旨在简化术语，并尽可能使用“标签”这个词。
- en: The field of machine learning is much broader than supervised machine learning
    and includes topics like unsupervised machine learning (where the label is not
    used or not available), reinforcement learning (where algorithms seek to maximize
    a reward), generative adversarial networks (where neural nets compete to generate
    and classify data), and more. However, even at Google, arguably a leader in adopting
    machine learning, more than 80% of the machine learning models that are put into
    production are based on supervised machine learning algorithms using structured
    data. Hence, this book focuses entirely on this important area of machine learning.[^(11)](#pgfId-1014116)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域远比监督式机器学习更广泛，包括无监督机器学习（标签不被使用或不可用）、强化学习（算法寻求最大化奖励）、生成对抗网络（神经网络竞争生成和分类数据）等等。然而，即使在谷歌这样一个在机器学习应用上领先的公司，超过80%的投入生产的机器学习模型都是基于使用结构化数据的监督式机器学习算法。因此，本书完全专注于这个重要的机器学习领域。[^(11)](#pgfId-1014116)
- en: Mathematical definition of supervised machine learning
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 监督式机器学习的数学定义
- en: 'Although it is not required for this book, here’s a more formal definition
    of supervised machine learning: if *y*[i] is the value to be estimated from a
    record with an index i, then a supervised machine learning model can be described
    as a function F that outputs the estimate *F*(*X[i]*) based on values *X*[i] of
    the remaining (i.e., other than *y*[i]) columns in the record. The training process
    for a supervised machine learning model describes construction of the function
    F based on a training data set *y*, *X*. The training algorithm is often iterative
    using *y*, *X*, *F*(*X*), where the base *F*(*X*) is produced from some random
    initialization of F.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书不要求这样做，但是这里有一个更正式的监督式机器学习定义：如果*y*[i]是要从索引i的记录中估计的值，则监督式机器学习模型可以被描述为一个函数F，它基于记录中除*y*[i]之外的列（即其他列）的值*X*[i]输出估计值*F*(*X[i]*)。监督式机器学习模型的训练过程描述了基于训练数据集*y*,
    *X*构建函数F的过程。训练算法通常使用*y*, *X*, *F*(*X*)进行迭代，其中基础*F*(*X*)是从F的某个随机初始化生成的。
- en: 'For an illustration of supervised machine learning, recall that in section
    A.2 you learned about a structured data set describing the fuel efficiency of
    Ford Mustangs. The training data set consisted of just two columns: a label with
    the mpg values, and a single feature with the value of the vehicle weight. This
    data set is also shown as an example in figure A.7\. The corresponding supervised
    machine learning model based on the data set can estimate the average fuel efficiency
    in miles per gallon (mpg column) for 1971 to 1982 models of Ford Mustangs, based
    on the values from the weight column.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明监督式机器学习，回想一下在A.2节中您学到的描述福特野马燃油效率的结构化数据集。训练数据集仅包含两列：一个标签列，其中包含mpg值，和一个带有车辆重量值的单个特征。该数据集也作为示例在图A.7中显示。基于该数据集的相应监督式机器学习模型可以根据车重列的值估计1971年到1982年款福特野马的平均燃油效率（mpg列）。
- en: '![A-07](Images/A-07.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![A-07](Images/A-07.png)'
- en: Figure A.7 A sample data set of Ford Mustang fuel efficiency in terms of miles
    per gallon.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.7 是一份关于福特野马燃油效率的样本数据集，以每加仑英里数为单位。
- en: 'With most machine learning algorithms, training is an iterative process (illustrated
    in figure A.8) that starts with a machine learning algorithm producing the first
    iteration of a machine learning model. Some algorithms use the training data set
    to create the first iteration of the model, but this is not required: most neural
    network-based deep learning models are initialized according to a simple random
    scheme.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法都是一个迭代过程的训练过程（如图A.8所示），从使用机器学习算法生成第一个迭代的机器学习模型开始。一些算法使用训练数据集创建模型的第一个迭代，但这不是必需的：大多数基于神经网络的深度学习模型是根据简单的随机方案进行初始化的。
- en: Once the first iteration of the model is ready, it is used to output the first
    iteration of the estimates (predictions) based on the features from the training
    data set. Next, the quality of the estimates is evaluated by the machine learning
    algorithm by comparing how close the estimates are to the labels from the training
    data set. This quantifiable measure used to evaluate the quality (i.e., the performance)
    of the machine learning model is known as *loss* (also known as the *cost* or
    *objective function*) and is covered in more detail in section A.4\. For the next
    iteration of the process, the loss along with the training data set are used by
    the algorithm to produce the next iteration of the model. The non-iterative machine
    learning algorithms can output a machine learning model after a single iteration
    of the process shown in figure A.8.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型的第一次迭代完成，它就会根据训练数据集中的特征输出第一次迭代的估计值（预测）。 接下来，机器学习算法通过比较估计值与训练数据集中的标签的接近程度来评估估计的质量。
    用于评估机器学习模型质量（即性能）的可量化措施称为*损失*（也称为*成本*或*目标函数*），在第 A.4 节中有更详细的介绍。 对于流程的下一次迭代，算法使用损失以及训练数据集来生成下一个模型的迭代。
    在图 A.8 中显示的过程的单次迭代后，非迭代机器学习算法可以输出一个机器学习模型。
- en: '![A-08](Images/A-08.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![A-08](Images/A-08.png)'
- en: Figure A.8 The initial machine learning model produced by the machine learning
    algorithm is used to output estimated label values (y_est), estimates per record
    based on the records’ feature values. The machine learning model is then improved
    iteratively by changing the model (specifically the model parameters) to improve
    the model performance score (loss), which is based on the comparison of the estimated
    and label values.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.8 机器学习算法产生的初始机器学习模型用于输出估计的标签值（y_est），这是基于记录的特征值的记录的估计。 然后通过更改模型（特别是模型参数）来迭代改进机器学习模型以改善模型性能得分（损失），该性能得分基于估计值和标签值的比较。
- en: Iterative machine learning algorithms vary in how they approach the decision
    to stop the training process; some have built-in criteria used to stop iterating,
    while others require the machine learning practitioner to provide explicit stopping
    criteria, or to provide a schedule or a function for deciding when to stop. Additional
    details on how to approach the stopping criteria when covering different machine
    learning algorithms starts in section A.4.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代机器学习算法在如何决定停止训练过程上有所不同； 一些具有内置标准用于停止迭代，而其他一些则要求机器学习从业者提供显式的停止标准，或者提供用于决定何时停止的时间表或函数。
    在涵盖不同机器学习算法的停止标准时，如何处理这些停止标准的其他细节始于第 A.4 节。
- en: So far, this appendix has used the phrase *numeric values* intuitively, without
    providing a clear definition of the numeric values suitable for machine learning.
    As mentioned in section A.1, machine learning algorithms require a custom prepared
    data set and may fail when used with arbitrary data values. Hence, it is imperative
    for a machine learning practitioner to have a clear understanding of the numeric
    values present in a structured data set. Illustrated in figure A.9 is a detailed
    taxonomy (originating from a similar taxonomy in statistics) for classifying numeric
    variables based on their values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本附录直观地使用了短语*数字值*，但没有提供适用于机器学习的数字值的清晰定义。 如第 A.1 节所述，机器学习算法需要准备定制的数据集，并且在使用任意数据值时可能会失败。
    因此，机器学习从业者必须清楚地了解结构化数据集中存在的数字值。 在图 A.9 中详细说明了基于值对数字变量进行分类的详细分类法（源自统计学中的类似分类法）。
- en: '![A-09](Images/A-09.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![A-09](Images/A-09.png)'
- en: Figure A.9 The categories of numeric values for supervised machine learning
    are adapted from a well-known framework for classifying statistical variables
    by Stanley Smith Stevens ([http://mng.bz/0w4z](http://mng.bz/0w4z)). Numeric values
    can be classified into mutually exclusive subsets of continuous and categorical
    values. Continuous values can be further classified as interval or ratio, while
    categorical can be either nominal or ordinal.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.9 数字值的类别适用于受监督机器学习，这是根据斯坦利·史密斯·史蒂文斯（[http://mng.bz/0w4z](http://mng.bz/0w4z)）对统计变量进行分类的著名框架改编而来。
    数字值可以被分类为互斥的连续值和分类值子集。 连续值可以进一步分类为区间或比率，而分类值可以是名义或有序的。
- en: This appendix and the rest of the book focus on machine learning with continuous
    and categorical variables, specifically using interval, ratio, and nominal data.
    Whenever possible, the book provides hints and tips for working with ordinal values;
    however, the project in the book does not cover any specific use cases for these
    two types of values. As a machine learning practitioner, you are expected to prepare
    and convert the messy, real-world data sets to the numeric values that can be
    correctly used by machine learning. Much of part 1 is dedicated to sharpening
    your skills in this domain.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '-   本附录和本书的其余部分侧重于具有连续和分类变量的机器学习，特别是使用间隔、比率和名义数据。本书在可能的情况下，为处理有序值提供提示和技巧；然而，本书中的项目不涵盖这两种值的任何具体用例。作为机器学习从业者，你需要准备并将混乱的现实世界数据集转换为机器学习能够正确使用的数值。第一部分的大部分内容都致力于磨练你在这个领域的技能。'
- en: In this section, you learned about training, the sequence of steps performed
    by a machine learning algorithm to produce a machine learning model. During training,
    a subset of the structured data set (known as the *training data set*) is used
    by the algorithm to produce the model, which can then be used to output the estimates
    (also known as *predictions*), given the feature values from a record.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在这一节中，你了解了训练，即机器学习算法执行的一系列步骤，以生成机器学习模型。在训练过程中，算法使用结构化数据集的一个子集（称为*训练数据集*）来生成模型，然后可以使用该模型输出估计值（也称为*预测*），给定记录的特征值。'
- en: A.4 Regression with structured data sets
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   A.4 结构化数据集的回归'
- en: 'In this section, you will learn about the two commonly used categories of supervised
    machine learning problems: regression and classification. The section introduces
    a definition of loss (also known as the cost or the objective function), a quantitative
    and technical measure of the performance of a machine learning model on a data
    set of labels and features. By the conclusion of the section, you will be familiar
    with the terminology related to the problems and review applications of regression.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在本节中，你将了解两种常用的监督式机器学习问题类别：回归和分类。本节介绍了损失的定义（也称为成本或目标函数），这是对机器学习模型在标签和特征数据集上的性能的定量和技术性度量。通过本节的结论，你将熟悉与这些问题相关的术语，并回顾回归应用。'
- en: '*Regression* for structured data sets is a supervised machine learning problem
    where the label is a continuous (as defined in figure A.9) variable. For example,
    when estimating Ford Mustang fuel efficiency in section A.2, you worked on an
    instance of a regression problem since mpg is a continuous (more precisely an
    interval) value. In section A.5, you will learn more about classification for
    structured data sets, a supervised machine learning problem where the label is
    a categorical variable. Comprehensive understanding of these machine learning
    problems is critical to a machine learning practitioner, since, as explained in
    section A.3, regression and classification make up over 80% of production machine
    learning models at top information technology companies like Google.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '-   结构化数据集的*回归*是一个监督式机器学习问题，其中标签是一个连续的（如图 A.9 中定义的）变量。例如，在第 A.2 节中估计福特野马的燃油效率时，你处理了一个回归问题的实例，因为
    mpg 是一个连续（更确切地说是一个间隔）值。在第 A.5 节中，你将了解更多关于结构化数据集的分类，这是一个监督式机器学习问题，其中标签是一个分类变量。对这些机器学习问题的全面理解对于机器学习从业者至关重要，因为正如在第
    A.3 节中解释的那样，回归和分类占据了像谷歌这样的顶级信息技术公司生产机器学习模型的80%以上。'
- en: An example of a regression problem and the related loss calculations based on
    the Ford Mustang data set is shown in figure A.10\. Recall that in section A.2
    you played the role of the machine learning algorithm and intuited a mental model
    for estimating the mpg value. Suppose that you reprise the same role in this section,
    but here you estimate the mpg value by taking the value of the weight and multiplying
    it by 0.005\. Since the training process is iterative, the value of 0.005 is just
    an initial (perhaps a lucky) but a reasonable guess. Better approaches for making
    the guesses will be introduced shortly. In the meantime, the values of the estimates
    based on this calculation are shown in the Estimate column of figure A.10.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据福特野马数据集的回归问题示例和相关损失计算，图 A.10 中显示了模型损失的平方误差。回想一下，在第 A.2 节中，您扮演了机器学习算法的角色，并推断了一个用于估计每加仑英里数（mpg）值的心理模型。假设您在本节中再次扮演相同的角色，但在这里，您通过取重量值并将其乘以
    0.005 来估计 mpg 值。由于训练过程是迭代的，0.005 的值只是一个初始（也许是幸运的）但合理的猜测。更好的猜测方法将很快介绍。与此同时，基于此计算的估计值显示在图
    A.10 的 Estimate 列中。
- en: '![A-10](Images/A-10.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![A-10](Images/A-10.png)'
- en: Figure A.10 In regression problems, many machine learning practitioners start
    with an application of the mean squared error loss function to establish a baseline
    before moving on to more complex machine learning approaches and experimenting
    with more complex loss functions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.10 在回归问题中，许多机器学习实践者首先应用均方误差损失函数来建立基线，然后再转向更复杂的机器学习方法并尝试更复杂的损失函数。
- en: Recall from the process explained in figure A.8 that the next step is to evaluate
    the loss, a quantifiable measure of the quality of the estimates produced by the
    machine learning model. The choice of the loss function depends on the machine
    learning problem and more precisely on the numeric type of both the label and
    estimated values in the problem.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 A.8 中解释的过程中回想一下，下一步是评估损失，这是机器学习模型产生的估计质量的可量化度量。损失函数的选择取决于机器学习问题，更精确地说是问题中标签和估计值的数字类型。
- en: In the regression problem, one of the most frequently used loss functions is
    the *mean squared error* (MSE), defined as the arithmetic mean of the individual
    squared error (also known as *residual* or *difference*) values, as illustrated
    in the Squared Error column of figure A.10.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，最常用的损失函数之一是 *均方误差*（MSE），定义为个别平方误差（也称为 *残差* 或 *差值*）值的算术平均值，如图 A.10 的 Squared
    Error 列所示。
- en: The Python code to derive the values shown in the columns of figure A.10 is
    provided.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 提供生成图 A.10 中各列值的 Python 代码。
- en: Listing A.5 The squared errors for the calculation of the model loss
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 A.5 计算模型损失的平方误差
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Instantiate the Ford Mustang data set as a pandas DataFrame. For brevity and
    following an accepted practice, this example uses y for the label and X for the
    feature(s). A more detailed explanation is available in listing A.1.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将福特野马数据集实例化为 pandas DataFrame。为简洁起见，并遵循已接受的做法，本示例使用 y 表示标签，X 表示特征。有关更详细的说明，请参阅清单
    A.1。
- en: ❷ The variable name W is often used to represent the values for the machine
    learning model parameters. Notice that the NumPy slicing notation [:, None] is
    equivalent to using reshape(1,1) to reshape W to a matrix needed in the next step.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 变量名 W 通常用于表示机器学习模型参数的值。注意，NumPy 的切片表示法 [:, None] 等效于使用 reshape(1,1) 将 W 重塑为下一步中所需的矩阵。
- en: ❸ The double-squared bracket notation used by the expression df[['X']] returns
    a matrix of feature values and the matrix product (using the @ operation), with
    the matrix containing the model parameter value producing the resulting matrix
    with a single column containing the weights multiplied by 0.005\. Matrix multiplication
    is used here since it easily scales to many features and model parameter values
    without having to change the implementation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 表达式 df[['X']] 使用的双方括号符号返回特征值矩阵，并使用 @ 操作进行矩阵乘积，其中包含生成结果的单列的模型参数值的矩阵，该列包含权重乘以
    0.005。在这里使用矩阵乘法，因为它可以轻松扩展到许多特征和模型参数值，而无需更改实现。
- en: ❹ The error is just the difference between the label and the estimated values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 误差只是标签与估计值之间的差异。
- en: ❺ The squared_error is calculated using Python ** exponentiation notation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 Python 中的 ** 指数表示法来计算平方误差。
- en: ❻ The list of the column names is specified to ensure that the order of the
    columns in the output corresponds to the order shown in figure A.10.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 列名列表被指定，以确保输出的列顺序与图 A.10 中显示的顺序相对应。
- en: Assuming the values for the squared error are stored in a pandas DataFrame column
    named squared_error, the corresponding value for MSE can be computed using simply
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设均方误差的值存储在名为 squared_error 的 pandas DataFrame 列中，则只需使用简单地计算即可得到 MSE 的相应值
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: which in the case of the values from figure A.10 outputs a number that is approximately
    equal to
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 A.10 中的值的情况下，会输出一个近似等于的数字
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you should expect, since the value of W was picked randomly, the value of
    the mean squared error is nowhere near zero. In figure A.11, you can explore the
    results of various random choices for W (subplot (a) corresponds to using 0.005
    as W) as well as the relationship between the random values of W and the corresponding
    mean squared error.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所期望的，由于 W 的值是随机选择的，均方误差的值远非零。在图 A.11 中，您可以探索对 W 的各种随机选择的结果（子图（a）对应于使用0.005作为
    W），以及随机值 W 和相应均方误差之间的关系。
- en: '![A-11](Images/A-11.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![A-11](Images/A-11.png)'
- en: Figure A.11 Subplots a—d illustrate the impact of choosing alternative, randomly
    selected values of W on the mean squared error. Notice that in all cases, the
    value of W corresponds to the slope of the line that passes through the points
    specified by the pair of the feature and label values. Unlike the linear regression
    model used in figure A.5, the lines on this figure pass through the origin and
    hence do not capture the pattern of lower weight corresponding to higher miles
    per gallon in fuel efficiency.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.11 子图 a—d 说明了选择替代的、随机选择的 W 值对均方误差的影响。请注意，在所有情况下，W 的值对应于通过特征和标签值对的点确定的直线的斜率。与图
    A.5 中使用的线性回归模型不同，该图中的线经过原点，因此无法捕捉燃油效率中更低重量对应的模式。
- en: Since the line-based (linear) model based on W is so simple, instead of randomly
    guessing the value of W, you can rely on an analytical solution for the problem
    of estimating W that minimizes the mean squared error for the data set. The analytical
    solution known as the *ordinary least squares* (OLS) *formula* (X^TX)^(-1)X^Ty
    can be implemented using Python code.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于 W 的基于线性的模型非常简单，所以不需要随机猜测 W 的值，可以依靠解决估计最小化数据集均方误差的问题的分析解。这个分析解被称为*普通最小二乘法*（OLS）*公式*（X^TX)^(-1)X^Ty，可以使用
    Python 代码来实现。
- en: Listing A.6 Ordinary least squares solution for linear regression
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 A.6 线性回归的普通最小二乘法解
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Assign X as a NumPy array of the feature values.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 X 分配为特征值的 NumPy 数组。
- en: ❷ Assign y as a NumPy array of the label values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将 y 分配为标签值的 NumPy 数组。
- en: ❸ Compute the expression X.T @ X from the OLS formula, convert it to a NumPy
    matrix (using ndmin=2), and invert the resulting matrix using np.linalg.inv.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算OLS公式中的X.T @ X 表达式，将其转换为 NumPy 矩阵（使用ndmin=2），并使用 np.linalg.inv 倒置所得矩阵。
- en: ❹ Multiply the inverted matrix by X.T @ y from the OLS formula.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用OLS公式中的X.T @ y 乘以倒置矩阵。
- en: 'This returns a 1 × 1 matrix:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回一个 1 × 1 的矩阵：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can confirm, using the sample code from listing A.5, that when using the
    optimal value of W based on the OLS formula produces MSE of 40.88. What does this
    mean? Note that unlike the LinearRegression model shown in figure A.5, the model
    based only on W is not complex enough to capture the underlying pattern in the
    data: the heavier weight leads to lower fuel efficiency. Of course, just by visual
    examination of the subplots in figure A.11 the reason is obvious: a line based
    on a single W parameter must pass though the origin (i.e., the y intercept is
    zero); hence, it is impossible to use it to model the inverse relationship between
    the greater-than-zero values in the mpg and weight data columns.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用列表 A.5 中的示例代码确认，当使用基于 OLS 公式的最优 W 值时，会产生 40.88 的 MSE。这意味着什么？请注意，与图 A.5
    中显示的 LinearRegression 模型不同，仅基于 W 的模型不够复杂，无法捕捉数据中的基本模式：较大的权值会导致较低的燃油效率。当然，仅通过对图
    A.11 中的子图进行目测检查，原因是显而易见的：基于单个 W 参数的线必须经过原点（即 y 截距为零）；因此，不可能使用它来模拟 mpg 和 weight
    数据列中大于零值之间的倒数关系。
- en: However, when working with more complex data sets, where the data has too many
    dimensions to be easily visualized, a visualization does not help with the decision
    about whether the model is sufficiently flexible to capture the desired patterns
    in the data set. Instead of relying on a visualization, you can perform additional
    tests to evaluate the flexibility of the model. In a regression problem, you can
    compare your model to the mean squared error of estimating the label values using
    the mean label value from the training data set. For example, using the DataFrame
    with the training data set, this can be done by evaluating
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当处理更复杂的数据集时，数据维度过多以至于难以进行可视化，可视化对于判断模型是否足够灵活以捕捉到数据集中期望的模式没有帮助。除了依赖可视化，你可以进行额外的测试来评估模型的灵活性。在回归问题中，你可以将你的模型与平均标签值估计的均方误差进行比较，使用来自训练数据集的平均标签值。例如，使用训练数据集的
    DataFrame，可以通过如下方式进行评估：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: which should output approximately
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 应该得到一个大约如下的输出
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The difference between the optimal model MSE of 40.88 and the naive MSE (using
    the average) of 27.03 shows that the model is not sufficiently complex (has too
    few parameters) to capture the desired pattern in the data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最优模型的均方误差为40.88，而使用平均值的简单均方误差为27.03，这表明该模型不够复杂（参数太少）以捕捉数据中期望的模式。
- en: A.5 Classification with structured data sets
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.5 结构化数据集的分类
- en: This section introduces you to and illustrates the cross-entropy loss function
    used by many machine learning algorithms to train classification models. With
    the understanding of cross-entropy in place, the section walks you through the
    steps needed to implement the one-hot encoding of labels and how to use the encoded
    labels to compute values of cross-entropy loss. Before concluding, the section
    teaches you the best practices for using NumPy, pandas, and scikit-learn so that
    you can train and evaluate a baseline LogisticRegression classification model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向您介绍并演示许多机器学习算法用于训练分类模型的交叉熵损失函数。在理解了交叉熵的基础上，本节将引导您完成实现标签的独热编码及如何使用编码后的标签计算交叉熵损失值的步骤。最后，本节将教您使用
    NumPy、pandas 和 scikit-learn 的最佳实践，以便您可以训练和评估一个基准的 LogisticRegression 分类模型。
- en: Recall from section A.4 that classification for structured data sets is a machine
    learning problem of estimating the value of a categorical label from the feature(s).
    For example, the Ford Mustang data set shown in figure A.12 can be used to train
    a classification model (also known as a *classifier*) to estimate the decade of
    the model year, 1970s versus 1980s, using the mpg (fuel efficiency) and weight
    features.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 A.4 节可以回顾到，对于结构化数据集的分类是一个机器学习问题，其目标是从特征中估计出一个分类标签的值。例如，图 A.12 中展示的福特野马数据集可以用来训练一个分类模型（也被称为*分类器*），来估计车型年份的十年代，可以选择1970年代和1980年代，利用
    mpg（燃油效率）和重量两个特征。
- en: '![A-12](Images/A-12.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![A-12](Images/A-12.png)'
- en: Figure A.12 The one-hot encoding of the model year label using columns named
    1970s and 1980s representing the Ford Mustang decade of the car’s make (left).
    The one-hot nature of the encoding refers to a single 1 value across every row
    in the columns used for the encoding; the remainder of the values are 0. A scatter
    plot (right) for the data set illustrates the vehicles from 1970s and 1980s using
    “x” and “•” markers, respectively. Given any location on the grid (not limited
    to the locations shown using the markers), a trained classification model must
    be able to estimate whether the vehicle was manufactured in the 1970s or 1980s.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.12 使用名为1970s和1980s的列对车型年份标签进行独热编码，用于表示福特野马车的十年代（左）。编码的独热性指的是在用于编码的列中，每一行都只有一个1值；其余值为0。数据集的散点图（右）以“x”和“•”标记分别表示1970年代和1980年代的车辆。对于网格上的任何位置（不限于标记所示的位置），一个经过训练的分类模型必须能够判断车辆是在1970年代还是1980年代生产的。
- en: Although the mean squared error loss function can be used for some classification
    problems,[^(12)](#pgfId-1015715) many baseline classifiers use the *cross-entropy
    loss*. Machine learning algorithms designed to optimize the cross-entropy loss
    include logistic regression (which is a machine learning algorithm for classification
    and should not be confused with the regression machine learning problem) and neural
    networks, among others. A closely related loss function known as *Gini impurity*
    is used by the popular decision tree and random forest algorithms. This section
    explains classification using the cross-entropy loss first, to prepare you to
    understand the variations on cross-entropy used by the more advanced classification
    machine learning algorithms.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然均方误差损失函数可用于一些分类问题，但许多基线分类器使用*交叉熵损失*。用于优化交叉熵损失的机器学习算法包括 logistic 回归（这是一种用于分类的机器学习算法，不应与回归机器学习问题混淆）和神经网络等。流行的决策树和随机森林算法使用的一个密切相关的损失函数称为*基尼不纯度*。本节首先解释使用交叉熵损失进行分类，以便为您理解更高级的分类机器学习算法所使用的交叉熵的变体做准备。
- en: Unlike the mean squared loss, which expects the output of a regression model
    to be a single numeric value, cross-entropy loss expects the classification model
    to output a probability for each possible value of the categorical label. Continuing
    with the working data set for estimation of the decade of Ford Mustang models,
    figure A.13 illustrates four informative examples of the outputs for a hypothetical
    classification model based on the data set.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与均方损失不同，均方误差损失期望回归模型的输出为单个数值，而交叉熵损失期望分类模型为分类标签的每个可能值输出一个概率。继续使用用于估计福特野马车型年代的工作数据集，图
    A.13 展示了基于数据集的假设分类模型的四个信息示例的输出。
- en: In example 1 shown on the upper left of figure A.13, the classification model
    assigns a 0.6 probability to the estimate of 1970s as the decade of the model
    year. Since probabilities must add up to 1, the estimate of 1980s is 0.4. In this
    example, the corresponding loss value (shown in the title of the example) of approximately
    0.51 is significantly greater than zero since the classification model estimates
    the correct value (1970s) but lacks confidence in the estimate. Observe from example
    2 on the upper right of figure A.13 that when the model is completely uncertain
    of the correct value, lacking confidence or preference to estimate either 1970s
    or 1980s (due to 0.5 probability for both),[^(13)](#pgfId-1015871) the loss increases
    further, reaching approximately 0.6931.[^(14)](#pgfId-1015894) In a nutshell,
    the cross-entropy loss function decreases toward zero when the classification
    model outputs a high probability (effectively a high confidence) value for the
    correct label estimate and increases otherwise.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 A.13 左上方显示的示例 1 中，分类模型将 1970 年代的估计概率为 0.6。由于概率必须加起来等于 1，因此 1980 年代的估计概率为
    0.4。在这个例子中，相应的损失值（显示在示例标题中）约为 0.51，这个值显著大于零，因为分类模型虽然估计了正确的值（1970 年代），但对估计缺乏信心。从图
    A.13 右上方的示例 2 中可以看出，当模型完全不确定正确的值，缺乏对 1970 年代或 1980 年代的估计信心或偏好（由于两者的概率均为 0.5）时，损失进一步增加，达到约
    0.6931。简而言之，当分类模型输出正确标签估计的高概率（实际上是高置信度）时，交叉熵损失函数向零减少，否则增加。
- en: The loss function increases even further, beyond the loss number reported in
    case of the complete uncertainty, if the classification model is incorrect in
    the estimation of the label value, as shown in example 3 on the lower left of
    figure A.13\. In this example, the correct label value is 1980s while the model
    is slightly more confident in the estimate of 1970s than 1980s with probabilities
    of 0.6 and 0.4, respectively. Note that the loss value increases further, from
    0.9163 in example (3) to 4.6052 in example 4, on lower right of figure A.13, where
    the classification model is highly confident about the wrong estimate.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类模型在估计标签值时不正确，损失函数会进一步增加，即使在完全不确定的情况下也是如此，就像图 A.13 左下方的示例 3 中所报告的损失数字一样。在这个例子中，正确的标签值是
    1980 年代，而模型对 1970 年代的估计比对 1980 年代的稍微有点信心，分别为 0.6 和 0.4。请注意，损失值进一步增加，从示例（3）中的 0.9163
    增加到图 A.13 右下方的示例 4 中的 4.6052，其中分类模型对错误估计非常有信心。
- en: '![A-13](Images/A-13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![A-13](Images/A-13.png)'
- en: Figure A.13 (1) The model is slightly more confident in the correct value. (2)
    The model is entirely uncertain about which of the two values to choose. (3) The
    model is slightly more confident in the incorrect value. (4) The model is highly
    confident in the incorrect value.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.13（1）模型对正确值的信心略高。 （2）模型完全不确定选择哪个值。 （3）模型对错误值的信心略高。 （4）模型对错误值的信心很高。
- en: Since the output of a classification model consists of probabilities (a probability
    distribution) for label values, the original labels in the working data set must
    be encoded (converted) into the same format before training or testing the machine
    learning model. The result of the encoding is shown in columns 1970s and 1980s
    on the left side of figure A.12\. The process of this conversion is known as *one-hot
    encoding*, referring to the fact that just a single value is set to be one (1)
    across the entire row in the columns encoding the label.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分类模型的输出由标签值的概率（概率分布）组成，所以在训练或测试机器学习模型之前，工作数据集中的原始标签必须被编码（转换）成相同的格式。编码的结果显示在图
    A.12 左侧的 1970 年代和 1980 年代列中。这种转换过程被称为*one-hot 编码*，指的是在编码标签的列中，整行中只有一个值被设置为一（1）。
- en: The cross-entropy loss function can be defined in terms of NumPy operations.
    The xe_loss function definition implements the calculation for cross-entropy loss
    given an array of the classification model output y_est and the corresponding
    one-hot encoded array of the label values y. Note that with the implementation
    you need to take care not to confuse the label and the model output array parameters
    because the np.log function outputs -Inf and 0.0 for the values of 0 and 1, respectively.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失函数可以用 NumPy 操作来定义。 xe_loss 函数定义实现了对给定分类模型输出 y_est 和相应的 one-hot 编码的标签值 y
    数组的交叉熵损失的计算。请注意，使用此实现时，需要注意不要混淆标签和模型输出数组参数，因为 np.log 函数分别对值 0 和 1 输出 -Inf 和 0.0。
- en: Listing A.7 xe_loss computing and returning the cross-entropy loss
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 A.7 xe_loss 计算并返回交叉熵损失
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Compute the loss value of 0.9163 based on example 3 of figure A.13.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据图 A.13 的例 3，计算损失值为 0.9163。
- en: ❷ Compute the loss value of 0.6931 based on example 2 of figure A.13.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据图 A.13 的例 2，计算损失值为 0.6931。
- en: ❸ Compute the loss value of 0.9163 based on example 3 of figure A.13.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据图 A.13 的例 3，计算损失值为 0.9163。
- en: ❹ Compute the loss value of 4.6052 based on example 4 of figure A.13.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据图 A.13 的例 4，计算损失值为 4.6052。
- en: 'Running the code from listing A.7 outputs the following cross-entropy loss
    values corresponding to examples 1—4 from figure A.13:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表 A.7 中的代码，输出对应于图 A.13 中示例 1—4 的以下交叉熵损失值：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Mathematical definition of cross-entropy loss.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失的数学定义。
- en: 'Here’s a mathematical description of cross-entropy loss: given a single training
    example *y*, *X* of a label and features, respectively, the function is defined
    as ![A-13_EQ01](Images/A-13_EQ01.png), where *K* is the number of the values of
    the categorical label variable, *y[k]* is the probability of a specific label
    value in a one-hot encoded label *y*, and ![A-13_EQ02](Images/A-13_EQ02.png) is
    the probability estimate of a specific label value for the estimate ![A-13_EQ03](Images/A-13_EQ03.png)produced
    by a classification model.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是交叉熵损失的数学描述：给定单个训练示例 *y*，*X* 的标签和特征，函数定义为 ![A-13_EQ01](Images/A-13_EQ01.png)，其中
    *K* 是分类标签变量的值的数量，*y[k]* 是 one-hot 编码标签 *y* 中特定标签值的概率， ![A-13_EQ02](Images/A-13_EQ02.png) 是分类模型产生的估计 ![A-13_EQ03](Images/A-13_EQ03.png) 的特定标签值的概率估计。
- en: The examples used so far in this section relied on label values that have already
    been one-hot encoded for you. In practice, you must implement the label encoding
    before training a classification model. Although it is possible to use a comprehensive
    set of scikit-learn classes to one-hot encode the model year column label,[^(15)](#pgfId-1016630)
    since in this appendix the data set is instantiated as a pandas DataFrame it is
    easier to use a generic pandas get_dummies method for label encoding. The whimsical
    get_dummies naming of the method stems from *dummy variables*, a term used in
    statistics to describe binary indicator variables that are either 1 to indicate
    a presence or 0 to indicate absence of a value.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分中到目前为止使用的示例依赖于已经为您进行独热编码的标签值。在实践中，在训练分类模型之前，您必须实现标签编码。虽然可以使用一套全面的scikit-learn类来对模型年份列标签进行独热编码，[^(15)](#pgfId-1016630)
    ，但由于在附录中数据集是作为一个pandas DataFrame实例化的，所以更容易使用通用的pandas get_dummies方法进行标签编码。这个方法的奇怪的get_dummies命名来自于*虚拟变量*，这是一个用于描述二进制指示变量的术语，它们要么是1表示存在，要么是0表示不存在一个值。
- en: Given the label and the features for the data set as a pandas DataFrame, a direct
    application of the get_dummies method to the model year label is shown.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集的标签和特征作为pandas DataFrame，将get_dummies方法直接应用于模型年份标签。
- en: Listing A.8 Using get_dummies on a categorical label
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 清单A.8 使用get_dummies对分类标签进行编码
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Instantiate the data set as a pandas DataFrame. Since this instantiation uses
    model year as the label, it is placed in the leading column.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据集实例化为pandas DataFrame。由于此实例化使用模型年份作为标签，因此它放置在前导列中。
- en: ❷ Using get_dummies with a pandas Series identifies the set of unique values
    in a series and creates a new column for each value in the set. The prefix parameter
    ensures that each new column is named using the specified prefix. Setting sparse
    to True may result in, but does not guarantee, reduced memory utilization by the
    resulting DataFrame. Labels with a larger number of distinct values and correspondingly
    more columns in a one-hot encoded format benefit from sparse array representations
    enabled by sparse set to True.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用带有pandas Series的get_dummies可以识别系列中的唯一值集，并为集合中的每个值创建一个新列。prefix参数确保每个新列都使用指定的前缀命名。将sparse设置为True可以导致结果DataFrame的内存利用率降低，但不保证。具有较大数量的不同值和对应更多列的标签在独热编码格式下受益于由sparse
    set为True启用的稀疏数组表示。
- en: ❸ Print the resulting enc_df DataFrame without the zero-based index.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 不打印以零为基础的索引的enc_df DataFrame。
- en: This produces
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了
- en: '[PRE17]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'which is not the desired encoding. Although you can easily implement the code
    to convert the exact model year of the vehicle from the column name to the desired
    encoding of the model decade, *binning* is an alternative and a more flexible
    approach to perform label encoding for this use case. Using the pandas cut method,
    you can “bin” the label values into a range:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '这不是所需的编码。尽管您可以轻松地实现将车辆的确切年份从列名转换为模型十年的代码，*分箱*是一种替代方法和更灵活的方法，用于执行此用例的标签编码。使用pandas
    cut方法，您可以将标签值“分箱”为一个范围:'
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'which outputs a pandas.Series of range intervals:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '它输出一个区间范围的pandas.Series:'
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that the first three vehicles were correctly placed in the 1970s (1969
    is excluded, as indicated by the open parenthesis) while the remaining vehicles
    are placed in the 1980s.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到前三辆车是在1970年代正确放置的（1969年被排除在外，如括号所示），而其余的车辆则放置在1980年代。
- en: Combining the label binning with get_dummies for one-hot encoding,
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结合标签分箱和get_dummies进行独热编码，
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'outputs the desired encoding from figure A.12:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '输出了图A.12所示的所需编码:'
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Prior to evaluating the cross-entropy loss function with the encoded values,
    it is convenient to join the columns of the label encoding with the original data
    set, replacing the original label values with the encoded ones:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '在使用编码值评估交叉熵损失函数之前，将标签编码的列与原始数据集合并，用编码值替换原始标签值很方便:'
- en: '[PRE22]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: which results in
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 它的结果是
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: At this point the data set is prepared to be partitioned into the label and
    the features used for training and then converted to NumPy arrays. Starting with
    the label values,
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，数据集已准备好分割为用于训练的标签和特征，并转换为NumPy数组。从标签值开始，
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: outputs
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To place the feature values into the NumPy X_train array, use
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要将特征值放置到NumPy X_train数组中，使用
- en: '[PRE26]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: which prints out
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: At this point you are ready to train a LogisticRegression classifier model,
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您已经准备好训练一个LogisticRegression分类器模型，
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: and compute the cross-entropy loss,
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 并计算交叉熵损失，
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: which outputs
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出为
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: A.6 Training a supervised machine learning model
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.6 训练监督机器学习模型
- en: When training a machine learning model, you will virtually never use your entire
    structured data set as the training data set. Instead, the pattern followed by
    most machine learning practitioners is to partition the initial data set into
    two mutually exclusive subsets:[^(16)](#pgfId-1017607) a development (dev) data
    set and a test (also known as a held-out) data set.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练机器学习模型时，几乎永远不会将整个结构化数据集用作训练数据集。相反，大多数机器学习从业者遵循的模式是将初始数据集划分为两个相互排斥的子集：开发（dev）数据集和测试（也称为保留）数据集。
- en: '![A-14](Images/A-14.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![A-14](Images/A-14.png)'
- en: Figure A.14 Once the machine learning project data set is partitioned to extract
    the held-out, test data set, it is common to start exploratory data analysis and
    machine learning model training directly with the dev data set, in other words
    to use the dev data set as the training data set (left). A more mature machine
    learning training workflow that can help detect overfitting early and optimize
    the hyperparameters includes further partitioning of the dev data set into training
    and validation data sets, as well using cross-validation with the dev data set.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.14 一旦机器学习项目数据集被分割以提取保留的测试数据集，通常会直接开始探索性数据分析和机器学习模型训练，换句话说，使用开发数据集作为训练数据集（左侧）。一个更成熟的机器学习训练工作流程可以帮助及早检测过拟合并优化超参数，包括将开发数据集进一步分割为训练和验证数据集，以及使用交叉验证与开发数据集。
- en: Many newcomers to the machine learning field are not familiar with the concept
    of the dev data set. While it is possible to use it directly as the training data
    set (and many online courses and tutorials use this simplified approach), training
    machine learning for production requires a more robust approach. The distinction
    between the two approaches is illustrated in figure A.14\. As shown on the right-hand
    side of figure A.14, the dev data set is further split into the training and validation
    data sets. As before, the purpose of the training data set is to train the machine
    learning model; however, the purpose of the validation (or evaluation) data set
    is to estimate the expected performance of the trained machine learning model
    on the held-out (test) data set. For example, in the Ford Mustang fuel efficiency
    data set, one record can be chosen randomly to be in the test data set, and four
    records in the dev data set. Next, one record chosen randomly from the dev data
    set can again be placed in the validation data set, and the remaining three records
    placed in the training data set to train a machine learning model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习领域的新手不熟悉开发数据集的概念。虽然可以直接将其用作训练数据集（许多在线课程和教程使用这种简化方法），但为了生产训练机器学习需要更健壮的方法。这两种方法之间的区别在图
    A.14 中有所说明。如图 A.14 右侧所示，开发数据集进一步分为训练和验证数据集。与以前一样，训练数据集的目的是训练机器学习模型；但验证（或评估）数据集的目的是估计训练好的机器学习模型在保留（测试）数据集上的预期性能。例如，在福特野马燃油效率数据集中，可以随机选择一个记录放入测试数据集，四个记录放入开发数据集。接下来，再次从开发数据集中随机选择一个记录放入验证数据集，其余三个记录放入训练数据集以训练机器学习模型。
- en: Since the purpose of the validation data set is to estimate the performance
    of the training machine learning model on the test data set, having just one record
    in the validation data set is an issue. However, it is also valuable to use as
    many observations from the dev data set for training as possible. One solution
    to this dilemma is to use a technique known as *K-fold* *cross-validation*, illustrated
    in figure A.15\. The key idea behind using K-fold cross-validation is to train
    K different machine learning models by reusing the dev data set K times, each
    time partitioning the dev data set into K folds where K-1 folds are used as the
    training data set and the remaining K-th fold is used as the validation data set.
    The example in figure A.15 uses three partitions, or three-fold cross-validation.
    When the number of observations in the data set does not divide without a remainder
    into K-folds, the partition with the smallest number of observations is designated
    as the validation data set. Otherwise, all the partitions have the same size in
    terms of the number of the observations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于验证数据集的目的是估计训练机器学习模型在测试数据集上的表现，只在验证数据集中拥有一条记录是一个问题。然而，尽可能多地利用开发数据集进行训练也很有价值。解决这个困境的一个方法是使用一种称为*K-fold*
    *交叉验证*的技术，如图 A.15 所示。使用 K 折交叉验证的关键思想是通过 K 次重复使用开发数据集训练 K 个不同的机器学习模型，每次将开发数据集划分为
    K 个折叠，其中 K-1 折叠被用作训练数据集，剩下的第 K 个折叠被用作验证数据集。图 A.15 中的示例使用三个分区，即三折交叉验证。当数据集中的观测值不能被
    K 折折叠时，具有最小观测值数量的分区被指定为验证数据集。否则，所有分区的观测值数量都相同。
- en: Next, K separate machine learning models are trained using the K-1 training
    data set partitions and are validated using the remaining K-th validation partition.
    Hence, in the example in figure A.15, three separate machine learning models are
    trained using the two training folds in each of the three different partitions.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用 K-1 训练数据集分区分别训练 K 个不同的机器学习模型，并使用剩余的第 K 个验证分区进行验证。因此，在图 A.15 的示例中，使用每个三个不同分区中的两个训练折叠来训练三个单独的机器学习模型。
- en: '![A-15](Images/A-15.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![A-15](Images/A-15.png)'
- en: Figure A.15 The K-fold cross-validation technique includes training K different
    machine learning models and reporting the training as well as the validation loss
    (and metric) based on the average of the training and validation values obtained
    from each of the K independent models. Note that each of the K models is validated
    using a different validation partition of the dev data set, using the remainder
    of the dev data set for training.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.15 K 折交叉验证技术包括训练 K 个不同的机器学习模型并报告基于每个 K 个独立模型获得的训练和验证损失（和度量）的平均值。请注意，K 个模型中的每一个都是使用开发数据集的不同验证分区进行验证的，使用开发数据集的剩余部分进行训练。
- en: The dev data set may be used as-is to train a machine learning model, in other
    words, as a training data set. However, this is rarely the case for production
    machine learning models.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 开发数据集可以按原样用作训练数据集，换句话说，作为训练数据集。然而，对于生产机器学习模型来说，这很少发生。
- en: Instead, the dev data set is split further into training and validation data
    sets. Chapter 4 explains and illustrates this process in more detail, but for
    the purposes of this appendix, you can expect that the validation data set is
    used to estimate the performance of the machine learning model on the held-out
    (test) data set, which is not used for training.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，开发数据集被进一步划分为训练数据集和验证数据集。第 4 章详细解释和阐述了这个过程，但是对于本附录的目的，您可以预计验证数据集用于估计机器学习模型在未使用的（测试）数据集上的性能。
- en: ^(1.)A comprehensive review of the traditional computer science algorithms is
    available from Donald E. Knuth in his seminal work *The Art of Computer Programming*
    (Addison-Wesley Professional, 2011).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)Donald E. Knuth 在他的经典著作 *计算机编程艺术*（Addison-Wesley Professional，2011年）中提供了传统计算机科学算法的全面回顾。
- en: ^(2.)The values are based on a data set from the publicly available University
    of California Irvine Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)这些值基于公开可用的加利福尼亚大学尔湾机器学习库[https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)
    中的数据集
- en: ^(3.)Also known as panel or tabular data, structured data sets are based on
    values organized into rows and columns
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)也称为面板或表格数据，结构化数据集是基于行和列组织的值
- en: ^(4.)NumPy is a Python library for high-performance numerical computing. pandas
    wraps the NumPy library and uses it for high-performance data analysis.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy是一个用于高性能数值计算的Python库。pandas封装了NumPy库并将其用于高性能数据分析。
- en: ^(5.)scikit-learn ([scikit-learn.org](http://scikit-learn.org)) was designed
    for machine learning with in-memory data sets as is used here to illustrate machine
    learning with a simple example. Machine learning with out-of-memory data sets
    using cloud computing requires other frameworks.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn ([scikit-learn.org](http://scikit-learn.org))被设计用于机器学习与内存数据集，就像这里用于说明机器学习的一个简单示例一样。使用云计算的大内存数据集进行机器学习需要其他框架。
- en: ^(6.)Although it is commonly associated with modern statistics, linear regression
    originates from the early 19th-century work by Gauss and Lagendere on predictions
    of planetary movement.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它通常与现代统计学联系在一起，但线性回归源自高斯和拉格朗日在19世纪早期关于行星运动预测的工作。
- en: ^(7.)As well as in many other scientific fields, including statistics, econometrics,
    astronomy, and more.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以及许多其他科学领域，包括统计学，计量经济学，天文学等等。
- en: ^(8.)It is possible to implement this code more concisely at the expense of
    having to explain additional NumPy and pandas concepts, including multidimensional
    arrays. If you are interested in learning more about this topic and a more general
    topic of tensors, check out chapter 5.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 可以以更简洁的方式实现这段代码，但需要解释更多的NumPy和pandas概念，包括多维数组。如果你对这个主题和张量的更一般的主题感兴趣，请参阅第5章。
- en: ^(9.)To be mathematically precise, the observations are expected to be statistically
    independent and identically distributed, though most real-world data sets exist
    in the gray area of this definition.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了数学上的精确性，观察结果被期望是统计独立和同分布的，尽管大多数真实世界的数据集存在这一定义的灰色地带。
- en: ^(10.)Hence, you should not be surprised when you hear the label described as
    a “target value.”
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你听到标签被描述为“目标值”时，不应感到惊讶。
- en: '^(11.)If you are interested in broadening your knowledge of machine learning
    beyond supervised machine learning and are willing to read more math-heavy books,
    check out *Artificial Intelligence: A Modern Approach* by Stuart Russell and Peter
    Norvig (Pearson, 2020); *Pattern Recognition and Machine Learning* by Christopher
    Bishop (Springer, 2006); *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and
    Aaron Courville (The MIT Press, 2016); and *The Elements of Statistical Learning*
    by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (Springer, 2016).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣扩展你的机器学习知识超出监督学习，并愿意阅读更多数学密集型的书籍，请查看Stuart Russell和Peter Norvig（Pearson，2020）的《人工智能：一种现代方法》；Christopher
    Bishop（Springer，2006）的《模式识别与机器学习》；Ian Goodfellow，Yoshua Bengio和Aaron Courville（麻省理工学院出版社，2016）的《深度学习》；以及Trevor
    Hastie，Robert Tibshirani和Jerome Friedman（Springer，2016）的《统计学习的要素》。
- en: ^(12.)It is possible to use mean squared error for classification problems if
    the label is binary and is encoded as either —1 or 1.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标签是二进制的，并且编码为—1或1，那么可以使用均方误差来进行分类问题的评估。
- en: ^(13.)This state of a uniform probability distribution is also known as *maximum
    entropy*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀概率分布的这种状态也被称为*最大熵*。
- en: ^(14.)You may have noticed that this is approximately the value of the e constant
    since the cross-entropy calculations here use natural logarithms.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，这几乎是e常数的值，因为这里的交叉熵计算使用的是自然对数。
- en: ^(15.)scikit-learn provides a comprehensive set of classes, including LabelEncoder
    and LabelBinarizer, designed to help with label encoding, as well as OneHotEncoder
    and OrdinalEncoder for feature encoding; these classes are best suited to the
    development scenarios that do not use pandas to store and manage data sets. For
    example, if your entire data set is stored as NumPy arrays, these scikit-learn
    classes are a good option. However, if your data set is a pandas DataFrame or
    a pandas Series, it is more straightforward to apply pandas’s own get_dummies
    method for both label and feature encoding.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了一套全面的类，包括LabelEncoder和LabelBinarizer，旨在帮助进行标签编码，以及OneHotEncoder和OrdinalEncoder用于特征编码；这些类最适合于不使用pandas存储和管理数据集的开发场景。例如，如果你的整个数据集都存储为NumPy数组，那么这些scikit-learn类是一个不错的选择。然而，如果你的数据集是一个pandas
    DataFrame或一个pandas Series，对于标签和特征编码，直接应用pandas自己的get_dummies方法会更简单。
- en: ^(16.)Here “mutually exclusive” means that duplicate records are removed prior
    to partitioning, and following the de-duplication any given record exists either
    in one or the other subsets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^(16.)这里的“相互排斥”意味着在分区之前会移除重复记录，并在去重之后，任何给定的记录都存在于其中一个子集中。
