- en: '9 Natural language processing with TensorFlow: Sentiment analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 使用 TensorFlow 进行自然语言处理：情感分析
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖内容
- en: Preprocessing text with Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 对文本进行预处理
- en: Analyzing text-specific attributes important for the model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析对模型重要的文本特定属性
- en: Creating a data pipeline to handle text sequences with TensorFlow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 创建处理文本序列的数据管道
- en: Analyzing sentiments with a recurrent deep learning model (LSTM)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用递归深度学习模型（LSTM）进行情感分析
- en: Training the model on imbalanced product reviews
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不平衡的产品评论进行模型训练
- en: Implementing word embeddings to improve model performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现词嵌入以提高模型性能
- en: 'In the previous chapters, we looked at two compute-vision-related applications:
    image classification and image segmentation. Image classification focuses on recognizing
    if an object belonging to a certain class is present in an image. Image segmentation
    tasks look not only at recognizing the objects present in the image, but also
    which pixels in the image belong to a certain object. We also anchored our discussions
    around learning the backbone of complex convolutional neural networks such as
    Inception net (image classification) and DeepLab v3 (image segmentation) models.
    If we look beyond images, text data is also a prominent modality of data. For
    example, the world wide web is teeming with text data. We can safely assume that
    it is the most common modality of data available in the web. Therefore, natural
    language processing (NLP) has been and will be a deeply rooted topic, enabling
    us to harness the power of the freely available text (e.g., through language modeling)
    and build machine learning products that can leverage textual data to produce
    meaningful outcomes (e.g., sentiment analysis).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们看了两个与计算机视觉相关的应用程序：图像分类和图像分割。图像分类侧重于识别图像中是否存在属于某个类别的对象。图像分割任务不仅关注于识别图像中存在的对象，还关注于图像中哪些像素属于某个对象。我们还围绕学习复杂卷积神经网络的基础展开了讨论，比如
    Inception net（图像分类）和 DeepLab v3（图像分割）模型。如果我们超越图像，文本数据也是一种重要的数据形式。例如，全球范围内的网络充斥着文本数据。我们可以有把握地认为，它是网络上最常见的数据形式。因此，自然语言处理（NLP）一直是一个根深蒂固的主题，使我们能够利用免费的文本（例如，通过语言建模）的力量，并构建能够利用文本数据产生有意义结果的机器学习产品（例如，情感分析）。
- en: '*NLP* is a term that we give to the overarching notion that houses a plethora
    of tasks having to do with text. Everything from simple tasks, such as changing
    the case of text (e.g., converting uppercase to lowercase), to complex tasks,
    such as translating languages and word sense disambiguation (inferring the meaning
    of a word with the same spelling depending on the context) falls under the umbrella
    of NLP. Following are some of the notable tasks that you will experience if you
    enter the realm of NLP:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理*是一个概括性概念，涵盖了与文本相关的各种任务。从简单的任务，比如改变文本的大小写（例如，将大写转换为小写），到复杂的任务，比如翻译语言和词义消歧（根据上下文推断具有相同拼写的单词的含义）都属于自然语言处理的范畴。以下是您在进入自然语言处理领域时将遇到的一些显著任务：'
- en: '*Stop word removal*—Stop words are uninformative words that frequent text corpora
    (e.g., “and,” “it,” “the,” “am,” etc.). Typically, these words add very little
    or nothing to the semantics (or meaning) of text. To reduce the feature space,
    many tasks remove stop words in early stages before feeding text to the model.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*停用词去除*——停用词是频繁出现在文本语料库中的无信息词（例如，“and”，“it”，“the”，“am”等）。通常情况下，这些词对文本的语义（或含义）几乎没有或没有任何贡献。为了减少特征空间，许多任务在将文本馈送到模型之前的早期阶段去除停用词。'
- en: '*Lemmatization*—This is another technique to reduce the feature space the model
    has to deal with. Lemmatization will convert a given word to its base form (e.g.,
    buses to bus, walked to walk, went to go, etc.), which reduces the size of the
    vocabulary and, in turn, the dimensionality of data the model needs to learn from.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词形还原*——这是另一种减少模型需要处理的特征空间的技术。词形还原将给定单词转换为其基本形式（例如，将 buses 转换为 bus，walked 转换为
    walk，went 转换为 go 等），从而减少词汇表的大小，进而减少模型需要学习的数据的维度。'
- en: '*Part of speech (PoS) tagging*—PoS tagging does exactly what it says: it tags
    every word in a given text with a part of speech (e.g., noun, verb, adjective,
    etc.). The Penn Treebank project provides one of the most popular and comprehensive
    list of PoS tags available. To see the full list, go to [http://mng.bz/mO1W](http://mng.bz/mO1W).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词性标注（PoS）*—PoS标注就是标注给定文本中的每个单词的词性（例如名词、动词、形容词等）。Penn Treebank项目提供了最流行和最全面的PoS标签列表之一。要查看完整列表，请访问[http://mng.bz/mO1W](http://mng.bz/mO1W)。'
- en: '*Named entity recognition (NER**)*—NER is responsible for extracting various
    entities (e.g., Names of people/companies, geo locations, etc.) from text.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*命名实体识别（NER）*—NER负责从文本中提取各种实体（例如人名/公司名、地理位置等）。'
- en: '*Language modeling*—Language modeling is the task of predicting the *n*^(th)
    word given 1, . . . , *w* -1^(th) word. Language modeling can be used to generate
    songs, movie scripts, and stories by training the model on relevant data. Due
    to the highly accessible nature of the data needed for language modeling (i.e.,
    it does not require any labeled data), it commonly serves as a pretraining method
    to inject language understanding for decision-support NLP models.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言建模*—语言建模的任务是预测第*n*（n>1）个单词，给定前面1至*w*-1个单词。通过在相关数据上训练模型，可以使用语言建模生成歌曲、电影脚本、故事等。由于语言建模所需的数据具有高度的可访问性（即不需要任何标记数据），因此通常作为预训练方法用于为决策支持NLP模型注入语言理解。'
- en: '*Sentiment analysis*—Sentiment analysis is the task of identifying the sentiment
    given a piece of text. For example, a sentiment analyzer can analyze product/
    movie reviews and automatically produce a score to indicate how good the product
    is.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*情感分析*—情感分析是根据一段文本的情感来进行评价的任务。例如，情感分析器可以分析产品/电影评论并自动生成一个得分来表示产品质量。'
- en: '*Machine translation*—Machine translation is the task of converting a phrase/
    sentence in a source language to a phrase/sentence in a target language. These
    models are trained using bilingual parallel text corpora.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器翻译*—机器翻译是将源语言中的短语/句子翻译为目标语言中的短语/句子的任务。这些模型是使用双语平行文本语料库进行训练的。'
- en: It is rare that you will not come across an NLP task as a data scientist or
    a ML researcher. To solve NLP tasks quickly and successfully, it is important
    to understand processing data, standard models used, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家或ML研究人员，几乎不可能不遇到NLP任务。要快速而成功地解决NLP任务，重要的是要了解处理数据、使用标准模型等等。
- en: 'In this chapter, you will learn how to develop a sentiment analyzer for video
    game review classification. You will start by exploring the data and learn about
    some common NLP preprocessing steps. You will also note that the data set is imbalanced
    (i.e., does not have a roughly equal number of samples for all the classes) and
    learn what can be done about that. We will then develop a TensorFlow data pipeline
    with which we will pipe data to our model to train. Here, you’ll encounter a new
    machine learning model known as a *long short-term memory* (LSTM) model that has
    made its mark in the NLP domain. LSTM models can process sequences (e.g., a sentence—a
    sequence of words in a particular order) by going through each element iteratively
    to produce some outcome at the end. In this task, the model will output a binary
    value (0 for negative reviews and 1 for positive reviews). While traversing the
    sequence, an LSTM model maintains the memory of what it has seen so far. This
    makes LSTM models very powerful and able to process long sequences and learn patterns
    in them. After the model is trained, we will evaluate it on some test data to
    make sure it performs well and then save it. The high-level steps we will follow
    to develop this model include the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何为视频游戏评论分类开发情感分析器。您将开始探索数据，并了解一些常见的NLP预处理步骤。您还将注意到数据集不平衡（即对于所有的类别数量不大致相等），并学习该怎么做。然后，我们将开发一个TensorFlow数据管道，通过该管道，我们将向模型输入数据进行训练。在这里，您将遇到一种在NLP领域中有影响的新型机器学习模型，即*长期短期记忆*（LSTM）模型。LSTM模型可以通过迭代地处理序列（例如，句子中以特定顺序排列的一串单词）来生成最终结果。在这个任务中，模型将输出一个二进制值（0表示负面评价，1表示正面评价）。在遍历序列时，LSTM模型保持迄今为止所见到的部分的记忆。这使得LSTM模型非常强大，能够处理长序列并学习其中的模式。训练模型后，我们将在某些测试数据上对其进行评估以确保其表现良好，然后保存它。我们将遵循以下高级步骤来开发此模型：
- en: Download the data. We will use a video game review corpus from Amazon.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据。我们将使用Amazon的一个视频游戏评论语料库。
- en: Explore and clean the data. Incorporate some text cleaning steps (e.g., lemmatization)
    to clean the data and prepare the corpus for the modeling task.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索并清理数据。加入一些文本清理步骤（例如，词形还原）来清理数据并为建模任务准备语料库。
- en: Create a data pipeline to convert raw text to a numerical format understood
    by machine learning models.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据管道，将原始文本转换为机器学习模型理解的数值格式。
- en: Train the model on the data produced by the data pipeline.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据管道生成的数据上训练模型。
- en: Evaluate the model on validation, and test data to ensure model’s generalizability.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证集和测试集上评估模型，以确保模型的泛化能力。
- en: Save the trained model, and write the performance results.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存训练好的模型，并写下性能结果。
- en: 9.1 What the text? Exploring and processing text
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 这段文字是什么？探索和处理文字
- en: You are building a sentiment analyzer for a popular online video game store.
    They want a bit more than the number of stars, as the number of stars might not
    reflect the sentiment accurately due to the subjectivity of what a star means.
    The executives believe the text is more valuable than the number of stars. You’ve
    been asked to develop a sentiment analyzer that can determine how positive or
    negative a review is, given the text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在为一个流行的在线视频游戏商店构建情感分析器。他们希望比星级数量更多一些，因为星级数量可能由于星级的主观性而不能准确反映情感。高管们相信文字比星级更有价值。你被要求开发一个情感分析器，可以根据文本确定评论的积极或消极程度。
- en: 'You have decided to use an Amazon video game review data set for this. It contains
    various reviews posted by users along with the number of stars. Text can be very
    noisy due to the complexity of language, spelling mistakes, and so on. Therefore,
    some type of preprocessing will act as the gatekeeper for producing clean data.
    In this section, we will examine the data and some basic statistics. Then we will
    perform several preprocessing steps: comprising, lowering the case (e.g., convert
    “John” to “john”), removing punctuation/numbers, removing stop words (i.e., uninformative
    words like “to,” “the,” “a,” etc.) and lemmatization (converting words to their
    base form; e.g., “walking” to “walk”).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定使用亚马逊视频游戏评论数据集。它包含用户发布的各种评论以及星级数量。由于语言的复杂性、拼写错误等原因，文本可能会非常嘈杂。因此，某种类型的预处理将充当生成干净数据的守门人。在本节中，我们将检查数据和一些基本统计数据。然后，我们将执行几个预处理步骤：包括、转换为小写（例如，将“约翰”转换为“john”）、删除标点符号/数字、删除停用词（即无信息的词，如“to”、“the”、“a”等）和词形还原（将单词转换为其基本形式；例如，“walking”转换为“walk”）。
- en: As the very first step, let’s download the data set in the next listing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们在下一个清单中下载数据集。
- en: Listing 9.1 Downloading the Amazon review data set
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9.1 下载亚马逊评论数据集
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ If the gzip file has not been downloaded, download it and save it to the disk.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果 gzip 文件尚未下载，请下载并保存到磁盘上。
- en: ❷ If the gzip file is located in the local disk, don’t download it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果 gzip 文件位于本地磁盘中，则无需下载。
- en: ❸ If the gzip file exists but has not been extracted, extract it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果 gzip 文件存在但尚未解压，请解压它。
- en: 'This code will download the data to a local folder if it doesn’t already exist
    and extract the content. It will have a JSON file that will contain the data.
    JSON is a format for representing data and is predominately used to transfer data
    in web requests. It allows us to define data as key-value pairs. If you look at
    the JSON file, you will see that it has one record per line, where each record
    is a set of key-value pairs, and key is the column name and value is the value
    of that column for that record. You can see a few records extracted from the data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将数据下载到本地文件夹（如果尚未存在）并提取内容。它将包含一个包含数据的 JSON 文件。 JSON 是一种用于表示数据的格式，主要用于在 Web
    请求中传输数据。它允许我们将数据定义为键值对。如果你查看 JSON 文件，你会看到每行有一条记录，每条记录都是一组键值对，键是列名，值是该记录的该列的值。你可以从数据中提取出几条记录：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will further explore the data we have:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进一步探索我们拥有的数据：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The data is in JSON format. pandas provides a pd.read_json() function to read
    JSON data easily. When reading JSON data, you have to make sure that you set the
    orient argument correctly. This is because the orient argument enables pandas
    to understand the structure of JSON data. JSON data is unstructured compared to
    CSV files, which have a more consistent structure. Setting orient='records' will
    enable pandas to read data structured in this way (one record per line) correctly
    into a pandas DataFrame. Running the previous code snippet will produce the output
    shown in table 9.1.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是以JSON格式呈现的。pandas提供了一个pd.read_json()函数来轻松读取JSON数据。在读取JSON数据时，你必须确保正确设置orient参数。这是因为orient参数使pandas能够理解JSON数据的结构。与具有更一致结构的CSV文件相比，JSON数据是非结构化的。设置orient='records'将使pandas能够正确读取以这种方式结构化的数据（每行一个记录）到一个pandas
    DataFrame中。运行上述代码片段将产生表9.1中所示的输出。
- en: Table 9.1 Sample data from the Amazon review data set
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 Amazon评论数据集的示例数据
- en: '|  | **overall** | **verified** | **reviewTime** | **reviewText** |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | **整体** | **验证** | **评论时间** | **评论文本** |'
- en: '| 0 | 5 | True | 10 17, 2015 | This game is a bit hard to get the hang of,
    bu... |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 5 | True | 10 17, 2015 | 这个游戏有点难以掌握，但...'
- en: '| 1 | 4 | False | 07 27, 2015 | I played it a while but it was alright. The
    st... |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4 | False | 07 27, 2015 | 我玩了一会儿，还行。...'
- en: '| 2 | 3 | True | 02 23, 2015 | ok game. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | True | 02 23, 2015 | 好吧，游戏还可以。'
- en: '| 3 | 2 | True | 02 20, 2015 | found the game a bit too complicated, not what...
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | True | 02 20, 2015 | 觉得这个游戏有点太复杂，不是我想要的...'
- en: '| 4 | 5 | True | 12 25, 2014 | great game, I love it and have played it since...
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5 | True | 12 25, 2014 | 好游戏，我喜欢它，从那时起就一直在玩...'
- en: 'We will now remove any records that have an empty or null value in the reviewText
    column:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将删除评论文本列中的任何空值或null值记录：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you may have already noticed, there’s a column that says whether the review
    is from a verified buyer. To preserve the integrity of our data, let’s only consider
    the reviews from verified buyers. But before that, we have to make sure that we
    have enough data after filtering unverified reviews. To do that, let’s see how
    many records there are for different values (i.e., True and False) of the verified
    column. For that, we will use panda’s built-in value_counts() function as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，有一列显示评论是否来自验证购买者。为了保护我们数据的完整性，让我们只考虑来自验证购买者的评论。但在此之前，我们必须确保在过滤未经验证的评论后有足够的数据。为此，让我们看看不同值（即True和False）的验证列有多少记录。为此，我们将使用pandas的内置value_counts()函数，如下所示：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will return
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这将会返回
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'That’s great news. It seems we have more data from verified buyers than unverified
    users. Let’s create a new DataFrame called verified_df that only contains verified
    reviews:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个好消息。看起来我们从验证购买者那里得到的数据比未验证用户的数据要多。让我们创建一个名为verified_df的新DataFrame，其中只包含验证过的评论：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, out of the verified reviews, we will evaluate the number of reviews for
    each different rating in the overall column:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将评估整体列中每个不同评分的评论数量：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will give
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将会给出
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is an interesting finding. Typically, we want to have equal amounts of
    data for each different rating. But that’s never the case in the real world. For
    example, here, we have four times more 5-star reviews than 4-star reviews. This
    is known as *class imbalance*. Real-world data is often noisy, imbalanced, and
    dirty. We will see these characteristics as we look further into the data. We
    will circle back to the issue of class imbalance in the data when we are developing
    our model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '这是一个有趣的发现。通常，我们希望对每种不同的评分有相等数量的数据。但这在现实世界中从来都不是这样的。例如，在这里，我们有比4星评价多四倍的5星评价。这被称为*类别不平衡*。现实世界的数据通常是嘈杂的、不平衡的和肮脏的。当我们进一步研究数据时，我们将看到这些特征。在开发我们的模型时，我们将回到数据中类别不平衡的问题。 '
- en: 'Sentiment analysis is designed as a classification problem. Given the review
    (as a sequence of words, for example), the model predicts a class out of a set
    of discrete classes. We are going to focus on two classes: positive or negative.
    We will make the assumption that 5 or 4 stars indicate a positive sentiment, whereas
    3, 2, or 1 star mean a negative sentiment. Astute problem formulation, such as
    reducing the number of classes, can make the classification task easier. To do
    this, we can use the convenient built-in pandas function map(). map() takes a
    dictionary, where the key indicates the current value, and the value indicates
    the value the current value needs to be mapped to:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Sentiment analysis 被设计为一个分类问题。给定评论（例如，作为单词序列），模型预测一组离散类别中的一个类别。我们将专注于两个类别：积极或消极。我们将假设
    5 或 4 星表示积极情感，而 3、2 或 1 星表示消极情感。精明的问题形式化，比如减少类别数量，可以使分类任务变得更容易。为此，我们可以使用方便的内置
    pandas 函数 map()。map() 接受一个字典，其中键表示当前值，值表示当前值需要映射到的值：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now let’s check the number of instances for each class after the transformation
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在转换后检查每个类别的实例数量
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: which will return
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'There’s around 83% positive samples and 17% negative samples. That’s a significant
    discrepancy in terms of the number of samples. The final step of our simple data
    exploration is to make sure there’s no order in the data. To shuffle the data,
    we will use panda’s sample() function. sample()is technically used to sample a
    small fraction of data from a large data set. But by setting frac=1.0 and a fixed
    random seed, we can get the full data set shuffled in a random manner:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 积极样本约占 83%，消极样本约占 17%。这在样本数量上存在明显的不一致。我们简单的数据探索的最后一步是确保数据没有顺序。为了对数据进行洗牌，我们将使用
    pandas 的 sample() 函数。sample() 技术上是用于从大数据集中抽样一小部分数据的。但通过设置 frac=1.0 和固定的随机种子，我们可以以随机的方式获取全部数据集的洗牌：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we will separate the inputs and labels into two separate variables,
    as this will make processing easier for the next steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将输入和标签分别分开成两个变量，因为这将使下一步的处理更容易：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will focus on an imperative task, which will ultimately improve the
    quality of the data that is going into the model: cleaning and preprocessing the
    text. Here we will focus on performing the following subtasks. You will learn
    more details about every subtask in the coming discussion:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将专注于一项关键任务，这将最终改善进入模型的数据的质量：清理和预处理文本。在这里，我们将专注于执行以下子任务。您将在接下来的讨论中了解每个子任务的更多细节：
- en: Lower the case of words.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将单词的大小写转换为小写。
- en: Treat shortened forms of words (e.g., “aren’t,” “you’ll,” etc.).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理单词的缩写形式（例如，“aren’t”、“you’ll”等）。
- en: Tokenize text into words (known as *tokenization*).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本标记为单词（称为 *分词*）。
- en: Remove uninformative text, such as numbers, punctuation, and stop words. Stop
    words are words that are frequent in text corpora but do not justify the value
    of their presence enough for most NLP tasks (e.g., “and,” “the,” “am,” “are,”
    “it,” “he,” “she,” etc.).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除不相关的文本，如数字、标点和停用词。停用词是文本语料库中频繁出现但对于大多数自然语言处理任务来说其存在的价值不足以证明的单词（例如，“and”、“the”、“am”、“are”、“it”、“he”、“she”等）。
- en: Lemmatize words. Lemmatization is the process of converting words to their base-form
    (e.g., plural nouns to singular nouns and past-tense verbs to present-tense verbs).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lemmatize words. 词形还原是将单词转换为其基本形式的过程（例如，将复数名词转换为单数名词，将过去式动词转换为现在式动词）。
- en: 'To do most of these tasks, we will rely on a famous and well-known Python library
    for text processing known as NLTK (Natural Language Toolkit). If you have set
    up the development environment, you should have the NLTK library installed. But
    our job is not done yet. To perform some of the subtasks, we need to download
    several external resources provided by NLTK:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行大多数这些任务，我们将依赖于一个著名且广为人知的用于文本处理的 Python 库，称为 NLTK（自然语言工具包）。如果您已设置开发环境，应该已安装
    NLTK 库。但我们的工作还没有完成。为了执行一些子任务，我们需要下载 NLTK 提供的几个外部资源：
- en: averaged_perceptron_tagger—Identifies part of speech
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: averaged_perceptron_tagger—用于识别词性
- en: wordnet *and* omw-1.4-_Will be used to lemmatize (i.e., convert words to their
    base form)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: wordnet *和* omw-1.4-_将被用于词形还原（即，将单词转换为其基本形式）
- en: stopwords—Provides the list of stop words for various languages
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: stopwords—提供各种语言的停用词列表
- en: punkt—Used to tokenize text to smaller components (e.g., words, sentences, etc.)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: punkt—用于将文本标记为较小的组件（例如，单词、句子等）
- en: 'Let’s first do that:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们这样做：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can now continue with our project. To understand the various preprocessing
    steps that’ll be laid out here, we will zoom in on a single review, which is simply
    a Python string (i.e., a sequence of characters). Let’s call this single review
    doc.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续我们的项目了。为了理解这里列出的各种预处理步骤，我们将放大一个单独的评价，它只是一个Python字符串（即字符序列）。让我们将这个单独的评价称为doc。
- en: 'First we can convert a string to lowercase simply by calling the function lower()
    on a string. lower() is a Python built-in function available for strings that
    will convert characters in a given string to lowercase characters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过在字符串上调用lower()函数将字符串转换为小写。lower()是Python中的一个内置函数，可用于将给定字符串中的字符转换为小写字符：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we will expand the "n''t" to "not" if it’s present:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果存在"n't"，我们将将其扩展为"not"：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To do so, we will use regular expressions. Regular expressions give us a way
    to match arbitrary patterns and manipulate them in various ways. Python has a
    built-in library to handle regular expressions, known as re. Here, re.sub() will
    substitute words that match a certain pattern (i.e., any sequence of alphabetical
    characters followed by "n't; e.g., “don’t,” “can’t”) and replace them with a string
    passed as repl (i.e., “not “) in the string doc. For example, “won’t” will be
    replaced with “not.” We do not care about the prefix “will,” as it will be removed
    anyway during the stop word removal we will perform later. If you’re interested,
    you can read more about regular expression syntax at [https://www.rexegg.com/regex-quickstart.xhtml](https://www.rexegg.com/regex-quickstart.xhtml).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将使用正则表达式。正则表达式使我们能够匹配任意模式并以各种方式操纵它们。Python有一个用于处理正则表达式的内置库，称为re。在这里，re.sub()将用作repl参数（即“not
    ”）中的字符串替换符合某个模式的单词（即任何字母字符序列后跟“n't；例如，“don’t”，“can’t”），并将它们替换为字符串doc中的字符串。例如，“won’t”将被替换为“not”。我们不关心前缀“will”，因为在稍后我们将执行的停用词移除过程中它将被移除。如果你感兴趣，可以在[https://www.rexegg.com/regex-quickstart.xhtml](https://www.rexegg.com/regex-quickstart.xhtml)了解更多关于正则表达式语法的信息。
- en: 'We will remove shortened forms such as ’ll, ’re, ’d, and ’ve. You might notice
    that this will result in uncomprehensive phrases like “wo” (i.e., “won’t” becomes
    “wo” + “not”); we can safely ignore them. Notice that we are treating the shortened
    form of “not” quite differently from other shortened forms. This is because, unlike
    the other shortened forms, if present, “not” can have a significant impact on
    what a review actually conveys. We will talk about this again in just a little
    while:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将删除如'll、're、'd和've等缩写形式。你可能会注意到，这将导致不太完整的词组，比如“wo”（即“won’t”变成“wo”+“not”）；但我们可以放心地忽略它们。请注意，我们对“not”的缩写形式处理与其他缩写形式有所不同。这是因为与其他缩写形式不同，如果存在，“not”可以对评价实际传达的意义产生重要影响。我们稍后将再次讨论这个问题：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, to replace the shortened forms of ’ll, ’re, ’d, and ’ve, we are again
    using regular expressions. Here, r"(?:\''ll|\''re|\''d|\''ve)" is a regular expression
    in Python that essentially identifies any occurrence of ''ll/''re/''d/''ve in
    doc. Then we will remove any digits in doc using the re.sub() function like before:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，为了替换'll、're、'd和've的缩写形式，我们再次使用正则表达式。在这里，r"(?:\'ll|\'re|\'d|\'ve)"是Python中的一个正则表达式，它主要识别出doc中的任何出现的'll/'re/'d/'ve。然后我们将使用re.sub()函数如前所述删除doc中的任何数字：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will remove stop words and any punctuation as our next step. As mentioned
    earlier, stop words are words that appear in text but add very little value to
    the meaning of the text. In other words, even if the stop words are missing from
    the text, you’ll still be able to infer the meaning of what’s being said. The
    library NLTK provides a list of stop words, so we don’t have to come up with them:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤中，我们将删除停用词和任何标点符号。如前所述，停用词是出现在文本中但对文本的含义几乎没有贡献的词。换句话说，即使文本中没有停用词，你仍然能够推断出所说内容的意义。NLTK库提供了一个停用词列表，因此我们不必自己编写停用词：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, to access the stop words, all you have to do is call from nltk.corpus
    import stopwords and then call stopwords.words('english'). This will return a
    list. If you look at the words present in the list of stop words, you’ll observe
    almost all the common words (e.g., “I,” “you,” “a,” “the,” “am,” “are,” etc.)
    you’d encounter while reading a text. But as we stressed earlier, the word “not”
    is a special word, especially in the context of sentiment analysis. The presence
    of words like “no” and “not” can completely flip the meaning of a text in our
    case.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问停用词，您只需调用nltk.corpus中的stopwords并调用stopwords.words('english')。这将返回一个列表。如果您查看停用词列表中的单词，您会发现几乎所有常见单词（例如，“I”，“you”，“a”，“the”，“am”，“are”等），这些单词在阅读文本时都会遇到。但正如我们之前强调的，词“not”是一个特殊的词，特别是在情感分析的背景下。诸如“no”和“not”之类的单词的存在可以完全改变我们情况下文本的含义。
- en: Also note the use of the function word_tokenize(). This is a special processing
    step known as *tokenization*. Here, passing a string to word_tokenize() returns
    a list, with each word being an element. Word tokenization might look very trivial
    for a language like English, where words are delimited by a space character or
    a period. But this can be a complex task in other languages (e.g., Japanese) where
    separation between tokens is not explicit.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意一下函数word_tokenize()的使用。这是一种特殊的处理步骤，称为*标记化*。在这里，将字符串传递给word_tokenize()会返回一个列表，其中每个单词都是一个元素。对于像英语这样的语言，单词是由空格字符或句号分隔的，单词标记化可能看起来非常微不足道。但是在其他语言（例如，日语）中，标记之间的分隔并不明显，这可能是一个复杂的任务。
- en: Do “not” let stop words fool you!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不要让停用词愚弄你！
- en: If you look at most stop word lists, you will see that the words “no” and “not”
    are considered stop words as they are common words to see in a text corpus. However,
    for our task of sentiment analysis, these words play a significant role in changing
    the meaning (and possibly the label) of a review. The review “this is a great
    video game” means the opposite of “this is not a great video game” or “this game
    is no good.” For this reason, we specifically remove the words “no” and “not”
    from the list of stop words.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看大多数停用词列表，您会发现单词“no”和“not”被视为停用词，因为它们是在文本语料库中常见的单词。但是，对于我们的情感分析任务，这些单词在改变评价的含义（以及可能的标签）方面起着重要作用。评价“这是一个很棒的视频游戏”的含义与“这不是一个很棒的视频游戏”或“这个游戏不好”相反。因此，我们特别从停用词列表中删除单词“no”和“not”。
- en: 'Next, we have another treatment known as *lemmatization*. Lemmatization truncates/
    stems a given word to a base form, for example, converting plural nouns to singular
    nouns or past tense verbs to present tense, and so on. This can be done easily
    using a lemmatizer object shipped with the NLTK package:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有另一种称为*词形还原*的处理方法。词形还原将给定的单词截断/变形为基本形式，例如将复数名词转换为单数名词或将过去时动词转换为现在时，等等。这可以通过NLTK包中附带的一个词形还原器对象轻松完成：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here we are downloading the WordNetLemmatizer. WordNetLemmatizer is a lemmatizer
    built on the well-renowned WordNet database. If you haven’t heard of WordNet,
    it is a famous lexical database (in the form of a network/graph) that you can
    utilize for tasks such as information retrieval, machine translation, text summarization,
    and so on. WordNet comes in many sizes and flavors (e.g., Multilingual WordNet,
    Non-English WordNet, etc.). You can explore more about WordNet and browse the
    database online at [https://wordnet.princeton.edu/](https://wordnet.princeton.edu/):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在下载WordNetLemmatizer。WordNetLemmatizer是建立在著名的WordNet数据库上的词形还原器。如果您还没有听说过WordNet，那么它是一个著名的词汇数据库（以网络/图形的形式），您可以将其用于信息检索、机器翻译、文本摘要等任务。WordNet有多种大小和口味（例如，多语言WordNet，非英语WordNet等）。您可以在线探索WordNet并浏览数据库[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)。
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'By calling the function lemmatizer.lemmatize(), you can convert any given word
    to its base form (if it is not already in base form). But when calling the function,
    you need to pass in an important argument called pos. pos refers to the PoS tag
    (part-of-speech tag) of that word. PoS tagging is a special NLP task, where the
    task is to classify a given word to a PoS tag from a given set of discrete PoS
    tags. Here are a few examples of PoS tags:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用lemmatizer.lemmatize()函数，您可以将任何给定的单词转换为其基本形式（如果尚未处于基本形式）。但是在调用函数时，您需要传递一个重要的参数，称为pos。pos是该单词的词性标签（part-of-speech
    tag）的缩写。词性标注是一项特殊的自然语言处理任务，任务是从给定的一组离散的词性标签中将给定的单词分类到一个词性标签中。以下是一些词性标签的示例：
- en: '*DT*—Determiner (e.g., a, the)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DT*—限定词（例如，a，the）'
- en: '*JJ*—Adjective (e.g., beautiful, delicious)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*JJ*—形容词（例如，beautiful，delicious）'
- en: '*NN*—Noun, singular or mass (e.g., person, dog)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NN*—名词，单数或质量（例如，person，dog）'
- en: '*NNS*—Noun, plural (e.g., people, dogs)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NNS* — 名词，复数（例如，people，dogs）'
- en: '*NNP*—Proper noun, singular (e.g., I, he, she)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NNP* — 专有名词，单数（例如，我，他，她）'
- en: '*NNPS*—Proper noun, plural (e.g., we, they)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NNPS* — 专有名词，复数（例如，we，they）'
- en: '*VB*—Verb, base form (e.g., go, eat, walk)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VB* — 动词，基本形式（例如，go，eat，walk）'
- en: '*VBD*—Verb, past tense (e.g., went, ate, walked)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VBD* — 动词，过去时（例如，went，ate，walked）'
- en: '*VBG*—Verb, gerund or present participle (e.g., going, eating, walking)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VBG* — 动词，动名词或现在分词（例如，going，eating，walking）'
- en: '*VBN*—Verb, past participle (e.g., gone, eaten, walked)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VBN* — 动词，过去分词（例如，gone，eaten，walked）'
- en: '*VBP*—Verb, non-third-person singular present'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VBP* — 动词，第三人称单数现在时'
- en: '*VBZ*—Verb, third-person singular present'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VBZ* — 动词，第三人称单数现在时'
- en: You can find the full list of PoS tags at [http://mng.bz/mO1W](http://mng.bz/mO1W).
    A note-worthy observation is how the tags are organized. You can see that if you
    consider only the first two characters of the tags, you get a broader set of classes
    (e.g., NN, VB), where all the nouns will be classified with NN and verbs will
    be classified with VB, and so on. We will use this property to make our lives
    easier.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [http://mng.bz/mO1W](http://mng.bz/mO1W) 找到完整的词性标签列表。值得注意的是标签是如何组织的。您可以看到，如果只考虑标签的前两个字符，您会得到一个更广泛的类别集（例如，NN，VB），其中所有名词都将被分类为
    NN，动词将被分类为 VB，以此类推。我们将利用这一特性来简化我们的生活。
- en: 'Back to our code: let’s assimilate how we are using PoS tags to lemmatize words.
    When lemmatizing words, you have to pass in the PoS tag for the word you are lemmatizing.
    This is important as the lemmatization logic is different for different types
    of words. We will first get a list that has (<word>, <pos>) elements for the words
    in tokens (returned by the tokenization process). Then we iterate through the
    pos_tags list and call the lemmatizer.lemmatize() function with the word and the
    PoS tag. We will only lemmatize verbs and nouns to save computational time.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的代码：让我们看看我们如何使用词性标签来将词形还原。在将词形还原为词时，您必须传递该词的词性标签。这一点很重要，因为不同类型的词的词形还原逻辑是不同的。我们首先会得到一个列表，其中包含
    tokens（由分词过程返回）中的单词的（<word>，<pos>）元素。然后，我们遍历 pos_tags 列表并调用 lemmatizer.lemmatize()
    函数，传入单词和词性标签。我们只会将动词和名词进行词形还原，以节省计算时间。
- en: More about WordNet
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 关于WordNet的更多信息
- en: Word net is a lexical database (sometimes called a lexical ontology) that is
    in the form of an interconnected network. These connections are based on how similar
    two words are. For example, the words “car” and “automobile” have a smaller distance,
    whereas “dog” and “volcano” are far apart.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: WordNet是一个以互联网络形式存在的词汇数据库（有时称为词汇本体论）。这些连接基于两个词的相似程度。例如，单词“car”和“automobile”的距离较近，而“dog”和“volcano”则相距甚远。
- en: The words in WordNet are grouped into *synsets* (short for synonym sets). A
    synset captures the words that share a common meaning (e.g., dog, cat, hamster).
    Each word can belong to one or multiple synsets. Each synset has *lemmas*, which
    are the words in that synset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: WordNet中的单词被分组为*同义词集*（简称为synsets）。一个同义词集包含共享共同含义的单词（例如，dog，cat，hamster）。每个单词可以属于一个或多个同义词集。每个同义词集都有*词条*，这些词条是该同义词集中的单词。
- en: 'Going a level up, there are relationships between synsets. There are four different
    relationships:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 向上一层，词集之间存在关系。有四种不同的关系：
- en: '*Hypernyms*—A hypernym synset is the synset that is more general than a given
    synset. For example, “animal” is a hypernym synset of “pet.”'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*超类词* — 超类词集是比给定词集更一般的词集。例如，“动物”是“宠物”的超类词集。'
- en: '*Hyponyms*—A hyponym synset is a more specific synset than a given synset.
    For example, “car” is a hyponym synset of the “vehicle” synset.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下义词* — 下义词集是比给定词集更具体的词集。例如，“car”是“vehicle”词集的下义词集。'
- en: '*Holonyms*—A holonym synset is a synset that a given synset is a part of (is-part-of
    relationship). For example, “engine” is a holonym synset of the “car” synset.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*部分词* — 部分词集是给定词集的一部分（是-部分-关系）。例如，“engine”是“car”词集的部分词集。'
- en: '*Meronyms*—A meronym synset is a synset that a given synset is made of (is-made-of
    relationship). For example, “leaf” is a meronym synset of the “plant” synset.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成员词* — 成员词集是给定词集所构成的词集（是-构成-关系）。例如，“leaf”是“plant”词集的成员词集。'
- en: Due to this organization of interconnected synsets, using WordNet you can measure
    the distance between two words as well. Similar words will have a smaller distance,
    whereas disparate words will have a larger distance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种互联词集的组织，使用WordNet还可以测量两个词之间的距离。相似的词将具有较小的距离，而不同的词将具有较大的距离。
- en: You can experiment with these ideas in NLTK by importing WordNet from nltk.corpus
    import wordnet. For more information refer to [https://www.nltk.org/howto/wordnet.xhtml](https://www.nltk.org/howto/wordnet.xhtml).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过从 nltk.corpus 导入 WordNet 来在 NLTK 中尝试这些想法。有关更多信息，请参阅 [https://www.nltk.org/howto/wordnet.xhtml](https://www.nltk.org/howto/wordnet.xhtml)。
- en: This concludes the series of steps we are incorporating to build the preprocessing
    workflow for our text. We will encapsulate these steps in a function called clean_
    text(), as in the following listing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们正在合并的一系列步骤，以构建我们文本的预处理工作流程。我们将这些步骤封装在一个名为 clean_ text() 的函数中，如下所示。
- en: Listing 9.2 Preprocessing logic for reviews in the dataset
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.2 数据集中评论的预处理逻辑
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Turn to lower case.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 转换为小写。
- en: ❷ Expand the shortened form n’t to “not.”
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将缩写形式 n’t 扩展为“not”。
- en: ❸ Remove shortened forms like ’ll, ’re, ’d, ’ve, as they don’t add much value
    to this task.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 删除缩写形式，如’ll，’re，’d，’ve，因为它们对此任务没有太多价值。
- en: ❹ Remove digits.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 删除数字。
- en: ❺ Break the text into tokens (or words); while doing that, ignore stop words
    from the result.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将文本分解为标记（或单词）；在此过程中，忽略结果中的停用词。
- en: ❻ Get the PoS tags for the tokens in the string.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取字符串中标记的词性标签。
- en: ❼ To lemmatize, get the PoS tag of each token; if it is N (noun) or V (verb)
    lemmatize, else keep the original form.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对于每个标记，获取其词性标签；如果是名词（N）或动词（V），则进行词形还原，否则保留原始形式。
- en: You can check the processing done in the function by calling it on a sample
    text
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在示例文本上调用函数来检查函数中完成的处理。
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: which returns
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will leverage this function along with panda’s apply function to apply this
    processing pipeline on each row of text that we have in our data DataFrame:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用此函数以及panda的 apply 函数，在我们的数据 DataFrame 中的每一行文本上应用此处理管道：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You might want to leave your computer for a while to grab a coffee or to check
    on your friends. It might take close to an hour to run this one-liner. The final
    result looks like table 9.2.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想离开电脑一会儿去喝咖啡或看看朋友。运行这个一行代码可能需要接近一个小时。最终结果看起来像表 9.2。
- en: Table 9.2 Original text versus preprocessed text
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.2 原始文本与预处理文本
- en: '| **Original text** | **Clean text (tokenized)** |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| **原始文本** | **清理文本（标记化）** |'
- en: '| Worked perfectly on Wii and GameCube.No issues with compatibility or loss
    of memory. | [''work'', ''perfectly'', ''wii'', ''gamecube'', ‘no’, ''issue'',
    ''compatibility'', ''loss'', ''memory''] |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 在 Wii 和 GameCube 上完美运行。与兼容性或内存丢失无关的问题。 | [''work'', ''perfectly'', ''wii'',
    ''gamecube'', ‘no’, ''issue'', ''compatibility'', ''loss'', ''memory''] |'
- en: '| Loved the game, and the other collectibles that came with it are well made.
    The mask is big, and it almost fits my face, so that was impressive. | [''loved'',
    ''game'', ''collectible'', ''come'', ''well'', ''make'', ''mask'', ''big'', ''almost'',
    ''fit'', ''face'', ''impressive''] |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 喜欢这款游戏，而且其他附带的收藏品做得很好。面具很大，几乎适合我的脸，所以令人印象深刻。 | [''loved'', ''game'', ''collectible'',
    ''come'', ''well'', ''make'', ''mask'', ''big'', ''almost'', ''fit'', ''face'',
    ''impressive''] |'
- en: '| It’s an okay game. To be honest, I am very bad at these types of games and
    to me it’s very difficult! I am always dying, which depresses me. Maybe if I had
    more skill I would enjoy this game more! | ["''s", ''okay'', ''game'', ''honest'',
    ''bad'', ''type'', ''game'', ''--'', "''s", ''difficult'', ''always'', ''die'',
    ''depresses'', ''maybe'', ''skill'', ''would'', ''enjoy'', ''game''] |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个可以的游戏。说实话，我对这类游戏很差劲，对我来说这很困难！我总是死去，这让我沮丧。也许如果我更有技巧，我会更喜欢这款游戏！ | ["''s",
    ''okay'', ''game'', ''honest'', ''bad'', ''type'', ''game'', ''--'', "''s", ''difficult'',
    ''always'', ''die'', ''depresses'', ''maybe'', ''skill'', ''would'', ''enjoy'',
    ''game''] |'
- en: '| Excellent product as described. | [''excellent'', ''product'', ''describe'']
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 产品描述很好。 | [''excellent'', ''product'', ''describe''] |'
- en: '| The level of detail is great; you can feel the love for cars in this game.
    | [''level'', ''detail'', ''great'', ''feel'', ''love'', ''car'', ''game''] |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 细节水平很高；你可以感受到这款游戏对汽车的热爱。 | [''level'', ''detail'', ''great'', ''feel'', ''love'',
    ''car'', ''game''] |'
- en: '| I can’t play this game. | [''not'', ''play'', ''game''] |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 我不能玩这个游戏。 | [''not'', ''play'', ''game''] |'
- en: 'Finally, to avoid overdosing on coffee or pestering your friends by running
    this too many times, we will save the data to the disk:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了避免因运行次数过多而过度依赖咖啡或打扰朋友，我们将数据保存到磁盘上：
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, we are going to define a data pipeline to transform the data to a format
    that is understood by the model and can be used to train and evaluate the model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个数据管道，将数据转换为模型理解的格式，并用于训练和评估模型。
- en: Exercise 1
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1
- en: Given the string s, “i-purchased-this-game-for-99-i-want-a-refund,” you’d like
    to replace the dash “-” with a space and then lemmatize only the verbs in the
    text. How would you do that?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 给定字符串s，“i-purchased-this-game-for-99-i-want-a-refund”，你想用空格替换短横线“-”，然后仅对文本中的动词进行词形还原。你会如何做？
- en: 9.2 Getting text ready for the model
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备模型的文本
- en: 'You have a clean data set with text stripped of any unnecessary or unwarranted
    linguistic complexities for the problem we’re solving. Additionally, the binary
    labels have been generated from the number of stars given for each review. Before
    pushing ahead with the model training and evaluation, we have to do some further
    processing of our data set. Specifically, we will create three subsets of data—training,
    validation and testing—which will be used to train and evaluate the model. Next,
    we will look at two important characteristics of our data sets: the vocabulary
    size and the distribution of the sequence length (i.e., the number of words) in
    the examples we have. Finally, you will convert the words to numbers (or numerical
    IDs), as machine learning models do not understand strings but numbers.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个干净的数据集，其中的文本已经剥离了任何不必要或不合理的语言复杂性，以解决我们正在解决的问题。此外，二元标签是根据每条评论给出的星级生成的。在继续进行模型训练和评估之前，我们必须对数据集进行进一步处理。具体来说，我们将创建三个数据子集——训练、验证和测试——用于训练和评估模型。接下来，我们将查看数据集的两个重要特征：词汇量和示例中序列长度（即单词数量）的分布。最后，你将把单词转换为数字（或数字ID），因为机器学习模型不理解字符串而理解数字。
- en: 'In this section, we will further prepare the data to be consumed by the model.
    Right now, we have a very nice layout of processing steps to go from a noisy,
    inconsistent review to a simple, consistent text string that preserves the semantics
    of the review. But we haven’t addressed the elephant in the room! That is, machine
    learning models understand numerical data, not textual data. The string “not a
    great game” does not mean anything to a model if you present it as is. We have
    to further refine our data so that we end up with a number sequence instead of
    a word sequence. In our journey to get the data ready for the model, we will perform
    the following subtasks:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进一步准备数据以供模型使用。现在，我们有一个非常好的处理步骤布局，可以从杂乱、不一致的评论转变为保留评论语义的简单、一致的文本字符串。但是，我们还没有解决问题的关键！也就是说，机器学习模型理解的是数值数据，而不是文本数据。如果你直接呈现字符串“not
    a great game”，对模型来说没有任何意义。我们必须进一步完善我们的数据，以便最终得到一系列数字，而不是单词序列。在准备好数据供模型使用的过程中，我们将执行以下子任务：
- en: Check the size of the vocabulary/word frequency after preprocessing. This will
    later be used as a hyperparameter of the model.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预处理后检查词汇表/单词频率的大小。这将稍后用作模型的超参数。
- en: Check the summary statistics of the sequence length (mean, median, and standard
    deviation). This will later be used as a hyperparameter of the model.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查序列长度的摘要统计信息（均值、中位数和标准偏差）。这将稍后用作模型的超参数。
- en: Create a dictionary that will map each unique word to a unique ID (we will call
    this a tokenizer).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个字典，将每个唯一的单词映射到一个唯一的ID（我们将其称为分词器）。
- en: 9.2.1 Splitting training/validation and testing data
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割训练/验证和测试数据
- en: A word of caution! When performing these tasks, you might inadvertently create
    oozing data leakages in our model. We have to make sure we perform these tasks
    *using only the training data set* and keep the validation and testing data separate.
    Therefore, our first goal should be separating training/validation/test data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 警告！在执行这些任务时，你可能会无意中在我们的模型中创建渗漏数据泄露。我们必须确保我们只使用训练数据集来执行这些任务，并将验证和测试数据保持分开。因此，我们的第一个目标应该是分离训练/验证/测试数据。
- en: The lurking data leakages in NLP
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在的自然语言处理数据泄漏
- en: You might be thinking, “Great! All I have to do is load the processed text corpus
    and perform the analysis or tasks on that.” Not so fast! That is the incorrect
    way to do it. Before doing any data-specific processing/analysis, such as computing
    the vocabulary size or developing a tokenizer, you have to separate the data into
    training/validation and test sets, and then perform this processing/analysis on
    the training data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：“太好了！我只需加载处理过的文本语料库并在上面执行分析或任务。”不要那么快！这是错误的做法。在执行任何特定于数据的处理/分析之前，如计算词汇量或开发分词器，你必须将数据分割成训练/验证和测试集，然后在训练数据上执行此处理/分析。
- en: The purpose of the validation data is to act as a compass for choosing hyperparameters
    and to determine when to stop training. The test data set is your benchmark on
    how well the model is going to perform in the real world. Given the nature of
    the purpose fulfilled by the validation/test data, they should not be part of
    your analysis, but only be used to evaluate performance. Using validation/test
    data in your analysis to develop a model gives you an unfair advantage and causes
    what’s known as *data leakage*. Data leakage refers to directly or indirectly
    providing access to the examples you evaluate the model on. If the validation/test
    data is used for any analysis we do, we are providing access to those data sets
    before the evaluation phase. Models with data leakages can lead to poor performance
    in the real world.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据的目的是作为选择超参数的指南，并确定何时停止训练。测试数据集是你的基准，用于评估模型在实际世界中的表现。考虑到验证/测试数据的用途性质，它们不应该成为你分析的一部分，而只应该用于评估性能。在你的分析中使用验证/测试数据来开发模型会给你一个不公平的优势，并导致所谓的*数据泄漏*。数据泄漏是指直接或间接提供访问你在模型上评估的示例。如果验证/测试数据在我们进行任何分析时被使用，我们在评估阶段之前提供了对这些数据集的访问。带有数据泄漏的模型可能导致在实际世界中性能不佳。
- en: 'We know we have an imbalanced data set. Despite having an imbalanced data set,
    we have to make sure our model is good at identifying both positive and negative
    reviews. This means the data sets we’ll be evaluating on need to be balanced.
    To achieve this, here’s what we will do:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们有一个不均衡的数据集。尽管有一个不均衡的数据集，我们必须确保我们的模型能够很好地识别出积极和消极的评论。这意味着我们将要评估的数据集需要是平衡的。为了实现这一点，我们将做以下操作：
- en: Create balanced (i.e., equal count of positive and negative samples) validation
    and test sets
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建平衡的（即正负样本数量相等的）验证集和测试集
- en: Assign the remaining datapoints to the training set
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将剩余的数据点分配给训练集
- en: Figure 9.1 depicts this process.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1描述了这个过程。
- en: '![09-01](../../OEBPS/Images/09-01.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![09-01](../../OEBPS/Images/09-01.png)'
- en: Figure 9.1 The process for splitting training/valid/test data
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 分割训练/验证/测试数据的过程
- en: 'We will now see how we can do this in Python. First, we start by identifying
    the indices that correspond to positive labels and negative labels separately:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看如何在Python中实现这一点。首先，我们分别识别对应于正标签和负标签的索引：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Stratified sampling: An alternative to imbalanced data sets'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 分层抽样：不均衡数据集的替代方法
- en: 'Your design of validation and test sets will dictate how you will define performance
    metrics to evaluate the trained model. If you create equally balanced validation/test
    sets, then you can safely use accuracy as a metric to evaluate the trained model.
    That is what we will do here: create balanced validation/test data sets, and then
    use accuracy as a metric to evaluate the model. But you might not be so lucky
    all the time. There can be scenarios where the minority class is so scary you
    can’t afford to create balanced data sets.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你对验证和测试集的设计将决定你如何定义性能指标来评估训练模型。如果你创建了同样平衡的验证/测试集，那么你可以安全地使用准确率作为评估训练模型的指标。这就是我们将要做的：创建平衡的验证/测试数据集，然后使用准确率作为评估模型的指标。但你可能并不总是那么幸运。有时候可能会出现少数类别非常恐怖，你无法承担创建平衡数据集的情况。
- en: In such instances, you can use *stratified sampling*. Stratified sampling creates
    individual data sets, roughly maintaining the original class ratios in the full
    data set. When this is the case, you have to carefully choose your metric, because
    standard accuracy can no longer be trusted. For example, if you care about identifying
    positive samples with high accuracy at the cost of a few false positives, then
    you should use recall (or F1 score with higher weight given to recall) as the
    performance metric.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以使用*分层抽样*。分层抽样创建单独的数据集，大致保持完整数据集中原始类别比例。在这种情况下，你必须谨慎选择你的度量标准，因为标准准确率不能再被信任。例如，如果你关心以高准确率识别正样本而牺牲一些误报率，那么你应该使用召回率（或F1分数，对召回率给予更高的权重）作为性能指标。
- en: 'Next, we will define the size of our validation/test set as a function of train_fraction
    (a user-defined argument that determines how much data to leave for the training
    set). We will use a default value of 0.8 for the train_fraction:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的验证/测试集的大小作为train_fraction的函数（一个用户定义的参数，确定留给训练集的数据量）。我们将使用train_fraction的默认值0.8：
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It might look like a complex computation, but it is, in fact, a simple one.
    We will use the valid fraction as half of the fraction of data left for training
    data (the other half is used for the testing set). And finally, to convert the
    fractional value to the actual number of samples, we multiply the fraction by
    the smallest of counts of positive and negative samples. This way, we make sure
    the underrepresented class stays as the focal point during the data split. We
    keep the validation set and the test set equal. Therefore
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 它可能看起来像是一个复杂的计算，但事实上，它是一个简单的计算。我们将使用有效分数作为留给训练数据的数据分数的一半（另一半用于测试集）。最后，为了将分数值转换为实际样本数，我们将分数乘以正样本和负样本计数中较小的那个。通过这种方式，我们确保少数类在数据拆分过程中保持为焦点。我们保持验证集和测试集相等。所以
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we define the three sets of indices (for train/validation/test datasets)
    for each label type (positive and negative). We will create a funneling process
    to assign data points to different data sets. First, we do the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为每种标签类型（正和负）定义三组索引（用于训练/验证/测试数据集）。我们将创建一个漏斗过程来将数据点分配到不同的数据集中。首先，我们执行以下操作：
- en: Randomly sample n_test number of indices from the negative indices (neg_ test_indices).
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从负索引（neg_ test_indices）中随机抽样 n_test 个索引。
- en: Then randomly sample n_valid indices from the remaining indices (neg_ valid_inds).
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从剩余的索引中随机抽样 n_valid 个索引（neg_ valid_inds）。
- en: The remaining indices are kept as the training instances (neg_train_inds).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的索引被保留为训练实例（neg_train_inds）。
- en: 'The same process is then repeated for positive indices to create three index
    sets for training/validation/test data sets:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对正索引重复相同的过程，以创建用于训练/验证/测试数据集的三个索引集：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'With the negative and positive indices to slice the inputs and labels, now
    it’s time to create actual data sets:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用负索引和正索引来切片输入和标签，现在是时候创建实际的数据集了：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, (tr_x, tr_y), (v_x, v_y), and (ts_x, ts_y) represent the training, validation,
    and testing data sets, respectively. Here, the data sets suffixed with _x come
    from the inputs, and the data sets suffixed with _y come from the labels. Finally,
    we can wrap the logic we discussed in a single function as in the following listing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，（tr_x，tr_y），（v_x，v_y）和（ts_x，ts_y）分别代表训练，验证和测试数据集。在这里，以 _x 结尾的数据集来自输入，以 _y
    结尾的数据集来自标签。最后，我们可以将我们讨论的逻辑包装在一个单独的函数中，如下面的清单所示。
- en: Listing 9.3 Splitting training/validation/testing data sets
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9.3 拆分训练/验证/测试数据集
- en: '[PRE32]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Separate indices of negative and positive data points.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将负数据点和正数据点的索引分开。
- en: ❷ Compute the valid and test data set sizes (for minority class).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算有效和测试数据集的大小（针对少数类）。
- en: ❸ Get the indices of the minority class that goes to the test set.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取进入测试集的少数类索引。
- en: ❹ Get the indices of the minority class that goes to the validation set.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取进入验证集的少数类索引。
- en: ❺ The rest of the indices in the minority class belong to the training set.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 少数类中其余的索引属于训练集。
- en: ❻ Compute the majority class indices for the test/validation/train sets
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算用于测试/验证/训练集的多数类索引
- en: ❼ Get the training/valid/test data sets using the indices created.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用创建的索引获取训练/验证/测试数据集。
- en: 'Then simply call the function to generate training/validation/testing data:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后只需调用函数来生成训练/验证/测试数据：
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Next, we’re going to examine the corpus a bit more to explore the vocabulary
    size and sequence length with respect to the reviews we have in the training set.
    These will be used as hyperparameters to the model later on.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进一步检查语料库，以探索与我们在训练集中拥有的评论相关的词汇量和序列长度。稍后这些将作为模型的超参数。
- en: 9.2.2 Analyze the vocabulary
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 分析词汇
- en: 'Vocabulary size is an important hyperparameter for the model. Therefore, we
    have to find the optimal vocabulary size that will allow us to capture enough
    information to solve the task accurately. To do that, we will first create a long
    list, where each element is a word:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇量是模型的重要超参数。因此，我们必须找到最佳的词汇量，以便能够捕获足够的信息以准确解决任务。为此，我们首先会创建一个长列表，其中每个元素都是一个单词：
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This line goes through each doc in tr_x and then through each word (w) in that
    doc and creates a flattened sequence of words that are present in all the documents.
    Because we have a Python list, where each element is a word, we can utilize Python’s
    built-in Counter objects to get a dictionary, where each word is mapped to a key
    and the value represents the frequency of that word in the corpus. Note how we
    are using the training data set only for this analysis in order to avoid data
    leakage:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此行遍历 tr_x 中的每个文档，然后遍历该文档中的每个单词（w），并创建一个展平的序列，其中包含所有文档中存在的单词。由于我们有一个 Python 列表，其中每个元素都是一个单词，我们可以利用
    Python 的内置 Counter 对象来获取一个字典，其中每个单词都映射到一个键，该值表示该单词在语料库中的频率。请注意，我们仅在此分析中使用训练数据集，以避免数据泄漏：
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'With our word frequency dictionary out of the way, let’s look at some of the
    most common words in our corpus:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的单词频率字典，让我们来看看我们语料库中一些最常见的单词：
- en: '[PRE36]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will return the following result, where you can see the top words that
    appear in the text. Looking at the results, it makes sense. It’s no surprise that
    words like “game,” “like,” and “play” get priority in terms of frequency over
    the other words:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下结果，您可以看到出现在文本中的最常见的单词。从结果来看，这是合理的。毫不奇怪，像 “game”、“like” 和 “play” 这样的词在频率上优先于其他单词：
- en: '[PRE37]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Going a step forward, let’s compute the summary statistics on the text corpus.
    By doing this, we can see the average frequency of words, standard deviation,
    minimum, maximum, and so on:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，让我们对文本语料库进行摘要统计。通过这样做，我们可以看到单词的平均频率、标准偏差、最小值、最大值等：
- en: '[PRE38]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will give some important basic statistics about the frequency of words.
    For example, from this we can say the average frequency of words is ~76 with a
    standard deviation of ~1754:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供有关单词频率的一些重要基本统计信息。例如，从中我们可以说单词的平均频率是约为 ~76，标准偏差为 ~1754：
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will then create a variable called n_vocab that will hold the size of the
    vocabulary containing the words appearing at least 25 times in the corpus. You
    should get a value close to 11,800 for n_vocab:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个名为 n_vocab 的变量，该变量将保存在语料库中至少出现 25 次的单词的词汇量的大小。您应该得到接近 11,800 的 n_vocab
    值：
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 9.2.3 Analyzing the sequence length
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 分析序列长度
- en: 'Remember that tr_x is a pandas Series object, where each row contains a review
    and each review is a list of words. When the data is in this format, we can use
    the pd.Series.str.len() function to get the length of each row (or the number
    of words in each review):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，tr_x 是一个 pandas Series 对象，其中每一行都包含一条评论，每个评论都是一个单词列表。当数据处于这种格式时，我们可以使用 pd.Series.str.len()
    函数来获取每行的长度（或每条评论中的单词数）：
- en: '[PRE41]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'When computing the basic statistics, we will do things a bit differently. Our
    goal here is to find three bins of sequence lengths so we can bin them to short,
    medium, and long sequences. We will use these bucket boundaries when defining
    our TensorFlow data pipeline. To do that, we will first identify the cut-off points
    (or quantiles) to remove the top and bottom 10% of data. This is because top and
    bottom slices are full of outliers, and, as you know, they will skew the statistics
    like mean. In pandas, you can get the quantiles with the quantile() function,
    where you pass a fractional value to indicate which quantile you’re interested
    in:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算基本统计量时，我们将采取一些不同的方法。我们的目标是找到三个序列长度的区间，以便将它们分为短、中、长序列。我们将在定义 TensorFlow 数据流水线时使用这些桶边界。为此，我们将首先确定截断点（或分位数），以去除数据的前
    10% 和后 10%。这是因为顶部和底部切片都充满了异常值，正如你所知，它们会使诸如均值之类的统计量产生偏差。在 pandas 中，您可以使用 quantile()
    函数获取分位数，其中您传递一个分数值来指示您感兴趣的分位数：
- en: '[PRE42]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then you simply filter the data between those quantiles. Next, we use the describe
    function with the 33% percentile and 66% percentile, as we want to bin to three
    different categories:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您只需在这些分位数之间过滤数据。接下来，我们使用 describe 函数，其中包含 33% 分位数和 66% 分位数，因为我们希望将其分为三个不同的类别：
- en: '[PRE43]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If you run this code, you’ll get the following output:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果运行此代码，您将得到以下输出：
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Following the results, we will use 5 and 15 as our bucket boundaries. In other
    words, reviews are classified according to the following logic:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们将使用 5 和 15 作为我们的桶边界。换句话说，评论按照以下逻辑进行分类：
- en: Review length in [0, 5) are short reviews.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度在 [0, 5) 的评论为短评论。
- en: Review length in [5, 15) are medium reviews.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度在 [5, 15) 的评论为中等评论。
- en: Review length in [15, inf) are long reviews.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度在 [15, inf) 的评论为长评论。
- en: The last two subsections conclude our analysis to find the vocabulary size and
    sequence length. The outputs presented here provided all the information to pick
    our hyperparameters with a principled mind-set.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个小节总结了我们分析以找到词汇表大小和序列长度的过程。这里呈现的输出提供了所有信息，以便以有原则的态度选择我们的超参数。
- en: 9.2.4 Text to words and then to numbers with Keras
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 使用 Keras 将文本转换为单词，然后转换为数字
- en: 'We have a clean, processed corpus of text as well as the vocabulary size and
    sequence length parameters we’ll use later. Our next task is to convert text to
    numbers. There are two standard steps in converting text to numbers:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一段干净的、经过处理的文本语料库，以及我们稍后将使用的词汇表大小和序列长度参数。我们的下一个任务是将文本转换为数字。将文本转换为数字有两个标准步骤：
- en: Split text to tokens (e.g., characters/words/sentences).
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本拆分为标记（例如，字符/单词/句子）。
- en: Create a dictionary that maps each unique token to a unique ID.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个将每个唯一标记映射到唯一 ID 的字典。
- en: For example, if you have the sentence
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您有以下句子
- en: '[PRE45]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: we will first tokenize this into words, resulting in
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将其标记化为单词，得到
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: and have the dictionary
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 并且有字典
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then you can create the following sequence to represent the original text:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以创建以下序列来表示原始文本：
- en: '[PRE48]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The Keras Tokenizer object supports exactly this functionality. It takes a
    corpus of text, tokenizes it with some user-defined parameters, builds the dictionary
    automatically, and saves it as a state. This way, you can use the Tokenizer to
    convert any arbitrary text as many times as you like to numbers. Let’s look at
    how we can do this using the Keras Tokenizer:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Keras Tokenizer 对象支持这种功能。它接受一段文本语料库，使用一些用户定义的参数进行标记化，自动构建词典，并将其保存为状态。这样，您可以使用
    Tokenizer 将任意文本转换为数字，次数不限。让我们看看如何使用 Keras Tokenizer 完成这个过程：
- en: '[PRE49]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You can see there are several arguments passed to the Tokenizer. Let’s look
    at these arguments in a bit more detail:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到 Tokenizer 传递了几个参数。让我们稍微详细地看看这些参数：
- en: num_words—This defines the vocabulary size to limit the size of the dictionary.
    If num_words is set to 1,000, it will consider the most common 1,000 words in
    the corpus and assign them unique IDs.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: num_words——这定义了词汇表大小，以限制字典的大小。如果 num_words 设置为 1,000，则会考虑语料库中最常见的 1,000 个单词，并为它们分配唯一的
    ID。
- en: oov_token—This argument treats the words that fall outside the defined vocabulary
    size. The words that appear in the corpus but are not captured within the most
    common num_words words will be replaced with this token.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: oov_token——这个参数处理落在定义的词汇表大小之外的词。出现在语料库中但未包含在最常见的 num_words 个词中的单词将被替换为此标记。
- en: lower—This determines whether to perform case lowering on the text. Since we
    have already done that, we will set it to False.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lower——这确定是否对文本进行大小写转换。由于我们已经做过了，我们将其设置为 False。
- en: filter—This defines any character(s) you want removed from the text before tokenizing.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: filter——这定义了在标记化文本之前要删除的任何字符。
- en: split—This is the separator character that will be used to tokenize your text.
    We want individual words to be tokens; therefore, we will use space, as words
    are usually separated by a space.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: split——这是用于标记化文本的分隔符字符。我们希望单词是标记，因此我们将使用空格，因为单词通常用空格分隔。
- en: char_level—This indicates whether to perform character-level tokenization (i.e.,
    each character is a token).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: char_level——这指示是否执行字符级标记化（即，每个字符都是一个标记）。
- en: Before we move forward, let’s remind ourselves what our data looks like in the
    current state. Remember that we have
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们回顾一下我们的数据在当前状态下是什么样子。请记住，我们有
- en: Cleaned data
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理后的数据
- en: Preprocessed data
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据
- en: Split each review into individual words
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个评论拆分为单独的单词
- en: 'By the end of this process, we have data as shown. First, we have the input,
    which is a pd.Series object that contains the list of clean words. The number
    in front of the text is the index of that record in the pd.Series object:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程结束时，我们的数据如下所示。首先，我们有输入，它是一个 pd.Series 对象，包含一系列干净的单词列表。文本前面的数字是该记录在 pd.Series
    对象中的索引：
- en: '[PRE50]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, we have the labels, where each label is a binary label to indicate whether
    the review is a positive review or a negative review:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有标签，其中每个标签都是一个二进制标签，用于指示评论是正面评论还是负面评论：
- en: '[PRE51]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In a way, that first step of tokenizing the text has already happened. The
    Keras Tokenizer is smart enough to skip that step if it has already happened.
    To build the dictionary of the Tokenizer, you can call the tf.keras.preprocessing.text.Tokenizer.fit_on_texts()
    function, as shown:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，标记文本的第一步已经完成了。如果已经完成了这一步，那么 Keras Tokenizer 足够智能，会跳过这一步。要构建 Tokenizer
    的字典，可以调用 tf.keras.preprocessing.text.Tokenizer.fit_on_texts() 函数，如下所示：
- en: '[PRE52]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The fit_on_texts() function accepts a list of strings, where each string is
    a single entity of what you’re processing (e.g., a sentence, a review, a paragraph,
    etc.) or a list of lists of tokens, where a token can be a word, a character,
    or even a sentence. As you fit the Tokenizer on some text, you can inspect some
    of the internal state variables. You can check the word to ID mapping using
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: fit_on_texts() 函数接受一个字符串列表，其中每个字符串是你正在处理的单个实体（例如，一个句子、一个评论、一个段落等），或者是一个标记列表的列表，其中标记可以是一个单词、一个字符，甚至是一个句子。当你将
    Tokenizer 拟合到某些文本时，你可以检查一些内部状态变量。你可以使用以下方式检查单词到 ID 的映射：
- en: '[PRE53]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: which will return
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE54]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: You also can check the ID to word mapping (i.e., the reverse operation of mapping
    a word to an ID) using
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用以下方式检查 ID 到单词的映射（即将单词映射到 ID 的反向操作）：
- en: '[PRE55]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: which will return
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE56]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To convert a text corpus to a sequences of indices, you can use the texts_to_sequences()
    function. It takes a list of lists of tokens and returns a list of lists of IDs:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要将文本语料库转换为索引序列，你可以使用 texts_to_sequences() 函数。它接受一个标记列表的列表，并返回一个 ID 列表的列表：
- en: '[PRE57]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Let’s see some of the results of the text_to_sequences() function that converted
    some samples:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下 text_to_sequences() 函数转换的一些样本的结果：
- en: '[PRE58]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Great! We can see that text is converted to ID sequences perfectly. We will
    now proceed to defining the TensorFlow pipeline using the data returned by the
    Keras Tokenizer.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们可以看到文本完美地转换为了 ID 序列。我们现在将继续使用 Keras Tokenizer 返回的数据定义 TensorFlow 管道。
- en: Exercise 2
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2
- en: Given the string s, "a_b_B_c_d_a_D_b_d_d", can you define a tokenizer, tok,
    that lowers the text, splits by the underscore character “_”, has a vocabulary
    size of 3, and fits the Tokenizer on s. If the Tokenizer ignores the out-of-vocabulary
    index words starting from 1, what would be the output if you call tok.texts_to_sequences([s])?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 给定字符串 s，“a_b_B_c_d_a_D_b_d_d”，你能否定义一个 tokenizer，tok，将文本转换为小写形式，按下划线字符“_”拆分，具有
    3 个词汇大小，并将 Tokenizer 拟合到 s 上。如果 Tokenizer 忽略从 1 开始的词汇索引词，那么如果你调用 tok.texts_to_sequences([s])，输出会是什么？
- en: 9.3 Defining an end-to-end NLP pipeline with TensorFlow
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 使用 TensorFlow 定义端到端的 NLP 管道
- en: You have defined a clean data set that is in the numerical format the model
    expects it to be in. Here, we will define a TensorFlow data set pipeline to produce
    batches of data from the data we have defined. In the data pipeline, you will
    generate a batch of data, where the batch consists of a tuple (x,y). x represents
    a batch of text sequences, where each text sequence is an arbitrarily long sequence
    of token IDs. y is a batch of labels corresponding to the text sequences in the
    batch. When generating a batch of examples, first the text sequences are assigned
    to buckets depending on the sequence length. Each bucket has a predefined allowed
    sequence length interval. Examples in a batch consist only of examples in the
    same bucket.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经定义了一个干净的数据集，它是模型期望的数字格式。在这里，我们将定义一个 TensorFlow 数据集管道，以从我们定义的数据中生成数据批次。在数据管道中，你将生成一批数据，其中批次由元组（x，y）组成。x
    表示一批文本序列，其中每个文本序列都是一个任意长的标记 ID 序列。y 是与批次中的文本序列对应的标签批次。在生成一批示例时，首先根据序列长度将文本序列分配到桶中。每个桶都有一个预定义的允许序列长度间隔。批次中的示例仅由同一桶中的示例组成。
- en: We are now in a great position. We have done quite a lot of preprocessing on
    the data and have converted text to machine readable numbers. In the next step,
    we will build a tf.data pipeline to convert the output of the Tokenizer to a model-friendly
    output.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在处于一个很好的位置。我们已经对数据进行了相当多的预处理，并将文本转换为了机器可读的数字。在下一步中，我们将构建一个 tf.data 管道，将 Tokenizer
    的输出转换为适合模型的输出。
- en: 'As the first step, we are going to concatenate the target label (having a value
    of 0/1) to the input. This way, we can shuffle the data in any way we want and
    still preserve the relationship between inputs and the target label:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将目标标签（具有值 0/1）连接到输入。这样，我们可以以任何我们想要的方式对数据进行洗牌，并仍然保持输入和目标标签之间的关系：
- en: '[PRE59]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, we will create a special type of tf.Tensor object known as a *ragged
    tensor* (i.e., tf.RaggedTensor). In a standard tensor, you have fixed dimensions.
    For example, if you define a 3 × 4-sized tensor, every single row needs to have
    four columns (i.e., four values). Ragged tensors are a special type of tensor
    that supports variable-sized tensors. For example, it is perfectly fine to have
    data like this as a ragged tensor:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一种特殊类型的tf.Tensor对象，称为*ragged tensor*（即tf.RaggedTensor）。在标准张量中，您具有固定的维度。例如，如果您定义了一个3×4大小的张量，则每行都需要有四列（即四个值）。Ragged
    tensors是一种支持可变大小的张量的特殊类型。例如，可以将以下数据作为ragged tensor：
- en: '[PRE60]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This tensor has three rows, where the first row has two values, the second
    five values, and the final row three values. In other words, it has a variable
    second dimension. This is a perfect data structure for our problem because each
    review has a different number of words, leading to variable-sized ID sequences
    corresponding to each review:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 此张量有三行，其中第一行有两个值，第二行有五个值，最后一行有三个值。换句话说，它具有可变的第二维。由于每篇评论的字数不同，因此导致与每篇评论对应的变大小的ID序列，这是我们问题的完美数据结构：
- en: '[PRE61]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Primer on tf.RaggedTensor
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对tf.RaggedTensor的初步了解
- en: tf.RaggedTensor objects are a special type of tensor that can have variable-sized
    dimensions. You can read more about ragged tensors at [http://mng.bz/5QZ8](http://mng.bz/5QZ8).
    There are many ways to define a ragged tensor.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: tf.RaggedTensor对象是一种特殊类型的张量，可以具有可变大小的维度。您可以在[http://mng.bz/5QZ8](http://mng.bz/5QZ8)了解更多关于ragged
    tensors的信息。有许多方法来定义一个ragged tensor。
- en: 'We can define a ragged tensor by passing a nested list containing values to
    the tf.ragged.constant() function:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传递包含值的嵌套列表来定义ragged tensor，以tf.ragged.constant()函数：
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You can also define a flat sequence of values and define where to split the
    rows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以定义一系列值并定义分割行的位置：
- en: '[PRE63]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Here, each value in the row_splits argument defines where subsequent rows in
    the resulting tensor end. For example, the first row will contain elements from
    index 0 to 3 (i.e., 0, 1, 2). This will output
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，row_splits参数中的每个值定义了结果张量中随后的行结束在哪里。例如，第一行将包含从索引0到3（即0, 1, 2）的元素。 这将输出
- en: '[PRE64]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: You can get the shape of the tensor using b.shape, which will return
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用b.shape获取张量的形状，这将返回
- en: '[PRE65]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You can even have multidimensional ragged tensors, where you have more than
    one variable-sized dimension as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 您甚至可以拥有多维的ragged tensors，其中有多个可变大小的维度，如下所示：
- en: '[PRE66]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here, the nested_row_splits is a list of 1D tensors, where the *i* ^(th) tensor
    represents the row split for the *i* ^(th) dimension. c will look as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，nested_row_splits是1D张量的列表，其中第i个张量表示第i个维度的行拆分。c如下所示：
- en: '[PRE67]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You can perform slicing and indexing on ragged tensors, similar to how you
    do on normal tensors:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在ragged tensors上执行切片和索引操作，类似于在普通张量上的操作：
- en: '[PRE68]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This will return
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE69]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: where
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里
- en: '[PRE70]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This will return
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回
- en: '[PRE71]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Finally, with
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，随着
- en: '[PRE72]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: you will get
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 您将获得
- en: '[PRE73]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We will limit the maximum length of the reviews to max_length. This is done
    under the assumption that max_length words are adequate to capture the sentiment
    in a given review. This way, we can avoid final data being excessively long because
    of one or two extremely long comments present in the data. The higher the max_length,
    the better, in terms of capturing the information in the review. But a higher
    max_length value comes with a hefty price tag in terms of required computational
    power:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将限制评论的最大长度为max_length。这是在假设max_length个字足以捕获给定评论中的情感的情况下完成的。这样，我们可以避免由于数据中存在一两个极长的评论而导致最终数据过长。最大长度越高，就能更好地捕获评论中的信息。但是，更高的max_length值会带来极大的计算开销：
- en: '[PRE74]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We will create a data set using the tf.data.Dataset.from_tensor_slices() function.
    This function on the ragged tensor, which we just created, will extract one row
    (i.e., a single review) at a time. It’s important to remember that each row will
    have a different size. We will filter any reviews that are empty. You could do
    this using the tf.data.Dataset.filter() function:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用tf.data.Dataset.from_tensor_slices()函数创建一个数据集。该函数在我们刚创建的ragged tensor上将逐一提取一行（即一篇评论）。重要的是要记住每行的大小都不同。我们将过滤掉任何空评论。您可以使用tf.data.Dataset.filter()函数来做到这一点：
- en: '[PRE75]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Essentially, we are saying here that any review that has a size smaller than
    or equal to 1 will be discarded. Remember that each record will have at least
    a single element (which is the label). This is an important step because having
    empty reviews can cause problems in the model down the track.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们在这里说的是任何大小小于或等于1的评论将被丢弃。记住，每条记录至少会有一个元素（即标签）。这是一个重要的步骤，因为空评论会在模型后续处理中引起问题。
- en: Next, we will address an extremely important step and the highlight of our impressive
    data pipeline. In sequence processing, you might have heard the term *bucketing*
    (or *binning*). Bucketing refers to, when batching data, using similar-sized inputs.
    In other words, a single batch of data includes similar-sized reviews and will
    not have reviews with drastically different lengths in the same batch. The following
    sidebar explains the process of bucketing in more detail.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解决一个极其重要的步骤，以及我们令人印象深刻的数据管道的重点。在序列处理过程中，你可能听过*分桶*（或*分箱*）这个术语。分桶是指在批处理数据时，使用相似大小的输入。换句话说，一批数据包括长度相似的评论，不会在同一批中有长度差距悬殊的评论。下面的侧栏更详细地解释了分桶的过程。
- en: 'Bucketing: Similar length sequences stick together!'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 分桶：相似长度的序列聚集在一起！
- en: 'Let’s look at an example. Assume that you have a list of reviews, [r1(5), r2(11),
    r3(6), r4(4), r5(15), r6(18), r7(25), r8(29), r9(30)], where the code rx represents
    the review ID and the number within brackets represents the number of words in
    the review. If you select a batch size of 3, it makes sense to batch data the
    following way:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。假设你有一个评论列表，[r1(5), r2(11), r3(6), r4(4), r5(15), r6(18), r7(25), r8(29),
    r9(30)]，其中代码rx代表评论ID，括号内的数字代表评论中的单词数。如果你选择批量大小为3，那么以下列方式分组数据是有意义的：
- en: '[PRE76]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'You can see that the similar-length reviews are batched together. This is implemented
    practically by a process known as bucketing. First, we create several buckets
    with predefined boundaries. For instance, in our example, there can be three buckets
    with the following intervals:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到长度相近的评论被分组在一起。这实际上是通过一个称为分桶的过程来实现的。首先，我们创建几个预定义边界的桶。例如，在我们的示例中，可能有三个间隔如下的桶：
- en: '[PRE77]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Then, depending on the length of the review, each review is assigned to a bucket.
    Finally, when getting batches of data, a batch (randomly sampled) from a single
    bucket at random is selected.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据评论的长度，每个评论被分配到一个桶中。最后，在获取数据批次时，从一个随机选择的桶中随机采样批次数据。
- en: 'After identifying the buckets, we have to batch the data so that we end up
    with a fixed-sequence length. This is achieved by padding zeros to the end until
    we have all sequences in that batch with equal lengths. Let’s assume the reviews
    r1, r3, and r4 have the following word ID sequences:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定桶之后，我们必须将数据分组，使得最终得到固定序列长度。这是通过在序列的末尾填充零直到我们在该批次中拥有长度相等的所有序列来实现的。假设评论r1、r3和r4有以下单词ID序列：
- en: '[PRE78]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: To batch these sequences, we will pad zeros to the end of short sequences, resulting
    in
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些序列分组，我们将在短序列的末尾填充零，结果是
- en: '[PRE79]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: You can see that now we have a batch of data with fixed-sequence lengths that
    can be converted to a tf.Tensor.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，现在我们有一批数据，具有固定序列长度，可以转换为tf.Tensor。
- en: 'Fortunately, all we need to worry about in order to use bucketing is understanding
    the syntax of a convenient TensorFlow function that is already provided: tf.data
    .experimental.bucket_by_sequence_length(). The experimental namespace is a special
    namespace allocated for TensorFlow functionality that has not been fully tested.
    In other words, there might be edge cases where these functions might fail. Once
    the functionality is well tested, these cases will move out of the experimental
    namespace into a stable one. Note that this function returns another function
    that performs the bucketing on a data set. Therefore, you have to use this function
    in conjunction with tf.data.Dataset.apply() in order to execute the returned function.
    The syntax can be slightly cryptic at first glance. But things will be clearer
    when we take a deeper look at the arguments. You can see that we’re using the
    bucket boundaries we identified earlier when analyzing the sequence lengths of
    the reviews:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，为了使用桶化，我们需要关心的只是理解一个方便的TensorFlow函数tf.data.experimental.bucket_by_sequence_length()的语法。实验性命名空间是为尚未完全测试的TensorFlow功能分配的特殊命名空间。换句话说，可能存在这些函数可能失败的边缘情况。一旦功能经过充分测试，这些情况将从实验性命名空间移出，进入稳定的命名空间。请注意，该函数返回另一个在数据集上执行桶化的函数。因此，你必须将此函数与tf.data.Dataset.apply()一起使用，以执行返回的函数。这个语法乍一看可能有点晦涩。但当我们深入研究参数时，事情会变得更清晰。你可以看到，当分析评价的序列长度时，我们正在使用我们之前确定的桶边界：
- en: '[PRE80]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Let’s examine the arguments provided to this function:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查该函数提供的参数：
- en: elment_length_func—This is at the heart of the bucketing function as it tells
    the function how to compute the length of a single record or instance coming in.
    Without the length of the record, bucketing is impossible.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: elment_length_func—这是桶函数的核心，因为它告诉函数如何计算单个记录或实例的长度。如果没有记录的长度，桶是无法实现的。
- en: bucket_boundaries—Defines the upper bound of bucket boundaries. This argument
    accepts a list of values in increasing order. If you provided bucket_bounderies
    [x, y, z], where x < y < z, then the bucket intervals would be [0, x), [x, y),
    [y, z), [z, inf).
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bucket_boundaries—定义桶边界的上限。该参数接受一个按升序排列的值列表。如果你提供了bucket_bounderies [x, y, z]，其中x
    < y < z，那么桶的间隔将是[0, x)，[x, y)，[y, z)，[z, inf)。
- en: bucket_batch_sizes—Batch size for each bucket. You can see that we have defined
    the same batch size for all the buckets. But you can also use other strategies,
    such as higher batch size for shorter sequences.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bucket_batch_sizes—每个桶的批次大小。你可以看到，我们为所有桶定义了相同的批次大小。但你也可以使用其他策略，比如更短序列的更大批次大小。
- en: padded_values—This defines, when bringing sequences to the same length, what
    to pad the short sequences with. Padding with zero is a very common method. We
    will stick with that.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: padded_values—当将序列调整为相同长度时，定义短序列应该用什么填充。用零填充是一种非常常见的方法。我们将坚持使用这种方法。
- en: pad_to_bucket_boundary—This is a special Boolean argument that will decide the
    final size of the variable dimension of each batch. For example, assume you have
    a bucket with the interval [0, 11) and a batch of sequences with lengths [4, 8,
    5]. If pad_to_bucket_boundary=True, the final batch will have the variable dimension
    of 10, which means every sequence is padded to the maximum limit. If pad_to_bucket_boundary=False,
    you’ll have the variable dimension of 8 (i.e., the length of the longest sequence
    in the batch).
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pad_to_bucket_boundary—这是一个特殊的布尔参数，将决定每个批次的变量维度的最终大小。例如，假设你有一个区间为[0, 11)的桶和一批序列长度为[4,
    8, 5]。如果pad_to_bucket_boundary=True，最终批次将具有变量维度为10，这意味着每个序列都被填充到最大限制。如果pad_to_bucket_boundary=False，你将得到变量维度为8（即批次中最长序列的长度）。
- en: 'Remember that we had a tf.RaggedTensor initially fed to the tf.data.Dataset.from_tensor_slices
    function. When returning slices, it will return slices with the same data type.
    Unfortunately, tf.RaggedTensor objects are not compatible with the bucketing function.
    Therefore, we perform the following hack to convert slices back to tf.Tensor objects.
    We simply call the map function with the lambda function lambda x: x. With that,
    you can call the tf.data.Dataset.apply() function with the bucket_fn as the argument:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '请记住，最初传递给tf.data.Dataset.from_tensor_slices函数的是tf.RaggedTensor。在返回切片时，它将返回相同的数据类型的切片。不幸的是，tf.RaggedTensor对象与桶函数不兼容。因此，我们执行以下方法将切片转换回tf.Tensor对象。我们只需使用lambda函数lambda
    x: x调用map函数。通过这样做，你可以使用tf.data.Dataset.apply()函数，并将桶函数作为参数来调用它：'
- en: '[PRE81]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: At this point, we have done all the hard work. By now, you have implemented
    the functionality to accept a data set with arbitrary length sequences and to
    sample a batch of sequences from that using the bucketing strategy. The bucketing/binning
    strategy used here makes sure that we don’t group sequences with large differences
    in their lengths, which will lead to excessive padding.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一步，我们已经完成了所有的工作。到目前为止，您已经实现了接受任意长度序列数据集的功能，以及使用分桶策略从中抽取一批序列的功能。这里使用的分桶策略确保我们不会将长度差异很大的序列分组在一起，这将导致过多的填充。
- en: 'As we have done many times, let’s shuffle the data to make sure we observe
    enough randomness during the training phase:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们之前做过很多次一样，让我们打乱数据，以确保在训练阶段观察到足够的随机性：
- en: '[PRE82]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Remember that we combined the target label and the input to ensure correspondence
    between inputs and targets. Now we can safely split the target and input into
    two separate tensors using tensor slicing syntax as shown:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们将目标标签和输入结合在一起，以确保输入和目标之间的对应关系。现在，我们可以使用张量切片语法将目标和输入安全地分割成两个独立的张量，如下所示：
- en: '[PRE83]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Now we can let out a sigh of relief. We have completed all the steps of the
    journey from raw unclean text to clean semi-structured text that can be consumed
    by our model. Let’s wrap this in a function called get_tf_pipeline(), which takes
    a text_seq (list of lists of word IDs), labels (list of integers), batch_size
    (int), bucket_boundaries (list of ints), max_length (int), and shuffle (Boolean)
    arguments (see the following listing).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以松口气了。我们已经完成了从原始杂乱的文本到可以被我们的模型消化的干净半结构化文本的旅程。让我们把它封装成一个名为get_tf_pipeline()的函数，该函数接受一个text_seq（单词ID列表的列表）、标签（整数列表）、批处理大小（整数）、桶边界（整数列表）、最大长度（整数）和随机混洗（布尔值）的参数（参见下面的列表）。
- en: Listing 9.4 The tf.data pipeline
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4 tf.data数据管道
- en: '[PRE84]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: ❶ Concatenate the label and the input sequence so that we don’t mess up the
    order when we shuffle.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 连接标签和输入序列，以防止洗牌时混乱顺序。
- en: ❷ Define the variable sequence data set as a ragged tensor.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将变量序列数据集定义为不规则张量。
- en: ❸ Create a data set out of the ragged tensor.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从不规则张量创建数据集。
- en: ❹ Bucket the data (assign each sequence to a bucket depending on the length).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对数据进行分桶（根据长度将每个序列分配到不同的分桶中）。
- en: ❺ For example, for bucket boundaries [5, 15], you get buckets [0, 5], [5, 15],
    [15,inf].
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 例如，对于分桶边界[5, 15]，您可以得到分桶[0, 5]、[5, 15]、[15, 无穷大]。
- en: ❻ Apply bucketing.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 应用分桶技术。
- en: ❼ Shuffle the data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 打乱数据。
- en: ❽ Split the data to inputs and labels.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将数据分割为输入和标签。
- en: It’s been a long journey. Let’s reflect on what we have done so far. With the
    data pipeline done and dusted, let’s learn about the models that can consume this
    type of sequential data. Next, we will define the sentiment analyzer model that
    we’ve been waiting to implement.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个漫长的旅程。让我们回顾一下我们迄今为止所做的事情。数据管道已经完成并且稳定，现在让我们了解一下可以使用这种类型的顺序数据的模型。接下来，我们��定义情感分析器模型，这是我们一直在等待实现的模型。
- en: Exercise 3
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3
- en: If you want have the buckets (0, 10], (10, 25], (25, 50], [50, inf) and always
    return padded to the boundary of the bucket, how would you modify this bucketing
    function? Note that the number of buckets has changed from the number we have
    in the text.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望有桶(0, 10]、(10, 25]、(25, 50]、[50, 无穷大)并且始终返回填充到桶的边界上，您将如何修改这个分桶函数？请注意，桶的数量已从文本中的数量发生了变化。
- en: '9.4 Happy reviews mean happy customers: Sentiment analysis'
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 快乐的评论意味着快乐的顾客：情感分析
- en: 'Imagine you have converted the reviews to numbers and defined a data pipeline
    that generates batches of inputs and labels. Now it’s time to crunch them using
    a model to train a model that can accurately identify sentiments in a posted review.
    You have heard that long short-term memory models (LSTMs) are a great starting
    point for processing textual data. The goal is to implement a model based on LSTMs
    that produces one of two possible outcomes for a given review: a negative or a
    positive sentiment.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您已经将评论转换为数字，并定义了一个数据管道，生成输入和标签的批处理。现在是时候使用模型来处理它们，以训练一个能够准确识别发布的评论情感的模型。您听说过长短期记忆模型（LSTMs）是处理文本数据的一个很好的起点。目标是基于LSTMs实现一个模型，针对给定的评论产生两种可能的结果之一：负面情感或积极情感。
- en: 'If you have reached this point, you should be happy. You have ploughed through
    a lot. Now it’s time to reward yourself with information about a compelling family
    of models known as deep sequential models. Some example models of this family
    are as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经到达这一步，您应该会感到快乐。您已经完成了很多工作。现在是时候奖励自己获取关于一个称为深度顺序模型的引人注目的模型家族的信息。该家族的一些示例模型如下：
- en: Simple recurrent neural networks (RNNs)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单循环神经网络（RNNs）
- en: Long short-term memory (LSTM) networks
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）网络
- en: Gated recurrent units (GRUs)
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控循环单元（GRUs）
- en: 9.4.1 LSTM Networks
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 LSTM网络
- en: 'Previously we discussed a simple recurrent neural network and its application
    in predicting CO2 concentration levels in the future. In this chapter, we will
    look at the mechanics of LSTM networks. LSTMs models were very popular for almost
    a decade. They are a great choice for processing sequential data and typically
    have three important dimensions:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们讨论了简单的循环神经网络及其在预测未来CO2浓度水平中的应用。在本章中，我们将探讨LSTM网络的机制。LSTM模型在近十年间非常流行。它们是处理序列数据的绝佳选择，通常具有三个重要的维度：
- en: A batch dimension
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个批量维度
- en: A time dimension
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个时间维度
- en: A feature dimension
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个特征维度
- en: If you think about the data returned by the NLP pipeline we discussed, it has
    all these dimensions. The batch dimension is represented by each different review
    sampled into that batch. The time dimension is represented by the sequence of
    word IDs appearing in a single review. Finally, you can think of feature dimension
    being 1, as a single feature is represented by a single numerical value (i.e.,
    an ID; see figure 9.2). The feature dimension has values corresponding to features
    on that dimension. For example, if you have a weather model that has three features
    (e.g., temperature, precipitation, wind speed), the input to the model would be
    [<batch size>, <sequence length>, 3]-sized input.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑我们讨论的NLP管道返回的数据，它具有所有这些维度。批量维度由采样到该批量中的每个不同评论来表示。时间维度由单个评论中出现的词ID序列表示。最后，你可以将特征维度看作为1，因为一个单一的特征由一个单一的数值（即一个ID）表示（参见图9.2）。特征维度具有对应于该维度上的特征的值。例如，如果你有一个包含三个特征的天气模型（例如，温度、降水、风速），那么模型的输入将是大小为[<批量大小>,
    <序列长度>, 3]的输入。
- en: '![09-02](../../OEBPS/Images/09-02.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![09-02](../../OEBPS/Images/09-02.png)'
- en: Figure 9.2 3D view of sequential data. Typically, sequential data is found with
    three dimensions; batch size, sequence/time, and feature.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 序列数据的三维视图。通常，序列数据具有三个维度：批量大小、序列/时间和特征。
- en: The LSTM takes an input with the three dimensions discussed. Let’s zoom in a
    bit more to see how an LSTM operates on such data. To keep the discussion simple,
    assume a batch size of 1 or a single review. If we assume a single review r that
    has n words it can be written as
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM采用了讨论的三个维度的输入。让我们更深入地了解一下LSTM是如何在这样的数据上运作的。为了简化讨论，假设批量大小为1或仅有一个评论。如果我们假设有一个包含*n*个词的单一评论*r*，可以表示为
- en: '*r* = *w*[1,]*w*[2,...,]*w*[t,...,]*w*[n], where *w*[t] represents the ID of
    the word in the *t*^(th) position.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '*r* = *w*[1,]*w*[2,...,]*w*[t,...,]*w*[n]，其中*w*[t]表示第*t*个位置的词的ID。'
- en: At time step t, the LSTM model starts with
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步*t*，LSTM模型从以下状态开始
- en: The previous output state vector *h*[t-1]
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上一个输出状态向量*h*[t-1]
- en: The previous cell state vector *c*[t-1]
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上一个单元状态向量*c*[t-1]
- en: and computes
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 并计算
- en: The current cell state *c*[t] using the current input *w*[t] and the previous
    cell state *c*[t-1] and output state *h*[t-1] and
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用当前输入*w*[t]和上一个单元状态*c*[t-1]以及输出状态*h*[t-1]来计算当前单元状态*c*[t]
- en: The current output state *h*[t] using the current input *w*[t] and the previous
    state *h*[t-1] and the current cell state *c*[t]
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用当前输入*w*[t]、上一个状态*h*[t-1]和当前单元状态*c*[t]来计算当前输出状态*h*[t]
- en: This way, the model keeps iterating over all the timesteps (in our example,
    it’s word IDs in the sequence) until it reaches the end. While iterating this
    way, the model keeps producing a cell state and an output state (figure 9.3).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，模型就会持续迭代所有时间步（在我们的示例中，它是序列中的单词ID），直到到达末尾。在这种迭代过程中，模型会持续产生一个单元状态和一个输出状态（图9.3）。
- en: '![09-03](../../OEBPS/Images/09-03.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![09-03](../../OEBPS/Images/09-03.png)'
- en: Figure 9.3 High-level longitudinal view of an LSTM cell. At a given time step
    t, the LSTM cell takes in two previous states (*h*[t-1] and *c*[t-1]), along with
    the input, and produces two states (*h*[t] and *c*[t]).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 LSTM单元的高层纵向视图。在给定的时间步*t*，LSTM单元接收两个先前状态（*h*[t-1]和*c*[t-1]），以及输入，并产生两个状态（*h*[t]和*c*[t]）。
- en: 'With a good high-level understanding of the LSTM cell, let’s look at the equations
    that are cranking the gears of this model. The LSTM takes three inputs:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 有了对LSTM单元的良好高层次理解，让我们来看看推动该模型齿轮的方程式。LSTM接受三个输入：
- en: '*x*[t]—The input at timestep *t*'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[t]—时间步*t*处的输入'
- en: '*h*[t-1]—The output state at timestep *t*-1'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*[t-1]—时间步*t*-1处的输出状态'
- en: '*c*[t-1]—The cell state at timestep *t*-1'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c*[t-1]—时间步*t*-1处的单元状态'
- en: 'With that, the LSTM produces two outputs:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，LSTM会产生两个输出：
- en: (*c*[t])—The cell state at timestep *t*
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (*c*[t])—时刻 *t* 的单元状态
- en: (*h*[t])—The output state at timestep *t*
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (*h*[t])—时刻 *t* 的输出状态
- en: 'To produce these outputs, the LSTM model leverages a gating mechanism. These
    gates determine how much information flows through them to the next stage of computations.
    The LSTM cell has three gates:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为了产生这些输出，LSTM 模型利用了一个门控机制。这些门控机制决定了多少信息通过它们流向计算的下一阶段。LSTM 单元有三个门控：
- en: '*An input gate* (*i*[t])—Determines how much of the current input will affect
    the subsequent computations'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个输入门* (*i*[t])—确定当前输入对后续计算的影响程度'
- en: '*A forget gate* (*f*[t])—Determines how much of the previous cell state is
    discarded when computing the new cell state'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个遗忘门* (*f*[t])—确定在计算新单元状态时有多少先前单元状态被丢弃'
- en: '*An output gate* (*o*[t])—Determines how much of the current cell state contributes
    to the final output'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个输出门* (*o*[t])—确定当前单元状态对最终输出的贡献程度'
- en: These gates are made of trainable weights. This means that when training an
    LSTM model on a certain task, the gating mechanism will be jointly optimized to
    produce the optimal flow of information required to solve that task.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门由可训练的权重构成。这意味着当在特定任务上训练 LSTM 模型时，门控机制将进行联合优化，以产生解决该任务所需的最佳信息流动。
- en: What carries long-term and short-term memories in LSTMs?
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 中，长期记忆和短期记忆是由什么承载的？
- en: The cell state preserves the long-term information and relationships as the
    model progresses through the input over the time dimension. In fact, it has been
    found that LSTMs can remember up to hundreds of timesteps when learning time-series
    problems.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 单元状态在模型在时间维度上进展的过程中保留了长期信息和关系。事实上，研究发现 LSTM 在学习时间序列问题时可以记住长达数百个时间步骤。
- en: On the other hand, the output state can be thought as the short-term memory
    where it will look at the input, the long-term memory stored in the cell state,
    and decide the optimal amount of information required at that stage of the computation.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，输出状态可以被看作是短期记忆，它会查看输入、存储在单元状态中的长期记忆，并决定计算阶段所需的最佳信息量。
- en: 'You might still be wondering, “What is this gating mechanism actually achieving?”
    Let me illustrate that with a sentence. To solve almost all NLP tasks, capturing
    syntactic and semantic information as well parsing dependencies correctly in a
    given text input is imperative. Let’s see how an LSTM can help us to achieve that.
    Assume you’ve been given the sentence:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会想，“这个门控机制到底实现了什么？”让我用一个句子来说明。为了解决几乎所有的自然语言处理任务，捕捉句子中的句法和语义信息以及正确解析依赖关系是至关重要的。让我们看看
    LSTM 如何帮助我们实现这一目标。假设你获得了以下句子：
- en: the dog ran after the green ball and it got tired and barked at a plane
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 狗追着绿球跑，它累了并朝飞机狂吠
- en: In your mind, picture an LSTM model hopping from one word to another, processing
    them. Then assume you query the model for answers to questions at various stages
    of processing the sentence. Say you ask the question “Who ran?” while it’s processing
    the phrase “the dog ran.” The model will probably have its input gate widely open
    to absorb as much information as the model can, because the model starts with
    no prior knowledge about what language looks like. And if you think about it,
    the model does not really need to pay attention to its memory because the answer
    is one word away from the word “ran.”
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的脑海中，想象一个 LSTM 模型从一个词跳到另一个词，逐个处理它们。然后假设你在处理句子的各个阶段向模型提问。例如你问“谁跑了？”当它处理短语“狗跑了”时。模型可能会广泛打开输入门来吸收尽可能多的信息，因为模型开始时对语言的外观一无所知。而且如果你仔细考虑，模型实际上不需要关注它的记忆，因为答案离“跑”这个词只有一个词的距离。
- en: Next, you ask “Who got tired?” When processing “it got tired,” the model might
    want to tap into its cell state instead of focusing on the input, as the only
    clue in this phrase is “it.” If the model is to identify the relationship between
    it and dog, it will need to close the input gate slightly and open the forget
    gate so that more information flows from the past memory (about the dog) into
    the current memory.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你问“谁累了？”在处理“它累了”时，模型可能希望利用它的单元状态而不是关注输入，因为这个短语中唯一的线索是“它”。如果模型要识别它和狗之间的关系，它将需要稍微关闭输入门并打开遗忘门，以使更多的信息从过去的记忆（关于狗的记忆）流入当前记忆中。
- en: Finally, let’s say you ask “What was barked at?” by the time the model reaches
    the “barked at a plane” section. To produce the final output, you don’t need much
    information from past memory, so you might tighten the output gate to avoid too
    much information coming from the past memory. I hope these walkthroughs were useful
    in order to grok the purpose of these gates. Remember that this is just an analogy
    to understand the purpose of these gates. But in practice, the actual behavior
    can differ. It is also worth noticing that these gates are not binary; rather,
    the output of the gate is controlled by a sigmoidal function, meaning that it
    leads to a soft open/close state at a given time rather than a hard open/close
    state.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设当模型达到“对飞机吠叫”的部分时，你问：“被吠叫了什么？”为了产生最终输出，你不需要太多来自过去记忆的信息，因此你可能会紧缩输出门，以避免过多来自过去记忆的信息。我希望这些演示对理解这些门的目的有所帮助。请记住，这只是一种理解这些门目的的比喻。但在实践中，实际行为可能会有所不同。值得注意的是，这些门不是二进制的；相反，门的输出由一个
    S 型函数控制，这意味着它在给定时间上产生软开/关状态，而不是硬开/关状态。
- en: 'To complete our discussion, let’s inspect the equations that drive the computations
    in an LSTM cell. But you don’t have to memorize or understand these equations
    in detail, as it’s not a requirement to use LSTMs. But to make our discussion
    holistic, let’s look at them. The first computation computes the input gate:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们的讨论，让我们检查驱动 LSTM 单元中计算的方程式。但你不必详细记忆或理解这些方程式，因为使用 LSTM 不需要这样做。但为了使我们的讨论全面，让我们来看看它们。第一个计算计算输入门：
- en: '*i*[t] *= σ*(*W*[ih]*h*[t-1] *+ W*[ix]*x*[t] *+ b*[f])'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '*i*[t] *= σ*(*W*[ih]*h*[t-1] *+ W*[ix]*x*[t] *+ b*[f])'
- en: Here, *W*[ih] and *W*[ix] are trainable weights that produce the gate value,
    where *b*[i] is the bias. The computations here closely resemble the computations
    of a fully connected layer. This gate results in a vector for a given input whose
    values are between 0 and 1\. You can see its resemblance to a gate (assuming 0
    means closed and 1 means open). The rest of the gates follow a similar pattern
    of computations. The forget gate is computed as
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*[ih] 和 *W*[ix] 是可训练的权重，产生门值，其中 *b*[i] 是偏置。这里的计算与完全连接层的计算非常相似。该门产生了一个向量，其值介于
    0 和 1 之间。你可以看到它与门的相似性（假设 0 表示关闭，1 表示打开）。其余的门遵循类似的计算模式。遗忘门计算为
- en: '*f*[t] *= σ*(*W*[fh]*h*[t-1] *+ W*[fx]*x*[t] *+ b*[f])'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*[t] *= σ*(*W*[fh]*h*[t-1] *+ W*[fx]*x*[t] *+ b*[f])'
- en: 'Then the cell state is computed. Cell state is computed from a two-fold computation:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算单元状态。单元状态是从两重计算中计算得到的：
- en: '*C̃*[t] = tanh(*W*[ch] *h*[t][-1] + *W*[cx]*x*[t] = *b*[c])'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '*C̃*[t] = tanh(*W*[ch] *h*[t][-1] + *W*[cx]*x*[t] = *b*[c])'
- en: '*c*[t] = *f*[t]*h*[t][-1] + *i*[t]*C̃*[t]'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '*c*[t] = *f*[t]*h*[t][-1] + *i*[t]*C̃*[t]'
- en: The computation is quite intuitive. It uses the forget gate to control the previous
    cell state, where it uses the input gate to control *C̃*[t] computed using *x*[t]
    (the current input). Finally, the output gate and the state are computed as
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 计算相当直观。它使用遗忘门来控制先前的单元状态，其中使用输入门来控制使用 *x*[t]（当前输入）计算的 *C̃*[t]。最后，输出门和状态计算如下
- en: '*o*[t] *= σ*(*W*[oh]*h*[t-1] *+ W*[ox]*x*[t] *+ b*[0])'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '*o*[t] *= σ*(*W*[oh]*h*[t-1] *+ W*[ox]*x*[t] *+ b*[0])'
- en: '*h*[t] *= o*[t]*tanh*(*c*[t])'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[t] *= o*[t]*tanh*(*c*[t])'
- en: Here, *c*[t] is computed using the inputs controlled via the forget gate and
    the input gate. Therefore, in a way, *o*[t] gets to control how much the current
    input, the current cell state, the previous cell state, and the previous output
    state contribute to the final state output of the LSTM cell. In TensorFlow and
    Keras, you can define an LSTM with
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*c*[t] 是使用通过遗忘门和输入门控制的输入计算得到的。因此，在某种程度上，*o*[t] 控制着当前输入、当前单元状态、上一个单元状态和上一个输出状态对
    LSTM 单元最终状态输出的贡献。在 TensorFlow 和 Keras 中，你可以这样定义一个 LSTM：
- en: '[PRE85]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The first argument units is a hyperparameter of the LSTM layer. Similar to how
    the number of units defines the output size of a fully connected layer, the units
    argument defines the output, state, and gate vector dimensionality. The higher
    this number, the more representative power the model possesses. Next, the return_state=False
    means only the output state will be returned when the layer is called on an input.
    If return_state=True, both the cell state and the output state are returned. Finally,
    return_sequences=False means that only the final state(s) after processing the
    whole sequence is returned. If return_sequences=True, all the state(s) returned
    during processing every element in the sequence are returned. Figure 9.4 depicts
    the differences in these arguments’ results.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数 units 是 LSTM 层的超参数。类似于单位数定义了完全连接层的输出大小，units 参数定义了输出、状态和门向量的维度。这个数字越高，模型的代表能力就越强。接下来，return_state=False
    表示当在输入上调用该层时，只返回输出状态。如果 return_state=True，则同时返回细胞状态和输出状态。最后，return_sequences=False
    表示只返回处理整个序列后的最终状态。如果 return_sequences=True，则在处理序列中的每个元素时返回所有状态。图 9.4 描述了这些参数结果的差异。
- en: '![09-04](../../OEBPS/Images/09-04.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![09-04](../../OEBPS/Images/09-04.png)'
- en: Figure 9.4 The changes to the output of the LSTM layer resulted in changes to
    the return_state and return_sequences arguments.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 LSTM 层输出的更改导致了 return_state 和 return_sequences 参数的更改。
- en: Next, let’s define the final model.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义最终模型。
- en: 9.4.2 Defining the final model
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 定义最终模型
- en: 'We will define the final model using the Sequential API. Our model will have
    the following layers (figure 9.5):'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Sequential API 定义最终模型。我们的模型将包含以下层（图 9.5）：
- en: '*A masking layer*—This layer plays an important role in which input elements
    in the sequence will contribute to the training. We will learn more about this
    soon.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个屏蔽层* —— 这一层在决定序列中的哪些输入元素将有助于训练方面起着重要作用。我们很快将学到更多相关内容。'
- en: '*A one-hot encoding layer*—This layer will convert the word IDs to one-hot
    encoded sequences. This is an important transformation we have to perform before
    feeding our inputs to the model.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个独热编码层* —— 这一层将把单词 ID 转换为独热编码序列。这是在将输入馈送给模型之前必须执行的重要转换。'
- en: '*An LSTM layer*—The LSTM layer will return the final output state as the output.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个 LSTM 层* —— LSTM 层将最终输出状态作为输出返回。'
- en: '*A* Dense *layer with 512 nodes (ReLU activation)*—A Dense layer takes the
    output of the LSTM cell and produces an interim hidden output.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个具有 512 个节点（ReLU 激活）的 Dense 层* —— Dense 层接收 LSTM 单元的输出，并产生一个临时的隐藏输出。'
- en: '*A* Dropout *layer*—Dropout is a regularization technique that randomly switches
    outputs during the training process. We discussed the purpose of Dropout and how
    it works in chapter 7.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个* Dropout *层* —— Dropout 是一种在训练过程中随机关闭输出的正则化技术。我们在第 7 章中讨论了 Dropout 的目的和工作原理。'
- en: '*A final output layer* *with a single node (sigmoid activation)*—Note that
    we only need a single node to represent the output. If the value of the output
    is 0, it’s a negative sentiment. If the value is 1, it’s a positive sentiment.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*具有单个节点（sigmoid 激活）的最终输出层* —— 注意，我们只需要一个节点来表示输出。如果输出值为 0，则是负面情感。如果值为 1，则是正面情感。'
- en: '![09-05](../../OEBPS/Images/09-05.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![09-05](../../OEBPS/Images/09-05.png)'
- en: Figure 9.5 The high-level model architecture of the sentiment analyzer
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 情感分析器的高层模型架构
- en: 'Our tf.data pipeline produces a [<batch size>, <sequence length>]-shaped 2D
    tensor. In practice, they would both be None. In other words, it will be a [None,
    None]-sized tensor as we have to support variable-sized batches and variable-sized
    sequence lengths in our model. A dimension of size None means that the model can
    accept any sized tensor on that dimension. For example, with a [None, None] tensor,
    when actual data is retrieved, it can be a [5, 10]-, [12, 54]-, or [102, 14]-sized
    tensor. As the entry point to the model, we will use a reshaping layer wrapped
    in a lambda layer as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 tf.data pipeline 生成一个 [<batch size>, <sequence length>] 形状的二维张量。在实践中，它们都可以是
    None。换句话说，它将是一个 [None, None] 大小的张量，因为我们必须支持模型中可变大小的批次和可变大小的序列长度。大小为 None 的维度意味着模型可以在该维度上接受任何大小的张量。例如，对于一个
    [None, None] 张量，当实际数据被检索时，它可以是一个 [5, 10]、[12, 54] 或 [102, 14] 大小的张量。作为模型的入口点，我们将使用一个重塑层包装在
    lambda 层中，如下所示：
- en: '[PRE86]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: This layer takes our [None, None] input produced by the data pipeline and reshapes
    it to a [None, None, 1]-sized tensor. This reshaping is necessary for the next
    layer in line, making it a perfect opportunity to discuss the next layer. The
    next layer is a masking layer that serves a very special purpose. We have not
    seen a masking layer used in previous chapters. However, masking is commonly used
    in NLP problems. The need for masking arises from the padding operation we perform
    on the inputs during the bucketing of input sequences. In NLP data sets, you will
    seldom see text appearing with a fixed length. Typically, each text record has
    a different length. To batch these variable-sized text records together for the
    model, padding plays an essential role. Figure 9.6 illustrates what the data looks
    like after padding.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层接收到数据管道生成的 [None, None] 输入，并将其重新塑造成一个 [None, None, 1] 大小的张量。这种重新塑造对于接下来的层来说是必要的，这也是讨论下一层的一个绝佳机会。接下来的层是一个特殊用途的遮罩层。我们在之前的章节中没有看到过遮罩层的使用。然而，在自然语言处理问题中，遮罩是常用的。遮罩的需求源自我们在输入序列进行桶装过程中执行的填充操作。在自然语言处理数据集中，很少会看到文本以固定长度出现。通常，每个文本记录的长度都不同。为了将这些大小不同的文本记录批量处理给模型，填充起着至关重要的作用。图9.6展示了填充后数据的样子。
- en: '![09-06](../../OEBPS/Images/09-06.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![09-06](../../OEBPS/Images/09-06.png)'
- en: Figure 9.6 Text sequences before and after padding
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 填充前后的文本序列
- en: 'But this introduces an extra burden. The values introduced by padding (typically
    zero) do not carry any information. Therefore, they should be ignored in any computation
    that happens in the model. For example, the LSTM model should halt processing
    and return that last state just before encountering padded values when padding
    is used in the input. The tf.keras.layers.Masking layer helps us to do exactly
    that. The input to the masking layer must be a [batch size, sequence length, feature
    dimension]-sized 3D tensor. This alludes to our last point about reshaping the
    output of our tf.data pipeline to a 3D tensor. In TensorFlow you define a mask
    as follows:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这会引入额外的负担。填充引入的值（通常为零）不携带任何信息。因此，在模型中进行的任何计算都应该忽略它们。例如，当输入中使用填充时，LSTM模型应该停止处理，并在遇到填充值之前返回最后一个状态。tf.keras.layers.Masking
    层帮助我们做到了这一点。遮罩层的输入必须是一个 [批量大小，序列长度，特征维度] 大小的三维张量。这暗示了我们最后一点，即将我们的 tf.data 管道的输出重新塑造为三维张量。在
    TensorFlow 中，您可以按如下方式定义一个遮罩：
- en: '[PRE87]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The masking layer creates a special mask, and this mask is propagated to the
    subsequent layers in the model. Layers like the LSTM layer know what to do if
    a mask is passed from a layer below. More specifically, the LSTM model will output
    its state values just before it encountered zeros (if a mask is provided). It
    is also worth paying attention to the input_shape argument. The input to our model
    will be a two-dimensional tensor: an arbitrary-sized batch with an arbitrary-sized
    sequence length (due to bucketing). Therefore, we cannot specify a sequence length
    in the input_shape argument, so the model expects a (None, None, 1)-sized tensor
    as the input (the extra None is added automatically to represent the batch dimension).'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 遮罩层创建了一个特殊的遮罩，并且这个遮罩被传递到模型中的后续层。像 LSTM 层这样的层知道如果从下面的一层传递了一个遮罩，它应该做什么。更具体地说，如果提供了一个遮罩，LSTM
    模型将输出它遇到零值之前的状态值。此外，还值得注意 input_shape 参数。我们模型的输入将是一个二维张量：一个任意大小的批次，以及一个任意大小的序列长度（由于桶装）。因此，我们无法在
    input_shape 参数中指定一个序列长度，所以模型期望的输入是一个 (None, None, 1) 大小的张量（额外的 None 自动添加以表示批次维度）。
- en: 'With the mask defined, we will convert the word IDs to one-hot vectors using
    a custom layer. This is an essential step before feeding data to the LSTM. This
    can be achieved as follows:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了遮罩后，我们将使用自定义层将单词ID转换为独热向量。在将数据馈送到 LSTM 之前，这是一个关键步骤。这可以通过以下方式实现：
- en: '[PRE88]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Then call it with
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下方式调用它
- en: '[PRE89]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The layer is a mouthful, so let’s break it down. First you define a user-defined
    parameter called depth. This defines the feature dimension of the final result.
    Next, you have to define the call() function. The call() function takes in the
    inputs, casts them to ''int32'', and then removes the final dimension if the input
    is three-dimensional. This is because the masking layer we defined has a dimension
    of size 1 to represent the feature dimension. This dimension is not understood
    by the tf.one_hot() function that we use to generate one-hot encoded vectors.
    Therefore, it must be removed. Finally, we return the result of the tf.one_hot()
    function. Remember to provide the depth parameter when using tf.one_hot(). If
    it is not provided, TensorFlow tries to automatically infer the value, which leads
    to inconsistently sized tensors between different batches. We define the compute_mask()
    function to make sure we propagate the mask to the next layer. The layer simply
    takes the mask and passes it to the next layer. Finally, we define a get_config()
    function to update the parameters in that layer. It is essential for config to
    return the correct set of parameters; otherwise, you will run into problems saving
    the model. We define the LSTM layer as the next layer of the model:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层有点复杂，所以让我们一步步来。首先，您定义一个称为 depth 的用户定义参数。这定义了最终结果的特征维度。接下来，您必须定义 call() 函数。call()
    函数接受输入，将它们转换为'int32'，然后如果输入是三维的，则移除最终维度。这是因为我们定义的遮罩层具有大小为 1 的维度来表示特征维度。这个维度在我们用来生成一位热编码向量的
    tf.one_hot() 函数中不被理解。因此，它必须被移除。最后，我们返回 tf.one_hot() 函数的结果。记住在使用 tf.one_hot() 时提供
    depth 参数。如果没有提供，TensorFlow 尝试自动推断值，这会导致不同批次之间的张量大小不一致。我们定义 compute_mask() 函数来确保我们将遮罩传播到下一层。该层只是获取遮罩并将其传递给下一层。最后，我们定义一个
    get_config() 函数来更新该层中的参数。正确返回一组参数对于配置来说是至关重要的；否则，您将在保存模型时遇到问题。我们将 LSTM 层定义为模型的下一层：
- en: '[PRE90]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: More about propagating masks within models
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在模型中传播遮罩的更多信息
- en: It is important to remember a few things when using a masking layer. First,
    it is better to avoid using lambda layers in your model when using masking. This
    is because there have been several issues raised when using masking in conjunction
    with lambda layers (e.g., [https://github.com/tensorflow/tensorflow/issues/40085](https://github.com/tensorflow/tensorflow/issues/40085)).
    The best option is to write a custom layer as we have done. After defining a custom
    layer, you have to override the compute_mask() function to return the mask (with
    modifications, if required) for the next layer.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用遮罩层时，记住一些重要事项是很重要的。首先，在使用遮罩时最好避免使用 lambda 层。这是因为在使用遮罩与 lambda 层同时存在时可能会出现一些问题（例如，[https://github.com/tensorflow/tensorflow/issues/40085](https://github.com/tensorflow/tensorflow/issues/40085)）。最佳选择是编写自定义层，就像我们所做的那样。在定义了自定义层之后，您必须重写
    compute_mask() 函数以返回（如果需要的话进行修改的）下一层的遮罩。
- en: 'We have to be extra careful here. Depending on the arguments you provide when
    defining this layer, you will get vastly different outputs. To define our sentiment
    analyzer, we only want the final output state of the model. This means we’re not
    interested in the cell state, nor all the output states computed during processing
    the sequence. Therefore, we have to set the arguments accordingly. According to
    our requirements, we must set return_state=False and return_sequences=False. Finally,
    the final state output goes to a Dense layer with 512 units and ReLU activation:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里必须特别小心。根据您在定义此层时提供的参数，您将得到截然不同的输出。为了定义我们的情感分析器，我们只想要模型的最终输出状态。这意味着我们对单元状态不感兴趣，也不关心在处理序列期间计算的所有输出状态。因此，我们必须相应地设置参数。根据我们的要求，我们必须设置
    return_state=False 和 return_sequences=False。最后，最终状态输出进入一个具有 512 个单位和 ReLU 激活的密集层：
- en: '[PRE91]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: The dense layer is followed by a Dropout layer that will drop 50% of the inputs
    of the previous Dense layer during training.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 密集层后跟一个 Dropout 层，该层在训练期间会丢弃前一个密集层的 50% 输入。
- en: '[PRE92]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Finally, the model is crowned with a Dense layer having a single unit and sigmoidal
    activation, which will produce the final prediction. If the produced value is
    less than 0.5, it is considered label 0 and 1 otherwise:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型被一个具有单个单元和 sigmoid 激活的密集层加冕，这将产生最终的预测。如果生成的值小于 0.5，则被认为是标签 0，否则为标签 1：
- en: '[PRE93]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: We can define the full model as shown in the next listing.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照下一个清单中所示定义完整的模型。
- en: Listing 9.5 Implementation of the full sentiment analysis model
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 9.5 完整情感分析模型的实现
- en: '[PRE94]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: ❶ Create a mask to mask out zero inputs.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个遮罩来屏蔽零输入。
- en: ❷ After creating the mask, convert inputs to one-hot encoded inputs.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建掩码后，将输入转换为 one-hot 编码的输入。
- en: ❸ Define an LSTM layer that returns the last state output vector (from unmasked
    inputs).
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个 LSTM 层，返回最后一个状态输出向量（从未掩码输入中）。
- en: ❹ Define a Dense layer with ReLU activation.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用 ReLU 激活函数来定义一个 Dense 层。
- en: ❺ Define a Dropout layer with 50% dropout.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 用 50% 的 dropout 率定义一个 Dropout 层。
- en: ❻ Define a final prediction layer with a single node and sigmoidal activation.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 用一个节点和 sigmoid 激活函数来定义最终的预测层。
- en: 'Next, we’re off to compiling the model. Again, we have to be careful about
    the loss function we will be using. So far, we have used the categorical_crossentropy
    loss. This loss is used for multiclass classification problems (greater than two
    classes). Since we’re solving a binary classification problem, we must switch
    to binary_crossentropy instead. Using the wrong loss function can lead to numerical
    instabilities and inaccurately trained models:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要编译模型。再次，我们必须小心使用的损失函数。到目前为止，我们使用的是 categorical_crossentropy 损失函数。该损失函数用于多类别分类问题（大于两类）。由于我们解决的是二分类问题，我们必须改用
    binary_crossentropy 损失函数。使用错误的损失函数可能导致数值不稳定和训练不准确的模型：
- en: '[PRE95]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Finally, let’s examine the model by printing out the summary by running model.summary():'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过运行 model.summary() 来查看模型的概述：
- en: '[PRE96]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'This is our first encounter with a sequential model. Let’s review the model
    summary in more detail. First, we have a masking layer that returns an output
    of the same size as the input (i.e., [None, None]-sized tensor). Then a one-hot
    encoding layer returns a tensor with a feature dimension of 11865 (which is the
    vocabulary size). This is because, unlike the input that had a word represented
    by a single integer, one-hot encoding converts it to a vector of zeros, of the
    size of the vocabulary, and sets the value indexed by the word ID to 1\. The LSTM
    layer returns a [None, 128]-sized tensor. Remember that we are only getting the
    final state output vector, which will be a [None, 128]-sized tensor, where 128
    is the number of units. This last output returned by the LSTM goes to a Dense
    layer with 512 nodes and ReLU activation. A dropout layer with 50% dropout follows
    it. Finally, a Dense layer with one node produces the final prediction: a value
    between 0 and 1.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一次遇到顺序模型。让我们更详细地复习一下模型概述。首先，我们有一个返回与输入相同大小的输出的掩码层（即，[None, None] 大小的张量）。然后一个
    one-hot 编码层返回一个具有 11865 个特征维度（即词汇表大小）的张量。这是因为，与单个整数表示的输入不同，one-hot 编码将其转换为一个大小为词汇表大小的零向量，并将由单词
    ID 索引的值设置为 1。LSTM 层返回一个 [None, 128] 大小的张量。记住，我们只获取最终状态输出向量，它将是一个大小为 [None, 128]
    的张量，其中 128 是单元数。LSTM 返回的最后一个输出传递到一个具有 512 个节点和 ReLU 激活函数的 Dense 层。接下来是一个 50% 的
    dropout 层。最后，一个具有一个节点的 Dense 层产生最终的预测结果：一个介于 0 和 1 之间的值。
- en: In the following section, we will train the model on training data and evaluate
    it on validation and testing data to assess the performance of the model.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将在训练数据上训练模型，并在验证和测试数据上评估模型的性能。
- en: Exercise 4
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 4
- en: Define a model that has a single LSTM layer and a single Dense layer. The LSTM
    model has 32 units and accepts a (None, None, 30)-sized input (this includes the
    batch dimension) and produces all the state outputs (instead of the final one).
    Next, a lambda layer should sum up the states on the time dimension to produce
    a (None, 32)-sized output. This output goes to the Dense layer with 10 nodes and
    softmax activation. You can use the tf.keras.layers.Add layer to sum up the state
    vectors. You will need to use the functional API to implement this.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个模型，该模型只有一个 LSTM 层和一个 Dense 层。LSTM 模型有 32 个单元，并接受一个大小为 (None, None, 30) 的输入（包括批次维度），并输出所有状态输出（而不是最终输出）。接下来，一个
    lambda 层应该在时间维度上对状态进行求和，得到一个大小为 (None, 32) 的输出。这个输出传递到具有 10 个节点和 softmax 激活函数的
    Dense 层。你可以使用 tf.keras.layers.Add 层对状态向量进行求和。你需要使用功能API来实现这个模型。
- en: 9.5 Training and evaluating the model
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 训练和评估模型
- en: 'We’re all set to train the model we just defined. As the first step, let’s
    define two pipelines: one for the training data and one for the validation data.
    Remember that we split our data and created three different sets: training (tr_x
    and tr_y), validation (v_x and v_y), and testing (ts_x and ts_y). We will use
    a batch size of 128:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好训练刚刚定义的模型了。作为第一步，让我们定义两个 pipeline：一个用于训练数据，一个用于验证数据。记住，我们分割了数据并创建了三个不同的集合：训练集（tr_x
    和 tr_y），验证集（v_x 和 v_y）和测试集（ts_x 和 ts_y）。我们将使用批量大小为 128：
- en: '[PRE97]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Then comes a very important calculation. In fact, doing or not doing this computation
    can decided whether your model is going to work or not. Remember that we noticed
    a significant class imbalance in our data set in section 9.1\. Specifically, there
    are more positive classes than negative classes in the data set. Here we will
    define a weighing factor to assign a greater weight to negative samples when computing
    the loss and updating weights of the model. To do that, we will define the weighing
    factor:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是一个非常重要的计算。实际上，做或不做这个计算可以决定你的模型是否能够工作。记住，在第 9.1 节我们注意到数据集中存在显着的类别不平衡。具体来说，数据集中的正类比负类更多。在这里，我们将定义一个加权因子，以在计算损失和更新模型权重时为负样本分配更大的权重。为此，我们将定义加权因子：
- en: '*weight*[neg]*= count(positive samples)/count(negative samples)*'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '*weight*[neg]*= count(正样本)/count(负样本)*'
- en: 'This will result in a > 1 factor as there are more positive samples than negative
    samples. We can easily compute this using the following logic:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致一个 > 1 的因子，因为正样本比负样本多。我们可以使用以下逻辑轻松计算：
- en: '[PRE98]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'This results in *weight*[neg]~6 (i.e., approximately 6). Next, we will define
    the training step as follows:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致 *weight*[neg]~6（即约为 6）。接下来，我们将定义训练步骤如下：
- en: '[PRE99]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Here, train_ds is passed to x, but in fact contains both the inputs and targets.
    valid_ds, containing validation samples, is passed to validation_data argument.
    We will run this for 10 epochs. Finally, note that we are using the class_weight
    argument to tell the model that negative samples must be prioritized over positive
    samples (due to the under-representation in the data set). class_weight is defined
    as a dictionary, where the key is the class label and the value represents the
    weight given to the samples of that class. When passed, during the loss computations
    the losses resulting from negative classes will be multiplied by a factor of neg_weight,
    leading to more attention being given to negative samples during the optimization
    process. In practice, we are going to follow the same pattern as in other chapters
    and run the training process with three callbacks:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，train_ds 被传递给 x，但实际上包含了输入和目标。valid_ds，包含验证样本，被传递给 validation_data 参数。我们将运行这个
    10 次迭代。最后，注意我们使用 class_weight 参数告诉模型负样本必须优先于正样本（因为数据集中的不足表示）。class_weight 被定义为一个字典，其中键是类标签，值表示给定该类样本的权重。当传递时，在损失计算期间，由于负类而导致的损失将乘以
    neg_weight 因子，导致在优化过程中更多地关注负样本。实践中，我们将遵循与其他章节相同的模式并使用三个回调运行训练过程：
- en: A CSV logger
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV 记录器
- en: A learning rate scheduler
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率调度程序
- en: Early stopping
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早停
- en: The full code looks like the following listing.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码如下所示。
- en: Listing 9.6 Training procedure for the sentiment analyzer
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.6 情感分析器的训练过程
- en: '[PRE100]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: ❶ Log the performance metrics to a CSV file.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将性能指标记录到 CSV 文件中。
- en: ❷ The learning rate reduction callback
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 学习率降低回调
- en: ❸ The early stopping callback
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 早停回调
- en: ❹ Train the model.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练模型。
- en: 'You should get similar results:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似的结果：
- en: '[PRE101]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'It seems that by the end of the training, we have reached above 80% validation
    accuracy. That’s great news, because we made sure that the validation data set
    is a balanced data set. But we can’t be too sure. We will need to test our model
    on a data set that it hasn’t had the chance to see: the testing set. Before that,
    let’s save the model:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来在训练结束时，我们已经达到了超过 80% 的验证准确率。这是个好消息，因为我们确保验证数据集是一个平衡的数据集。但我们不能太肯定。我们需要在模型没有见过的数据集上测试我们的模型：测试集。在此之前，让我们保存模型：
- en: '[PRE102]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We have already created the test data set and have defined the NLP pipeline
    to process the data, so it’s a matter of calling the get_tf_pipeline() function
    with the data:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了测试数据集，并已经定义了处理数据的 NLP 管道，所以只需调用 get_tf_pipeline() 函数与数据：
- en: '[PRE103]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'It’s now as simple as calling the following one-liner to get the test performance
    of the model:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只需调用以下一行代码即可获得模型的测试性能：
- en: '[PRE104]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'The final result looks like this:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下所示：
- en: '[PRE105]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: We can now go to sleep knowing our model’s performance on unseen data is on
    par with the validation performance we saw during training.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以安心地睡觉，知道我们的模型在未见数据上的性能与训练期间看到的验证性能相当。
- en: Is good accuracy all we’re after?
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅好的准确率是我们追求的吗？
- en: The short answer is no. Solving a machine learning task involves many tasks
    working in harmony. And during the execution of these tasks, we perform various
    transformations/computations on inputs as well as outputs. The complexity of the
    whole process means that there are more chances for things to go wrong. Therefore,
    we should check as many things as we can during the process.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的答案是否定的。解决一个机器学习任务涉及到许多任务的和谐工作。在执行这些任务的过程中，我们对输入和输出进行各种转换/计算。整个过程的复杂性意味着出错的机会更多。因此，在整个过程中我们应该尽可能检查尽可能多的事情。
- en: Speaking solely about testing, we have to make sure that the test data is correctly
    processed while going through the data pipeline. Furthermore, we should check
    the final predictions. Among many other checks, you can check the topmost positive
    predictions and negative predictions to make sure the model’s decisions are sensible.
    You can simply visually inspect the input text and the corresponding prediction.
    We will discuss the specifics in a coming section.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 单单谈论测试，我们必须确保测试数据在通过数据管道时被正确处理。此外，我们还应该检查最终的预测结果。除了许多其他检查之外，你可以检查最顶部的正面预测和负面预测，以确保模型的决策是合理的。你只需简单地视觉检查输入文本和相应的预测。我们将在下一节中讨论具体内容。
- en: It will only slightly increase the time spent on your model. But it can save
    you hours of debugging as well as embarrassment or loss of reputation from releasing
    an inaccurate model.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 这只会略微增加你的模型时间。但它也可以为你节省数小时的调试时间，以及因发布不准确的模型而导致的尴尬或声誉损失。
- en: In the next section, we will further enhance our model by using word vectors
    to represent tokens fed into the model. Word vectors help machine learning models
    understand language better.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过使用词向量来表示输入模型的标记来进一步增强我们的模型。词向量有助于机器学习模型更好地理解语言。
- en: Exercise 5
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5
- en: 'Assume you have three classes in your training data set: A, B and C. You have
    10 records for A, 25 for B, and 50 for C. What do you think will be good weights
    for the three classes? Remember that the majority class should get a smaller weight.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的训练数据集中有三个类别：A、B 和 C。你有 10 条 A 类的记录，25 条 B 类的记录和 50 条 C 类的记录。你认为这三个类别的权重会是多少？记住，大多数类别应该获得较小的权重。
- en: 9.6 Injecting semantics with word vectors
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 用词向量注入语义
- en: You have built a model that can measure sentiments with ~80% accuracy, but you
    want to improve further. You believe word embeddings will provide the edge required
    to attain a higher accuracy. Word embeddings are a way to encode words as a feature
    vector. Word embeddings learn feature vectors for the words in the vocabulary
    jointly with the model training. An embedding layer introduces a trainable matrix,
    where each row represents a trainable vector for a single word in the vocabulary.
    This is much better than one-hot encoding because one-hot encoding suffers from
    the curse of dimensionality, which means that as the number of words grows, so
    does the dimensionality of the inputs to the model.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经建立了一个可以以约80%的准确率测量情绪的模型，但你希望进一步改进。你相信词嵌入将提供所需的优势以达到更高的准确率。词嵌入是将单词编码为特征向量的一种方法。词嵌入与模型训练一起学习词汇表中单词的特征向量。嵌入层引入了一个可训练矩阵，其中每一行表示词汇表中单词的一个可训练向量。这比独热编码要好得多，因为独热编码受到维度灾难的困扰，这意味着随着单词数量的增加，输入模型的维度也会增加。
- en: You should feel proud about having a reasonable model that can accurately classify
    positive and negative sentiments; 80% accuracy is a great starting point. But
    let’s see what we can improve in the already good model we have.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该为拥有一个可以准确分类正面和负面情绪的合理模型感到自豪；80%的准确率是一个很好的起点。但让我们看看我们已经拥有的优秀模型中可以改进的地方。
- en: A bottleneck that’s staring us right in the face is the one-hot encoding layer
    that’s in our model. One-hot encoding, despite its simplicity, is a very inefficient
    representation of words. It is a localized representation of words, meaning only
    one element (set to value 1) in the representation carries information. In other
    words, it’s a very sparse representation that has a very large number of elements
    set to zero and does not contribute with information. One-hot encoding also suffers
    from the curse of dimensionality. Finally, one-hot encoding completely disregards
    the valuable semantics present in the text. With one-hot encoding, you can’t say
    if a cat is more similar to a dog than it is to a volcano. Now the question is,
    are there better ways to represent words?
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 正面对着我们的一个瓶颈是我们模型中的单热编码层。尽管单热编码很简单，但它是一种非常低效的单词表示。它是单词的局部表示，意味着表示中只有一个元素（设为值1）携带信息。换句话说，这是一种非常稀疏的表示，其中有大量元素被设置为零且不贡献信息。单热编码还受到维度诅咒的影响。最后，单热编码完全忽略了文本中存在的宝贵语义。使用单热编码，你不能说一只猫和一只狗更相似还是更相似于一座火山。现在的问题是，是否有更好的表示单词的方法？
- en: 9.6.1 Word embeddings
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.1 词嵌入
- en: 'It’s time to usher in a new era of word representations known as *word embeddings*.
    Word embeddings, sometimes called word vectors, are a very information-rich and
    efficient representation of words. As opposed to the localized representations
    like one-hot vectors, word vectors provide a distributed representation. This
    means that all the elements in the vector play a role in defining the word represented
    by the vector. In other words, word vectors have dense representations, in contrast
    to the sparse representation of one-hot vectors. The dimensionality of word vectors
    does not depend on the size of vocabulary, which allows you to save memory and
    computational time. Finally, but most importantly, word vectors capture the semantics
    or similarity of words. With word vectors, you know that a cat is more similar
    to a dog than to a volcano. Before understanding word vectors, you have to understand
    the important role played by the context around a word. Word vectors heavily rely
    on the context of words to generate rich representations of words. The importance
    of the context is subliminally captured by a famous quote by J.R. Firth, an English
    linguist:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候迎来一个称为*词嵌入*的新时代的单词表示了。词嵌入，有时也称为词向量，是一种非常丰富和高效的单词表示。与像单热向量这样的局部表示相反，词向量提供了一种分布式表示。这意味着向量中的所有元素都在定义向量所表示的词中发挥作用。换句话说，词向量具有密集的表示，与单热向量的稀疏表示相反。词向量的维度不取决于词汇量的大小，这使你可以节省内存和计算时间。最重要的是，词向量捕捉到了词的语义或相似性。使用词向量，你知道猫更类似于狗而不是火山。在理解词向量之前，你必须理解上下文在单词周围扮演的重要角色。词向量在很大程度上依赖于单词的上下文来生成丰富的单词表示。上下文的重要性被一位英国语言学家J.R.弗斯的一句名言潜移默化地捕捉到：
- en: You shall know a word by the company it keeps.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过一个词的周围环境来了解它。
- en: 'To expand on this a little more, the context of a word plays an important role
    in defining the semantics of that word. For example, take the following sentence:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 再深入一点，一个词的上下文在定义该词的语义中起着重要作用。例如，看看下面这个句子：
- en: Our pet Toby is a ____; he enjoys playing with a ball.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的宠物托比是一只 ____；他喜欢玩球。
- en: 'What do you think is the right word here? We see words like “pet,” “playing,”
    and “ball” in the context. Most likely it’s a cat or a dog. This means that only
    a certain type of words (i.e., a type of pet) will appear in this context. Using
    this property, word vectors can generate vectors that preserve the semantics of
    the text. In this example, word vectors will capture that cats and dogs are very
    similar (not biologically, of course, but in the way we interact with or perceive
    them). In a more technical stance, the objective of word vector algorithms is
    as follows: if word *w*[i] and *w*[j] appear in the same context, for some distance
    measure *Dist(a,b)*, that measures the distance between two vectors *a* and *b*:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为这里应该填什么词？我们在上下文中看到了“宠物”、“玩耍”和“球”这样的词。很可能是猫或狗。这意味着只有某种类型的词（即某种宠物）会出现在这个上下文中。利用这个特性，词向量可以生成保留文本语义的向量。在这个例子中，词向量将捕捉到猫和狗非常相似（当然，不是生物学上的相似，而是我们与它们互动或感知的方式）。从更技术的角度来看，词向量算法的目标如下：如果词*w*[i]和*w*[j]出现在同样的上下文中，对于某个距离度量*Dist(a,b)*，它测量两个向量*a*和*b*之间的距离：
- en: '*Dist*(*w*[i]*, w*[j]) *~* 0'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dist*(*w*[i]*, w*[j]) *~* 0'
- en: The actual word vector algorithms are out of scope for this book. A few note-worthy
    algorithms are Skip-gram, CBoW (Continuous Bag-of-Words), GloVe (global vectors)
    and ELMo (Embeddings from Language Models). You can read more details about the
    Skip-gram and CBoW algorithms by reading the paper “Efficient Estimation of Word
    Representations in Vector Space” by Tomas Mikolov et al. ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)).
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的词向量算法超出了本书的范围。一些值得注意的算法包括 Skip-gram、CBoW（Continuous Bag-of-Words）、GloVe（全局向量）和
    ELMo（来自语言模型的嵌入）。您可以通过阅读 Tomas Mikolov 等人的论文“Efficient Estimation of Word Representations
    in Vector Space” ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf))
    了解更多关于 Skip-gram 和 CBoW 算法的细节。
- en: Show me the word vector algorithms
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 展示给我词向量算法
- en: Word vector algorithms train in an unsupervised manner. The training algorithm
    specifics will differ depending on the algorithm. The Skip-gram algorithm generates
    input target pairs by picking a probe word as the input and the context words
    as the targets. For example, from the sentence “I went to buy flowers” it will
    generate input target pairs such as [(went, I), (went, to), (to, went), (to, buy),
    . . . ]. Then it will solve the classification task of predicting the context
    of a probe word, which will lead to identifying good word vectors. However, word
    vector algorithms like Skip-gram suffer from the lack of a global view of the
    corpus because the algorithm only considers the small context around a word.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量算法以无监督的方式训练。训练算法的具体细节会根据算法而异。Skip-gram 算法通过选择一个探针单词作为输入，上下文单词作为目标生成输入目标对。例如，从句子“I
    went to buy flowers”中，它将生成如下输入目标对：[(went, I), (went, to), (to, went), (to, buy),
    . . . ]。然后，它将解决预测探测单词的上下文的分类任务，这将导致识别出良好的词向量。然而，像 Skip-gram 这样的词向量算法遭受语料库全局视图的缺乏，因为该算法只考虑单词周围的小上下文。
- en: 'GloVe, on the other hand, uses both local and global information to generate
    word vectors. To extract global information of a corpus, it leverages a co-occurrence
    matrix, which contains how many times the word “i” appeared in the context of
    “j” in the corpus. You can read more about this in the paper, “GloVe: Global Representations
    for Word Vectors” by Pennington et al. ([https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)).
    GloVe still does not address the problem of ambiguous words. By ambiguous words,
    I mean words that have different meanings depending on the context. For example,
    the word “bank” in the sentences “I went to the bank to deposit money” and “I
    walked on the river bank” has entirely different meanings. GloVe would give the
    same vector for both cases, which is not accurate.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，GloVe 使用局部和全局信息生成词向量。为了提取语料库的全局信息，它利用了一个共现矩阵，其中包含单词“i”在语料库中与“j”上下文中出现的次数。您可以在
    Pennington 等人的论文“GloVe: Global Representations for Word Vectors” ([https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf))
    中了解更多信息。GloVe 仍然没有解决模糊词的问题。所谓的模糊词是指根据上下文具有不同含义的词。例如，在句子“I went to the bank to
    deposit money”和“I walked on the river bank”中，“bank”这个词有完全不同的含义。GloVe 会为这两种情况给出相同的向量，这是不准确的。'
- en: Enter ELMo! ELMo was introduced in the paper “Deep Contextualized Word Representations”
    by Peters et al. ([https://arxiv.org/pdf/1802.05365.pdf](https://arxiv.org/pdf/1802.05365.pdf)).
    ELMo uses bidirectional LSTM models to generate word vectors. A bidirectional
    LSTM is similar to a standard LSTM but reads the sequence both forward and backward.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 ELMo！ELMo 是由 Peters 等人在论文“深度上下文化的词表示”中介绍的 ([https://arxiv.org/pdf/1802.05365.pdf](https://arxiv.org/pdf/1802.05365.pdf))。ELMo
    使用双向 LSTM 模型生成词向量。双向 LSTM 类似于标准 LSTM，但同时正向和反向读取序列。
- en: The final output of a word vector algorithm is a V × d-sized embedding matrix.
    The *i*^(th) row of this matrix represents the word vector for the word represented
    by the ID *i*. *d* is typically < 300 and is selected using a hyperparameter algorithm.
    Figure 9.7 depicts the word embedding matrix.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量算法的最终输出是一个 V × d 大小的嵌入矩阵。这个矩阵的第 *i* 行表示由 ID *i* 表示的单词的词向量。*d* 通常 < 300，并使用超参数算法选择。图
    9.7 描述了词嵌入矩阵。
- en: '![09-07](../../OEBPS/Images/09-07.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![09-07](../../OEBPS/Images/09-07.png)'
- en: Figure 9.7 An overview of how the embedding matrix is used to obtain word vectors.
    A lookup is performed using the input word IDs to fetch the vectors corresponding
    to those indices. The actual values of the vectors are learned during the model
    training.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 概述了如何使用嵌入矩阵获取词向量。使用输入单词 ID 进行查找以获取对应这些索引的向量。这些向量的实际值在模型训练期间学习。
- en: We will now enhance our sentiment analyzer model with word embeddings.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将用单词嵌入增强我们的情感分析器模型。
- en: 9.6.2 Defining the final model with word embeddings
  id: totrans-528
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.2 使用单词嵌入定义最终模型
- en: In general, any deep sequential model can benefit from word embeddings. As an
    added benefit, most of the time you don’t need to worry about the word vector
    algorithms themselves and can enjoy good performance by introducing a randomly
    initialized embedding space. Then the embeddings can be trained jointly with the
    other parts of the models during the specific NLP task we’re solving. Following
    the same pattern, let’s introduce a randomly initialized embedding space to our
    sentiment analyzer.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，任何深度序列模型都可以从单词嵌入中受益。作为额外的好处，大多数情况下，您不需要担心单词向量算法本身，可以通过引入一个随机初始化的嵌入空间来获得良好的性能。然后，这些嵌入可以与模型的其他部分一起在解决特定
    NLP 任务时进行联合训练。按照同样的模式，让我们向我们的情感分析器引入一个随机初始化的嵌入空间。
- en: 'Let’s remind ourselves of the previous model we implemented. The model consisted
    of a masking layer, a one-hot encoder layer, and an LSTM layer followed by two
    Dense layers (with dropout in between):'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们之前实现的模型。该模型由一个掩码层、一个独热编码器层和一个 LSTM 层，后面跟着两个 Dense 层（之间有丢弃）组成：
- en: '[PRE106]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'The model will remain more or less the same, except for two changes:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 除了两个改变外，模型将保持不变：
- en: We will replace the one-hot encoder layer with an tf.keras.layers.Embedding
    layer.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将用 tf.keras.layers.Embedding 层替换独热编码器层。
- en: The masking functionality will be absorbed into the tf.keras.layers.Embedding
    layer by setting the mask_zero=True in the layer.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在层中设置 mask_zero=True，掩码功能将被吸收到 tf.keras.layers.Embedding 层中。
- en: The tf.keras.layers.Embeddings layer introduces a large trainable matrix into
    the model. This matrix is a (V+1) x d-sized matrix, where V is the vocabulary
    size. The additional one is required as we use the special reserved ID zero. d
    is chosen through a hyperparameter optimization algorithm. In the following model,
    we will set d = 128 empirically. The line that has changed has been highlighted
    in bold in the listing.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: tf.keras.layers.Embeddings 层将一个大的可训练矩阵引入模型。该矩阵是一个 (V+1) x d 大小的矩阵，其中 V 是词汇表的大小。额外的一个是因为我们使用了特殊的保留
    ID 零。d 是通过超参数优化算法选择的。在下面的模型中，我们将 d = 128 设置为经验值。已经在列表中用粗体标出了已更改的行。
- en: Listing 9.7 Implementing the sentiment analyzer with word embeddings
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.7 使用单词嵌入实现情感分析器
- en: '[PRE107]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: ❶ Create a mask to mask out zero inputs.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个掩码以屏蔽零输入。
- en: ❷ Add an Embedding layer. It will look up word vectors for the word IDs passed
    in as the input.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加一个嵌入层。它将查找作为输入传递的单词 ID 的单词向量。
- en: ❸ Define an LSTM layer.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个 LSTM 层。
- en: ❹ Define Dense layers.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义 Dense 层。
- en: ❺ Define a Dropout layer.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义一个 Dropout 层。
- en: ❻ Define the final Dense layer with sigmoidal activation.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用 S 型激活函数定义最终的 Dense 层。
- en: ❼ Compile the model with binary cross-entropy as the loss.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用二元交叉熵编译模型。
- en: Empty inputs, the mask, and the LSTM layer
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 空输入、掩码和 LSTM 层
- en: 'We made sure that we don’t have any empty reviews in our data set by introducing
    a filter to filter out the empty reviews. It is worth understanding why we did
    this. In addition to playing a role as a data cleaning step, it serves an important
    purpose. Having empty reviews in the data set will result in an all-zero vector
    in our data pipeline. For example, an empty review if the sequence length is 5
    will return [0,0,0,0,0]. When using a Masking layer, all the inputs will be ignored.
    This is a problematic edge case for the LSTM layer and will raise the following
    error:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过引入一个过滤器来过滤掉数据集中的空评论，以确保我们的数据集中没有任何空评论。理解我们这样做的原因非常重要。除了起到数据清洗的作用外，它还具有重要的目的。在数据集中有空评论会导致我们的数据流水线中出现全零向量。例如，如果序列长度为
    5，则空评论将返回 [0,0,0,0,0]。当使用掩码层时，所有输入将被忽略。这对于 LSTM 层是一个问题性的边缘情况，并将引发以下错误：
- en: '[PRE108]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: For this reason, you must make sure that the empty reviews are filtered from
    the data before feeding the data to the model.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，在将数据提供给模型之前，您必须确保从数据中过滤掉空评论。
- en: With that, we will train the model we defined.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们将训练我们定义的模型。
- en: 9.6.3 Training and evaluating the model
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.3 训练和评估模型
- en: Training and evaluation code are identical to the implementation we discussed
    earlier. Therefore, we will not reiterate the discussion. When you train the new
    model, you will see a result similar to the following.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和评估代码与我们之前讨论的实现相同。因此，我们将不再赘述讨论。当您训练新模型时，您将看到类似于以下的结果。
- en: 'When you train the model, you will reach a validation accuracy that is slightly
    above the previous validation accuracy we experienced:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练模型时，你将达到略高于我们以前体验到的验证准确性的验证准确性：
- en: '[PRE109]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Evaluating the model can be done by running the following:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型可以通过运行以下代码完成：
- en: '[PRE110]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'It seems that adding an embedding layer leads to a slightly higher testing
    performance as well:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来添加一个嵌入层也会导致稍微更高的测试性能：
- en: '[PRE111]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Remember that we said we wouldn’t trust the accuracy alone. Now let’s dig in
    a bit deeper and see if our model is giving out sensible predictions. An easy
    way to do this is to check the top-k positive reviews and the top-k negative reviews
    in the test set and do a visual inspection. We exhausted the tf.data pipeline
    when we finished the evaluation. Therefore, we need to redefine the data pipeline:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们说过不应该单凭准确性来信任模型。现在让我们深入一点，看看我们的模型是否在做出合理的预测。一个简单的方法是检查测试集中前k个正面评价和前k个负面评价，并进行视觉检查。当我们完成评估时，我们耗尽了tf.data流水线。因此，我们需要重新定义数据流水线：
- en: '[PRE112]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Then we will go batch by batch, and for each batch, we will store the inputs,
    predictions, and targets in three separate lists: test_x, test_pred, and test_y,
    respectively:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将逐批次进行，并且对于每个批次，我们将把输入、预测和目标分别存储在三个单独的列表中：test_x，test_pred和test_y：
- en: '[PRE113]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'We will use argsort to get the indices of the sorted prediction array. This
    way, the start of the array will have the indices of the most negative reviews,
    whereas the end of the array will contain the most positive review indices. Let’s
    take the five top-most and five bottom-most reviews to visually check:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用argsort来获取排序预测数组的索引。这样，数组的开头将包含最负面的评价的索引，而数组的末尾将包含最正面的评价的索引。让我们取最上面的五个和最下面的五个评价来进行视觉检查：
- en: '[PRE114]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Let’s check the results:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查结果：
- en: '[PRE115]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: We can confidently say that our sentiment analyzer was a success! The negative
    and positive reviews identified by the model seem like they’re in the correct
    places. We had to overcome many obstacles having to do with data quality, data
    preparation, and model design. Through them all, we persevered! In the next chapter,
    we will discuss another NLP task known as language modeling.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以自信地说，我们的情感分析器取得了成功！模型识别出的负面和正面评价似乎都放在了正确的位置。我们不得不克服许多与数据质量、数据准备和模型设计有关的障碍。在所有这些困难中，我们都坚持了下来！在下一章中，我们将讨论另一个被称为语言建模的NLP任务。
- en: Exercise 6
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 练习6
- en: Word vector algorithms like Skip-gram use an embedding layer directly connected
    to a Dense layer that has the same size as the vocabulary. If you have a vocabulary
    of size 500, want to produce 32 dimensional word vectors, and have a dense layer
    with 500 units and a softmax activation, how would you implement a model? The
    model accepts a (None, 500)-sized (batch dimension is None) one-hot encoded vectors
    of words.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram等词向量算法使用直接连接到与词汇表大小相同的密集层的嵌入层。如果你有一个大小为500的词汇表，想要生成32维的词向量，并且有一个具有500个单元和softmax激活的密集层，你会如何实现一个模型？该模型接受大小为（None，500）的（批量维度为None）one-hot编码的单词向量。
- en: Summary
  id: totrans-569
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The NLTK library provides an API to perform various text preprocessing tasks,
    such as tokenizing to words, removing stop words, lemmatization, and so on.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK库提供了一个API来执行各种文本预处理任务，例如将文本标记为单词、删除停用词、词形还原等等。
- en: Preprocessing tasks need to be applied with care. For example, when removing
    stop words, the word “not” should not be removed. This is because in a sentiment
    analysis task, the word “not” carries very important information.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理任务需要谨慎处理。例如，在删除停用词时，单词“not”不应该被删除。这是因为在情感分析任务中，“not”这个词承载着非常重要的信息。
- en: tensorflow.keras.preprocessing.text.Tokenizer can be used to convert text to
    numbers. This is done by the Tokenizer first building a dictionary that maps each
    unique word to a unique ID. Then a given text can be converted to a sequence of
    IDs.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tensorflow.keras.preprocessing.text.Tokenizer可以用来将文本转换为数字。这是通过Tokenizer首先构建一个将每个唯一单词映射到唯一ID的字典。然后给定的文本可以被转换为一系列ID。
- en: Padding is a technique used to bring variable-length text to the same length.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充是一种将可变长度文本转换为相同长度的技术。
- en: Padding works by padding all sequences in a given text corpus to a fixed length
    by inserting zeros (at the end or at the beginning).
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充工作是通过在给定文本语料库中将所有序列填充到固定长度，通过在末尾或开头插入零来完成的。
- en: When processing variable-length sequences like text, there’s another strategy
    known as bucketing that is used to batch similar-length text sequences together.
    This helps the model keep the memory footprint small as well as to not waste computation
    on excessing padding.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理文本等变长序列时，还有一种称为“桶装”（bucketing）的策略，用于将相似长度的文本序列一起进行批处理。这有助于模型保持内存占用小，同时不浪费在过多填充上的计算。
- en: In TensorFlow, you can use tf.data.experimental.bucket_by_sequence_ length()
    *to bucket text sequences.*
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以使用 tf.data.experimental.bucket_by_sequence_length() *来对文本序列进行桶装*。
- en: LSTM (long short-term memory) models have shown superior performance in solving
    NLP tasks.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆）模型在解决自然语言处理任务方面表现出卓越的性能。
- en: LSTM models work by going from one timestep to the next, while processing the
    input at that time step to produce an output for each timestep.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 模型通过从一个时间步到下一个时间步，同时处理该时间步的输入以产生每个时间步的输出来工作。
- en: LSTM models have a mechanism to store both long-term and short-term memory.
    This is achieved through a gating mechanism that controls the flow of information
    in the LSTM cell.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 模型具有存储长期和短期记忆的机制。这是通过控制 LSTM 单元中信息流动的门控机制实现的。
- en: Word embeddings are a text encoding method that is superior to one-hot encoding
    and that has the ability to preserve the semantics of words when generating the
    numerical representation of words.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词嵌入是一种文本编码方法，优于一热编码，并且在生成单词的数值表示时具有保留单词语义的能力。
- en: Answers to exercises
  id: totrans-581
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1**'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: '[PRE116]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '**Exercise 2**'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: '[PRE117]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '**Exercise 3**'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: '[PRE118]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '**Exercise 4**'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4**'
- en: '[PRE119]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '**Exercise 5**'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 5**'
- en: '[PRE120]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '**Exercise 6**'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 6**'
- en: '[PRE121]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
