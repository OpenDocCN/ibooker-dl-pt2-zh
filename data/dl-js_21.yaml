- en: Chapter 12\. Testing, optimizing, and deploying models
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12章。测试、优化和部署模型
- en: '*—WITH CONTRIBUTIONS FROM YANNICK ASSOGBA, PING YU, AND NICK* *KREEGER*'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*—有贡献者：Yannick Assogba，Ping Yu和Nick Kreeger*'
- en: '*This chapter covers*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容包括*'
- en: The importance of and practical guidelines for testing and monitoring machine-learning
    code
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习代码测试和监控的重要性和实用指南
- en: How to optimize models trained in TensorFlow.js or converted to TensorFlow.js
    for faster loading and inference
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何优化在 TensorFlow.js 中训练或转换的模型，以实现更快的加载和推理
- en: How to deploy TensorFlow.js models to various platforms and environments, ranging
    from browser extensions to mobile apps, and from desktop apps to single-board
    computers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将 TensorFlow.js 模型部署到各种平台和环境中，从浏览器扩展到移动应用，从桌面应用到单板计算机
- en: As we mentioned in [chapter 1](kindle_split_011.html#ch01), machine learning
    differs from traditional software engineering in that it automates the discovery
    of rules and heuristics. The previous chapters of the book should have given you
    a solid understanding of this uniqueness of machine learning. However, machine-learning
    models and the code surrounding them are still code; they run as a part of your
    overall software system. In order to make sure that machine-learning models run
    reliably and efficiently, practitioners need to take similar precautions as they
    do when managing non-machine-learning code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第一章](kindle_split_011.html#ch01)中提到的，机器学习不同于传统软件工程，因为它自动发现规则和启发式方法。本书的前几章应该已经让你对这个机器学习的独特性有了扎实的理解。但是，机器学习模型及其周围的代码仍然是代码；他们作为整体软件系统的一部分运行。为了确保机器学习模型可靠高效地运行，从业者需要像管理非机器学习代码时那样采取相应的预防措施。
- en: This chapter is devoted to the practical aspects of using TensorFlow.js for
    machine learning as a part of your software stack. The first section explores
    the all-important but oft-neglected topic of testing and monitoring machine-learning
    code and models. The second section presents tools and tricks that help you reduce
    the size and computation footprint of your trained models, accelerating downloading
    and execution, which is a critical consideration for both client- and server-side
    model deployment. In the final section, we will give you a tour of the various
    environments in which models created with TensorFlow.js can be deployed. In doing
    so, we will discuss the unique benefits, constraints, and strategies that each
    of the deployment options involves.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍如何在软件堆栈中使用 TensorFlow.js 进行机器学习的实践应用。第一部分探讨了机器学习代码和模型的测试和监控这一至关重要但经常被忽视的话题。第二部分介绍了帮助你优化训练模型、减小计算量和模型大小，从而加快下载和执行速度的工具和技巧。这对于客户端和服务器端模型部署来说是一个至关重要的考虑因素。在最后一部分，我们将为您介绍
    TensorFlow.js 创作的模型可以部署到的各种环境。在此过程中，我们将讨论每个部署选项所涉及的独特优势、限制和策略。
- en: By the end of this chapter, you will be familiar with the best practices surrounding
    the testing, optimization, and deployment of deep-learning models in TensorFlow.js.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将熟悉关于在 TensorFlow.js 中深度学习模型的测试、优化和部署的最佳实践。
- en: 12.1\. Testing TensorFlow.js models
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1\. 测试 TensorFlow.js 模型
- en: So far, we’ve talked about how to design, build, and train machine-learning
    models. Now we’re going to dive into some of the topics that arise when you deploy
    your trained models, starting with testing—of both the machine-learning code and
    the related non-machine-learning code. Some of the key challenges you face when
    you’re seeking to surround your model and its training process with tests are
    the size of the model, the time required to train, and nondeterministic behavior
    that happens during training (such as randomness in the initialization of weights
    and certain neural network operations such as dropout). As we expand from an individual
    model to a complete application, you’ll also run across various types of skew
    or drift between training and inference code paths, model versioning issues, and
    population changes in your data. You’ll see that testing needs to be complemented
    by a robust monitoring solution in order to achieve the reliability and confidence
    that you want in your entire machine-learning system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何设计、构建和训练机器学习模型。现在，我们将深入探讨一些在部署训练好的模型时会出现的话题，首先是测试 - 包括机器学习代码和相关的非机器学习代码。当你试图在你的模型和其训练过程周围设置测试时，你面临的一些关键挑战包括模型的大小、训练所需的时间以及训练过程中发生的非确定性行为（例如权重初始化和某些神经网络操作（如
    dropout）中的随机性）。当我们从一个单独的模型扩展到一个完整的应用程序时，你还会遇到各种类型的偏差或漂移，包括训练和推断代码路径之间的偏差，模型版本问题以及数据中的人口变化。你会发现，为了实现你对整个机器学习系统的可靠性和信心，测试需要配合强大的监控解决方案。
- en: One key consideration is, “How is your model version controlled?” In most cases,
    the model is tuned and trained until a satisfactory evaluation accuracy is reached,
    and then the model needs no further tweaking. The model is not rebuilt or retrained
    as part of the normal build process. Instead, the model topology and trained weights
    should be checked into your version-control system, more similar to a binary large
    object (BLOB) than a text/code artifact. Changing the surrounding code should
    not cause an update of your model version number. Likewise, retraining a model
    and checking it in shouldn’t require changing the non-model source code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键考虑因素是，“你的模型版本是如何受控制的？”在大多数情况下，模型被调整和训练直到达到满意的评估精度，然后模型就不需要进一步调整了。模型不会作为正常构建过程的一部分进行重建或重新训练。相反，模型拓扑结构和训练权重应该被检入你的版本控制系统中，更类似于二进制大对象（BLOB）而不是文本/代码工件。改变周围的代码不应该导致你的模型版本号更新。同样，重新训练模型并检入它不应该需要改变非模型源代码。
- en: What aspect of a machine-learning system should be covered by tests? In our
    opinion, the answer is “every part.” [Figure 12.1](#ch12fig01) explains this answer.
    A typical system that goes from raw input data to a trained model ready for deployment
    consists of multiple key components. Some of them look similar to non-machine-learning
    code and are amenable to coverage by traditional unit testing, while others show
    more machine-learning-specific characteristics and hence require specially tailored
    testing or monitoring treatments. But the important take-home message here is
    never to ignore or underestimate the importance of testing just because you are
    dealing with a machine-learning system. Instead, we’d argue that unit testing
    is all the more important for machine-learning code, perhaps even more so than
    testing is for traditional software development, because machine-learning algorithms
    are typically more opaque and harder to understand than non-machine-learning ones.
    They can fail silently in the face of bad inputs, leading to issues that are hard
    to notice and debug, and the defense against such issues is testing and monitoring.
    In the following subsections, we will expand on various parts of [figure 12.1](#ch12fig01).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的哪些方面应该被测试？在我们看来，答案是“每个部分”。[图12.1](#ch12fig01)解释了这个答案。一个从原始输入数据到准备部署的训练好的模型的典型系统包含多个关键组件。其中一些看起来类似于非机器学习代码，并且适合于传统的单元测试覆盖，而其他一些则显示出更多的机器学习特性，因此需要专门定制的测试或监控处理。但这里的重要信息是永远不要忽视或低估测试的重要性，仅仅因为你正在处理一个机器学习系统。相反，我们会认为，单元测试对于机器学习代码来说更加重要，也许甚至比传统软件开发的测试更加重要，因为机器学习算法通常比非机器学习算法更加难以理解。它们在面对不良输入时可能会悄无声息地失败，导致很难察觉和调试的问题，而针对这些问题的防御措施是测试和监控。在接下来的小节中，我们将扩展[图12.1](#ch12fig01)的各个部分。
- en: 'Figure 12.1\. The coverage of a production-ready machine-learning system by
    testing and monitoring. The top half of the diagram includes the key components
    of a typical pipeline for machine-learning model creation and training. The bottom
    half shows the testing practice that can be applied to each of the components.
    Some of the components are amenable to traditional unit testing practice: the
    code that creates and trains the code, and the code that performs pre- and postprocessing
    of the model’s input data and output results. Other components require more machine-learning-specific
    testing and monitoring practice. These include the example validation for the
    quality of data, the monitoring of the byte size and inference speed of the trained
    model, and the fine-grained validation and evaluation of the predictions made
    by the trained model.'
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.1\. 生产就绪的机器学习系统的测试和监控覆盖范围。图的上半部分包括典型机器学习模型创建和训练管道的关键组件。下半部分显示可以应用于每个组件的测试实践。某些组件适合传统的单元测试实践：创建和训练代码以及执行模型输入数据和输出结果的预处理和后处理代码。其他组件需要更多的机器学习特定的测试和监控实践。这些包括用于数据质量的示例验证、监控经过训练的模型的字节大小和推断速度，以及对训练模型所做预测的细粒度验证和评估。
- en: '![](12fig01a_alt.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig01a_alt.jpg)'
- en: 12.1.1\. Traditional unit testing
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.1\. 传统单元测试
- en: Just as with non-machine-learning projects, reliable and lightweight unit tests
    should form the foundation of your test suites. However, special considerations
    are required to set up unit tests around machine-learning models. As you’ve seen
    in previous chapters, metrics such as accuracy on an evaluation dataset are often
    used to quantify the final quality of the model after successful hyperparameter
    tuning and training. Such evaluation metrics are important for monitoring by human
    engineers but are not suitable for automated testing. It is tempting to add a
    test that asserts that a certain evaluation metric is better than a certain threshold
    (for example, AUC for a binary-classification task is greater than 0.95, or MSE
    for a regression task is less than 0.2). However, these types of threshold-based
    assertions should be used with caution, if not completely avoided, because they
    tend to be fragile. The model’s training process contains multiple sources of
    randomness, including the initialization of weights and the shuffling of training
    examples. This leads to the fact that the result of model training varies slightly
    from run to run. If your datasets change (for instance, due to new data being
    added regularly), this will form an additional source of variability. As such,
    picking the threshold is a difficult task. Too lenient a threshold wouldn’t catch
    real problems when they occur. Too stringent a threshold would lead to a flaky
    test—that is, one that fails frequently without a genuine underlying issue.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 就像非机器学习项目一样，可靠且轻量级的单元测试应该构成测试套件的基础。然而，需要特殊考虑来设置围绕机器学习模型的单元测试。正如您在之前的章节中所见，诸如在评估数据集上的准确率之类的度量通常用于量化在成功超参数调整和训练后模型的最终质量。这些评估指标对于人工工程师的监控很重要，但不适合自动化测试。添加一个测试来断言某个评估指标优于某个阈值（例如，二分类任务的AUC大于0.95，或回归任务的MSE小于0.2）是很诱人的。然而，这些基于阈值的断言应该谨慎使用，如果不完全避免的话，因为它们往往很脆弱。模型的训练过程包含多个随机性来源，包括权重的初始化和训练示例的洗牌。这导致模型训练的结果在不同运行中略有不同。如果您的数据集发生变化（例如，由于定期添加新数据），这将形成额外的变异源。因此，选择阈值是一项困难的任务。太宽容的阈值在发生真正问题时无法捕捉到。太严格的阈值将导致一个不稳定的测试，即经常失败而没有真正的潜在问题。
- en: 'The randomness in a TensorFlow.js program can usually be disabled by calling
    the `Math.seedrandom()` function prior to creating and running the model. For
    example, the following line will seed the random state of weight initializers,
    data shuffler, and dropout layers with a determined seed so that subsequent model
    training will yield deterministic results:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 程序中的随机性通常可以通过在创建和运行模型之前调用 `Math.seedrandom()` 函数来禁用。例如，以下代码将以确定的种子来设置权重初始化器、数据混洗器和退出层的随机状态，以便随后的模型训练产生确定性结果：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1*** 42 is just an arbitrarily-selected, fixed random seed.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 42 只是一个任意选择的、固定的随机种子。'
- en: This is a useful trick in case you need to write tests that make assertions
    about the loss or metric values.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要编写对损失或度量值进行断言的测试，这是一个有用的技巧。
- en: However, even with deterministic seeding, testing only `model.fit()` or similar
    calls is not sufficient for good coverage of your machine-learning code. Like
    other hard-to-unit-test sections of code, you should aim to fully unit test the
    surrounding code that is easy to unit test and explore alternative solutions for
    the model portion. All your code for data loading, data preprocessing, postprocessing
    of model outputs, and other utility methods should be amenable to normal testing
    practices. Additionally, some nonstringent tests on the model itself—its input
    and output shapes, for instance—along with an “ensure model does not throw an
    exception when trained one step” style test can provide the bare minimum of a
    test harness around the model that allows confidence during refactoring. (As you
    might have noticed when playing with the example code from the previous chapters,
    we use the Jasmine testing framework for testing in tfjs-examples, but you should
    feel free to use whatever unit test framework and runner you and your team prefer.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使确定性种子，仅仅测试 `model.fit()` 之类的调用还不足以对您的机器学习代码进行良好的覆盖。像其他难以进行单元测试的代码部分一样，您应该努力对易于单元测试的周围代码进行全面的单元测试，并探索模型部分的替代解决方案。您用于数据加载、数据预处理、模型输出的后处理以及其他实用方法的代码应该符合正常的测试实践。此外，还可以对模型本身进行一些非严格的测试，比如测试其输入和输出形状，以及“确保模型在训练一步时不会抛出异常”的风格的测试，可以提供最基本的模型测试环境，以在重构过程中保持信心。（正如您在上一章的示例代码中所注意到的，我们在
    tfjs-examples 中使用了 Jasmine 测试框架进行测试，但您可以随意使用您和您的团队偏好的任何单元测试框架和运行器。）
- en: For an example of this in practice, we can look at the tests for the sentiment-analysis
    examples we explored in [chapter 9](kindle_split_021.html#ch09). As you look through
    the code, you should see data_test.js, embedding_test.js, sequence_utils_test.js,
    and train_test.js. The first three of these files are covering the non-model code,
    and they look just like normal unit tests. Their presence gives us heightened
    confidence that the data that goes into the model during training and inference
    is in the expected source format, and our manipulations on it are valid.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实践示例，我们可以看一下我们在 [第9章](kindle_split_021.html#ch09) 中探索过的情感分析示例的测试。当您查看代码时，您应该会看到
    `data_test.js`、`embedding_test.js`、`sequence_utils_test.js` 和 `train_test.js`
    这四个文件。这三个文件覆盖了非模型代码，它们看起来就像普通的单元测试一样。它们的存在使我们对训练和推理过程中进入模型的数据的源格式有了更高的信心，并且我们对它的处理是有效的。
- en: The final file in that list concerns the machine-learning model and deserves
    a bit more of our attention. The following listing is an excerpt from it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的最后一个文件与机器学习模型有关，值得我们更加关注。下面的代码片段是其中的一部分。
- en: Listing 12.1\. Unit tests of a model’s API—its input-output shapes and trainability
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.1\. 模型API的单元测试——其输入输出形状和可训练性
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1*** Ensures that the input and output of the model have the expected shape'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 确保模型的输入和输出具有预期的形状'
- en: '***2*** Trains the model very briefly; this should be fast, but it won’t be
    accurate.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 对模型进行非常简短的训练；这个过程应该很快，但不一定准确。'
- en: '***3*** Checks that training is reporting metrics for each training step as
    a signal that training occurred'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 检查训练是否报告了每次训练步骤的指标，作为训练是否发生的信号'
- en: '***4*** Runs a prediction through the model focused on verifying the API is
    as expected'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 对模型进行预测以验证API是否符合预期'
- en: '***5*** Makes sure the prediction is in the range of possible answers; we don’t
    want to check for the actual value, as the training was exceptionally brief and
    might be unstable.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 确保预测值在可能答案的范围内；我们不想检查实际值，因为训练时间非常短可能会不稳定。'
- en: 'This test is covering a lot of ground, so let’s break it down a little bit.
    We first build a model using a helper function. For this test, we don’t care about
    the structure of the model and will treat it like a black box. We then make assertions
    on the shape of the inputs and outputs:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个测试覆盖了很多方面，所以让我们稍微细分一下。我们首先使用一个辅助函数来构建一个模型。在这个测试中，我们并不关心模型的结构，并将其视为一个黑盒子。然后我们对输入和输出的形状进行断言：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These tests can catch problems in terms of misidentifying the batch dimension—regression
    versus classification, output shape, and so on. Next, we compile and train the
    model on a very small number of steps. Our goal is simply to ensure that the model
    is trainable—we’re not worried about accuracy, stability, or convergence at this
    point:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些测试可以检测到错误的批次维度（回归或分类）、输出形状等问题。之后，我们在很少的步骤中编译和训练模型。我们的目标仅是确保模型可以被训练，此时我们不担心准确性、稳定性或收敛性：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This snippet also checks that training reported the required metrics for analysis:
    if we trained for real, would we be able to inspect the progress of the training
    and the accuracy of the resulting model? Finally, we try a simple:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段还检查训练是否报告了所需的分析指标：如果我们进行了实际训练，我们能否检查训练的进度和生成模型的准确性？最后，我们尝试一个简单的：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’re not checking for any particular prediction result, as that might change
    based on the random initialization of weight values or possible future revisions
    to the model architecture. What we do check is that we get a prediction and that
    the prediction is in the expected range, in this case, between 0 and 1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不检查任何特定的预测结果，因为这可能会因权重值的随机初始化或可能的模型架构未来修订而发生变化。我们所检查的是我们获得了预测并且预测在预期范围内，在这种情况下是0到1。
- en: The most important lesson here is noticing that no matter how we change the
    inside of the model’s architecture, as long we don’t change its input or its output
    API, this test should always pass. If the test is failing, we have a problem in
    our model. These remain lightweight and fast tests that provide a strong degree
    of API correctness, and they are suitable for inclusion in whatever commonly run
    test hooks you use.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最重要的教训是注意到无论我们如何更改模型架构的内部，只要我们不改变其输入或输出 API，这个测试应该始终通过。如果测试失败了，那么我们的模型就有问题。这些测试仍然是轻量级且快速的，提供了强大的
    API 正确性，并适合包含在您使用的常见测试挂钩中。
- en: 12.1.2\. Testing with golden values
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.2\. 使用黄金值进行测试
- en: In the previous section, we talked about the unit testing we can do without
    asserting on a threshold metric value or requiring a stable or convergent training.
    Now let’s explore the types of testing people often want to run with a fully trained
    model, starting with checking predictions of particular data points. Perhaps there
    are some “obvious” examples that you want to test. For instance, for an object
    detector, an input image with a nice big cat in it should be labeled as such;
    for a sentiment analyzer, a text snippet that’s clearly a negative customer review
    should be classified as such. These correct answers for given model inputs are
    what we refer to as *golden values*. If you follow the mindset of traditional
    unit testing blindly, it is easy to fall into the trap of testing trained machine-learning
    models with golden values. After all, we want a well-trained object detector to
    always label the cat in an image with a cat in it, right? Not quite. Golden-value-based
    testing can be problematic in a machine-learning setting because we’re usurping
    our training, validation, and evaluation data split.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了在不断言阈值指标值或不要求稳定或收敛训练的情况下可进行的单元测试。现在让我们探讨人们通常希望在完全训练后的模型上运行的测试类型，从检查特定数据点的预测开始。也许有一些“明显”的示例需要测试。例如，对于一个物体检测器，具有可识别猫咪的输入图像应被标记为识别了猫咪；对于一个情感分析器，明显是负面的客户评论的文本片段应被分类为负面。这些针对给定模型输入的正确答案是我们所谓的“黄金值”。如果盲目地遵循传统单元测试的思路，很容易陷入使用“黄金值”测试训练后的机器学习模型的误区。毕竟，我们希望一个训练良好的物体检测器总是能够在图像中的猫咪上打上“猫”的标记，对吗？并不完全是这样。基于“黄金值”的测试在机器学习设置中可能会存在问题，因为我们篡改了训练、验证和评估数据分割。
- en: Assuming you had a representative sample for your validation and test datasets,
    and you set an appropriate target metric (accuracy, recall, and so on), why is
    any one example required to be right more than another? The training of a machine-learning
    model is concerned with accuracy on the entire validation and test sets. The predictions
    for individual examples may vary with the selection of hyperparameters and initial
    weight values. If there are some examples that must be classified correctly and
    are easy to identify, why not detect them before asking the machine-learning model
    to classify them and instead use a non-machine-learning code to handle them? Such
    examples are used occasionally in natural language processing systems, where a
    subset of query inputs (such as frequently encountered and easily identifiable
    ones) are automatically routed to a non-machine-learning module for handling,
    while the remaining queries are handled by a machine-learning model. You’ll save
    on compute time, and that portion of the code is easier to test with traditional
    unit testing. While adding a business-logic layer before (or after) the machine-learning
    predictor might seem like extra work, it gives you the hooks to control overrides
    of predictions. It’s also a place where you can add monitoring or logging, which
    you’ll probably want as your tool becomes more widely used. With that preamble,
    let’s explore the three common desires for golden values separately.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的验证集和测试集有代表性的样本，并且你设置了一个合适的目标指标（准确率、召回率等），为什么要求任何一个例子比另一个例子更准确？机器学习模型的训练关注的是整个验证集和测试集的准确率。对于单个样本的预测可能会随着超参数和初始权重值的选择而变化。如果有些例子必须被正确分类并且很容易识别，为什么不在要求机器学习模型对它们进行分类之前检测它们，而是使用非机器学习代码来处理它们呢？在自然语言处理系统中偶尔会使用这样的例子，其中查询输入的子集（如经常遇到且易于识别的输入）会自动路由到非机器学习模块进行处理，而其余的查询会由机器学习模型处理。你会节省计算时间，并且该部分代码更容易通过传统的单元测试进行测试。虽然在机器学习预测器之前（或之后）添加业务逻辑层似乎多此一举，但它为你提供了控制预测覆盖的钩子。这也是你可以添加监控或日志记录的地方，当你的工具变得更广泛使用时，你可能会需要。有了这个前提，让我们分别探讨三种常见的对金标值的需求。
- en: One common motivation of this type of golden-value test is in service to a full
    end-to-end test—given an unprocessed user input, what does the system output?
    The machine-learning system is trained, and a prediction is requested through
    the normal end-user code flow, with an answer being returned to the user. This
    is similar to our unit test in [listing 12.1](#ch12ex01), but the machine-learning
    system is in context with the rest of the application. We could write a test similar
    to [listing 12.1](#ch12ex01) that doesn’t care about the actual value of the prediction,
    and, in fact, that would be a more stable test. However, it’s very tempting to
    combine it with an example/prediction pair that makes sense and is easily understood
    when developers revisit the test.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的金标值测试的一个常见动机是为了进行完整的端到端测试——给定一个未经处理的用户输入，系统会输出什么？机器学习系统经过训练，通过正常的端用户代码流程请求预测，然后将答案返回给用户。这类似于我们在[列表
    12.1](#ch12ex01)中的单元测试，但机器学习系统是在应用程序的其余部分的上下文中。我们可以编写一个类似于[列表 12.1](#ch12ex01)的测试，它不关心预测的实际值，实际上，这将是一个更稳定的测试。但是，当开发人员重新访问测试时，将其与一个有意义且容易理解的示例/预测对结合起来是非常诱人的。
- en: This is when the trouble enters—we need an example whose prediction is known
    and guaranteed to be correct or else the end-to-end test fails. So, we add a smaller-scale
    test that tests that prediction through a subset of the pipeline covered by the
    end-to-end test. Now if the end-to-end test fails, and the smaller test passes,
    we’ve isolated the error to interactions between the core machine-learning model
    and other parts of the pipeline (such as data ingestion or postprocessing). If
    both fail in unison, we know our example/prediction invariant is broken. In this
    case, it’s more of a diagnostic tool, but the likely result of the paired failure
    is picking a new example to encode, not retraining the model entirely.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是问题出现的时候——我们需要一个其预测已知且保证正确的示例，否则端到端测试将失败。因此，我们添加了一个较小规模的测试，通过端到端测试涵盖的管道的子集来测试该预测。现在，如果端到端测试失败，而较小的测试通过，则我们已将错误隔离到核心机器学习模型与管道的其他部分之间的交互（例如数据摄取或后处理）。如果两者同时失败，我们知道我们的示例/预测不变式被打破了。在这种情况下，它更像是一种诊断工具，但配对失败的可能结果是选择一个新的示例进行编码，而不是重新训练模型。
- en: The next most common source is some form of business requirement. Some identifiable
    set of examples must be more accurate than the rest. As mentioned previously,
    this is the perfect setting for adding a pre- or post-model business-logic layer
    to handle these predictions. However, you can experiment with *example weighting*,
    in which some examples count for more than others when calculating the overall
    quality metrics. It won’t guarantee correctness, but it will bias the model toward
    getting those correct. If a business-logic layer is difficult because you can’t
    easily pre-identify the properties of the input that trigger the special case,
    you might need to explore a second model—one that is purely used to determine
    if override is needed. In this case, you’re using an ensemble of models, and your
    business logic is combining the predictions from two layers to do the correct
    action.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个最常见的来源是某种业务需求。某些可识别的示例集必须比其他示例更准确。如前所述，这是添加一个用于处理这些预测的模型前后业务逻辑层的完美设置。但是，您可以尝试*示例加权*，其中一些示例在计算整体质量指标时占比更多。这不会保证正确性，但它会使模型倾向于获得这些正确。如果由于无法轻松预先识别触发特殊情况的输入属性而导致业务逻辑层困难，则可能需要探索第二个模型——一个纯粹用于确定是否需要覆盖的模型。在这种情况下，您正在使用模型的集成，并且您的业务逻辑是将两个层的预测组合起来执行正确的操作。
- en: The last case here is when you have a bug report with a user-provided example
    that gave the wrong result. If it’s wrong for business reasons, we’re back in
    the immediately preceding case. If it’s wrong just because it falls into the failing
    percent of the model’s performance curve, there’s not a lot that we should do.
    It’s within the accepted performance of the trained algorithm; all models are
    expected to make some mistakes. You can add the example/correct prediction pair
    to your train/test/eval sets as appropriate to hopefully generate a better model
    in the future, but it’s not appropriate to use the golden values for unit testing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最后一种情况是当你有一个带有用户提供示例的错误报告，该示例产生了错误的结果。如果出于业务原因错误，我们回到了刚才讨论的情况。如果出错只是因为它落入模型性能曲线的失败百分比中，那我们应该做的事情就不多了。这在经过训练的算法的可接受性能范围内；所有模型都有可能出错。您可以将示例/正确预测对添加到适当的训练/测试/评估集中，以便希望在未来生成更好的模型，但不适合使用黄金值进行单元测试。
- en: An exception to this is if you’re keeping the model constant—you have the model
    weights and architecture checked into version control and are not regenerating
    them in the tests. Then it can be appropriate to use golden values to test the
    outputs of an inference system that uses the model as its core, as neither the
    model nor the examples are subject to change. Such an inference system contains
    parts other than the model, such as parts that preprocess the input data before
    feeding it to the model and ones that take the model’s outputs and transform them
    into forms more suitable for use by downstream systems. Such unit tests ensure
    the correctness of such pre- and postprocessing logic.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例外是如果您保持模型恒定——您已将模型权重和架构检入版本控制，并且在测试中不重新生成它们。那么使用黄金值来测试将使用模型作为其核心的推理系统的输出可能是适当的，因为模型和示例都不会发生变化。这样的推理系统包含除模型之外的其他部分，例如预处理输入数据并将其馈送到模型的部分以及获取模型输出并将其转换为更适合下游系统使用的形式的部分。这样的单元测试确保了这种预处理和后处理逻辑的正确性。
- en: 'Another legitimate use of golden values is outside unit testing: the monitoring
    of the quality of a model (but not as unit testing) as it evolves. We will expand
    on this when we discuss the model validator and evaluator in the next section.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个合理使用黄金值的场景是在单元测试之外：随着模型的演化监控模型的质量（但不作为单元测试）。我们将在下一节讨论模型验证器和评估器时进行详细展开。
- en: 12.1.3\. Considerations around continuous training
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.3\. 关于持续训练的考虑
- en: In many machine-learning systems, you get new training data at fairly regular
    intervals (every week or every day). Perhaps you’re able to use your logs for
    the previous day to generate new, more timely training data. In such systems,
    the model needs to be retrained frequently, using the latest data available. In
    these cases, there is a belief that the age or staleness of the model affects
    its power. As time goes on, the inputs to the model drift to a different distribution
    than it was trained on, so the quality characteristics will get worse. As an example,
    you might have a clothing-recommendation tool that was trained in the winter but
    is making predictions in the summer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多机器学习系统中，您会定期获得新的训练数据（每周或每天）。也许你能够使用前一天的日志生成新的、更及时的训练数据。在这种系统中，模型需要经常重新训练，使用最新可用的数据。在这些情况下，人们相信模型的年龄或陈旧程度会影响其能力。随着时间的推移，模型的输入会漂移到与其训练不同的分布，因此质量特征会变差。例如，你可能有一个服装推荐工具，在冬天训练过，但在夏天做出预测。
- en: Given the basic idea, as you begin to explore systems that require continuous
    training, you’ll have a wide variety of extra components that create your pipeline.
    A full discussion of these is outside the scope of this book, but TensorFlow Extended
    (TFX)^([[1](#ch12fn1)]) is an infrastructure to look at for more ideas. The pipeline
    components it lists that have the most relevance in a testing arena are the *example
    validator*, *model validator*, and *model evaluator*. The diagram in [figure 12.1](#ch12fig01)
    contains boxes that correspond to these components.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始探索需要连续训练的系统时，根据这个基本思想，你将拥有多种额外组件来创建你的流水线。关于这些组件的全面讨论超出了本书的范围，但TensorFlow
    Extended（TFX）^([[1](#ch12fn1)])是一个值得一看的基础设施，可以提供更多的想法。在测试领域中，它列出的最相关的流水线组件是*示例验证器*，*模型验证器*和*模型评估器*。[图12.1](#ch12fig01)中的图表包含与这些组件对应的框。
- en: ¹
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Denis Baylor et al., “TFX: A TensorFlow-Based Production-Scale Machine Learning
    Platform,” KDD 2017, [www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform](http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform).'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Denis Baylor等人，“TFX：基于TensorFlow的生产规模机器学习平台”，KDD 2017，[www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform](http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform)。
- en: 'The example validator is about testing the data, an easy-to-overlook aspect
    of testing a machine-learning system. There is a famous saying among machine-learning
    practitioners: “garbage in, garbage out.” The quality of a trained machine-learning
    model is limited by the quality of the data that goes into it. Examples with invalid
    feature values or incorrect labels will likely hurt the accuracy of the trained
    model when deployed for use (that is, if the model-training job doesn’t fail because
    of the bad examples first!). The example validator is used to ensure that properties
    of the data that go into model training and evaluation always meet certain requirements:
    that you have enough data, that its distribution appears valid, and that you don’t
    have any odd outliers. For instance, if you have a set of medical data, the body
    height (in centimeters) should be a positive number no larger than 280; the patient
    age should be a positive number between 0 and 130; the oral temperature (in degrees
    Celsius) should be a positive number between roughly 30 and 45, and so forth.
    If certain data examples contain features that fall outside such ranges or have
    placeholder values such as “None” or NaN, we know something is wrong with those
    examples, and they should be treated accordingly—in most cases, excluded from
    the training and evaluation. Typically, errors here indicate either a failure
    of the data-collection process or that the “world has changed” in ways incompatible
    with the assumptions you held when building the system. Normally, this is more
    analogous to monitoring and alerting than integration testing.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 示例验证器是关于测试数据的，这是测试机器学习系统时容易忽视的一个方面。在机器学习实践者中有一句著名的话：“垃圾进，垃圾出。”训练好的机器学习模型的质量受到输入数据质量的限制。具有无效特征值或不正确标签的示例在部署使用时（即使模型训练任务由于坏示例而失败！）可能会影响训练模型的准确性。示例验证器用于确保进入模型训练和评估的数据的属性始终满足某些要求：你有足够的数据，其分布看起来有效，并且没有任何奇怪的离群值。例如，如果你有一组医疗数据，身高（以厘米为单位）应该是一个不大于280的正数；患者年龄应该是0到130之间的正数；口腔温度（以摄氏度为单位）应该是大约在30到45之间的正数，等等。如果某些数据示例包含超出这些范围的特征或具有“None”或NaN等占位符值，我们就知道这些示例有问题，它们应该相应地处理——在大多数情况下，排除在训练和评估之外。通常，这里的错误表明数据收集过程失败，或者当构建系统时持有的假设与“世界变化”的方式不兼容。通常，这更类似于监视和警报，而不是集成测试。
- en: A component like an example validator is also useful for detecting *training-serving
    skew*, a particularly nasty type of bug that can arise in machine-learning systems.
    The two main causes are 1) training and serving data that belongs to different
    distributions and 2) data preprocessing involving code paths that behave differently
    during training and serving. An example validator deployed to both the training
    and serving environments has the potential to catch bugs introduced via either
    path.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 像示例验证器这样的组件还对检测*训练服务偏差*有用，这是机器学习系统中可能出现的一种特别严重的错误。两个主要原因是1）属于不同分布的训练和服务数据，以及2）数据预处理涉及在训练和服务期间行为不同的代码路径。部署到训练和服务环境的示例验证器有潜力捕获通过任一路径引入的错误。
- en: The model validator plays the role of the person building the model in deciding
    if the model is “good enough” to use in serving. You configure it with the quality
    metrics you care about, and then it either “blesses” the model or rejects it.
    Again, like the example validator, this is more of a monitor-and-alert-style interaction.
    You’ll also typically want to log and chart your quality metrics over time (accuracy
    and so on) in order to see if you’re having small-scale, systematic degradations
    that might not trigger an alert by themselves but might still be useful for diagnosing
    long-term trends and isolating their causes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 模型验证器扮演着构建模型的人在决定模型是否“足够好”以用于服务的角色。你可以根据自己关心的质量指标对其进行配置，然后它要么“祝福”该模型，要么拒绝它。再次强调，就像示例验证器一样，这更像是一种监视和警报式的交互。你还通常会想要随时间记录和绘制你的质量指标（准确度等），以便查看是否存在可能不会单独触发警报但可能仍然有用于诊断长期趋势并隔离其原因的小规模系统性恶化。
- en: The model evaluator is a sort of deeper dive into the quality statistics of
    the model, slicing and dicing the quality along a user-defined axis. Often, this
    is used to probe if the model is behaving fairly for different user populations—age
    bands, education bands, geographic, and so on. A simple example would be looking
    at the iris-flower examples we used in [section 3.3](kindle_split_014.html#ch03lev1sec3)
    and checking if our classification accuracy is roughly similar among the three
    iris species. If our test or evaluation sets are unusually biased toward one of
    the populations, it is possible we are always wrong on the smallest population
    without it showing up as a top-level accuracy problem. As with the model validator,
    the trends over time are often as useful as the individual point-in-time measurement.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估器是对模型质量统计的更深入的探究，沿着用户定义的轴切割和分析质量。通常，这用于探测模型是否对不同的用户群体——年龄段、教育水平、地理位置等——表现公平。一个简单的例子是查看我们在[第
    3.3 节](kindle_split_014.html#ch03lev1sec3)中使用的鸢尾花示例，并检查我们的分类准确率在三种鸢尾花物种之间是否大致相似。如果我们的测试或评估集对其中一种人口有异常偏向，那么可能我们总是在最小的人口上出错，但这并没有显示为一个最高级别的准确性问题。与模型验证器一样，随时间变化的趋势通常与个别时点的测量一样有用。
- en: 12.2\. Model optimization
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2\. 模型优化
- en: Once you have painstakingly created, trained, and tested your model, it is time
    to put it to use. This process, called *model deployment*, is no less important
    than the previous steps of model development. Whether the model is to be shipped
    to the client side for inference or executed at the backend for serving, we always
    want the model to be fast and efficient. Specifically, we want the model to
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您费尽心思地创建、训练和测试了您的模型，就该是将其投入使用的时候了。这个过程被称为*模型部署*，它与模型开发的前几个步骤同样重要。无论模型是要在客户端进行推理还是在后端进行服务，我们总是希望模型能够快速高效。具体而言，我们希望模型能够
- en: Be small in size and hence fast to load over the web or from disk
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体积小，因此在网络上或从磁盘加载时速度快
- en: Consume as little time, compute, and memory as possible when its `predict()`
    method is called
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当调用其`predict()`方法时，尽可能少地消耗时间、计算和内存。
- en: This section describes techniques available in TensorFlow.js for optimizing
    the size and inference speed of trained models before they are released for deployment.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了 TensorFlow.js 中用于优化训练模型大小和推理速度的技术，然后它们才会发布部署。
- en: The meaning of the word *optimization* is overloaded. In the context of this
    section, *optimization* refers to improvements including model-size reduction
    and computation acceleration. This is not to be confused with weight-parameter
    optimization techniques such as gradient descent in the context of model training
    and optimizers. This distinction is sometimes referred to as model *quality* versus
    model *performance*. Performance refers to how much time and resources the model
    consumes to do its task. Quality refers to how close the results are to an ideal.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*优化*一词的含义是多重的。在本节的语境中，*优化*指的是包括模型大小减小和计算加速在内的改进。这不应与权重参数优化技术混淆，比如模型训练和优化器中的梯度下降。这种区别有时被称为模型*质量*与模型*性能*。性能指的是模型完成任务所消耗的时间和资源。质量指的是结果与理想结果的接近程度。'
- en: 12.2.1\. Model-size optimization through post-training weight quantization
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.1\. 通过后训练权重量化实现模型大小优化
- en: The need to have small files that are swift to load over the internet should
    be abundantly clear to web developers. It is especially important if your website
    targets a very large user base or users with slow internet connections.^([[2](#ch12fn2)])
    In addition, if your model is stored on a mobile device (see [section 12.3.4](#ch12lev2sec9)
    for a discussion of mobile deployment with TensorFlow.js), the size of the model
    is often constrained by limited storage space. As a challenge for model deployment,
    neural networks are large and still getting larger. The capacity (that is, predictive
    power) of deep neural networks often comes at the cost of increased layer count
    and larger layer sizes. At the time of this writing, state-of-the-art image-recognition,^([[3](#ch12fn3)])
    speech-recognition,^([[4](#ch12fn4)]) natural language processing,^([[5](#ch12fn5)])
    and generative models^([[6](#ch12fn6)]) often exceed 1 GB in the size of their
    weights. Due to the tension between the need for models to be both small and powerful,
    a highly active area of research in deep learning is model-size optimization,
    or how to design a neural network with a size as small as possible that can still
    perform its tasks with an accuracy close to that of a larger neural network. Two
    general approaches are available. In the first approach, researchers design a
    neural network with the aim of minimizing model size from the outset. Second,
    there are techniques through which existing neural networks can be shrunk to a
    smaller size.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网上快速加载小文件的需求对于网页开发者来说应该是非常明确的。如果您的网站目标是非常庞大的用户群或者拥有较慢的互联网连接的用户，这一点尤为重要。^([[2](#ch12fn2)])此外，如果您的模型存储在移动设备上（请参阅
    [12.3.4 节](#ch12lev2sec9) 对使用 TensorFlow.js 进行移动部署的讨论），则模型的大小通常受到有限的存储空间的限制。作为模型部署的挑战，神经网络是庞大的，并且仍在不断增大。深度神经网络的容量（即，预测能力）往往是以增加层数和更大的层尺寸为代价的。在撰写本文时，最先进的图像识别、^([[3](#ch12fn3)])语音识别、^([[4](#ch12fn4)])自然语言处理、^([[5](#ch12fn5)])以及生成模型^([[6](#ch12fn6)])往往超过
    1 GB 的权重大小。由于模型需要同时小巧和强大之间的紧张关系，深度学习中一个极其活跃的研究领域是模型大小优化，即如何设计一个尽可能小但仍能以接近较大神经网络的准确度执行任务的神经网络。有两种一般方法可供选择。在第一种方法中，研究人员设计一个神经网络，旨在从一开始就将模型大小最小化。其次，还有通过这些方法将现有神经网络缩小至更小尺寸的技术。
- en: ²
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In March 2019, Google launched a Doodle featuring a neural network that can
    compose music in Johann Sebastian Bach’s style ([http://mng.bz/MOQW](http://mng.bz/MOQW)).
    The neural network runs in the browser, powered by TensorFlow.js. The model is
    quantized as 8-bit integers with the method described in this section, which cuts
    the model’s over-the-wire size by several times, down to about 380 KB. Without
    this quantization, it would be impossible to serve the model to an audience as
    wide as that of Google’s homepage (where Google Doodles appear).
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2019 年 3 月，Google 推出了一个涉及使用神经网络以约翰·塞巴斯蒂安·巴赫风格创作音乐的涂鸦（[http://mng.bz/MOQW](http://mng.bz/MOQW)）。这个神经网络在浏览器中运行，由
    TensorFlow.js 提供动力。该模型以本节描述的方法量化为 8 位整数，将模型的传输大小减少了数倍，降至约 380 KB。如果没有这种量化，将无法将模型提供给谷歌首页（Google
    涂鸦出现的地方）等如此广泛的观众。
- en: ³
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kaiming He et al., “Deep Residual Learning for Image Recognition,” submitted
    10 Dec. 2015, [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kaiming He 等人，“深度残差学习用于图像识别”，于 2015 年 12 月 10 日提交，[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)。
- en: ⁴
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Johan Schalkwyk, “An All-Neural On-Device Speech Recognizer,” Google AI Blog,
    12 Mar. 2019, [http://mng.bz/ad67](http://mng.bz/ad67).
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Johan Schalkwyk，“一种全神经元设备上的语音识别器”，Google AI 博客，于 2019 年 3 月 12 日，[http://mng.bz/ad67](http://mng.bz/ad67)。
- en: ⁵
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding,” submitted 11 Oct. 2018, [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Jacob Devlin 等人，“BERT: 深度双向转换器的预训练用于语言理解”，于 2018 年 10 月 11 日提交，[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)。'
- en: ⁶
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tero Karras, Samuli Laine, and Timo Aila, ”A Style-Based Generator Architecture
    for Generative Adversarial Networks,” submitted 12 Dec. 2018, [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948).
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tero Karras，Samuli Laine 和 Timo Aila，“用于生成对抗网络的基于风格的生成器架构”，于 2018 年 12 月 12
    日提交，[https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)。
- en: MobileNetV2, which we visited in the chapters on convnets, is produced by the
    first line of research.^([[7](#ch12fn7)]) It is a small, lightweight image model
    suitable for deployment on resource-restricted environments such as web browsers
    and mobile devices. The accuracy of MobileNetV2 is slightly worse compared to
    that of a larger image trained on the same tasks, such as ResNet50\. But its size
    (14 MB) is a few times smaller in comparison (ResNet50 is about 100 MB in size),
    which makes the slight reduction in accuracy a worthy trade-off.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在卷积神经网络章节中介绍过的MobileNetV2是第一行研究的产物。[[7](#ch12fn7)] 它是一种适用于资源受限环境（如Web浏览器和移动设备）部署的小型、轻量级图像模型。与在相同任务上训练的更大的图像模型（如ResNet50）相比，MobileNetV2的准确度略差一些。但是它的尺寸（14
    MB）比较小（ResNet50的尺寸约为100 MB），这使得准确度的轻微降低是值得的。
- en: ⁷
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mark Sandler et al., “MobileNetV2: Inverted Residuals and Linear Bottlenecks,”
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4510–4520,
    [http://mng.bz/NeP7](http://mng.bz/NeP7).'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Mark Sandler等人，“MobileNetV2: 反向残差和线性瓶颈”，IEEE计算机视觉与模式识别会议（CVPR），2018年，pp. 4510–4520，[http://mng.bz/NeP7](http://mng.bz/NeP7).'
- en: Even with its built-in size-squeezing, MobileNetV2 is still a little too large
    for most JavaScript applications. Consider the fact that its size (14 MB) is about
    eight times the size of an average web page.^([[8](#ch12fn8)]) MobileNetV2 offers
    a width parameter, which, if set to a value smaller than 1, reduces the size of
    all convolutional layers and hence provides further shrinkage in the size (and
    further loss in accuracy). For example, the version of MobileNetV2 with its width
    set to 0.25 is approximately a quarter of the size of the full model (3.5 MB).
    But even that may be unacceptable to high-traffic websites that are sensitive
    to increases in page weight and load time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 即使内置了尺寸压缩功能，MobileNetV2对于大多数JavaScript应用程序来说仍然稍大。考虑到其大小（14 MB）约为平均网页大小的八倍。[[8](#ch12fn8)]
    MobileNetV2提供了一个宽度参数，如果将其设置为小于1的值，则可以减小所有卷积层的尺寸，从而进一步减小尺寸（以及进一步降低准确度）。例如，将宽度设置为0.25的MobileNetV2版本大约是完整模型大小的四分之一（3.5
    MB）。但即便如此，对于对页面权重和加载时间的增加敏感的高流量网站来说，这可能仍然无法接受。
- en: ⁸
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'According to HTTP Archive, the average page weight (total transfer size of
    HTML, CSS, JavaScript, images, and other static files) is about 1,828 KB for desktop
    and 1,682 KB for mobile as of May 2019: [https://httparchive.org/reports/page-weight](https://httparchive.org/reports/page-weight).'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据HTTP Archive，截至2019年5月，桌面端平均页面权重（HTML、CSS、JavaScript、图像和其他静态文件的总传输大小）约为1,828
    KB，移动端约为1,682 KB：[https://httparchive.org/reports/page-weight](https://httparchive.org/reports/page-weight).
- en: 'Is there a way to further reduce the size of such models? Luckily, the answer
    is yes. This brings us to the second approach mentioned, model-independent size
    optimization. The techniques in this category are more generic in that they do
    not require changes to the model architecture itself and hence should be applicable
    to a wide variety of existing deep neural networks.The technique we will specifically
    focus on here is called *post-training weight quantization*. The idea is simple:
    after a model is trained, store its weight parameters at a lower numeric precision.
    [Info box 12.1](#ch12sb01) describes how this is done for readers who are interested
    in the underlying mathematics.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有办法进一步减小这种模型的尺寸？幸运的是，答案是肯定的。这将我们带到了提到的第二种方法，即模型无关的尺寸优化。这类技术更加通用，因为它们不需要对模型体系结构本身进行更改，因此应该适用于各种现有的深度神经网络。我们将在这里专门关注的技术称为*训练后权重量化*。其思想很简单：在模型训练完成后，以更低的数值精度存储其权重参数。
    [信息框12.1](#ch12sb01)描述了对于对底层数学感兴趣的读者如何实现这一点。
- en: '|  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The mathematics behind post-training weight quantization**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于训练后权重量化的数学原理**'
- en: The weight parameters of a neural network are represented as 32-bit floating-point
    (float32) numbers during training. This is true not only in TensorFlow.js but
    also in other deep-learning frameworks such as TensorFlow and PyTorch. This relatively
    expensive representation is usually okay because model training typically happens
    in environments with unrestricted resources (for example, the backend environment
    of a workstation equipped with ample memory, fast CPUs, and CUDA GPUs). However,
    empirical findings indicate that for many inference use cases, we can lower the
    precision of weights without causing a substantial decrease in accuracy. To reduce
    the representation precision, we map each float32 value onto an 8-bit or 16-bit
    integer value that represents the discretized location of the value within the
    range of all values in the same weight. This process is what we call *quantization*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的权重参数在训练过程中以32位浮点（float32）数表示。这不仅适用于TensorFlow.js，还适用于其他深度学习框架，如TensorFlow和PyTorch。尽管这种相对昂贵的表示通常在训练模型时没有问题（例如，配备了充足内存、快速CPU和CUDA
    GPU的工作站后端环境），但经验研究表明，对于许多推理用例，我们可以降低权重的精度而不会导致精度大幅度下降。为了降低表示精度，我们将每个float32值映射到一个8位或16位整数值，该值表示该权重中所有值范围内的离散位置。这个过程就是我们所说的*量化*。
- en: In TensorFlow.js, weight quantization is performed on a weight-by-weight basis.
    For example, if a neural network consists of four weight variables (such as the
    weights and biases of two dense layers), each of the weights will undergo quantization
    as a whole. The equation that governs quantization of a weight is
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow.js中，权重量化是逐个权重进行的。例如，如果神经网络由四个权重变量组成（例如两个密集层的权重和偏置），则每个权重将作为整体进行量化。控制权重量化的方程式如下：
- en: equation 12.1\.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程12.1。
- en: '![](12fig01_alt.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig01_alt.jpg)'
- en: In this equation, *B* is the number of bits that the quantization result will
    be stored in. It can be either 8 or 16, as currently supported by TensorFlow.js.
    *w*[Min] is the minimum value of the parameters of the weight. *w*Scale is the
    range of the parameters (the difference between the minimum and the maximum).
    The equation is valid, of course, only when *w*Scale is nonzero. In the special
    cases where *w*Scale is zero—that is, when all parameters of the weight have the
    same value—quantize(*w*) will return 0 for all *w*’s.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在此方程中，*B*是量化结果将存储的位数。它可以是8位或16位，如TensorFlow.js目前支持的。*w*[Min]是权重参数的最小值。*w*Scale是参数的范围（最大值与最小值之间的差异）。当然，只有在*w*Scale非零时，方程才有效。在*w*Scale为零的特殊情况下，即当权重的所有参数具有相同值时，quantize(*w*)将为所有*w*返回0。
- en: 'The two auxiliary values *w*[Min] and *w*Scale are saved together with the
    quantized weight values to support recovery of the weights (a process we refer
    to as *dequantization*) during model loading. The equation that governs dequantization
    is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 两个辅助值*w*[Min]和*w*Scale与量化后的权重值一起保存，以支持在模型加载期间恢复权重（我们称之为*去量化*）的过程。控制去量化的方程式如下：
- en: equation 12.2\.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程12.2。
- en: '![](12eqa01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](12eqa01.jpg)'
- en: This equation is valid whether or not *w*Scale is zero.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 无论*w*Scale是否为零，此方程式都有效。
- en: '|  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Post-training quantization provides considerable reduction in model size: 16-bit
    quantization cuts the model size by approximately 50%, 8-bit quantization by 75%.
    These percentages are approximate for two reasons. First, a fraction of the model’s
    size is devoted to the model’s topology, as encoded in the JSON file. Second,
    as stated in the info box, quantization requires the storage of two additional
    floating-number values (*w*[Min] and *w*[Scale]), along with a new integer value
    (the bits of quantization). However, these are usually minor compared to the reduction
    in the number of bits used to represent the weight parameters.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化可以大大减小模型大小：16位量化将模型大小减少约50％，8位量化则减少约75％。这些百分比是近似值，有两个原因。首先，模型大小的一部分用于模型的拓扑结构，如JSON文件中所编码的。其次，正如信息框中所述，量化需要存储两个额外的浮点数值（*w*[Min]和*w*[Scale]），以及一个新的整数值（量化的位数）。然而，与用于表示权重参数的位数的减少相比，这些通常是次要的。
- en: Quantization is a lossy transformation. Some information in the original weight
    values is lost as a result of the decreased precision. It is analogous to reducing
    the bit depth of a 24-bit color image to an 8-bit one (the kind you may have seen
    on Nintendo’s game consoles from the 1980s), the effect of which is easily visible
    to human eyes. [Figure 12.2](#ch12fig02) provides intuitive comparisons of the
    degree of discretization that 16-bit and 8-bit quantization lead to. As you might
    expect, 8-bit quantization leads to a more coarse-grained representation of the
    original weights. Under 8-bit quantization, there are only 256 possible values
    over the entire range of a weight’s parameters, as compared with 65,536 possible
    values under 16-bit quantization. Both are dramatic reductions in precision compared
    to the 32-bit float representation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种有损转换。由于精度降低，原始权重值中的一些信息会丢失。这类似于将24位颜色图像的位深度减少为8位（你可能在任天堂的游戏机上见过的类型），这种效果对人眼来说很容易看到。[图12.2](#ch12fig02)提供了16位和8位量化导致的离散程度的直观比较。正如你所预期的，8位量化会导致对原始权重的粗糙表示。在8位量化下，对于权重参数的整个范围，只有256个可能的值，而在16位量化下有65536个可能的值。与32位浮点表示相比，这两者都是精度的显著降低。
- en: Figure 12.2\. Examples of 16-bit and 8-bit weight quantization. An original
    identity function (y = x, panel A) is reduced in size with 16-bit and 8-bit quantization;
    the results are shown in panels B and C, respectively. In order to make the quantization
    effects visible on the page, we zoom in on a small section of the identity function
    in the vicinity of *x* = 0.
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2。16位和8位权重量化的示例。原始的恒等函数（y = x，面板A）通过16位和8位量化减小了尺寸；结果分别显示在面板B和面板C中。为了使页面上的量化效果可见，我们放大了恒等函数在*x*
    = 0附近的一小部分。
- en: '![](12eqa02_alt.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](12eqa02_alt.jpg)'
- en: Practically, does the loss of precision in weight parameters really matter?
    When it comes to the deployment of a neural network, what matters is its accuracy
    on test data. To answer this question, we compiled a number of models covering
    different types of tasks in the quantization example of tfjs-examples. You can
    run the quantization experiments there and see the effects for yourself. To check
    out the example, use
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，由于权重参数的精度损失真的重要吗？在神经网络的部署中，重要的是它在测试数据上的准确性。为了回答这个问题，我们在tfjs-examples的量化示例中编译了许多涵盖不同类型任务的模型。你可以在那里运行量化实验，并亲眼看到效果。要查看示例，请使用以下命令：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The example contains four scenarios, each showcasing a unique combination of
    a dataset and the model applied on the dataset. The first scenario involves predicting
    average housing prices in geographic regions of California by using numeric features
    such as median age of the properties, total number of rooms, and so forth. The
    model is a five-layer network that includes dropout layers for the mitigation
    of overfitting. To train and save the original (nonquantized model), use this
    command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包含四个场景，每个场景展示了一个数据集和应用于数据集的模型的独特组合。第一个场景涉及使用数值特征（例如物业的中位数年龄、房间总数等）来预测加利福尼亚州地理区域的平均房价。该模型是一个包含了防止过拟合的dropout层的五层网络。要训练和保存原始（非量化）模型，请使用以下命令：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following command performs 16- and 8-bit quantization on the saved model
    and evaluates how the two levels of quantization affect the model’s accuracy on
    a test set (a subset of the data unseen during the model’s training):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的命令对保存的模型进行16位和8位量化，并评估这两种量化水平对测试数据集（模型训练期间未见的数据的子集）的模型准确性产生了什么影响：
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This command wraps a lot of actions inside for ease of use. However, the key
    step that actually quantizes the model can be seen in the shell script at quantization/quantize_
    evaluate.sh. In the script, you can see the following shell command that quantizes
    a model at the path `MODEL_JSON_PATH` with 16-bit quantization. You can follow
    the example of this command to quantize your own TensorFlow.js-saved models. If
    the option flag `--quantization_bytes` is set to `1` instead, 8-bit quantization
    will be performed:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将许多操作封装在内，以便使用。然而，实际量化模型的关键步骤可以在quantization/quantize_evaluate.sh的shell脚本中看到。在该脚本中，你可以看到以下shell命令，它对路径为`MODEL_JSON_PATH`的模型进行16位量化。你可以按照这个命令的示例来量化自己的TensorFlow.js保存的模型。如果选项标志`--quantization_bytes`设置为`1`，则将执行8位量化：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The previous command shows how to perform weight quantization on a model trained
    in JavaScript. `tensorflowjs_converter` also supports weight quantization when
    converting models from Python to JavaScript, the details of which are shown in
    [info box 12.2](#ch12sb02).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令展示了如何在JavaScript中对训练模型执行权重量化。当将模型从Python转换为JavaScript时，`tensorflowjs_converter`还支持权重量化，其详细信息显示在[信息框
    12.2](#ch12sb02)中。
- en: '|  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Weight quantization and models from Python**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重量化和来自Python的模型**'
- en: 'In [chapter 5](kindle_split_016.html#ch05), we showed how models from Keras
    (Python) can be converted to a format that can be loaded and used by TensorFlow.js.
    During such Python-to-JavaScript conversion, you can apply weight quantization.
    To do that, use the same `--quantization_ bytes` flag as described in the main
    text. For example, to convert a model in the HDF5 (.h5) format saved by Keras
    with 16-bit quantization, use the following command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第五章](kindle_split_016.html#ch05)中，我们展示了如何将来自Keras（Python）的模型转换为可以加载和使用TensorFlow.js的格式。在此类Python到JavaScript的转换期间，您可以应用权重量化。要执行此操作，请使用与主文中描述的相同的`--quantization_bytes`标志。例如，要将由Keras保存的HDF5（.h5）格式的模型转换为具有16位量化的模型，请使用以下命令：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this command, `KERAS_MODEL_H5_PATH` is the path to the model exported by
    Keras, while `TFJS_MODEL_PATH` is the path to which the converted and weight-quantized
    model will be generated.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在此命令中，`KERAS_MODEL_H5_PATH`是由Keras导出的模型的路径，而`TFJS_MODEL_PATH`是转换并进行权重量化的模型将生成的路径。
- en: '|  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The detailed accuracy values you get will vary slightly from run to run due
    to the random initialization of weights and the random shuffling of data batches
    during training. However, the general conclusion should always hold: as shown
    by the first row of [table 12.1](#ch12table01), 16-bit quantization on weights
    leads to miniscule changes in the MAE of the housing-price prediction, while 8-bit
    quantization leads to a relatively larger (but still tiny in absolute terms) increase
    in the MAE.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重的随机初始化和训练过程中数据批次的随机洗牌，您获得的详细准确性值可能会有轻微变化。然而，总体结论应始终保持不变：正如[table 12.1](#ch12table01)的第一行所示，对权重进行16位量化会导致住房价格预测的MAE发生微小变化，而对权重进行8位量化会导致MAE相对较大（但在绝对值上仍然微小）的增加。
- en: Table 12.1\. Evaluation accuracies for four different models with post-training
    weight quantization
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.1。四个不同模型的评估准确性，经过训练后进行权重量化
- en: '| Dataset and model | Evaluation loss and accuracy under no-quantization and
    different levels of quantization |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 数据集和模型 | 在无量化和不同量化级别下的评估损失和准确性 |'
- en: '| --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 32-bit full precision (no quantization) | 16-bit quantization | 8-bit quantization
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 32位全精度（无量化） | 16位量化 | 8位量化 |'
- en: '| --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| California housing: MLP regressor | MAE^([[a](#ch12table01tn1)]) = 0.311984
    | MAE = 0.311983 | MAE = 0.312780 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 加利福尼亚房屋：MLP回归器 | MAE^([[a](#ch12table01tn1)]) = 0.311984 | MAE = 0.311983
    | MAE = 0.312780 |'
- en: '| MNIST: convnet | Accuracy = 0.9952 | Accuracy = 0.9952 | Accuracy = 0.9952
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| MNIST：卷积神经网络 | 准确率 = 0.9952 | 准确率 = 0.9952 | 准确率 = 0.9952 |'
- en: '| Fashion-MNIST: convnet | Accuracy = 0.922 | Accuracy = 0.922 | Accuracy =
    0.9211 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Fashion-MNIST：卷积神经网络 | 准确率 = 0.922 | 准确率 = 0.922 | 准确率 = 0.9211 |'
- en: '| ImageNet subset of 1,000: MobileNetV2 | Top-1 accuracy = 0.618 Top-5 accuracy
    = 0.788 | Top-1 accuracy = 0.624 Top-5 accuracy = 0.789 | Top-1 accuracy = 0.280
    Top-5 accuracy = 0.490 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet 1000子集：MobileNetV2 | Top-1准确率 = 0.618 Top-5准确率 = 0.788 | Top-1准确率
    = 0.624 Top-5准确率 = 0.789 | Top-1准确率 = 0.280 Top-5准确率 = 0.490 |'
- en: ^a
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The MAE loss function is used on the California-housing model. Lower is better
    for MAE, unlike accuracy.
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 加利福尼亚房屋模型使用MAE损失函数。对于MAE而言，较低的值更好，与准确率不同。
- en: 'The second scenario in the quantization example is based on the familiar MNIST
    dataset and deep convnet architecture. Similar to the housing experiment, you
    can train the original model and perform evaluation on quantized versions of it
    by using the following commands:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 量化示例中的第二个场景基于熟悉的MNIST数据集和深度卷积网络架构。与住房实验类似，您可以使用以下命令训练原始模型并对其进行量化版本的评估：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As the second row of [table 12.1](#ch12table01) shows, neither the 16-bit nor
    8-bit quantization leads to any observable change in the model’s test accuracy.
    This reflects the fact that the convnet is a multiclass classifier, so small deviations
    in its layer output values may not alter the final classification result, which
    is obtained with an `argMax()` operation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[table 12.1](#ch12table01)的第二行所示，16位和8位量化都不会导致模型的测试准确性发生可观的变化。这反映了卷积神经网络是一个多类分类器的事实，因此其层输出值的微小偏差可能不会改变最终的分类结果，该结果是通过`argMax()`操作获得的。
- en: Is this finding representative of image-oriented multiclass classifiers? Keep
    in mind that MNIST is a relatively easy classification problem. Even a simple
    convnet like the one used in this example achieves near-perfect accuracy. How
    does quantization affect accuracies when we are faced with a harder image-classification
    problem? To answer this question, look at the two other scenarios in the quantization
    example.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现是否代表了面向图像的多类分类器？请记住，MNIST 是一个相对容易的分类问题。即使是像本例中使用的简单卷积网络也能达到几乎完美的准确率。当我们面对更难的图像分类问题时，量化如何影响准确率？要回答这个问题，请看量化示例中的另外两个场景。
- en: 'Fashion-MNIST, which you encountered in the section on variational autoencoders
    in [chapter 10](kindle_split_022.html#ch10), is a harder problem that MNIST. By
    using the following commands, you can train a model on the Fashion-MNIST dataset
    and examine how 16- and 8-bit quantization affects its test accuracy:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion-MNIST，你在 [第 10 章](kindle_split_022.html#ch10) 中的变分自动编码器部分遇到的问题，是一个比
    MNIST 更难的问题。通过使用以下命令，你可以在 Fashion-MNIST 数据集上训练一个模型，并检查 16 位和 8 位量化如何影响其测试准确率：
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The result, which is shown in the third row of [table 12.1](#ch12table01), indicates
    that there is a small decrease in the test accuracy (from 92.2% to 92.1%) caused
    by 8-bit quantization of the weights, although 16-bit quantization still leads
    to no observable change.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在 [表 12.1](#ch12table01) 的第三行，表明由于权重的 8 位量化而导致测试准确率略微下降（从 92.2% 下降到 92.1%），尽管
    16 位量化仍然没有观察到变化。
- en: An even harder image-classification problem is the ImageNet classification problem,
    which involves 1,000 output classes. In this case, we download a pretrained MobileNetV2
    instead of training one from scratch, like we do in the other three scenarios
    in this example. The pretrained model is evaluated on a sample of 1,000 images
    from the ImageNet dataset, in its nonquantized and quantized forms. We opted not
    to evaluate the entire ImageNet dataset because the dataset itself is huge (with
    millions of images), and the conclusion we’d draw from that wouldn’t be much different.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 更难的图像分类问题是 ImageNet 分类问题，涉及 1,000 个输出类别。在这种情况下，我们下载了一个预先训练的 MobileNetV2，而不是像在本例的其他三个场景中那样从头开始训练一个模型。预训练模型在
    ImageNet 数据集的 1,000 张图像样本上以其非量化和量化形式进行评估。我们选择不评估整个 ImageNet 数据集，因为数据集本身非常庞大（有数百万张图像），并且我们从中得出的结论不会有太大不同。
- en: To evaluate the model’s accuracy on the ImageNet problem in a more comprehensive
    fashion, we calculate both the top-1 and top-5 accuracies. Top-1 accuracy is the
    ratio of correct predictions when only the highest single logit output of the
    model is considered, while top-5 accuracy counts a prediction as right if any
    of the highest five logits includes the correct label. This is a standard approach
    in evaluating model accuracies on ImageNet because—due to the large number of
    class labels, some of which are very close to each other—models often show the
    correct label not in the top logit, but in one of the top-5 logits. To see the
    MobileNetV2 + ImageNet experiment in action, use
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要更全面地评估模型在 ImageNet 问题上的准确性，我们计算了 top-1 和 top-5 的准确率。Top-1 准确率是仅考虑模型最高单个逻辑输出时的正确预测比率，而
    top-5 准确率则是在最高的五个逻辑中有任何一个包含正确标签时将预测视为正确。这是评估 ImageNet 模型准确性的标准方法，因为由于大量类标签，其中一些非常接近，模型通常不会在
    top 逻辑中显示正确标签，而是在 top-5 逻辑中之一。要查看 MobileNetV2 + ImageNet 实验的结果，请使用
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Unlike the previous three scenarios, this experiment shows a substantial impact
    of 8-bit on the test accuracy (see the fourth row of [table 12.1](#ch12table01)).
    Both the top-1 and top-5 accuracies of the 8-bit quantized MobileNet are way below
    the original model, making 8-bit quantization an unacceptable size-optimization
    option for MobileNet. However, 16-bit quantized MobileNet still shows accuracies
    comparable to the nonquantized model.^([[9](#ch12fn9)]) We can see that the effect
    of quantization on accuracy depends on the model and the data. For some models
    and tasks (such as our MNIST convnet), neither 16-bit nor 8-bit quantization leads
    to any observable reduction in test accuracy. In these cases, we should by all
    means use the 8-bit quantized model during deployment to enjoy the reduced download
    time. For some models, such as our Fashion-MNIST convnet and our housing-price
    regression model, 16-bit quantization leads to no observed deterioration in accuracy,
    but 8-bit quantization does lead to a slight worsening of accuracy. In such cases,
    use your judgment as to whether the additional 25% reduction in model size outweighs
    the decrease in accuracy. Finally, for some types of models and tasks (such as
    our MobileNetV2 classification of ImageNet images), 8-bit quantization causes
    a large decrease in accuracy, which is probably unacceptable in most cases. For
    such problems, you need to stick with the original model or the 16-bit quantized
    version of it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于前面的三种情况，这个实验显示了 8 位对测试准确率的重大影响（见[表 12.1](#ch12table01)的第四行）。8 位量化的 MobileNet
    的 top-1 和 top-5 准确率都远低于原始模型，使得 8 位量化成为 MobileNet 不可接受的尺寸优化选项。然而，16 位量化的 MobileNet
    仍然显示出与非量化模型相当的准确率[^9]。我们可以看到量化对准确率的影响取决于模型和数据。对于某些模型和任务（如我们的 MNIST convnet），16
    位和 8 位量化都不会导致测试准确率的任何可观察降低。在这些情况下，我们应该尽可能在部署时使用 8 位量化模型以减少下载时间。对于一些模型，如我们的 Fashion-MNIST
    convnet 和我们的房价回归模型，16 位量化不会导致准确率的任何观察到的恶化，但 8 位量化确实会导致准确率略微下降。在这种情况下，您应根据判断是否额外的
    25% 模型大小减小超过准确率减少。最后，对于某些类型的模型和任务（如我们的 MobileNetV2 对 ImageNet 图像的分类），8 位量化会导致准确率大幅下降，这在大多数情况下可能是不可接受的。对于这样的问题，您需要坚持使用原始模型或其
    16 位量化版本。
- en: ⁹
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, we can see small *increases* in accuracy, which are attributable to
    the random fluctuation on the relatively small test set that consists of only
    1,000 examples.
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实际上，我们可以看到准确率略微增加，这归因于由仅包含 1,000 个示例的相对较小的测试集上的随机波动。
- en: The cases in the quantization example are stock problems that may be somewhat
    simplistic. The problem you have at hand may be more complex and very different
    from those cases. The take-home message is that whether to quantize your model
    before deploying it and to what bit depth you should quantize it are empirical
    questions and can be answered only on a case-by-case basis. You need to try out
    the quantization and test the resulting models on real test data before making
    a decision. Exercise 1 at the end of this chapter lets you try your hand on the
    MNIST ACGAN we trained in [chapter 10](kindle_split_022.html#ch10) and decide
    whether 16-bit or 8-bit quantization is the right decision for such a generative
    model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 量化示例中的案例是可能有些简化的典型问题。您手头的问题可能更加复杂，与这些案例大不相同。重要的是，是否在部署之前对模型进行量化以及应该对其进行多少位深度的量化都是经验性问题，只能根据具体情况来回答。在做出决定之前，您需要尝试量化并在真实的测试数据上测试生成的模型。本章末尾的练习
    1 让您尝试使用我们在 [第 10 章](kindle_split_022.html#ch10) 中训练的 MNIST ACGAN 模型，并决定对于这样的生成模型是选择
    16 位还是 8 位量化是正确的决定。
- en: Weight quantization and gzip compression
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重量化和 gzip 压缩
- en: An additional benefit of 8-bit quantization that should be taken into account
    is the additional over-the-wire model-size reduction it provides under data-compression
    techniques such as gzip. gzip is widely used to deliver large files over the web.
    You should always enable gzip when serving TensorFlow.js model files over the
    web. The nonquantized float32 weights of a neural network are usually not very
    amenable to such compression due to the noise-like variation in the parameter
    values, which contains few repeating patterns. It is our observation that gzip
    typically can’t get more than 10–20% size reduction out of nonquantized weights
    for models. The same is true for models with 16-bit weight quantization. However,
    once a model’s weights undergo 8-bit quantization, there is often a considerable
    jump in the ratio of compression (up to 30–40% for small models and about 20–30%
    for larger ones; see [table 12.2](#ch12table02)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑到的8位量化的另一个好处是，在诸如gzip等数据压缩技术下提供的附加压缩模型大小的额外减少。gzip被广泛用于通过网络传输大文件。在通过网络提供TensorFlow.js模型文件时，应始终启用gzip。神经网络的非量化float32权重通常不太适合这种压缩，因为参数值中存在类似噪声的变化，其中包含很少的重复模式。我们观察到，对于模型的非量化权重，gzip通常不能获得超过10-20%的大小减小。对于具有16位权重量化的模型也是如此。然而，一旦模型的权重经过8位量化，通常压缩比例会有相当大的增加（对于小型模型可高达30-40%，对于较大的模型约为20-30%；见[table
    12.2](#ch12table02)）。
- en: Table 12.2\. The gzip compression ratios of model artifacts under different
    levels of quantization
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.2。不同量化级别下模型构件的gzip压缩比例
- en: '| Dataset and model | gzip compression ratio^([[a](#ch12table02tn01)]) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 数据集和模型 | gzip压缩比例^([[a](#ch12table02tn01)]) |'
- en: '| --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 32-bit full precision (no quantization) | 16-bit quantization | 8-bit quantization
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 32位全精度（无量化） | 16位量化 | 8位量化 |'
- en: '| --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| California-housing: MLP regressor | 1.121 | 1.161 | 1.388 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| California房屋：MLP回归器 | 1.121 | 1.161 | 1.388 |'
- en: '| MNIST: convnet | 1.082 | 1.037 | 1.184 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| MNIST：卷积网络 | 1.082 | 1.037 | 1.184 |'
- en: '| Fashion-MNIST: convnet | 1.078 | 1.048 | 1.229 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Fashion-MNIST：卷积网络 | 1.078 | 1.048 | 1.229 |'
- en: '| ImageNet subset of 1,000: MobileNetV2 | 1.085 | 1.063 | 1.271 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet 1000个子集：MobileNetV2 | 1.085 | 1.063 | 1.271 |'
- en: ^a
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: (total size of the model.json and weight file)/(size of gzipped tar ball)
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: （模型.json和权重文件的总大小）/（gzipped tar ball的大小）
- en: This is due to the small number of bins available under the drastically reduced
    precision (only 256), which causes many values (such as the ones around 0) to
    fall into the same bin, and hence leads to more repeating patterns in the weight’s
    binary representation. This is an additional reason to favor 8-bit quantization
    in cases where it doesn’t lead to unacceptable deterioration in test accuracy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于极大降低的精度（仅256）下可用的小箱数，导致许多值（例如0周围的值）落入相同的箱中，因此导致权重的二进制表示中出现更多的重复模式。这是在不会导致测试准确度不可接受的情况下更喜欢8位量化的另一个原因。
- en: In summary, with post-training weight quantization, we can substantially reduce
    the size of the TensorFlow.js models transferred over the wire and stored on disk,
    especially with help from data-compression techniques such as gzip. This benefit
    of improved compression ratios requires no code change on the part of the developer,
    as the browser performs the unzipping transparently for you when it downloads
    the model files. However, it doesn’t change the amount of computation involved
    in executing the model’s inference calls. Neither does it change the amount of
    CPU or GPU memory consumption for such calls. This is because the weights are
    dequantized after they are loaded (see [equation 12.2](#ch12equ02) in [info box
    12.1](#ch12sb01)). As regards the operations that are run and the data types and
    shapes of the tensors output by the operations, there is no difference between
    a nonquantized model and a quantized model. However, for model deployment, an
    equally important concern is how to make a model that runs as fast as possible,
    as well as make it consume as little memory as possible when it’s running, because
    that improves user experience and reduces power consumption. Are there ways to
    make an existing TensorFlow.js model run faster when deployed, without loss of
    prediction accuracy and on top of model-size optimization? Luckily, the answer
    is yes. In the next section, we will focus on inference-speed optimization techniques
    that TensorFlow.js provides.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，通过训练后的权重量化，我们可以大大减少通过网络传输和存储在磁盘上的TensorFlow.js模型的大小，尤其是在使用gzip等数据压缩技术的帮助下。这种改进的压缩比的好处不需要开发者进行代码更改，因为浏览器在下载模型文件时会自动进行解压缩。但是，这并不会改变执行模型推断调用所涉及的计算量。也不会改变这些调用的CPU或GPU内存消耗量。这是因为在加载权重后对它们进行去量化（参见[方程12.2](#ch12equ02)中的[信息框12.1](#ch12sb01)）。就运行的操作、张量的数据类型和形状以及操作输出的张量而言，非量化模型和量化模型之间没有区别。然而，对于模型部署，同样重要的问题是如何使模型在部署时以尽可能快的速度运行，并且使其在运行时消耗尽可能少的内存，因为这可以提高用户体验并减少功耗。在不丢失预测准确性和在模型大小优化之上，有没有办法使现有的TensorFlow.js模型运行得更快？幸运的是，答案是肯定的。在下一节中，我们将重点介绍TensorFlow.js提供的推断速度优化技术。
- en: 12.2.2\. Inference-speed optimization using GraphModel conversion
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.2\. 使用GraphModel转换进行推断速度优化
- en: This section is organized as follows. We will first present the steps involved
    in optimizing the inference speed of a TensorFlow.js model using the `GraphModel`
    conversion. We will then list detailed performance measurements that quantify
    the speed gain provided by this approach. Finally, we will explain how the `GraphModel`
    conversion approach works under the hood.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节的结构如下。我们将首先介绍使用`GraphModel`转换来优化 TensorFlow.js 模型的推断速度所涉及的步骤。然后，我们将列出详细的性能测量结果，量化了该方法所提供的速度增益。最后，我们将解释`GraphModel`转换方法在底层的工作原理。
- en: 'Suppose you have a TensorFlow.js model saved at the path my/layers-model; you
    can use the following command to convert it to a `tf.GraphModel`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个路径为my/layers-model的TensorFlow.js模型；您可以使用以下命令将其转换为`tf.GraphModel`：
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This command creates a model.json file under the output directory my/graph-model
    (the directory will be created if it doesn’t exist), along with a number of binary
    weight files. Superficially, this set of files may appear to be identical in format
    to the files in the input directory that contains the serialized `tf.LayersModel`.
    However, the output files encode a different kind of model called `tf.GraphModel`
    (the namesake of this optimization method). In order to load the converted model
    in the browser or Node.js, use the TensorFlow.js method `tf.loadGraphModel()`
    instead of the familiar `tf.loadLayersModel()`. Once the `tf.GraphModel` object
    is loaded, you can perform inference in exactly the same way as a `tf.LayersModel`
    by invoking the object’s `predict()` method. For example,
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将在输出目录my/graph-model下创建一个model.json文件（如果该目录不存在），以及若干二进制权重文件。表面上看，这一组文件在格式上可能与包含序列化`tf.LayersModel`的输入目录中的文件相同。然而，输出的文件编码了一种称为`tf.GraphModel`的不同类型的模型（这个优化方法的同名）。为了在浏览器或Node.js中加载转换后的模型，请使用
    TensorFlow.js 的`tf.loadGraphModel()`方法，而不是熟悉的`tf.loadLayersModel()`方法。加载`tf.GraphModel`对象后，您可以通过调用对象的`predict()`方法以完全相同的方式执行推断，就像对待`tf.LayersModel`一样。例如，
- en: '[PRE14]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1*** Or use an http:// or https:// URL if loading the model in the browser.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 如果在浏览器中加载模型，则可以使用 http:// 或 https:// URL。'
- en: '***2*** Perform inference using input data ''xs''.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用输入数据''xs''进行推断。'
- en: 'The enhanced inference speed comes with two limitations:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 提高的推理速度带来了两个限制：
- en: At the time of this writing, the latest version of TensorFlow.js (1.1.2) does
    not support recurrent layers such as `tf.layers.simpleRNN()`, `tf.layers.gru()`,
    and `tf.layers.lstm()` (see [chapter 9](kindle_split_021.html#ch09)) for `GraphModel`
    conversion.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在撰写本文时，最新版本的TensorFlow.js（1.1.2）不支持循环层，如`tf.layers.simpleRNN()`、`tf.layers.gru()`和`tf.layers.lstm()`（见[第9章](kindle_split_021.html#ch09)）用于`GraphModel`转换。
- en: The loaded `tf.GraphModel` object doesn’t have a `fit()` method and hence does
    not support further training (for example, transfer learning).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 载入的`tf.GraphModel`对象没有`fit()`方法，因此不支持进一步的训练（例如，迁移学习）。
- en: '[Table 12.3](#ch12table03) compares the inference speed of the two types of
    models with and without `GraphModel` conversion. Since `GraphModel` conversion
    does not support recurrent layers yet, only the results from an MLP and a convnet
    (MobileNetV2) are presented. To cover different deployment environments, the table
    presents results from both the web browser and tfjs-node running in the backend
    environment. From this table, we can see that `GraphModel` conversion invariably
    speeds up inference. However, the ratio of the speedup depends on model type and
    deployment environment. For the browser (WebGL) deployment environment, `GraphModel`
    conversion leads to a 20–30% speedup, while the speedup is more dramatic (70–90%)
    if the deployment environment is Node.js. Next, we will discuss why `GraphModel`
    conversion speeds up inference, as well as the reason why it speeds up the inference
    more for Node.js than for the browser environment.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[表12.3](#ch12table03)比较了两种模型类型在有和没有`GraphModel`转换时的推理速度。由于`GraphModel`转换尚不支持循环层，因此仅呈现了MLP和卷积神经网络（MobileNetV2）的结果。为了覆盖不同的部署环境，该表呈现了来自Web浏览器和后端环境中运行的tfjs-node的结果。从这个表中，我们可以看到`GraphModel`转换始终加快了推理速度。但是，加速比取决于模型类型和部署环境。对于浏览器（WebGL）部署环境，`GraphModel`转换会带来20-30%的加速，而如果部署环境是Node.js，则加速效果更加显著（70-90%）。接下来，我们将讨论为什么`GraphModel`转换加快了推理速度，以及它为什么在Node.js环境中比在浏览器环境中加速更多的原因。'
- en: Table 12.3\. Comparing the inference speed of two model types (an MLP and MobileNetV2)
    with and without `GraphModel` conversion optimization, and in different deployment
    environments^([[a](#ch12table03tn01)])
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.3 比较了两种模型类型（MLP和MobileNetV2）在不同部署环境下进行`GraphModel`转换优化和不进行优化时的推理速度^([[a](#ch12table03tn01)])
- en: ^a
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The code with which these results were obtained is available at [https://github.com/tensorflow/tfjs/tree/master/tfjs/integration_tests/](https://github.com/tensorflow/tfjs/tree/master/tfjs/integration_tests/).
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 获得这些结果的代码可在[https://github.com/tensorflow/tfjs/tree/master/tfjs/integration_tests/](https://github.com/tensorflow/tfjs/tree/master/tfjs/integration_tests/)找到。
- en: '| Model name and topology | predict() time (ms; lower is better) (Average over
    30 predict() calls preceded by 20 warm-up calls) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称和拓扑结构 | 预测时间（毫秒；值越低越好）（在20次热身调用之后的30次预测调用的平均值） |'
- en: '| --- | --- |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Browser WebGL | tfjs-node (CPU only) | tfjs-node-gpu |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器WebGL | tfjs-node（仅CPU） | tfjs-node-gpu |'
- en: '| --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LayersModel | GraphModel | LayersModel | GraphModel | LayersModel | GraphModel
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| LayersModel | GraphModel | LayersModel | GraphModel | LayersModel | GraphModel
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MLP^([[b](#ch12table03tn02)]) | 13 | 10 (1.3x) | 18 | 10 (1.8x) | 3 | 1.6
    (1.9x) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| MLP^([[b](#ch12table03tn02)]) | 13 | 10 (1.3x) | 18 | 10 (1.8x) | 3 | 1.6
    (1.9x) |'
- en: '| MobileNetV2 (width = 1.0) | 68 | 57 (1.2x) | 187 | 111 (1.7x) | 66 | 39 (1.7x)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| MobileNetV2（宽度=1.0） | 68 | 57 (1.2x) | 187 | 111 (1.7x) | 66 | 39 (1.7x)
    |'
- en: ^b
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^b
- en: ''
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The MLP consists of dense layers with unit counts: 4,000, 1,000, 5,000, and
    1\. The first three layers have relu activation; the last has linear activation.'
  id: totrans-191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MLP由单元数为4,000、1,000、5,000和1的密集层组成。前三层使用relu激活函数；最后一层使用线性激活函数。
- en: How GraphModel conversion speeds up model inference
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GraphModel转换如何加速模型推理
- en: How does `GraphModel` conversion boost TensorFlow.js models’ inference speed?
    It’s achieved by leveraging TensorFlow (Python)’s ahead-of-time analysis of the
    model’s computation graph at a fine granularity. The computation-graph analysis
    is followed by modifications to the graph that reduce the amount of computation
    while preserving the numeric correctness of the graph’s output result. Don’t be
    intimidated by terms such as *ahead-of-time analysis* and *fine granularity*.
    We will explain them in a bit.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`GraphModel` 转换是如何提高 TensorFlow.js 模型推断速度的？这是通过利用 TensorFlow（Python）对模型计算图进行细粒度的**提前分析**来实现的。计算图分析后，会对图进行修改，减少计算量同时保持图的输出结果的数值正确性。不要被**提前分析**和**细粒度**等术语吓到。稍后我们会对它们进行解释。'
- en: 'To give a concrete example of the sort of graph modification we are talking
    about, let’s consider how a BatchNormalization layer works in a `tf.LayersModel`
    and a `tf.GraphModel`. Recall that BatchNormalization is a type of layer that
    improves convergence and reduces overfitting during training. It is available
    in the TensorFlow.js API as `tf.layers.batchNormalization()` and is used by popular
    pretrained models such as MobileNetV2\. When a BatchNormalization layer runs as
    a part of a `tf.LayersModel`, the computation follows the mathematical definition
    of batch normalization closely:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给出我们所说的图修改的具体例子，让我们考虑一下在 `tf.LayersModel` 和 `tf.GraphModel` 中 BatchNormalization
    层的工作原理。回想一下，BatchNormalization 是一种在训练过程中改善收敛性和减少过拟合的类型的层。它在 TensorFlow.js API
    中可用作 `tf.layers.batchNormalization()`，并且被诸如 MobileNetV2 这样的常用预训练模型使用。当 BatchNormalization
    层作为 `tf.LayersModel` 的一部分运行时，计算会严格遵循批量归一化的数学定义：
- en: equation 12.3\.
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 12.3。
- en: '![](12fig02_alt.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig02_alt.jpg)'
- en: Six operations (or ops) are needed in order to generate the output from the
    input (`x`), in the rough order of
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从输入 (`x`) 生成输出，需要六个操作（或 ops），大致顺序如下：
- en: '`sqrt`, with `var` as input'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sqrt`，将 `var` 作为输入'
- en: '`add`, with `epsilon` and the result of step 1 as inputs'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`add`，将 `epsilon` 和步骤 1 的结果作为输入'
- en: '`sub`, with `x` and means as inputs'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sub`，将 `x` 和平均值作为输入'
- en: '`div`, with the results of steps 2 and 3 as inputs'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`div`，将步骤 2 和 3 的结果作为输入'
- en: '`mul`, with `gamma` and the result of step 4 as inputs'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mul`，将 `gamma` 和步骤 4 的结果作为输入'
- en: '`add`, with `beta` and the result of step 5 as inputs'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`add`，将 `beta` 和步骤 5 的结果作为输入'
- en: 'Based on simple arithmetic rules, it can be seen that [equation 12.3](#ch12equ03)
    can be simplified significantly, as long as the values of `mean`, `var`, `epsilon`,
    `gamma,` and `beta` are constant (do not change with the input or with how many
    times the layer has been invoked). After a model comprising a BatchNormalization
    layer is trained, all these variables indeed become constant. This is exactly
    what `GraphModel` conversion does: it “folds” the constants and simplifies the
    arithmetic, which leads to the following mathematically equivalent equation:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 基于简单的算术规则，可以看出[方程式 12.3](#ch12equ03)可以被显著简化，只要 `mean`、`var`、`epsilon`、`gamma`
    和 `beta` 的值是常量（不随输入或层被调用的次数而变化）。在训练包含 BatchNormalization 层的模型后，所有这些变量确实都变成了常量。这正是
    `GraphModel` 转换所做的：它“折叠”常量并简化算术，从而导致以下在数学上等效的方程式：
- en: equation 12.4\.
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 12.4。
- en: '![](12eqa03.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](12eqa03.jpg)'
- en: 'The values of *k* and *b* are calculated during `GraphModel` conversion, not
    during inference:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 和 *b* 的值是在 `GraphModel` 转换期间计算的，而不是在推断期间：'
- en: equation 12.5\.
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 12.5。
- en: '![](12eqa04.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](12eqa04.jpg)'
- en: equation 12.6\.
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 12.6。
- en: '![](12eqa05_alt.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](12eqa05_alt.jpg)'
- en: Therefore, [equations 12.5](#ch12equ05) and [12.6](#ch12equ06) do *not* factor
    into the amount of computation during inference; only [equation 12.4](#ch12equ04)
    does. Contrasting [equations 12.3](#ch12equ03) and [12.4](#ch12equ04), you can
    see that the constant folding and arithmetic simplification cut the number of
    operations from six to two (a `mul` op between *x* and *k* and an `add` op between
    *b* and the result of that `mul` operation), which leads to considerable speedup
    of this layer’s execution. But why does `tf.LayersModel` not perform this optimization?
    It’s because it needs to support training of the BatchNormalization layer, during
    which the values of `mean`, `var`, `gamma`, and `beta` are updated at every step
    of the training. `GraphModel` conversion takes advantage of the fact that these
    updated values are no longer required once the model training is complete.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，[方程式12.5](#ch12equ05)和[12.6](#ch12equ06)在推断过程中*不*计入计算量；只有[方程式12.4](#ch12equ04)计入。将[方程式12.3](#ch12equ03)和[12.4](#ch12equ04)进行对比，您会发现常数折叠和算术简化将操作数量从六个减少到了两个（*x*和*k*之间的`mul`操作，以及*b*和该`mul`操作结果之间的`add`操作），从而极大加速了该层的执行速度。但为什么`tf.LayersModel`不执行此优化？因为它需要支持BatchNormalization层的训练，在训练的每一步都会更新`mean`、`var`、`gamma`和`beta`的值。`GraphModel`转换利用了这一事实，即这些更新的值在模型训练完成后不再需要。
- en: The type of optimization seen in the BatchNormalization example is only possible
    if two requirements are met. First, the computation must be represented at a sufficiently
    *fine granularity*—that is, at the level of basic mathematical operations such
    as `add` and `mul`, instead of the coarser, layer-by-layer granularity at which
    the Layers API of TensorFlow.js resides. Second, all the computation is known
    ahead of time, before the calls to the model’s `predict()` method are executed.
    `GraphModel` conversion goes through TensorFlow (Python), which has access to
    a graph representation of the model that meets both criteria.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在BatchNormalization示例中看到的优化类型仅在满足两个要求时才可能实现。首先，计算必须以足够*细粒度*的方式表示——即在基本数学操作（如`add`和`mul`）的层面上，而不是TensorFlow.js的Layers
    API所在的更粗粒度的层面。其次，所有的计算在执行模型的`predict()`方法之前都是已知的。`GraphModel`转换经过了TensorFlow（Python），可以得到满足这两个条件的模型的图表示。
- en: 'Apart from the constant-folding and arithmetic optimization discussed previously,
    `GraphModel` conversion is capable of performing another type of optimization
    called *op fusion*. Take the frequently used dense layer type (`tf.layers.dense()`),
    for example. A dense layer involves three operations: a matrix multiplication
    (`matMul`) between the input *x* and the kernel *W*, a broadcasting addition between
    the result of the `matMul` and the bias (*b*), and the element-wise relu activation
    function ([figure 12.3](#ch12fig03), panel A). The op fusion optimization replaces
    the three separate operations with a single operation that carries out all the
    equivalent steps ([figure 12.3](#ch12fig03), panel B). This replacement may seem
    trivial, but it leads to faster computation due to 1) the reduced overhead of
    launching ops (yes, launching an op always involves a certain amount of overhead,
    regardless of the compute backend), and 2) more opportunity to perform smart tricks
    for speed optimization within the implementation of the fused op itself.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前讨论的常数折叠和算术优化外，`GraphModel`的转换还能执行另一种称为*op fusion*的优化类型。以经常使用的密集层类型(`tf.layers.dense()`)为例。密集层涉及三种操作：输入*x*和内核*W*的矩阵乘法(`matMul`)，`matMul`结果和偏置(*b*)之间的广播加法，以及逐元素的relu激活函数([图12.3](#ch12fig03)，面板A)。op
    fusion优化使用一种单一操作替换了这三个分开的操作，该单一操作执行了所有等效步骤([图12.3](#ch12fig03)，面板B)。这种替换可能看起来微不足道，但由于1）启动op的开销减少（是的，启动op总是涉及一定的开销，无论计算后端如何），以及2）在融合的op实现中执行速度优化的更多机会，这导致了更快的计算。
- en: Figure 12.3\. Schematic illustration of the internal operations in a dense layer,
    with (panel A) and without (panel B) op fusion
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.3\. 密集层内部操作的示意图，带有（面板A）和不带有（面板B）op fusion。
- en: '![](12fig03_alt.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig03_alt.jpg)'
- en: How is op fusion optimization different from the constant folding and arithmetic
    simplification we just saw? Op fusion requires that the special fused op (`Fused`
    `matMul+relu`, in this case) be defined and available for the compute backend
    being used, while constant folding doesn’t. These special fused ops may be available
    only for certain compute backends and deployment environments. This is the reason
    why we saw a greater amount of inference speedup in the Node.js environment than
    in the browser (see [table 12.3](#ch12table03)). The Node.js compute backend,
    which uses libtensorflow written in C++ and CUDA, is equipped with a richer set
    of ops than TensorFlow.js’s WebGL backend in the browser.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 操作融合优化与我们刚刚看到的常量折叠和算术简化有何不同？操作融合要求特殊融合操作（在本例中为 `Fused matMul+relu`）在所使用的计算后端中定义并可用，而常量折叠则不需要。这些特殊融合操作可能仅对某些计算后端和部署环境可用。这就是为什么我们在
    Node.js 环境中看到了比在浏览器中更大量的推理加速的原因（参见 [table 12.3](#ch12table03)）。Node.js 计算后端使用的是用
    C++ 和 CUDA 编写的 libtensorflow，它配备了比浏览器中的 TensorFlow.js WebGL 后端更丰富的操作集。
- en: Apart from constant folding, arithmetic simplification, and op fusion, TensorFlow
    (Python)’s graph-optimization system Grappler is capable of a number of other
    kinds of optimizations, some of which may be relevant to how TensorFlow.js models
    are optimized through `GraphModel` conversion. However, we won’t cover those due
    to space limits. If you are interested in finding out more about this topic, you
    can read the informative slides by Rasmus Larsen and Tatiana Shpeisman listed
    at the end of this chapter.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 除了常量折叠、算术简化和操作融合之外，TensorFlow（Python）的图优化系统 Grappler 还能够进行其他许多种类的优化，其中一些可能与如何通过
    `GraphModel` 转换优化 TensorFlow.js 模型相关。然而，由于空间限制，我们不会涵盖这些内容。如果你对此主题想要了解更多，你可以阅读本章末尾列出的
    Rasmus Larsen 和 Tatiana Shpeisman 的信息性幻灯片。
- en: In summary, `GraphModel` conversion is a technique provided by `tensorflowjs_
    converter`. It utilizes TensorFlow (Python)’s ahead-of-time graph-optimization
    capability to simplify computation graphs and reduce the amount of computation
    required for model inference. Although the detailed amount of inference speedup
    varies with model type and compute backend, it usually provides a speedup ratio
    of 20% or more, and hence is an advisable step to perform on your TensorFlow.js
    models before their deployment.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`GraphModel` 转换是由 `tensorflowjs_ converter` 提供的一种技术。它利用 TensorFlow（Python）的提前图优化能力简化计算图，并减少模型推理所需的计算量。尽管推理加速的详细量取决于模型类型和计算后端，但通常它提供了
    20% 或更多的加速比，因此在部署 TensorFlow.js 模型之前执行此步骤是明智的。
- en: '|  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**How to properly measure a TensorFlow.js model’s inference time**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何正确测量 TensorFlow.js 模型的推理时间**'
- en: 'Both `tf.LayersModel` and `tf.GraphModel` provide the unified `predict()` method
    to support inference. This method takes one or more tensors as input and returns
    one or more tensors as the inference result. However, it is important to note
    that in the context of WebGL-based inference in the web browser, the `predict()`
    method only *schedules* operations to be executed on the GPU; it does not await
    the completion of their execution. As a result, if you naively time a `predict()`
    call in the following fashion, the result of the timing measurement will be wrong:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.LayersModel` 和 `tf.GraphModel` 都提供了统一的 `predict()` 方法来支持推理。该方法接受一个或多个张量作为输入，并返回一个或多个张量作为推理结果。然而，在基于
    WebGL 的浏览器推理环境中，重要的是要注意，`predict()` 方法仅*安排*在 GPU 上执行的操作；它不等待它们执行完成。因此，如果你天真地按照以下方式计时
    `predict()` 调用，计时测量结果将是错误的：'
- en: '[PRE15]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1*** Incorrect way of measuring inference time!'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 测量推理时间的不正确方式！'
- en: 'When `predict()` returns, the scheduled operations may not have finished executing.
    Therefore, the prior example will lead to a time measurement shorter than the
    actual time it takes to complete the inference. To ensure that the operations
    are completed before `console.timeEnd()` is called, you need to call one of the
    following methods of the returned tensor object: `array()` or `data()`. Both methods
    download the texture values that hold the elements of the output tensor from GPU
    to CPU. In order to do so, they must wait for the output tensor’s computation
    to finish. So, the correct way to measure the timing looks like the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当`predict()`返回时，预定的操作可能尚未执行完毕。因此，前面的示例将导致比完成推理所需实际时间更短的时间测量。为了确保在调用`console.timeEnd()`之前操作已完成，需要调用返回的张量对象的以下方法之一：`array()`或`data()`。这两种方法都会将保存输出张量元素的纹理值从
    GPU 下载到 CPU。为了实现这一点，它们必须等待输出张量的计算完成。因此，正确的计时方法如下所示：
- en: '[PRE16]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1*** The array() call won’t return until the scheduled computation of outputTensor
    has completed, hence ensuring the correctness of the inference-time measurement.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** `array()`调用直到输出张量的计算完成才会返回，从而确保推理时间测量的正确性。'
- en: Another important thing to bear in mind is that like all other JavaScript programs,
    the execution time of a TensorFlow.js model’s inference is variable. In order
    to obtain a reliable estimate of the inference time, the code in the previous
    snippet should be put in a `for` loop so that the measurement can be performed
    multiple times (for example, 50 times), and the average time can be calculated
    based on the accumulated individual measurements. The first few executions are
    usually slower than the subsequent ones due to the need to compile new WebGL shader
    programs and set up initial states. So, performance-measuring code often omits
    the first few (such as the first five) runs, which are referred to as *burn-in*
    or *warm-up* runs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要记住的重要事情是，与所有其他 JavaScript 程序一样，TensorFlow.js 模型推理的执行时间是变化的。为了获得推理时间的可靠估计，应该将上面代码段放在一个`for`循环中，以便可以多次执行（例如，50次），并且可以根据累积的单个测量计算出平均时间。最初的几次执行通常比随后的执行慢，因为需要编译新的
    WebGL 着色程序并设置初始状态。因此，性能测量代码通常会忽略前几次运行（例如，前五次），这些被称为*热身*或*预热*运行。
- en: If you are interested in a deeper understanding of these performance-benchmarking
    techniques, work through exercise 3 at the end of this chapter.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些性能基准技术有更深入的了解感兴趣，可以通过本章末尾的练习3来进行实践。
- en: '|  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 12.3\. Deploying TensorFlow.js models on various platforms and environments
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3\. 在各种平台和环境上部署 TensorFlow.js 模型
- en: You’ve optimized your model, it’s fast and lightweight, and all your tests are
    green. You’re good to go! Hooray! But before you pop that champagne, there’s a
    bit more work to do.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经优化了你的模型，它又快又轻，而且所有的测试都通过了。你准备好了! 好消息! 但在你开香槟庆祝之前，还有更多的工作要做。
- en: It’s time to put your model into your application and get it out in front of
    your user base. In this section, we will cover a few deployment platforms. Deploying
    to the web and deploying to a Node.js service are well-known paths, but we’ll
    also cover a few more exotic deployment scenarios, like deploying to a browser
    extension or a single-board embedded hardware application. We will point to simple
    examples and discuss special considerations important for the platforms.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将你的模型部署到应用程序中，并让它出现在用户基础之前了。在本节中，我们将涵盖一些部署平台。部署到网络和部署到 Node.js 服务是众所周知的途径，但我们还将涵盖一些更奇特的部署情景，比如部署到浏览器扩展程序或单板嵌入式硬件应用。我们将指向简单的例子，并讨论平台重要的特殊注意事项。
- en: 12.3.1\. Additional considerations when deploying to the web
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.1\. 部署到网络时的额外考虑事项
- en: 'Let’s begin by revisiting the most common deployment scenario for TensorFlow.js
    models, deploying to the web as part of a web page. In this scenario, our trained,
    and possibly optimized, model is loaded via JavaScript from some hosting location,
    and then the model makes predictions using the JavaScript engine within the user’s
    browser. A good example of this pattern is the MobileNet image-classification
    example from [chapter 5](kindle_split_016.html#ch05). The example is also available
    to download from tfjs-examples/ mobilenet. As a reminder, the relevant code for
    loading a model and making a prediction can be summarized as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先重新审视 TensorFlow.js 模型最常见的部署场景：将其部署到网页中。在这种场景下，我们经过训练且可能经过优化的模型通过 JavaScript
    从某个托管位置加载，然后模型利用用户浏览器内的 JavaScript 引擎进行预测。一个很好的例子是 [第五章](kindle_split_016.html#ch05)
    中的 MobileNet 图像分类示例。该示例也可从 tfjs-examples/ mobilenet 进行下载。作为提醒，以下是加载模型并进行预测的相关代码概述：
- en: '[PRE17]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This model is hosted from a Google Cloud Platform (GCP) bucket. For low-traffic,
    static applications like this one, it is easy to host the model statically alongside
    the rest of the site content. Larger, higher-traffic applications may choose to
    host the model through a content delivery network (CDN) alongside the other heavy
    assets. One common development mistake is to forget to account for Cross-Origin
    Resource Sharing (CORS) when setting up a bucket in GCP, Amazon S3, or other cloud
    services. If CORS is set incorrectly, the model will fail to load, and you should
    get a CORS-related error message delivered to the console. This is something to
    watch out for if your web application works fine locally but fails when pushed
    to your distribution platform.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是从 Google 云平台（GCP）存储桶中托管的。对于像这样的低流量、静态应用程序，很容易将模型静态托管在站点内容的其他部分旁边。对于更大、更高流量的应用程序，可以选择通过内容交付网络（CDN）将模型与其他重型资产一起托管。一个常见的开发错误是在设置
    GCP、Amazon S3 或其他云服务中的存储桶时忘记考虑跨域资源共享（CORS）。如果 CORS 设置不正确，模型加载将失败，并且您应该会在控制台上收到与
    CORS 相关的错误消息。如果您的 Web 应用在本地正常工作，但在发布到分发平台后失败，请注意这一点。
- en: After the user’s browser loads the HTML and JavaScript, the JavaScript interpreter
    will issue the call to load our model. The process of loading a small model takes
    a few hundred milliseconds on a modern browser with a good internet connection,
    but after the initial load, the model can be loaded much faster from the browser
    cache. The serialization format ensures that the model is sharded into small enough
    pieces to support the standard browser cache limit.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户的浏览器加载 HTML 和 JavaScript 后，JavaScript 解释器将发出加载模型的调用。在具备良好的网络连接的现代浏览器中，加载一个小型模型的过程大约需要几百毫秒，但在初始加载后，可以从浏览器缓存中更快地加载模型。序列化格式确保将模型切分为足够小的部分以支持标准浏览器缓存限制。
- en: One nice property of web deployment is that prediction happens directly within
    the browser. Any data passed to the model is never sent over the wire, which is
    good for latency and great for privacy. Imagine a text-input prediction scenario
    where the model is predicting the next word for assistive typing, something that
    we see all the time in, for example, Gmail. If we need to send the typed text
    to servers in the cloud and wait for a response from those remote servers, then
    prediction will be delayed, and the input predictions will be much less useful.
    Furthermore, some users might consider sending their incomplete keystrokes to
    a remote computer an invasion of their privacy. Making predictions locally in
    their own browser is much more secure and privacy sensitive.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Web 部署的一个好处是预测直接在浏览器中进行。传递给模型的任何数据都不会通过网络传输，这对于延迟很有好处，且对于隐私保护也非常重要。想象一下，如果模型正在预测辅助输入法的下一个单词，这在我们常见的场景中经常出现，比如
    Gmail。如果需要将输入的文本发送到云端服务器，并等待远程服务器的响应，那么预测将会被延迟，输入的预测结果将会变得不太有用。此外，一些用户可能会认为将其不完整的按键输入发送到远程计算机中侵犯了他们的隐私。在用户自己的浏览器中进行本地预测更加安全和注重隐私。
- en: A downside of making predictions within the browser is model security. Sending
    the model to the user makes it easy for the user to keep the model and use it
    for other purposes. TensorFlow.js currently (as of 2019) does not have a solution
    for model security in the browser. Some other deployment scenarios make it harder
    for the user to use the model for purposes the developer didn’t intend. The distribution
    path with the greatest model security is to keep the model on servers you control
    and serve prediction requests from there. Of course, this comes at the cost of
    latency and data privacy. Balancing these concerns is a product decision.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中进行预测的缺点是模型安全性。将模型发送给用户使其容易保留该模型并将其用于其他目的。TensorFlow.js目前（截至2019年）在浏览器中没有模型安全的解决方案。其他一些部署场景使用户更难以将模型用于开发者未预期的目的。最大模型安全性的分发路径是将模型保留在你控制的服务器上，并从那里提供预测请求。当然，这需要牺牲延迟和数据隐私。平衡这些问题是产品决策。
- en: 12.3.2\. Deployment to cloud serving
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.2\. 云服务部署
- en: Many existing production systems provide machine-learning-trained prediction
    as a service, such as Google Cloud Vision AI ([https://cloud.google.com/vision](https://cloud.google.com/vision))
    or Microsoft Cognitive Services ([https://azure.microsoft.com/en-us/services/cognitive-services](https://azure.microsoft.com/en-us/services/cognitive-services)).
    The end user of such a service makes HTTP requests containing the input values
    to the prediction, such as an image for an object-detection task, and the response
    encodes the output of the prediction, such as the labels and positions of objects
    in the image.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有的生产系统提供机器学习训练预测服务，例如Google Cloud Vision AI([https://cloud.google.com/vision](https://cloud.google.com/vision))或Microsoft
    Cognitive Services([https://azure.microsoft.com/en-us/services/cognitive-services](https://azure.microsoft.com/en-us/services/cognitive-services))。这样的服务的最终用户会进行包含预测输入值的HTTP请求，例如用于对象检测任务的图像，响应会对预测的输出进行编码，例如图像中对象的标签和位置。
- en: As of 2019, there are two routes to serving a TensorFlow.js model from a server.
    The first route has the server running Node.js and performing the prediction using
    the native JavaScript runtime. Because TensorFlow.js is so new, we are not aware
    of production use cases that have chosen this approach, but proofs of concept
    are simple to build.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2019年，有两种方法可以从服务器上服务于TensorFlow.js模型。第一种方法是运行Node.js的服务器，使用原生JavaScript运行时进行预测。由于TensorFlow.js很新，我们不知道有哪些生产用例选择了这种方法，但概念证明很容易构建。
- en: 'The second route is to convert the model from TensorFlow.js into a format that
    can be served from a known existing server technology, such as the standard TensorFlow
    Serving system. From the documentation at [www.tensorflow.org/tfx/guide/serving](http://www.tensorflow.org/tfx/guide/serving):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 第二条路线是将模型从TensorFlow.js转换为可以从已知的现有服务器技术（例如标准的TensorFlow Serving系统）服务的格式。从[www.tensorflow.org/tfx/guide/serving](http://www.tensorflow.org/tfx/guide/serving)的文档中可知：
- en: '*TensorFlow Serving is a flexible, high-performance serving system for machine-learning
    models, designed for production environments. TensorFlow Serving makes it easy
    to deploy new algorithms and experiments, while keeping the same server architecture
    and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow
    models, but can be easily extended to serve other types of models and data.*'
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*TensorFlow Serving是一个为生产环境设计的灵活高效的机器学习模型服务系统。TensorFlow Serving使得部署新的算法和实验变得容易，同时保持相同的服务器体系结构和API。TensorFlow
    Serving提供了与TensorFlow模型的开箱即用的集成，但也可以轻松扩展以用于其他类型的模型和数据。*'
- en: The TensorFlow.js models we have serialized so far have been stored in a JavaScript-specific
    format. TensorFlow Serving expects models to be packaged in the TensorFlow standard
    SavedModel format. Fortunately, the tfjs-converter project makes it easy to convert
    to the necessary format.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将TensorFlow.js模型序列化为JavaScript特定的格式。TensorFlow Serving期望模型使用TensorFlow标准的SavedModel格式打包。幸运的是，tfjs-converter项目使转换为所需格式变得容易。
- en: 'In [chapter 5](kindle_split_016.html#ch05) (transfer learning) we showed how
    SavedModels built with the Python implementation of TensorFlow could be used in
    TensorFlow.js. To do the reverse, first install the tensorflowjs pip package:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](kindle_split_016.html#ch05)（迁移学习）中，我们展示了如何在TensorFlow.js中使用Python实现的TensorFlow构建SavedModels。要做相反的操作，首先安装tensorflowjs
    pip包：
- en: '[PRE18]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, you must run the converter binary, specifying the input:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您必须运行转换器二进制文件，并指定输入：
- en: '[PRE19]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This will create a new saved-model directory, which will contain the required
    topology and weights in a format that TensorFlow Serving understands. You should
    then be able to follow the instructions for building the TensorFlow Serving server
    and make gRPC prediction requests against the running model. Managed solutions
    also exist. For instance, Google Cloud Machine Learning Engine provides a path
    for you to upload your saved model to Cloud Storage and then set up serving as
    a service, without needing to maintain the server or the machine. You can learn
    more from the documentation at [https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个新的 saved-model 目录，其中包含 TensorFlow Serving 可理解的所需拓扑和权重格式。然后，您应该能够按照构建 TensorFlow
    Serving 服务器的说明，并对运行中的模型进行 gRPC 预测请求。也存在托管解决方案。例如，Google Cloud 机器学习引擎提供了一条路径，您可以将保存的模型上传到
    Cloud Storage，然后设置为服务，而无需维护服务器或机器。您可以从文档中了解更多信息：[https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models)。
- en: The advantage of serving your model from the cloud is that you are in complete
    control of the model. It is easy to perform telemetry on what sorts of queries
    are being performed and to quickly detect problems. If it is discovered that there
    is some unforeseen problem with a model, it can be quickly removed or upgraded,
    and there is little risk of other copies on machines outside of your control.
    The downside is the additional latency and data privacy concerns, as mentioned.
    There is also the additional cost—both in monetary outlay and maintenance costs—in
    operating a cloud service, as you are in control of the system configuration.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从云端提供模型的优点是您完全控制模型。可以很容易地对执行的查询类型进行遥测，并快速检测出问题。如果发现模型存在一些意外问题，可以快速删除或升级，并且不太可能出现在您控制之外的机器上的其他副本。缺点是额外的延迟和数据隐私问题，如前所述。还有额外的成本——无论是货币支出还是维护成本——在操作云服务时，您控制着系统配置。
- en: 12.3.3\. Deploying to a browser extension, like Chrome Extension
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.3\. 部署到浏览器扩展，如 Chrome 扩展
- en: Some client-side applications may require your application to be able to work
    across many different websites. Browser extension frameworks are available for
    all the major desktop browsers, including Chrome, Safari, and FireFox, among others.
    These frameworks enable developers to create experiences that modify or enhance
    the browsing experience itself by adding new JavaScript and manipulating the DOM
    of websites.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一些客户端应用程序可能需要您的应用程序能够跨多个不同的网站工作。所有主要桌面浏览器都提供了浏览器扩展框架，包括 Chrome、Safari 和 FireFox
    等。这些框架使开发人员能够创建通过添加新的 JavaScript 和操作网站的 DOM 来修改或增强浏览体验的体验。
- en: Since the extension is operating on top of JavaScript and HTML within the browser’s
    execution engine, what you can do with TensorFlow.js in a browser extension is
    similar to what is possible in a standard web page deployment. The model security
    story and data privacy story are identical to the web page deployment. By performing
    prediction directly within the browser, the users’ data is relatively secure.
    The model security story is also similar to that of web deployment.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于扩展是在浏览器执行引擎内部的 JavaScript 和 HTML 之上运行的，您可以在浏览器扩展中使用 TensorFlow.js 的功能与在标准网页部署中相似。模型安全性和数据隐私性与网页部署相同。通过在浏览器内直接执行预测，用户的数据相对安全。模型安全性与网页部署的情况也类似。
- en: As an example of what is possible using a browser extension, see the chrome-extension
    example within tfjs-examples. This extension loads a MobileNetV2 model and applies
    it to images on the web, selected by the user. Installing and using the extension
    is a little different from the other examples we have seen, since it is an extension,
    not a hosted website. This example requires the Chrome browser.^([[10](#ch12fn10)])
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用浏览器扩展的可能性的示例，请参阅 tfjs-examples 中的 chrome-extension 示例。此扩展加载一个 MobileNetV2
    模型，并将其应用于用户在网络上选择的图像。安装和使用该扩展与我们看到的其他示例有点不同，因为它是一个扩展，而不是托管网站。这个示例需要 Chrome 浏览器。^([[10](#ch12fn10)])
- en: ^(10)
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Newer versions of Microsoft Edge also offer some support for cross-browser extension
    loading.
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 较新版本的 Microsoft Edge 也为跨浏览器扩展加载提供了一些支持。
- en: 'First, you must download and build the extension, similar to how you might
    build one of the other examples:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您必须下载并构建扩展，类似于您可能构建其他示例的方式：
- en: '[PRE20]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After the extension has finished building, it is possible to load the unpacked
    extension in Chrome. To do so, you must navigate to chrome://extensions, enable
    developer mode, and then click Load Unpacked, as shown in [figure 12.4](#ch12fig04).
    This will bring up a file-selection dialog, where you must select the dist directory
    created under the chrome-extension directory. That’s the directory containing
    manifest.json.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展构建完成后，可以在Chrome中加载未打包的扩展。要这样做，您必须导航至chrome://extensions，启用开发者模式，然后单击“加载未打包”，如[图12.4](#ch12fig04)所示。这将弹出一个文件选择对话框，在这里您必须选择在chrome-extension目录下创建的dist目录。那个包含manifest.json文件的目录。
- en: Figure 12.4\. Loading the TensorFlow.js MobileNet Chrome extension in developer
    mode
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.4。在开发者模式下加载TensorFlow.js MobileNet Chrome扩展
- en: '![](12fig04_alt.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig04_alt.jpg)'
- en: Once the extension is installed, you should be able to classify images in the
    browser. To do so, navigate to some site with images, such as the Google image
    search page for the term *tiger* used here. Then right-click the image you wish
    to classify. You should see a menu option for `Classify Image with TensorFlow.js.
    Clicking that menu option should cause the extension to execute the MobileNet
    model on the image and then add some text over the image, indicating the prediction
    (see [figure 12.5](#ch12fig05).)`
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 安装扩展后，您应该能够在浏览器中对图像进行分类。要这样做，请导航至一些包含图像的网站，例如在此处使用的Google图像搜索页面上的“tiger”关键词。然后右键单击要分类的图像。您应该会看到一个名为“使用TensorFlow.js对图像进行分类”的菜单选项。单击该菜单选项将使扩展执行MobileNet模型的操作，并在图像上添加一些文本，表示预测结果（参见[图12.5](#ch12fig05)）。
- en: Figure 12.5\. The TensorFlow.js MobileNet Chrome extension helps classify images
    in a web page.
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.5。TensorFlow.js MobileNet Chrome扩展可帮助分类网页中的图像。
- en: '![](12fig05_alt.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig05_alt.jpg)'
- en: To remove the extension, click Remove on the Extensions page (see [figure 12.4](#ch12fig04)),
    or use the Remove from Chrome menu option when right-clicking the extension icon
    at top-right.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除扩展名，请在“扩展”页面上单击“删除”（参见[图12.4](#ch12fig04)），或右键单击右上角的扩展图标时使用“从Chrome菜单中删除”选项。
- en: Note that the model running in the browser extension has access to the same
    hardware acceleration as the model running in the web page and, indeed, uses much
    of the same code. The model is loaded with a call to `tf.loadGraphModel(...)`
    using a suitable URL, and predictions are made using the same `model.predict(...)`
    API we’ve seen. Migrating technology or a proof of concept from a web page deployment
    into a browser extension is relatively easy.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，运行在浏览器扩展中的模型可以访问与运行在网页中的模型相同的硬件加速，并且确实使用了大部分相同的代码。该模型使用适当的URL来调用`tf.loadGraphModel(...)`进行加载，并使用相同的`model.predict(...)`
    API进行预测。从网页部署迁移技术或概念验证到浏览器扩展相对较容易。
- en: 12.3.4\. Deploying TensorFlow.js models in JavaScript-based mobile applications
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.4。在基于JavaScript的移动应用中部署TensorFlow.js模型
- en: For many products, the desktop browser does not provide enough reach, and the
    mobile browser does not provide the smoothly animated customized product experience
    that customers have come to expect. Teams working on these sorts of projects are
    often faced with the dilemma of how to manage the codebase for their web app alongside
    repositories for (typically) both Android (Java or Kotlin) and iOS (Objective
    C or Swift) native apps. While very large groups can support such an outlay, many
    developers are increasingly choosing to reuse much of their code across these
    deployments by leveraging hybrid cross-platform development frameworks.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多产品来说，桌面浏览器提供的覆盖范围不够，移动浏览器也无法提供顾客所期望的平稳动画化的定制产品体验。在这些项目上工作的团队通常面临着如何管理他们的Web应用程序代码库以及（通常）Android（Java或Kotlin）和iOS（Objective
    C或Swift）本机应用程序中的存储库的困境。虽然非常庞大的团队可以支持这样的支出，但许多开发人员越来越倾向于通过利用混合跨平台开发框架在这些部署之间重复使用大部分代码。
- en: Cross-platform app frameworks, like React Native, Ionic, Flutter, and Progressive
    Web-Apps, enable you to write the bulk of an application once in a common language
    and then compile that core functionality to create native experiences with the
    look, feel, and performance that users expect. The cross-platform language/runtime
    handles much of the business logic and layout, and connects to native platform
    bindings for the standardized affordance visuals and feel. How to select the right
    hybrid app development framework is the topic of countless blogs and videos on
    the web, so we will not revisit that discussion here, but will rather focus on
    just one popular framework, React Native. [Figure 12.6](#ch12fig06) illustrates
    a minimal React Native app running a MobileNet model. Notice the lack of any browser
    top bar. Though this simple app doesn’t have UI elements, if it did, you would
    see that they match the native Android look and feel. The same app built for iOS
    would match *those* elements.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 跨平台应用程序框架，如React Native、Ionic、Flutter和渐进式Web应用程序，使您能够使用通用语言编写应用程序的大部分功能，然后编译这些核心功能，以创建具有用户期望的外观、感觉和性能的本机体验。跨平台语言/运行时处理大部分业务逻辑和布局，并连接到本机平台绑定以获得标准的外观和感觉。如何选择合适的混合应用程序开发框架是网络上无数博客和视频的主题，因此我们不会在这里重新讨论这个问题，而是将重点放在一个流行的框架上，即React
    Native。[图12.6](#ch12fig06)示例了一个运行MobileNet模型的简单React Native应用程序。请注意任何浏览器顶部栏的缺失。虽然这个简单的应用程序没有UI元素，但如果有的话，你会发现它们与本机Android的外观和感觉匹配。为iOS构建的相同应用程序也会匹配*那些*元素。
- en: Figure 12.6\. A screenshot from a sample native Android app built with React
    Native. Here, we are running a TensorFlow.js MobileNet model within the native
    app.
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.6\. React Native构建的样本本机Android应用程序的屏幕截图。在这里，我们在本机应用程序中运行了一个TensorFlow.js
    MobileNet模型。
- en: '![](12fig06.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig06.jpg)'
- en: 'Happily, the JavaScript runtime within React Native supports TensorFlow.js
    natively without any special work. The tfjs-react-native package is still in alpha
    release (as of December 2019) but provides GPU support with WebGL via expo-gl.
    The user code looks like the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 令人高兴的是，React Native中的JavaScript运行时原生支持TensorFlow.js，无需做任何特殊工作。tfjs-react-native包目前仍处于alpha发布阶段（截至2019年12月），但通过expo-gl提供了基于WebGL的GPU支持。用户代码如下所示：
- en: '[PRE21]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The package also provides a special API for assisting with loading and saving
    model assets within the mobile app.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 该软件包还提供了特殊API，用于帮助在移动应用程序中加载和保存模型资源。
- en: Listing 12.2\. Loading and saving a model within a mobile app built with React-Native
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.2\. 在使用React-Native构建的移动应用程序中加载和保存模型
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1*** Saves the model to AsyncStorage—a simple key-value storage system global
    to the app'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将模型保存到AsyncStorage——一种对应用程序全局可见的简单键-值存储系统'
- en: '***2*** Loads the model from AsyncStorage'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 从AsyncStorage加载模型'
- en: While native app development through React Native still requires learning a
    few new tools, such as Android Studio for Android and XCode for iOS, the learning
    curve is shallower than diving straight into native development. That these hybrid
    app development frameworks support TensorFlow.js means that the machine-learning
    logic can live in a single codebase rather than requiring us to develop, maintain,
    and test a separate version for each hardware surface—a clear win for developers
    who wish to support the native app experience! But what about the native desktop
    experience?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过React Native进行本机应用程序开发仍需要学习一些新的工具，比如Android Studio用于Android和XCode用于iOS，但学习曲线比直接进行本机开发要平缓。这些混合应用程序开发框架支持TensorFlow.js意味着机器学习逻辑可以存在于一个代码库中，而不需要我们为每个硬件平台表面开发、维护和测试一个单独的版本，这对于希望支持本机应用体验的开发人员来说是一个明显的胜利！但是，本机桌面体验呢？
- en: 12.3.5\. Deploying TensorFlow.js models in JavaScript-based cross-platform-
    m desktop applications
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.5\. 在基于JavaScript的跨平台桌面应用程序中部署TensorFlow.js模型
- en: JavaScript frameworks such as Electron.js allow desktop applications to be written
    in a cross-platform manner reminiscent of cross-platform mobile applications written
    in React Native. With such frameworks, you need to write your code only once,
    and it can be deployed and run on mainstream desktop operating systems, including
    macOS, Windows, and major distributions of Linux. This greatly simplifies the
    traditional development workflow of maintaining separate codebases for largely
    incompatible desktop operating systems. Take Electron.js, the leading framework
    in this category, for example. It uses Node.js as the virtual machine that undergirds
    the application’s main process; for the GUI portion of the app, it uses Chromium,
    a full-blown and yet lightweight web browser that shares much of its code with
    Google Chrome.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 Electron.js 等 JavaScript 框架允许以类似于使用 React Native 编写跨平台移动应用程序的方式编写桌面应用程序。
    使用这样的框架，您只需编写一次代码，就可以在主流桌面操作系统（包括 macOS、Windows 和主要的 Linux 发行版）上部署和运行。 这大大简化了传统开发流程，即为大部分不兼容的桌面操作系统维护单独的代码库。
    以此类别中的主要框架 Electron.js 为例。 它使用 Node.js 作为支撑应用程序主要进程的虚拟机； 对于应用程序的 GUI 部分，它使用了 Chromium，一个完整但轻量级的网络浏览器，它与
    Google Chrome 共享大部分代码。
- en: TensorFlow.js is compatible with Electron.js, as is demonstrated by the simple
    example in the tfjs-examples repository. This example, found in the electron directory,
    illustrates how to deploy a TensorFlow.js model for inference in an Electron.js-based
    desktop app. The app allows users to search the filesystem for image files that
    visually match one or more keywords (see the screenshot in [figure 12.7](#ch12fig07)).
    This search process involves applying a TensorFlow.js MobileNet model for inference
    on a directory of images.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 兼容 Electron.js，如 tfjs-examples 仓库中简单示例所示。 这个示例位于 electron 目录中，演示了如何在基于
    Electron.js 的桌面应用中部署 TensorFlow.js 模型以进行推理。 该应用允许用户搜索文件系统中与一个或多个关键词视觉匹配的图像文件（请参阅
    [图 12.7](#ch12fig07) 中的截图）。 这个搜索过程涉及在一组图像目录上应用 TensorFlow.js MobileNet 模型进行推理。
- en: Figure 12.7\. A screenshot from the example Electron.js-based desktop application
    that utilizes a TensorFlow.js model, from tfjs-examples/electron
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.7\. 一个使用 TensorFlow.js 模型的示例 Electron.js 桌面应用程序的屏幕截图，来自 tfjs-examples/electron
- en: '![](12fig07_alt.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig07_alt.jpg)'
- en: 'Despite its simplicity, this example app illustrates an important consideration
    in deploying TensorFlow.js models to Electron.js: the choice of the compute backend.
    An Electron.js application runs on a Node.js-based backend process as well as
    a Chromium-based frontend process. TensorFlow.js can run in either of those environments.
    As a result, the same model can run in either the application’s node-like backend
    process or the browser-like frontend process. In the case of backend deployment,
    the @tensorflow/tfjs-node package is used, while the @tensorflow/tfjs package
    is used for the frontend case ([figure 12.8](#ch12fig08)). A check box in the
    example application’s GUI allows you to switch between the backend and frontend
    inference modes ([figure 12.7](#ch12fig07)), although in an actual application
    powered by Electron.js and TensorFlow.js, you would normally decide on one environment
    for your model beforehand. We will next briefly discuss the pros and cons of the
    options.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个示例应用程序很简单，但它展示了在将 TensorFlow.js 模型部署到 Electron.js 时一个重要的考虑因素：计算后端的选择。 Electron.js
    应用程序在基于 Node.js 的后端进程和基于 Chromium 的前端进程上运行。 TensorFlow.js 可以在这两种环境中运行。 因此，同一个模型可以在应用程序的类似
    node 的后端进程或类似浏览器的前端进程中运行。 在后端部署的情况下，使用 @tensorflow/tfjs-node 包，而在前端情况下使用 @tensorflow/tfjs
    包（[图 12.8](#ch12fig08)）。 示例应用程序的 GUI 中的复选框允许您在后端和前端推理模式之间切换（[图 12.7](#ch12fig07)），尽管在由
    Electron.js 和 TensorFlow.js 驱动的实际应用程序中，您通常会事先决定模型的运行环境。 接下来我们将简要讨论各个选项的优缺点。
- en: Figure 12.8\. The architecture of an Electron.js-based desktop application that
    utilizes TensorFlow.js for accelerated deep learning. Different compute backends
    of TensorFlow.js can be invoked, from either the main backend process or the in-browser
    renderer process. Different compute backends cause models to be run on different
    underlying hardware. Regardless of the choice of compute backend, the code that
    loads, defines, and runs deep-learning models in TensorFlow.js is largely the
    same. The arrowheads in this diagram indicate invocation of library functions
    and other callable routines.
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.8\. 基于 Electron.js 的桌面应用程序架构，利用 TensorFlow.js 进行加速深度学习。TensorFlow.js 的不同计算后端可以从主后端进程或浏览器渲染器进程中调用。不同的计算后端导致模型在不同的底层硬件上运行。无论计算后端的选择如何，在
    TensorFlow.js 中加载、定义和运行深度学习模型的代码基本相同。此图中的箭头表示库函数和其他可调用程序的调用。
- en: '![](12fig08_alt.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig08_alt.jpg)'
- en: As [figure 12.8](#ch12fig08) shows, different choices of the compute backend
    cause the deep-learning computation to happen on different computation hardware.
    Backend deployment based on @tensorflow/tfjs-node assigns the workload to the
    CPU, leveraging the multithreaded and SIMD-enabled libtensorflow library. This
    Node.js-based model-deployment option is usually faster than the frontend option
    and can accommodate larger models due to the fact that the backend environment
    is free of resource restrictions. However, their major downside is the large package
    size, which is a result of the large size of libtensorflow (for tfjs-node, approximately
    50 MB with compression).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图12.8](#ch12fig08)所示，不同的计算后端选择会导致深度学习计算在不同的计算硬件上进行。基于 @tensorflow/tfjs-node
    的后端部署将工作负载分配到 CPU 上，利用多线程和 SIMD（单指令多数据）功能的 libtensorflow 库。这种基于 Node.js 的模型部署选项通常比前端选项更快，并且由于后端环境没有资源限制，可以容纳更大的模型。然而，它们的主要缺点是包大小较大，这是由于
    libtensorflow 的体积较大（对于 tfjs-node，约为 50 MB 带有压缩）。
- en: The frontend deployment dispatches deep-learning workloads to WebGL. For small-to-medium-sized
    models, and in cases where the latency of inference is not of major concern, this
    is an acceptable option. This option leads to a smaller package size, and it works
    out of the box for a wide range of GPUs, thanks to the wide support for WebGL.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 前端部署将深度学习工作负载分派给 WebGL。对于中小型模型以及推理延迟不是主要关注点的情况，这是一个可接受的选项。此选项导致包大小较小，并且由于对 WebGL
    的广泛支持，可在广泛范围的 GPU 上直接运行。
- en: As [figure 12.8](#ch12fig08) also illustrates, the choice of compute backend
    is a largely separate concern from the JavaScript code that loads and runs your
    model. The same API works for all three options. This is clearly demonstrated
    in the example app, where the same module (`ImageClassifier` in electron/image_classifier.js)
    subserves the inference task in both the backend and frontend environments. We
    should also point out that although the tfjs-examples/electron example shows only
    inference, you can certainly use TensorFlow.js for other deep-learning workflows,
    such as model creation and training (for example, transfer learning) in Electron.js
    apps equally well.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图12.8](#ch12fig08)所示，计算后端的选择在很大程度上是与加载和运行模型的 JavaScript 代码分开的。相同的 API 对所有三个选项都适用。这在示例应用中得到了清楚的展示，其中相同的模块（`ImageClassifier`
    在 electron/image_classifier.js 中）在后端和前端环境下均用于执行推理任务。我们还应指出，尽管 tfjs-examples/electron
    示例仅显示推理，但您确实可以将 TensorFlow.js 用于 Electron.js 应用程序中的其他深度学习工作流程，例如模型创建和训练（例如，迁移学习）同样有效。
- en: 12.3.6\. Deploying TensorFlow.js models on WeChat and other JavaScript-bas-
    sed mobile app plugin systems
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.6\. 在微信和其他基于 JavaScript 的移动应用插件系统上部署 TensorFlow.js 模型
- en: There are some places where the main mobile-app-distribution platform is neither
    Android’s Play Store nor Apple’s App Store, but rather a small number of “super
    mobile apps” that allow for third-party extensions within their own first-party
    curated experience.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 有些地方的主要移动应用分发平台既不是 Android 的 Play Store 也不是 Apple 的 App Store，而是一小部分“超级移动应用程序”，它们允许在其自己的第一方精选体验中使用第三方扩展。
- en: A few of these super mobile apps come from Chinese tech giants, notably Tencent’s
    WeChat, Alibaba’s Alipay, and Baidu. These use JavaScript as their main technology
    to enable the creation of third-party extensions, making TensorFlow.js a natural
    fit for deploying machine learning on their platform. The set of APIs available
    within these mobile app plugin systems is not the same as the set available in
    native JavaScript, however, so some additional knowledge and work is required
    to deploy there.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超级移动应用程序中有一些来自中国科技巨头，特别是腾讯的微信、阿里巴巴的支付宝和百度。它们使用JavaScript作为主要技术，以实现第三方扩展的创建，使得TensorFlow.js成为在其平台上部署机器学习的自然选择。然而，这些移动应用程序插件系统中可用的API集与本机JavaScript中可用的集合不同，因此在那里部署需要一些额外的知识和工作。
- en: Let’s use WeChat as an example. WeChat is the most widely used social media
    app in China, with over 1 billion monthly active users. In 2017, WeChat launched
    Mini Program, a platform for application developers to create JavaScript mini-programs
    within the WeChat system. Users can share and install these mini-programs inside
    the WeChat app on-the-fly, and it’s been a tremendous success. By Q2 2018, WeChat
    had more than 1 million mini-programs and over 600 million daily active mini-program
    users. There are also more than 1.5 million developers who are developing applications
    on this platform, partly because of the popularity of JavaScript.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以微信为例。微信是中国最广泛使用的社交媒体应用程序，每月活跃用户数超过10亿。2017年，微信推出了小程序，这是一个让应用开发者在微信系统内部创建JavaScript小程序的平台。用户可以在微信应用程序内分享和安装这些小程序，这是一个巨大的成功。截至2018年第二季度，微信拥有100多万个小程序和6亿多日活跃用户。还有超过150万的开发者在这个平台上开发应用程序，部分原因是JavaScript的流行。
- en: WeChat mini-program APIs are designed to provide developers easy access to mobile
    device sensors (the camera, microphone, accelerometer, gyroscope, GPS, and so
    on). However, the native API provides very limited machine-learning functionality
    built into the platform. TensorFlow.js brings several advantages as a machine-learning
    solution for mini-programs. Previously, if developers wanted to embed machine
    learning in their applications, they needed to work outside the mini-program development
    environment with a server-side or cloud-based machine-learning stack. Doing so
    makes the barrier high for the large number of mini-program developers to build
    and use machine learning. Standing up an external serving infrastructure is outside
    of the scope of possibilities for most mini-program developers. With TensorFlow.js,
    machine-learning development happens right within the native environment. Furthermore,
    since it is a client-side solution, it helps reduce network traffic and improves
    latency, and it takes advantage of GPU acceleration using WebGL.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: WeChat小程序API旨在为开发者提供便捷访问移动设备传感器（摄像头、麦克风、加速计、陀螺仪、GPS等）的功能。然而，原生API在平台上提供的机器学习功能非常有限。TensorFlow.js作为小程序的机器学习解决方案带来了几个优势。以前，如果开发者想要在他们的应用程序中嵌入机器学习，他们需要在小程序开发环境之外使用服务器端或基于云的机器学习堆栈。这使得大量小程序开发者想要构建和使用机器学习变得更难。搭建外部服务基础设施对于大多数小程序开发者来说是不可能的。有了TensorFlow.js，机器学习开发就在本地环境中进行。此外，由于它是一个客户端解决方案，它有助于减少网络流量和改善延迟，并利用了WebGL的GPU加速。
- en: The team behind TensorFlow.js has created a WeChat mini-program you can use
    to enable TensorFlow.js for your mini-program (see [https://github.com/tensorflow/tfjs-wechat](https://github.com/tensorflow/tfjs-wechat)).
    The repository also contains an example mini-program that uses PoseNet to annotate
    the positions and postures of people sensed by the mobile device’s camera. It
    uses TensorFlow.js accelerated by a newly added WebGL API from WeChat. Without
    access to the GPU, the model would run too slowly to be useful for most applications.
    With this plugin, a WeChat mini-program can have the same model execution performance
    as a JavaScript app running inside mobile browsers. In fact, we have observed
    that the WeChat sensor API typically *outperforms* the counterpart in the browser.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js团队创建了一个微信小程序，你可以使用它来为你的小程序启用TensorFlow.js（见[https://github.com/tensorflow/tfjs-wechat](https://github.com/tensorflow/tfjs-wechat)）。该存储库还包含一个使用PoseNet来注释移动设备摄像头感知到的人的位置和姿势的示例小程序。它使用了微信新增加的WebGL
    API加速的TensorFlow.js。如果没有GPU加速，模型的运行速度对大多数应用程序来说太慢了。有了这个插件，微信小程序可以拥有与在移动浏览器内运行的JavaScript应用程序相同的模型执行性能。事实上，我们已经观察到微信传感器API通常*优于*浏览器中的对应API。
- en: As of late 2019, developing machine-learning experiences for super app plugins
    is still very new territory. Getting high performance may require some help from
    the platform maintainers. Still, it is the best way to deploy your app in front
    of the hundreds of millions of people for whom the super mobile app *is* the internet.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 到2019年底，为超级应用插件开发机器学习体验仍然是非常新的领域。获得高性能可能需要一些来自平台维护者的帮助。但是，这仍是将您的应用部署到数以亿计的将超级移动应用作为互联网的人民面前的最佳方式。
- en: 12.3.7\. Deploying TensorFlow.js models on single-board computers
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.7\. 在单板计算机上部署TensorFlow.js模型
- en: For many web developers, deploying to a headless single-board computer sounds
    very technical and foreign. However, thanks to the success of the Raspberry Pi,
    developing and building simple hardware devices has never been easier. Single-board
    computers provide a platform to inexpensively deploy intelligence without depending
    on network connections to cloud servers or bulky, costly computers. Single-board
    computers can be used to back security applications, moderate internet traffic,
    control irrigation—the sky’s the limit.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 对许多网页开发者来说，部署到无头单板计算机听起来非常技术化和陌生。然而，多亏了树莓派的成功，开发和构建简单的硬件设备变得前所未有的容易。单板计算机提供了一个平台，可以廉价部署智能，而不依赖于云服务器的网络连接或笨重昂贵的计算机。单板计算机可用于支持安全应用、调节互联网流量、控制灌溉，无所不能。
- en: Many of these single-board computers provide general-purpose input-output (GPIO)
    pins to make it easy to connect to physical control systems, and include a full
    Linux install to allow educators, developers, and hackers to develop a wide range
    of interactive devices. JavaScript has quickly become a popular language for building
    on these types of devices. Developers can use node libraries such as rpi-gpio
    to interact electronically at the lowest level, all in JavaScript.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些单板计算机提供通用输入输出（GPIO）引脚，以便轻松连接物理控制系统，并包含完整的Linux安装，允许教育工作者、开发人员和黑客开发各种互动设备。JavaScript迅速成为构建这些类型设备的一种流行语言。开发人员可以使用像rpi-gpio这样的Node库在JavaScript中以最低层次进行电子交互。
- en: 'To help support these users, TensorFlow.js currently has two runtimes on these
    embedded ARM devices: `tfjs-node (CPU^([[11](#ch12fn11)])) and tfjs-headless-nodegl
    (GPU). The entire TensorFlow.js library runs on these devices through those two
    backends. Developers can run inference using off-the-shelf models or train their
    own, all on the device hardware!`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助支持这些用户，TensorFlow.js当前在这些嵌入式ARM设备上有两个运行时：`tfjs-node (CPU^([[11](#ch12fn11)]))`和`tfjs-headless-nodegl
    (GPU)`。整个TensorFlow.js库通过这两个后端在这些设备上运行。开发人员可以在设备硬件上运行推断，使用现有模型或自己训练模型！
- en: ^(11)
  id: totrans-305
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-306
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you are looking to utilize the CPU with ARM NEON acceleration, you should
    use the tfjs-node package on these devices. This package ships support for both
    ARM32 and ARM64 architectures.
  id: totrans-307
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您希望在这些设备上利用ARM NEON加速CPU，则应该使用tfjs-node软件包。该软件包支持ARM32和ARM64架构。
- en: The release of recent devices such as the NVIDIA Jetson Nano and Raspberry Pi
    4 brings a system-on-chip (SoC) with a modern graphics stack. The GPU on these
    devices can be leveraged by the underlying WebGL code used in core TensorFlow.js.
    The headless WebGL package `(`tfjs-backend-nodegl) allows users to run TensorFlow.js
    on Node.js purely accelerated by the GPU on these devices (see [figure 12.9](#ch12fig09)).
    By delegating the execution of TensorFlow.js to the GPU, developers can continue
    to utilize the CPU for controlling other parts of their devices.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 近期推出的设备（如NVIDIA Jetson Nano和Raspberry Pi 4）带来了现代图形堆栈的SoC（系统级芯片）。这些设备上的GPU可以被TensorFlow.js核心中使用的基础WebGL代码利用。无头WebGL包`(`tfjs-backend-nodegl)`允许用户纯粹通过这些设备上的GPU加速在Node.js上运行TensorFlow.js（见[图12.9](#ch12fig09)）。通过将TensorFlow.js的执行委托给GPU，开发人员可以继续利用CPU来控制设备的其他部分。
- en: Figure 12.9\. TensorFLow.js executing MobileNet using headless WebGL on a Raspberry
    Pi 4
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.9\. 使用无头WebGL在树莓派4上执行MobileNet的TensorFLow.js
- en: '![](12fig09_alt.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](12fig09_alt.jpg)'
- en: Model security and data security are very strong for the single-board computer
    deployment. Computation and actuation are handled directly on the device, meaning
    data does not need to go to a device outside of the owner’s control. Encryption
    can be used to guard the model even if the physical device is compromised.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 单板计算机部署的模型安全性和数据安全性非常强。计算和执行直接在设备上处理，这意味着数据不需要传输到所有者无法控制的设备上。即使物理设备遭到破坏，也可以使用加密保护模型。
- en: Deployment to single-board computers is still a very new area for JavaScript
    in general, and TensorFlow.js in particular, but it unlocks a wide range of applications
    that other deployment areas are unsuitable for.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于JavaScript来说，将部署到单板计算机仍然是一个非常新颖的领域，尤其是TensorFlow.js，但它为其他部署领域不适用的广泛应用提供了可能。
- en: 12.3.8\. Summary of deployments
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.8\. 部署摘要
- en: In this section, we’ve covered several different ways to get your TensorFlow.js
    machine-learning system out in front of the user base ([table 12.4](#ch12table04)
    summarizes them). We hope we’ve kindled your imagination and helped you dream
    about radical applications of the technology! The JavaScript ecosystem is vast
    and wide, and in the future, machine-learning-enabled systems will be running
    in areas we couldn’t even dream of today.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了几种不同的方法，可以使您的TensorFlow.js机器学习系统走在用户基础的前面（[表12.4](#ch12table04)总结了它们）。我们希望我们能激发您的想象力，并帮助您梦想着技术的激进应用！JavaScript生态系统广阔而广阔，在未来，具有机器学习功能的系统将在我们今天甚至无法想象的领域运行。
- en: Table 12.4\. Target environments to which TensorFlow.js models can be deployed,
    and the hardware accelerator each environment can use
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表12.4\. TensorFlow.js模型可以部署到的目标环境以及每个环境可以使用的硬件加速器
- en: '| Deployment | Hardware accelerator support |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 部署 | 硬件加速器支持 |'
- en: '| --- | --- |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Browser | WebGL |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | WebGL |'
- en: '| Node.js server | CPU with multithreading and SIMD support; CUDA-enabled GPU
    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Node.js服务器 | 具有多线程和SIMD支持的CPU；具有CUDA支持的GPU |'
- en: '| Browser plugin | WebGL |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器插件 | WebGL |'
- en: '| Cross-platform desktop app (such as Electron) | WebGL, CPU with multithreading
    and SIMD support, or CUDA-enabled GPU |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 跨平台桌面应用程序（如Electron） | WebGL，支持多线程和SIMD的CPU，或者具有CUDA支持的GPU |'
- en: '| Cross-platform mobile app (such as React Native) | WebGL |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 跨平台移动应用程序（如React Native） | WebGL |'
- en: '| Mobile-app plugin (such as WeChat) | Mobile WebGL |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 移动应用程序插件（如微信） | 移动WebGL |'
- en: '| Single-board computer (such as Raspberry Pi) | GPU or ARM NEON |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 单板计算机（如Raspberry Pi） | GPU或ARM NEON |'
- en: Materials for further reading
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步阅读材料
- en: 'Denis Baylor et al., “TFX: A TensorFlow-Based Production-Scale Machine Learning
    Platform,” KDD 2017, [www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform](http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform).'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denis Baylor等，“TFX：基于TensorFlow的生产规模机器学习平台”，KDD 2017，[www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform](http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform)。
- en: 'Raghuraman Krishnamoorthi, “Quantizing Deep Convolutional Networks for Efficient
    Inference: A Whitepaper,” June 2018, [https://arxiv.org/pdf/1806.08342.pdf](https://arxiv.org/pdf/1806.08342.pdf).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghuraman Krishnamoorthi，“为高效推理量化深度卷积网络：一份白皮书”，2018年6月，[https://arxiv.org/pdf/1806.08342.pdf](https://arxiv.org/pdf/1806.08342.pdf)。
- en: Rasmus Munk Larsen and Tatiana Shpeisman, “TensorFlow Graph Optimization,” [https://ai.google/research/pubs/pub48051](https://ai.google/research/pubs/pub48051).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasmus Munk Larsen和Tatiana Shpeisman，“TensorFlow图优化”，[https://ai.google/research/pubs/pub48051](https://ai.google/research/pubs/pub48051)。
- en: Exercises
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: Back in [chapter 10](kindle_split_022.html#ch10), we trained an Auxiliary Class
    GAN (ACGAN) on the MNIST dataset to generate fake MNIST digit images by class.
    Specifically, the example we used is in the mnist-acgan directory of the tfjs-examples
    repository. The generator part of the trained model has a total size of about
    10 MB, most of which is occupied by the weights stored as 32-bit floats. It’s
    tempting to perform post-training weight quantization on this model to speed up
    the page loading. However, before doing so, we need to make sure that no significant
    deterioration in the quality of the generated images results from such quantization.
    Test 16- and 8-bit quantization and determine whether either or both of them is
    an acceptable option. Use the `tensorflowjs_converter` workflow described in [section
    12.2.1](#ch12lev2sec4). What criteria will you use to evaluate the quality of
    the generated MNIST images in this case?
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第10章](kindle_split_022.html#ch10)中，我们训练了一个辅助类生成对抗网络（ACGAN）来生成MNIST数据集的假冒图像，以类别为单位。具体来说，我们使用的示例位于tfjs-examples存储库的mnist-acgan目录中。训练模型的生成器部分总共约占用了大约10
    MB的空间，其中大部分是以32位浮点数存储的权重。诱人的是对该模型进行训练后的权重量化以加快页面加载速度。但是，在执行此操作之前，我们需要确保这种量化不会导致生成的图像质量显着下降。测试16位和8位量化，并确定它们中的任何一个或两者都是可接受的选项。使用[section
    12.2.1](#ch12lev2sec4)中描述的`tensorflowjs_converter`工作流程。在这种情况下，您将使用什么标准来评估生成的MNIST图像的质量？
- en: Tensorflow models that run as Chrome extensions have the advantage of being
    able to control Chrome itself. In the speech-commands example in [chapter 4](kindle_split_015.html#ch04),
    we showed how to use a convolutional model to recognize spoken words. The Chrome
    extension API gives you the ability to query and change tabs. Try embedding the
    speech-commands model into an extension, and tune it to recognize the phrases
    “next tab” and “previous tab.” Use the results of the classifier to control the
    browser tab focus.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为 Chrome 扩展运行的 Tensorflow 模型具有控制 Chrome 本身的优势。在[第 4 章](kindle_split_015.html#ch04)中的语音命令示例中，我们展示了如何使用卷积模型识别口语。Chrome
    扩展 API 允许您查询和更改标签页。尝试将语音命令模型嵌入到扩展中，并调整它以识别“下一个标签页”和“上一个标签页”短语。使用分类器的结果来控制浏览器标签焦点。
- en: '[Info box 12.3](#ch12sb03) describes the correct way to measure the time that
    a TensorFlow.js model’s `predict()` call (inference call) takes and the cautionary
    points it involves. In this exercise, load a MobileNetV2 model in TensorFlow.js
    (see the simple-object-detection example in [section 5.2](kindle_split_016.html#ch05lev1sec2)
    if you need a reminder of how to do that) and time its `predict()` call:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[信息框 12.3](#ch12sb03)描述了正确测量 TensorFlow.js 模型的`predict()`调用（推断调用）所需时间以及涉及的注意事项。在这个练习中，加载一个
    TensorFlow.js 中的 MobileNetV2 模型（如果需要提醒如何做到这一点，请参见 [5.2 节](kindle_split_016.html#ch05lev1sec2)
    中的简单对象检测示例），并计时其`predict()`调用：'
- en: As the first step, generate a randomly valued image tensor of shape `[1, 224,
    224, 3]` and the model’s inference on it by following the steps laid out in [info
    box 12.3](#ch12sb03). Compare the timing result with and without the `array()`
    or `data()` call on the output tensor. Which one is shorter? Which one is the
    correct time measurement?
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第一步，生成一个形状为`[1, 224, 224, 3]`的随机值图像张量，并按照[信息框 12.3](#ch12sb03)中的步骤对其进行模型推断。将结果与输出张量上的`array()`或`data()`调用进行比较。哪一个更短？哪一个是正确的时间测量？
- en: When the correct measurement is done 50 times in a loop, plot the individual
    timing numbers using the tfjs-vis line chart ([chapter 7](kindle_split_019.html#ch07))
    and get an intuitive appreciation of the variability. Can you see clearly that
    the first few measurements are significantly different from the rest? Given this
    observation, discuss the importance of performing burn-in or warm-up runs during
    performance benchmarking.
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当正确的测量在循环中执行 50 次时，使用 tfjs-vis 折线图（[第 7 章](kindle_split_019.html#ch07)）绘制单独的时间数字，并直观地了解可变性。你能清楚地看到前几次测量与其余部分明显不同吗？鉴于这一观察结果，讨论在性能基准测试期间执行
    burn-in 或预热运行的重要性。
- en: Unlike tasks a and b, replace the randomly generated input tensor with a real
    image tensor (such as one obtained from an `img` element using `tf.browser.fromPixels()`),
    and then repeat the measurements in step b. Does the content of the input tensor
    affect the timing measurements in any significant way?
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与任务 a 和 b 不同，将随机生成的输入张量替换为真实的图像张量（例如，使用`tf.browser.fromPixels()`从`img`元素获取的图像张量），然后重复步骤
    b 中的测量。输入张量的内容是否对时间测量产生任何重大影响？
- en: Instead of running inference on a single example (batch size = 1), try increasing
    the batch size to 2, 3, 4, and so forth until you reach a relatively large number,
    such as 32\. Is the relation between the average inference time and batch size
    a monotonically increasing one? A linear one?
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要在单个示例（批量大小 = 1）上运行推断，尝试将批量大小增加到 2、3、4 等，直到达到相对较大的数字，例如 32。平均推断时间与批量大小之间的关系是单调递增的吗？是线性的吗？
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概要
- en: Good engineering discipline around testing is as important to your machine-learning
    code as it is to your non-machine-learning code. However, avoid the temptation
    to focus strongly on “special” examples or make assertions on “golden” model predictions.
    Instead, rely on testing the fundamental properties of your model, such as its
    input and output specifications. Furthermore, remember that all the data-preprocessing
    code before your machine-learning system is just “normal” code and should be tested
    accordingly.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于机器学习代码，良好的工程纪律围绕测试同样重要，就像对非机器学习代码一样重要。然而，避免过分关注“特殊”示例或对“黄金”模型预测进行断言的诱惑。相反，依靠测试模型的基本属性，如其输入和输出规范。此外，请记住，机器学习系统之前的所有数据预处理代码都只是“普通”代码，应该相应地进行测试。
- en: Optimizing the speed of downloading and inference is an important factor to
    the success of client-side deployment of TensorFlow.js models. Using the post-training
    weight quantization feature of the `tensorflowjs_converter` binary, you can reduce
    the total size of a model, in some cases without observable loss of inference
    accuracy. The graph-model conversion feature of `tensorflowjs_converter` helps
    to speed up model inference through graph transformations such as op fusion. You
    are highly encouraged to test and employ both model-optimization techniques when
    deploying your TensorFlow.js models to production.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化下载速度和推断速度是 TensorFlow.js 模型客户端部署成功的重要因素。 使用`tensorflowjs_converter`二进制文件的后训练权重量化功能，您可以减小模型的总大小，在某些情况下，无需观察到推断精度的损失。
    `tensorflowjs_converter`的图模型转换功能可通过操作融合等图转换来加速模型推断。 在部署 TensorFlow.js 模型到生产环境时，强烈建议您测试和采用这两种模型优化技术。
- en: A trained, optimized model is not the end of the story for your machine-learning
    application. You must find some way to integrate it with an actual product. The
    most common way for TensorFlow.js applications to be deployed is within web pages,
    but this is just one of a wide variety of deployment scenarios, each with its
    own strengths. TensorFlow.js models can run as browser extensions, within native
    mobile apps, as native desktop applications, and even on single-board hardware
    like the Raspberry Pi.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过训练和优化的模型并不是您机器学习应用程序的终点。 您必须找到一种方法将其与实际产品集成。 TensorFlow.js 应用程序最常见的部署方式是在网页中，但这只是各种部署方案中的一个，每种部署方案都有其自身的优势。
    TensorFlow.js 模型可以作为浏览器扩展程序运行，在原生移动应用程序中运行，作为原生桌面应用程序运行，甚至在树莓派等单板硬件上运行。
