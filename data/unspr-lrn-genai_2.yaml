- en: 2 Clustering techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 聚类技术
- en: 'In this second chapter, we are going to cover the following topics:'
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在这第二章中，我们将涵盖以下主题：
- en: Clustering techniques and salient use cases in the industry
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行业中的聚类技术及其显著用例
- en: Various clustering algorithms available
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的各种聚类算法
- en: K-means, hierarchical clustering, and DBSCAN clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值、层次聚类和DBSCAN聚类
- en: Implementation of algorithms in Python
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中实现算法
- en: Case study on cluster analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类分析案例研究
- en: '*“Simplicity is the ultimate sophistication”* – Leonardo da Vinci'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*“简单是最终的精妙”* – 列奥纳多·达·芬奇'
- en: Nature loves simplicity, and teaches us to follow the same path. Most of the
    time, our decisions are simple choices. Simple solutions are easier to comprehend,
    less time consuming, and painless to maintain and ponder over. The machine learning
    world is no different. An elegant machine learning solution is not one which is
    the most complicated algorithm available, but one which solves the business problem.
    A robust machine learning solution is easy enough to readily decipher and pragmatic
    enough to implement. Clustering solutions are generally easier to be understood.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大自然崇尚简单，教导我们走同样的路。大多数时候，我们的决定都是简单的选择。简单的解决方案更容易理解，耗时更少，更容易维护和思考。机器学习世界也不例外。一个优雅的机器学习解决方案不是最复杂的可用算法，而是解决业务问题的一个。一个强大的机器学习解决方案易于理解，并且实施起来实用。聚类解决方案通常更容易被理解。
- en: 'In the previous chapter, we defined unsupervised learning and discussed the
    various unsupervised algorithms available. We will cover each of those algorithms
    as we work through this book; in this second chapter we are going to focus in
    on the first of these: Clustering algorithms.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们定义了无监督学习，并讨论了可用的各种无监督算法。随着我们在本书中的工作，我们将涵盖每一种算法；在这第二章中，我们将专注于其中的第一个：聚类算法。
- en: We will define clustering first and then study the different types of clustering
    techniques. We will examine the mathematical foundation, accuracy measurements,
    and pros and cons of each algorithm. We will implement three of these algorithms
    using Python code on a dataset to complement the theoretical knowledge. The chapter
    ends with the various use cases of clustering techniques in the pragmatic business
    scenario to prepare for the actual business world. This technique is being followed
    throughout the book where we study the concepts first, implement the actual code
    to enhance the Python skills, and dive into real-world business problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义聚类，然后学习不同类型的聚类技术。我们将检查每种算法的数学基础、准确度测量以及优缺点。我们将使用Python代码在数据集上实现其中三种算法，以补充理论知识。本章结束时，我们将探讨聚类技术在实际业务场景中的各种用例，为进入实际业务世界做准备。这种技术在整本书中都在使用，我们首先学习概念，然后实现实际代码以提高Python技能，并深入研究实际的业务问题。
- en: We are going to study basic clustering algorithms in this chapter which are
    K-means clustering, hierarchical clustering, and DBSCAN clustering. These clustering
    algorithms are generally the starting points whenever we want to study clustering.
    In the later chapters of the book, we are going to explore more complex algorithms
    like spectrum clustering, Gaussian Mixture Models, time series clustering, fuzzy
    clustering etc. If you have a good understanding of K-means clustering, hierarchical
    clustering, and DBSCAN – you can skip to the next chapter. Still, it is advisable
    to read the chapter once – you might find something useful to refresh your concepts!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习基本的聚类算法，包括K均值聚类、层次聚类和DBSCAN聚类。这些聚类算法通常是我们研究聚类时的起点。在本书的后面章节中，我们将探讨更复杂的算法，如谱聚类、高斯混合模型、时间序列聚类、模糊聚类等。如果你对K均值聚类、层次聚类和DBSCAN有很好的理解
    - 你可以跳到下一章。但建议你读完本章一次 - 你可能会发现一些有用的东西来更新你的概念！
- en: Let’s first understand what we mean by “*clustering*”. All the very best on
    your journey to master unsupervised learning based clustering techniques!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解一下“*聚类*”是什么意思。祝你在掌握基于无监督学习的聚类技术的旅程中一切顺利！
- en: 2.1 Technical toolkit
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 技术工具箱
- en: We are using the 3.6+ version of Python in this chapter. A basic understanding
    of Python and code execution is expected. You are advised to refresh concepts
    of object-oriented programming and Python concepts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将使用Python的3.6+版本。我们期望你对Python和代码执行有基本的了解。建议你复习面向对象编程和Python概念。
- en: Throughout the book, we are using Jupyter notebook to execute the code. Jupyter
    offers flexibility in execution and debugging, hence it is being used. It is quite
    user-friendly and is platform or operating system agnostic. So if you are using
    Windows or Mac OS or Linux, Jupyter should work just fine.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书始终使用 Jupyter 笔记本执行代码。Jupyter 在执行和调试方面提供了灵活性，因此被广泛使用。它非常用户友好，可以在任何平台或操作系统上使用。所以，无论你使用
    Windows、Mac OS 还是 Linux，Jupyter 都可以正常工作。
- en: All the datasets and code files are checked-in to the Github repository at ([https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter2](main.html)).
    You need to install the following Python libraries to execute the code – `numpy`,
    `pandas`, `matplotlib`, `scipy`, `sklearn`. CPU is good enough for execution,
    but if you face some computing lags, and would like to speed up the execution,
    switch to GPU or Google Collaboratory (colab). Google colab offers free-of-cost
    computation for machine learning solutions. You are advised to study more about
    Google Colab and how to use it for training the machine learning algorithms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据集和代码文件都已经检入到 Github 代码库中（[https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter2](main.html)）。你需要安装以下
    Python 库来执行代码 – `numpy`, `pandas`, `matplotlib`, `scipy`, `sklearn`。CPU 足够执行，但如果遇到一些计算延迟，并且想加快执行速度，切换到
    GPU 或 Google 协作平台（colab）。Google 协作平台为机器学习解决方案提供免费计算。建议你了解更多关于谷歌协作平台以及如何用它来训练机器学习算法的内容。
- en: Now, we are starting with clustering in the following section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在下一节开始聚类。
- en: 2.2 Clustering
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 聚类
- en: Consider this scenario. A group of children are asked to group the items in
    a room into different segments. Each child can use their own logic. Someone might
    club the objects based on the weight, other children might use material while
    someone might use all three - weight, material, and colour. The permutations are
    many and depend on the *parameters* used for grouping. Here, a child is segmenting
    or *clustering* the objects based on the chosen logic.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这样的情景。一群孩子被要求将房间中的物品分成不同的部分。每个孩子都可以使用自己的逻辑。有人可能根据重量来分类物品，其他孩子可能使用材料，而另一些人可能同时使用重量、材料和颜色。排列组合很多，取决于用于分组的
    *参数*。在这里，一个孩子根据所选择的逻辑对物品进行分组或 *clustering*。
- en: Formally put, *clustering* is used to group objects with similar attributes
    in the same segments, and the objects with different attributes in different segments.
    The resultant clusters share similarities within themselves while they are more
    heterogeneous between each other. We can understand it better by means of the
    following diagram (Figure 2.1).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，*clustering* 用于将具有相似属性的对象分组到同一段中，具有不同属性的对象分到不同段中。结果的聚类在自身之间具有相似性，而它们在彼此之间更加异质。我们可以通过下图（图2.1）更好地理解它。
- en: Figure 2.1 Clustering is grouping of objects with similar attributes into logical
    segments. The grouping is based on a similarity trait shared by different observations
    and hence they are grouped into a group. We are using shape as a variable for
    clustering here.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 聚类是将具有相似属性的对象分组到逻辑段中。分组是基于不同观测共享的相似性特征，因此它们被分组到一组中。在这里，我们使用形状作为聚类的变量。
- en: '![02_01](images/02_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![02_01](images/02_01.png)'
- en: Cluster analysis is not one individual algorithm or solution, rather it is used
    as a problem solving mechanism in practical business scenarios. They are a class
    of algorithms under unsupervised learning. It is an iterative process following
    a logical approach and qualitative business inputs. It results in generating a
    thorough understanding of the data, logical patterns in it, pattern discovery,
    and information retrieval. Being an unsupervised approach, clustering does not
    need a target variable. It performs segmenting by analysing underlying patterns
    in the dataset which are generally multi-dimensional and hence difficult to analyse
    with traditional methods.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析不是单个算法或解决方案，而是用于解决实际业务场景中的问题的机制。它们是无监督学习下的一类算法。这是一个迭代过程，遵循逻辑方法和定性业务输入。它导致对数据的深入理解、其中的逻辑模式、模式发现和信息检索。作为一种无监督方法，聚类不需要目标变量。它通过分析数据集中的潜在模式来进行分段，这些模式通常是多维的，因此难以用传统方法分析。
- en: 'Ideally we would want the clustering algorithms to have the following attributes:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望聚类算法具有以下特点：
- en: The output clusters should be easy to explain and comprehend, usable and should
    make business sense. The number of clusters should not be too little or too much.
    For example, if we have only 2 clusters the division is not clear and decisive.
    Or if we have 20 clusters, the handling will become a challenge.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出的聚类应该易于解释和理解，可用且应该具有商业意义。聚类数目不应该太少或太多。例如，如果我们只有2个聚类，分割就不够清晰和明确。或者如果我们有20个聚类，处理将变得很有挑战性。
- en: The algorithm should not be too sensitive to outliers or missing values or the
    noise in the dataset. Generally put, a good solution will be able to handle multiple
    data types.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法不应过于敏感于异常值、缺失值或数据集中的噪声。通常来说，一个好的解决方案应该能够处理多种数据类型。
- en: A good solution will require less domain understanding for the input parameters
    used for the clustering purpose. It allows analysts with less domain understanding
    to train the clustering algorithm.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个好的解决方案对于用于聚类目的的输入参数需要较少的领域理解。这允许具有较少领域理解的分析师训练聚类算法。
- en: The algorithm should be independent of the order of the input parameters. If
    the order matters, the clustering is biased on the order and hence it will add
    more confusion to the process.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法应该独立于输入参数的顺序。如果顺序很重要，那么聚类就会对顺序产生偏见，因此会给过程增加更多的混乱。
- en: As we generate new datasets continuously, the clusters have to be scalable to
    newer training examples and should not be a time-consuming process.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着我们持续生成新的数据集，聚类必须能够适应新的训练示例，并且不应该是一个耗时的过程。
- en: As one could imagine, the clustering output will depend on the attributes used
    for grouping. In the (Figure 2.2) shown below, there can be two logical groupings
    for the same dataset and both are equally valid. Hence, it is prudent that the
    attributes or *variables* for clustering are chosen wisely and often it depends
    on the business problem at hand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，聚类输出将取决于用于分组的属性。在下面所示的（图2.2）中，对于相同的数据集，可以有两个逻辑分组，而且两者都是同样有效的。因此，明智地选择用于聚类的属性或*变量*通常取决于手头的业务问题。
- en: Figure 2.2 Using different attributes for clustering results in different clusters
    for the same dataset. Hence, choosing the correct set of attributes define the
    final set of results we will achieve
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 使用不同的属性进行聚类会得到相同数据集的不同聚类结果。因此，选择正确的属性集定义我们将实现的最终结果。
- en: '![02_02](images/02_02.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![02_02](images/02_02.png)'
- en: Along with the attributes used in clustering, the actual technique used also
    makes a lot of difference. There are quite a few (in fact more than 80) clustering
    techniques researchers have worked upon. For the interested audience, we are providing
    a list of all the clustering algorithms in the Appendix. We are starting with
    understanding different clustering techniques in the next section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于聚类的属性之外，所使用的实际技术也会产生很大影响。研究人员已经开发了相当多（事实上超过80个）的聚类技术。对于感兴趣的观众，我们在附录中提供了所有聚类算法的列表。我们将在下一节开始学习不同的聚类技术。
- en: 2.2.1 Clustering techniques
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 聚类技术
- en: Clustering can be achieved using a variety of algorithms. These algorithms use
    different methodologies to define similarity between objects. For example, density
    based clustering, centroid based clustering, distribution based methods etc. Even
    to measure the distance between objects, there are multiple techniques like Euclidean
    distance, Manhattan distance etc. The choice of distance measurement leads to
    different similarity scores. We are going to study these similarity measurement
    parameters in a later section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以通过各种算法实现。这些算法使用不同的方法来定义对象之间的相似性。例如，基于密度的聚类，基于中心点的聚类，基于分布的方法等。甚至在衡量对象之间距离时，也有多种技术，如欧氏距离，曼哈顿距离等。选择距离测量方法会导致不同的相似度分数。我们将在后面的部分研究这些相似度测量参数。
- en: 'At a high level we can identify two broad clustering methods: *hard clustering*
    and *soft clustering* (see Figure 2.3). When the decision is quite clear that
    an object belongs to a certain class or cluster it is referred as Hard clustering.
    In hard clustering an algorithm is quite sure of an object’s class. On the other
    hand, soft clustering assigns a likelihood score for an object to belong to a
    particular cluster. So a soft clustering method will not put an object into a
    cluster, rather an object can belong to multiple clusters. Soft clustering sometimes
    is also called *fuzzy* clustering.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们可以确定两种广义的聚类方法：*硬聚类*和*软聚类*（见图2.3）。当决定非常明确一个对象属于某个类或簇时，称为硬聚类。在硬聚类中，算法非常确定对象的类。另一方面，软聚类为对象被归属于某个特定簇分配了可能性得分。因此，软聚类方法不会将对象放入一个簇中，而是一个对象可以属于多个簇。有时软聚类也被称为*模糊*聚类。
- en: Figure 2.3 Hard clustering has distinct clusters whereas in the case of soft
    clustering, a data point can belong to multiple clusters and we get likelihood
    score for a data point to belong to a cluster. The first figure on the left is
    hard clustering and the one on the right is soft clustering.
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 硬聚类具有明显的簇，而在软聚类的情况下，数据点可以属于多个簇，我们得到数据点属于簇的可能性分数。左边的第一个图是硬聚类，右边的图是软聚类。
- en: '![02_03](images/02_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![02_03](images/02_03.png)'
- en: 'We can broadly classify the clustering techniques as shown in the (Table 2.1)
    below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下图所示广义地将聚类技术进行分类：
- en: Table 2.1 Classification of clustering methodologies, brief descriptions and
    examples
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表2.1 聚类方法的分类、简要描述和示例
- en: '| S. No. | Clustering methodology | A brief description of the method | Example
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 序号 | 聚类方法 | 方法的简要描述 | 示例 |'
- en: '| 1 | Centroid based clustering | Distance from a defined centroid | k-means
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 基于质心的聚类 | 到指定质心的距离 | k-means |'
- en: '| 2 | Density based models | Data points are connected in dense regions in
    a vector space | DBSCAN, OPTICS |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 基于密度的模型 | 数据点在向量空间的密集区域内连接 | DBSCAN, OPTICS |'
- en: '| 3 | Connectivity based clustering | Distance connectivity is the modus operandi
    | Hierarchical clustering, BIRCH |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 基于连接性的聚类 | 距离连接性是行动方式 | 分层聚类, BIRCH |'
- en: '| 4 | Distribution models | Modelling is based on statistical distributions
    | Gaussian Mixture models |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 分布模型 | 建模基于统计分布 | 高斯混合模型 |'
- en: '| 5 | Deep learning models | Unsupervised neural network based | Self-organizing
    maps |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 深度学习模型 | 无监督的基于神经网络的 | 自组织映射 |'
- en: The methods described in (Table 2.1) are not the only ones which are available
    to be used. We can have graph-based models, overlapping clustering, subspace models
    etc.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1中描述的方法并不是唯一可用的方法。我们可以有基于图的模型、重叠的聚类、子空间模型等。
- en: 'Generally, the popular six algorithms used in clustering in the industry are
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，工业中使用的六种流行的聚类算法如下：
- en: K-means clustering (with variants like k-medians, k-medoids)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K均值聚类（带有诸如k-中值、k-中心点之类的变种）
- en: Agglomerative clustering or hierarchical clustering
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凝聚式聚类或者分层聚类
- en: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DBSCAN (基于密度的空间应用聚类)
- en: Spectral Clustering
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 光谱聚类
- en: Gaussian mixture models or GMM
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯混合模型或GMM
- en: BIRCH (Balanced Iterative Reducing & Clustering using Hierarchies)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BIRCH（平衡迭代减少 & 使用层次聚类）
- en: There are multiple other algorithms available like Chinese whisper, canopy clustering,
    SUBCLU, FLAME etc. We are studying the first three algorithms in this chapter
    and some of the advanced ones in subsequent chapters in the book.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 还有多种其他可用的算法，如Chinese whisper，canopy聚类，SUBCLU，FLAME等。我们在本章中学习前三种算法以及书中后续章节中的一些高级算法。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png)  快速测验-回答这些问题以检查你的理解.. 答案在书的末尾'
- en: 1.   DBSCAN clustering is centroid-based clustering technique. TRUE or FALSE.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1.   DBSCAN聚类是基于质心的聚类技术。TRUE or FALSE.
- en: 2.   Clustering is a supervised learning technique having a fixed target variable.
    TRUE or FALSE.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 2.   聚类是一种监督学习技术，具有固定的目标变量。TRUE or FALSE.
- en: 3.   What is the difference between hard clustering and soft clustering?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 3.   硬聚类和软聚类有什么区别？
- en: In the next section, we are starting with the centroid based clustering methods
    where we will study k-means clustering.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开始学习基于质心的聚类方法，其中我们将学习k-means聚类。
- en: 2.3 Centroid based clustering
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 基于质心的聚类
- en: Centroid (*see Appendix if you are not sure what is centroid*) based algorithms
    measure similarity of the objects based on their distance to the centroid of the
    clusters. The distance is measured between a specific data point to the centroid
    for the cluster. The smaller the distance is, higher is the similarity. We can
    understand the concept by looking at Figure 2.4 that follows. The figure on the
    right side represents the respective centroids for each of the group of clusters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重心的算法根据对象到聚类重心的距离来衡量它们的相似性。距离是针对聚类的特定数据点到重心的距离来衡量的。距离越小，相似度越高。我们可以通过接下来的图2.4来理解这个概念。右侧的图表示了每个聚类组的相应重心。
- en: Note
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: To get more clarity on the concept of centroid and other mathematical concepts,
    refer to the appendix at the end.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要更清楚地了解重心和其他数学概念，请参考末尾的附录。
- en: Figure 2.4 Centroid based clustering methods create a centroid for the respective
    clusters and the similarity is measured based on the distance from the centroid.
    In this case, we have 5 centroids. And hence, we have five distinct clusters here
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 基于重心的聚类方法为各自的聚类创建一个重心，并根据到重心的距离来衡量相似度。在这种情况下，我们有5个重心。因此，这里有五个不同的聚类。
- en: '![02_03a](images/02_03a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![02_03a](images/02_03a.png)'
- en: 'In clustering, distance plays a central part as many algorithms use it as a
    metric to measure the similarity. In centroid-based clustering, distance is measured
    between points and between centroids. There are multiple ways to measure the distance.
    The most widely used are listed below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，距离起着核心作用，因为许多算法将其用作度量相似性的度量。在基于重心的聚类中，距离是在点之间和重心之间进行衡量的。有多种方法来度量距离。最常用的是下面列出的：
- en: '**Euclidean distance**: It is the most common distance metric used. It represents
    the straight line distance between the two points in space and is the shortest
    path between the two points. If we want to calculate the distance between points
    P[1] and P[2] where coordinates of P[1] are (x[1], y[1]) and P[2] are (x[2], y[2]),
    then Euclidean distance is given by (Equation 2.1) below. The geometric representation
    is shown in Figure 2.5'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**欧几里德距离**：它是最常用的距离度量。它表示空间中两点之间的直线距离，是两点之间的最短路径。如果我们想计算点P[1]和P[2]之间的距离，其中P[1]的坐标为(x[1],
    y[1])，P[2]的坐标为(x[2], y[2])，那么欧几里德距离由下面的（方程式2.1）给出。几何表示如图2.5所示'
- en: (Equation 2.1)
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (方程式 2.1)
- en: Distance = √(y[2] – y[1])² + (x[2] – x[1])²
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 距离 = √(y[2] – y[1])² + (x[2] – x[1])²
- en: If you want to refresh the concepts of geometry (coordinate geometry) refer
    to the Appendix
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如果你想复习几何概念（坐标几何），请参考附录。
- en: '**Chebyshev distance**: Named after Russian mathematician *Pafnuty Chebyshev*,
    it is defined as the distance between two points such that their differences are
    maximum value along any co-ordinate dimension. Mathematically, we can represent
    Chebyshev distance in (Equation 2.2) below and shown in (Figure 2.5):'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**切比雪夫距离**：以俄罗斯数学家*帕夫努蒂·切比雪夫*的名字命名，它被定义为两点之间的距离，以便它们的差异在任何坐标维度上的最大值。数学上，我们可以在下面的（方程式2.2）中表示切比雪夫距离，并在（图2.5）中显示：'
- en: (Equation 2.2)
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (方程式 2.2)
- en: Distance [Chebyshev] = max (|y[2] – y[1]|, |x[2] – x[1]|)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 距离[切比雪夫] = max (|y[2] – y[1]|, |x[2] – x[1]|)
- en: '**Manhattan distance**: Manhattan distance is a very easy concept. It simply
    calculates the distance between two points in a grid-like path and the distance
    is hence measured along the axes at right angles. Hence, sometimes it is also
    referred to as city block distance or taxi cab metric. Mathematically, we can
    represent Manhattan distance in (Equation 2.3) and as shown in Figure 2.5:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**曼哈顿距离**：曼哈顿距离是一个非常简单的概念。它只是计算网格路径上两点之间的距离，因此距离是沿着右角轴测量的。因此，有时它也被称为城市街区距离或出租车度量。数学上，我们可以在（方程式2.3）中表示曼哈顿距离，并如图2.5所示：'
- en: (Equation 2.3)
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (方程式 2.3)
- en: Distance [Manhattan] = (|y[2] – y[1]| + |x[2] – x[1]|)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 距离[曼哈顿] = (|y[2] – y[1]| + |x[2] – x[1]|)
- en: Manhattan distance in L1 norm form while Euclidean distance is L2 norm form.
    You can refer to the Appendix to study the L1 norm and L2 norm in detail. If we
    have high number of dimensions or variables in the dataset, Manhattan distance
    is a better choice than Euclidean distance. This is due to *Curse of Dimensionality*
    which we will be studying in Chapter 3 of the book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离以L1范数形式而欧几里德距离以L2范数形式。您可以参考附录详细学习L1范数和L2范数。如果数据集中有大量的维度或变量，曼哈顿距离比欧几里德距离更好。这是由于本书第三章将要学习的*维度诅咒*。
- en: '**Cosine distance**: Cosine distance is used to measure the similarity between
    two points in a vector-space diagram. In trigonometry, cosine of 0 is 1 and cosine
    of 90^o is 0\. Hence, if two points are similar to each other, the angle between
    them will be zero hence cosine will be 1 which means the two points are very similar
    to each other, and vice versa. Mathematically, cosine similarity can be shown
    as (Equation 2.4). If we want to measure the cosine between two vectors A and
    B, then cosine is'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**余弦距离**：余弦距离用于测量向量空间图中两点之间的相似性。在三角学中，0度的余弦是1，90度的余弦是0。因此，如果两个点彼此相似，则它们之间的角度将为零，因此余弦将为1，这意味着这两个点彼此非常相似，反之亦然。从数学上讲，余弦相似性可以表示为（公式2.4）。如果我们想要测量向量A和B之间的余弦值，则余弦是'
- en: (Equation 2.4)
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （公式2.4）
- en: Distance [cosine] = (A . B) / (||A|| ||B||)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 距离 [余弦] = (A . B) / (||A|| ||B||)
- en: If you want to refresh the concepts of vector factorization refer to the Appendix.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如果您想刷新向量分解的概念，请参考附录。
- en: Figure 2.5 Euclidean distance, Manhattan distance, Chebyshev distance and cosine
    similarity are the primary distance metrics used. Note, how the distance is different
    for two points using these metrics. In Euclidean distance, the direct distance
    is measured between two points as shown by the first figure on the left.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.5 欧氏距离、曼哈顿距离、切比雪夫距离和余弦相似度是主要使用的距离度量。请注意，使用这些度量标准时，两个点之间的距离是不同的。在欧氏距离中，直接距离被测量为左侧第一张图所示的两点之间的距离。
- en: '![02_03b](images/02_03b.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![02_03b](images/02_03b.png)'
- en: There are other distance measuring metrics like Hamming distance, Jaccard distance
    etc. Mostly, we use Euclidean distance in our pragmatic business problems but
    other distance metrics are also used sometimes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他距离度量标准，如汉明距离、Jaccard距离等。在我们实际的业务问题中，通常使用欧氏距离，但有时也会使用其他距离度量标准。
- en: Note
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The above distance metrics are true for other clustering algorithms too. You
    are advised to test the Python codes in the book with different distance metrics
    and compare the performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述距离度量标准对其他聚类算法也适用。建议您使用本书中的Python代码测试不同的距离度量标准，并比较性能。
- en: Now we have understood the various distance metrics, we will proceed to study
    k-means clustering which is the most widely used algorithm.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了各种距离度量标准，我们将继续学习k-means聚类，这是最广泛使用的算法。
- en: 2.3.1 K-means clustering
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 K-means聚类
- en: k-means clustering is an easy and straightforward approach. It is arguably the
    most widely used clustering method to segment the data points and create non-overlapping
    clusters. We have to specify the number of clusters “k” we wish to create as an
    input and the algorithm will associate each observation to exactly one of the
    k clusters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类是一种简单直接的方法。它可以说是最广泛使用的聚类方法，用于分段数据点并创建非重叠聚类。我们必须指定我们希望创建的聚类数“k”作为输入，算法将将每个观察结果关联到k个聚类中的一个。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: k-means clustering is sometimes confused with k-nearest neighbor classifier
    (knn). Though there is some relationship between the two, knn is used for classification
    and regression problems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有时人们会将k-means聚类与k最近邻分类器（knn）混淆。虽然两者之间存在一定的关系，但knn用于分类和回归问题。
- en: It is quite an elegant approach and starts with some initial cluster centers
    and then iterates to assign each observation to the closest centre. In the process
    the centers are re-calculated as the mean of points in the cluster. Let’s study
    the approach used in step-by-step fashion by using the diagram in (Figure 2.6)
    below. For the sake of simplicity, we are assuming that there are three clusters
    in the dataset below.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种相当优雅的方法，它从一些初始的聚类中心开始，然后迭代地将每个观察结果分配给最接近的中心。在这个过程中，中心点被重新计算为聚类中的点的平均值。让我们通过下面的图表（图2.6）逐步学习所使用的方法。为了简单起见，我们假设数据集中有三个聚类。
- en: '**Step 1**: Let us assume that we have all the data points as shown below in
    Step 1.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1**：让我们假设我们有如下所示的所有数据点。'
- en: Figure 2.6 Step 1 represents the raw data set. In step 2, the algorithm initiates
    random three centroids as we have given the input of a number of clusters as three.
    In step 3, all the neighboring points of the centroids, are assigned the same
    cluster
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.6 步骤1代表原始数据集。在步骤2中，算法初始化三个随机质心，因为我们已经给定了三个聚类的输入数。在步骤3中，质心的所有相邻点都被分配到同一个聚类中。
- en: '![02_04](images/02_04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![02_04](images/02_04.png)'
- en: '**Step 2**: The *three* centers are initialized randomly as shown by three
    squares – blue, red and green. This input of three is the final number of clusters
    we wish to have.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二步**：初始时随机初始化了三个中心，如三个方块所示-蓝色、红色和绿色。这个三是我们希望最后的簇的数量。'
- en: '**Step 3**: The distance of all the data points is calculated to the centers
    and the points are assigned to the nearest centre. Note that the points have attained
    blue, red and green colors as they are nearest to those respective centers.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三步**：计算所有数据点到中心的距离，并将点分配给最近的中心。注意，由于它们最接近相应的中心，点的颜色变为蓝色、红色和绿色。'
- en: '**Step 4**: The three centers are re-adjusted in this step. The centers are
    re-calculated as the mean of the points in that cluster as shown in Figure 2.7\.
    We can see that in Step 4, the three squares have changed their respective positions
    as compared to Step 3.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**第四步**：在这一步中，重新调整了三个中心。中心被重新计算为该簇中点的平均值，如图2.7所示。我们可以看到，在第四步中，与第三步相比，三个方块的位置发生了变化。'
- en: Figure 2.7 The centroids are re-calculated in step 4\. In step 5, the data points
    are again re-assigned new centers. In step 6, the centroids are again re-adjusted
    as per the new calculations
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7是在第4步重新计算质心。在第5步中，数据点再次被重新分配新中心。在第6步中，根据新的计算结果，质心再次进行调整。
- en: '![02_05](images/02_05.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![02_05](images/02_05.png)'
- en: '**Step 5**: The distance of all the data points is re-calculated to the new
    centres and the points are reassigned to the nearest centres again. Note that
    two blue data points have become red while a red point has become green in this
    step.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**第五步**：再次重新计算所有数据点到新中心的距离，并将点重新分配给最近的中心。请注意，在这一步中，两个蓝色数据点变成了红色，而一个红点变成了绿色。'
- en: '**Step 6**: The centers are again re-adjusted as they were done in Step 4.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**第六步**：中心再次进行调整，与第四步类似。'
- en: Figure 2.8 The centroids are re-calculated and this process continues till we
    can no longer improve the clustering. And then the process stops as shown in step
    8
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8 是重新计算质心，并且这个过程一直持续到无法进一步改善聚类为止。然后过程停止，如第8步所示
- en: '![02_06](images/02_06.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![02_06](images/02_06.png)'
- en: '**Step 7**: The data points are again assigned a new cluster as shown in the
    preceding figure (Figure 2.8).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**第七步**：数据点再次被分配到新的簇中，如前图（图2.8）所示。'
- en: '**Step 8**: And the process will continue till the convergence is achieved.
    In other words, the process continues till there is no more reassignment of the
    data points. And hence, we cannot improve the clustering further and the final
    clustering is achieved.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**第八步**：该过程将继续，直到达到收敛。换句话说，该过程将继续，直到不再重新分配数据点。因此，我们无法进一步改进聚类，已实现最终聚类。'
- en: The objective of k-means clustering is to ensure that the within-cluster variation
    is as small as possible while the difference between clusters is as big as possible.
    In other words, the members of the same cluster are most similar to each other
    while members in different clusters are dissimilar. Once the results no longer
    change, we can conclude that a local optimum has been reached and clustering can
    stop. Hence, the final clusters are homogeneous within themselves while heterogeneous
    with each other.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类的目标是确保簇内变化尽可能小，而簇之间的差异尽可能大。换句话说，同一簇的成员彼此最相似，而不同簇的成员则不相似。一旦结果不再改变，我们可以得出结论，已达到局部最优，聚类可以停止。因此，最终的簇在内部是同质的，而在彼此之间是异质的。
- en: 'It is imperative to note two points here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有两点需要注意：
- en: Since k-means clustering initializes the centers randomly, hence it finds a
    local optimum solution rather than a global optimum solution. Hence, it is advisable
    to iterate the solution multiple times and choose the best output from all the
    results. By iteration, we mean to repeat the process multiple times as in each
    of the iteration, the centroid chosen randomly will be different.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 k-means 聚类是随机初始化中心点，因此它只能找到局部最优解，而非全局最优解。因此，建议多次迭代解决方案，并从所有结果中选择最佳输出。迭代意味着多次重复该过程，因为在每次迭代中，随机选择的质心将不同。
- en: We have to input the number of final clusters “k” we wish to have and it changes
    the output drastically. A very small value of k relative to the data size, will
    result in redundant clusters as there will not be any use. Or in other words,
    if we have a very small value of k relative to a big sized data, data points with
    different characteristics will be cobbled together in a few groups. Having a very
    high value of k, will create clusters which are different from each other minutely.
    Moreover, having a very high number of clusters will be difficult to manage and
    refresh in the long run. Let’s study by an example. If a telecom operator has
    1 million subscribers, then if we take number of clusters as 2 or 3, the resultant
    cluster size will be very large. It can also lead to different customers classified
    in the same segment. On the other hand, if we take the number of clusters as 50
    or 60, due to the sheer number of clusters – the output becomes unmanageable to
    manage, analyze and maintain.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须输入我们希望的最终簇数“k”，它会显著改变输出。与数据规模相比，如果k值非常小，结果将是多余的簇，因为没有任何用处。换句话说，如果相对于庞大的数据具有非常小的k值，具有不同特征的数据点将被混合在几个群中。具有非常高的k值将创建微不同的簇。此外，具有非常高数量的簇将难以在长期内管理和更新。让我们通过一个例子来研究。如果一个电信运营商有100万订阅者，那么如果我们将簇数设为2或3，得到的簇的规模将非常大。它还可能导致将不同的客户分类到相同的段中。另一方面，如果我们将簇数设为50或60，由于簇数庞大，输出变得难以管理、分析和维护。
- en: With different values of “k” we get different results, hence it is necessary
    that we understand how we can choose the optimum number of clusters for a dataset.
    Now, lets examine the process to measure the accuracy of clustering solutions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的“k”值，我们会得到不同的结果，因此我们需要了解如何为数据集选择最佳簇数。现在，让我们研究如何测量聚类解决方案的准确性的过程。
- en: 2.3.2 Measure the accuracy of clustering
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 测量聚类的准确性
- en: One objective of clustering is to find the cleanest clusters. Theoretically
    (though not ideal), if we have the same number of clusters as the number of observations
    the results will be completely accurate. In other words, if we have 1 million
    customers, the purest clustering will have 1 million clusters – wherein each customer
    is in a separate cluster. But it is not the best approach and is not a pragmatic
    solution. Clustering intends to create group of similar observations in one cluster
    and we use the same principle to measure the accuracy of our solution.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的一个目标是找到最干净的簇。理论上（虽然不理想），如果我们有与观察数量相同的簇数，结果将完全准确。换句话说，如果有100万客户，最纯净的聚类将有100万个簇
    - 每个客户在一个单独的簇中。但这并不是最佳方法，也不是一种实用的解决方案。聚类旨在在一个簇中创建相似观察结果的组，并且我们使用相同的原理来衡量解决方案的准确性。
- en: '**Within the cluster sum of squares (WCSS) or Cohesion**: This index measures
    the variability of the data points with respect to the distance they are from
    the centroid of the cluster. This metric is average distance of each data point
    from the cluster’s centroid, which is repeated for each data point. If the value
    is too large, it shows there is a large data spread whereas the smaller value
    indicates that the data points are quite similar and homogeneous and hence the
    cluster is compact.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**簇内平方和(WCSS)或聚合**：这个指标衡量数据点相对于它们距离簇质心的距离的变异性。这个指标是每个数据点距离簇的质心的平均距离，对每个数据点重复。如果值太大，表明数据扩散很大，而较小的值表示数据点非常相似和均匀，因此簇是紧凑的。'
- en: Sometimes, this intracluster distance is also referred to as *inertia* for that
    cluster. It is simply the summation of all the distances. Lower the value of inertia,
    better the cluster is.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，这个簇内距离也被称为该簇的*惯性*。它简单地是所有距离的总和。惯性值越低，簇就越好。
- en: Figure 2.9 Intra cluster vs inter cluster distance – both are used to measure
    the purity of the final clusters and the performance of the clustering solution
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9 簇内距离与簇间距离 - 两者都用于衡量最终簇的纯度和聚类解决方案的性能
- en: '![02_07](images/02_07.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![02_07](images/02_07.png)'
- en: '**Inter cluster sum of squares**: This metric is used to measure the distance
    between centroids of all the clusters. To get it, we measure the distance between
    centroids of all the clusters and divide it by the number of clusters to get the
    average value. The bigger it is, better is the clustering indicating that clusters
    are heterogeneous and distinguishable from each other as we have represented in
    (Figure 2.9).'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚类间平方和**：此度量用于衡量所有聚类质心之间的距离。为了得到它，我们测量所有聚类的质心之间的距离，并将其除以聚类的数量以获得平均值。它越大，聚类越好，表明聚类是异质的，并且彼此可以区分，正如我们在（图2.9）中所表示的那样。'
- en: '**Silhouette Value** is one of the metrics used to measure the success of clustering.
    It ranges from -1 to +1 and a higher value is better. It measures how an data
    point is similar to other data points in its own cluster as compared to other
    clusters. As a first step, for each observation - we calculate the average distance
    from all the data points in the same cluster, let’s call is x[i]. Then we calculate
    the average distance from all the data points in the nearest cluster, let’s call
    it y[i]. We will then calculate the coefficient by the equation (Equation 2.5)
    below'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**轮廓系数**是衡量聚类成功的指标之一。它的取值范围从-1到+1，数值越高越好。它衡量了数据点与其所属聚类中其他数据点相似程度，与其他聚类相比。作为第一步，对于每个观察值——我们计算与同一聚类中所有数据点的平均距离，我们称之为x[i]。然后我们计算与最近聚类中所有数据点的平均距离，我们称之为y[i]。然后我们通过下面的方程（方程2.5）计算系数'
- en: (Equation 2.5)
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程2.5）
- en: Silhouette Coefficient = (y[i] – x[i])/ max (y[i], x[i])
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数 = （y[i] - x[i]）/ max（y[i]，x[i]）
- en: If the value of coefficient is -1, it means that the observation is in the wrong
    cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系数的值为-1，则意味着观察点位于错误的聚类中。
- en: If it is 0, the observation is very close to the neighboring clusters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为0，则该观察点与相邻聚类非常接近。
- en: If the values of coefficient +1, it means that the observation is at a distance
    from the neighboring clusters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系数的值为+1，则意味着该观察点与相邻聚类之间存在距离。
- en: Hence, we would expect to get the highest value for the coefficient to have
    a good clustering solution.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们期望获得系数的最高值以获得良好的聚类解决方案。
- en: '**Dunn Index** can also be used to measure the efficacy of the clustering.
    It uses the inter and intra distance measurements defined in point 2 and point
    3 above and is given by the (Equation 2.6) below'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dunn指数**也可用于衡量聚类的效果。它使用了点2和点3中定义的聚类间距离和聚类内距离测量，并由下面的方程（方程2.6）给出'
- en: (Equation 2.6)
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程2.6）
- en: Dunn Index = min (Inter cluster distance)/max (Intra cluster distance)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Dunn指数 = min（聚类间距离）/max（聚类内距离）
- en: Clearly, we would strive to maximize the value of Dunn index. To achieve it,
    the numerator should be as big as possible implying that clusters are at a distance
    from each other, while the denominator should be as low as possible signifying
    that the clusters are quite robust and close-packed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们会努力最大化Dunn指数的值。为了实现这一点，分子应尽可能大，意味着聚类之间相距较远，而分母应尽可能低，表明聚类非常健壮且紧密排列。
- en: Now we have examined the methods to measure the performance of our algorithm.
    We will now move to find out the best value of “k” for k-means clustering.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检验了衡量算法性能的方法。我们现在将转向寻找k-means聚类的最佳“k”值。
- en: 2.3.3 Finding the optimum value of “k”
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 寻找最佳的“k”值
- en: Choosing the most optimum number of clusters is not easy. As we have said earlier,
    the finest clustering is when the number of clusters equals the number of observations
    – but as we studied in the last section, it is not practically possible. But we
    have to provide the number of clusters “k” as an input to the algorithm.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最优的聚类数量并不容易。正如我们之前所说，最好的聚类是当聚类数量等于观察数量时——但是正如我们在上一节中所学到的，这在实际中是不可能的。但是我们必须将聚类数量“k”作为算法的输入提供。
- en: Figure 2.10 Elbow method to find the optimal number of clusters. The red circle
    shows the kink. But the final number of clusters is dependent on business logic
    and often we merge/split clusters as per business knowledge. Ease to maintain
    the clusters also plays a crucial role in the same
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10 肘方法寻找最优聚类数量。红色圆圈表示转折点。但最终的聚类数量取决于业务逻辑，通常根据业务知识合并/拆分聚类。维护聚类的便利性在其中也扮演了至关重要的角色
- en: '![02_08](images/02_08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![02_08](images/02_08.png)'
- en: Perhaps the most widely used method for finding the optimum value of “k” is
    the *Elbow Method.* In this method, we calculate within the cluster sum of squares
    or WCSS for different values of “k”. The process is the same as discussed in the
    last section. Then, WCSS is plotted on a graph against different values of “k”.
    Wherever we observe a kink or elbow, as shown in (Figure 2.10), it is the most
    optimum number of clusters for the dataset. Notice the sharp edge depicted in
    (Figure 2.10).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 或许找到“k”的最优值最广泛使用的方法是*Elbow Method*。在这种方法中，我们计算不同“k”值的簇内平方和或 WCSS。该过程与上一节中讨论的相同。然后，将
    WCSS 绘制在图表上，针对不同的“k”值。在我们观察到一个弯曲或肘部的地方，如（图2.10）所示，它就是数据集的最优簇数。注意（图2.10）中描绘的锋利边缘。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 测验 - 回答这些问题来检查你的理解。书的结尾有答案。'
- en: 1.   K-means clustering does not require number of clusters as an input- TRUE
    or FALSE
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 1. K-means 聚类不需要作为输入的簇数- 真 或 假
- en: 2.   Knn and k-means clustering are one and the same thing – TRUE or FALSE
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 2. Knn 和 k-means 聚类是一回事 - 真 或 假
- en: 3.   Describe one possible process to find the most optimal value of “k”
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 描述一种可能的找到最优“k”值的过程
- en: But it does not mean that it is the final number of clusters we suggest for
    the business problem. Based on the number of observations falling in each of the
    clusters, a few clusters might be combined or broken into sub-clusters. We also
    consider the computation cost required to create the clusters. Higher the number
    of clusters, greater is the computation cost and the time required.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不意味着这是我们建议用于业务问题的最终簇数。根据落入每个簇的观测数量，几个簇可能被合并或分解成子簇。我们还考虑创建簇所需的计算成本。簇的数量越多，计算成本和所需时间就越大。
- en: We can also find the optimum number of clusters using the Silhouette Coefficient
    we discussed earlier.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用前面讨论过的 Silhouette Coefficient 找到最优的簇数。
- en: 'Note:'
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意：
- en: It is imperative that business logic of merging a few clusters or breaking a
    few clusters is explored. Ultimately, the solution has to be implemented in real-world
    business scenarios.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 探讨合并几个簇或拆分几个簇的业务逻辑是至关重要的。最终，解决方案必须在实际的业务场景中实施。
- en: With this, we have examined nuts and bolts of k-means clustering – the mathematical
    concepts and the process, the various distance metrics and determining the best
    value of k. We will now study the advantages k-means algorithm offers to us.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们已经研究了 k-means 聚类的点点滴滴 - 数学概念和过程，各种距离度量以及确定最佳 k 值。现在我们将研究 k-means 算法为我们提供的优势。
- en: 2.3.4 Pros and cons of k-means clustering
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.4 k-means 聚类的优缺点
- en: 'k-means algorithm is quite a popular and widely implemented clustering solution.
    The solution offers the following advantages:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法是一个非常流行和广泛实施的聚类解决方案。该解决方案提供以下优势：
- en: It is simple to comprehend and relatively easier to implement as compared to
    other algorithms. The distance measurement calculation makes it quite intuitive
    to understand even by users from non-statistics backgrounds.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他算法相比，它简单易懂且相对容易实现。距离测量计算使得即使是非统计背景的用户也很容易理解。
- en: If the number of dimensions is large, k-means algorithm is faster than other
    clustering algorithms and creates tighter clusters. It is hence preferred if the
    number of dimensions are quite big.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果维度的数量很大，k-means 算法比其他聚类算法更快，创建的簇更紧凑。因此，如果维度的数量相当大，则更倾向于使用它。
- en: It quickly adapts to new observations and can generalize very well to clusters
    of various shapes and sizes.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它很快适应新的观察结果，并且能够非常好地概括各种形状和大小的簇。
- en: The solution produces results through a series of iterations of re-calculations.
    Most of the time the Euclidean distance metric is used which makes it less computationally
    expensive. It also ensures that the algorithm surely converges and produces results.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决方案通过一系列重新计算的迭代产生结果。大多数情况下使用欧氏距离度量，这使得计算成本较低。它还确保算法一定会收敛并产生结果。
- en: 'K-means is widely used for real life business problems. Though there are clear
    advantages of k-means clustering, we do face certain challenges with the algorithm:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 在现实生活中的业务问题中被广泛使用。尽管 k-means 聚类有明显的优点，但我们确实面临着算法的某些挑战：
- en: Choosing the most optimum number of clusters is not easy. We have to provide
    it as an input. With different values of “k”, the results will be completely different.
    The process to choose the best value of “k” is explored in the previous section.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳聚类数目并不容易。我们必须将其作为输入提供。使用不同的“k”值，结果会完全不同。选择最佳“k”值的过程在上一节中已经探讨过。
- en: The solution is dependent on the initial values of centroids. Since the centroids
    are initialized randomly, the output will be different with each iteration. Hence,
    it is advisable to run multiple versions of the solution and choose the best one.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决方案取决于质心的初始值。由于质心是随机初始化的，因此每次迭代的输出都将不同。因此，建议运行多个解决方案的版本并选择最佳的一个。
- en: The algorithm is quite sensitive to outliers. They can mess up the final results
    and hence it is imperative that we treat outliers before starting with clustering.
    We can also implement other variants of k-means algorithm like *k-modes* clustering
    to deal with the issue of outliers. We are discussing dealing with outliers in
    subsequent chapters.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对异常值非常敏感。它们可能会破坏最终结果，因此在开始聚类之前，我们必须处理异常值。我们还可以实现k均值算法的其他变体，如*k-modes*聚类，以应对异常值的问题。我们将在后续章节讨论处理异常值的方法。
- en: Since the basic principle of k-means clustering is to calculate the distance,
    hence the solution is not directly applicable for categorical variables. Or in
    other words, we cannot use categorical variables directly, since we can calculate
    the distance between numeric values but cannot perform mathematical calculations
    on categorical variables. To resolve it, we can convert categorical variables
    to numeric ones using one-hot encoding which we are discussing towards the end
    of this chapter.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于k均值聚类的基本原理是计算距离，因此该解决方案不直接适用于分类变量。换句话说，我们不能直接使用分类变量，因为我们可以计算数字值之间的距离，但不能对分类变量进行数学计算。为了解决这个问题，我们可以使用独热编码将分类变量转换为数字变量，这是我们在本章末尾讨论的内容之一。
- en: Despite these problems, k-means clustering is one of the most used clustering
    solutions owing to its simplicity and ease to implement. There are different implementations
    of k-means algorithm like k-medoids, k-median etc. which are sometimes used to
    resolve the problems faced.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些问题，k均值聚类是最常用的聚类解决方案之一，因其简单性和易于实现。还有一些不同版本的k均值算法，如k-medoids、k-中位数等，有时用于解决所面临的问题。
- en: As the name suggests, **k-median clustering** is based on medians of the dataset
    as compared to centroid in k-means. This increases the amount of computation time
    as median can be found only after the data has been sorted. But at the same time,
    k-means is sensitive to outliers whereas k-medians is less affected by them.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如其名，**k-中位数聚类**是基于数据集中位数而不是k均值中心的。这增加了计算时间，因为只有在数据排序之后才能找到中位数。但与此同时，k均值对异常值很敏感，而k中位数对它们的影响较小。
- en: Next, we have **k-medoids clustering** as one of the variants of the k-means
    algorithm. Medoids are similar to means except they are always from the same dataset
    and are implemented when it is difficult to get means like images. A medoid can
    be thought as the most central point in a cluster which is least dissimilar to
    all the other members in the cluster. K-medoids choose the actual observations
    as the centers as compared to k-means where the centroids may not even be part
    of the data. It is less sensitive to outliers as compared to k-means clustering
    algorithm.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们有**k-medoids聚类**作为k均值算法的变体之一。Medoids与均值类似，只是它们始终来自同一数据集，并且在难以获得均值的情况下实施，比如图像。Medoid可以被认为是簇中最核心的点，与簇中的所有其他成员最不相似。K-medoids选择实际观测值作为中心，而不是k均值，其中质心甚至可能不是数据的一部分。与k均值聚类算法相比，它对异常值的敏感性较低。
- en: There are other versions too like kmeans++, mini-batch k-means etc. Generally,
    in the industry kmeans is used for most of the clustering solutions. You can explore
    other options like kmeans++, mini-batch kmeans etc. if the results are not desirable
    or if the computation is taking a lot of time. Moreover, having different distance
    measurement metrics may produce different results for k-means algorithm.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他版本，如kmeans++、小批量k均值等。一般来说，在工业界，大多数聚类解决方案都使用k均值。如果结果不理想或计算时间太长，您可以探索其他选项，如kmeans++、小批量k均值等。此外，使用不同的距离测量指标可能会为k均值算法产生不同的结果。
- en: This section concludes our discussion on k-means clustering algorithm. It is
    time to hit the lab and develop actual Python code!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本节结束了我们对k-means聚类算法的讨论。是时候进入实验室，开发真正的Python代码了！
- en: 2.3.5 k-means clustering implementation using Python
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 使用Python实现k-means聚类
- en: We will now create a Python solution for k-means clustering. In this case, we
    are using the dataset from the link at github at
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为k-means聚类创建一个Python解决方案。在这种情况下，我们使用了链接中的数据集
- en: '[https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter2](main.html)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter2](main.html)'
- en: This dataset has information about features of four models of cars. Based on
    the features of the car, we are going to group them into different clusters.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含了四种车型的特征信息。基于车辆的特征，我们将把它们分成不同的群。
- en: '**Step 1**: Import the libraries and the dataset into a dataframe. Here, vehicles.csv
    is the input data file. If the data file is not in the same folder as the Jupyter
    notebook, you would have to provide the complete path to the file. Dropna is used
    to remove the missing values, if any.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1步：** 将库和数据集导入到一个数据框中。在这里，vehicles.csv是输入数据文件。如果数据文件不在与Jupyter笔记本相同的文件夹中，您需要提供文件的完整路径。Dropna用于删除可能存在的缺失值。'
- en: '[PRE0]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2:** Perform some initial checks on the data, like shape, info, top
    five rows, distribution of classes etc. This is to ensure that we have loaded
    the complete dataset and there is no corruption while loading the dataset. `Shape`
    command will give the number of rows and columns in the data, `info` will describe
    all the variables and their respective types and `head` will display the first
    5 rows. The `value_counts` displays the distribution for the `class` variable.
    Or in other words, `value_counts` returns the count of the unique values.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2步：** 对数据进行一些初始检查，比如形状、信息、前五行、类别分布等。这是为了确保我们已经加载了完整的数据集，并且在加载数据集时没有损坏。`Shape`命令将给出数据中的行数和列数，`info`将描述所有变量及其类型，`head`将显示前5行。`value_counts`显示`class`变量的分布。换句话说，`value_counts`返回唯一值的计数。'
- en: '[PRE1]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3:** Let’s generate two plots for the variable `“class”`. The dataset
    has more examples from car while for bus and van it is a balanced data. We have
    used matplotlib library to plot these graphs. The output of the plots are shown
    below.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3步：** 让我们为变量`“class”`生成两个图表。数据集中的汽车示例更多，而对于公共汽车和货车，数据是平衡的。我们使用matplotlib库来绘制这些图表。图表的输出如下所示。'
- en: '[PRE2]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![02_08a](images/02_08a.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![02_08a](images/02_08a.png)'
- en: '**Step 4:** We will now check if there are any missing data points in our dataset.
    There are no missing data points in our dataset as we have already dealt with
    them.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**第4步：** 现在我们将检查数据集中是否有任何缺失的数据点。我们的数据集中没有缺失的数据点，因为我们已经处理过了。'
- en: '[PRE3]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: We will be discussing the methods to deal with missing values in later chapters
    as dropping the missing values is generally not the best approach.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中讨论处理缺失值的方法，因为删除缺失值通常不是最好的方法。
- en: '**Step 5:** We will standardize our dataset now. It is a good practice to standardize
    the dataset for clustering. It is important as the different dimensions might
    be on a different scale, and one dimension may dominate the computation of distance
    if its values are naturally much larger than other dimensions. This is done using
    `zscore` and `StandardScaler()` function below. Refer to the appendix of the book
    to examine the difference between `zscore` and `StandardScaler()` function.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**第5步：** 现在我们将对数据集进行标准化。对于聚类来说，标准化数据集是一个很好的实践。这是很重要的，因为不同的维度可能处于不同的尺度，如果某个维度的值自然上比其他维度的值要大得多，那么一个维度可能会在距离计算中占据主导地位。下面使用`zscore`和`StandardScaler()`函数来实现。请参考书的附录，了解`zscore`和`StandardScaler()`函数之间的区别。'
- en: '[PRE4]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Step 6:** We will now have a quick look at the dataset by generating a scatter
    plot. The plot displays the distribution of all the data points we have created
    as `X_standard` in the last step.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**第6步：** 我们现在要快速查看数据集，生成一个散点图。该图显示了我们在上一步中创建的`X_standard`的所有数据点的分布。'
- en: '[PRE5]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![02_08b](images/02_08b.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![02_08b](images/02_08b.png)'
- en: '**Step 7:** We will now perform k-means clustering. First, we have to select
    the optimum number of clusters using the elbow method. From the sklearn library,
    we are importing KMeans. In a for loop, we iterate for the values of clusters
    from 1 to 10\. In other words, the algorithm will create 1, 2,3, 4 up to 10 clusters
    and will then generate the results for us to choose the most optimal value of
    k.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 7：** 我们现在将执行 k-means 聚类。首先，我们必须使用肘部法选择最佳聚类数。从 sklearn 库中，我们导入 KMeans。在一个
    for 循环中，我们对聚类值从 1 到 10 进行迭代。换句话说，算法将创建 1、2、3、4 到 10 个聚类，然后生成结果供我们选择最优 k 值。'
- en: In the code snippet below, the model object contains the output of the KMeans
    algorithm which is then fit on the X_standard generated in the last step. Here,
    Euclidean distance has been used as a distance metric.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，模型对象包含了在上一步生成的 X_standard 上适配的 KMeans 算法的输出。这里，欧几里得距离被用作距离度量。
- en: '[PRE6]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![02_08c](images/02_08c.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![02_08c](images/02_08c.png)'
- en: '**Step 8:** As we can observe, the optimal number of clusters is 3\. It is
    the point, where we can observe a sharp kink in the graph. We will continue with
    k-means clustering with a number of clusters as 3\. While there is nothing special
    about the number 3 here, it is best suited for this dataset, we might also use
    4 or 5 for the number of clusters. `random_state` is a parameter that is used
    to determine random numbers for centroid initialization. We are setting it to
    a value to make randomness deterministic.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 8：** 正如我们所观察到的，最佳聚类数为 3\. 这是一个尖锐转折点，在图表中清晰可见。我们将使用 3 个聚类数进行 k-means 聚类。尽管这里的数字
    3 没有特别之处，但它最适合这个数据集，我们也可以使用 4 或 5 个聚类数。`random_state` 是一个用于确定质心初始化的参数。我们将其设置为一个值以使随机性变得确定性。'
- en: '[PRE7]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Step 9:** Get the centroids for the clusters'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 9：** 获取聚类的质心'
- en: '[PRE8]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Step 10:** Now we are using the centroids so that they can be profiled by
    the columns.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 10：** 现在我们将使用质心，以便它们可以按列进行分析。'
- en: '[PRE9]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Step 11:** We will now create a `dataframe` only for the purpose of creating
    the labels and then we are converting it into categorical variables.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 11：** 我们现在将创建一个仅用于创建标签的 `dataframe`，然后将其转换为分类变量。'
- en: '[PRE10]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Step 12:** In this step, we are joining the two `dataframes`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 12：** 在这一步中，我们将两个 `dataframes` 进行连接'
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Step 13:** A group by is done to create a data frame requires for the analysis'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 13：** 执行 group by 操作以创建用于分析的数据框'
- en: '[PRE12]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step 14:** Now, will create a visualization for the clusters we have defined.
    This is done using the `mpl_toolkits` library. The logic is simple to understand.
    The data points are coloured as per the respective labels. The rest of the steps
    are related to the display of plot by adjusting the label, title, ticks etc. Since
    it is not possible to plot all the 18 variables in the plot, we have chosen 3
    variables to show in the plot.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 14：** 现在，我们将为我们定义的聚类创建可视化。这是使用 `mpl_toolkits` 库完成的。逻辑很容易理解。数据点根据相应的标签着色。其余的步骤与调整标签、标题、刻度等有关的细节。由于在绘图中不可能绘制所有的
    18 个变量，我们选择了 3 个变量来显示在图中。'
- en: '[PRE13]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![02_08d](images/02_08d.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![02_08d](images/02_08d.png)'
- en: We can also test the above code with multiple other values of k. We have created
    the code with different values of k. In the interest of space, we have put the
    code of testing with different values of k at the github location.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用多个其他值对上述代码进行测试。我们已经创建了使用不同值的代码。出于空间考虑，我们将不同值的测试代码放在了github位置。
- en: In the example above, we first did a small exploratory analysis of the dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们首先对数据集进行了小型的探索性分析。
- en: Note
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Exploratory data analysis (EDA) holds the key to a robust machine learning solution
    and a successful project. In the subsequent chapters, we will create detailed
    EDA for datasets.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）是实现稳健机器学习解决方案和成功项目的关键。在随后的章节中，我们将为数据集创建详细的 EDA。
- en: It was followed by identifying the optimum number of clusters which in this
    case comes out to be three. Then we implemented k-means clustering. You are expected
    to iterate the k-means solution with different initializations and compare the
    results, iterate with different values of k, and visualize to analyze the movements
    of data points. We will be using the same dataset later in the chapter where we
    will create hierarchical clustering using Python.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是确定最佳聚类数，本例中为三。然后我们实施了 k-means 聚类。您应该迭代不同的初始化 k-means 解决方案并比较结果，迭代不同的 k 值，并可视化以分析数据点的移动。在本章后面，我们将使用
    Python 创建层次聚类的相同数据集。
- en: Centroid-based clustering is one of the most recommended solutions owing to
    its less complicated logic, ease to implement, flexibility and trouble-free maintenance.
    Whenever we require clustering as a solution, mostly we start with creating a
    k-means clustering solution that acts as a benchmark. The algorithm is highly
    popular and generally one of the first solutions utilized for clustering. Then
    we test and iterate with other algorithms.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 基于质心的聚类是最推荐的解决方案之一，因为其逻辑较少，易于实现，灵活性高且易于维护。每当我们需要聚类作为解决方案时，大多数情况下我们都会从创建一个作为基准的k均值聚类解决方案开始。该算法非常受欢迎，通常是聚类的首选解决方案之一。然后我们会测试并迭代其他算法。
- en: This marks the end of the discussion on centroid-based clustering algorithms.
    We will now move forward to connectivity-based solutions and discuss hierarchical
    clustering in the next section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着对基于质心的聚类算法的讨论结束。我们现在将继续前进，讨论连接性解决方案，并在下一节中讨论层次聚类。
- en: 2.4 Connectivity based clustering
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 基于连接性的聚类
- en: “Birds of the same feather flock together” is the principle followed in connectivity-based
    clusters. The core concept is - objects which are connected with each other are
    similar to each other. Hence, based on the connectivity between these objects
    they are clubbed into clusters. An example of such a representation is shown in
    Figure 2.11, where we can iteratively group observations. As an example, we are
    initiating with all things, dividing into living and non-living and so on. Such
    representation is better shown using the diagram on the right, called the *Dendrogram.*
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: “物以类聚”是连接性聚类中遵循的原则。核心概念是-彼此连接的对象相似。因此，根据这些对象之间的连接性，它们被组合成簇。图2.11中展示了这种表示的一个示例，我们可以迭代地对观察进行分组。例如，我们从所有事物开始，分成生物和非生物等等。这种表示最好使用右侧的图表展示，称为*Dendrogram*。
- en: Figure 2.11 Hierarchical clustering utilizes grouping similar objects iteratively.
    On the right, we have the visual representation of the clustering called dendrogram
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.11 层次聚类利用迭代地对相似对象进行分组。右侧是聚类的可视化表示，称为树状图。
- en: '![02_09](images/02_09.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![02_09](images/02_09.png)'
- en: Since there is a tree-like structure, connectivity-based clustering is sometimes
    referred as *Hierarchical* clustering.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在树状结构，连接性聚类有时被称为*层次*聚类。
- en: Hierarchical clustering fits nicely into human intuition, and hence is easy
    to comprehend by us. Unlike k-means clustering, in hierarchical clustering, we
    do not have to input the number of final clusters but the method does require
    a termination condition i.e. when the clustering should stop. At the same time,
    hierarchical clustering does not suggest the optimum number of clusters. From
    the hierarchy/ dendrogram generated, we have to choose the best number of clusters
    ourselves. We will explore more on it when we create the Python code for it in
    subsequent sections.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类很好地符合人类直觉，因此我们容易理解它。与k均值聚类不同，在层次聚类中，我们不必输入最终聚类的数量，但该方法确实需要一个终止条件，即聚类应何时停止。同时，层次聚类不建议聚类的最佳数量。从生成的层次结构/树状图中，我们必须自己选择最佳的聚类数。在接下来的部分中，当我们为其创建Python代码时，我们将更多地探讨它。
- en: Hierarchical clustering can be understood by means of Figure 2.12, which follows.
    Here the first node is the root, which is then iteratively split into nodes and
    subnodes. Whenever a node cannot be split further, it is called a terminal node
    or *leaf*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类可以通过图2.12来理解，如下所示。这里第一个节点是根节点，然后迭代地分裂成节点和子节点。每当一个节点不能再进一步分裂时，它被称为终端节点或*叶子*。
- en: Figure 2.12 Hierarchical clustering has a root that splits into nodes and subnodes.
    A node that cannot be split further is called the leaf. In the bottom-up approach,
    merging of the leaves will take place
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12 层次聚类具有一个根节点，分裂为节点和子节点。不能再进一步分裂的节点称为叶子。在自底向上的方法中，将合并叶子节点
- en: '![02_10](images/02_10.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![02_10](images/02_10.png)'
- en: 'Since there is more than one process or logic to merge the observations into
    clusters, we can generate a large number of dendrograms which is given by (Equation
    2.7) below:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将观察结果合并成簇的过程或逻辑不止一个，我们可以生成大量的树状图，如下所示的(Equation 2.7)：
- en: (Equation 2.7)
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (Equation 2.7)
- en: Number of dendrograms = (2n-3)!/[2^((n-2)) (n-2)!]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图数量 = (2n-3)!/[2^((n-2)) (n-2)!]
- en: where n is the number of observations or the leaves. So if we have only 2 observations,
    we can have only 1 dendrogram. If we have 5 observations, we can have 105 dendrograms.
    Hence, based on the number of observations we can generate a lot of dendograms.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 n 是观察数或叶子数。所以如果我们只有2个观察，我们只能有1个树状图。如果有5个观察，我们可以有105个树状图。因此，根据观察数，我们可以生成大量的树状图。
- en: Hierarchical clustering can be further classified based on the process used
    to create grouping of observations, which we are exploring next.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用于创建观察值分组的过程，层次聚类可以进一步分类，我们将在下面探讨这一点。
- en: 2.4.1 Types of hierarchical clustering
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 层次聚类的类型
- en: 'Based on the strategy to group, hierarchical clustering can be subdivided into
    two types: *agglomerative* clustering and *divisive* clustering.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分组策略，层次聚类可以细分为两种类型：*聚合*聚类和*分裂*聚类。
- en: '| S.No. | Agglomerative methodology | Divisive methodology |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 序号 | 聚合方法 | 分裂方法 |'
- en: '| 1 | Bottom-up approach | Top-down approach |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 自底向上方法 | 自顶向下方法 |'
- en: '| 2 | Each observation creates its own cluster and then merging takes place
    as the algorithm goes up | We start with one cluster and then observations are
    iteratively split to create a tree-like structure |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 每次观察都会创建自己的群集，然后随着算法的进行而进行合并 | 我们从一个群集开始，然后观察被迭代地分割以创建类似树状的结构 |'
- en: '| 3 | Greedy approach is followed to merge (greedy approach is described below)
    | Greedy approach is followed to split |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 采用贪婪方法进行合并（下面描述了贪婪方法） | 采用贪婪方法进行分割 |'
- en: '| 4 | An observation will find the best pair to merge and the process completes
    when all the observations have merged with each other | All the observations are
    taken at the start and then based on division conditions, splitting takes place
    until all the observations are exhausted or the termination condition is met |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 观察会找到最佳配对进行合并，当所有观察都与彼此合并时，流程完成 | 一开始会取全部观察，然后根据分裂条件进行分割，直到所有观察都被用完或达到终止条件
    |'
- en: Figure 2.13 Step followed in hierarchical clustering. Left-to-right we have
    agglomerative clustering (splitting of the nodes) while right-to-left we have
    divisive clustering (merging of the nodes)
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.13 中层次聚类的步骤。从左到右是聚合聚类（节点的分裂），从右到左是分裂聚类（节点的合并）
- en: '![02_11](images/02_11.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![02_11](images/02_11.png)'
- en: Let’s explore the meaning of *greedy approach* first. Greedy approach or greedy
    algorithm is any algorithm which makes a best choice at each step without considering
    the impact on the future states. In other words, we live in-the-moment and choose
    the best option from the available choices at that moment. The current choice
    is independent of the future choices and the algorithm will solve the subproblems
    later. Greedy approach may *not* provide the most optimal solution but generally
    provides a locally optimal solution which is close to the optimal solution in
    a reasonable time. Hierarchical clustering follows this greedy approach while
    merging or splitting at a node.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先探讨*贪婪方法*的含义。贪婪方法或贪婪算法是指在每一步都会做出最佳选择，而不考虑对未来状态的影响。换句话说，我们活在当下，并从当下可用的选择中选择最佳选项。当前选择与未来选择无关，算法会在以后解决子问题。贪婪方法可能*不会*提供最优解，但通常在合理时间内会提供接近最优解的局部最优解。层次聚类在合并或分割节点时遵循这种贪婪方法。
- en: 'We will now examine the steps followed in hierarchical clustering approach:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将审查层次聚类方法中的步骤：
- en: '**Step 1:** As shown in (Figure 2.13) above, let us say we have five observations
    in our data set – 1, 2, 3, 4 and 5.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一步：** 如上图（图2.13）所示，假设我们的数据集中有五个观察值– 1, 2, 3, 4 和5。'
- en: '**Step 2:** In this step, observation 1 and 2 are grouped into one, 4 and 5
    are clubbed in one. 3 is not clubbed in any one.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二步：** 在这一步中，观察1和2被分为一组，4和5被合并成一组。3没有被合并到任何一组中。'
- en: '**Step 3:** Now in this step, we group the output of 4,5 in the last step and
    observation 3 into one cluster.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三步：** 现在，在这一步中，我们将前一步中4,5的输出和观察3分为一个群集。'
- en: '**Step 4:** The output from step 3 is clubbed with the output of 1,2 as a single
    cluster.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**第四步：** 步骤3的输出与1,2的输出结合成为一个单一的群集。'
- en: In this approach, from left-to-right, we have an agglomerative approach and
    from right-to-left a divisive approach is represented. In an agglomerative approach,
    we are merging the observations while in a divisive approach we are splitting
    the observations. We can use both agglomerative or divisive approaches for hierarchical
    clustering. Divisive clustering is an exhaustive approach and sometimes might
    take more time than the other.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，从左到右，我们有一种自下而上的方法，而从右到左则是一种自上而下的方法。在自下而上的方法中，我们正在合并观测结果，而在自上而下的方法中，我们正在分割观测结果。我们可以同时使用自下而上或自上而下的方法进行层次聚类。分割性聚类是一种详尽的方法，有时可能比其他方法花费更多时间。
- en: Similar to k-means clustering, the distance metric used to measure plays a significant
    role here. We are aware and understand how to measure the distance between data
    points but there are multiple methods to define that distance which we are studying
    now.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与k-means聚类类似，在这里用于测量距离的距离度量标准起着重要作用。我们知道并了解如何测量数据点之间的距离，但是有多种方法来定义该距离，我们现在正在研究这些方法。
- en: 2.4.2 Linkage criterion for distance measurement
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 距离测量的链接标准
- en: 'We are aware that we can use Euclidean distance or Manhattan distance or Chebyshev
    distance etc. to measure the distance between two observations. At the same time,
    we can employ various methods to define that distance. And based on this input
    criterion, the resultant clusters will be different. The various methods to define
    the distance metric are:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们可以使用欧氏距离、曼哈顿距离或切比雪夫距离等距离度量标准来衡量两个观测之间的距离。同时，我们可以采用各种方法来定义该距离。根据这个输入标准，得到的聚类结果将不同。定义距离度量标准的各种方法包括：
- en: '**Nearest neighbors or single linkages** use the distance between the two nearest
    points in different clusters. The distance between the closest neighbors in distinct
    clusters is calculated and it is used to determine the next split/merging. It
    is done by an exhaustive search among all the pairs.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最近邻或单链接** 使用不同聚类中两个最近点之间的距离。计算不同聚类中最近邻的距离，并用于确定下一个分裂/合并。通过对所有成对进行穷举搜索来完成。'
- en: '**Farthest neighbor or complete linkage** is opposite of the nearest neighbor
    approach. Here, instead of taking the nearest neighbors we concentrate on most-distant
    neighbors in different clusters. In other words, we determine the distance between
    the clusters is calculated by the greatest distance between two objects.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最远邻或完全链接** 是最近邻方法的相反。在这里，我们不是考虑最近邻，而是专注于不同聚类中的最远邻。换句话说，我们确定聚类之间的距离是通过两个对象之间的最大距离计算的。'
- en: '**Group average linkage** calculates the average of distances between all the
    possible pairs of objects in two different clusters.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**群体平均链接** 计算两个不同聚类中所有可能的对象对之间距离的平均值。'
- en: '**Ward linkage** method aims to minimize the variance of the clusters which
    are getting merged into one.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ward链接** 方法旨在最小化合并为一体的聚类的方差。'
- en: We can use these options of distance metrics while we are developing the actual
    code for hierarchical clustering, and compare the accuracies to determine the
    best distance metrics for the dataset. During the algorithm training, the algorithm
    merges the observations which will minimize the linkage criteria chosen.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们为层次聚类开发实际代码时，我们可以使用这些距离度量的选项，并比较准确度以确定数据集的最佳距离度量。在算法训练期间，算法将合并最小化所选择的链接标准的观测。
- en: Note
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Such inputs to the algorithm are referred to as hyper-parameters. These are
    the parameters we feed to the algorithm to generate the results as per our requirement.
    An example of hyperparameter is “k” in k-means clustering.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的这些输入被称为超参数。这些是我们向算法提供的参数，以根据我们的要求生成结果。超参数的一个例子是k-means聚类中的“k”。
- en: We can visualise the various linkages in Figure 2.14 below.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的图2.14中可视化各种链接。
- en: Figure 2.14 (i) Single linkage is for closest neighbors (ii) Complete linkage
    is for farthest neighbors and (iii) Group average is for average of the distance
    between clusters
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.14（i）单链接是最近邻（ii）完全链接是最远邻（iii）群体平均是聚类之间距离的平均值
- en: '![02_12](images/02_12.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![02_12](images/02_12.png)'
- en: With this, we have understood the working mechanisms in hierarchical clustering.
    But we have still not addressed the mechanism to determine the optimum number
    of clusters using hierarchical clustering, which we are examining next.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们已经理解了层次聚类中的工作机制。但我们仍然没有解决使用层次聚类确定最优聚类数量的机制，我们正在研究这个问题。
- en: 2.4.3 Optimal number of clusters
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 最佳聚类数量
- en: Recall in k-means clustering we have to give the number of clusters as an input
    to the algorithm. We use elbow method to determine the optimum number of clusters.
    In the case of hierarchical clustering, we do not have to specify the number of
    clusters to the algorithm, but still we have to identify the number of final clusters
    we wish to have. We use a dendrogram to answer that problem.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在k均值聚类中，我们必须将聚类的数量作为算法的输入。我们使用肘部法确定最佳聚类数量。在层次聚类中，我们不必向算法指定聚类的数量，但仍然必须确定我们希望拥有的最终聚类数量。我们使用树状图来解决这个问题。
- en: Let us assume that we have 10 data points in total at the bottom of the chart
    as shown in Figure 2.15\. The clusters are merged iteratively till we get the
    one final cluster at the top. The height of the dendrogram at which two clusters
    get merged with each other represents the respective distance between the said
    clusters in the vector-space diagram.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们总共有10个数据点显示在图2.15底部。聚类会迭代地合并，直到我们得到一个最终聚类在顶部。树状图的高度，在两个聚类相互合并时表示向量空间图中的两个聚类之间的相应距离。
- en: Figure 2.15 Dendrogram to identify the optimum number of clusters. The distance
    between X and Y is more than A & B and P & Q, hence we choose that as the cut
    to create clusters and number of clusters chosen are 5\. The x-axis represents
    the clusters while the y-axis represents the distance (dissimilarity) between
    two clusters
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.15 树状图用于确定最佳聚类数量。X和Y之间的距离大于A＆B和P＆Q，因此我们选择该位置作为切割点来创建聚类，并选择聚类数量为5。x轴表示聚类，y轴表示两个聚类之间的距离（不相似度）。
- en: '![02_13](images/02_13.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![02_13](images/02_13.png)'
- en: From a dendrogram, the number of clusters is given by the number of vertical
    lines being cut by a horizontal line. The *optimum* number of clusters is given
    by the number of the vertical lines in the dendrogram cut by a horizontal line
    such that it intersects the tallest of the vertical lines. Or if the cut is shifted
    from one end of the vertical line to another, the length hence covered is the
    maximum. A dendrogram utilizes branches of clusters to show how closely various
    data points are related to each other. In a dendrogram, clusters that are located
    at the same height level are more closely related than clusters that are located
    at different height levels.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从树状图中，聚类的数量由水平线切割的垂直线数量决定。*最佳*聚类数量由树状图中被水平线切割的垂直线数量给出，使得它与最高的垂直线相交。或者如果切割从垂直线的一端移动到另一端，那么所覆盖的长度是最大的。树状图利用聚类的分支显示各个数据点之间的关系密切程度。在树状图中，位于相同高度水平的聚类比位于不同高度水平的聚类之间更密切相关。
- en: In the example shown in Figure 2.15, we have shown three potential cuts – AB,
    PQ and XY. If we take a cut above AB it will result in two very broad clusters
    while below PQ will result in nine clusters which will become difficult to analyze
    further.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.15中显示的示例中，我们展示了三个潜在的切割点 - AB，PQ和XY。如果我们在AB上方切割，将导致两个非常宽泛的聚类，而在PQ下方切割，将导致九个聚类，这将变得难以进一步分析。
- en: Here the distance between X and Y is more than A & B and P & Q. So we can conclude
    that the distance between X and Y is the maximum and hence we can finalize that
    as the best cut. This cut, intersects at five distinct points hence we should
    have five clusters. Hence, the height of the cut in the dendrogram is similar
    to the value of k in k-means clustering. In k-means clustering, “k” determines
    the number of clusters. In hierarchical clustering, the best cut determines the
    number of clusters we wish to have.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，X和Y之间的距离大于A＆B和P＆Q。因此，我们可以得出结论，X和Y之间的距离最大，因此我们可以将其确定为最佳切割。该切割在五个不同点相交，因此我们应该有五个聚类。因此，树状图中切割的高度类似于k均值聚类中的k值。在k均值聚类中，“k”确定聚类的数量。在层次聚类中，最佳切割确定我们希望拥有的聚类数量。
- en: Similar to k-means clustering, the final number of clusters is not dependent
    on the choice from the algorithm only. The business acumen and the pragmatic logic
    play a vital role in determining the final number of clusters. Recall that one
    of the important attributes of clusters is their usability which we discussed
    in section 2.2 earlier.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于k均值聚类，最终的聚类数量并不仅取决于算法的选择。商业眼光和实用逻辑在确定最终聚类数量方面起着至关重要的作用。回顾一下，聚类的重要属性之一是它们的可用性，我们在之前的第2.2节中已经讨论过。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 小测验——回答这些问题来检验你的理解力。答案在书的结尾。'
- en: 1.   What is the greedy approach used in hierarchical clustering?
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 层次聚类中使用的贪婪方法是什么？
- en: 2.   Complete linkage is used for finding distances for closest neighbors –
    TRUE or FALSE
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 完全连接法用于查找最接近邻居的距离——正确还是错误？
- en: 3.   What is the difference between group linkage and ward linkage?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 群组联接与瓦德联接有什么区别？
- en: 4.   Describe the process to find the most optimal value of “k”
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 4.   描述找到“k”最优值的过程
- en: We have now covered the background of hierarchical clustering and how we determine
    the clusters. We will now discuss the advantages and challenges we face with hierarchical
    clustering.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了层次聚类的背景以及如何确定聚类。现在我们将讨论我们在层次聚类中面临的优势和挑战。
- en: 2.4.4 Pros and cons of hierarchical clustering
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 层次聚类的优缺点
- en: 'Hierarchical clustering is a strong clustering technique and quite popular
    too. Similar to k-means, it is also using distance as a metric to measure the
    similarity. At the same time, there are a few challenges with the algorithm. We
    are discussing pros and cons of hierarchical clustering now. The advantages of
    hierarchical clustering are:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种强大的聚类技术，也很受欢迎。类似于K均值，它也使用距离作为衡量相似性的度量。同时，该算法也存在一些挑战。我们正在讨论层次聚类的利与弊。层次聚类的优势包括：
- en: Perhaps the biggest advantage we have with hierarchical clustering is reproducibility
    of results. Recall in k-means clustering, the process starts with random initialization
    of centroids giving different results. In hierarchical clustering we can reproduce
    the results.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也许我们在层次聚类中最大的优势就是结果的可重现性。回顾一下，在K均值聚类中，过程始于对中心点的随机初始化，因而会得到不同的结果。而在层次聚类中，我们可以重现结果。
- en: In hierarchical clustering, we do not have to input the number of clusters to
    segment the data.
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在层次聚类中，我们不需要输入要对数据进行分段的聚类数。
- en: The implementation is easy to implement and comprehend. Since it follows a tree-like
    structure, it is explainable to users from non-technical backgrounds.
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施是容易实现和理解的。由于它遵循类似树的结构，因此可以向非技术背景的用户解释清楚。
- en: The dendrogram generated can be interpreted to generate a very good understanding
    of the data with a visualization.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的树状图可以通过可视化来解释数据并产生非常好的理解。
- en: 'At the same time, we do face some challenges with hierarchical clustering algorithm
    which are:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们在使用层次聚类算法时面临一些挑战：
- en: The biggest challenge we face with hierarchical clustering is the time taken
    to converge. The time complexity for k-means is linear while for hierarchical
    clustering is quadratic. For example, if we have “n” data points, then for k-means
    clustering the time complexity will be O(n) while for hierarchical clustering
    is O(n³).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类最大的挑战是收敛所需的时间。K均值的时间复杂度是线性的，而层次聚类是二次的。举例来说，如果我们有“n”个数据点，那么对于K均值聚类，时间复杂度将是O(n)，而对于层次聚类，则是O(n³)。
- en: You can refer to the appendix of the book if you want to study O(n)
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如果你想学习O(n)，可以参考本书的附录。
- en: Since the time complexity is O(n³), it is a time-consuming task. Moreover, the
    memory required to compute is at least O(n²) making hierarchical clustering quite
    a time consuming and memory intensive process. And this is the issue even if the
    dataset is medium. The computation required might not be a challenge if we are
    using really high-end processors but surely can be a concern for regular computers
    we use.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于时间复杂度为O(n³)，这是一个耗时的任务。此外，计算所需的内存至少是O(n²)，使得层次聚类成为一个非常耗时和内存密集型的过程。即使数据集是中等大小，这也是一个问题。如果我们使用高端处理器，则计算可能不是一个挑战，但对于普通计算机来说可能会成为一个问题。
- en: The interpretation of dendrograms at times can be subjective hence due diligence
    is required while interpretation of dendrograms. The key to interpret a dendrogram
    is to focus on the height at which any two data points are connected with each
    other. It can be subjective as different analysts can decipher different cuts
    and try to prove their methodology. Hence, it is advisable to interpret the results
    in the light of mathematics and marry the results with real-world business problem.
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时树状图的解释可能会有主观性，因此在解释树状图时需要谨慎。解释树状图的关键是注重任何两个数据点连接的高度。由于不同的分析人员可能会解释出不同的切割点并试图证明他们的方法，因此解释可能存在主观性。因此，建议在数学的光线下解释结果，并将结果与现实世界的业务问题相结合。
- en: The hierarchical clustering cannot undo the previous steps it has done. In case,
    we feel that a connection made is not proper and should be rolled back, still
    there is no mechanism to remove the connection.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类无法撤销其之前所做的步骤。如果我们觉得某个连接不正确，应该撤销，但目前还没有机制可以移除连接。
- en: The algorithm is very sensitive to outliers and messy dataset. Presence of outliers,
    NULL, missing values, duplicates etc. make a dataset messy. And hence the resultant
    output might not be proper and not what we expected.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对异常值和混乱的数据集非常敏感。异常值、NULL、缺失值、重复项等使数据集变得混乱。因此，结果可能不正确，也不是我们预期的。
- en: But despite all the challenges, hierarchical clustering is one of the most widely
    used clustering algorithms. Generally, we create both k-means clustering and hierarchical
    clustering for the same dataset to compare the results of the two. If the number
    of clusters suggested and the distribution of respective clusters look similar,
    we get more confident on the clustering methodology used.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在种种挑战，层次聚类是最常用的聚类算法之一。通常，我们为同一数据集创建 k-means 聚类和层次聚类，以比较两者的结果。如果建议的簇的数量和各自簇的分布看起来相似，我们对所使用的聚类方法更有信心。
- en: We have covered the theoretical understanding of hierarchical clustering. It
    is time for action and jump into Python for coding.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了对层次聚类的理论理解。现在是行动的时候，跳入 Python 进行编码。
- en: 2.4.5 Hierarchical clustering case study using Python
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 的层次聚类案例研究
- en: We will now create a Python solution for hierarchical clustering, using the
    same dataset we used for k-means clustering.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为层次聚类创建一个 Python 解决方案，使用与 k-means 聚类相同的数据集。
- en: '**Steps 1-6:** Load the required libraries and dataset. For this, follow the
    steps 1 to 6 we have followed in k-means algorithm.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1-6：** 加载所需的库和数据集。为此，请按照我们在 k-means 算法中所遵循的步骤 1 到 6 进行操作。'
- en: '**Step 7:** Next, we are going to create hierarchical clustering using three
    linkages methods – average, ward and complete. Then the clusters are getting plotted.
    The input to the method is the X_Standard variable, linkage method used and the
    distance metric. Then, using matplotlib library, we are plotting the dendrogram.
    In the code snippet, simply change the method from ‘average’ to ‘ward’ and ‘complete’
    and get the respective results.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 7：** 接下来，我们将使用三种链接方法（平均、Ward 和完全）创建层次聚类。然后对簇进行绘图。该方法的输入是 X_Standard 变量、使用的链接方法和距离度量。然后，使用
    matplotlib 库，我们绘制树状图。在代码片段中，只需将方法从“average”更改为“ward”和“complete”，就可以得到相应的结果。'
- en: '[PRE14]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![02_13a](images/02_13a.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![02_13a](images/02_13a.png)'
- en: '**Step 8:** We now want to choose the number of clusters we wish to have. For
    this purpose, let’s recreate the dendrogram by sub-setting the last 10 merged
    clusters. We have chosen 10 as it is generally an optimal choice, you are advised
    to test with other values too.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 8：** 现在，我们想要选择我们希望拥有的簇的数量。为此目的，让我们通过对最后 10 个合并的簇进行子集划分来重新创建树状图。我们选择了 10，因为这通常是一个最佳选择，建议您也尝试其他值。'
- en: '[PRE15]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![02_13b](images/02_13b.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![02_13b](images/02_13b.png)'
- en: '**Step 9:** We can observe that the most optimal distance is 10.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 9：** 我们可以观察到，最优距离是 10。'
- en: '**Step 10:** Cluster the data into different groups. By using the logic described
    in the last section, the number of optimal clusters is coming to be four.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 10：** 将数据聚类到不同的组中。通过使用上一节描述的逻辑，最优簇的数量被确定为四个。'
- en: '[PRE16]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 11:** Plot the distinct clusters using matplotlib library.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 11：** 使用 matplotlib 库绘制不同的簇。'
- en: '[PRE17]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![02_13c](images/02_13c.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![02_13c](images/02_13c.png)'
- en: '**Step 12:** For different values of distance, the number of clusters will
    change and hence the plot will look different. We are showing different results
    for distances of 5, 15 20, and different numbers of clusters generated for each
    iteration. Here, we can observe that we get completely different results for different
    values of distances while we move from left to right. We have to be cautious while
    we choose the value of the distance and sometimes, we might have to iterate a
    few times to get the best value.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 12：** 对于不同的距离值，簇的数量会改变，因此图表会呈现不同的样子。我们展示了距离为 5、15、20 的不同结果，并且每次迭代生成的簇的数量也不同。在这里，我们可以观察到，当我们从左到右移动时，对于不同距离值，我们得到完全不同的结果。在选择距离值时，我们必须要谨慎，有时候，我们可能需要多次迭代才能得到最佳值。'
- en: '![02_13d](images/02_13d.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![02_13d](images/02_13d.png)'
- en: Hence, we can observe that using hierarchical clustering, we have segmented
    the data on the left side to the one on the right side of Figure 2\. below. The
    left side is the representation of the raw data while on the right we have a representation
    of the clustered dataset.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以观察到，使用分层聚类，我们已将数据从图2下面的左侧分割到右侧。左侧是原始数据的表示，而右侧是聚类数据集的表示。
- en: '![02_13e](images/02_13e.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![02_13e](images/02_13e.png)'
- en: Hierarchical clustering is a robust method and is a highly recommended one.
    Along with k-means, it creates a great foundation for clustering-based solutions.
    Most of the time, at least these two techniques are worked upon when we create
    clustering solutions. And then we move to iterate with other methodologies.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 分层聚类是一种强大的方法，也是一种高度推荐的方法。与 k-means 一起，它为基于聚类的解决方案建立了良好的基础。大多数情况下，至少这两种技术在我们创建聚类解决方案时会被考虑。然后，我们会继续尝试其他方法。
- en: This marks the end of the discussion on connectivity-based clustering algorithms.
    We will now move forward to density-based solutions and discuss DBSCAN clustering
    in the next section.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着对基于连通性的聚类算法的讨论的结束。我们现在将前进到基于密度的解决方案，并在下一节讨论 DBSCAN 聚类。
- en: 2.5 Density based clustering
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 基于密度的聚类
- en: We have studied k-means in the earlier sections. Recall how it uses a centroid-based
    method to assign a cluster to each of the data points. If an observation is an
    outlier, the outlier point pulls the centroid towards itself and is also assigned
    a cluster like a normal observation. These outliers do not necessarily bring information
    to the cluster and can impact other data points disproportionally but are still
    made a part of the cluster. Moreover, getting clusters of arbitrary shape as shown
    in Figure 2.16 is a challenge with the k-means algorithm. Density based clustering
    methods solve the problem for us.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在早期章节中学习了 k-means。回想一下它如何使用基于质心的方法将聚类分配给每个数据点。如果一个观测值是异常值，异常值将质心拉向自己，并像正常观测值一样分配到一个聚类中。这些异常值不一定为聚类带来信息，并且可能会不成比例地影响其他数据点，但仍然被作为聚类的一部分。此外，如图2.16所示，使用
    k-means 算法获取任意形状的聚类是一项挑战。基于密度的聚类方法为我们解决了这个问题。
- en: Figure 2.16 DBSCAN is highly-recommended for irregular shaped clusters. With
    k-means we generally get spherical clusters, DBSCAN can resolve it for us
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.16 DBSCAN 非常适用于不规则形状的聚类。通过 k-means，我们通常会得到球形聚类，而 DBSCAN 可以为我们解决这个问题。
- en: '![02_14](images/02_14.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![02_14](images/02_14.png)'
- en: In a density-based clustering algorithm, we resolve all of these problems. In
    the density-based method, the clusters are identified as the areas which have
    a higher density as compared to the rest of the dataset. In other words, given
    a vector-space diagram where the data points are represented – a cluster is defined
    by adjacent regions or neighboring regions of high-density points. This cluster
    will be separated from other clusters by regions of low-density points. The observations
    in the sparse areas or separating regions are considered as noise or outliers
    in the dataset. A few examples of density-based clustering are shown in (Figure
    2.16).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于密度的聚类算法中，我们解决了所有这些问题。在基于密度的方法中，将那些与数据集的其他部分相比具有更高密度的区域标识为聚类。换句话说，在表示数据点的向量空间图中
    - 一个聚类由高密度点的相邻区域或邻近区域定义。该聚类将被低密度点的区域与其他聚类分开。在稀疏区域或分隔区域中的观测被视为数据集中的噪音或异常值。密度基础聚类的几个示例显示在（图2.16）中。
- en: We mentioned two terms - “neighborhood” and “density”. To understand density-based
    clustering, we will study these terms in the next section.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到了两个术语 - “邻域”和“密度”。为了理解基于密度的聚类，我们将在下一节中学习这些术语。
- en: 2.5.1 Neighborhood and density
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 邻域和密度
- en: Imagine we represent data observations in a vector-space. And we have a point
    P. We now define the neighborhood for this point P. The representation is shown
    in Figure 2.17 below.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们在一个向量空间中表示数据观测。我们有一个点 P。现在我们为这个点 P 定义邻域。表示如下所示：图2.17。
- en: Figure 2.17 Representation of data points in a vector-space diagram. On the
    right-side we have a point P and the circle drawn is of radius ε. So, for ε >
    0, the neighborhood of P is defined by the set of points which are at less than
    equal to ε distance from the point P
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.17 数据点在向量空间图中的表示。在右侧，我们有一个点 P，绘制的圆是半径 ε。因此，对于 ε > 0，点 P 的邻域由距离点 P 小于等于 ε
    的点集定义。
- en: '![02_15](images/02_15.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![02_15](images/02_15.png)'
- en: As we can make out from Figure 2.17 above, for a point P we have defined a ε
    - neighborhoods for it which are the points equidistant from P. In a 2-D space,
    it is represented by a circle, in a 3-D space it is a sphere, and for a n-dimensional
    space it is n-sphere with center P and radius ε. This defines the concept of *neighborhood*.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从上面的图 2.17 中可以看出的那样，对于一个点 P，我们已经为其定义了 ε - 邻域，这些点与 P 等距。在二维空间中，它由一个圆表示，在三维空间中它是一个球体，在
    n 维空间中它是以 P 为中心和半径 ε 的 n-球体。这定义了*邻域*的概念。
- en: Now, let’s explore the term, *density*. Recall density is mass divided by volume
    (mass/volume). Higher the mass, the higher the density, and the lower the mass,
    the lower the density. Conversely, the lower the volume, the higher the density,
    and the higher the volume, the lower the density.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探讨术语*密度*。回想密度是质量除以体积（质量/体积）。质量越大，密度越高，质量越低，密度越低。反之，体积越小，密度越高，体积越大，密度越低。
- en: In the context above, mass is the number of points in the neighborhood. In (Figure
    2.18) below, we can observe the impact of ε on the number of data points or the
    mass.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在上文的背景下，质量是邻域中的点数。在下面的图 2.18 中，我们可以观察到 ε 对数据点或质量的数量的影响。
- en: Figure 2.18 The impact of radius ε, on the left side the number of points is
    more than on the right-side. So, the mass of right side is less since it contains
    a smaller number of data points
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.18 对半径 ε 的影响，左侧的点数比右侧多。因此，右侧的质量较低，因为它包含较少的数据点。
- en: '![02_16](images/02_16.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![02_16](images/02_16.png)'
- en: When it comes to volume, in the case of 2-d space, volume is πr², while for
    a sphere that is three-dimensional, it is 4/3 πr³. For spheres of n-dimensions,
    we can calculate the respective volume as per the number of dimensions which will
    be π times a numerical constant raised to the number of dimensions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到体积时，在二维空间的情况下，体积是 πr²，而对于三维的球体，它是 4/3 πr³。对于 n 维度的球体，我们可以根据维度的数量计算相应的体积，这将是
    π 乘以一个数值常数的维度数次方。
- en: So, in the two cases shown in (Figure 2.18), for a point “P” we can get the
    number of points (mass) and volumes and then we can calculate the respective densities.
    But the absolute values of these densities mean nothing to us, but how they are
    similar (or different) from nearby areas. It is used to cluster the points having
    similar densities. Or in other words, the points which are in the same neighborhood
    and have similar densities can be clubbed in one cluster.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在图 2.18 中显示的两种情况下，对于一个点“P”，我们可以得到点（质量）和体积的数量，然后我们可以计算相应的密度。但是这些密度的绝对值对我们来说没有任何意义，而是它们与附近区域的相似性（或不同）如何。它用于对具有相似密度的点进行聚类。换句话说，处于同一邻域并具有相似密度的点可以被归为一个簇。
- en: In an ideal case scenario, we wish to have highly dense clusters having maximum
    number of points. In the two cases shown in (Figure 2.19) below, we have a less
    dense cluster depicted on the left and high-dense one on the right-hand side.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，我们希望拥有具有最大数量点的高密度簇。在下面图 2.19 中显示的两种情况下，我们有一个左侧显示较少密集簇，右侧显示高密集簇。
- en: Figure 2.19 Denser clusters are preferred over less dense ones. Ideally a dense
    cluster, with maximum number of data points is what we aim to achieve from clustering
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.19 密集簇优于较少密集簇。理想情况下，我们希望从聚类中获得具有最大数据点数量的密集簇。
- en: '![02_17](images/02_17.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![02_17](images/02_17.png)'
- en: 'From the discussion above, we can conclude that:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的讨论中，我们可以得出结论：
- en: If we *increase* the value of ε, we will get a *higher* volume but not necessarily
    a *higher* number of points (mass). It depends on the distribution of the data
    points.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们*增加* ε 的值，我们将得到*更大*的体积，但不一定会得到*更多*的点（质量）。这取决于数据点的分布。
- en: Similarly, if we *decrease* the value of ε, we will get a *lower* volume but
    not necessarily a *lower* number of points (mass).
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，如果我们*减小* ε 的值，我们将得到*更小*的体积，但不一定会得到*更少*的点（质量）。
- en: These are the fundamental points we adhere to. Hence, it is imperative that
    while choosing the clusters we choose clusters that have high density and cover
    the maximum number of neighboring points.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们遵循的基本要点。因此，在选择簇时，选择具有高密度并覆盖最大数量邻近点的簇是至关重要的。
- en: We have hence concluded the concepts for density-based clustering. These concepts
    are the building blocks for DBSCAN clustering which we are discussing next!
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经总结了密度聚类的概念。这些概念是我们接下来讨论的 DBSCAN 聚类的基石！
- en: 2.5.2 DBSCAN Clustering
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 DBSCAN 聚类
- en: Density-Based Spatial Clustering of Applications with Noise or DBSCAN clustering
    is a one of the highly recommended density-based algorithms. It clusters the data
    observations which are closely packed in a densely populated area but not considering
    the outliers in low-density regions. Unlike k-means, we do not specify the number
    of clusters and the algorithm is able to identify irregular-shaped clusters whereas
    k-means generally proposes spherical-shaped clusters. Similar to hierarchical
    clustering, it works by connecting the data points but with the observations which
    satisfy the density-criteria or the threshold value. The more can be understood
    in the steps we are describing below.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 具有噪声的基于密度的空间聚类应用，或者称为DBSCAN聚类，是高度推荐的基于密度的算法之一。它对密集人口区域紧密打包的数据观察进行聚类，但不考虑低密度区域的异常值。与k均值不同，我们不需要指定簇的数量，该算法能够识别不规则形状的簇，而k均值通常提出球形簇。与层次聚类类似，它通过连接数据点工作，但是只有满足密度标准或阈值的观察。更多内容可以在我们下面描述的步骤中了解。
- en: 'Note:'
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意：
- en: DBSCAN was proposed in 1996 by Martin Ester, Hans-Peter Kriegal, Jörg Sander
    and Xiaowei Xu. The algorithm was awarded the test of time award in 2014 at ACM
    SIGKDD. The paper can be assessed at [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.1980](viewdoc.html).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN是由Martin Ester, Hans-Peter Kriegal, Jörg Sander和Xiaowei Xu于1996年提出的。该算法于2014年在ACM
    SIGKDD获得了时间测试奖。可以在[http://citeseerx.ist. psu.edu/viewdoc/summary?doi=10.1.1.71.1980]（viewdoc.html）中查阅论文。
- en: DBSCAN works on the concepts of neighborhood we discussed in the last section.
    We will now dive deep into the working methodology and building blocks of DBSCAN.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN基于我们在上一节讨论的邻域概念。我们现在将深入研究DBSCAN的工作方法和构建模块。
- en: nuts and bolts of DBSCAN clustering
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN聚类的实质
- en: We will now examine the core building blocks of DBSCAN clustering. We know it
    is a density-based clustering algorithm and hence neighborhood concept is applicable
    over here.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将对DBSCAN聚类的核心构建模块进行检查。我们知道这是一个基于密度的聚类算法，因此邻域概念在这里适用。
- en: 'Consider we have a few data observations which we need to cluster. We also
    locate a data point “P”. Then, we can easily define two hyperparameter terms:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些数据观察结果需要进行聚类。我们还要找到一个数据点“P”。然后，我们可以轻松地定义两个超参数术语：
- en: The radius of the neighborhood around P, known as *ε*, which we have discussed
    in the last section.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P周围邻域的半径，即我们在上一节中讨论的*ε*。
- en: The minimum number of points we wish to have in the neighborhood of P or in
    other words, minimum number of points that are required to create a dense region.
    This is referred to as *minPts*. And is one of the parameters we can input by
    applying a threshold on minPts.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望在P的邻域内至少有minPts个点，或者换句话说，需要至少有一定数量的点来创建一个密集区域。这被称为*minPts*。这是我们可以通过在minPts上应用阈值来输入的参数之一。
- en: 'Based on the concepts above, we can classify the observations into three broad
    categories - core points, border or reachable points, and outliers:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 基于以上概念，我们可以将观察结果分为三大类 - 核心点、边界点或可达点和异常值：
- en: '**Core points**: any data point “x” can be termed as a core point if at least
    minPts are within ε distance of it (including x itself), shown as squares in (Figure
    2.20) below. They are the building blocks of our clusters and hence are called
    core. We use the same value of radius (ε) for each point and hence the *volume*
    of each neighborhood remains constant. But the number of points will vary and
    hence the *mass* varies. Consequently therefore, the density varies as well. Since
    we put a threshold using minPoints, we are putting a limit on density. So, we
    can conclude that core points fulfil the minimum density threshold requirement.
    It is imperative to note that we can choose different values of ε and minPts to
    iterate and fine-tune the clusters.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**核心点**：如果至少有minPts个点与它的ε距离以内（包括x本身），则任何数据点“x”都可以被称为核心点，如下面的（图2.20）所示。它们是我们聚类的构建模块，因此被称为核心点。我们为每个点使用相同的半径值（ε），因此每个邻域的*体积*保持不变。但是点的数量会有所变化，因此*质量*会有所变化。因此密度也会发生变化。由于我们使用minPoints设置阈值，我们正在对密度进行限制。因此，我们可以得出结论，核心点满足最低密度阈值要求。需要注意的是，我们可以选择不同的ε和minPts值来迭代和微调簇。'
- en: Figure 2.20 Core points are shown in square, border points are shown in filled
    circle while noise is unfilled circles. Together these three are the building
    blocks for DBSCAN clustering
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.20 核心点显示为方形，边界点显示为填充圆，而噪声显示为未填充圆。这三者共同是DBSCAN聚类的构建模块
- en: '![02_18](images/02_18.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![02_18](images/02_18.png)'
- en: '**Border points or reachable points**: a point that is not a core point in
    the clusters is called a border point, shown as filled circles in Figure 2.20.'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**边界点或可达点**：在簇中不是核心点的点称为边界点，如图2.20中的填充圆所示。'
- en: A point “y” is directly reachable from x if y is within ε distance of core point
    x. A point can only be approached from a core point and it is the primary condition
    or rule to be followed. Only a core-point can reach a non-core point and the opposite
    is not true. In other words, a non-core point can only be reached by other core-points,
    it cannot reach anyone else. In Figure 2.20, border points are represented as
    dark circles.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如果点“y”距离核心点x的距离在ε范围内，则点“y”直接从x可达。一个点只能从核心点到达，这是必须遵循的主要条件或规则。只有核心点才能到达非核心点，反之不成立。换句话说，非核心点只能被其他核心点到达，它无法到达其他任何点。在图2.20中，边界点表示为黑色圆圈。
- en: To understand the process better, we have to understand the term *density-reachable*
    or *connectedness*. As shown in (Figure 2.21) below, we have two core points X
    and Y. We can directly go from X to Y. Point Z is not in the neighborhood of X
    but is the neighborhood of Y. So, we cannot directly reach Z from X. But we can
    surely reach Z from X through Y or in other words using neighborhood of Y, we
    can travel to Z from X. We cannot go from Z to Z since, Z is the border point
    and as described earlier, we cannot travel from a border point.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解过程，我们必须理解*密度可达*或*连接性*这个术语。如下图2.21所示，我们有两个核心点X和Y。我们可以直接从X到Y。点Z不在X的邻域内，但是在Y的邻域内。所以，我们不能直接从X到达Z。但是我们可以确实从X通过Y到达Z，换句话说，使用Y的邻域，我们可以从X到达Z。我们不能从Z到达Z，因为Z是边界点，并且如前所述，我们不能从边界点出发。
- en: Figure 2.21 X and Y are the core points and we can travel from X to Y. Though
    Z is not in the immediate neighborhood of X, we can still reach Z from X through
    Y. It is the core concept of density-connected points
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.21中的X和Y是核心点，我们可以从X到Y。虽然Z不在X的直接邻域内，但我们仍然可以通过Y从X到达Z。这是密度连接点的核心概念。
- en: '![02_19](images/02_19.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![02_19](images/02_19.png)'
- en: '**Outliers**: all the other points are outliers. In other words, if it is not
    a core point or is not a reachable point, it is an outlier, shown as unfilled
    circles in (Figure 2.20) above. They are not assigned any cluster.'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**离群值**：所有其他点都是离群值。换句话说，如果它不是核心点或不是可达点，则它是离群值，如上图2.20中的未填充圆所示。它们不被分配任何簇。'
- en: Now we have defined the building block for DBSCAN. We will now proceed to the
    process followed in DBSCAN in the next section.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了DBSCAN的构建模块。我们现在将在下一节中继续介绍DBSCAN的流程。
- en: steps in DBSCAN clustering
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN聚类的步骤
- en: 'Now we have defined the building block for DBSCAN. We will now examine the
    steps followed in DBSCAN:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了DBSCAN的构建模块。我们将现在检查DBSCAN中遵循的步骤：
- en: We start with assigning values for ε and minimum points (minPts) required to
    create a cluster.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先为ε和创建簇所需的最小点数（minPts）分配值。
- en: We start with picking a random point let’s say “P” which is not yet given any
    label i.e. which has not been analyzed and assigned any cluster.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先选择一个随机点，比如说“P”，该点尚未被分析并分配任何簇。
- en: We then analyze the neighborhood for P. If it contains a sufficient number of
    points i.e. higher than minPts, then the condition is met to start a cluster.
    If so, we tag the point P as *core-point*. If a point cannot be recognized as
    a core-point, we will assign it the tag of *outlier* or *noise*. We should note
    this point can be made a part of a different cluster later. We go back to step
    2 then.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们分析P的邻域。如果它包含足够数量的点，即高于minPts，则满足开始一个簇的条件。如果是这样，我们将点P标记为*核心点*。如果一个点不能被识别为核心点，我们将为其分配*离群值*或*噪声*的标签。我们应该注意到这一点后来可以成为不同簇的一部分。然后我们回到步骤2。
- en: Once this core point “P” is found, we start creating this cluster by adding
    all directly reachable points from P and then increase this cluster size by adding
    more directly points reachable from P. Then we add all the points to the cluster,
    which can be included using the neighborhood by iterating through all of these
    points. If we add an outlier point to the cluster, the tag of the outlier point
    is changed to a border point.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到了这个核心点“P”，我们就开始通过添加所有从P直接可达的点来创建这个簇，然后通过添加更多从P直接可达的点来增加这个簇的大小。然后我们通过迭代所有这些点来将所有点添加到簇中，这些点可以使用邻域包含。如果我们将一个离群值点添加到簇中，则离群值点的标签将更改为边界点。
- en: This process continues till density-cluster is complete. We then find a new
    unassigned point and repeat the process.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会一直持续，直到密度聚类完成。然后我们找到一个新的未分配点并重复这个过程。
- en: Once all the points have been assigned to a cluster or called as an outlier,
    we stop our clustering process.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有点都被分配到一个簇或称为异常值，我们就停止我们的聚类过程。
- en: There are iterations done in the process. And once the clustering concludes,
    we utilize business logic to either merge or split a few clusters.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程中进行迭代。一旦聚类结束，我们就利用业务逻辑来合并或拆分一些簇。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 小测验 - 回答这些问题来检查你的理解。答案在本书末尾'
- en: 1.   Compare and contrast the importance of DBSCAN clustering with respect to
    kmeans clustering.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 将 DBSCAN 聚类的重要性与 kmeans 聚类进行比较和对比。
- en: 2.   A non-core point can reach a core point and vice versa is also true – TRUE
    or FALSE
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 非核心点可以到达核心点，反之亦然 - 真或假？
- en: 3.   Explain the significance of neighborhood and minPts.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 解释邻域和 minPts 的重要性。
- en: 4.   Describe the process to find the most optimal value of “k”
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 描述找到“k”的最优值的过程
- en: Now we are clear with the process of DBSCAN clustering. Before creating the
    Python solution, we will examine the advantages and disadvantages of the DBSCAN
    algorithm.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们清楚了 DBSCAN 聚类的过程。在创建 Python 解决方案之前，我们将检查 DBSCAN 算法的优缺点。
- en: pros and cons of DBSCAN clustering
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN 聚类的优缺点
- en: 'DBSCAN has following advantages:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 具有以下优点：
- en: Unlike k-means, we need not specify the number of clusters to DBSCAN.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 k-means 不同，我们不需要为 DBSCAN 指定簇的数量。
- en: The algorithm is quite a robust solution for unclean datasets. Unlike other
    algorithms, it can deal with outliers effectively.
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对不干净的数据集是一种相当强大的解决方案。与其他算法不同，它可以有效地处理异常值。
- en: We can determine irregular-shaped clusters too. Arguably, it is the biggest
    advantage of DBSCAN clustering.
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也可以确定不规则形状的簇。可以说，这是 DBSCAN 聚类的最大优势。
- en: Only the input of radius and minPts is required by the algorithm.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法只需要半径和 minPts 的输入。
- en: 'DBSCAN suffers from the following challenges:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 面临以下挑战：
- en: The differentiation in clusters is sometimes not clear using DBSCAN. Depending
    on the order of processing the observations, a point can change its cluster. In
    other words, if a border point P is accessible by more than one cluster, P can
    belong to either cluster, which is dependent on the order of processing the data.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DBSCAN 时，聚类的差异有时并不明显。根据处理观察的顺序，一个点可以改变其簇。换句话说，如果边界点 P 可以被多个簇访问，P 可以属于任一簇，这取决于处理数据的顺序。
- en: If the difference in densities among different areas of the datasets is very
    big, then the optimum combination of ε and minPts will be difficult to determine
    and hence, DBSCAN will not be generating effective results.
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据集不同区域的密度差异很大，则确定 ε 和 minPts 的最佳组合将变得困难，因此，DBSCAN 将无法生成有效结果。
- en: The distance metric used plays a highly significant role in clustering algorithms
    including DBSCAN. Arguably, the most common metric used in Euclidean distance,
    but if the number of dimensions is quite large then it becomes a challenge to
    compute.
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的距离度量在包括 DBSCAN 在内的聚类算法中发挥了非常重要的作用。可以说，最常用的度量是欧几里得距离，但如果维度的数量相当大，则计算将变得很困难。
- en: The algorithm is very sensitive to different values of ε and minPts. Sometimes,
    finding the most optimum value becomes a challenge.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法对 ε 和 minPts 的不同取值非常敏感。有时，找到最优值成为一项挑战。
- en: We will now create a Python solution for DBSCAN clustering.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为 DBSCAN 聚类创建一个 Python 解决方案。
- en: python solution for DBSCAN clustering
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DBSCAN 聚类的 Python 解决方案
- en: We will use the same dataset we have used for k-means and hierarchical clustering.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与 k-means 和层次聚类相同的数据集。
- en: '**Steps 1-6:** Load the libraries and dataset up to step 6 in k-means algorithm.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1-6 步：** 加载库和数据集，直到 k-means 算法的第 6 步。'
- en: '**Step 7:** Import additional libraries'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 7 步：** 导入额外的库'
- en: '[PRE18]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 8:** We are fitting the model with a value for minDist and radius.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 8 步：** 我们正在使用 minDist 和半径的值来拟合模型。'
- en: '[PRE19]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Step 9:** The number of distinct clusters are one.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 9 步：** 不同的簇数为 1。'
- en: '[PRE20]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Step 10:** We are not getting any results for clustering here. In other words,
    there will not be any logical results of clustering since we have not provided
    the optimal values for minPts and ε.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 10 步：** 我们在这里得不到任何聚类的结果。换句话说，由于我们没有提供 minPts 和 ε 的最优值，所以聚类没有任何逻辑结果。'
- en: '**Step 11:** Now, we will find out the optimum values for the ε. For this,
    we will calculate the distance to nearest points for each point and then sort
    and plot the results. Wherever the curvature is maximum, it is the best value
    for ε. For minPts, generally minPts ≥ d+1 where d is the number of dimensions
    in the dataset.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 11：** 现在，我们将找出ε的最佳值。为此，我们将计算每个点的最近点距离，然后对结果进行排序和绘制。无论何时弯曲程度最大，它就是ε的最佳值。对于minPts，通常minPts
    ≥ d+1，其中d是数据集中的维数。'
- en: '[PRE21]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![02_19a](images/02_19a.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![02_19a](images/02_19a.png)'
- en: Note
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注
- en: You are advised to go through the paper at the link to further study on how
    to choose the values of radius for DBSCAN [https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf](012012.html)
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 建议你阅读链接中的文章，进一步研究如何为DBSCAN选择半径的值[https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf](012012.html)
- en: '**Step 12:** The best value is coming as 1.5 as observed the point of defection
    above. We will use it and set the minPts as 5 which is generally taken as a standard.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 12：** 最佳值为1.5，正如上面观察到的缺陷点所示。我们将使用它，并将minPts设置为5，通常视为标准。'
- en: '[PRE22]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Step 13:** Now we can observe that we are getting more than one cluster.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 13：** 现在我们可以观察到我们得到了不止一个簇。'
- en: '[PRE23]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Step 14:** Let’s plot the clusters.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 14：** 让我们绘制这些簇。'
- en: '[PRE24]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![02_19b](images/02_19b.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![02_19b](images/02_19b.png)'
- en: We have thus created a solution using DBSCAN. You are advised to compare the
    results from all three algorithms. In real-world scenarios, we test the solution
    with multiple algorithms, iterate with hyperparameters, and then choose the best
    solution.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了DBSCAN创建了一个解决方案。建议你比较来自所有三种算法的结果。在现实世界的情景中，我们测试使用多种算法的解决方案，用超参数进行迭代，然后选择最佳解决方案。
- en: Density-based clustering is quite an efficient solution and to a certain extent
    very effective one too. It is heavily recommended if the shape of the clusters
    is suspected to be irregular.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类是相当高效且在一定程度上非常有效的解决方案。如果怀疑簇的形状是不规则的，强烈建议使用它。
- en: With this, we conclude our discussion on DBSCAN clustering. In the next section,
    we are solving a business use case on clustering. In the case study, the focus
    is less on technical concepts but more on the business understanding and the solution
    generation.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，我们结束了对DBSCAN聚类的讨论。在下一节中，我们将解决一个关于聚类的业务用例。在案例研究中，重点不太在技术概念上，而更多地在商业理解和解决方案生成上。
- en: 2.6 Case study using clustering
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 使用聚类的案例研究
- en: We will now define a case study which employs clustering as one of the solutions.
    The objective of the case study is to give you a flavour of the practical and
    real-life business world. This case study-based approach is also followed in job
    related interviews wherein a case is discussed during the interview stage. And
    hence, it is highly recommended for you to understand how we implement machine
    learning solutions in pragmatic business scenarios.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义一个使用聚类作为解决方案之一的案例研究。案例研究的目标是让你了解实际的商业世界。这种基于案例研究的方法也在与工作相关的面试中使用，在面试阶段会讨论一个案例。因此，强烈建议你了解我们如何在实际的商业场景中实施机器学习解决方案。
- en: A case study typically has a business problem, the data set available, the various
    solutions which can be used, challenges faced and the final chosen solution. We
    also discuss the issues faced while implementing the solution in real-world business.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 一个案例研究通常涉及一个商业问题，可用的数据集，可以使用的各种解决方案，面临的挑战以及最终选择的解决方案。我们还讨论在实际商业中实施解决方案时遇到的问题。
- en: So, let’s start our case study on clustering using unsupervised learning. In
    the case study, we are focusing on the steps we take to solve the case study and
    not on the technical algorithms, as there can be multiple technical solutions
    to a particular problem.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们开始使用无监督学习进行聚类的案例研究。在案例研究中，我们关注解决案例研究所采取的步骤，而不是技术算法，因为对于特定问题可能存在多个技术解决方案。
- en: '**Business context**: The industry we are considering can be retail, telecom,
    BFSI, aviation, health-care. Basically, any business which deals with customers
    (almost all businesses have customers). For any business, the objective is to
    generate more revenue for the business and ultimately to increase the overall
    profit of the business. And to increase the revenue, the business would wish to
    have increasingly newer customers. The business would also wish the existing consumers
    to buy more and buy more often. So, the business always strives to keep the consumers
    engaged, keep them happy and increase their transactional value with themselves.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业背景**：我们考虑的行业可能是零售、电信、银行金融保险、航空、医疗保健。基本上，任何涉及客户的业务（几乎所有业务都有客户）。对于任何业务，目标都是为业务产生更多收入，最终增加业务的整体利润。为了增加收入，业务希望拥有越来越多的新客户。业务也希望现有的消费者购买更多，更经常购买。因此，业务始终努力让消费者参与其中，让他们感到满意，并增加他们与自己的交易价值。'
- en: 'For this to happen, the business should have a thorough understanding of a
    consumer base, know their preferences, tastes, price points, liking of categories
    etc. And once the business has examined and understood the consumer base minutely,
    then:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，业务应该彻底了解消费者群体，了解他们的偏好、口味、价格点、对类别的喜好等。一旦业务详细审查并理解了消费者群体，那么：
- en: The product team can improve the product features as per the consumer’s need.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品团队可以根据消费者的需求改进产品特性。
- en: The pricing team can improve the price of the products by aligning them to customer’s
    preferred prices. The prices can be customized for a customer or loyalty discounts
    can be offered.
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定价团队可以通过将产品价格与客户的首选价格对齐来改进产品价格。价格可以根据客户定制，或者提供忠诚度折扣。
- en: The marketing team and customer relationship team (CRM), can target the consumers
    by a customized offer.
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场营销团队和客户关系团队（CRM）可以通过定制的优惠向消费者推广。
- en: The teams can win-back the consumers which are going to churn or stop buying
    from the business, can enhance their spend, increase the stickiness and increase
    the customer lifetime value.
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队可以挽回那些即将流失或停止购买业务的消费者，可以增加他们的消费、增加粘性并增加客户生命周期价值。
- en: Overall, different teams can align their offerings as per the understanding
    of the consumers generated. And the end consumer will be happier, more engaged,
    more loyal to the business leading to more fruitful consumer engagement.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总的来说，不同的团队可以根据生成的对消费者的理解来调整其提供的内容。最终消费者会更加幸福，更加投入，更加忠诚于业务，从而使消费者参与更加富有成果。
- en: The business hence has to dive deep into the consumers' data and generate an
    understanding of the base. The customer data can look like as shown in the next
    section.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，业务必须深入研究消费者的数据，并生成对基础的理解。客户数据可能看起来像下一节中所示的样子。
- en: '**Dataset for the analysis**: We are taking an example for an apparel retailer
    (H&M, Uniqlo etc). A retailer having a loyalty program saves the customer’s transaction
    details. The various (not exhaustive) data sources can be as shown below:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '**用于分析的数据集**：我们以服装零售商（H&M、优衣库等）为例。拥有忠诚计划的零售商保存客户的交易明细。各种（不是详尽）数据来源如下所示：'
- en: '![02_19c](images/02_19c.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![02_19c](images/02_19c.png)'
- en: We can have store details which have all the details of a store like store ID,
    store name, city, area, number of employees, etc. We can have the item hierarchies
    table which has all the details of the items like price, category, etc. Then we
    can have customer demographic details like age, gender, city, and customer transactional
    history which has details of the consumer’s past sales with us. Clearly, by joining
    such tables, we will be able to create a master table that will have all the details
    in one place.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以有存储详细信息的商店细节，如商店 ID、商店名称、城市、地区、员工数量等。我们可以有项目层次结构表，其中包含价格、类别等项目的所有详细信息。然后我们可以有客户人口统计详细信息，如年龄、性别、城市和客户与我们过去的销售的交易历史的详细信息。显然，通过联合这些表，我们将能够创建一个将所有详细信息放在一个地方的主表。
- en: Note
  id: totrans-416
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You are advised to develop a good skill set for SQL. It is required in almost
    each of the domains related to data – be it data science, data engineering or
    data visualization, SQL is ubiquitous.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您培养良好的 SQL 技能。几乎所有与数据相关的领域都需要它——无论是数据科学、数据工程还是数据可视化，SQL 都是无处不在的。
- en: We are depicting an example of a master table below. It is not an exhaustive
    list of variables and the number of variables can be much larger than the ones
    below. The master table has some raw variables like Revenue, Invoices, etc. and
    derived variables like Average Transaction value and Average basket size etc.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面展示了一个主表的例子。它不是变量的详尽列表，变量的数量可能比下面的这些要多得多。主表中包括一些原始变量，如收入、发票等，以及衍生变量，如平均交易金额和平均购物篮大小等。
- en: '![02_19d](images/02_19d.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![02_19d](images/02_19d.png)'
- en: We could also take an example of a telecom operator. In that subscriber usage,
    call rate, revenue, days spent on the network, data usage, etc. will be the attributes
    we will be analyzing. Hence, based on the business domain at hand, the data sets
    will change.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以以电信运营商为例。在这种情况下，我们将分析的属性包括用户使用情况、通话率、收入、在网络上停留的天数、数据使用等。因此，根据手头的业务领域，数据集可能会发生变化。
- en: Once we have got the data set, we generally create derived attributes from them.
    For example, the average transaction value attributes is total revenue divided
    by the number of invoices. We create such attributes in addition to the raw variables
    we already have.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获取到数据集，通常我们会从中创建衍生属性。例如，平均交易金额属性是总收入除以发票数量。除了已有的原始变量之外，我们会创建这样的属性。
- en: '**Suggested solutions**: There can be multiple solutions to the problem, some
    of which we are describing below:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '**建议的解决方案**：对于这个问题可能有多种解决方案，我们以下描述其中的一些：'
- en: We can create a dashboard to depict the major KPI (key performance indicators).
    It will allow us to analyze the history and take necessary actions based on it.
    But the solution will be more reporting in nature with trends, KPI which we already
    are aware of.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以创建一个仪表盘来展示主要的关键绩效指标（KPI）。它将允许我们分析历史数据并根据分析结果采取必要的行动。但这个解决方案更多是报告性质的，包括我们已经熟悉的趋势和KPI。
- en: We can perform data analysis using some of techniques we used in the solutions
    in the earlier sections. It will solve a part of the problem and moreover, it
    is difficult to consider multiple dimensions simultaneously.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用在前几节解决方案中使用的一些技术进行数据分析。这将解决问题的一部分，而且同时考虑多个维度是困难的。
- en: We can create predictive models to predict if the customers are going to shop
    in the coming months or going to churn in the next X days, but it will not solve
    the problem completely. To be clear, churn here refers that customer no longer
    shops with the retailer in the next X days. Here, duration X is defined as per
    the business domain. For example, for telecom domain X will be lesser than insurance
    domain. It is due to the fact that people use mobile phone everyday whereas for
    insurance domain, most customers might be paying premium yearly. So customer interaction
    is less for insurance.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以创建预测模型来预测客户在未来几个月是否会购物或在接下来的X天内流失，但这并不能完全解决问题。要明确，这里的流失指的是客户在接下来的X天内不再与零售商购物。在这里，持续时间X根据业务领域的不同而有所差异。例如，在电信领域，X的时间会比保险领域短。这是因为人们每天都在使用手机，而在保险领域，大部分客户可能一年只支付一次保费。因此，与保险业务相比，客户的互动较少。
- en: We can create customer segmentation solutions wherein we are grouping customers
    based on their historical trends and attributes. This is the solution we will
    use to solve this business problem.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以创建客户分割解决方案，根据客户的历史趋势和属性将客户分组。这是我们用来解决这个业务问题的解决方案。
- en: '**Solution for the problem**: Recall in Chapter 1 in Figure 1.9, where we discussed
    the steps, we follow in the machine learning algorithm. Everything starts with
    defining the business problem and then data discovery, pre-processing etc. For
    the case study above, we will utilize a similar strategy. We have already defined
    the business problem; data discovery is done and we have completed the EDA and
    pre-processing of the data. We wish to create a segmentation solution using clustering.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题的解决方案**：回想一下第一章中的图1.9，我们讨论了机器学习算法中的步骤。一切都始于定义业务问题，然后进行数据发现、预处理等。对于以上案例研究，我们将采用类似的策略。我们已经定义了业务问题；数据发现已经完成，我们已经完成了数据的探索性数据分析和预处理。我们希望使用聚类创建一个分割解决方案。'
- en: '**Step 1:** We start with finalizing the dataset we wish to feed to the clustering
    algorithms. We might have created some derived variables, treated some missing
    values or outliers etc. In the case study, we would want to know the minimum/maximum/average
    values of transactions, invoices, items bought, etc. We would be interested to
    know the gender and age distribution. We also would like to know the mutual relationships
    between these variables like if women customers use online mode more than male
    customers. All these questions are answered as part of this step.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：** 我们从确定要提供给聚类算法的数据集开始。我们可能已经创建了一些派生变量，处理了一些缺失值或异常值等。在案例研究中，我们想要知道交易、发票、购买商品的最小/最大/平均值等。我们对性别和年龄分布感兴趣。我们也想知道这些变量之间的相互关系，比如女性客户是否比男性客户更多地使用在线模式。所有这些问题都作为这一步的一部分得到回答。'
- en: A Python Jupyter notebook is checked-in at the Github repository, which provides
    detailed steps and code for the exploratory data analysis(EDA) and data pre-processing.
    Check it out!
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在Github存储库中提交了一个Python Jupyter笔记本，提供了探索性数据分析（EDA）和数据预处理的详细步骤和代码。快去看看吧！
- en: '**Step 2:** We create the first solution using k-means clustering followed
    by hierarchical clustering. For each of the algorithms, iterations are done by
    changing hyperparameters. In the case study, we will choose parameters like the
    number of visits, total revenue, distinct categories purchased, online/offline
    transactions ratio, gender, age, etc. as parameters for clustering.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：** 我们使用k均值聚类和层次聚类创建第一个解决方案。对于每个算法，通过更改超参数进行迭代。在案例研究中，我们将选择访问次数、总收入、购买的不同类别、在线/离线交易比率、性别、年龄等作为聚类参数。'
- en: '**Step 3:** A final version of the algorithm and respective hyperparameters
    are chosen. The clusters are analyzed further in the light of business understanding.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：** 选择算法的最终版本以及相应的超参数。根据业务理解进一步分析聚类。'
- en: '**Step 4:** More often, the clusters are merged or broken, depending on the
    size of the observations and the nature of the attributes present in them. For
    example, if the total customer base is 1 million, it will be really hard to action
    on a cluster of size 100\. At the same time, it will be equally difficult to manage
    a cluster of size 700,000.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4：** 更常见的情况是，根据观察的大小和它们所包含属性的性质，对聚类进行合并或分割。例如，如果总客户群有100万人，要对100个人的聚类采取行动将非常困难。同时，要管理70万人的聚类同样也很困难。'
- en: '**Step 5:** We then analyze the clusters we have finally got. The clusters
    distribution is checked for the variables, their distinguishing factors are understood
    and we give logical names to the clusters. We can expect to see such a clustering
    output as shown in (Figure 3-) below.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5：** 然后，我们分析最终得到的聚类。检查变量的聚类分布，理解它们的区别因素，并为聚类赋予逻辑名称。我们可以期待在下面的（图3-）中看到这样的聚类输出。'
- en: In the example clusters shown below, we have depicted spending patterns, responsiveness
    to previous campaigns, life stage, and overall engagement as a few dimensions.
    And respective sub-divisions of each of these dimensions have also been shown.
    The clusters will be a logical combination of these dimensions. The actual dimensions
    can be much higher.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面示例的聚类中，我们描述了消费模式、对之前活动的反应、生命周期阶段和整体参与度等几个维度。还展示了每个维度的相应子细分。聚类将是这些维度的逻辑组合。实际的维度可能会更多。
- en: '![02_19e](images/02_19e.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![02_19e](images/02_19e.png)'
- en: The segmentation shown above can be used for multiple domains and businesses.
    The parameters and attributes might change, the business context is different,
    the extent of data available might vary – but the overall approach remains similar.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示的细分可以用于多个领域和业务。参数和属性可能会改变，业务背景不同，可用数据的范围可能会有所不同，但总体方法保持相似。
- en: 'In addition to the few applications, we saw in the last section, we are examining
    some of the use cases now:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上一节中看到的少数应用之外，我们现在正在研究一些用例：
- en: Market research utilizes clustering to segment the groups of consumers into
    market segments. And then the groups can be analyzed better in terms of their
    preferences. The product placement can be improved, pricing can be made tighter
    and geography selection will be more scientific.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 市场研究利用聚类将消费者分组为市场细分。然后可以更好地分析这些组的偏好。产品摆放可以改进，定价可以更紧密，地理选择将更加科学。
- en: In the bioinformatics and medical industry, clustering can be used to group
    the genes into distinct categories. Groups of genes can be segmented and comparisons
    can be assessed by analyzing the attributes of the groups.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生物信息学和医疗行业，聚类可用于将基因分组为不同的类别。基因组可以被划分为不同的组，并且可以通过分析组的属性来进行比较。
- en: It is used as an effective data pre-processing step before we create algorithms
    using supervised learning solutions. It can also be used to reduce the data size
    by focusing on the data points belonging to a cluster.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它被用作在创建使用监督学习解决方案的算法之前的有效数据预处理步骤。它还可以通过关注属于一个聚类的数据点来减少数据大小。
- en: It is utilized for pattern detection across both structured and unstructured
    datasets. We have already studied the case for a structured dataset. For text
    data, it can be used to group similar types of documents, journals, news, etc.
    We can also employ clustering to work and develop solutions for images. We are
    going to study unsupervised learning solutions for text and images in later chapters.
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它被用于跨结构化和非结构化数据集的模式检测。我们已经研究了结构化数据集的情况。对于文本数据，它可以用于对类似类型的文档、期刊、新闻等进行分类。我们还可以利用聚类来处理并为图像开发解决方案。我们将在后续章节中研究文本和图像的无监督学习解决方案。
- en: As the algorithms work on similarity measurements, it can be used to segment
    the incoming data set as fraud or genuine, which can be used to reduce the amount
    of criminal activities.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于算法基于相似性度量，因此可以用于将传入的数据集分段为欺诈或真实数据，这可以用来减少犯罪活动的数量。
- en: The use cases of clustering are quite a lot. We have discussed only the prominent
    ones. It is one of the algorithms which change the working methodologies and generate
    a lot of insights around the data. It is widely used across telecom, retail, BFSI,
    aviation, etc.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的使用案例相当多。我们只讨论了其中一些突出的案例。它是一种改变工作方法并在数据周围生成大量见解的算法之一。它被广泛应用于电信、零售、银行保险、航空等领域。
- en: At the same time, there are a few issues with the algorithm. We will now examine
    the common problems we face with clustering in the next section.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，该算法也存在一些问题。接下来我们将在下一节中讨论我们在聚类中常见的问题。
- en: 2.7 Common challenges faced in clustering
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 聚类中面临的常见挑战
- en: 'Clustering is not a completely straight-forward solution without any challenges.
    Similar to any other solution in the world, clustering too has its share of issues
    faced. We are discussing the most common challenges we face in clustering which
    are:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类并不是一个完全直截了当、没有任何挑战的解决方案。与世界上任何其他解决方案类似，聚类也面临着自己的一些问题。我们正在讨论我们在聚类中面临的最常见的挑战，它们包括：
- en: 'Sometimes the magnitude of the data is quite big and there are a lot of dimensions
    available. In such a case, it becomes really difficult to manage the data set.
    The computation power might be limited and like any project, there is finite time
    available. To overcome the issue, we can:'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时，数据的数量非常大且有许多维度可用。在这种情况下，难以管理数据集。计算能力可能是有限的，而且像任何项目一样，时间是有限的。为了解决这个问题，我们可以：
- en: Try to reduce the number of dimensions by finding the most significant variables
    by using a supervised learning-based regression approach or decision tree algorithm
    etc.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用监督学习回归方法或决策树算法等方法找到最重要的变量，尝试通过减少维度数量。
- en: Reduce the number of dimensions by employing Principal Component Analysis (PCA)
    or Singular Value Decomposition (SVD) etc.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用主成分分析（PCA）或奇异值分解（SVD）等方法来减少维度数量。
- en: 'Noisy data set: “Garbage in garbage out” – this cliché is true for clustering
    too. If the data set is messy it creates a lot of problems. The issues can be:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嘈杂的数据集：“垃圾进了，垃圾出”-这个陈词滥调对于聚类也是真实的。如果数据集混乱，会引发很多问题。问题可能包括：
- en: Missing values i.e. NULL, NA, ? , blanks etc.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缺失值，即NULL、NA、?、空白等。
- en: Outliers are present in the dataset.
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集中存在异常值。
- en: 'Junk values are present like #€¶§^ etc. are present in the dataset.'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集中存在类似#€¶§^等垃圾值。
- en: Wrong entries are made in the data. For example, if name are entered in the
    revenue field it is an incorrect entry.
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据中存在错误的输入。例如，如果将名称输入到收入字段中，那就是一个错误的输入。
- en: We are going to discuss the steps and process to resolve these issues in each
    of the chapters. In this chapter, we are examining – how to work with categorical
    variables
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每个章节讨论解决这些问题的步骤和过程。在本章中，我们正在研究 - 如何处理分类变量
- en: 'Categorical variables: Recall that while discussing we discussed the issue
    with k-means not able to use categorical variables. We are solving that issue
    now.'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类变量：回想一下，在讨论中我们讨论过 k-means 无法使用分类变量的问题。我们正在解决这个问题。
- en: To convert categorical variables into numeric one, we can use *one-hot encoding*.
    This technique adds additional columns equal to the number of distinct classes
    as shown in (Figure 2.) below. The variable city has unique values as London and
    NewDelhi. We can observe that two additional columns have been created with 0
    or 1 filled for the values.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 要将分类变量转换为数值变量，我们可以使用*独热编码*。该技术在下图（Figure 2.）中显示的变量 `city` 有唯一值 `London` 和 `NewDelhi`。我们可以观察到已创建了两个额外的列，用于填充值为0或1。
- en: '![02_19f](images/02_19f.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![02_19f](images/02_19f.png)'
- en: But using one-hot encoding does not always ensure an effective and efficient
    solution. Imagine if the number of cities in the example above is 100, then we
    will have 100 additional columns in the dataset and most of the values will be
    filled with zero. Hence, in such a situation it is advisable to group a few values.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 但是使用独热编码并不能始终保证有效和高效的解决方案。想象一下，如果上述例子中的城市数量是100，那么数据集中将会有100个额外的列，而且其中大部分值都将填充为零。因此，在这种情况下，建议对几个值进行分组。
- en: 'Distance metrics: with different distance metrics we might get different results.
    Though there is no “one size fits all”, mostly you would find Euclidean distance
    is used for measuring distance.'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 距离度量：使用不同的距离度量可能会得到不同的结果。虽然没有“一刀切”，但大多数情况下，欧几里德距离被用于测量距离。
- en: Interpretations for the clusters are quite subjective. By using different attributes,
    completely different clustering can be done for the same datasets. As discussed
    earlier, the focus should be on solving the business problem at hand. This holds
    the key to choosing the hyperparameters and the final algorithm.
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对聚类的解释是非常主观的。通过使用不同的属性，可以对相同的数据集进行完全不同的聚类。正如前面讨论的那样，重点应该放在解决手头的业务问题上。这是选择超参数和最终算法的关键。
- en: 'Time-consuming: since a lot of dimensions have to be dealt with simultaneously,
    sometimes converging the algorithm takes a lot of time.'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 耗时：由于同时处理了许多维度，有时算法的收敛需要很长时间。
- en: But despite all these challenges, clustering is a widely recognized and utilized
    technique. We have discussed the use cases of clustering in the real-world in
    the last section.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 但是尽管面临所有这些挑战，聚类仍然是一种广泛认可和使用的技术。我们在最后一节中讨论了聚类在现实世界中的应用案例。
- en: This marks the end of the discussion on challenges with clustering. Let’s conclude
    the chapter with some closing thoughts.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着本章关于聚类的讨论结束。让我们用一些总结思考来结束本章。
- en: 2.8 Concluding Thoughts
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 总结思考
- en: Unsupervised learning is not an easy task. But it is certainly a very engaging
    one. It does not require any target variable and the solution identifies the patterns
    itself, which is one of the biggest advantages of unsupervised learning algorithms.
    And the implementations are already creating a great impact on the business world.
    We studied one of such solution classes called clustering in this chapter.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习不是一项易事。但它肯定是一项非常有吸引力的工作。它不需要任何目标变量，解决方案自身识别模式，这是无监督学习算法最大的优点之一。并且这些实现已经在商业世界产生了巨大的影响。在本章中，我们研究了一类称为聚类的解决方案。
- en: Clustering is an unsupervised learning solution that is useful for pattern identifications,
    exploratory analysis and of course segmenting the data points. Organizations heavily
    use clustering algorithms and proceed to the next level of understanding consumer
    data. Better prices can be offered, more relevant offers can be suggested, consumer
    engagement can be improved and overall customer experience becomes better. After
    all, a happy consumer is the goal of any business. Not only structured data, we
    can use clustering for text data, images, videos, and audio too. Owing to its
    capability in finding patterns across multiple data sets using a large number
    of dimensions – clustering is the go-to solution whenever we want to analyze multiple
    dimensions together.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习解决方案，用于模式识别、探索性分析和数据点的分割。组织机构广泛使用聚类算法，并继续深入了解消费者数据。可以提供更好的价格、提供更相关的优惠、提高消费者参与度，并改善整体客户体验。毕竟，满意的消费者是任何企业的目标。不仅可以对结构化数据使用聚类，还可以对文本数据、图像、视频和音频使用聚类。由于其能够使用大量维度在多个数据集中找到模式，聚类是想要一起分析多个维度时的解决方案。
- en: In this second chapter of this book, we introduced concepts of unsupervised-based
    clustering methods. We examined different types of clustering algorithms – k-means
    clustering, hierarchical clustering, and DBSCAN clustering along with their mathematical
    concepts, respective use cases, and pros and cons with an emphasis on creating
    actual Python code for the same datasets.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二章中，我们介绍了基于无监督的聚类方法的概念。我们研究了不同类型的聚类算法——k均值聚类、层次聚类和DBSCAN聚类，以及它们的数学概念、各自的用例以及优缺点，重点放在为相同数据集创建实际Python代码上。
- en: In the following chapter, we will study dimensionality reduction techniques
    like PCA and SVD. The building blocks for techniques, their mathematical foundation,
    advantages and disadvantages, use cases and actual Python implementation will
    be done.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将研究像 PCA 和 SVD 这样的降维技术。将对技术的构建模块、它们的数学基础、优点和缺点、用例以及实际的Python实现进行讨论。
- en: You can proceed to the question section now!
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以进入问题部分了！
- en: Practical next steps and suggested readings
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实用的下一步和建议阅读材料
- en: Get the online retail data from the link ([https://www.kaggle.com/hellbuoy/online-retail-customer-clustering](hellbuoy.html)).
    This dataset contains all the online transactions occurring between 1/12/2010
    and 09/12/2011 for a UK based retailer. Apply the three algorithms described in
    the chapter to identify which customers the company should target and why.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从链接获取在线零售数据（[https://www.kaggle.com/hellbuoy/online-retail-customer-clustering](hellbuoy.html)）。这个数据集包含了一个总部位于英国的零售商在2010年12月1日至2011年12月9日期间发生的所有在线交易。应用本章描述的三种算法，确定公司应该针对哪些客户以及为什么。
- en: Get the IRIS dataset from the link ([https://www.kaggle.com/uciml/iris](uciml.html)).
    It includes three iris species with 50 samples each having some properties of
    the flowers. Use kmeans and DBSCAN and compare the results.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从链接获取 IRIS 数据集（[https://www.kaggle.com/uciml/iris](uciml.html)）。它包括三种鸢尾花品种，每种50个样本，具有一些花的特性。使用
    kmeans 和 DBSCAN 并比较结果。
- en: Explore the dataset at UCI for clustering ([http://archive.ics.uci.edu/ml/index.php](.html))
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索 UCI 的聚类数据集（[http://archive.ics.uci.edu/ml/index.php](.html)）
- en: Study the following papers on kmeans clustering, hierarchical clustering and
    DBSCAN clustering
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究关于 kmeans 聚类、层次聚类和 DBSCAN 聚类的以下论文
- en: 'a) Kmeans algorithm:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 'a) Kmeans 算法:'
- en: i.   [https://www.ee.columbia.edu/~dpwe/papers/PhamDN05-kmeans.pdf](papers.html)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: i.   [https://www.ee.columbia.edu/~dpwe/papers/PhamDN05-kmeans.pdf](papers.html)
- en: ii.   [https://www.researchgate.net/publication/271616608_A_Clustering_Method_Based_on_K-Means_Algorithm](publication.html)
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ii.   [https://www.researchgate.net/publication/271616608_A_Clustering_Method_Based_on_K-Means_Algorithm](publication.html)
- en: iii.   [https://ieeexplore.ieee.org/document/1017616](document.html)
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: iii.   [https://ieeexplore.ieee.org/document/1017616](document.html)
- en: b) Hierarchical clustering
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: b) 层次聚类
- en: i.   [https://ieeexplore.ieee.org/document/7100308](document.html)
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: i.   [https://ieeexplore.ieee.org/document/7100308](document.html)
- en: ii.   [https://papers.nips.cc/paper/7200-hierarchical-clustering-beyond-the-worst-case.pdf](paper.html)
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: ii.   [https://papers.nips.cc/paper/7200-hierarchical-clustering-beyond-the-worst-case.pdf](paper.html)
- en: iii.   [https://papers.nips.cc/paper/8964-foundations-of-comparison-based-hierarchical-clustering.pdf](paper.html)
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: iii.   [https://papers.nips.cc/paper/8964-foundations-of-comparison-based-hierarchical-clustering.pdf](paper.html)
- en: c) DBSCAN clustering
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: c) DBSCAN 聚类
- en: i.   [https://arxiv.org/pdf/1810.13105.pdf](pdf.html)
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: i.   [https://arxiv.org/pdf/1810.13105.pdf](pdf.html)
- en: ii.   [https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.9220](viewdoc.html)
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ii.   [https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.9220](viewdoc.html)
- en: 2.9 Summary
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 摘要
- en: We discussed an unsupervised learning technique known as clustering. Using clustering,
    we find out the underlying patterns in a data set and find out the natural groupings
    in the data.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了一种称为聚类的无监督学习技术。通过聚类，我们找出数据集中的潜在模式，并找出数据中的自然分组。
- en: We understood that clustering is used for a variety of purposes across all industries,
    be it retail, telecom, finance, pharma, etc. Clustering solutions are implemented
    for customer segmentation, and marketing segmentation to understand the customer
    base better which further improves the targeting of the customers.
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们了解到，聚类在各行业中都被用于各种目的，包括零售、电信、金融、制药等。聚类解决方案被用于客户分割和营销分割，以更好地理解客户群体，从而进一步提高对客户的定位。
- en: We studied and understood that there can be multiple clustering techniques based
    on the methodology. A few examples are K-means clustering, hierarchical clustering,
    DBSCAN, fuzzy clustering, etc.
  id: totrans-490
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们学习并理解了基于方法论的多种聚类技术。一些示例包括k均值聚类、层次聚类、DBSCAN、模糊聚类等。
- en: We covered K-means clustering, hierarchical clustering, and DBSCAN clustering
    algorithms in detail.
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们详细介绍了K均值聚类、层次聚类和DBSCAN聚类算法。
- en: We studied that kmeans is based on the centroid of the cluster while hierarchical
    clustering is an agglomerative clustering technique. DBSCAN is a density-based
    clustering algorithm.
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们学习了k均值（kmeans）是基于聚类的质心，而层次聚类是一种凝聚式聚类技术。DBSCAN是一种基于密度的聚类算法。
- en: We also went through the pros and cons of these clustering algorithms in detail.
    For example, for kmeans we have to specify the number of clusters, hierarchical
    clustering is quite time-consuming while DBSCAN’s output depends on the order
    of processing of observations.
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还详细讨论了这些聚类算法的优缺点。例如，对于k均值，我们必须指定聚类的数量，层次聚类非常耗时，而DBSCAN的输出取决于观测数据处理的顺序。
- en: We covered that to measure the accuracy of the clustering technique we can take
    the help of WCSS, inter-cluster sum of squares, Silhouette value and Dunn Index.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了用于测量聚类技术准确性的方法，包括WCSS（组内平方和）、轮廓系数和Dunn指数。
- en: We implemented Python-based solutions for each of the techniques. The main library
    used is sklearn.
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们为每种技术实现了基于Python的解决方案。主要使用的库是sklearn。
- en: Towards the end of the chapter, we had the practical case study to complement
    the study.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章末尾，我们提供了实际案例研究来补充学习。
