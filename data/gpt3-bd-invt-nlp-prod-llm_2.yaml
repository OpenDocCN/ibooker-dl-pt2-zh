- en: Chapter 2\. Using the OpenAI API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。使用 OpenAI API
- en: Even though GPT-3 is the most sophisticated and complex language model in the
    world, its capabilities are abstracted to a simple “text-in, text-out” interface
    to end users. This chapter will get you started with using that interface, Playground,
    and cover the technical nuances of the OpenAI API, because it is always the details
    that reveal the true gems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 GPT-3 是世界上最复杂和复杂的语言模型，其功能也被抽象为一个简单的“文本输入，文本输出”的接口提供给最终用户。本章将带您开始使用该接口、Playground，并涵盖
    OpenAI API 的技术细节，因为细节总是揭示真正的精华。
- en: To work through this chapter, you will need to sign up for an OpenAI account
    at [*https://beta.openai.com/signup*](https://beta.openai.com/signup). If you
    haven’t done that, please do so now.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的学习，您需要在 [*https://beta.openai.com/signup*](https://beta.openai.com/signup)
    注册一个 OpenAI 账户。如果您还没有，请立即注册。
- en: Navigating the OpenAI Playground
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浏览 OpenAI Playground
- en: 'Your OpenAI developer account provides access to the API and infinite possibilities.
    We’ll start with the Playground, a web-based sandbox environment that allows you
    to experiment with the API, learn how its components work, and access developer
    documentation and the OpenAI community. We will then show you how to build robust
    prompts that generate favorable responses for your application. We’ll finish the
    chapter with examples of GPT-3 performing four NLP tasks: classification, named
    entity recognition (NER), summarization, and text generation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 OpenAI 开发者账户提供了访问 API 和无限可能性的权限。我们将从 Playground 开始，这是一个基于 Web 的沙盒环境，允许您尝试
    API，了解其组件的工作原理，并访问开发者文档和 OpenAI 社区。然后，我们将向您展示如何构建强大的提示，以生成您应用程序的良好响应。最后，我们将通过
    GPT-3 执行四个自然语言处理任务的示例来结束本章：分类、命名实体识别（NER）、摘要和文本生成。
- en: In an interview with Peter Welinder, vice president of product and partnerships
    at OpenAI, we asked for key advice on navigating the Playground for first-time
    users. He told us his advice depends on the persona of the user. If the user has
    a machine learning background, Peter encourages them to “start by forgetting the
    things that they already know, and just go to the Playground and try to get GPT-3
    to do what you [want] it to do by just asking it.” He suggests users “imagine
    GPT-3 as a friend or a colleague that you’re asking to do something. How would
    you describe the task that you want them to do? And then, see how GPT-3 responds.
    And if it doesn’t respond in the way that you want, iterate on your instructions.”
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次与 OpenAI 产品和合作副总裁彼得·韦林德（Peter Welinder）的采访中，我们询问了如何为首次使用 Playground 的用户提供关键建议。他告诉我们，他的建议取决于用户的角色。如果用户具有机器学习背景，彼得鼓励他们“首先忘记他们已经知道的东西，然后只需进入
    Playground，尝试让 GPT-3 做你希望它做的事情，只需询问它。”他建议用户“把 GPT-3 想象成你要求做某事的朋友或同事。你会如何描述你希望他们做的任务？然后，看看
    GPT-3 如何回应。如果它的回应不符合你的要求，就修改你的指令。”
- en: 'As YouTuber and NLP influencer [Bakz Awan](https://oreil.ly/sPTfo) puts it,
    “The non-technical people ask: Do I need a degree to use this? Do I need to know
    how to code to use it? Absolutely not. You can use the Playground. You don’t need
    to write a single line of code. You’ll get results instantly. Anybody can do this.”'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 YouTuber 和 NLP 影响者 [Bakz Awan](https://oreil.ly/sPTfo) 所说：“非技术人员问：我需要学位来使用这个吗？我需要懂编程才能使用它吗？绝对不需要。您可以使用
    Playground。您不需要编写一行代码。您会立即得到结果。任何人都可以做到。”
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before you start using the Playground, we recommend reading OpenAI’s [Quickstart
    tutorial](https://oreil.ly/Zivxx) guide and the [developer documentation](https://oreil.ly/btPCR).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用 Playground 之前，我们建议阅读 OpenAI 的 [快速入门教程](https://oreil.ly/Zivxx) 和 [开发者文档](https://oreil.ly/btPCR)。
- en: 'Here are the steps to get started with the Playground:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是开始使用 Playground 的步骤：
- en: Log in at [*https://openai.com*](https://openai.com) and navigate to the Playground
    from the main menu.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到 [*https://openai.com*](https://openai.com)，并从主菜单导航到 Playground。
- en: Take a look at the Playground screen ([Figure 2-1](#the_playground_interfacecomma_screensho)).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 Playground 屏幕（[图 2-1](#the_playground_interfacecomma_screensho)）。
- en: The big text box marked 1 is where you provide text input (prompts).
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记为 1 的大文本框是您提供文本输入（提示）的位置。
- en: The box marked 2 on the right is the parameter-setting pane, which enables you
    to tweak the parameters.
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧标记为 2 的框是参数设置窗格，它使您可以调整参数。
- en: 'The box marked 3 allows you to load a *preset*: an example prompt and Playground
    settings. Provide your own training prompt or load an existing preset.'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记为 3 的框允许您加载一个 *预设*：一个示例提示和 Playground 设置。提供您自己的训练提示或加载现有的预设。
- en: '![](Images/gpt3_0201.png)'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](Images/gpt3_0201.png)'
- en: Figure 2-1\. The Playground interface, screenshot taken on January 10, 2022
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. Playground 界面，2022 年 1 月 10 日截图
- en: Select the existing Q&A preset (marked 3). This will automatically load the
    training prompt along with the associated parameter settings. Click the Generate
    button (marked 4 in [Figure 2-1](#the_playground_interfacecomma_screensho)).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择现有的问答预设（标记为 3）。这将自动加载训练提示以及相关的参数设置。单击生成按钮（在[图 2-1](#the_playground_interfacecomma_screensho)中标记为
    4）。
- en: The API processes your input and provides a response (called a *completion*)
    in the same text box. It also shows you the number of tokens utilized. *Tokens*
    are numerical representations of words used to determine the pricing of each API
    call; we’ll discuss them later in this chapter.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: API 处理您的输入并在同一文本框中提供响应（称为*完成*）。它还显示了您使用的令牌数量。*令牌*是用于确定每个 API 调用价格的单词的数值表示；我们将在本章后面讨论它们。
- en: At the bottom of the screen on the right you’ll see the token count and on the
    left you have a Generate button (see [Figure 2-2](#qampersanda_prompt_completion_along_wit)).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 屏幕底部右侧是令牌计数，左侧是一个生成按钮（参见[图 2-2](#qampersanda_prompt_completion_along_wit)）。
- en: '![](Images/gpt3_0202.png)'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](Images/gpt3_0202.png)'
- en: Figure 2-2\. Q&A prompt completion along with token count
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 带令牌计数的问答提示完成
- en: Every time you click the Generate button, GPT-3 takes the prompt and completions
    within the text input field into account and treats them as part of your training
    prompt for the next completion.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次单击生成按钮，GPT-3 都会考虑文本输入字段中的提示和完成，并将它们视为下一个完成的训练提示的一部分。
- en: 'Here is the prompt you can see in [Figure 2-2](#qampersanda_prompt_completion_along_wit):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您可以在[图 2-2](#qampersanda_prompt_completion_along_wit)中看到的提示：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And here is the completion:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完成：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that you understand the basic outline of the Playground, let’s get into
    the nitty gritty of prompt engineering and design.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了 Playground 的基本概述，让我们深入了解提示工程和设计的细节。
- en: Prompt Engineering and Design
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程和设计
- en: The OpenAI API radically changed the way we interact with an AI model, stripping
    out layers and layers of complicated programming languages and frameworks. Andrej
    Karpathy, director of AI at Tesla, said jokingly as soon as GPT-3 was released
    that programming 3.0 is all about prompt design (the meme he tweeted is in [Figure 2-3](#meme_source_unknowncomma_tweeted_by_and)).
    There is a direct relation between the training prompt you provide and the quality
    of the completion you get. The structure and arrangement of your words heavily
    influence the output. Understanding prompt design is the key to unlocking GPT-3’s
    true potential.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API 彻底改变了我们与 AI 模型互动的方式，剥离了复杂的编程语言和框架。特斯拉人工智能总监 Andrej Karpathy 在 GPT-3
    发布后开玩笑地说，编程 3.0 就是关于提示设计的（他发布的表情包在[图 2-3](#meme_source_unknowncomma_tweeted_by_and)）。您提供的训练提示与您获得的完成质量之间存在直接关系。您的文字结构和排列方式会严重影响输出。了解提示设计是释放
    GPT-3 真正潜力的关键。
- en: The secret to writing good prompts is understanding what GPT-3 knows about the
    world. As Awan points out, “It has only seen text. That means you shouldn’t expect
    that it knows about the physical world, even though it obviously does. It could
    describe the Mona Lisa, [could] tell you [about] the significance, the importance,
    the history [of] it probably, but it’s never seen [the painting] because it’s
    only trained on text.”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 写好提示的秘诀在于了解 GPT-3 对世界的了解。正如 Awan 指出的那样：“它只看过文本。这意味着你不应该期望它了解物理世界，尽管它显然了解。它可以描述《蒙娜丽莎》，[可能]告诉你[关于]它的意义，重要性，历史[的]，但它从未见过[这幅画]，因为它只受过文本训练。”
- en: Your job is to get the model to use the information it already has to generate
    useful results. In the game of charades, the performer gives the other players
    just enough information to figure out the secret word. Similarly, with GPT-3,
    we give the model just enough context (in the form of a training prompt) to figure
    out patterns and perform the given task.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你的工作是让模型利用它已有的信息生成有用的结果。在字谜游戏中，表演者给其他玩家足够的信息来猜出秘密单词。同样，对于 GPT-3，我们只给模型足够的上下文（以训练提示的形式）来找出模式并执行给定的任务。
- en: '![](Images/gpt3_0203.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0203.png)'
- en: Figure 2-3\. Meme source unknown, [tweeted by Andrej Karpathy](https://oreil.ly/Fs6hp)
    on June 18, 2020
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 表情包来源未知，由[Andrej Karpathy 在 2020 年 6 月 18 日发布](https://oreil.ly/Fs6hp)
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'While designing the training prompt, aim for a *zero-shot* response from the
    model: that is, see if you can get the kind of response you want without priming
    the model with external training examples. If not, move forward by showing it
    a few examples rather than an entire dataset. The standard flow for designing
    a training prompt is to try for zero-shot first, then few-shot, then go for corpus-based
    fine-tuning (described later in this chapter).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计训练提示时，目标是从模型中获得一个*零-shot*的响应：也就是说，看看你是否可以在没有引导模型的外部训练示例的情况下获得你想要的响应。如果不能，那么向前移动，而不是向它展示整个数据集，而是向它展示一些例子。设计训练提示的标准流程是首先尝试零-shot，然后是几-shot，然后是基于语料库的微调（稍后在本章中描述）。
- en: GPT-3 is the first step toward general purpose artificial intelligence and thus
    has its limitations. It doesn’t know everything and can’t reason on a human level,
    but it’s highly capable when you know how to talk to it. That’s where the art
    of prompt engineering comes in.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是通向通用人工智能的第一步，因此它也有其局限性。它并不知道一切，也无法以人类的方式推理，但是当你知道如何与它交流时，它是非常有能力的。这就是提示工程艺术的地方。
- en: 'GPT-3 isn’t a truth-teller, but it is an exceptional story-teller. It takes
    in the text input and attempts to respond with the text it thinks best completes
    the input. If you give it a few lines from your favorite novel, it will try to
    continue in the same style. It works by navigating through the context, and without
    proper context, it can generate inconsistent responses. Let’s look at an example
    to understand how GPT-3 processes the input prompt and generates the output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3并非真相说者，但它是一位出色的故事讲述者。它接受文本输入，并试图以它认为最佳完成输入的文本来回应。如果你给它几行你最喜欢的小说，它会试图以相同风格继续。它通过导航上下文来工作，没有适当的上下文，它会生成不一致的回应。让我们看一个例子，以了解GPT-3是如何处理输入提示并生成输出的：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you provide a prompt like this to GPT-3 without any context, you are essentially
    asking it to look for general answers from its universe of training data. The
    result will be generalized and inconsistent responses, as the model doesn’t know
    which part of training data to use for answering the question.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你向GPT-3提供这样的提示而没有任何上下文，你实际上是在要求它从其训练数据宇宙中寻找一般性答案。结果将是一般化和不一致的回应，因为模型不知道使用哪部分训练数据来回答问题。
- en: 'On the other hand, providing the right context will exponentially improve the
    quality of responses. It simply limits the universe of training data that the
    model has to examine for answering a question, resulting in more specific and
    to-the-point responses:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，提供正确的上下文将会极大地提高回应的质量。它简单地限制了模型必须检查以回答问题的训练数据的宇宙，从而产生更具体和切题的回应：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can think of GPT-3 processing the input in the same way as the human brain.
    When somebody asks us any question without proper context we tend to give random
    responses. This happens because without any proper direction or context, it’s
    difficult to get to the precise response. The same is true of GPT-3; its universe
    of training data is so big that it’s difficult to navigate to a correct response
    without any external context or direction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把GPT-3处理输入的方式想象成人类大脑的处理方式。当有人向我们提出任何没有适当上下文的问题时，我们往往会给出随机的回应。这是因为没有任何适当的方向或上下文，很难得到精确的回应。GPT-3也是如此；它的训练数据宇宙如此之大，以至于在没有任何外部上下文或方向的情况下，很难导航到正确的响应。
- en: 'LLMs like GPT-3 are capable of creative writing and answering factual questions
    given the right context. Here is our five-step formula for creating efficient
    and effective training prompts:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-3这样的LLM在给定正确上下文的情况下能够进行创意写作和回答事实性问题。以下是我们用于创建高效和有效的训练提示的五步公式：
- en: Define the problem you are trying to solve and what kind of NLP task it is,
    such as classification, Q&A, text generation, or creative writing.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义你试图解决的问题以及它是什么类型的NLP任务，例如分类、问答、文本生成或创意写作。
- en: Ask yourself if there is a way to get a zero-shot solution. If you think that
    you need external examples to prime the model for your use case, think really
    hard.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想一想是否有办法得到一个零-shot解决方案。如果你认为你需要外部示例来为你的用例引导模型，那么请认真考虑。
- en: 'Now think of how you might formulate the problem in a textual fashion given
    the “text-in, text-out” interface of GPT-3\. Think about all the possible scenarios
    to represent your problem in textual form. For example, say you want to build
    an ad copy assistant that can generate creative copy by looking at product name
    and description. To frame this goal in the “text-in, text-out” format, you can
    define the input as the product name and description and the output as the ad
    copy:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在想一想，考虑到 GPT-3 的“文本输入，文本输出”接口，您可能如何以文本形式表达问题。考虑所有可能的场景来以文本形式表示您的问题。例如，假设您想要构建一个广告文案助手，可以通过查看产品名称和描述来生成创意文案。为了以“文本输入，文本输出”格式来界定这个目标，您可以将输入定义为产品名称和描述，输出定义为广告文案：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you do end up using external examples, use as few as possible and try to
    incorporate diversity, capturing all the representations to avoid overfitting
    the model or skewing the predictions.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您最终使用外部示例，请尽量少用，并尝试包含多样性，捕捉所有表示以避免模型过拟合或预测偏差。
- en: These steps will act as a standard framework whenever you create a training
    prompt from scratch. Before you can build an end-to-end solution for your data
    problems, you need to understand a few more things about how the API works. Let’s
    dig deeper by looking at its components.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将作为每次从零开始创建训练提示时的标准框架。在您可以为数据问题构建端到端解决方案之前，您需要更多地了解 API 的工作原理。让我们通过查看其组件来深入了解。
- en: How the OpenAI API Works
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI API 的工作原理
- en: We’ll discuss all of these components in [Figure 2-4](#components_of_the_api)
    in more detail in the chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在章节中更详细地讨论 [图 2-4](#components_of_the_api) 中的所有这些组件。
- en: '![](Images/gpt3_0204.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0204.png)'
- en: Figure 2-4\. Components of the API
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. API 的组件
- en: '[Table 2-1](#components_in_the_openai_api) shows an overview of the components
    in the OpenAI API.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2-1](#components_in_the_openai_api) 显示了 OpenAI API 中组件的概述。'
- en: Table 2-1\. Components in the OpenAI API
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. OpenAI API 中的组件
- en: '| Component | Function |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 功能 |'
- en: '| --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Execution engine | Determines the language model used for execution |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 执行引擎 | 确定用于执行的语言模型 |'
- en: '| Response length | Sets a limit on how much text the API includes in its completion
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 响应长度 | 设置 API 在其完成中包含的文本量的限制 |'
- en: '| Temperature and Top P | Temperature controls the randomness of the response,
    represented as a range from 0 to 1. Top P controls how many random results the
    model should consider for completion, as suggested by the temperature; it determines
    the scope of randomness. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 温度和 Top P | 温度控制响应的随机性，表示为从 0 到 1 的范围。Top P 控制模型应考虑的随机结果数量，如温度所示；它确定了随机性的范围。
    |'
- en: '| Frequency penalty and Presence penalty | Frequency penalty decreases the
    likelihood that the model will repeat the same line verbatim by “punishing” it.
    Presence penalty increases the likelihood that it will talk about new topics.
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 频率惩罚和存在惩罚 | 频率惩罚通过“惩罚”模型来减少重复生成相同行的可能性。存在惩罚增加模型讨论新主题的可能性。 |'
- en: '| Best of | Lets you specify the number of completions (n) to generate on the
    server side and returns the best of “n” completions |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 最佳选择 | 允许您在服务器端指定要生成的完成数（n），并返回“n”个完成中的最佳选择 |'
- en: '| Stop sequence | Specifies a set of characters that signals the API to stop
    generating completions |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 停止序列 | 指定信号 API 停止生成完成的一组字符 |'
- en: '| Inject start and restart text | Inject start text allows you to insert text
    at the beginning of the completion. Inject restart text allows you to insert text
    at the end of the completion. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 注入起始和重新启动文本 | 注入起始文本允许您在完成的开头插入文本。注入重新启动文本允许您在完成的末尾插入文本。 |'
- en: '| Show probabilities | Lets you debug the text prompt by showing the probability
    of tokens that the model can generate for a given input |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 显示概率 | 允许您通过显示模型对于给定输入可以生成的标记的概率来调试文本提示 |'
- en: Execution Engine
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行引擎
- en: 'The *execution engine* determines the language model used for execution. Choosing
    the right engine is the key to determining your model’s capabilities and in turn
    getting the right output. GPT-3 comes with four execution engines of varying sizes
    and capabilities: Davinci, Ada, Babbage, and Curie. Davinci is the most powerful
    and the Playground’s default.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*执行引擎* 确定用于执行的语言模型。选择正确的引擎是确定您模型能力的关键，从而获得正确输出。GPT-3 提供了四种大小和功能各异的执行引擎：Davinci、Ada、Babbage
    和 Curie。Davinci 是最强大的，也是 Playground 的默认引擎。'
- en: Response Length
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 响应长度
- en: 'The *response length* sets a limit on how much text the API includes in its
    completion. Because OpenAI charges by the length of text generated per API call
    (as noted earlier, this is translated into tokens, or numeric representations
    of words), response length (also measured in tokens) is a crucial parameter for
    anyone on a budget. A higher response length will use more tokens and cost more.
    For example, if you do a classification task, it is not a good idea to set the
    response text dial to 100: the API could generate irrelevant text and use extra
    tokens that will incur charges on your account. The API supports a maximum of
    2048 tokens in the prompt and completion combined due to technical limitations.
    So, while using the API you need to be careful that the prompt and expected completion
    don’t exceed the maximum response length to avoid abrupt completions. If your
    use case involves large text prompts and completions, the workaround is to think
    of creative ways to solve problems within token limits, such as condensing your
    prompt, breaking the text into smaller pieces, and chaining together multiple
    requests.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*响应长度*设置了 API 在其完成中包含多少文本的限制。因为 OpenAI 按 API 调用生成的文本长度收费（如前所述，这转化为令牌，或者是单词的数字表示），响应长度（也以令牌表示）对于预算有限的人来说是一个关键参数。更高的响应长度将使用更多的令牌并且成本更高。例如，如果进行分类任务，将响应文本调节器设置为
    100 不是一个好主意：API 可能会生成无关的文本并使用额外的令牌，这将在您的账户上产生费用。由于技术限制，API 支持最多 2048 个令牌的提示和完成的组合。因此，在使用
    API 时，您需要小心，确保提示和预期完成不超过最大响应长度，以避免突然完成。如果您的用例涉及大文本提示和完成，解决方法是想出在令牌限制内解决问题的创造性方法，例如缩短提示，将文本分成较小的片段，并链接多个请求。'
- en: Temperature and Top P
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 温度和 Top P
- en: The *temperature* dial controls the creativity of the response, represented
    as a range from 0 to 1\. A lower value of temperature means the API will predict
    the first thing that the model sees, resulting in the most correct, but perhaps
    boring, text, with small variation. On the other hand a higher value of temperature
    means the model evaluates possible responses that could fit into the context before
    predicting the result. The generated text will be more diverse, but there is a
    higher possibility of grammar mistakes and the generation of nonsense.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*温度*调节器控制响应的创造性，表示为从 0 到 1 的范围。温度值较低意味着 API 将预测模型看到的第一件事情，导致最正确但可能枯燥，变化较小的文本。另一方面，温度值较高意味着模型在预测结果之前评估可能适合上下文的响应。生成的文本将更加多样化，但语法错误和无意义生成的可能性更高。'
- en: '*Top P* controls how many random results the model should consider for completion,
    as suggested by the temperature dial; it determines the *scope* of randomness.
    Top P’s range is from 0 to 1\. A value close to zero means the random responses
    will be limited to a certain fraction: for example, if the value is 0.1, then
    only 10% of the random responses will be considered for completion. This makes
    the engine *deterministic*, which means that it will always generate the same
    output for a given input text. If the value is set to 1, the API will consider
    all responses for completion, taking risks and coming up with creative responses.
    A lower value limits creativity; a higher value expands horizons.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*Top P* 控制模型应考虑的随机结果数量，如温度调节器所建议的；它确定了随机性的*范围*。Top P 的范围是从 0 到 1。接近零的值意味着随机响应将受到限制：例如，如果值为
    0.1，则只有 10% 的随机响应将被视为完成。这使得引擎*确定性*，这意味着它将始终为给定的输入文本生成相同的输出。如果值设置为 1，API 将考虑所有响应以完成，冒险并提出创造性的响应。较低的值限制了创造力；较高的值扩展了视野。'
- en: 'Temperature and Top P have a significant effect on output. It can be confusing
    at times to get your head around when and how to use them to get the desired output.
    The two are correlated: changing the value of one will affect the other. So, by
    setting Top P to 1, you can allow the model to unleash its creativity by exploring
    the entire spectrum of responses and control the randomness by using the temperature
    dial.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 温度和 Top P 对输出产生重要影响。有时候，要想明白何时以及如何使用它们以获得所需的输出可能会令人困惑。两者相关联：改变其中一个的值会影响另一个。因此，通过将
    Top P 设置为 1，您可以允许模型通过探索整个响应谱来释放其创造力，并通过使用温度调节器来控制随机性。
- en: Tip
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We always advise changing either Top P or temperature and keeping the dial for
    the other set at 1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终建议更改 Top P 或温度，并将另一个设置为 1。
- en: Large language models rely on probabilistic approaches rather than conventional
    logic. They can generate a variety of responses for the same input, depending
    on how you set the model’s parameters. The model tries to find the best probabilistic
    match within the universe of data it has been trained on, instead of looking for
    a perfect solution every time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型依赖概率方法而不是传统逻辑。它们可以为相同输入生成各种响应，具体取决于您设置模型参数的方式。模型试图在其训练的数据宇宙中找到最佳的概率匹配，而不是每次都寻找完美解决方案。
- en: 'As we mentioned in [Chapter 1](ch01.xhtml#the_era_of_large_language_models),
    GPT-3’s universe of training data is huge, consisting of a variety of publicly
    available books, internet forums, and Wikipedia articles specially curated by
    OpenAI, allowing it to generate a wide variety of completions for a given prompt.
    That’s where temperature and Top P, sometimes called the “creativity dials,” come
    in: you can tune them to generate more natural or abstract responses with an element
    of playful creativity.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.xhtml#the_era_of_large_language_models)中提到的，GPT-3的训练数据宇宙是巨大的，包括各种公开可用的书籍，互联网论坛和OpenAI专门策划的维基百科文章，使其能够为给定提示生成各种完成。这就是温度和Top
    P，有时被称为“创造力旋钮”，发挥作用的地方：您可以调节它们以生成更自然或更抽象的响应，其中包含一些富有创意的元素。
- en: 'Let’s say you are going to use GPT-3 to generate names for your start-up. You
    can set the temperature dial to a higher level to get the most creative response.
    When we were spending days and nights trying to come up with the perfect name
    for our start-up, we dialed up the temperature. GPT-3 came to the rescue and helped
    us to arrive at a name we love: Kairos Data Labs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你要使用GPT-3为你的初创公司生成名称。您可以将温度调节旋钮设置为较高水平，以获得最具创意的响应。当我们日夜努力寻找我们初创公司的完美名称时，我们调高了温度。GPT-3出手相助，帮助我们找到了一个我们喜欢的名字：Kairos数据实验室。
- en: 'On other occasions, your task might require little to no creativity: classification
    and question-answering tasks, for example. For these, keep the temperature lower.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，您的任务可能需要很少或根本不需要创造力：例如分类和问答任务。对于这些任务，请保持较低的温度。
- en: Let’s look at [Figure 2-5](#temperature_component) with a simple classification
    example that categorizes companies into general buckets or categories based on
    their names.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下[图 2-5](#temperature_component)，使用一个简单的分类示例，根据它们的名称将公司归类为一般的桶或类别。
- en: '![](Images/gpt3_0205.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0205.png)'
- en: Figure 2-5\. Temperature component
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 温度组件
- en: 'Our prompt:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提示：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And the output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以及输出：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see in [Figure 2-5](#temperature_component), we have again used temperature
    to control the degree of randomness. You can also do this by changing Top P while
    keeping the temperature dial set to 1.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[图 2-5](#temperature_component)中看到的那样，我们再次使用温度来控制随机性的程度。您还可以通过将温度旋钮保持设置为1而更改Top
    P来做到这一点。
- en: Frequency and Presence Penalties
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频率和存在惩罚
- en: Like the temperature and Top P dials, the frequency penalty and presence penalty
    dials consider text prompts (the previous completion plus the new input) instead
    of internal model parameters when deciding on output. Existing text thus influences
    the new completions. The *frequency penalty* decreases the likelihood that the
    model will repeat the same line verbatim by “punishing” it. The *presence penalty*
    increases the likelihood that it will talk about new topics.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与温度和Top P调节旋钮一样，频率惩罚和存在惩罚调节旋钮考虑的是文本提示（前一个完成加上新输入），而不是内部模型参数，当决定输出时，现有文本因此影响新的完成。*频率惩罚*通过“惩罚”它来减少模型完全重复相同行的可能性。*存在惩罚*增加了它谈论新话题的可能性。
- en: These come in handy when you want to prevent the same completion text from being
    repeated across multiple completions. Although these dials are similar, there
    is one important distinction. The frequency penalty is applied if the suggested
    text output is repeated (for example, the model used the exact token in previous
    completions or during the same session) and the model chooses an old output over
    a new one. The presence penalty is applied if a token is present in a given text
    *at all*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望防止相同的完成文本在多个完成之间重复时，这些就派上用场了。尽管这些旋钮相似，但有一个重要区别。如果建议的文本输出重复（例如，模型在之前的完成或同一会话期间使用了完全相同的标记），并且模型选择旧输出而不是新输出，则会应用频率惩罚。如果给定文本中存在标记*，则将应用存在惩罚。
- en: Best Of
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳效果
- en: GPT-3 uses the *best of* feature to generate multiple completions on the server
    side, evaluate them behind the scenes, and then provide you with the best probabilistic
    result. Using the “best of” parameter, you can specify the number of completions
    (*n*) to generate on the server side. The model will return the best of *n* completions
    (the one with the lowest log probability per token).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 使用 *最佳选择* 功能在服务器端生成多个完成，对它们进行幕后评估，然后为您提供最佳的概率结果。使用“最佳选择”参数，您可以指定要在服务器端生成的完成数
    (*n*)。模型将返回 *n* 个完成中的最佳完成（每个标记的最低对数概率）。
- en: 'This enables you to evaluate multiple prompt completions in a single API call
    rather than calling the API repeatedly to check the quality of different completions
    for the same input. However, using “best of” is expensive: it costs *n* times
    the tokens in the prompt. For example, if you set the “best of” value to 2, then
    you will be charged double the tokens present in the input prompt because on the
    backend the API will generate two completions and show you the best one.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您能够在单个 API 调用中评估多个提示完成，而不是重复调用 API 来检查相同输入的不同完成的质量。但是，使用“最佳选择”是昂贵的：它花费了 *n*
    倍于提示中的标记。例如，如果您将“最佳选择”值设置为 2，则将收取输入提示中标记的两倍的费用，因为在后台，API 将生成两个完成并显示给您最佳的一个。
- en: “Best of” can range from 1 to 20 depending on your use case. If your use case
    serves clients for whom the quality of output needs to be consistent, then you
    can set the “best of” value to a higher number. On the other hand, if your use
    case involves too many API invocations, then it makes sense to have a lower “best
    of” value to avoid unnecessary latency and costs. We advise keeping response length
    minimal while generating multiple prompts using the “best of” parameter to avoid
    additional charges.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳选择”根据您的使用情况可以从1到20不等。如果您的使用情况为为客户提供一致的输出质量，则可以将“最佳选择”值设置为较高的数字。另一方面，如果您的使用情况涉及太多的
    API 调用，则将“最佳选择”值设置为较低的数字是有意义的，以避免不必要的延迟和成本。我们建议在使用“最佳选择”参数生成多个提示时保持响应长度最小，以避免额外收费。
- en: Stop Sequence
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停止序列
- en: A *stop sequence* is a set of characters that signal the API to stop generating
    completions. This helps avoid using unnecessary tokens, an essential cost-saving
    feature for regular users.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*停止序列* 是一组字符，用于指示 API 停止生成完成。这有助于避免使用不必要的标记，对于常规用户来说，这是一项重要的节省成本功能。'
- en: You can provide up to four sequences for the API to stop generating further
    tokens.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为 API 提供最多四个序列，以停止生成进一步的标记。
- en: 'Let’s look at the example language translation task in [Figure 2-6](#stop_sequence_component)
    to understand how stop sequence works. In this example, English phrases are being
    translated into French. We use the restart sequence “English:” as a stop sequence:
    whenever the API encounters that phrase, it will stop generating new tokens.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 [图 2-6](#stop_sequence_component) 中的示例语言翻译任务，以了解停止序列的工作原理。在此示例中，将英语短语翻译为法语。我们使用重启序列“英语：”作为停止序列：每当
    API 遇到该短语时，它将停止生成新的标记。
- en: '![](Images/gpt3_0206.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0206.png)'
- en: Figure 2-6\. Stop sequence component
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 停止序列组件
- en: Inject Start Text and Inject Restart Text
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注入开始文本和注入重启文本
- en: The *inject start text* and *inject restart text* parameters allow you to insert
    text at the beginning or end of the completion, respectively. You can use them
    to keep a desired pattern going. Often, these settings work in tandem with the
    stop sequence, as in our example. The prompt has the pattern where an English
    sentence is provided with the prefix “English:” (the restart text) and the translated
    output is generated with the prefix “French:” (the start text). As a result, anyone
    can easily distinguish between the two and create a training prompt that both
    the model and the user can clearly comprehend.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*注入开始文本* 和 *注入重启文本* 参数允许您分别在完成的开始或结束处插入文本。您可以使用它们来保持所需的模式。通常，这些设置与停止序列一起使用，就像我们的示例中一样。提示具有模式，其中提供了一个带有前缀“英语：”（重启文本）的英语句子，然后生成带有前缀“法语：”（开始文本）的翻译输出。因此，任何人都可以轻松区分两者，并创建一个模型和用户都可以清楚理解的训练提示。'
- en: Whenever we run the model for such kinds of prompts, it automatically injects
    the start text “French:” before the output and the restart text “English:” before
    the next input, so that this pattern can be sustained.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们为此类提示运行模型时，它会自动在输出之前注入开始文本“法语：”，并在下一个输入之前注入重启文本“英语：”，以便可以保持这种模式。
- en: Show Probabilities
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显示概率
- en: The *show probabilities* parameter is at the bottom of the Playground settings
    pane. In conventional software engineering, developers use a *debugger* to troubleshoot
    (debug) a piece of code. You can use the show probabilities parameter to debug
    your text prompt. Whenever you select this parameter, you will see highlighted
    text. Hovering over it with the cursor will show a list of tokens that the model
    can generate for the particular input specified, with their respective probabilities.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*显示概率*参数位于 Playground 设置窗格的底部。在传统软件工程中，开发人员使用 *调试器* 来调试（调试）一段代码。您可以使用显示概率参数来调试您的文本提示。每当您选择此参数，您将看到突出显示的文本。将光标悬停在上面将显示模型可以为指定的特定输入生成的标记列表，以及它们的相应概率。'
- en: 'You can use this parameter to examine your options. In addition, it can make
    it easier to see alternatives that might be more effective. The show probabilities
    parameter has three settings:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此参数来检查您的选项。此外，它可以帮助您看到可能更有效的替代方案。显示概率参数有三个设置：
- en: Most Likely
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最有可能
- en: Lists the tokens most likely to be considered for completion, in decreasing
    order of probability.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列出最有可能被考虑作为完成的标记，按概率递减的顺序。
- en: Least Likely
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最不可能
- en: Lists the tokens least likely to be considered for completion, in decreasing
    order of probability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列出最不可能被考虑作为完成的标记，按概率递减的顺序。
- en: Full Spectrum
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 完整光谱
- en: Shows the entire universe of tokens that could be selected for completion.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 显示可能被选择完成的整个标记宇宙。
- en: 'Let’s look at this parameter in the context of a simple prompt. We want to
    start the output sentence with a simple, well-known phrase: “Once upon a time.”
    We provide the API with the prompt “Once upon a” and then we check the Most Likely
    option in the show probabilities tab.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个简单提示的情境下看看这个参数。我们想要用一个简单、众所周知的短语开头输出句子：“从前有一天。”我们向 API 提供提示“从前有一”，然后我们在显示概率选项中选中“最有可能”选项。
- en: As [Figure 2-7](#show_probabilities_component_showing_th) shows, it generates
    “time” as the response. Because we have set the “show probabilities” parameter
    to Most Likely, the API shows not only the response but a list of possible options
    along with their probabilities.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 2-7](#show_probabilities_component_showing_th)所示，它将“time”作为响应生成。因为我们将“显示概率”参数设置为最有可能，API
    不仅显示响应，还显示可能的选项列表以及它们的概率。
- en: Now that you’ve had an overview, let’s look at these components in more detail.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有了一个概述，让我们更详细地看看这些组件。
- en: '![](Images/gpt3_0207.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0207.png)'
- en: Figure 2-7\. Show probabilities component showing the most likely tokens
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 显示概率组件显示最有可能的标记
- en: Execution Engines
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行引擎
- en: As noted in [Figure 2-7](#show_probabilities_component_showing_th), the OpenAI
    API offers four execution engines, differentiated by number of parameters and
    performance capabilities. Execution engines power the OpenAI API. They serve as
    “autoML” solutions, providing automated ML methods and processes to make machine
    learning available to nonexperts. They are easy to configure and adapt to a given
    dataset and task.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 2-7](#show_probabilities_component_showing_th)所述，OpenAI API 提供四个执行引擎，通过参数数量和性能能力进行区分。执行引擎驱动
    OpenAI API。它们是“autoML”解决方案，提供自动化的 ML 方法和流程，使机器学习可用于非专家。它们易于配置并适应给定的数据集和任务。
- en: 'The four primary execution engines were named after famous scientists in alphabetical
    order: Ada (named after Ada Lovelace), Babbage (Charles Babbage), Curie (Madame
    Marie Curie), and Davinci (Leonardo da Vinci). Let’s take a deep dive into each
    of these execution engines to understand when to use which engine when working
    with GPT-3, beginning with Davinci.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 四个主要的执行引擎按字母顺序命名，分别是：Ada（以埃达·洛夫莱斯命名）、Babbage（查尔斯·巴贝奇）、Curie（居里夫人玛丽·居里）和Davinci（列奥纳多·达·芬奇）。让我们深入了解每个执行引擎，以了解在使用
    GPT-3 时何时使用哪个引擎，首先从达芬奇开始。
- en: Davinci
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 达芬奇
- en: Davinci is the largest execution engine and the default when you first open
    the Playground. It can do anything the other engines can, often with fewer instructions
    and better outcomes. However, the trade-off is that it costs more to use per API
    call and is slower than other engines. You might want to use other engines to
    optimize costs and runtimes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 达芬奇是最大的执行引擎，在打开 Playground 时是默认的。它可以做任何其他引擎可以做的事情，通常需要更少的指令并且结果更好。然而，这样做的折衷是它的每个
    API 调用成本更高，比其他引擎慢。您可能想使用其他引擎来优化成本和运行时间。
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When testing new ideas and prompts, we recommend starting with Davinci because
    of its superior capabilities. Experimenting with Davinci is a great way to find
    out what the API is capable of doing. You can then slowly move down the ladder
    to optimize budgets and runtimes as you become comfortable with your problem statement.
    Once you have an idea of what you want to accomplish, you can either stay with
    Davinci (if cost and speed are not a concern) or you can move on to Curie or other
    less costly engines and try to optimize the output around its capabilities. You
    can use [OpenAI’s Comparison Tool](https://oreil.ly/EDggA) to generate an Excel
    spreadsheet that compares engines’ outputs, settings, and response times.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试新想法和提示时，我们建议从Davinci开始，因为它具有出色的能力。尝试Davinci是了解API能力的好方法。然后，随着您对问题陈述的了解逐渐加深，您可以逐渐降低预算和运行时间。一旦您知道自己想要实现什么，您可以选择留在Davinci（如果成本和速度不是问题），或者您可以转向Curie或其他成本较低的引擎，并尝试根据其能力优化输出。您可以使用[OpenAI’s
    Comparison Tool](https://oreil.ly/EDggA)生成比较引擎输出、设置和响应时间的Excel电子表格。
- en: Davinci should be your first choice for tasks that require understanding the
    content, like summarizing meeting notes or generating creative ad copy. It’s great
    at solving logic problems and explaining the motives of fictional characters.
    It can even write a story. Davinci has also been able to solve some of the most
    challenging AI problems involving cause and effect.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Davinci应该是您处理需要理解内容的任务的首选，例如总结会议记录或生成创意广告文案。它擅长解决逻辑问题并解释虚构角色的动机。它甚至可以编写故事。Davinci还能够解决一些涉及因果关系的最具挑战性的AI问题。
- en: Curie
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Curie
- en: Curie aims to find an optimal balance between power and speed that is very important
    for performing high-frequency tasks like classification on a large scale or putting
    a model into production.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Curie旨在找到功率和速度之间的最佳平衡，这对于执行大规模分类等高频任务或将模型投入生产非常重要。
- en: Curie is also quite good at performing Q&As and serving as a general purpose
    chatbot. For instance, if you are building a customer support chatbot, you might
    choose Curie to serve high-volume requests faster.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Curie在执行问答和作为通用聊天机器人方面也表现不错。例如，如果您正在构建客户支持聊天机器人，您可能会选择Curie来更快地服务高容量请求。
- en: While Davinci is stronger at analyzing complicated texts, Curie can perform
    with low latency and lightning-fast speed. It is always sensible to figure out
    what your use case is and do a cost-benefit analysis before choosing Davinci over
    Curie in production.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Davinci在分析复杂文本方面更强大，但Curie可以以低延迟和闪电般的速度执行。在选择Davinci还是Curie投入生产之前，弄清楚您的用例并进行成本效益分析是明智的。
- en: Babbage
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Babbage
- en: Babbage is faster than Curie but not capable of performing tasks that involve
    understanding complex intent. However, it is quite capable and is preferable when
    it comes to semantic search rankings and analyzing how well documents match up
    with search queries. It’s less expensive than Curie and Davinci and is a preferred
    choice for simple problems involving frequent API calls.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Babbage比Curie更快，但无法执行涉及理解复杂意图的任务。然而，它非常能干，并且在语义搜索排名和分析文档与搜索查询匹配程度方面是首选。它比Curie和Davinci便宜，并且是涉及频繁API调用的简单问题的首选选择。
- en: Ada
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ada
- en: Ada is the fastest and least expensive of the available engines. It can perform
    simple tasks that do not require a nuanced understanding of context, like parsing
    text, correcting grammar, or simple classification. It is often possible to improve
    Ada’s performance by providing more context with the input. For use cases involving
    frequent API invocations, Ada can be the preferred model; with the right configuration
    of settings, it can achieve results similar to bigger models. The more you experiment
    with the API parameters, the more understanding you will gain on what settings
    work for your use case.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Ada是可用引擎中最快且成本最低的。它可以执行不需要细微理解背景的简单任务，例如解析文本、纠正语法或简单分类。通过提供更多输入上下文，可以提高Ada的性能。对于涉及频繁API调用的用例，Ada可以是首选模型；通过正确配置设置，它可以实现与更大模型类似的结果。您对API参数进行的实验越多，您就会对适用于您的用例的设置有更多了解。
- en: Instruct Series
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Instruct Series
- en: Based on the four primary models, OpenAI has launched a series of [InstructGPT](https://oreil.ly/fzVk4)
    models that are better at understanding instructions and following them, while
    being less toxic and more truthful than the original GPT-3\. They have been developed
    using techniques coming from OpenAI’s alignment research. These models are trained
    with humans in the loop and are now deployed as the default language models on
    the OpenAI API.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基于四个主要模型，OpenAI推出了一系列[InstructGPT](https://oreil.ly/fzVk4)模型，这些模型在理解指令并遵循它们方面表现更好，同时比原始的GPT-3毒性更低、更真实。它们是使用来自OpenAI对齐研究的技术开发的。这些模型是在与人类的合作中训练的，并且现在作为OpenAI
    API的默认语言模型部署。
- en: Figures [2-8](#output_generated_by_instructgpt_davinci) and [2-9](#output_generated_by_gpt_davinci_model)
    present two outputs generated by the InstructGPT and GPT series of Davinci engines
    for the same input.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2-8](#output_generated_by_instructgpt_davinci)和[2-9](#output_generated_by_gpt_davinci_model)展示了InstructGPT和GPT系列Davinci引擎针对相同输入生成的两个输出。
- en: '![](Images/gpt3_0208.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0208.png)'
- en: Figure 2-8\. Output generated by InstructGPT Davinci model
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8\. InstructGPT Davinci模型生成的输出
- en: '![](Images/gpt3_0209.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0209.png)'
- en: Figure 2-9\. Output generated by GPT Davinci model
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9\. GPT Davinci模型生成的输出
- en: 'Our input:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'InstructGPT output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT输出：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'GPT output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: GPT输出：
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To make the process of building prompts really efficient, OpenAI decided to
    publicly launch the InstructGPT versions of the four models: text-davinci-001,
    text-curie-001, text-babbage-001, and text-ada-001\. With clear instructions,
    these models can produce better results than their base counterparts and are now
    the [default models of the API](https://oreil.ly/LoiuE). This series is an important
    step in bridging the gap between how humans think and how models operate.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使构建提示的过程真正高效，OpenAI决定公开发布四个模型的InstructGPT版本：text-davinci-001、text-curie-001、text-babbage-001和text-ada-001。通过清晰的说明，这些模型可以产生比它们的基础版本更好的结果，并且现在是[API的默认模型](https://oreil.ly/LoiuE)。这个系列是在弥合人类思维与模型操作之间的差距方面的重要一步。
- en: Tip
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We recommend using this model series as your default for all text-related tasks.
    The base versions of GPT-3 models are available as Davinci, Curie, Babbage, and
    Ada and are meant to be used with the fine-tuning, search, classification, and
    answers endpoints.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您将此模型系列作为所有与文本相关的任务的默认模型。GPT-3模型的基础版本可用作Davinci、Curie、Babbage和Ada，并且旨在与微调、搜索、分类和回答端点一起使用。
- en: Endpoints
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端点
- en: 'The Playground is a graphical web interface that calls the OpenAI API behind
    the scenes, but there are several other ways to call the API. To do this, you
    will need to get familiar with its *endpoints*: the remote APIs that communicate
    back and forth when they are called. In this section, you will get familiar with
    the functionality and usage of eight API endpoints.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Playground是一个图形化的Web界面，它在幕后调用OpenAI API，但还有其他几种调用API的方式。要做到这一点，您需要熟悉其*端点*：在调用时相互通信的远程API。在本节中，您将熟悉八个API端点的功能和用法。
- en: List Engines
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列出引擎
- en: 'The *list engines endpoint*, also known as the *metadata endpoint*, provides
    a list of available engines along with specific metadata associated with each
    engine, such as owner and availability. To access it, you can invoke the following
    URI with the HTTP GET method without passing any request parameters:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*列出引擎端点*，也称为*元数据端点*，提供了可用引擎的列表，以及与每个引擎相关的特定元数据，例如所有者和可用性。要访问它，您可以使用HTTP GET方法调用以下URI，而不传递任何请求参数：'
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Retrieve Engine
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索引擎
- en: 'When you provide an engine name to the *retrieve engine endpoint*, it returns
    detailed metadata about that engine. To access it, invoke the following URI with
    the HTTP GET method without passing any request parameters:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当您向*检索引擎端点*提供引擎名称时，它会返回有关该引擎的详细元数据。要访问它，请使用HTTP GET方法调用以下URI，而不传递任何请求参数：
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Completions
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成
- en: '*Completions* is GPT-3’s most famous and widely used endpoint. It simply takes
    in the text prompt as input and returns the completed response as output. It uses
    the HTTP POST method and requires an engine ID as part of the URI path. As part
    of the HTTP Body, the completions endpoint accepts several of the additional parameters
    discussed in the previous section. Its signature is:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*完成*是GPT-3最著名和广泛使用的端点。它简单地将文本提示作为输入，并返回完成的响应作为输出。它使用HTTP POST方法，并且需要将引擎ID作为URI路径的一部分。作为HTTP
    Body的一部分，完成端点接受了前一节讨论的几个附加参数。它的签名是：'
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Semantic Search
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义搜索
- en: The *semantic search endpoint* enables you to provide a query in natural language
    to search a set of documents, which can be words, sentences, paragraphs, or even
    longer texts. It will score and rank the documents based on how semantically related
    they are to the input query. For example, if you provide the documents [“school”,
    “hospital”, “park”] and the query “the doctor”, you’ll get a different similarity
    score for each document.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*语义搜索端点*允许您以自然语言提供查询以搜索一组文档，这些文档可以是单词、句子、段落，甚至更长的文本。它将根据文档与输入查询的语义相关性对文档进行评分和排名。例如，如果您提供文档[“学校”，“医院”，“公园”]和查询“医生”，您将为每个文档获得不同的相似度分数。'
- en: The *similarity score* is a positive score that usually ranges from 0 to 300
    (but can sometimes go higher), where a score above 200 usually indicates that
    the document is semantically similar to the query. The higher the similarity score,
    the more semantically similar the document is to the query (in this example, “hospital”
    will be most similar to “the doctor”). You can provide up to two hundred documents
    as part of your request to the API.^([1](ch02.xhtml#ch01fn3))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*相似度分数*是一个正分数，通常范围从0到300（但有时可能更高），其中分数大于200通常表示文档在语义上与查询相似。相似度分数越高，文档与查询的语义相似度就越高（在此示例中，“医院”将与“医生”最相似）。您可以作为API请求的一部分提供高达两百个文档。^([1](ch02.xhtml#ch01fn3))'
- en: 'Following is the signature for the semantic search endpoint:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是语义搜索端点的签名：
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Files
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件
- en: 'The *files endpoint* can be used across different endpoints like answers, classification,
    and semantic search. It is used to upload documents or files to the OpenAI storage,
    which is accessible throughout the API. The same endpoint can be used with different
    signatures to perform the following tasks:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*文件端点*可用于跨不同端点，如答案、分类和语义搜索。它用于将文档或文件上传到OpenAI存储，该存储可在整个API中访问。相同的端点可使用不同的签名执行以下任务：'
- en: List files
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列出文件
- en: 'Returns a list of the files that belong to the user’s organization or that
    are linked to a particular user account. It’s an HTTP GET call that doesn’t require
    any parameters to be passed with the request:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 返回属于用户组织或与特定用户帐户链接的文件列表。这是一个HTTP GET调用，不需要传递任何参数：
- en: '[PRE14]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Upload files
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 上传文件
- en: 'Uploads files that contain documents to be used across various endpoints. It
    uploads the documents to the already allocated internal space by OpenAI for the
    user’s organization. It’s a HTTP POST call that requires the file path be added
    with the API request:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上传包含要在各种端点之间使用的文档的文件。它将文档上传到OpenAI为用户组织已分配的内部空间中。这是一个HTTP POST调用，需要在API请求中添加文件路径：
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Retrieve file
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 检索文件
- en: 'Returns information about a specific file by providing the file ID as the request
    parameter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供文件ID作为请求参数，返回关于特定文件的信息：
- en: '[PRE16]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Delete file
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 删除文件
- en: 'Deletes a specific file by providing the file ID as the request parameter:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供文件ID作为请求参数，删除特定文件：
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Classification (Beta)
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类（测试版）
- en: The *classification endpoint* lets you leverage a labeled set of examples without
    fine-tuning. It classifies the query using the provided examples, thereby avoiding
    fine-tuning, and in turn eliminating the need for hyperparameter tuning. You can
    use it for virtually any machine learning classification task.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类端点*允许您利用带标签的示例集而无需进行微调。它使用提供的示例对查询进行分类，从而避免微调，进而消除超参数调整的需要。您可以将其用于几乎任何机器学习分类任务。'
- en: 'This endpoint provides an easy-to-configure “autoML” solution that can easily
    be adapted to the changing label schema. You can provide up to two hundred labeled
    examples as part of the request, or a pre-uploaded file can be provided during
    the query. In addition to providing a URI path, this endpoint requires a model
    and query, along with examples. Its signature is:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此端点提供了一个易于配置的“autoML”解决方案，可轻松适应不断变化的标签模式。您可以作为请求的一部分提供高达两百个带标签的示例，或者在查询期间提供预先上传的文件。除了提供URI路径外，此端点还需要模型和查询以及示例。其签名为：
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Answers (Beta)
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案（测试版）
- en: GPT-3’s *question-answering endpoint* is still in beta as of this writing in
    late 2021\. When given a question, the QA endpoint generates answers based on
    information provided in a set of documents or training examples.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的*问答端点*截至2021年底的写作仍处于测试阶段。当给出一个问题时，QA端点基于一组文档或训练示例提供答案。
- en: 'For example, if you want to implement a QA endpoint on a set of PDFs, you just
    upload them using the files endpoint and provide the file ID with the request
    parameters. The answers endpoint will use those files as the context to answer
    any query. It also allows you to steer the model’s contextual tone and responses
    by providing a list of (question, answer) pairs in the form of training examples.
    It first searches the provided documents or examples to find the relevant context,
    and then combines it with relevant examples and questions to generate a response.
    Its signature is:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想在一组 PDF 上实现 QA 端点，您只需使用文件端点上传它们，并在请求参数中提供文件 ID。答案端点将使用这些文件作为上下文来回答任何查询。它还允许您通过提供（问题，答案）对列表作为训练示例的形式来控制模型的上下文语调和响应。它首先搜索提供的文档或示例以找到相关的上下文，然后将其与相关示例和问题结合起来生成响应。其签名为：
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Embeddings
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: Another experimental endpoint of the API is *embeddings*. Embeddings are the
    core of any machine learning model and allow you to capture semantics from the
    text by converting it into high-dimensional vectors. Currently, developers tend
    to use open source models to create embeddings for their data that can be used
    for a variety of tasks like recommendation, topic modeling, semantic search, etc.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: API 的另一个实验性端点是*嵌入*。嵌入是任何机器学习模型的核心，它们允许您通过将文本转换为高维向量来捕获语义。目前，开发者倾向于使用开源模型为其数据创建嵌入，这些数据可以用于各种任务，如推荐，主题建模，语义搜索等。
- en: 'OpenAI realized that GPT-3 holds a great potential to power embedding-driven
    use cases and come up with state-of-the-art results. Generating embeddings for
    the input data is very straightforward and wrapped in the form of an API call.
    To create an embedding vector representing the input text, you can use the following
    signature:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 意识到 GPT-3 具有驱动基于嵌入的用例的巨大潜力，并提供了最先进的结果。为输入数据生成嵌入向量非常简单，可以通过 API 调用的形式来实现。要创建代表输入文本的嵌入向量，您可以使用以下签名：
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To invoke the embeddings endpoint, you can choose the type of engine depending
    on your use case by referring to the [embeddings documentation](https://oreil.ly/A8hQN).
    Each engine has its specific dimensions of embedding, with Davinci being the biggest
    and Ada the smallest. All the embedding engines are derived from the four base
    models and classified based on the use cases to allow efficient and cost friendly
    usage.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用嵌入端点，您可以根据您的用例选择引擎类型，参考[嵌入文档](https://oreil.ly/A8hQN)。每个引擎都有其特定的嵌入维度，其中 Davinci
    是最大的，而 Ada 是最小的。所有嵌入引擎都来自四个基本模型，并根据用例进行分类，以允许有效且成本友好的使用。
- en: Customizing GPT-3
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 GPT-3 进行定制
- en: OpenAI’s research paper [*“*Process for Adapting Language Models to Society
    (PALMS) with Values-Targeted Datasets*”*](https://oreil.ly/HM7jQ) by Irene Solaiman
    and Christy Dennison (June 2021) led the company to launch a first-of-its-kind
    fine-tuning endpoint that allows you to get more out of GPT-3 than was previously
    possible by customizing the model for your particular use case. (We discuss more
    about PALMS in [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and).)
    Customizing GPT-3 improves performance of any natural language task GPT-3 is capable
    of performing for your specific use case.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的研究论文[*“*适应社会的语言模型过程（PALMS）与价值定向数据集*”*](https://oreil.ly/HM7jQ)，作者为 Irene
    Solaiman 和 Christy Dennison（2021年6月），推动了公司推出了一种前所未有的微调端点，允许您通过定制模型以满足特定用例来更充分地利用
    GPT-3。 (我们在[第6章](ch06.xhtml#challengescomma_controversiescomma_and)中更多地讨论了关于 PALMS
    的内容。) 对 GPT-3 进行定制可以提高任何自然语言任务的性能，而这些任务正是符合您特定用例的。
- en: Let us explain how that works first.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先解释一下这是如何工作的。
- en: OpenAI pre-trained GPT-3 on a [specially prepared dataset](https://oreil.ly/IR1SM)
    in a semi-supervised fashion. When given a prompt with just a few examples, it
    can often intuit what task you are trying to perform and generate a plausible
    completion. This is called few-shot learning, as you learned in [Chapter 1](ch01.xhtml#the_era_of_large_language_models).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 在半监督方式下预训练了 GPT-3，使用了[特别准备的数据集](https://oreil.ly/IR1SM)。当只给出几个示例的提示时，它通常能够直觉地了解您正在尝试执行的任务并生成一个合理的完成。这被称为少样本学习，正如您在[第1章](ch01.xhtml#the_era_of_large_language_models)中所学到的。
- en: Users can now fine-tune GPT-3 on their own data, creating a custom version of
    the model tailored to their project. Customizing makes GPT-3 reliable for a variety
    of use cases and makes running the model cheaper, more efficient, and faster.
    *Fine-tuning* is about tweaking the whole model so that it performs every time
    in the way you wish it to perform. You can use an existing dataset of any shape
    and size, or incrementally add data based on user feedback.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 用户现在可以根据自己的数据对 GPT-3 进行微调，创建适合其项目的定制版本。定制使 GPT-3 对各种用例可靠，并使运行模型更便宜、更高效和更快速。*微调*是调整整个模型，使其每次表现都符合您的期望。您可以使用任何形状和大小的现有数据集，或根据用户反馈逐步添加数据。
- en: The capability and knowledge of the model will be narrowed and focused on the
    contents and semantics of the dataset used for fine-tuning. This in turn will
    limit the range of creativity and topic selections, which will be good for downstream
    tasks like classifying internal documents, or for any use case involving internal
    jargon. It works by focusing the attention of GPT-3 on the fine-tuned data and
    limiting its knowledge base.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的能力和知识将被狭窄化并集中在用于微调的数据集的内容和语义上。这反过来将限制创造力和主题选择的范围，这对于诸如分类内部文件之类的下游任务或涉及内部行话的任何用例都是有利的。它通过将
    GPT-3 的注意力集中在微调数据上并限制其知识库来实现。
- en: Once a model has been fine-tuned, you won’t need to provide examples in the
    prompt anymore. This saves costs, decreases response times, and increases the
    quality and reliability of the outputs. Customizing GPT-3 seems to yield better
    results than what can be achieved with prompt design, because during this process
    you can provide more examples.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被微调，您就不再需要在提示中提供示例了。这样可以节省成本，减少响应时间，并提高输出的质量和可靠性。定制 GPT-3 似乎比通过提示设计所能实现的结果更好，因为在此过程中您可以提供更多示例。
- en: With fewer than one hundred examples you can already start seeing the benefits
    of fine-tuning GPT-3, and performance continues to improve as you add more data.
    In the PALMS research paper, OpenAI showed how fine-tuning with fewer than one
    hundred examples can improve GPT-3’s performance on certain tasks. OpenAI also
    found that each doubling of the number of examples tends to improve quality linearly.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不到一百个示例，您就可以开始看到微调 GPT-3 的好处，随着您添加更多数据，性能将继续提高。在 PALMS 研究论文中，OpenAI 展示了如何使用不到一百个示例对
    GPT-3 的性能进行改进。OpenAI 还发现，示例数量加倍通常会线性提高质量。
- en: Apps Powered by Customized GPT-3 Models
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于定制 GPT-3 模型的应用
- en: Customizing GPT-3 improves the reliability of output, offering more consistent
    results that you can count on for production use cases. Existing OpenAI API customers
    found that customizing GPT-3 could dramatically reduce the frequency of unreliable
    outputs, and there is a growing group of customers that can vouch for it with
    their performance numbers. Let’s look at four companies that have customized GPT-3.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 定制 GPT-3 改善了输出的可靠性，提供了更一致的结果，您可以依赖它来进行生产用例。现有的 OpenAI API 客户发现，定制 GPT-3 可以显着减少不可靠输出的频率，并且有一个日益增长的客户群可以通过其性能数据为此作证。让我们来看看定制了
    GPT-3 的四家公司。
- en: Keeper Tax helps independent contractors and freelancers with their taxes. It
    uses various models to extract text and classify transactions, and then identifies
    easy-to-miss tax write-offs to help customers file their taxes directly from the
    app. By customizing GPT-3, Keeper Tax experienced an increase in accuracy from
    85% to 93%. And it continuously improves thanks to adding 500 new training examples
    to its model once a week, which is leading to about a 1% accuracy improvement
    per week.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Keeper Tax 帮助独立承包商和自由职业者处理税务。它使用各种模型来提取文本和分类交易，然后识别易于忽视的税收减免，以帮助客户直接从应用程序申报税款。通过定制
    GPT-3，Keeper Tax 的准确率从 85% 提高到了 93%。并且，由于每周向模型添加 500 个新的训练示例，这一持续改进正在导致准确率每周提高约
    1%。
- en: Viable helps companies get insights from their customer feedback. By customizing
    GPT-3, Viable was able to transform massive amounts of unstructured data into
    readable natural language reports and increase the reliability of its reports.
    As a result, accuracy in summarizing customer feedback has improved from 66% to
    90%. For an in-depth insight into Viable’s journey, refer to our interview with
    Viable’s CEO in [Chapter 4](ch04.xhtml#gpt_three_as_a_launchpad_for_next_gener).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Viable帮助公司从客户反馈中获得见解。通过定制GPT-3，Viable能够将大量的非结构化数据转化为可读的自然语言报告，并提高报告的可靠性。因此，总结客户反馈的准确性提高了从66%到90%。想要深入了解Viable的旅程，请参考我们在[第四章](ch04.xhtml#gpt_three_as_a_launchpad_for_next_gener)中与Viable首席执行官的访谈。
- en: Sana Labs is a global leader in the development and application of AI to learning.
    The company’s platform powers personalized learning experiences for businesses
    by leveraging the latest ML breakthroughs to personalize content. By customizing
    GPT-3 with its own data, Sana’s question and content generation went from grammatically
    correct but general responses to highly accurate responses. This yielded a 60%
    improvement, enabling more personalized experiences for their users.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Sana Labs是人工智能在学习领域开发和应用的全球领导者。该公司的平台利用最新的机器学习突破，为企业提供个性化学习体验。通过使用自己的数据定制GPT-3，Sana的问题和内容生成从语法上正确但泛泛的响应变为高度准确的响应。这带来了60%的改进，为他们的用户提供了更个性化的体验。
- en: 'Elicit is an AI research assistant that helps directly answer research questions
    using findings from academic papers. The assistant finds the most relevant abstracts
    from a large corpus of research papers, then applies GPT-3 to generate the claim
    that the paper makes about the question. A custom version of GPT-3 outperformed
    prompt design and led to improvement in three areas: results were 24% easier to
    understand, 17% more accurate, and 33% better overall.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Elicit是一款AI研究助手，可以直接回答研究问题，利用学术论文的发现。该助手从大量研究论文中找到最相关的摘要，然后应用GPT-3生成论文对问题的主张。定制版本的GPT-3优于提示设计，在三个方面取得了改进：结果更容易理解24%，准确性提高17%，整体效果提高33%。
- en: How to Customize GPT-3 for Your Application
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何为您的应用程序定制GPT-3
- en: To get started customizing GPT-3, you’ll just run a single command in the OpenAI
    command line tool with a file you provide. Your custom version will start training
    and then be available immediately in the OpenAI API.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始定制GPT-3，你只需在OpenAI命令行工具中运行一个命令，并提供一个文件。你定制的版本将开始训练，然后立即在OpenAI API中可用。
- en: 'At a very high level, customizing GPT-3 for your application involves the following
    three steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常高层次上，为你的应用程序定制GPT-3涉及以下三个步骤：
- en: Prepare new training data and upload it to the OpenAI server
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备新的训练数据并上传至OpenAI服务器
- en: Fine-tune the existing models with the new training data
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用新的训练数据微调现有模型
- en: Use the fine-tuned model
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用微调后的模型
- en: Prepare and upload training data
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备并上传训练数据
- en: Training data is what the model takes in as input for fine-tuning. Your training
    data must be a JSONL document, where each line is a prompt-completion pair corresponding
    to a training example. For model fine-tuning you can provide an arbitrary number
    of examples. It is highly recommended that you create a values-targeted dataset
    (which we’ll define and discuss in [Chapter 6](ch06.xhtml#challengescomma_controversiescomma_and))
    to provide the model with high-quality data and wide representation. Fine-tuning
    improves performance with more examples, so the more examples you provide, the
    better the outcome.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据是模型接受的微调输入。你的训练数据必须是一个JSONL文档，其中每一行是一个与训练示例相对应的提示-完成对。对于模型微调，你可以提供任意数量的示例。强烈建议你创建一个以价值为目标的数据集（我们将在[第六章](ch06.xhtml#challengescomma_controversiescomma_and)中定义和讨论），以提供高质量且广泛表达的数据给模型。微调通过提供更多示例来改善性能，所以你提供的示例越多，结果就越好。
- en: 'Your JSONL document should look something like this:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你的JSONL文档应如下所示：
- en: '[PRE21]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Where *prompt text* should include the exact prompt text you want to complete,
    and *ideal generated text* should include an example of the desired completion
    text that you want GPT-3 to generate.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在*提示文本*中应包括你想完成的确切提示文本，而*理想生成的文本*应包括你想要GPT-3生成的期望完成文本的示例。
- en: 'You can use OpenAI’s CLI data preparation tool to easily convert your data
    into this file format. The CLI data preparation tool accepts files in different
    formats; the only requirement is that they contain a prompt and a completion column/key.
    You can pass a CSV, TSV, XLSX, JSON, or JSONL file, and the tool will save the
    output into a JSONL file ready for fine-tuning. To do this, use the following
    command:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用OpenAI的CLI数据准备工具轻松地将数据转换成这种文件格式。CLI数据准备工具接受不同格式的文件；唯一要求是它们包含提示和完成列/键。你可以传递CSV、TSV、XLSX、JSON或JSONL文件，该工具将将输出保存到一个准备进行微调的JSONL文件中。要执行此操作，请使用以下命令：
- en: '[PRE22]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Where *LOCAL_FILE* is the file you prepared for conversion.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*LOCAL_FILE*是你准备用于转换的文件。
- en: Train a new fine-tuned model
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练一个新的微调模型
- en: 'Once you prepare your training data as described above, you can move on to
    the fine-tuning job with the help of the OpenAI CLI. For that, you need the following
    command:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦按上述方法准备好训练数据，你就可以借助OpenAI CLI开始微调作业。为此，你需要以下命令：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Where *BASE_MODEL* is the name of the base model you’re starting from (Ada,
    Babbage, Curie, or Davinci). Running this command does several things:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*BASE_MODEL*是你正在使用的基础模型名称（Ada、Babbage、Curie或Davinci）。运行此命令会执行几件事：
- en: Uploads the file using the files endpoint (as discussed earlier in this chapter)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文件端点上传文件（如在本章中讨论的）
- en: Fine-tunes the model using the request configuration from the command
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从命令中获取的请求配置微调模型
- en: Streams the event logs until the fine-tuning job is completed
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式传输事件日志，直到完成微调作业
- en: Log streaming is helpful to understand what’s happening in real time and to
    respond to any incidents/failures as they happen. The streaming may take from
    minutes to hours depending on the number of jobs in the queue and the size of
    your dataset.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 日志流式传输有助于实时了解正在发生的情况，并在发生任何事件/故障时做出响应。流式传输可能需要从几分钟到几个小时的时间，具体取决于队列中作业数量和数据集大小。
- en: Use the fine-tuned model
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用微调的模型
- en: Once the model is successfully fine-tuned, you can start using it! You can now
    specify this model as a parameter to the completion endpoint and make requests
    to it using the Playground.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型成功微调，你就可以开始使用它了！你现在可以将这个模型指定为完成端点的参数，并使用Playground向其发出请求。
- en: Tip
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: After the fine-tuning job completes, it may take several minutes for your model
    to become ready to handle requests. If completion requests to your model time
    out, it is likely because your model is still being loaded. If this happens, try
    again in a few minutes.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 微调作业完成后，可能需要几分钟的时间才能使你的模型准备好处理请求。如果对模型进行完成请求超时，很可能是因为你的模型仍在加载中。如果发生这种情况，请在几分钟后重试。
- en: 'You can start making requests by passing the model name as the model parameter
    of a completion request using the following command:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用以下命令将模型名称作为完成请求的模型参数来开始发送请求：
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Where *FINE_TUNED_MODEL* is the name of your model and *YOUR_PROMPT* is the
    prompt you want to complete in this request.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*FINE_TUNED_MODEL*是你的模型名称，*YOUR_PROMPT*是你想在此请求中完成的提示。
- en: You can continue to use all the completion endpoint parameters that were discussed
    in this chapter, like temperature, frequency penalty, presence penalty, etc.,
    on these requests to the newly fine-tuned model as well.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续在这些请求中使用在本章中讨论的所有完成端点参数，如温度、频率惩罚、存在惩罚等，对新微调的模型进行请求。
- en: Note
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: No engine is specified on these requests. This is the intended design and something
    that OpenAI plans on standardizing across other API endpoints in the future.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这些请求未指定引擎。这是故意设计的，OpenAI计划在将来对其他API端点进行标准化。
- en: For more information, refer to OpenAI’s [fine-tuning documentation](https://oreil.ly/dSZao).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 获取更多信息，请参考OpenAI的[fine-tuning documentation](https://oreil.ly/dSZao)。
- en: Tokens
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 令牌
- en: Before diving deeper into how different prompts consume tokens, let’s look more
    closely at what a token is.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究不同提示如何消耗令牌之前，让我们更仔细地看看什么是令牌。
- en: We’ve told you that tokens are numerical representations of words or characters.
    Using tokens as a standard measure, GPT-3 can handle training prompts from a few
    words to entire documents.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们告诉过你，令牌是单词或字符的数值表示。使用令牌作为标准度量，GPT-3可以处理从几个单词到整个文档的训练提示。
- en: For regular English text, *1 token consists of approximately 4 characters*.
    It translates to roughly three-quarters of a word, so for one hundred tokens there
    will be approximately 75 words. As a point of reference, the collected works of
    Shakespeare consist of about 900,000 words, which roughly translates to 1.2 million
    tokens.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于常规英文文本，*1 个令牌大约包含 4 个字符*。这大致相当于三分之四个单词，因此一百个令牌大约会有大约 75 个单词。作为参考，莎士比亚的作品约有
    900,000 个单词，这大致相当于 120 万个令牌。
- en: To maintain the latency of API calls, OpenAI imposes a limit of 2,048 tokens
    (approximately 1,500 words) for prompts and completions.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持 API 调用的延迟，OpenAI 对提示和完成设置了 2,048 个令牌（大约 1,500 个单词）的限制。
- en: To further understand how tokens are calculated and consumed in the context
    of GPT-3 and to stay within the limits set by the API, let’s walk through the
    ways you can measure the token count.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步了解在 GPT-3 的背景下如何计算和消耗令牌，并保持 API 设定的限制之内，让我们逐步了解您可以衡量令牌计数的方式。
- en: In the Playground, as you enter text into the interface, you can see the token
    count update in real time in the footer at the bottom right. It displays the number
    of tokens that will be consumed by the text prompt after hitting the Generate
    button. You can use it to monitor your token consumption every time you interact
    with the Playground (see [Figure 2-10](#token_count_in_the_playground)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Playground 中，当您将文本输入界面时，您可以看到右下角页脚实时更新令牌计数。点击生成按钮后，它将显示由文本提示消耗的令牌数量。您可以使用它来监控每次与
    Playground 交互时的令牌消耗情况（参见 [图2-10](#token_count_in_the_playground)）。
- en: '![](Images/gpt3_0210.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0210.png)'
- en: Figure 2-10\. Token count in the Playground
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10\. Playground 中的令牌计数
- en: The other way to measure the consumption of tokens is by using the GPT-3 Tokenizer
    tool ([Figure 2-11](#tokenizer_tool_by_openai)) that lets you visualize the formation
    of tokens from characters. You can interact with the Tokenizer via a simple text
    box where you type the prompt text and Tokenizer shows you the token and character
    counts along with a detailed visualization.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量令牌消耗的另一种方法是使用 GPT-3 Tokenizer 工具 ([图2-11](#tokenizer_tool_by_openai))，该工具允许您可视化从字符到令牌的形成过程。您可以通过一个简单的文本框与
    Tokenizer 交互，在其中键入提示文本，Tokenizer 会显示令牌和字符计数以及详细的可视化。
- en: '![](Images/gpt3_0211.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0211.png)'
- en: Figure 2-11\. Tokenizer tool by OpenAI
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11\. OpenAI 的 Tokenizer 工具
- en: For integrating the token count metric in your API calls to different endpoints,
    you can patch the logprobs and echo attributes along with the API request to get
    the full list of tokens consumed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要将令牌计数指标集成到您对不同端点的 API 调用中，您可以通过 API 请求补丁 logprobs 和 echo 属性，以获取所消耗的完整令牌列表。
- en: In the next section we will cover how tokens are priced based on the different
    execution engines.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍基于不同执行引擎定价令牌的方式。
- en: Pricing
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定价
- en: In the last section we talked about tokens, which is the smallest fungible unit
    used by OpenAI to determine the pricing for API calls. Tokens allow greater flexibility
    than measuring the number of words or sentences used in the training prompt, and
    due to the granularity of tokens, they can be easily processed and used to measure
    the pricing for a wide range of training prompts.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们谈到了令牌，这是 OpenAI 用来确定 API 调用定价的最小可互换单元。令牌比测量训练提示中使用的单词或句子的数量具有更大的灵活性，由于令牌的粒度，它们可以很容易地被处理和用于衡量各种训练提示的定价。
- en: Every time you call the API from either the Playground or programmatically,
    behind the scenes the API calculates the number of tokens used in the training
    prompt along with the generated completion and charges each call on the basis
    of the total number of tokens used.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你从 Playground 或以编程方式调用 API，API 在幕后计算用于训练提示和生成完成的令牌数量，并根据使用的总令牌数量收取每次调用的费用。
- en: OpenAI generally charges a flat fee per 1,000 tokens, with the fee depending
    on the execution engine used in the API call. Davinci is the most powerful and
    expensive, while Curie, Babbage, and Ada are cheaper and faster.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 通常每 1,000 个令牌收取固定费用，费用取决于 API 调用中使用的执行引擎。Davinci 是最强大且最昂贵的，而 Curie、Babbage
    和 Ada 则更便宜且更快。
- en: '[Table 2-2](#model_pricing) shows the pricing for the various API engines at
    the time this chapter was written (December 2021).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-2](#model_pricing)显示了本章撰写时（2021年12月）各种 API 引擎的定价。'
- en: Table 2-2\. Model pricing
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-2\. 模型定价
- en: '| Model | Price per 1,000 tokens |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Model | Price per 1,000 tokens |'
- en: '| --- | --- |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Davinci (most powerful) | $0.0600 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Davinci（最强大）| $0.0600 |'
- en: '| Curie | $0.0060 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Curie | $0.0060 |'
- en: '| Babbage | $0.0012 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 巴贝奇 | $0.0012 |'
- en: '| Ada (fastest) | $0.0008 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 艾达（最快） | $0.0008 |'
- en: The company works on the cloud pricing model of “pay as you go.” For updated
    pricing check the [online pricing schedule](https://oreil.ly/2yKos).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 该公司采用“按使用量付费”的云定价模式。有关更新的定价，请查看[在线定价表](https://oreil.ly/2yKos)。
- en: Instead of monitoring the tokens for each API call, OpenAI provides a [reporting
    dashboard](https://oreil.ly/rvMM9) to monitor daily cumulative token usage. Depending
    on your usage, it may look something like [Figure 2-12](#api_usage_dashboard).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 提供了一个[报告仪表板](https://oreil.ly/rvMM9)，以监控每日累积的 token 使用情况，而不是监视每个 API
    调用的 token。根据你的使用情况，它可能会类似于[图 2-12](#api_usage_dashboard)。
- en: '![](Images/gpt3_0212.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0212.png)'
- en: Figure 2-12\. API usage dashboard
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. API 使用仪表板
- en: In [Figure 2-12](#api_usage_dashboard) you can see a bar graph showing the daily
    token consumption. The dashboard helps you monitor token usage and costs for your
    organization, so that you can regulate API usage and stay within your budget.
    There is also an option to monitor cumulative usage and get a breakdown of token
    count per API call. This should give you enough flexibility to create policies
    around token consumption and pricing for your organization. Now that you understand
    the ins and outs of the Playground and the API, we will take a look at GPT-3’s
    performance on typical language modeling tasks.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2-12](#api_usage_dashboard)中，你可以看到一个条形图显示了每日 token 的消耗量。该仪表板帮助你监控组织的 token
    使用情况和成本，以便你可以调节 API 使用情况，并保持在预算范围内。还有一个选项可以监控累积使用量，并获取每个 API 调用的 token 计数的细分。这应该为你提供足够的灵活性，以制定围绕
    token 消耗和定价的政策。现在你已经了解了 Playground 和 API 的方方面面，我们将看一下 GPT-3 在典型语言建模任务上的表现。
- en: Tip
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Beginners who have just started with GPT-3 can find it hard to wrap their heads
    around token consumption. Many users enter prompt texts that are too long, which
    leads to the overuse of credits, followed by unexpected fees. To avoid this, during
    your initial days, use the API usage dashboard to observe the number of tokens
    consumed and see how the length of prompts and completions affect token usage.
    It can help prevent uncontrolled use of credits and keep everything within budget.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 刚开始接触 GPT-3 的初学者可能会发现很难理解 token 的消耗。许多用户输入的提示文本过长，导致信用额度的过度使用，随后产生意外费用。为了避免这种情况，在最初的几天里，使用
    API 使用仪表板观察 token 消耗量，并了解提示文本和完成的长度如何影响 token 使用情况。这可以帮助防止信用额度的不受控制使用，并保持一切在预算范围内。
- en: GPT-3’s Performance on Conventional NLP Tasks
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-3 在传统自然语言处理任务上的表现
- en: 'GPT-3 is a highly advanced and sophisticated successor to the NLP field, built
    and trained using the core NLP approaches and deep neural networks. For any AI-based
    modeling approach, the model performance is evaluated in the following way: First
    you train the model for a specific task (like classification, question and answer,
    text generation, etc.) on training data; then you verify the model performance
    using the test data (new, previously unseen data).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 是自然语言处理领域的一个高度先进和复杂的继任者，采用核心自然语言处理方法和深度神经网络构建和训练。对于任何基于人工智能的建模方法，模型性能是通过以下方式评估的：首先，你使用训练数据为特定任务（如分类、问答、文本生成等）训练模型；然后，使用测试数据（新的、之前未见的数据）验证模型性能。
- en: In a similar way, there is a standard set of NLP benchmarks for evaluating the
    performance of NLP models and coming up with a relative model ranking or comparison.
    This comparison, or *relative ranking*, allows you to pick and choose the best
    model for a specific NLP task (business problem).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，有一套用于评估自然语言处理模型性能并制定相对模型排名或比较的标准自然语言处理基准。这种比较，或*相对排名*，使你可以为特定的自然语言处理任务（业务问题）选择最佳模型。
- en: In this section we will discuss the performance of GPT-3 on some standard NLP
    tasks as seen in [Figure 2-13](#conventional_nlp_tasks) and compare it with the
    performance of similar models on the respective NLP tasks.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 GPT-3 在一些标准自然语言处理任务中的表现，如[图 2-13](#conventional_nlp_tasks)中所示，并将其与类似模型在相应自然语言处理任务中的表现进行比较。
- en: '![](Images/gpt3_0213.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0213.png)'
- en: Figure 2-13\. Conventional NLP tasks
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 传统自然语言处理任务
- en: Text Classification
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分类
- en: '*Text classification* is the process of categorizing text into organized groups.
    By using NLP, text classification can automatically analyze text and then assign
    a set of predefined tags or categories based on its context.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本分类*是将文本分类到组织好的群组中的过程。通过使用自然语言处理，文本分类可以自动分析文本，然后根据其上下文分配一组预定义的标签或类别。'
- en: Text classification involves analyzing the text provided as input and assigning
    it a label, score, or other attribute that characterizes it. Some common examples
    of text classification are sentiment analysis, topic labeling, and intent detection.
    You can use a number of approaches to get GTP-3 to classify text, again ranging
    from zero-shot classification (where you don’t give any examples to the model)
    to single-shot and few-shot classification (where you show some examples to the
    model).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类涉及分析提供的文本并为其分配标签、分数或其他属性来描述它。一些常见的文本分类示例包括情感分析、主题标记和意图检测。您可以使用多种方法让GTP-3对文本进行分类，从零样本分类（不给模型任何示例）到单样本和少样本分类（向模型展示一些示例）。
- en: Zero-shot classification
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零样本分类
- en: Modern artificial intelligence has long aimed to develop models that can perform
    predictive functions on data they have never seen before. This important research
    area is called zero-shot learning. Similarly, a *zero-shot classification* is
    a classification task where no prior training or fine-tuning on labeled data is
    required for the model to classify a piece of text. GPT-3 currently produces results
    for unseen data that are either better than or on par with state-of-the-art AI
    models fine-tuned for that specific purpose. To perform zero-shot classification
    with GPT-3, we must provide it with a compatible prompt. Here is an example of
    a zero-shot classification where the goal is to perform a fact-check analysis
    to determine if the information included in the tweet is correct or incorrect.
    [Figure 2-14](#zero_shot_classification_example) shows a pretty impressive information
    correctness classification result based on a zero-shot example.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能长期以来一直致力于开发能够对它们从未见过的数据执行预测功能的模型。这个重要的研究领域被称为零样本学习。同样，*零样本分类*是一种分类任务，其中模型在对一段文本进行分类时不需要在标记数据上进行先前训练或微调。GPT-3目前在对未见过的数据产生的结果上要么优于，要么与针对特定目的进行微调的最先进的AI模型相媲美。要使用GPT-3执行零样本分类，我们必须为其提供兼容的提示。以下是一个零样本分类的示例，目标是执行事实核查分析，以确定推文中包含的信息是否正确或不正确。[图 2-14](#zero_shot_classification_example)显示了一个基于零样本示例的信息正确性分类结果，相当令人印象深刻。
- en: '![](Images/gpt3_0214.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0214.png)'
- en: Figure 2-14\. Zero-shot classification example
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-14\. 零样本分类示例
- en: 'And here is our prompt:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: '[PRE25]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And the output:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 和输出结果：
- en: '[PRE26]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Single-shot and few-shot classification
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单样本和少样本分类
- en: The other approach to text classification is via fine-tuning an AI model on
    a single or a few training examples, known as single-shot or few-shot text classification,
    respectively. When you provide examples of how to classify text, the model can
    learn information about the object categories based on those examples. This is
    a superset of zero-shot classification that allows you to classify text by providing
    the model with three to four diversified examples. This can be useful specifically
    for downstream use cases, which require some level of context setting.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的另一种方法是通过对单个或少数训练示例对AI模型进行微调，分别称为单样本或少样本文本分类。当您提供如何对文本进行分类的示例时，模型可以根据这些示例学习有关对象类别的信息。这是零样本分类的超集，它允许您通过向模型提供三到四个多样化的示例来对文本进行分类。这对于需要一定程度的上下文设置的下游用例特别有用。
- en: 'Let’s look at the following example of few-shot classification. We are asking
    the model to perform a tweet sentiment analysis classification and giving it three
    tweet examples to illustrate each of the possible labels: positive, neutral, and
    negative. As you can see in [Figure 2-15](#few_shot_classification_example), the
    model, equipped with such a detailed context based on a few examples, is able
    to very easily perform the sentiment analysis of the next tweet.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个少样本分类的例子。我们要求模型执行推文情感分析分类，并给出三个推文示例来说明每个可能的标签：积极的、中性的和消极的。正如你在[图 2-15](#few_shot_classification_example)中所看到的，配备了这样详细的上下文的模型，能够非常容易地对下一个推文进行情感分析。
- en: Note
  id: totrans-293
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: When you recreate prompt examples from the book, or create your own, make sure
    to have adequate line spacing in your prompt. An additional line after a paragraph
    can result in a very different outcome, so you’ll want to play with that and see
    what works best for you.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 当你重新创建书中的提示示例，或者创建自己的示例时，请确保你的提示中有足够的行间距。段落之后的额外一行可能会导致非常不同的结果，所以你需要尝试一下，看看哪种效果最好。
- en: '![](Images/gpt3_0215.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0215.png)'
- en: Figure 2-15\. Few-shot classification example
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-15\. 少样本分类示例
- en: 'Here is our prompt:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: '[PRE27]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And the output:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE28]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Batch classification
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量分类
- en: After understanding the few-shot classification with GPT-3, let’s dive deeper
    into classification with batch classification, which enables you to classify input
    samples in batches in a single API call instead of classifying just one example
    per API call. It is suitable for applications where you want to classify multiple
    examples in a single go, just like the tweet sentiment analysis task we examined,
    but analyzing a few tweets in a row.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了 GPT-3 的少量样本分类后，让我们深入了解批量分类，它使您能够在单个 API 调用中对输入样本进行批量分类，而不是每次只对一个示例进行分类。它适用于您想要一次性分类多个示例的应用程序，就像我们检查的推文情感分析任务一样，但是分析一系列推文。
- en: As with few-shot classification, you want to provide enough context for the
    model to achieve the desired result but in a batch configuration format. Here,
    we define the different categories of tweet sentiment classification using various
    examples in the batch configuration format (Figures [2-16](#batch_classification_example_left_pare)
    and [2-17](#batch_classification_example_left_paren)). Then we ask the model to
    analyze the next batch of tweets.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与少量样本分类类似，您希望提供足够的上下文以使模型实现期望的结果，但以批量配置格式提供。在这里，我们使用批量配置格式中的不同示例定义了不同类别的推文情感分类（图
    [2-16](#batch_classification_example_left_pare) 和 [2-17](#batch_classification_example_left_paren)）。然后我们要求模型分析下一批推文。
- en: '![](Images/gpt3_0216.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0216.png)'
- en: Figure 2-16\. Batch-classification example (part 1)
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-16\. 批量分类示例（第一部分）
- en: '![](Images/gpt3_0217.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0217.png)'
- en: Figure 2-17\. Batch-classification example (part 2)
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-17\. 批量分类示例（第二部分）
- en: 'Here is our prompt:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: '[PRE29]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And the output:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE30]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, the model recreated the batch sentiment analysis format and
    classified the tweets successfully. Now let’s move on to see how it performs with
    named entity recognition tasks.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模型重新创建了批量情感分析格式并成功对推文进行了分类。现在让我们看看它在命名实体识别任务中的表现。
- en: Named Entity Recognition
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Named entity recognition (NER) is an information extraction task that seeks
    to locate and classify named entities mentioned in unstructured text into predefined
    categories such as person names, organizations, locations, expressions of time,
    quantities, monetary values, percentages, etc.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是一项信息提取任务，旨在将未结构化文本中提及的命名实体定位并分类到预定义类别中，如人名、组织、地点、时间表达式、数量、货币价值、百分比等。
- en: NER helps to make the responses more personalized and relevant but the current
    state-of-the-art approaches require massive amounts of data for training before
    you even start with the prediction. GPT-3, on the other hand, can recognize general
    entities like people, places, and organizations out of the box without humans
    providing even a single training example.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: NER 有助于使响应更加个性化和相关，但当前最先进的方法在进行预测之前需要大量的数据进行训练。另一方面，GPT-3 可以在没有人提供任何训练示例的情况下，直接识别出人物、地点和组织等通用实体。
- en: 'In the following example we use a davinci-instruct-series version of the model
    that was in beta at the time of writing this book, and the model gathers prompts
    to train and improve the future OpenAI API models. We give it a simple task: to
    extract contact information from an example email. It successfully completes the
    task on the first attempt ([Figure 2-18](#ner_example)).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用了此书编写时处于测试版状态的 davinci-instruct-series 模型版本，该模型收集提示以训练和改进未来的 OpenAI
    API 模型。我们给它一个简单的任务：从示例电子邮件中提取联系信息。它在第一次尝试时成功完成了任务（[图 2-18](#ner_example)）。
- en: '![](Images/gpt3_0218.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0218.png)'
- en: Figure 2-18\. NER example
  id: totrans-318
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-18\. NER 示例
- en: 'Here is our input:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输入：
- en: '[PRE31]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And the output:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE32]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Text Summarization
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本摘要
- en: '*Text summarization* is a technique for generating a concise and exact summary
    of lengthy texts while focusing on the sections that convey useful information,
    without losing the overall meaning. GPT-3-based text summarization aims to transform
    lengthy pieces of tl;dr^([2](ch02.xhtml#ch01fn4)) texts into their condensed versions.
    Such tasks are generally difficult and costly to accomplish manually. With GPT-3,
    it is a matter of a single input and a few seconds!'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本摘要*是一种生成长度文本的简洁准确摘要的技术，重点放在传达有用信息的部分上，而不失去整体意义。基于 GPT-3 的文本摘要旨在将冗长的 tl;dr^([2](ch02.xhtml#ch01fn4))
    文本转换为它们的简化版本。这样的任务通常很难且成本高昂地手动完成。而有了 GPT-3，只需要一个输入和几秒钟的时间！'
- en: NLP models can be trained to comprehend documents and identify the sections
    that convey important facts and information before producing the summarized texts.
    However, such models need a large amount of training samples before they can learn
    the context and start summarizing unseen input.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: NLP模型可以被训练以理解文档，并识别传达重要事实和信息的部分，然后生成摘要文本。然而，这样的模型在它们能够学习上下文并开始总结未见输入之前，需要大量的训练样本。
- en: 'GPT-3’s abstractive summarization is the key to solving the problem of information
    extraction. By producing summaries instead of merely extracting key information,
    GPT-3 can provide a more comprehensive and accurate understanding of the text.
    It uses a zero-shot or few-shot approach toward text summarization, making it
    useful for a variety of use cases. With GPT-3 there are multiple ways you can
    go about summarizing the text depending on your use case: basic summaries, one-line
    summaries, or grade-level summaries. Let’s have a quick walk-through of these
    approaches.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的抽象总结是解决信息提取问题的关键。通过生成摘要而不仅仅是提取关键信息，GPT-3可以提供对文本的更全面和准确的理解。它采用零样本或少样本方法进行文本摘要，因此适用于各种用例。使用GPT-3，您可以根据用例的不同方式进行文本摘要：基本摘要、一行摘要或年级摘要。让我们快速浏览一下这些方法。
- en: Most of the time the model is able to generate decent results in the form of
    a review summary, but sometimes it can output irrelevant results depending on
    the prior context. To avoid the problem of getting unwanted results, you can set
    the “best of” parameter to 3, which will always give you the best of three results
    generated by the API. In the example shown in [Figure 2-19](#text_summarization_example),
    after a few tries and minor parameter tweaking, we got decent results.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，该模型能够以评论摘要的形式生成体面的结果，但有时会根据先前的上下文输出无关的结果。为了避免获得不想要的结果问题，您可以将“最佳”的参数设置为3，这将始终给您API生成的三个最佳结果中的最佳结果。在[图2-19](#text_summarization_example)所示的示例中，经过几次尝试和微小参数调整后，我们获得了体面的结果。
- en: '![](Images/gpt3_0219.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0219.png)'
- en: Figure 2-19\. Text summarization example
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-19\. 文本摘要示例
- en: 'Here is our prompt:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: '[PRE33]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'And the output:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE34]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Since many people spend hours per day reading and writing emails, summarizing
    them is a widely desired use case for GPT-3\. Let’s see how GPT-3 does with summarizing
    a three-paragraph email into one crisp line ([Figure 2-20](#email_summarization_example)).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多人每天花费数小时阅读和撰写电子邮件，将它们进行摘要是GPT-3的一种广泛期望的用例。让我们看看GPT-3如何将三段邮件摘要为一行简洁的句子（参见[图2-20](#email_summarization_example)）。
- en: '![](Images/gpt3_0220.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0220.png)'
- en: Figure 2-20\. Email summarization example
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-20\. 电子邮件摘要示例
- en: To achieve the desired result, we pasted the full email and then simply added
    “one-sentence summary:” at the end. We also included a “.” stop sequence to tell
    the model that it should stop its summary generation after a single sentence.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到期望的结果，我们粘贴了完整的电子邮件，然后简单地在末尾添加了“一句话摘要：”。我们还包括了一个“.”停止序列，告诉模型在生成摘要后应该停止其摘要生成。
- en: 'Our prompt:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提示：
- en: '[PRE35]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And the output:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE36]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Text Generation
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本生成
- en: Before the introduction of GPT-3, there was a general understanding that AI
    models were capable of carrying on short conversations with humans that answered
    specific questions or handled specific tasks. However, the models were not sophisticated
    enough to handle complicated text generation tasks, and they started to lose track
    of the conversation whenever they encountered something complex or abstract.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入GPT-3之前，人们普遍认为AI模型能够与人类进行短暂的对话，回答特定问题或处理特定任务。然而，这些模型还不够复杂，无法处理复杂的文本生成任务，一旦遇到复杂或抽象的内容，它们就开始失去对话的脉络。
- en: In the complicated world of natural language generation, GPT-3 has shaken the
    notion of language models being limited to trivial tasks. Text generation is the
    greatest strength of GPT-3\. It is capable of generating textual content that
    is almost indistinguishable from human-written text. GPT-3 is trained on billions
    of words from the training dataset to generate text in response to a variety of
    prompts. It generates an average of 4.5 billion words per day, [according to OpenAI](https://oreil.ly/fbyhM).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言生成的复杂世界中，GPT-3已经动摇了语言模型仅限于琐碎任务的观念。文本生成是GPT-3的最大优势。它能够生成几乎与人类写作的文本难以区分的文本内容。GPT-3受到训练数据集中数十亿字词的训练，以响应各种提示生成文本。根据OpenAI的说法，它每天生成平均45亿字，[根据OpenAI的说法](https://oreil.ly/fbyhM)。
- en: In the next two examples, we experiment with using GPT-3 to create content for
    a personal productivity app start-up and social media posts. We give the model
    only minimal context, and it generates many of the responses in Figures [2-21](#article_generation_example)
    and [2-22](#social_media_post_generation_example) on the first take.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个示例中，我们尝试使用GPT-3为个人生产力应用创业公司和社交媒体帖子创建内容。我们只给模型提供了最少的上下文，它在第一次尝试时就生成了图[2-21](#article_generation_example)和[2-22](#social_media_post_generation_example)中的许多响应。
- en: Article generation
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文章生成
- en: '![](Images/gpt3_0221.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0221.png)'
- en: Figure 2-21\. Article generation example
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-21\. 文章生成示例
- en: 'Our prompt:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提示：
- en: '[PRE37]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'And the output:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE38]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Social media post generation
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社交媒体帖子生成
- en: '![](Images/gpt3_0222.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/gpt3_0222.png)'
- en: Figure 2-22\. Social media post generation example
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-22\. 社交媒体帖子生成示例
- en: 'Here is our prompt:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: '[PRE39]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'And the output:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE40]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Conclusion
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter we have covered the OpenAI Playground, prompt engineering, and
    the different components of the OpenAI API, followed by Playground examples covering
    the major NLP tasks. By now, you should have an understanding of how the API works
    in tandem with different components and how to use the Playground as the base
    to design and experiment with different training prompts.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了OpenAI Playground，提示工程以及OpenAI API的不同组件，接着是覆盖主要NLP任务的Playground示例。到目前为止，你应该已经了解了API是如何与不同组件配合工作的，以及如何使用Playground作为设计和尝试不同训练提示的基础。
- en: In the next chapter, we’ll walk you through how to use GPT-3 with different
    programming languages to integrate the API in your product or build a completely
    new application from scratch.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将指导您如何使用GPT-3与不同的编程语言结合使用API来将其整合到您的产品中或从头开始构建全新的应用程序。
- en: ^([1](ch02.xhtml#ch01fn3-marker)) For more than two hundred documents, OpenAI
    offers a [beta API](https://oreil.ly/cY0Z6).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#ch01fn3-marker)) 对于两百多个文档，OpenAI提供了[beta API](https://oreil.ly/cY0Z6)。
- en: ^([2](ch02.xhtml#ch01fn4-marker)) A longstanding internet abbreviation for “too
    long; didn’t read.”
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#ch01fn4-marker)) 一个长期存在的互联网缩写，意思是“太长，没看”。
