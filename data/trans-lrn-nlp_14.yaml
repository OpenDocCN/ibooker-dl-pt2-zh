- en: 11 Conclusions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 结论
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Summarizing important concepts covered by this book
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结本书涵盖的重要概念
- en: Summarizing related important emerging concepts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结相关的重要新兴概念
- en: Considering limitations and environmental and ethical considerations around
    transfer learning methods for NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑自然语言处理中关于迁移学习方法的局限性以及环境和伦理考虑
- en: Envisioning the future of transfer learning in NLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展望自然语言处理中的迁移学习未来
- en: Keeping up with latest developments in the field
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟上该领域的最新发展
- en: We’ve covered a great deal of material in the preceding chapters—we hope they
    were informative and engaging. This concluding chapter attempts to provide a meaningful
    summary of everything we did and looks to the future of the field and emerging
    research trends. Due to the prolific output and the quick-moving nature of the
    field, we certainly have not covered every single influential architecture or
    promising research direction. To mitigate this, we include a brief discussion
    of various research trends we did not get a chance to cover in this book, making
    connections to, and framing in the context of, covered material as much as possible.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们涵盖了大量的材料——我们希望它们既具信息性又引人入胜。这一结论性章节试图对我们所做的一切进行有意义的总结，并展望该领域的未来和新兴的研究趋势。由于该领域的多产产出和快速发展的性质，我们当然没有涵盖每一个有影响力的架构或有前途的研究方向。为了减轻这一点，我们对我们在本书中没有机会涵盖的各种研究趋势进行了简要讨论，尽可能与已涵盖的材料进行联系和框架化。
- en: In this chapter, we also try to provide a broader context by touching on emerging
    questions that have not traditionally been given much attention, such as ethical
    considerations and the environmental impact of the various models. These are closely
    tied to the awareness of the limitations of these models, which we try to highlight
    as much as possible in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还试图通过涉及一些传统上没有受到太多关注的新兴问题，例如伦理考虑和各种模型的环境影响，提供更广泛的背景。这些与对这些模型的局限性的认识密切相关，我们在本章中尽可能突出这一点。
- en: Crucially, we discuss various tips for staying up-to-date in a rapidly moving
    field such as this one. It is important to stress that having mastered the content
    of the book, you are only now beginning your journey in the field. The tools and
    skills presented will change over time, and each unique application of them may
    require creativity on your part or new techniques yet to be developed. Retaining
    a competitive edge in a rapidly moving field such as this is truly a journey,
    not a destination. We encourage readers to keep an inquisitive attitude toward
    ongoing research and to continue to contribute in some capacity to its development.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，我们讨论了在这样一个快速发展的领域中保持最新的各种提示。强调掌握了本书内容后，您现在只是开始您在该领域的旅程。所提出的工具和技能会随时间变化，并且它们的每个独特应用可能需要您的创造力或者尚未开发的新技术。在这样一个快速发展的领域中保持竞争优势确实是一次旅程，而不是一个目的地。我们鼓励读者对正在进行的研究保持探究的态度，并在某种程度上继续为其发展做出贡献。
- en: Let’s begin this final chapter by overviewing the key concepts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过概述关键概念来开始这一最后一章。
- en: 11.1 Overview of key concepts
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 关键概念概述
- en: Transfer learning aims to leverage prior knowledge from different settings—be
    it a different task, language, or domain—to help solve a problem at hand. It is
    inspired by the way in which humans learn, because we typically do not learn things
    from scratch for any given problem, but rather build on prior knowledge that may
    be related. Allowing a practitioner without substantial computing resources to
    achieve state-of-the-art performance is considered an important step toward democratizing
    access to the fruits of the ongoing technological revolution. As a more concrete
    motivation, consider the representative costs of training various sizes of the
    BERT model, as shown in figure 11.1.[¹](#pgfId-1247306)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习旨在利用不同设置中的先前知识——无论是不同的任务、语言还是领域——来帮助解决手头的问题。它受到人类学习方式的启发，因为我们通常不会从头开始学习任何给定的问题，而是建立在可能相关的先前知识上。使没有实质计算资源的从业者能够达到最先进的性能被认为是向民主化获得正在进行的技术革命成果的重要一步。作为更具体的动机，考虑一下训练不同大小的
    BERT 模型的代表性成本，如图 11.1 所示。[¹](#pgfId-1247306)
- en: '![11_01](../Images/11_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![11_01](../Images/11_01.png)'
- en: Figure 11.1 Training costs for various sizes of BERT. Two representative costs
    are shown—for a single run and for an entire training process that includes hyperparameter
    tuning. The largest size of 1.5 billion parameters costs $80k for single run,
    and $1.6 million when all optimization steps are accounted for!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 BERT 不同规模的训练成本。展示了两种代表性成本——单次运行和包括超参数调整在内的整个训练过程。15亿参数的最大规模单次运行成本为$80k，而所有优化步骤计算下的成本为$1.6百万！
- en: As can be seen in the figure, the largest-sized BERT training cost can reach
    into millions of dollars. Transfer learning literally enables you to reuse this
    precious knowledge on your personal computing projects within a few hours and,
    at worst, a few dollars for fine-tuning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图所示，最大规模的BERT训练成本可能高达数百万美元。迁移学习确实可以让您在几小时内，最坏情况下花费几美元用于微调，将这些宝贵的知识重复利用于您的个人计算项目中。
- en: Transfer learning was popularized in computer vision before it recently came
    to be used heavily by the natural language processing (NLP) community. Whereas
    computer vision deals with teaching computers how to understand and act on images
    and videos, NLP considers how to do the same with human speech, be it text or
    speech audio. In this book, we focused on text. Some NLP tasks of particular interest
    to us include document classification, machine translation, and question answering.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中推广的迁移学习最近开始被自然语言处理（NLP）社区大量使用。而计算机视觉涉及教计算机如何理解和处理图像和视频，NLP则考虑如何处理人类语音，无论是文本还是语音音频。在本书中，我们关注的是文本。我们特别感兴趣的一些NLP任务包括文档分类、机器翻译和问答。
- en: Although historically such tasks were initially solved by attempting to specify
    fixed rules for every scenario—a paradigm now known as symbolic AI—machine learning
    has now become the dominant trend. Instead of explicitly programming a computer
    for every possible scenario, the computer is *trained* to associate input to output
    signals by seeing many examples of such corresponding input-output pairs. The
    methods used to *learn* appropriate input-output associations have traditionally
    included decision trees, random forests, kernel methods such as SVM, and neural
    networks. Neural networks have recently become the clear favorite for learning
    such representations for perceptual problems, that is, computer vision and NLP.
    As such, this is the most important class of methods we explored in this book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在历史上，这些任务最初是通过试图为每种情况制定固定规则来解决的——这种范式现在被称为符号AI——但机器学习现在已成为主导趋势。计算机不再为每种可能的情况明确编程，而是通过看到许多这种相应的输入-输出对的示例来*训练*计算机将输入与输出信号相关联。传统上用于*学习*适当的输入-输出关系的方法包括决策树、随机森林、诸如SVM的核方法和神经网络。神经网络最近已成为解决感知问题（即计算机视觉和NLP）的表示学习方法的首选。因此，这是我们在本书中探讨的最重要的方法类别。
- en: 'Before diving into modern transfer learning method for NLP, we conducted a
    review experiment with some traditional machine learning methods. Specifically,
    we employed the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究现代NLP的迁移学习方法之前，我们进行了一项关于传统机器学习方法的回顾性实验。具体来说，我们采用了以下方法：
- en: Logistic regression
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Support vector machines
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Random forests
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient-boosting machines
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升机
- en: 'to look at two important problems: email spam detection and Internet Movie
    Database (IMDB) movie review classification. To turn text into numbers for this
    experiment, we used the bag-of-words model. This model simply counts the frequency
    of word tokens contained in each email and thereby represents it as a vector of
    such frequency counts.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以解决两个重要问题：电子邮件垃圾检测和互联网电影数据库（IMDB）电影评论分类。为了将文本转换为数字，我们使用了词袋模型。该模型简单地计算了每封电子邮件中包含的单词标记的频率，从而将其表示为这些频率计数的向量。
- en: Modern NLP methodologies have focused on vectorizing sections of text—words,
    subwords, sentences, and so on—via techniques such as word2vec and sent2vec. The
    resulting numerical vectors are then further processed as features of a traditional
    machine learning approach, such as being used for classification with random forests.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现代自然语言处理（NLP）方法学主要集中在将文本部分（词语、子词、句子等）向量化上，采用诸如word2vec和sent2vec之类的技术。然后将得到的数值向量进一步处理，作为传统机器学习方法的特征，例如用于随机森林分类。
- en: 'As was outlined in the first chapter of this book, this important subarea of
    NLP research has a rich history originating with the *term-vector model of information
    retrieval* in the 1960s. This culminated with pretrained shallow neural-network-based
    techniques such as the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '正如本书第一章所概述的，这一重要的自然语言处理研究子领域起源于20世纪60年代的*信息检索术语向量模型*。这在预训练的浅层神经网络技术方面达到了高潮，包括以下内容:'
- en: fastText
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastText
- en: GloVe
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GloVe
- en: word2vec, which came in several variants in the mid-2010s including Continuous
    Bag of Words (CBOW) and Skip-Gram
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec，在2010年代中期推出了几个变体，包括连续词袋（CBOW）和Skip-Gram
- en: Both CBOW and Skip-Gram are extracted from shallow neural networks that were
    trained for various goals. Skip-Gram attempts to predict words neighboring any
    target word in a sliding window, whereas CBOW attempts to predict the target word
    given the neighbors. GloVe, which stands for “Global Vectors,” attempts to extend
    word2vec by incorporating global information into the embeddings. It optimizes
    the embeddings such that the cosine product between words reflects the number
    of times they co-occur, with the goal of making the resulting vectors more interpretable.
    The fastText technique attempts to enhance word2vec by repeating the Skip-Gram
    methods on character n-grams (versus word n-grams), thereby able to handle previously
    unseen words. Each of these variants of pretrained embeddings has its strengths
    and weaknesses. As a numerical demonstration of this class of methods, we used
    fastText word embeddings to revisit the IMDB movie classification example, where
    the bag-of-words method was replaced with fastText for converting the text into
    numbers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW和Skip-Gram都来自于训练用于各种目标的浅层神经网络。Skip-Gram试图预测滑动窗口中任何目标词附近的单词，而CBOW试图预测给定邻居的目标词。GloVe，代表“全局向量”，试图通过将全局信息合并到嵌入中来扩展word2vec。它优化了嵌入，使得单词之间的余弦乘积反映它们共同出现的次数，从而使得结果向量更具可解释性。fastText技术试图通过对字符n-gram（而不是单词n-gram）重复Skip-Gram方法来增强word2vec，从而能够处理以前未见过的单词。这些预训练嵌入的每个变体都有其优点和缺点。作为这类方法的数值演示，我们使用fastText词嵌入来重新访问IMDB电影分类示例，那里将词袋模型替换为fastText以将文本转化为数字。
- en: 'Several techniques were inspired by word2vec to try to embed larger section
    of text into vector spaces in such a way that sections of text with similar meanings
    would be closer to each other in the induced vector space. This enables arithmetic
    to be performed on these sections of text to make inference with regards to analogies,
    combined meanings, and so forth. Such methods include the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '几种技术受word2vec的启发，试图将较大的文本部分嵌入到向量空间中，以便在诱导向量空间中含义类似的文本部分彼此靠近。这使得可以在这些文本部分上进行算术运算，以进行关于类比、组合含义等推理。这样的方法包括以下内容:'
- en: Paragraph vectors, or *doc2vec*, which exploits the concatenation (versus averaging)
    of words from pretrained word embeddings in summarizing them
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段落向量，或*doc2vec*，利用了从预训练词嵌入中摘要单词的连接（而不是平均）。
- en: '*Sent2vec*, which extends the classic Continuous Bag of Words (CBOW) of word2vec—where
    a shallow network is trained to predict a word in a sliding window from its context—to
    sentences by optimizing word and word n-gram embeddings for an accurate averaged
    representation'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sent2vec*扩展了word2vec的经典连续词袋（CBOW）—其中一个浅层神经网络被训练以从其上下文中的滑动窗口中预测一个词—到通过优化词和词n-gram的嵌入来对句子进行准确的平均表示。'
- en: As a numerical demonstration of this class of methods, we used an implementation
    of sent2vec that builds on fastText, instead of bag-of-words, to perform the IMDB
    movie classification experiment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这类方法的数值演示，我们使用了一个基于fastText而非词袋模型的sent2vec的实现来执行IMDB电影分类实验。
- en: 'Several authors[²](#pgfId-1247340), [³](#pgfId-1247343), [⁴](#pgfId-1247346)
    have suggested various classification systems for categorizing transfer learning
    methods into groups. Roughly speaking, categorization is based on whether transfer
    occurs among different languages, tasks, or data domains. Each of these types
    of categorization is usually correspondingly referred to as the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '一些作者[²](#pgfId-1247340), [³](#pgfId-1247343), [⁴](#pgfId-1247346)提出了各种分类系统，将迁移学习方法归类到不同的组别中。粗略地说，分类是基于迁移是否发生在不同的语言、任务或数据领域之间。通常，这些分类类型对应着以下内容:'
- en: '*Cross-lingual learning*'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跨语言学习*'
- en: '*Multitask learning*'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多任务学习*'
- en: '*Domain adaptation*'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域自适应*'
- en: We performed a multitask transfer learning experiments using the familiar tasks
    of IMDB classification and email spam detection to illustrate the concept. To
    illustrate domain adaptation via example, we used an autoencoder to adapt a model
    trained for IMDB movie review classification to the domain of Amazon book reviews.
    This exercise also allowed us to illustrate an instance of zero-shot transfer
    learning, where no fine-tuning in the Amazon book review domain was necessary
    to begin providing valuable results.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一系列多任务迁移学习实验，使用了 IMDB 分类和电子邮件垃圾邮件检测这些熟悉的任务来说明这个概念。为了通过示例说明领域自适应，我们使用了自动编码器来调整一个在
    IMDB 电影评论分类上训练的模型，以适应亚马逊图书评论的领域。这个练习还允许我们说明了零-shot 迁移学习的一个实例，即在亚马逊图书评论领域不需要微调就可以开始提供有价值的结果。
- en: Advances in sequence-to-sequence modeling brought forth a revolution for tasks
    such as machine translation. The encoder and decoder in this setup were initially
    recurrent neural networks (RNNs). Due to problems with long input sequences, the
    technique known as attention was developed to allow output to focus only on relevant
    sections of the input. Although initially this was combined with RNNs, it evolved
    into the technique of self-attention being used to build out both the encoder
    and decoder. Self-attention differs from the original attention formulation in
    that associations are sought between parts of a sequence and other parts of the
    same sequence, rather than between parts of two distinct input and output sequences.
    The architecture in which self-attention replaced attention came to be known as
    *the transformer*, and it scales better than earlier RNN-based sequence-to-sequence
    models on parallel computing architecture. This improved scalability drove its
    wide adoption over the competing architectures. We used a pretrained translation
    transformer model between English and the Ghanaian language Twi to probe the efficacy
    and other characteristics of this important architecture.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列建模的进展为诸如机器翻译之类的任务带来了革命。该设置中的编码器和解码器最初是循环神经网络（RNNs）。由于输入序列过长的问题，发展了一种称为注意力的技术，允许输出仅关注输入的相关部分。尽管最初这与
    RNNs 结合使用，但它发展成为了使用自注意力构建编码器和解码器的技术。自注意力与最初的注意力公式不同，因为它寻求序列的部分与同一序列的其他部分之间的关联，而不是两个不同输入和输出序列的部分之间的关联。自注意力取代注意力的架构被称为
    *变压器*，它在并行计算架构上比早期基于 RNN 的序列到序列模型更具可伸缩性。这种改进的可扩展性推动了它在竞争架构中的广泛采用。我们使用了一个预训练的英文到加纳语
    Twi 的翻译变压器模型来探索这一重要架构的效能和其他特性。
- en: Early explorations of transfer learning for NLP focused on analogies to computer
    vision, where it has been used successfully for a while. One such model—SIMOn—employed
    character-level convolutional neural networks (CNNs) combined with bidirectional
    LSTMs for structural semantic text classification. SIMOn stands for *Semantic
    Inference for the Modeling of Ontologies*. It was developed during DARPA’s Data-Driven
    Discovery of Models (D3M)[⁵](#pgfId-1247361) program,[⁶](#pgfId-1247364) which
    was an attempt to automate some typical tasks faced by data scientists. It demonstrated
    NLP transfer learning methods directly analogous to those that have been used
    in computer vision. The features learned by this model were shown to also be useful
    for unsupervised learning tasks and to work well on social media language data,
    which can be somewhat idiosyncratic and very different from the kind of language
    on Wikipedia and other large book-based datasets. Column-type classification was
    used as an illustrative example for this modeling framework.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 的迁移学习的早期探索侧重于与计算机视觉的类比，而计算机视觉在此方面已经成功使用了一段时间。其中一个模型——SIMOn——采用了字符级卷积神经网络（CNNs）结合双向LSTM用于结构语义文本分类。SIMOn
    代表 *本体建模的语义推理*。它是在 DARPA 的数据驱动模型发现（D3M）[⁵](#pgfId-1247361) 计划中开发的，该计划是为了自动化数据科学家面临的一些典型任务。它展示了与计算机视觉中使用的方法直接类似的
    NLP 迁移学习方法。该模型学到的特征还被证明对无监督学习任务也有用，并且在社交媒体语言数据上表现良好，这些数据可能有些特殊，与维基百科和其他大型基于书籍的数据集上的语言非常不同。列类型分类被用作该建模框架的说明性示例。
- en: 'By way of reminder, the heuristics for fine-tuning in computer vision go roughly
    as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，计算机视觉中微调的启发式大致如下：
- en: A threshold is moved away from the output (and toward the input) as more data
    becomes available in the target domain. Parameters between the threshold and output
    are *unfrozen* and trained while the rest are kept the same. This is motivated
    by the fact that an increased amount of data can be used to train more parameters
    effectively than could be done otherwise.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着目标域中的数据越来越多，阈值从输出中移动（并朝向输入）。在阈值和输出之间的参数被*解冻*并进行训练，而其余参数保持不变。这是由于增加的数据量可以有效地用于训练更多的参数，而否则无法完成。
- en: Additionally, movement of the threshold must happen away from the output and
    toward the input, because this allows us to retain parameters encoding general
    features that are close to the input, while retraining layers closer to the output
    that encode features specific to the source domain.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，阈值的移动必须远离输出并朝向输入，因为这样可以保留编码靠近输入的通用特征的参数，同时重新训练更靠近输出的层，这些层编码源域特定特征。
- en: Moreover, when the source and the target are highly dissimilar, some of the
    more specific parameters/layers can be fully discarded.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，当源和目标高度不同时，一些更具体的参数/层可以完全丢弃。
- en: One major weakness of the early embedding methods such as word2vec was disambiguation—distinguishing
    between various uses of a word that may have different meanings depending on context.
    These words are technically referred to as homographs, for example, duck (posture)
    versus duck (bird) and fair (a gathering) versus fair (just). *Embeddings from
    Language Models*—abbreviated as ELMo after the popular *Sesame Street* character—is
    one of the earliest attempts to develop contextualized embeddings of words, using
    bidirectional *long short-term memory networks* (bi-LSTMs). ELMo is arguably the
    most popular early pretrained language model associated with the ongoing NLP transfer
    learning revolution. It shares a lot of architectural similarities with SIMOn,
    also being composed of character-level CNNs followed by bi-LSTMs. The embedding
    of a word in this model depends on its context, which ELMo achieves by being trained
    to predict the next word in a sequence of words. Huge datasets, such as Wikipedia
    and various datasets of books, were used to train this model. We applied ELMo
    to an illustrative example problem, namely fake news detection, as a practical
    demonstration.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 早期嵌入方法（如word2vec）的一个主要弱点是消歧义 - 区分一个词的各种用法，这些用法根据上下文可能有不同的含义。这些词在技术上被称为同形异义词，例如，duck（姿势）与duck（鸟）和fair（集会）与fair（公平）。*来自语言模型的嵌入*
    - 在流行的*Sesame Street*角色之后缩写为ELMo - 是最早尝试开发单词的上下文化嵌入的方法之一，使用双向*长短期记忆网络*（bi-LSTMs）。ELMo可以说是与正在进行的NLP迁移学习革命相关联的最流行的早期预训练语言模型之一。它与SIMOn具有许多架构相似之处，后者由字符级CNNs和bi-LSTMs组成。这个模型中一个词的嵌入取决于其上下文，ELMo通过被训练来预测单词序列中的下一个单词来实现这一点。大量数据集，如维基百科和各种图书数据集，被用来训练这个模型。我们将ELMo应用于一个说明性的示例问题，即假新闻检测，作为一个实际演示。
- en: '*Universal Language Model Fine-Tuning* (ULMFiT) took this a step further by
    formalizing a method to fine-tune any neural network-based language model for
    any particular task. This framework introduces and demonstrates some key techniques
    and concepts enabling adapting a pretrained language model for new settings more
    effectively. These include discriminative fine-tuning and gradual unfreezing.
    Discriminative fine-tuning stipulates that because the different layers of a language
    model contain different type of information, they should be tuned at different
    rates. Gradual unfreezing describes a procedure for fine-tuning progressively
    more parameters in a gradual manner with the aim of reducing the risks of overfitting.
    The ULMFiT framework also includes innovations in varying the learning rate in
    a unique way during the adaptation process. We illustrated these concepts numerically
    using the fast.ai library.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*通用语言模型微调*（ULMFiT）进一步提出了一种方法，为任何特定任务微调基于神经网络的语言模型。该框架介绍并演示了一些关键的技术和概念，以更有效地适应预训练语言模型的新设置。这些包括区分性微调和渐进解冻。区分性微调规定，因为语言模型的不同层包含不同类型的信息，它们应该以不同的速率进行微调。渐进解冻描述了一种逐渐地以渐进方式微调更多参数的过程，目的是减少过拟合的风险。ULMFiT框架还包括在适应过程中以独特方式改变学习率的创新。我们使用fast.ai库对这些概念进行了数值上的说明。'
- en: The OpenAI *Generative Pretrained Transformer* (GPT) modified the encoder-decoder
    architecture of the transformer to achieve a fine-tunable language model for NLP.
    It discarded the encoders, retaining the decoders and their self-attention sublayers.
    It is trained with a *causal modeling objective*—to predict the next word in a
    sequence. It is particularly suited for text generation. We demonstrated how you
    can quickly use the pretrained GPT-2 model for text generation using the transformers
    library by Hugging Face, which we introduced earlier in the book.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '-   OpenAI的*生成式预训练变压器*（GPT）修改了变压器的编码器-解码器架构，以实现NLP的可微调语言模型。它丢弃了编码器，保留了解码器及其自注意子层。它是以*因果建模目标*进行训练的——预测序列中的下一个词。它特别适用于文本生成。我们展示了如何使用Hugging
    Face的transformers库快速使用预训练的GPT-2模型进行文本生成，该库在本书中早已介绍过。'
- en: Bidirectional Encoder Representations from Transformers (BERT) did arguably
    the opposite, modifying the transformer architecture by preserving the encoders
    and discarding the decoders, also relying on *masking* of words, which would then
    need to be predicted accurately as the training metric. More specifically, it
    is trained with the *masked modeling objective*—to fill in the blanks. Additionally,
    it is trained with the *next sentence prediction* task—to determine whether a
    given sentence is a plausible following sentence after a target sentence. Although
    not suited for text generation, this model performs very well on other general
    language tasks such as classification and question answering. We applied it to
    the two important applications of question answering and document classification.
    The document classification use case was spam detection. We also demonstrated
    its application to filling in the blanks and detecting whether one sentence is
    a plausible next sentence for another one.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '-   从转换器的双向编码器表示（BERT）可以说是相反的，通过保留编码器并丢弃解码器来修改转换器架构，还依赖于*掩码*词语，然后需要准确预测作为训练度量的这些词语。更具体地说，它是以*掩码建模目标*训练的——填补空白。此外，它还通过*下一个句子预测*任务进行训练——确定给定句子是否是目标句子之后的一个合理的跟随句子。虽然不适用于文本生成，但该模型在其他一般语言任务（如分类和问题回答）上表现非常好。我们将其应用于问题回答和文档分类的两个重要应用。文档分类用例是垃圾邮件检测。我们还展示了它在填补空白和检测一个句子是否是另一个句子的合理的下一个句子方面的应用。'
- en: The model mBERT, which stands for “multilingual BERT,” is effectively BERT pretrained
    on over 100 languages simultaneously. Naturally, this model is particularly well-suited
    for cross-lingual transfer learning. We showed how the multilingual pretrained
    weights checkpoint could facilitate creating BERT embeddings for languages that
    were not even originally included in the multilingual training corpus. Both BERT
    and mBERT were created at Google.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '-   mBERT模型，代表“多语言BERT”，实际上是同时在100多种语言上预训练的BERT。自然地，这个模型特别适合跨语言迁移学习。我们展示了多语言预训练权重检查点如何有助于为原本未包含在多语言训练语料库中的语言创建BERT嵌入。BERT和mBERT都是由Google创建的。'
- en: 'In all of these language-model-based methods—ELMo, ULMFiT, GPT, and BERT—it
    was shown that embeddings generated could be fine-tuned for specific downstream
    NLP tasks with relatively few labeled data points. This explains the focus the
    NLP community has paid to language models: it validates the conjecture that the
    hypothesis set induced by them would be generally useful.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在所有这些基于语言模型的方法——ELMo、ULMFiT、GPT和BERT中——都显示出生成的嵌入可以用相对较少的标记数据点进行特定下游NLP任务的微调。这解释了NLP社区对语言模型的关注：它验证了它们诱导的假设集通常是有用的。'
- en: 'We also covered some adaptation strategies for the deep NLP transfer learning
    modeling architectures that were covered. In other words, given a pretrained architecture
    such as ELMo, BERT, or GPT, how can transfer learning be carried out more efficiently?
    We focused on *parameter efficiency* for this purpose, where the goal is to yield
    a model with the fewest parameters possible while suffering minimal reduction
    in performance. The purpose of this is to make the model smaller and easier to
    store, which would make it easier to deploy on smartphone devices, for instance.
    Alternatively, smart adaptation strategies may be required just to get to an acceptable
    level of performance in some difficult transfer cases. The adaptation strategies
    we covered follow:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了一些关于已覆盖的深度NLP迁移学习建模架构的适应策略。换句话说，针对预训练架构，如ELMo、BERT或GPT，如何更有效地进行迁移学习？我们专注于*参数效率*，目标是在尽可能减少参数的情况下产生一个性能损失最小的模型。这样做的目的是让模型更小、更容易存储，从而更容易在智能手机等设备上部署。或者，智能的适应策略可能仅仅是为了在一些困难的转移情况下达到可接受的性能水平。我们介绍的适应策略有：
- en: The first adaptation strategies we explored were the aforementioned ULMFiT techniques
    of *gradual unfreezing* and *discriminative fine-tuning* with the fast.ai library.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探索的第一种适应策略是前面提到的ULMFiT技术，即*逐步解冻*和*区分微调*，使用的是fast.ai库。
- en: We then explored the model compression method known as *knowledge distillation*,
    due to its recent prominence in the NLP field. This process essentially attempts
    to mimic the output from the larger *teacher* model by the significantly smaller
    *student* model. In particular, we use an implementation of the method DistilBERT
    in the transformers library to demonstrate that the size of a BERT model can be
    more than halved by this approach.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们探索了被称为*知识蒸馏*的模型压缩方法，因为它最近在NLP领域显赫一时。这个过程本质上试图通过显著更小的*学生*模型来模拟更大*教师*模型的输出。特别是，我们使用transformers库中知识蒸馏方法DistilBERT的实现来证明通过这种方式可以将BERT模型的大小减少一半以上。
- en: The next adaptation strategy we touched on revolves around two ideas aimed at
    creating transformer-based language models that scale more favorably with a bigger
    vocabulary and longer input length. The first involves clever factorization, or
    splitting up of a larger matrix of weights into two smaller matrices, allowing
    you to increase the dimensions of one without affecting the dimensions of the
    other. The second idea involves sharing parameters across all layers. These two
    strategies are the bedrock of the method known as ALBERT (A Lite BERT). We used
    the implementation in the transformers library to get some hands-on experience
    with the method.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们接触到的下一个适应策略围绕着两个想法，旨在创建更有利于更大词汇量和更长输入长度的基于变压器的语言模型。第一种方法涉及巧妙的因式分解，或者将一个更大的权重矩阵分解为两个较小的矩阵，允许你增加一个维度而不影响另一个维度。第二种想法涉及跨所有层共享参数。这两种策略是ALBERT（A
    Lite BERT）方法的基础。我们使用transformers库中的实现来亲身体验这一方法。
- en: Consequently, we built on the idea of multitask learning, where a model is trained
    to perform a variety of tasks at once and yields a more generalizable model. When
    faced with a transfer scenario where we have insufficient training data to fine-tune
    on a given task, why not fine-tune on multiple tasks? Discussing this idea provided
    a great opportunity to introduce the *General Language Understanding Evaluation*
    (GLUE) dataset, a collection of data for several tasks representative of human
    language reasoning, such as detecting similarity between sentences, similarity
    between questions, paraphrasing, sentiment analysis, and question answering. We
    showed how to quickly leverage the transformers library for multitask fine-tuning
    using this dataset. This exercise also demonstrated how to similarly fine-tune
    a model from the BERT family on a custom dataset from one of these important classes
    of problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们基于多任务学习的想法，即模型被训练以同时执行多种任务，并产生更具有普遍性的模型。当面临转移场景时，我们没有足够的训练数据来在给定任务上进行微调时，为什么不在多个任务上进行微调？讨论这个想法给我们提供了一个很好的机会来介绍*通用语言理解评估*（GLUE）数据集，这是一组代表人类语言推理的几项任务的数据集，例如检测句子之间的相似性，问题之间的相似性，释义，情感分析和问题回答。我们展示了如何使用这个数据集快速利用transformers库进行多任务微调。这个练习还演示了如何类似地在来自这些重要问题类别之一的自定义数据集上微调来自BERT系列的模型。
- en: We also built on the idea of domain adaptation, particularly the fact that the
    similarity of the source and target domains plays a crucial role in the effectiveness
    of transfer learning. Greater similarity implies an easier transfer learning process
    in general. When the source and target are too dissimilar, it may be impossible
    to carry out the process in a single step. In those circumstances, the idea of
    *sequential adaptation* may be used to break the overall desired transfer into
    simpler, more manageable steps. By way of example, we sequentially adapted a “fill-in-the-blanks”
    objective pretrained BERT to a low-resource sentence similarity-detection scenario,
    by first adapting to a data-rich question-similarity scenario. Data for both scenarios
    in the experiment came from the GLUE dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还建立在领域自适应的思想上，特别是源域和目标域的相似性对于迁移学习的有效性起着至关重要的作用。更大的相似性通常意味着一般情况下更容易进行迁移学习。当源域和目标域之间相差太大时，可能无法在单一步骤中执行该过程。在这种情况下，可能会使用*顺序自适应*的想法将所需的整体迁移分解为更简单、更易管理的步骤。举例来说，我们首先将一个“填空”目标预训练的
    BERT 逐步适应到一个低资源的句子相似度检测场景中，再适应到一个数据丰富的问题相似度场景中。实验中两个场景的数据都来自 GLUE 数据集。
- en: The final adaptation strategy we explored was the use of so-called *adaptation
    modules* or *adapters*. These are newly introduced modules of only a few parameters
    between layers of a pretrained neural network. Fine-tuning this modified model
    for new tasks requires training only these few additional parameters. The weights
    of the original network are kept the same. Virtually no loss in performance, compared
    to fine-tuning the entire model, is often observed when adding just 3-4% additional
    parameters per task. These adapters are also modular and easily shared between
    researchers. We used the AdapterHub framework to load some of these adapters and
    show how they can be used to adapt a general BERT model to one expected to do
    well on a sentiment classification task.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨的最终适应策略是使用所谓的*适应模块*或*适配器*。这些是仅在预训练神经网络的层之间具有少量参数的新引入的模块。对于新任务微调这个修改过的模型只需要训练这几个额外的参数。原始网络的权重保持不变。与微调整个模型相比，通常只添加了
    3-4% 的额外参数时往往几乎没有性能损失。这些适配器也是模块化的，并且容易在研究人员之间共享。我们使用了 AdapterHub 框架来加载其中一些适配器，并展示它们如何用于将通用
    BERT 模型适应为在情感分类任务上表现良好的模型。
- en: 11.2 Other emerging research trends
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 其他新兴研究趋势
- en: Throughout the book, we have tried to emphasize that in a fast-moving field
    such as transfer learning for NLP, it would be impossible for a single book like
    this one to fully cover every architecture or innovation. Instead, we have taken
    the approach of focusing on the architectures and techniques we think are fundamental.
    Future innovations are likely to be in some sense derived from such architectures
    and techniques, so readers can possibly pick them up by doing a little bit of
    their own legwork. To facilitate this even further, we focus this section on a
    brief discussion of various research trends we did not get a chance to cover in
    this book but which have become somewhat influential in the field. We frame these
    in the context of what we did cover as much as possible to ease your picking up
    of those topics if needed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们试图强调，在诸如 NLP 的迁移学习这样的快速发展领域中，像这样一本单独的书籍完全涵盖每种架构或创新是不可能的。相反，我们采取的方法是专注于我们认为是基础的架构和技术。未来的创新很可能在某种程度上是从这些架构和技术中派生出来的，因此读者可能通过自己的一些努力来学习它们。为了进一步促进这一点，我们将这一部分重点放在了我们在这本书中没有涵盖但在该领域中已经有些影响的各种研究趋势的简要讨论上。我们尽可能将它们置于我们已经涵盖的内容的背景中，以便您在需要时更轻松地学习这些主题。
- en: We begin the exercise by overviewing RoBERTa[⁷](#pgfId-1247413)—Robustly Optimized
    BERT Approach—which employs some optimization tricks to improve BERT efficiency.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过概述 RoBERTa[⁷](#pgfId-1247413)——Robustly Optimized BERT Approach——来开始这个练习，它采用了一些优化技巧来提高
    BERT 的效率。
- en: 11.2.1 RoBERTa
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 RoBERTa
- en: The study in question attempted to replicate BERT while paying extra attention
    to various training hyperparameters and settings and their effect on the outcome.
    In general, it was observed that the performance of the original BERT could be
    improved significantly via careful design choices. One such choice is removing
    the next-sentence prediction (NSP) task while maintaining the masked language
    modeling (MLM)—fill-in-the-blanks—task. In other words, they found NSP degraded
    performance on downstream tasks and showed that removing it was beneficial. Additional
    design choices included large learning rates and mini-batches during training.
    It is implemented in the transformers library by Hugging Face that we introduced
    earlier in the book.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所讨论的研究试图复制BERT，同时特别关注各种训练超参数和设置以及它们对结果的影响。总的来说，观察到通过谨慎的设计选择可以显著改善原始BERT的性能。这样的选择之一是去除下一句预测（NSP）任务，同时保留掩码语言建模（MLM）—
    填空— 任务。换句话说，他们发现NSP会降低下游任务的性能，并显示去除它是有益的。其他设计选择包括在训练过程中使用较大的学习率和小批量。它是由我们在本书中介绍的Hugging
    Face的transformers库中实现的。
- en: Next, we look at one of the largest language model developed yet—GPT-3[⁸](#pgfId-1247425)—which
    has gathered much buzz lately, culminating in it winning the Best Paper Award
    at the NeurIPS 2020 virtual research conference (December 2020).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看迄今为止开发的最大语言模型之一 — GPT-3，最近引起了很多关注，并在NeurIPS 2020虚拟研究会议（2020年12月）上获得了最佳论文奖。
- en: 11.2.2 GPT-3
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 GPT-3
- en: You may recall from our coverage of it that the GPT model has undergone several
    iterations—GPT, GPT-2, and, more recently, GPT-3\. At the time of writing, GPT-3
    happens to be one of the largest pretrained language models at 175 billion parameters.
    Its predecessor GPT-2 stands at 1.5 billion parameters and was also considered
    the largest when it was released just a year prior. Prior to the release of GPT-3
    in June 2020, the largest model was Microsoft’s Turing NLG, which stands at 17
    billion parameters and was released in February 2020\. The sheer speed of progress
    on some of these metrics has been mind-blowing, and these records tend to be broken
    very quickly. For comparison, this explosion in parameters is illustrated in figure
    11.2.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得我们的报道，GPT模型经历了几次迭代 — GPT、GPT-2，最近是GPT-3。在撰写时，GPT-3恰好是拥有1750亿参数的最大的预训练语言模型之一。它的前身GPT-2拥有15亿参数，在发布时也被认为是最大的，仅仅在前一年发布。在2020年6月发布GPT-3之前，最大的模型是微软的图灵NLG，拥有170亿参数，并于2020年2月发布。这些指标的进展速度之快令人震惊，这些记录往往很快就会被打破。为了比较，这些参数爆炸在图11.2中有所体现。
- en: '![11_02](../Images/11_02.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![11_02](../Images/11_02.png)'
- en: Figure 11.2 The growth in the number of parameters for the largest models over
    time. As can be seen in the diagram, the explosion in the size of the models appears
    to be accelerating, with one of the most recent advances—GPT-3—representing a
    growth factor of 10×.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 模型参数数量随时间增长的趋势。如图所示，模型大小的爆炸趋势似乎在加速，其中最近的一个进步 — GPT-3 — 表示了10倍的增长因子。
- en: As can be seen in the figure, GPT-3 represents an increase of more than 10x
    over the previously largest Turing NLG, a leap eclipsing prior progresses. Indeed,
    an architecture called the Switch Transformer,[⁹](#pgfId-1247441) which leverages
    sparsity by assigning separate transformer block sections to different inputs,
    claimed to have achieved a trillion parameter size in January 2021\. We did not
    include this architecture in figure 11.2 because it is still undergoing peer review
    at the time of writing. It is abundantly clear, however, that this trend of growing
    model sizes appears to be accelerating.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，GPT-3相比之前最大的图灵NLG增长了超过10倍，这是一次超越以往进步的飞跃。实际上，一种称为Switch Transformer的架构，通过为不同的输入分配单独的transformer块部分利用了稀疏性，并声称在2021年1月达到了1万亿参数的规模。由于在撰写时仍在进行同行评审，我们没有在图11.2中包含这种架构。然而，很明显，这种模型大小增长的趋势似乎正在加速。
- en: In the GPT-3 paper, the authors show that this huge model can perform a broad
    range of tasks with very few examples. For instance, it can be primed to translate
    one language to another by seeing only a few example translations or to detect
    spam email by seeing only a few example spam emails. In fact, some unexpected
    applications, such as writing code from a description of it, have been widely
    reported. At the moment, the model has not been released to the public by the
    OpenAI authors, only to a few early adopters by invitation and via a paid API.
    The stated reason by OpenAI for restricting access is monitoring use and thereby
    limiting any potential harmful applications of this technology. An early adopter
    of GPT-3 is an app called Shortly, which provides access to GPT-3 for creative
    writing purposes and which anyone can try for a small fee.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-3 论文中，作者展示了这个巨大的模型可以在很少的样本情况下执行广泛的任务。例如，只需看几个示例翻译，它就可以被启动以将一种语言翻译成另一种语言，或者只需看几个示例垃圾邮件，即可检测垃圾邮件。
    事实上，一些意想不到的应用，例如从编码的描述中编写代码，已经被广泛报道。目前，该模型尚未由 OpenAI 作者发布给公众，只有通过邀请和付费 API 向少数早期采用者提供。
    OpenAI 限制接入的理由是监控使用，从而限制这种技术的任何潜在有害应用。 GPT-3 的早期采用者是一款名为 Shortly 的应用程序，它为创意写作提供了
    GPT-3 访问权限，任何人都可以以少量费用尝试。
- en: 'Additionally, a recent smaller but powerful open source alternative to GPT-3
    is already available in the transformers library: GPT-Neo from EleutherAI. This
    organization aims to build a model equivalent to the full-size GPT-3 and make
    it available to the public under an open license.[^(10)](#pgfId-1247447) Different
    models of varying sizes are available in their repository,[^(11)](#pgfId-1247451)
    where you can also test-run the models using the Hugging Face-hosted inference
    API in the browser. We also provide a companion Kaggle notebook[^(12)](#pgfId-1247455)
    showing it in action for the exercise we performed in chapter 7\. Upon inspection,
    you should find its performance better but, naturally, at a significantly higher
    cost. (The weights are more than 10 GB in size for the largest model!)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个最近更小但功能强大的 GPT-3 开源替代品已经在 transformers 库中可用：EleutherAI 的 GPT-Neo。该组织旨在构建一个与全尺寸
    GPT-3 相当的模型，并在开放许可下向公众提供。他们的存储库中提供了不同大小的模型，您也可以使用 Hugging Face 托管的推理 API，通过浏览器测试这些模型。我们还提供了伴随
    Kaggle 笔记本，演示了我们在第七章中进行的练习的运行情况。通过检查，您应该会发现其性能更好，但自然而然地，成本也更高。 （最大型号的重量超过 10 GB！）
- en: An important thing to note about the GPT-3 work is that the authors themselves
    recognized in the paper that the benefits from making language models bigger has
    approached a limit. And even at that limit, performance on certain types of tasks,
    such as text generation concerning an understanding of common-sense physics, remains
    poor. Thus, although it does represent an important technological breakthrough,
    innovation in modeling approaches (versus simply scaling up models) has to be
    the path forward.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 GPT-3 工作的一个重要事项是，作者本人在论文中认识到，使语言模型更大的好处已接近极限。即便在极限下，处理某些类型的任务（例如关于对常识物理的理解的文本生成）的性能仍然较差。因此，虽然它确实代表了一项重要的技术突破，但在建模方法（而不是简单地扩大模型规模）上的创新必须成为前进的道路。
- en: Next we look at a set of methods aimed at improving the performance of transformer-based
    models on longer input sequences. This is important because vanilla transformer
    models scale quadratically in run time and memory usage with input length.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一组旨在改善基于 transformer 的模型在更长输入序列上的性能的方法。这很重要，因为香草 transformer 模型会随着输入长度呈平方级别的运行时间和内存使用。
- en: 11.2.3 XLNet
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3 XLNet
- en: XLNet,[^(13)](#pgfId-1247463) which builds on a similar earlier model Transformer-XL,[^(14)](#pgfId-1247468)
    was designed to work better for longer input sequences. One of its critical component
    ideas is causal language modeling (CLM), which we discussed when covering GPT
    and which involves the classical language modeling task of predicting the next
    word in a sequence. Recall that future tokens are masked in this approach. The
    authors of the XLNet paper refer to this equivalently as autoregressive language
    modeling. The other critical component of XLNet is performing CLM over all possible
    permutations of the words in the input sequence. This idea is sometimes referred
    to as permutation language modeling (PLM). By combining PLM and CLM, bidirectionality
    is achieved because all tokens get to be included as past tokens in some permutation.
    Both XLNet and Transformer-XL have no sequence length limit and are implemented
    in the transformers library by Hugging Face.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet[^(13)](#pgfId-1247463)是在类似早期模型Transformer-XL[^(14)](#pgfId-1247468)的基础上构建的，旨在更好地处理更长的输入序列。其中一个关键组成部分的想法是因果语言建模（CLM），我们在讨论GPT时已经讨论过，它涉及到预测序列中下一个词的经典语言建模任务。请注意，此方法中的未来标记已被屏蔽。XLNet论文的作者等效地将其称为自回归语言建模。XLNet的另一个关键组成部分是对输入序列的所有可能排列执行CLM。这个想法有时被称为排列语言建模（PLM）。通过结合PLM和CLM，实现了双向性，因为所有标记都有机会在某个排列中作为过去标记被包含。XLNet和Transformer-XL都没有序列长度限制，并且由Hugging
    Face的transformers库实现。
- en: With that view of XLNet in mind, let’s move on to consider BigBird,[^(15)](#pgfId-1247475)
    an innovation introducing the idea of a *sparse attention mechanism* for greater
    computational efficiency.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种对XLNet的看法，让我们继续考虑BigBird[^(15)](#pgfId-1247475)，这是一种引入*稀疏注意机制*的创新，以实现更高的计算效率。
- en: 11.2.4 BigBird
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.4 BigBird
- en: BigBird reduces the quadratic dependency of traditional transformer-based models
    to linear by introducing a sparse attention mechanism that is shown to approximate
    and maintain the properties of the original full attention. Instead of applying
    the full attention to the entire input sequence at once, sparse attention looks
    at the sequence token by token, allowing it to be more intelligent and drop some
    connections. Sequences with a length up to eight times as long as what could be
    handled with traditional transformer-based models can be handled on similar hardware.
    It is implemented in the transformers library by Hugging Face.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: BigBird通过引入一种稀疏注意机制将传统基于Transformer的模型的二次依赖关系减少到线性，该机制被证明能够近似并保持原始完整注意力的性质。与一次应用完整注意力到整个输入序列不同，稀疏注意力逐个查看序列标记，允许它更加智能地且舍弃一些连接。可以处理长达传统基于Transformer模型处理的八倍长度的序列，且可在类似硬件上处理。它是由Hugging
    Face的transformers库实现的。
- en: Next, we touch on Longformer,[^(16)](#pgfId-1247486) another innovation on the
    traditional full self-attention of the transformer that scales better with input
    length.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍Longformer[^(16)](#pgfId-1247486)，这是对Transformer传统完整自注意力的另一项创新，能够更好地随着输入长度的增加而扩展。
- en: 11.2.5 Longformer
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.5 Longformer
- en: Longformer is yet another attempt at battling the quadratic scaling of the traditional
    transformer attention. The innovation here is to combine a local windowed attention
    with a global task-oriented attention. The local attention is used for contextual
    representation whereas the global attention is used to build a full-sequence representation
    that is used in prediction. The scaling achieved is linear in the input sequence
    length, similar to BigBird. Longformer is implemented in the transformers library
    by Hugging Face.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer是针对传统Transformer注意力的二次缩放的又一尝试。这里的创新是将局部窗口注意力与全局任务导向注意力相结合。局部注意力用于上下文表示，而全局注意力用于构建在预测中使用的完整序列表示。所达到的缩放在输入序列长度方面是线性的，类似于BigBird。Longformer是由Hugging
    Face的transformers库实现的。
- en: We introduce the Reformer[^(17)](#pgfId-1247496) next, which is another approach
    to alleviating the quadratic scaling of the original self-attention.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来介绍Reformer[^(17)](#pgfId-1247496)，这是另一种减轻原始自注意力二次缩放的方法。
- en: 11.2.6 Reformer
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.6 Reformer
- en: The Reformer introduces two techniques for combating the quadratic scaling in
    computing time and memory with the input length of the original transformer. Replacing
    the original full self-attention with one that employs locally sensitive hashing
    reduces redundant computations and time complexity from quadratic to O(L*log*L)
    (where L is the input sequence length). A technique known as *reversible layers*
    allows storing activations only once. In practice, this means that instead of
    storing activations *N* times for a model with *N* layers, only a small fraction
    of the memory is used. Depending on the value of *N*, the memory savings can be
    huge. Reformer is implemented in the transformers library by Hugging Face.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Reformer引入了两种技术来对抗原始Transformer输入长度的二次扩展的计算时间和内存消耗。用局部敏感哈希替换原始的完全自注意力机制，减少了冗余计算和时间复杂度从二次到O(L*log*L)（其中L是输入序列长度）。一种称为*可逆层*的技术允许只存储激活一次。在实践中，这意味着，与为具有*N*层的模型存储激活*N*次相比，只使用了一小部分内存。根据*N*的值，内存节省可能非常大。Reformer是由Hugging
    Face实现的transformers库中的一个模型。
- en: Clearly, making transformer-based models work better with longer input lengths
    has emerged as a meta research trend. We have likely not included all the great
    research on the subject here, and you will likely find much more if you do some
    of your own digging.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，使基于Transformer的模型在处理更长的输入长度时表现更好已成为一个元研究趋势。我们在这里可能没有包括所有关于此主题的重要研究，如果你自己深入研究，可能会发现更多。
- en: Next, we will touch on recently reemerging sequence-to-sequence modeling approaches.
    These attempt to cast the various problems we have encountered in this book into
    a text-to-text modeling framework.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将谈论最近重新出现的序列到序列建模方法。这些尝试将本书中遇到的各种问题统一到一个文本到文本建模框架中。
- en: 11.2.7 T5
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.7 T5
- en: You may recall in our discussion in this book that sequence-to-sequence models
    have played an important role in NLP. First appearing in the context of recurrent
    neural network (RNN) models, they were also explored notably by the translation
    application area in the context of the original transformer architecture. T5,
    the “Text-to-Text Transfer Transformer,”[^(18)](#pgfId-1247518) is an attempt
    to cast a broad range of NLP problems into a unifying sequence-to-sequence framework.
    It allows for the application of the same model, objective, training procedure,
    and decoding process for every task. Handled problem classes vary from summarization
    to sentiment analysis and question answering, among many others. Translation for
    the language pairs between English and each of Romanian, German, and French is
    included in training. Some representative data transformations, which make it
    possible to train a single model on diverse tasks, are shown in figure 11.3 for
    translation and summarization (it is inspired by figure 1 from the T5 paper).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得在本书中我们讨论过，序列到序列模型在自然语言处理中发挥了重要作用。首次出现在循环神经网络（RNN）模型的背景下，它们也被翻译应用领域在原始Transformer架构的背景下所探索。T5，“文本到文本转换Transformer”，是将各种自然语言处理问题统一到一个序列到序列框架中的尝试。它允许对每个任务应用相同的模型、目标、训练过程和解码过程。处理的问题类别包括从摘要到情感分析和问答等众多领域。英语和罗马尼亚语、德语、法语之间的语言对翻译被包括在训练中。一些代表性的数据转换，使得可以在多种任务上训练单一模型，如图11.3所示的翻译和摘要（灵感来源于T5论文的图1）。
- en: '![11_03](../Images/11_03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![11_03](../Images/11_03.png)'
- en: Figure 11.3 T5 is a sequence-to-sequence model that employs a number of transformations
    to enable a single model, decoding procedure, and objective to be trained on a
    variety of tasks simultaneously. It can be thought of as an interesting variation
    of multitask learning.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 T5是一个序列到序列模型，它采用了一系列转换，使得可以同时在各种任务上训练单一模型、解码过程和目标。它可以被看作是多任务学习的一个有趣变体。
- en: As can be seen in the figure, task data is transformed by prepending a standard
    task descriptor to the original text data. Training data included the datasets
    GLUE and SuperGLUE, CNN/Daily Mail dataset for abstractive summarization, and
    so on. The goal was to handle the included diverse set of natural language understanding
    tasks without modifying the model. In that sense, it can be thought of as an interesting
    variant or iteration of the multitask learning idea that we have been bringing
    up throughout the book. The inclusion of such a variety of tasks for simultaneous
    learning is likely to enable parameter sharing and better generalizability of
    the resulting model. Crucially, the model is initially trained on a dataset the
    authors call the Colossal Clean Crawled Corpus (C4) using a masked language modeling
    or autoencoding objective, before fine-tuning on the aforementioned variety of
    tasks. Basically, 15% of all tokens are dropped, and the result is fed to the
    input, while the uncorrupted input is fed to the output for prediction. Note that
    the C4 corpus is essentially the internet for the target language (English) with
    pornographic material, code, and other “garbage data” filtered out. The model
    architecture used for training is similar to the transformer architecture we used
    in chapter 7 for translation. State-of-the-art results were achieved by the resulting
    model on many of the included tasks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，任务数据通过在原始文本数据前加上标准任务描述符来转换。训练数据包括 GLUE 和 SuperGLUE 数据集、用于抽象摘要的 CNN/Daily
    Mail 数据集等。目标是处理包含的多样化的自然语言理解任务，而不修改模型。从这个意义上说，它可以被看作是我们在整本书中一直提到的多任务学习思想的一个有趣变体或迭代。同时学习如此多种任务的包含可能会实现参数共享和生成模型的更好泛化能力。关键是，模型最初是在作者称之为“巨大干净爬行语料库”（C4）的数据集上使用蒙版语言建模或自动编码目标进行训练，然后在上述各种任务上进行微调。基本上，所有标记的
    15% 被丢弃，结果被送入输入，而未损坏的输入被送入输出以进行预测。请注意，C4 语料库本质上是目标语言（英语）的互联网，其中过滤掉了色情材料、代码和其他“垃圾数据”。用于训练的模型架构类似于我们在第
    7 章中用于翻译的转换器架构。由所得模型在许多包含的任务上实现了最先进的结果。
- en: In addition to the original T5 model, a multilingual version, unsurprisingly
    called mT5, was also developed[^(19)](#pgfId-1247530) by training on 101 languages
    simultaneously. Both T5 and mT5 are implemented in the transformers library by
    Hugging Face.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始的 T5 模型之外，还开发了一个多语言版本，不足为奇地称为 mT5，通过同时对 101 种语言进行训练。T5 和 mT5 都在 Hugging
    Face 的 transformers 库中实现。
- en: In what follows, we briefly introduce BART, which is similar to T5 in that it
    is a transformer-based sequence-to-sequence modeling framework.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们简要介绍 BART，它与 T5 类似，都是基于转换器的序列到序列建模框架。
- en: 11.2.8 BART
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.8 BART
- en: BART,[^(20)](#pgfId-1247537) which stands for Bidirectional and Autoregressive
    Transformers, can be thought of as T5 minus the single unifying transformation
    to enable an unmodified model to be applied to a variety of tasks. Instead, a
    standard transformer encoder-decoder architecture is first pretrained to reproduce
    corrupted input via a variety of noising approaches. This ranges from masked language
    modeling, as in BERT and T5, to permutation language modeling as in XLNet, among
    others. The model is then modified for a variety of tasks, such as SQuAD, summarization,
    and so on, and fine-tuned separately for each such task similar to what we did
    with the traditional BERT. This model performs particularly well in language-generation
    tasks, such as summarization, translation, and dialogue. A multilingual version—mBART[^(21)](#pgfId-1247543)—obtained
    by training on 25 languages simultaneously, has also been developed. Both BART
    and mBART are also implemented in the transformers library by Hugging Face.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: BART，即双向自回归转换器，可以被视为 T5 减去单一统一变换的模型，以使未经修改的模型能够应用于各种任务。相反，首先对标准的转换器编码器-解码器架构进行预训练，以通过各种噪声方法重现损坏的输入。这包括蒙版语言建模，如
    BERT 和 T5，以及排列语言建模，如 XLNet 等。然后，该模型被修改用于各种任务，例如 SQuAD、摘要等，并针对每个任务分别进行微调，类似于我们对传统
    BERT 所做的操作。这个模型在语言生成任务中表现特别好，例如摘要、翻译和对话。同时也开发了一个多语言版本 mBART，通过同时对 25 种语言进行训练而获得。BART
    和 mBART 都在 Hugging Face 的 transformers 库中实现。
- en: In the following subsection, we look at a recent cross-lingual model that goes
    beyond merely training on multiple languages simultaneously, by modeling cross-lingual
    transfer explicitly via a modified language modeling objective when parallel data
    is available.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个小节中，我们将审视一种最新的跨语言模型，它不仅仅是同时在多种语言上进行训练，还通过在有并行数据时修改语言建模目标来显式建模跨语言转移。
- en: 11.2.9 XLM
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.9 XLM
- en: XLM,[^(22)](#pgfId-1247551) which the authors use to mean a “cross-lingual language
    model,” is a modeling framework that combines both monolingual and parallel data
    cross-lingual learning approaches. Monolingual embeddings learned on different
    languages can be *aligned* using a small vocabulary of words with known numerical
    representation. If parallel data is available, the authors introduce an approach
    they call *translation language modeling* (TLM) and exploit it for cross-lingual
    learning simultaneously. Essentially, this applies masked language modeling to
    a concatenated sequence of parallel data in both languages, with words dropped
    and asked to be predicted in both parts of the concatenated sequence.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: XLM,[^(22)](#pgfId-1247551) 其作者用来指代“跨语言语言模型”，是一个结合单语和并行数据的跨语言学习方法的建模框架。在不同语言上学习得到的单语嵌入可以使用已知数值表示的小词汇表进行*对齐*。如果有并行数据可用，作者提出了一种他们称之为*翻译语言建模*（TLM）的方法，并同时利用它进行跨语言学习。本质上，这涉及将并行数据的连接序列应用掩码语言建模，在两种语言的连接序列的各个部分中，让一些单词消失并预测它们。
- en: Significant improvements were observed in cross-lingual learning tasks. It also
    spurred a number of similar models, notably XLM-R,[^(23)](#pgfId-1247559) which
    combines the ideas of XLM with those of RoBERTa for improved performance. Both
    XLM and XLM-R are implemented in the transformers library by Hugging Face.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨语言学习任务中观察到显著的改进。它还激发了许多类似模型的产生，特别是XLM-R,[^(23)](#pgfId-1247559) 它将XLM的思想与RoBERTa的思想结合起来，以提高性能。XLM和XLM-R都在
    Hugging Face 的 transformers 库中实现。
- en: Finally, we briefly touch on a model specializing in an important class of problem
    data we encountered in the book—tabular data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要谈到了一种在书中遇到的重要问题数据类别——表格数据的专门模型。
- en: 11.2.10 TAPAS
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.10 TAPAS
- en: In chapters 5 and 6, we discussed the method SIMOn and its handling of tabular
    data type classification—an important class of problem data typically encountered
    by data scientists. TAPAS[^(24)](#pgfId-1247570) is an attempt to extend the modeling
    benefits of transformer-based models to this important class of problems, by modeling
    and specializing for question answering in tabular data explicitly. TAPAS stands
    for Table Parser. In chapter 8, we discussed applying BERT to the task of question
    answering. The output of the resulting specialized model was the beginning and
    end positions of a potential answer to the question of interest in the input context
    paragraph. In addition to this, TAPAS learns to detect which cell in a table might
    contain the context paragraph from which the answer can be extracted with similar
    start and end indices. Like most of the other models discussed in this section,
    this model is implemented in the transformers library by Hugging Face.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章和第6章中，我们讨论了SIMOn方法及其对表格数据类型分类的处理——这是数据科学家通常会遇到的一个重要问题类别。TAPAS[^(24)](#pgfId-1247570)
    是尝试将基于变换器的模型的建模优势扩展到这一重要问题类别的一种尝试，通过显式地为表格数据中的问答建模和专门化。TAPAS 代表表格解析器。在第8章中，我们讨论了将BERT应用于问答任务。结果专门化模型的输出是输入上下文段落中感兴趣问题的潜在答案的开始和结束位置。除此之外，TAPAS
    还学会检测表格中哪个单元可能包含可以从中提取答案的上下文段落，且起始和结束索引类似。与本节中讨论的其他大多数模型一样，该模型在 Hugging Face 的
    transformers 库中实现。
- en: This brings us to the end of this overview tour of recent work that we have
    not had an opportunity to analyze in detail in this book. Most of these model
    architectures can be used via code very similar to what we have used to work with
    BERT and DistilBERT in the transformers library.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对本书中尚未有机会详细分析的近期工作的概述之旅的结束。这些模型架构大多可以通过与我们在 transformers 库中使用BERT和DistilBERT非常相似的代码来使用。
- en: In the next section, we try to make an educated guess on where we can expect
    the field to move next—what sorts of topics might remain and/or become popular
    given current and emerging research trends.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将尝试对这个领域下一步可能的走向做出有根据的猜测——鉴于当前和新兴的研究趋势，哪些主题可能保持或变得流行。
- en: 11.3 Future of transfer learning in NLP
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 NLP中转移学习的未来
- en: In this section, we attempt to extrapolate the trends described in the previous
    two sections by anticipating what the immediate future of the field is likely
    to look like.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们尝试通过预测该领域即将出现的形态来推断前面两节描述的趋势。
- en: Critical analysis of the past two sections reveals two arguably orthogonal meta-trends—a
    push to make the models as large as possible, as well as a push to develop more
    efficient versions of larger models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对过去两节的批判性分析揭示了两个可以说是正交的元趋势——一个是将模型尽可能地变大，另一个是开发更高效的更大模型的推动。
- en: GPT-3, one of the largest leaps in the number of parameters we have observed
    yet—10 times—initially raised some concerns among researchers that top research
    companies would begin to focus on size over clever modeling. However, as we discussed
    in the previous section, the limitations of scaling up models quickly became apparent,
    with the authors of the GPT-3 paper admitting to limits that have likely been
    reached. Given that GPT-3 is currently available only via a limited paid API,
    we can expect the other players in the space to attempt to build even larger models
    soon as they have a monetary incentive to do so (we already mentioned the trillion-parameter
    Switch Transformer undergoing peer review). This race will likely culminate in
    a similar model from Google and/or Facebook being released, which will likely
    push GPT-3 to be fully open sourced (a similar scenario historically played out
    with GPT-2). Beyond that, we expect more resources to begin to be dedicated to
    more efficient methods of achieving similar performance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3，目前我们观察到的参数数量迈出的最大一步——是以前的10倍，最初引起了一些研究人员对研究公司开始将重点放在规模而不是巧妙建模上的担忧。然而，正如我们在上一节讨论的那样，扩大模型的局限性很快变得明显，GPT-3论文的作者们承认已经达到了可能的极限。考虑到GPT-3目前仅通过有限的付费API可用，我们可以预期空间中的其他参与者将尝试很快构建更大的模型，因为他们有货币激励这样做（我们已经提到正在进行同行评审的拥有万亿参数的Switch
    Transformer）。这场竞赛很可能最终将导致谷歌和/或Facebook发布一个类似的模型，这很可能会推动GPT-3完全开源（类似的情况在GPT-2历史上已经发生过）。除此之外，我们预计会有更多资源开始致力于实现类似性能的更有效方法。
- en: Most of the interesting problems remaining in the immediate future of NLP transfer
    learning likely lie in the movement some have termed *TinyML*. This can be defined
    as a general goal to shrink the size of models so that they can fit on smaller
    hardware. We saw an instance of this when we demonstrated that the size of BERT
    can be roughly halved with little loss in performance via an approach such as
    DistilBERT in chapter 9\. Another approach that accomplished this for us was ALBERT
    in chapter 10, achieving a 90% reduction in model size. A large fraction of the
    world’s population now owns a smartphone, which can run these smaller versions
    of cutting-edge models. The opportunities this opens for a field such as the Internet
    of Things (IoT), where devices form smart networks with each node executing complex
    functions independently, cannot be overstated. Although many phone apps featuring
    translation and other tools today probably feature a server backend where the
    actual translation and other computation happens, the ability to run such algorithms
    locally on smartphone devices without an internet connection is becoming a more
    feasible and prevalent paradigm. We expect the drive to make models such as BERT
    and its derivatives smaller and more parameter-efficient to continue at a fever
    pitch over the next couple years.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP迁移学习的即将到来的有趣问题中，大多数可能都在被一些人称为*TinyML*的运动中。这可以被定义为一个将模型大小缩小到可以适应较小硬件的通用目标。我们在第9章演示了一个例子，即通过DistilBERT等方法可以将BERT的大小大致缩小一半，而性能损失很小。我们在第10章取得了类似的成就的另一种方法是ALBERT，它实现了模型大小的90%缩减。现在全球大部分人口都拥有智能手机，可以运行这些先进模型的较小版本。这给物联网（IoT）等领域带来的机会是巨大的，其中设备形成智能网络，每个节点都能独立地执行复杂功能。尽管今天许多手机应用程序可能都包含翻译和其他工具的服务器后端，用于进行实际的翻译和其他计算，但在没有互联网连接的情况下在智能手机上本地运行这些算法的能力正在成为更可行和更普遍的范式。我们预计在未来几年内，在使BERT及其衍生产品更小和更具参数效率的努力将继续如火如荼。
- en: Another trend you may have picked up from the previous section is the increasing
    focus on cross-lingual models. In fact, the past year has seen an increase in
    global investment in methods for so-called “low-resource” languages. We alluded
    to this via an example in chapter 7, when we used a transformer architecture to
    translate a low-resource West African language, Twi, into English. Many popular
    economic models project that an increasingly important class of consumers is emerging
    in the African market, which is likely at least one driver behind sudden interest
    and investment in this space. For many low-resource languages, the initial barrier
    to the applicability of all the methods that we discussed tends to be data availability.
    Therefore, we can expect appropriate multilingual data development to receive
    much attention over the upcoming year or so, followed by intense research into
    language-specific methodology enhancements in the subsequent years. Notable places
    to keep an eye on for these developments, specifically as they pertain to African
    languages, include NLP Ghana,[^(25)](#pgfId-1247586) Masakhane,[^(26)](#pgfId-1247589)
    EthioNLP,[^(27)](#pgfId-1247592) Zindi Africa,[^(28)](#pgfId-1247595) AfricaNLP,[^(29)](#pgfId-1247598)
    and Black in AI.[^(30)](#pgfId-1247601)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个你可能从前一节中注意到的趋势是对跨语言模型的日益关注。事实上，过去一年全球对所谓的“低资源”语言的方法的投资有所增加。我们在第7章中通过一个例子提到了这一点，当时我们使用了一个变压器架构将低资源的西非语言特威（Twi）翻译成了英语。许多流行的经济模型预测，非洲市场正在出现一个日益重要的消费者群体，这很可能是引起对这一领域突然兴趣和投资的至少一个推动因素。对于许多低资源语言来说，应用我们讨论的所有方法的初始障碍往往是数据的可用性。因此，我们可以预期在接下来的一年左右，适当的多语言数据开发将受到很多关注，随后将进行对语言特定方法的深入研究。值得关注的地方，特别是涉及非洲语言的地方，包括
    NLP Ghana，Masakhane，EthioNLP，Zindi Africa，AfricaNLP 和 Black in AI。
- en: 'Speech is another NLP research frontier that is poised for a watershed moment.
    Until recently, automatic speech recognition models, which transcribe speech into
    text, required many hours of parallel speech-text data to achieve good results.
    A recent architecture, Wav2Vec2[^(31)](#pgfId-1247605) from Facebook, demonstrated
    that pretraining on speech in many languages simultaneously dramatically reduced
    how much parallel data is required. This is similar to the functionality of mBERT
    we explored in this book for text. The Wav2Vec2 model is available in the transformers
    library and can be fine-tuned on new languages with just a few hours of annotated
    speech data. We expect this to spur the development of speech-recognition tools
    for many languages for the first time over the next year. Moreover, we anticipate
    that something similar is on the horizon for the reverse direction: text-to-speech,
    that is, the generation of speech from text.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 语音是另一个即将迎来转折时刻的NLP研究前沿。直到最近，自动语音识别模型，将语音转录成文本，需要大量的平行语音文本数据才能取得良好的结果。Facebook
    最近的一种架构 Wav2Vec2 展示了，同时在许多语言上对语音进行预训练可以极大地减少所需的平行数据量。这类似于我们在本书中探讨的文本中使用的 mBERT
    的功能。Wav2Vec2 模型已经在 transformers 库中提供，并且可以通过只使用几个小时的标注语音数据对新语言进行微调。我们预计这将在接下来的一年内首次推动多种语言的语音识别工具的开发。此外，我们预计类似的事情也将在不久的将来出现在另一个方向上：文本到语音，也就是从文本生成语音。
- en: In chapter 1, we described how transfer learning in NLP was inspired by its
    advances in computer vision. Interestingly enough, recent advances in NLP transfer
    learning seem to be inspiring further advances in computer vision. One specific
    example is DALL-E,[^(32)](#pgfId-1247609) a version of GPT-3 trained on text description-image
    pairs, which has learned to generate images from text prompts. A wider trend is
    to build contextual scene-based object embeddings,[^(33)](#pgfId-1247612) which
    try to predict missing objects from other observable objects in the scene, analogously
    to the fill-in-the-blanks objective for words utilized by BERT and similar masked
    language models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们描述了NLP中的迁移学习是如何受到计算机视觉的进展的启发的。有趣的是，最近NLP迁移学习的进展似乎又激发了计算机视觉的进一步发展。一个具体的例子是
    DALL-E，这是一个在文本描述 - 图像对上训练的 GPT-3 的版本，它已经学会了从文本提示中生成图像。一个更广泛的趋势是构建上下文情景的对象嵌入，试图从场景中其他可观察到的对象预测缺失的对象，类似于
    BERT 和类似的遮蔽语言模型所使用的填空词目标。
- en: 'Another research question that seems to be gaining more attention recently
    is this: What are the environmental and ethical impacts of these models? At the
    outset of the recent research spike, researchers seemed content with releasing
    models that improved technical metrics only, but over time, the field has come
    to value detailed explorations of any potential ethical impacts. Of related increased
    interest are questions around explainability: Can we actually explain how a model
    came to its decisions, so we know for sure that it is not discriminating? We dive
    into these ethical questions further in the next section.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个最近似乎越来越受到关注的研究问题是：这些模型的环境和道德影响是什么？在最近的研究高潮初期，研究人员似乎满足于发布只改善技术指标的模型，但随着时间的推移，这个领域开始重视对潜在道德影响的详细探讨。与之相关的是对可解释性的提高兴趣：我们能否真正解释模型是如何做出决定的，以确保它不会歧视？我们将在下一节进一步探讨这些道德问题。
- en: 11.4 Ethical and environmental considerations
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 道德和环境考虑
- en: You may recall that when we looked at the problem of fake news detection in
    chapters 5 and 6, we raised the point that what can be called fake is debatable.
    If care is not taken with the quality of the data labels, the biases ingrained
    in whoever prepared the labels for the training data will likely just transfer
    to the classification system. This was our first encounter with the important
    need to be fully aware of any potential limitations of these models before deploying
    them in circumstances which can significantly impact human lives.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们在第5章和第6章讨论关于假新闻检测的问题时提出了所谓的假新闻是什么是一个有争议的观点。如果在数据标签的质量上不加注意，那么准备训练数据标签的人身上植入的偏见很可能会转移到分类系统上。这是我们首次遭遇到了在部署这些模型到可能会显著影响人类生活的情况下，充分意识到潜在局限性的重要性。
- en: When we fine-tuned mBERT on the JW300 dataset prepared by Jehovah’s Witnesses
    in chapter 8, we found that it filled in the blanks in a biased way. When we tried
    to predict a basic noun “school,” it would offer words such as Eden as plausible
    completions. This indicated strong religious bias, and it was the second time
    during our journey that we were alerted of the fact that blindly applying these
    models to some data can have biased and unforeseen outcomes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第8章用Jehovah's Witnesses准备的JW300数据集对mBERT进行微调时，我们发现它会以有偏见的方式填补空白。当我们试图预测一个基本名词“学校”时，它会提供像伊甸园这样的词作为合理的补充。这表明了强烈的宗教偏见，这是我们在旅程中第二次被提醒到，盲目地将这些模型应用于某些数据可能会产生有偏见和意想不到的结果。
- en: In this section, we will discuss this on a broader level, considering potential
    ethical and environmental considerations that should probably be kept at the back
    of the mind of any practitioner working on deploying these models. This is a topic
    that is receiving increasing attention recently, but it is far from new in the
    machine learning field in general.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将从更广泛的角度讨论这个问题，考虑到可能需要放在从业者脑后的道德和环境考虑因素。这是一个近来受到越来越多关注的话题，但在机器学习领域并不算新鲜。
- en: Early high-profile machine learning studies of bias predictably happened in
    computer vision. The landmark work, “Gender Shades,”[^(34)](#pgfId-1247623) studied
    the accuracy of commercial gender-classification systems along racial and gender
    dimensions. It found that these systems underperformed for dark-skinned women
    as compared to lighter-skinned males by up to 35 absolute percentage points. This
    has an immense practical impact on minority communities, which may be policed
    by some of these automatic computer vision systems in some regions. An incorrect
    classification or detection can mean wrongful arrest, which even if cleared can
    mean job loss in the most vulnerable communities. There have been multiple widespread
    reports of this happening to real people. A cynical power imbalance was uncovered
    behind systems parading as “objective” and “scientific,” where the richer communities
    where these systems were developed and their economic benefits were mostly reaped
    did not suffer the harm inflicted on the poorer communities. The impact of this
    work and related studies was significant, with the US Congress recently taking
    up related mitigating regulation arguably as a direct consequence. Companies such
    as IBM and Amazon have also been forced to review how they share these technologies
    with law enforcement authorities, with IBM completely discontinuing the service.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 早期关于偏见的知名机器学习研究可预见地发生在计算机视觉领域。具有里程碑意义的作品，“性别和肤色”，[^(34)](#pgfId-1247623)研究了商业性别分类系统在种族和性别维度上的准确性。它发现，与较浅肤色的男性相比，这些系统在较深肤色的女性身上表现不佳，绝对百分比高达35个百分点。这对少数族裔社区有着巨大的实际影响，在一些地区可能会受到一些自动计算机视觉系统的监控。错误的分类或检测可能意味着错误的逮捕，即使被清除，也可能意味着在最脆弱的社区中失去工作。有多次广泛报道这种情况发生在真实的人身上。一个愤世嫉俗的力量失衡在那些打着“客观”和“科学”旗号的系统背后被揭露出来，在这些系统被开发的更富裕的社区和它们的经济利益主要是被搬走的地方并没有遭受到对较贫困的社区造成的伤害。这项工作及相关研究的影响是巨大的，最近美国国会最近出台了相关的减轻监管措施，可以说这是直接后果。像IBM和亚马逊这样的公司也被迫重新审视他们与执法机构分享这些技术的方式，IBM甚至完全停止了这项服务。
- en: Concerns about bias of pretrained NLP language models has also been high recently.
    In fact, the GPT-3 paper[^(35)](#pgfId-1247627) includes a dedicated section for
    a study along several dimensions, namely race, sex, and religion. It has become
    more and more common to see academic articles do this recently, which is quite
    encouraging to see. The GPT-3 study in particular probes the association the model
    learned from the training data for the various dimensions of interest. For instance,
    they discovered that occupations usually associated with a higher level of education
    were more closely associated with male pronouns when filling in the blanks. Similarly,
    prompts implying professional competence were more likely to be completed by male
    pronouns and specifiers. This is likely gender bias directly learned by the model
    from the internet, which we probably can’t expect to be an unbiased source. On
    the other hand, positive descriptors were assigned to nouns primed with “Asian”
    and “White” at a significantly higher rate than for “Black” personas. Again, a
    racial bias was clearly learned by the model from the internet, and blind application
    of the model would simply propagate this bias. On the religious dimension, the
    word “Islam” was associated with the word “terrorism” among the most likely completions.
    As a direct real-world impact of this bias, consider the case of the Palestinian
    man whose benign “good morning” Facebook post was incorrectly translated as “attack
    them” and led to significant unfair consequences.[^(36)](#pgfId-1247630)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最近对预训练的自然语言处理语言模型存在偏见的担忧也很高。实际上，GPT-3论文[^(35)](#pgfId-1247627)专门包括了一个研究的部分，涵盖了几个方面，即种族、性别和宗教。最近越来越常见地看到学术文章做这样的研究，这是非常令人鼓舞的。特别是GPT-3的研究探讨了模型从训练数据中学到的与各个关注维度的关联。例如，他们发现通常与较高教育水平相关联的职业在填空时更倾向于与男性代词相关联。同样，暗示专业能力的提示更有可能由男性代词和说明者完成。这很可能是模型直接从互联网学到的性别偏见，我们可能无法指望互联网是一个无偏见的信息来源。另一方面，积极的描述词更有可能被“亚洲”和“白人”等词引导的名词以比“黑人”人物高得多的速度分配。同样，模型显然从互联网学到了种族偏见，而对模型的盲目应用只会传播这种偏见。在宗教维度上，“伊斯兰”一词与“恐怖主义”一词相关联，是最有可能的完成之一。作为这种偏见的直接现实影响，考虑一下那位巴勒斯坦人的善意“早上好”Facebook帖子被错误地翻译为“攻击他们”，并导致了重大的不公平后果。[^(36)](#pgfId-1247630)
- en: Another way pretrained NLP language models could inadvertently affect poor communities
    is via climate change. In fact, these models have recently been shown to have
    quite the carbon footprint.[^(37)](#pgfId-1247634), [^(38)](#pgfId-1247637) Although
    the carbon footprint of training a single BERT model once was found to be equivalent
    to a single average roundtrip flight between New York and San Francisco, during
    fine-tuning and hyperparameter optimization, the model is actually trained many
    times. If the model was to be deployed via *neural architecture search*, where
    various architecture hyperparameters are exhaustively varied and the best-performing
    model selected, the researchers found a single model deployment to cost the footprint
    of up to five average cars over their entire lifetime. Again, this is serious
    and particularly egregious, because the effects of climate change, directly linked
    to these carbon footprints, hit hardest in the poorest communities, which do not
    experience the direct benefits of these models. It is clear that these costs need
    to be taken into account when evaluating these models. This realization is arguably
    one of the forces driving the field toward more parameter-efficient models.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的自然语言处理语言模型可能无意中影响贫困社区的另一种方式是通过气候变化。实际上，这些模型最近被发现具有相当大的碳足迹。[^(37)](#pgfId-1247634),
    [^(38)](#pgfId-1247637)虽然发现单个 BERT 模型的一次训练的碳足迹相当于纽约到旧金山的一个普通往返航班，但在微调和超参数优化期间，模型实际上会进行多次训练。如果模型通过
    *神经架构搜索* 部署，其中各种架构超参数被详尽地变化，然后选择性能最佳的模型，研究人员发现单个模型部署的碳足迹相当于五辆普通汽车的寿命。再次强调，这是严重的，特别是因为与这些碳足迹直接相关的气候变化影响最严重的是贫困社区，而这些社区并没有体验到这些模型的直接好处。很明显，在评估这些模型时，这些成本需要纳入考虑。这一认识可以说是驱使该领域朝着更具参数效率的模型发展的力量之一。
- en: 'A lingering criticism of pretrained language models, and deep learning in general,
    is that the models tend to be not very *explainable*—it is hard to explain how
    a model arrived at its predictions for any particular scenario. This is related
    to the earlier bias discussion we had in this section—having the model explain
    how it arrived at associations related to education, for instance, could help
    detect if such a decision was made based on the race or sex variable. Most notable
    recent approaches at achieving this, such as bertviz,[^(39)](#pgfId-1247642) try
    to build on the attention visualization we explored in chapter 7\. This still
    does not address the lack of training data transparency question that remains:
    the training of language models takes place at such a large data scale that it
    is virtually impossible for researchers to ensure that it is unbiased. Therefore,
    we expect to see an investment of time and effort into the development of methods
    that could perform comparably from significantly smaller, well-curated datasets.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型及深度学习普遍存在的一个长期批评是，这些模型往往不太*可解释*——很难解释模型是如何在特定情景下得出预测的。这与本节早些时候讨论的偏见问题有关——让模型解释其对教育相关联的决策是如何做出的，例如，可以帮助检测这样的决定是否基于种族或性别变量。最值得注意的最近的方法之一是
    bertviz，[^(39)](#pgfId-1247642)试图在第7章中探索的注意力可视化基础上进行改进。然而，这仍然没有解决训练数据透明度缺失的问题：语言模型的训练规模如此之大，以至于研究人员几乎无法确保其是无偏的。因此，我们期望看到人们投入时间和精力开发出可以从更小、精心策划的数据集中执行相当的方法。
- en: With our brief discussion of some ethical issues that should be kept in mind
    complete, we provide some tips in the next section on how to stay up-to-date in
    this fast-moving field.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们对一些应该记住的伦理问题的简要讨论完成后，我们在下一节中提供一些关于如何在这个快速发展的领域中保持时效性的建议。
- en: 11.5 Staying up-to-date
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 保持时效性
- en: As we have stressed throughout this chapter, the state of transfer learning
    methods in NLP updates very quickly. The material covered in this book should
    be viewed only as a platform from which to continue keeping yourself updated on
    latest developments. In this section, we present a few basic tips on how one might
    achieve this. To summarize, participating in various relevant competitions on
    the Kaggle and/or Zindi platforms can be a good way to work on realistic, yet
    sufficiently clean, trending relevant data and problems. Keeping track of latest
    papers on *arXiv* is a must, and although news and social media coverage can be
    sensationalist and otherwise unreliable, it can still help one spot impactful
    papers early on.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中一再强调的那样，NLP 中的迁移学习方法的状况更新速度很快。本书涵盖的材料应该仅被视为一个平台，用于继续跟踪最新发展。在本节中，我们提供了一些关于如何实现这一目标的基本提示。总的来说，在
    Kaggle 和/或 Zindi 平台上参加各种相关竞赛可能是一个处理现实情况的好方法，但数据足够干净且与时俱进。跟踪 *arXiv* 上的最新论文是必不可少的，尽管新闻和社交媒体的报道可能夸张并且不可靠，但它仍然有助于及早发现有影响力的论文。
- en: 11.5.1 Kaggle and Zindi competitions
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 Kaggle 和 Zindi 竞赛
- en: Throughout the book, we have encouraged you to use Kaggle to run the various
    code presented. Although the free GPU compute provided by the platform and ease
    of setup were immediately stated benefits, the biggest benefit may not have been
    explicitly stated until now. Arguably the most powerful aspect of the Kaggle platform
    is access to the numerous constantly ongoing and archived-for-posterity competitions
    on the platform.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们鼓励您使用 Kaggle 来运行所呈现的各种代码。尽管该平台提供的免费 GPU 计算和易用的设置立即成为其优点，但最大的好处可能直到现在才被明确说明。可以说，Kaggle
    平台最强大的方面是可以访问该平台上众多持续进行和归档供后人参考的竞赛。
- en: Top firms facing all sorts of technical challenges use the platform to stimulate
    research and development of solutions to said problems by offering cash prizes,
    sometimes into the thousands of dollars for top-place finishes. This means that
    by tracking these competitions, you are kept updated on what the most pressing
    problems in industry are, while having access to the representative data for immediate
    testing and experimentation. You could browse current and past competitions by
    topic to find data to test any ideas you might have—all you need to do is attach
    the dataset to the notebooks we have been using in this book, change some paths,
    and you likely will be ready to produce some preliminary insights. Winning the
    competitions is, of course, great, if you can do so, but the learning value you
    will get from experimenting, failing, and trying again is what is truly invaluable.
    Indeed, in my experience, a solution to a contest problem that may be considered
    mediocre by leaderboard placement may be the one that leads to a real-world impact,
    if it is easier to deploy and scale in practice, for instance. We provide some
    specific tips for using Kaggle in appendix A to help beginners get started with
    it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 各种顶级公司面临各种技术挑战，使用该平台来刺激对这些问题的解决方案的研究和开发，通过提供现金奖励，有时可达数千美元的头等奖。这意味着通过跟踪这些竞赛，您可以了解到行业中最紧迫的问题是什么，同时可以访问代表性数据进行即时测试和实验。您可以按主题浏览当前和过去的竞赛，以找到测试任何想法所需的数据——您所需要做的就是将数据集附加到本书中使用的笔记本上，更改一些路径，然后您可能已经准备好产生一些初步的见解了。当然，如果您能够做到这一点，赢得竞赛是很好的，但您从实验、失败和再试中获得的学习价值才是真正无价的。实际上，根据我的经验，一个在排行榜上可能被认为是平庸的竞赛问题解决方案，如果在实践中易于部署和扩展，可能会导致真正的现实影响。我们在附录
    A 中提供了一些使用 Kaggle 的具体提示，以帮助初学者入门。
- en: We have also highlighted the recent increase in focus on low-resource languages
    in NLP. It is thus crucial to mention the Zindi Africa platform, which provides
    many of the same functionalities as Kaggle but focuses on African languages and
    problems. If you are a researcher who wants to see how your methods might perform
    on some of these types of languages, this platform would be a good place to find
    related contests and data for experimentation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还强调了 NLP 中对低资源语言的关注日益增加。因此，重要的是提到 Zindi Africa 平台，该平台提供与 Kaggle 类似的许多功能，但专注于非洲语言和问题。如果您是一位研究人员，想要了解您的方法在这些类型的语言中可能的表现，那么这个平台将是一个寻找相关竞赛和实验数据的好地方。
- en: 11.5.2 arXiv
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 arXiv
- en: Machine learning, and by extension NLP, is arguably the most open field of research
    today. With a few exceptions, results are typically published on the open platform
    *arXiv* immediately when they become available. This allows research teams to
    claim priority to any discovery, while going through refinements and paper publication
    formalities. This means that the most cutting-edge research is already available
    to you if you are able to find it. *arXiv* is archived by Google Scholar, so setting
    alerts there for keywords that are important to you can help you detect relevant
    papers early on.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，以及延伸开来的自然语言处理，可以说是当今最开放的研究领域。除了几个例外，一般来说，结果通常在一旦出现后立即在开放平台*arXiv*上发表。这使研究团队能够提前对任何发现进行申请，同时进行完善和论文出版手续。这意味着，如果你能找到它的话，最前沿的研究已经对你可用。*arXiv*由Google
    Scholar存档，因此你可以在那里设定关键词的警报，帮助你及早发现相关的论文。
- en: The volume of paper uploads to the *arXiv* platform is huge, and it is hard
    to find the most important papers that might be relevant to you. To address this
    issue, I recommend following the authors of your favorite papers on social media—Twitter
    seems to be the platform preferred by researchers in this space. Keeping an eye
    on media coverage can also be helpful, as long as you treat all claims with a
    grain of salt. We say a few more words about that next.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*arXiv*平台上传的论文数量巨大，要找到与你相关的最重要的论文可能会有些困难。为了解决这个问题，我建议关注你喜欢的论文的作者在社交媒体上的动态——Twitter似乎是这一领域的研究人员比较喜欢的平台。关注媒体报道也可能会有所帮助，只要你对所有声明持保留态度。接下来我们会多说几句关于这个问题。'
- en: 11.5.3 News and social media (Twitter)
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.3 新闻和社交媒体（Twitter）
- en: In general, it is good to treat news and social media coverage of scientific
    topics as potentially sensationalist and otherwise technically unreliable. This
    makes sense if one thinks about the incentives a media outlet might have related
    to covering the technology and the fact that often journalists may not have a
    technical background on the subject. However, vetted news can be a good indicator
    of excitement in the community about a particular paper or topic, and that is
    always a good thing to be aware of.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，把科学主题的新闻和社交媒体报道看作可能具有煽动性和技术上不可靠是件好事。如果我们考虑一个媒体机构可能与报道技术相关的激励以及通常记者对主题可能没有技术背景的事实，这就说得通了。然而，经过核实的新闻可能是关于特定论文或主题的社区兴奋的一个良好指标，这总是一个需要考虑的好事。
- en: If you use a platform such as Google News, you can set alerts for topics such
    as “language models” in your feed. You will probably get a lot of hits, and not
    all of them might be worth your attention. I usually really dig into a paper deeply
    only after I see it come up in venues that I consider consistently “reliable”
    over a period of time, which gives me some confidence that the claims have withstood
    at least a short period of open review. The case of GPT-3 comes up when thinking
    about a recent example—this was one whose impact was immediately evident by following
    this heuristic on Google News.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用像谷歌新闻这样的平台，你可以在你的订阅中设置“语言模型”等主题的警报。你可能会得到很多信息，而且并非所有信息都值得关注。通常，我只会在这些论坛上深入研究一篇论文，只有在我认为一段时间内一直“可靠”的场所出现后，我才会深入研究一篇论文，这使我对这些论点至少经受了一段时间的公开评审产生了信心。GPT-3的情况正好是个最近的例子——通过谷歌新闻上的这个启发式方法，我立刻就能体会到其影响。
- en: With regard to social media, Twitter appears to be the platform of choice for
    machine learning research scientists. In fact, many are very open about their
    work and will gladly engage with you directly on the platform if you just ask
    them a question. This is one of the things I love most about working in this field.
    Please feel free to reach out to me at @pazunre. Your favorite author or scientist
    probably shares their latest favorite papers on their feed, and by just following
    them you could have these delivered to you directly. Some popular accounts you
    may find interesting in this space include @fchollet, @seb_ruder, and @huggingface.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关于社交媒体，Twitter似乎是机器学习研究科学家的选择平台。事实上，许多人对他们的工作非常开放，并且如果你向他们提问，他们会很乐意在平台上直接回答你。这也是我最喜欢在这个领域工作的原因之一。请随时在@pazunre上联系我。你最喜欢的作家或科学家可能会在他们的订阅中分享他们最新最喜欢的论文，通过关注他们，你可以直接收到这些信息。在这一领域，你可能会对以下一些受欢迎的账户感兴趣：@fchollet，@seb_ruder和@huggingface。
- en: Beyond competitions, reading papers on *arXiv*, and tracking news and social
    media, nothing is really a good substitute for working on real-world practical
    challenges with these tools. For many people, this might just mean holding a job
    in machine learning and/or NLP and working on a practical application of them
    daily. Practical experience is what is valued most by the majority of potential
    employers in this field. If you have yet to gain such practical experience in
    this area and are looking to break in, open source projects can be a great way
    to do that—check out TensorFlow, PyTorch, Hugging Face, NLP Ghana, Masakhane,
    and so on. The list is endless, and there is no shortage of interesting problems
    to solve and contribute to, while also potentially benefitting everyone.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了竞赛、阅读 *arXiv* 上的论文、追踪新闻和社交媒体，没有什么比使用这些工具解决实际的挑战更好的了。对于许多人来说，这可能意味着在机器学习和/或自然语言处理领域拥有一份工作，并且每天都在解决一个实际的应用。实际的经验是这个领域大多数潜在雇主最重视的。如果你还没有在这个领域获得这样的实际经验，并且希望进入，那么开源项目可能是一个很好的途径——看看
    TensorFlow、PyTorch、Hugging Face、NLP Ghana、Masakhane 等等。列表是无穷尽的，有很多有趣的问题可以解决和贡献，同时也可能使每个人受益。
- en: I hope these tips will help guide you into your machine learning and NLP future,
    where you are empowered to make a significant positive impact on your society.
    It has been a privilege to share a part of your journey with you.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这些提示能帮助你进入你的机器学习和自然语言处理的未来，让你有能力对你的社会产生重大的积极影响。能够与你分享你旅程的一部分是我的荣幸。
- en: 11.6 Final words
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 最后的话
- en: This is it! You have done it—you have read all of this book. I had an incredible
    time writing it, interacting with many researchers in the process to discuss ideas
    and working through the many challenges. I sincerely hope that you enjoyed the
    journey as much as I did. As you go forth and change the world with these tools,
    remember to be kind to those around you, do no harm to the ecosystem, and stay
    vigilant about the potential misuses of your tech. From my brief time interacting
    with some of the brilliant minds working in this field, I sincerely believe that
    most are excited about making these technological breakthroughs a source for good.
    Thus, I watch the research news with excitement day after day, impatient to see
    what our collective human mind will come up with next. I can only hope that you
    share some of this excitement.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是了！你做到了——你已经读完了整本书。在写作过程中，我度过了难忘的时光，与许多研究人员互动，讨论思想并克服了许多挑战。我真诚地希望你享受这个旅程，就像我一样。当你带着这些工具改变世界的时候，请记得善待你周围的人，不要伤害生态系统，并且保持警惕，以防技术被滥用。通过与这个领域中一些杰出头脑互动的短暂时间内，我真诚地相信大多数人对于将这些技术突破变成善良的源头都感到兴奋。因此，我每天都迫不及待地关注研究新闻，渴望看到我们集体的人类思维将会产生什么样的惊喜。我只能希望你也能分享一些这种兴奋。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概要
- en: You are only at the beginning of your journey in this rapidly evolving field;
    retaining a competitive edge is a journey and not a destination.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你只是在这个迅速发展的领域中旅程的开端；保持竞争优势是一个旅程，而不是一个终点。
- en: The skills you have picked up by working through this book have put you in a
    good position to enable you to stay up-to-date with continued effort.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过学习这本书所掌握的技能，使你处于一个良好的位置，能够通过持续的努力保持更新。
- en: Some key fundamental pretrained transfer-learning-enabled language modeling
    architectures we covered include the Transformer, BERT, mBERT, ELMo, and GPT.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们涵盖的一些关键的基础预训练迁移学习启用的语言建模架构包括 Transformer、BERT、mBERT、ELMo 和 GPT。
- en: The desire to make these larger models smaller and more efficient led to the
    development of architectures/techniques such as ALBERT, DistilBERT, and ULMFiT,
    which we covered as well.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些更大的模型变得更小、更高效的愿望导致了像 ALBERT、DistilBERT 和 ULMFiT 这样的架构/技术的发展，我们也进行了介绍。
- en: Emerging architectures that are the descendants of the aforementioned models,
    which are not covered by the book in detail but which you should be aware of,
    include BART, T5, Longformer, Reformer, XLNet, and many more.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新兴的体系结构是前述模型的后代，书中没有详细介绍但你应该知道的，包括 BART、T5、Longformer、Reformer、XLNet 等等。
- en: It is important to be aware of potential ethical and environmental impacts of
    these models when deploying them in practice.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中部署这些模型时，意识到它们可能带来的潜在的道德和环境影响是很重要的。
- en: Recent concerns about ethical and environmental impacts, as well as the desire
    to put model capabilities on smartphones and IoT, will likely continue to fuel
    the development of increasingly more efficient transformer architectures in the
    near future.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近期对道德和环境影响的关注，以及希望将模型能力应用于智能手机和物联网，很可能会在不久的将来继续推动更高效的变压器架构的开发。
- en: '1. Sharir O. et al., “The Cost of Training NLP Models: A Concise Overview,”
    arXiv (2020).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 1. Sharir O.等，“训练NLP模型的成本：简要概述”，arXiv（2020）。
- en: 2. S.J. Pan and Q. Yang, “A Survey on Transfer Learning,” IEEE Transactions
    on Knowledge and Data Engineering (2009).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 2. S.J. Pan和Q. Yang，“迁移学习概述”，IEEE知识与数据工程交易（2009）。
- en: 3. S. Ruder, “Neural Transfer Learning for Natural Language Processing,” National
    University of Ireland, Galway, Ireland (2019).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 3. S. Ruder，“自然语言处理的神经迁移学习”，爱尔兰加尔韦国立大学（2019）。
- en: 4. D. Wang and T. F. Zheng, “Transfer Learning for Speech and Language Processing,”
    Proceedings of 2015 Asia-Pacific Signal and Information Processing Association
    Annual Summit and Conference (APSIPA).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 4. D. Wang和T. F. Zheng，“语音和语言处理的迁移学习”，2015年亚太信号与信息处理协会年度峰会和会议（APSIPA）论文集。
- en: 5. Lipmann Richard et al., “An Overview of the DARPA Data-Driven Discovery of
    Models (D3M) Program,” Proceedings of the 29th Conference on Neural Information
    Processing Systems (NeurIPS) (2016).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 5. Lipmann Richard等，“DARPA数据驱动模型发现（D3M）计划概述”，第29届神经信息处理系统（NeurIPS）会议论文集（2016）。
- en: 6. [https://datadrivendiscovery.org/](https://datadrivendiscovery.org/)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 6. [https://datadrivendiscovery.org/](https://datadrivendiscovery.org/)
- en: '7. Yinhan Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach,”
    arXiv (2019).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 7. Yinhan Liu等，“RoBERTa：稳健优化的BERT预训练方法”，arXiv（2019）。
- en: 8. Tom B. Brown et al., “Language Models Are Few-Shot Learners,” NeurIPS (2020).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 8. Tom B. Brown等，“语言模型是少样本学习者”，NeurIPS（2020）。
- en: '9. W. Fedus et al., “Switch Transformers: Scaling to Trillion Parameter Models
    with Simple and Efficient Sparsity,”arXiv (2021).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 9. W. Fedus等，“Switch变压器：用简单高效的稀疏性扩展到万亿参数模型”，arXiv（2021）。
- en: 10. [https://www.eleuther.ai/projects/gpt-neo/](https://www.eleuther.ai/projects/gpt-neo/)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 10. [https://www.eleuther.ai/projects/gpt-neo/](https://www.eleuther.ai/projects/gpt-neo/)
- en: 11. [https://huggingface.co/EleutherAI](https://huggingface.co/EleutherAI)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 11. [https://huggingface.co/EleutherAI](https://huggingface.co/EleutherAI)
- en: 12. [https://www.kaggle.com/azunre/tlfornlp-chapter7-gpt-neo](https://www.kaggle.com/azunre/tlfornlp-chapter7-gpt-neo)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 12. [https://www.kaggle.com/azunre/tlfornlp-chapter7-gpt-neo](https://www.kaggle.com/azunre/tlfornlp-chapter7-gpt-neo)
- en: '13. Z. Yang et al., “XLNet: Generalized Autoregressive Pretraining for Language
    Understanding,” NeurIPS (2019).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 13. Z. Yang等，“XLNet：用于语言理解的广义自回归预训练”，NeurIPS（2019）。
- en: '14. Z. Dai et al., “Transformer-XL: Attentive Language Models beyond a Fixed-Length
    Context,” ACL (2019).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 14. Z. Dai等，“Transformer-XL：超越固定长度上下文的关注语言模型”，ACL（2019）。
- en: '15. M. Zaheer et al., “BigBird: Transformers for Longer Sequences,” arXiv (2020).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 15. M. Zaheer等，“BigBird：更长序列的变压器”，arXiv（2020）。
- en: '16. I. Beltagy et al., “Longformer: The Long-Document Transformer,” arXiv (2020).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 16. I. Beltagy等，“Longformer：长文档变压器”，arXiv（2020）。
- en: '17. N. Kitaev et al., “Reformer: The Efficient Transformer,” arXiv (2020).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 17. N. Kitaev等，“Reformer：高效变压器”，arXiv（2020）。
- en: 18. C. Raffel et al., “Exploring the Limits of Transfer Learning with a Unified
    Text-to-Text Transformer,” arXiv (2020).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 18. C. Raffel等，“探索统一文本到文本变压器的迁移学习极限”，arXiv（2020）。
- en: '19. L. Xue et al., “mT5: A Massively Multilingual Pre-Trained Text-to-Text
    Transformer,” arXiv (2019).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 19. L. Xue等，“mT5：大规模多语言预训练文本到文本变压器”，arXiv（2019）。
- en: '20. M. Lewis et al., “BART: Denoising Sequence-to-Sequence Pre-training for
    Natural Language Generation, Translation, and Comprehension,” arXiv (2020).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 20. M. Lewis等，“BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练”，arXiv（2020）。
- en: 21. Y. Liu et al., “Multilingual Denoising Pre-Training for Neural Machine Translation,”
    arXiv (2020).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 21. Y. Liu等，“用于神经机器翻译的多语言去噪预训练”，arXiv（2020）。
- en: 22. G. Lample and A. Conneau, “Cross-Lingual Language Model Pretraining,” arXiv
    (2019).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 22. G. Lample和A. Conneau，“跨语言语言模型预训练”，arXiv（2019）。
- en: 23. A. Conneau et al., “Unsupervised Cross-Lingual Representation Learning at
    Scale,” arXiv (2019).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 23. A. Conneau等，“规模化的无监督跨语言表示学习”，arXiv（2019）。
- en: '24. J. Herzig et al., “TaPas: Weakly Supervised Table Parsing via Pre-Training,”
    arXiv (2020).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 24. J. Herzig等，“TaPas：通过预训练进行弱监督表解析”，arXiv（2020）。
- en: 25. [https://ghananlp.org/](https://ghananlp.org/)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 25. [https://ghananlp.org/](https://ghananlp.org/)
- en: 26. [https://www.masakhane.io/](https://www.masakhane.io/)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 26. [https://www.masakhane.io/](https://www.masakhane.io/)
- en: 27. [https://ethionlp.github.io/](https://ethionlp.github.io/)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 27. [https://ethionlp.github.io/](https://ethionlp.github.io/)
- en: 28. [https://zindi.africa/](https://zindi.africa/)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 28. [https://zindi.africa/](https://zindi.africa/)
- en: 29. [https://www.k4all.org/project/language-dataset-fellowship/](https://www.k4all.org/project/language-dataset-fellowship/)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 29. [https://www.k4all.org/project/language-dataset-fellowship/](https://www.k4all.org/project/language-dataset-fellowship/)
- en: 30. [https://blackinai.github.io/](https://blackinai.github.io/)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 30. [https://blackinai.github.io/](https://blackinai.github.io/)
- en: 31. [https://huggingface.co/facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 31. [https://huggingface.co/facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)
- en: 32. [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 32. [https://openai.com/blog/dall-e/](https://openai.com/blog/dall-e/)
- en: '33. A. Dosovitskiy et al, “An Image Is Worth 16x16 Words: Transformers for
    Image Recognition at Scale,” arXiv (2020).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '33. A. Dosovitskiy et al, “An Image Is Worth 16x16 Words: Transformers for
    Image Recognition at Scale,” arXiv (2020).'
- en: '34. J. Builamwini and T. Gebru, “Gender Shades: Intersectional Accuracy Disparities
    in Commercial Gender Classification,” Journal of Machine Learning Research 81
    (2018).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '34. J. Builamwini and T. Gebru, “Gender Shades: Intersectional Accuracy Disparities
    in Commercial Gender Classification,” Journal of Machine Learning Research 81
    (2018).'
- en: 35. Tom B. Brown et al., “Language Models Are Few-Shot Learners,” NeurIPS (2020).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 35. Tom B. Brown et al., “Language Models Are Few-Shot Learners,” NeurIPS (2020).
- en: 36. [http://mng.bz/w0V2](http://mng.bz/w0V2)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 36. [http://mng.bz/w0V2](http://mng.bz/w0V2)
- en: 37. E. Strubell et al., “Energy and Policy Considerations for Deep Learning
    in NLP,” ACL (2019).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 37. E. Strubell et al., “Energy and Policy Considerations for Deep Learning
    in NLP,” ACL (2019).
- en: '38. E. Bender et al., “On the Dangers of Stochastic Parrots: Can Language Models
    Be Too Big?” FAccT (2021).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '38. E. Bender et al., “On the Dangers of Stochastic Parrots: Can Language Models
    Be Too Big?” FAccT (2021).'
- en: 39. Jesse Vig, “A Multiscale Visualization of Attention in the Transformer Model,”
    ACL (2019).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 39. Jesse Vig, “A Multiscale Visualization of Attention in the Transformer Model,”
    ACL (2019).
