- en: 6 Deep transfer learning for NLP with recurrent neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.循环神经网络用于自然语言处理的深度迁移学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Three representative modeling architectures for transfer learning in NLP relying
    on RNNs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于RNN的自然语言处理迁移学习的三种代表性建模架构
- en: Applying these methods to the two problems introduced in the previous chapter
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些方法应用于上一章中介绍的两个问题
- en: Transferring knowledge obtained from training on simulated data to real labeled
    data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将在模拟数据训练中获得的知识传递到真实标记数据
- en: An introduction to some more sophisticated model adaptation strategies via ULMFiT
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍一些更复杂的模型适应策略，通过ULMFiT
- en: In the previous chapter, we introduced two example problems for the experiment
    we will conduct in this chapter—column-type classification and fake news detection.
    Recall that the goal of the experiment is to study the deep transfer learning
    methods for NLP that rely on recurrent neural networks (RNNs) for key functions.
    In particular, we will focus on three such methods—SIMOn, ELMo, and ULMFiT—which
    were briefly introduced in the previous chapter. In this chapter, we will apply
    them to the example problems, starting with SIMOn in the next section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了两个用于本章实验的例子问题——列类型分类和虚假新闻检测。回顾一下，实验的目标是研究依赖于循环神经网络（RNN）的深度迁移学习方法，以用于自然语言处理的关键功能。具体而言，我们将重点研究三种方法——SIMOn、ELMo和ULMFiT，这些方法在上一章中已经简要介绍过。在下一节中，我们将从SIMOn开始，将它们应用于示例问题。
- en: 6.1 Semantic Inference for the Modeling of Ontologies (SIMOn)
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1语义推理用于本体建模（SIMOn）
- en: As we discussed briefly in the previous chapter, SIMOn was designed as a component
    of an automatic machine learning (AutoML) pipeline for the Data-Driven Discovery
    of Models (D3M) DARPA program. It was developed as a classification tool for the
    column type in a tabular dataset but can also be viewed as a more general text
    classification framework. We will present the model in the context of arbitrary
    text input first and then specialize it to the tabular case.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中简要讨论的那样，SIMOn是作为自动机器学习（AutoML）管道的一个组成部分而设计的，用于数据驱动的模型发现（D3M）DARPA计划。它被开发为用于表格数据集中列类型的分类工具，但也可以看作是一个更一般的文本分类框架。我们将首先在任意文本输入的环境下介绍该模型，然后将其专门用于表格案例。
- en: By design, SIMOn is a character-level model, as opposed to a word-level model,
    in order to handle misspellings and other social media features, such as emoticons
    and niche vernacular. Because it encodes input text at the character level, the
    input needs to be expressed with only allowable characters to be useful for classification.
    This allows the model to easily adapt to the dynamic nature of social media language.
    The character-level nature of the model is contrasted with word-level models in
    figure 6.1\. On the left of the figure, we show the word-level encoder, for which
    input has to be a valid word. Obviously, an out-of-vocabulary word due to misspelling
    or vernacular is an invalid input. For character-level encoders, shown on the
    right and resembling those of ELMo and SIMOn, input only has to be a valid character,
    which helps in handling misspellings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: SIMOn是一个字符级模型，而不是单词级模型，以处理拼写错误和其他社交媒体特征，如表情符号和专业知识的口头语。因为它以字符级别编码输入文本，所以输入只需要用于分类的允许字符即可。这使得模型能够轻松适应社交媒体语言的动态特性。模型的字符级本质在图6.1中与单词级模型进行对比。在图的左侧，我们展示了单词级编码器，其输入必须是一个有效的单词。显然，由于拼写错误或行话，一个词汇表外的词是无效的输入。对于字符级编码器，如ELMo和SIMOn所示，输入只需要是一个有效的字符，这有助于处理拼写错误。
- en: '![06_01](../Images/06_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![06_01](../Images/06_01.png)'
- en: Figure 6.1 Contrasting word-level to character-level models for text classification
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 对比基于单词级和字符级的文本分类模型
- en: 6.1.1 General neural architecture overview
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 通用神经架构概述
- en: The network, which can be split into two major coupled components, takes a document
    tokenized into sentences as an input. The first component is a network that encodes
    each individual sentence, whereas the second takes the encoded sentences and creates
    an encoding for the entire document.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络可以分为两个主要耦合的部分，将一个被分割为句子的文档作为输入。第一个部分是一个用于编码每个独立句子的网络，而第二个部分则使用编码的句子创建整个文档的编码。
- en: The sentence encoder first one-hot encodes the input sentence at the character
    level using a dictionary of 71 characters. This includes all possible English
    alphabets, as well as numbers and punctuation marks. The input sentence is also
    standardized to a length of `max_len`. It is then passed through a sequence of
    convolutional, max-pooling, dropout, and bidirectional LSTM layers. Please refer
    to the first two stages of figure 5.1, duplicated here for your convenience, for
    a summary visualization. The convolutions essentially form the concept of “words”
    in each sentence, whereas the bidirectional LSTM “looks” in both directions around
    a word to determine its local context. The output from this stage is an embedding
    vector of a default dimension 512 for each sentence. Also compare the equivalent
    pictorial representation of the bi-LSTMs in figure 5.1 and figure 6.1 to make
    things concrete.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 句子编码器首先对输入句子进行字符级的独热编码，使用了一个包含71个字符的字典。这包括所有可能的英文字母，以及数字和标点符号。输入句子也被标准化为长度为`max_len`。然后通过一系列的卷积、最大池化、失活和双向LSTM层。请参考图5.1的前两个阶段，这里为了方便起见重复一次，进行一个摘要可视化。卷积层在每个句子中实质上形成了“词”的概念，而双向LSTM“查看”一个词周围的两个方向，以确定其局部上下文。这一阶段的输出是每个句子的默认维度为512的嵌入向量。还可以比较图5.1和图6.1中双向LSTM的等效图示来使事情具体化。
- en: '![06_01_05_01](../Images/06_01_05_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![06_01_05_01](../Images/06_01_05_01.png)'
- en: Figure 5.1 (Duplicated from the previous chapter for your convenience.) Visualizing
    SIMOn architecture in the context of the tabular column-type classification example
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1（为了方便起见，从上一章中重复）在表格列类型分类示例中可视化SIMOn架构
- en: The document encoder takes the sentence-embedding vectors as input and similarly
    passes them through a sequence of dropout and bidirectional LSTM layers. The length
    of each document is standardized to `max_cells` such embedding vectors. This can
    be thought of as a process of forming higher-level “concepts” or “topics” from
    sentences, within their context with respect to other concepts present in the
    document. This produces an embedding vector for each document, which is then passed
    through a classification layer that outputs probabilities for each different type
    or class.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 文档编码器将句子嵌入向量作为输入，类似地通过一系列的随机失活和双向LSTM层来处理它们。每个文档的长度被标准化为`max_cells`个这样的嵌入向量。可以将这看作是从句子中形成更高级的“概念”或“主题”的过程，这些概念与文档中存在的其他概念相关联。这为每个文档产生了一个嵌入向量，然后通过一个分类层传递，输出每种不同类型或类的概率。
- en: 6.1.2 Modeling tabular data
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 对表格数据进行建模
- en: Modeling tabular data is surprisingly straightforward; it just requires viewing
    every cell of a column within a tabular dataset as a sentence. Naturally, each
    such column is viewed as a document to be classified.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对表格数据进行建模出人意料的简单；它只需要将表格数据集中每个单元格都视为一个句子。当然，每个这样的列被视为要进行分类的一个文档。
- en: This means that to apply the SIMOn framework to unstructured text, one just
    needs to transform the text into a table with one document per column and one
    sentence per cell. An illustration of this process is shown in figure 6.2\. Note
    that in this simple example, we have chosen `max_cells` to be equal to 3 for the
    sake of illustration.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着要将SIMOn框架应用到非结构化文本，只需将文本转换成一张表，每列一个文档，每个单元格一个句子。这个过程的示意图在图6.2中展示。请注意，在这个简单的例子中，我们选择`max_cells`等于3，只是为了示例。
- en: '![06_02](../Images/06_02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![06_02](../Images/06_02.png)'
- en: Figure 6.2 The process of converting unstructured text for consumption by SIMOn
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 将非结构化文本转换为SIMOn可消化的过程
- en: 6.1.3 Application of SIMOn to tabular column-type classification data
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 将SIMOn应用于表格列类型分类数据
- en: 'In its original form, SIMOn was initially trained on simulated data for a set
    of base classes. It was then transferred to a smaller set of hand-labeled data.
    Knowing how to generate simulated data can be useful, and so we briefly illustrate
    the process using the following set of commands, which use the library Faker under
    the hood:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在其原始形式中，SIMOn最初是在一组基础类的模拟数据上进行训练的。然后转移到一组手工标记的较小数据。了解如何生成模拟数据可能是有用的，因此我们用以下一组命令简要地说明了这个过程，这些命令在底层使用了库Faker：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Simulated/fake data-generation utility (using the library Faker)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模拟/伪造数据生成实用工具（使用库Faker）
- en: ❷ Number of columns to generate, chosen arbitrarily for the sake of simple illustration
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成的列数，为了简单起见任意选择
- en: ❸ Number of cells/rows per column, chosen arbitrarily for the sake of simple
    illustration
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每列的单元格/行数，为了简单说明而任意选择
- en: ❹ Don't reuse data, but rather generate fresh data for more variability in the
    dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 不要重用数据，而是为数据集中的变化性生成新鲜数据。
- en: ❺ Prints results
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印结果
- en: 'Executing this code yields the following output, which shows generated samples
    of various data types and their corresponding labels:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码会产生以下输出，显示各种数据类型的生成样本及其相应的标签：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The top level of the SIMOn repository contains the file types.json, which specifies
    mappings from the Faker library classes to the classes displayed previously. For
    instance, the second column of names in the previous example is labeled “text”
    because we did not require identifying names for our purposes. You could quickly
    change this mapping and generate simulated data for your own projects and sets
    of classes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SIMOn 仓库的顶层包含了 types.json 文件，该文件指定了从 Faker 库类到先前显示的类别的映射。例如，前一个示例中名称的第二列被标记为“文本”，因为我们不需要为我们的目的识别名称。您可以快速更改此映射，并为您自己的项目和类别集生成模拟数据。
- en: We do not train on simulated data here, because that process can take a few
    hours, and we already have access to the pretrained model capturing that knowledge.
    We do, however, perform an illustrative transfer learning experiment that involves
    expanding the set of classes supported beyond those available in the pretrained
    model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里不使用模拟数据进行训练，因为该过程可能需要几个小时，而我们已经可以访问捕捉到这些知识的预训练模型。但是，我们会进行一项说明性的迁移学习实验，涉及扩展支持的类别集合，超出了预训练模型中可用的类别。
- en: 'Recall that we loaded the SIMOn Classifier class in section 5.1.2, along with
    model configurations, including the encoder. We can then generate a Keras SIMOn
    model, load our downloaded weights into it, and compile it using the following
    sequence of commands:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在第 5.1.2 节中加载了 SIMOn 分类器类以及模型配置，包括编码器。然后我们可以生成一个 Keras SIMOn 模型，将下载的权重加载到其中，并使用以下命令序列进行编译：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Generates the model
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成模型
- en: ❷ Loads the weights
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载权重
- en: ❸ Compiles the model, using binary_crossentropy loss for multilabel classification
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编译模型，使用二元交叉熵损失进行多标签分类
- en: 'It is a good idea to take a look at the model architecture before proceeding,
    which we can do using this command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，查看模型架构是个好主意，我们可以使用以下命令来做到这一点：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This displays the following output and allows you to get a better sense for
    what is going on under the hood:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出，并允许您更好地了解内部发生的情况：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `time_distributed_1` layer is the sentence encoder applied to every input
    sentence. We see that is followed by forward and backward LSTMs that are concatenated,
    some regularization via dropout, and output probabilities from the `dense_2` layer.
    Recall that the number of classes handled by the pretrained model is exactly 9,
    which matches the dimension of the output `dense_2` layer. Also note that, coincidentally,
    the model has 9 layers total.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`time_distributed_1` 层是应用于每个输入句子的句子编码器。我们看到其后是前向和后向的 LSTM，它们被连接在一起，一些通过 dropout
    进行的正则化，以及来自 `dense_2` 层的输出概率。回想一下，预训练模型处理的类别数恰好为 9，这与输出 `dense_2` 层的维度匹配。还要注意的是，巧合的是，模型总共有
    9 层。'
- en: 'Having gained a sense for the compiled model’s architecture, let’s go ahead
    and see what it thinks the types of the baseball dataset columns are. We do this
    by executing the following sequence of commands:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下一系列命令，我们已经对编译模型的架构有了一定的了解，现在让我们继续查看它认为棒球数据集列的类型是什么。我们通过执行以下命令序列来实现这一点：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The probability threshold for deciding the membership of class
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于决定类成员身份的概率阈值
- en: ❷ Predicts the baseball dataset column classes
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测棒球数据集列的类别
- en: ❸ Converts probabilities to class labels
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将概率转换为类别标签
- en: ❹ Displays the output
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 显示输出
- en: 'The corresponding code output looks as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的代码输出如下所示：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Looking back to section 5.1.1 for a displayed slice of this data, which we
    are replicating here, we see that the model gets every column exactly right with
    high confidence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾第 5.1.1 节以显示此数据的切片，我们在此复制，我们看到模型以高置信度完全正确地获取了每一列：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, suppose we were interested in detecting columns with percentage values
    in them for a project. How could we use the pretrained model to achieve this quickly?
    We can investigate this scenario using the second tabular dataset we prepared
    in the previous chapter—the multiyear British Columbia public library statistics
    dataset. Naturally, the first step is to predict that data using the pretrained
    model directly. The following sequence of commands achieves this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有兴趣在项目中检测具有百分比值的列。我们如何快速使用预训练模型来实现这一点呢？我们可以使用上一章中准备的第二个表格数据集来调查这种情况——多年来的不列颠哥伦比亚公共图书馆统计数据集。当然，第一步是直接使用预训练模型预测这些数据。以下一系列命令实现了这一点：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Encodes the data using the original frame
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用原始框架对数据进行编码
- en: ❷ Predicts the classes
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测类别
- en: ❸ Converts probabilities to class labels
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将概率转换为类标签
- en: 'This produces the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Looking back to section 5.1.1 for a displayed slice of this data, we see that
    the integer column is correctly identified, whereas the percent column is identified
    as text:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾5.1.1节的一个数据切片，我们看到整数列被正确识别，而百分比列被识别为文本：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That is not incorrect, but it is not exactly what we are looking for, either,
    because it is not specific enough.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那并不是不正确，但也不完全是我们正在寻找的，因为它不够具体。
- en: 'We will quickly transfer the pretrained model to a very small set of training
    data that includes percentage samples. Let’s first inform ourselves of the size
    of the raw library DataFrame using the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速将预训练模型转移到一个非常小的包含百分比样本的训练数据集。首先让我们使用以下命令了解原始库DataFrame的大小：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We find the size to be (1207,2), which appears to be a sufficient number of
    rows to build a small dataset!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现尺寸为(1207,2)，这似乎是足够构建一个小数据集的行数！
- en: In listing 6.1, we show the script that can be used to split this dataset into
    many smaller columns of 20 cells each. The number 20 was chosen arbitrarily, driven
    by the desire to create sufficient unique columns—approximately 50—in the resulting
    dataset. This process yields a new DataFrame, `new_raw_data`, of size 20 rows
    by 120 columns—the first 60 corresponding to percentage values and the next 60
    corresponding to integer values. It also produces a corresponding list of `header`
    labels.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在清单6.1中，我们展示了可用于将此数据集分割为许多每个20个单元格的更小列的脚本。数字20是任意选择的，是为了创建足够多的唯一列——大约50个——在生成的数据集中。此过程产生一个新的DataFrame，`new_raw_data`，大小为20行120列——前60列对应于百分比值，后60列对应于整数值。它还生成一个相应的`header`标签列表。
- en: Listing 6.1 Transforming long library data into many shorter sample columns
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 清单6.1 将长库数据转换为许多较短样本列
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Turns the data into two lists
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据转换为两个列表
- en: ❷ Breaks it up into individual sample columns of 20 cells each
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将其分解为每个样本列20个单元格
- en: ❸ Original length, 1207
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 原始长度，1207
- en: ❹ List of the indices of the new columns
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 新列的索引列表
- en: ❺ Initializes the new DataFrame to hold new data
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 初始化新的DataFrame以保存新数据
- en: ❻ Populates the new DataFrame
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用新的DataFrame填充
- en: ❼ Populates the DataFrame with percentage values
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用百分比值填充DataFrame
- en: ❽ Populates the DataFrame with *integer* values
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用*整数*值填充DataFrame
- en: ❾ Let’s create a corresponding header for our training data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 让我们为我们的训练数据创建相应的标题。
- en: Recall that the final layer of the pretrained model has an output dimension
    of 9, matching the number of handled classes. To add another class, we need to
    increase the output dimension to a size of 10\. We should also initialize this
    new dimension’s weights to those of the text class because that is the most similar
    class handled by the pretrained model. This was determined when we predicted the
    percentage data as text with the pretrained model earlier. This is accomplished
    by the script shown in the next listing. In the script, we add percent to the
    list of supported categories, increase the output dimension by 1 to accommodate
    this addition, and then initialize the corresponding dimension weights to those
    of the closest category text values.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 记得预训练模型的最后一层具有输出维度为9，与处理的类的数量相匹配。要添加另一个类，我们需要将输出维度增加到大小为10。我们还应该将这个新维度的权重初始化为文本类的权重，因为这是预训练模型处理的最相似的类。这是在我们之前使用预训练模型将百分比数据预测为文本时确定的。这是通过下一个清单中显示的脚本完成的。在脚本中，我们将百分比添加到支持的类别列表中，将输出维度增加1以容纳此添加，然后将相应维度的权重初始化为最接近的类别文本值的权重。
- en: Listing 6.2 Creating new weights for the final output layer, including the percent
    class
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 6.2 创建最终输出层的新权重，包括百分比类
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Grabs the last layer weights for initialization
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 抓取初始化的最后一层权重
- en: ❷ Finds the old weight index for the closest category—text
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 找到最接近类别的旧权重索引—文本
- en: ❸ Updates the encoder with the new category list
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用新的类别列表更新编码器
- en: ❹ Sorts the new list alphabetically
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对新列表按字母顺序排序
- en: ❺ Finds the index of the new category
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 找到新类别的索引
- en: ❻ Initializes the new weights to old weights
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将新权重初始化为旧权重
- en: ❼ Inserts the text weights at the percent weight location
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在百分比权重位置插入文本权重
- en: ❽ Inserts the text biases at the percent biases location
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在百分比偏差位置插入文本偏差
- en: After executing the code in listing 6.2, you should double-check the shapes
    of the arrays `old_weights` and `new_weights`. You should find that the former
    is (128,9), whereas the latter is (128,10), if things worked as expected.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行清单 6.2 中的代码之后，您应该仔细检查数组`old_weights`和`new_weights`的形状。如果一切按预期进行，您应该会发现前者是（128,9），而后者是（128,10）。
- en: 'Now that we have prepared the weights with which to initialize the new model
    before pretraining, let’s actually build and compile this new model. SIMOn API
    includes the following function that makes it very easy to build the model:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好在预训练之前用来初始化新模型的权重，让我们实际构建和编译这个新模型。SIMOn API 包含以下函数，使构建模型非常容易：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The transfer model returned by this function is exactly analogous to the one
    we built before, with the exception that the final layer now has a new dimension,
    as specified by the input `category_count+1`. Additionally, because we did not
    give it any initialization information for the newly created output layer, this
    layer is presently initialized to weights of all zeros.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此函数返回的转移模型与我们之前构建的模型完全类似，唯一的区别是最终层现在具有新的维度，由输入`category_count+1`指定。另外，因为我们没有为新创建的输出层提供任何初始化信息，所以这一层目前被初始化为全零权重。
- en: 'Before we can train this new transfer model, let’s make sure that only the
    final output layer is trainable. We do this, along with compiling the model, via
    the following code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以训练这个新的转移模型之前，让我们确保只有最终输出层是可训练的。我们通过以下代码片段完成这一点，并编译模型：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Makes all layers untrainable to start
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 开始时使所有层都不可训练
- en: ❷ Only the last layer should be trainable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 只有最后一层应该是可训练的。
- en: ❸ Sets the weights of final layer to the previously determined initialization
    values
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将最终层的权重设置为先前确定的初始化值
- en: ❹ Compiles the model
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 编译模型
- en: We can now train the built, initialized, and compiled transfer model on the
    new data using the code in the following listing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下清单中的代码在新数据上训练构建的、初始化的和编译的转移模型。
- en: Listing 6.3 Training the initialized and compiled new transfer model
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 6.3 训练初始化和编译的新转移模型
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Encodes the new data (standardization, transposition, conversion to NumPy
    array)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 编码新数据（标准化、转置、转换为NumPy数组）
- en: ❷ Encodes the labels
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码标签
- en: ❸ Prepares the data in the expected format -> 60/30/10 train/validation/test
    data split
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 准备预期格式的数据 -> 60/30/10 训练/验证/测试数据拆分
- en: ❹ Trains the data
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练数据
- en: We visualize the convergence information produced by this code in figure 6.3\.
    We see that a validation accuracy of 100% is achieved at the seventh epoch, and
    the time for training was 150 seconds. It appears that our experiment worked,
    and we have successfully fine-tuned the pretrained model to handle a new class
    of data! We note that for this new model to handle all 10 classes accurately,
    we need to include a few samples of each class in the training data during the
    transfer step. The fine-tuned model at this stage is suitable only for predicting
    classes included in the transfer step—`integer` and `percent`. Because our goal
    here was merely illustrative, we leave this as a caution for the reader and do
    not concern ourselves further with it here.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 6.3 中可视化了此代码生成的收敛信息。我们看到在第七个时期实现了100％的验证准确率，训练时间为150秒。看来我们的实验成功了，我们已成功地微调了预训练模型以处理新的数据类！我们注意到，为了使这个新模型能够准确地处理所有10个类，我们需要在转移步骤中的训练数据中包含每个类的一些样本。在这个阶段，微调的模型只适用于预测包含在转移步骤中的类——`整数`和`百分比`。因为我们这里的目标仅仅是说明性的，我们将此作为读者的警告，并不进一步关注。
- en: '![06_03](../Images/06_03.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![06_03](../Images/06_03.png)'
- en: Figure 6.3 Visualization of the convergence of the percent class transfer tabular
    data experiment
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 百分比类转移表格数据实验收敛可视化
- en: 'As a final step of our transfer experiment, let’s dig even deeper into its
    performance by comparing predicted labels for the test set with true labels. This
    can be done via the following code snippet:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作为转移实验的最后一步，让我们通过比较测试集的预测标签和真实标签来深入了解其性能。可以通过以下代码片段来完成这个任务：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Predicts classes
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶预测类别
- en: ❷ Converts probabilities to class labels
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷将概率转换为类标签
- en: ❸ Inspects
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查
- en: 'The produced output follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '生成的输出如下： '
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We see that the fine-tuned model has predicted every single example correctly,
    further validating our transfer learning experiment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，微调模型已经完全正确地预测了每个例子，进一步验证了我们的迁移学习实验。
- en: In closing, remember that the SIMOn framework can be applied to arbitrary input
    text, not just tabular data, via the adaptation procedure described in section
    6.1.2\. Several application examples have yielded promising results.[¹](#pgfId-1099840)
    Hopefully, the exercise in this section has sufficiently prepared you to deploy
    it in your own classification applications, as well as to adapt the resulting
    classifiers to new situations via transfer learning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要记住，通过在6.1.2节中描述的适应过程，SIMOn框架可以应用于任意输入文本，而不仅仅是表格数据。几个应用示例取得了有希望的结果。[¹](#pgfId-1099840)希望本节的练习已经充分准备您在自己的分类应用程序中部署它，并通过迁移学习将生成的分类器适应新情况。
- en: We will now proceed to exploring the application of ELMo to the fake news classification
    example.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续探讨将ELMo应用于虚假新闻分类示例的情况。
- en: 6.2 Embeddings from Language Models (ELMo)
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2来自语言模型的嵌入（ELMo）
- en: As was mentioned briefly in the previous chapter, Embeddings from Language Models
    (ELMo) is arguably one of the most popular early pretrained language models associated
    with the ongoing NLP transfer learning revolution. It shares some similarities
    with SIMOn, in that it is also composed of character-level CNNs and bidirectional
    LSTMs. Please refer to figure 5.2, duplicated here, for a bird’s-eye view of these
    modeling components.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章节简要提到的，来自语言模型的嵌入（ELMo）可以说是与正在进行的NLP迁移学习革命相关的最受欢迎的早期预训练语言模型之一。它与SIMOn有一些相似之处，因为它也由字符级CNN和双向LSTM组成。请参考图5.2，这里重复了一遍，以便鸟瞰这些建模组件。
- en: '![06_03_05_02](../Images/06_03_05_02.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![06_03_05_02](../Images/06_03_05_02.png)'
- en: Figure 5.2 (Duplicated) Visualizing ELMo architecture in the context of the
    tabular column-type classification example
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2（重复）在表格列类型分类示例的背景下可视化ELMo架构
- en: Also review figure 6.1, particularly the more detailed (than what is shown in
    figure 5.2) equivalent pictorial representation of bi-LSTMs, which are composed
    of a forward model and a backward model. If you have been following this book
    chronologically, then you have also applied ELMo to the problems of spam detection
    and IMDB movie review sentiment classification in section 3.2.1\. As you have
    probably picked up by now, ELMo produces word representations that are a function
    of the entire input sentence. In other words, the model is a word embedding that
    is context-aware.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 还要查看图6.1，特别是比图5.2更详细的相当于双向LSTM的图示。如果您按照本书的时间顺序阅读，那么您也已经在3.2.1节中将ELMo应用于垃圾邮件检测和IMDB电影评论情感分类问题。正如您现在可能已经了解到的那样，ELMo产生的词表示是整个输入句子的函数。换句话说，该模型是上下文感知的词嵌入。
- en: This section takes a deeper dive into the modeling architecture of ELMo. What
    exactly does ELMo do to the input text to build context and disambiguate? To answer
    this, bidirectional language modeling with ELMo is first expounded, followed by
    applying the model to the fake news detection problem to make things concrete.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入探讨了ELMo的建模架构。ELMo确切地对输入文本做了什么来构建上下文和消岐？为了回答这个问题，首先介绍了使用ELMo进行双向语言建模，接着将该模型应用于虚假新闻检测问题以使问题具体化。
- en: 6.2.1 ELMo bidirectional language modeling
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 ELMo双向语言建模
- en: Recall that language modeling attempts to model the probability of a token,
    usually a word, appearing in a given sequence. Consider a scenario where we have
    a sequence of *N* tokens, for instance, words in a sentence or paragraph. A word-level
    forward language model computes the joint probability of the sequence by taking
    a product of the probabilities of every token in the sequence conditioned on its
    left-to-right history, as visualized in figure 6.4\. Consider the short sentence,
    “*You can be.*” According to the formula in figure 6.4, the forward language model
    computes the probability of the sentence as the probability of the first word
    in a sentence being “*You*,” times the probability of the second word being “*can*,”
    given that the first word is “*You,*” times the probability of the third word
    being “*be*,” given that the first two are “*You can*.”
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，语言建模试图对一个令牌的出现概率进行建模，通常是一个词，在给定序列中出现。考虑这样一个情景，我们有一个*N*令牌的序列，例如，句子或段落中的单词。一个以单词为单位的前向语言模型通过取序列中每个令牌在其从左到右的历史条件下的概率的乘积来计算序列的联合概率，如图6.4所示。考虑这个简短的句子，“*你可以*”。根据图6.4中的公式，前向语言模型计算句子的概率为第一个词在句子中是“*你*”的概率乘以第二个词是“*可以*”的概率，假设第一个词是“*你*”，再乘以第三个词是“*是*”的概率，假设前两个词是“*你可以*”。
- en: '![06_04](../Images/06_04.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![06_04](../Images/06_04.png)'
- en: Figure 6.4 Forward language model equation
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 前向语言模型方程
- en: A word-level backward language model does the same thing, but in reverse, as
    expressed by the equation in figure 6.5\. It models the joint probability of the
    sequence by taking a product of the probabilities of every token conditioned on
    the right-to-left token history.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一个以单词为单位的反向语言模型做的是相同的事情，但是反过来，如图6.5中的方程所示。它通过对每个令牌在右到左令牌历史条件下的概率的乘积来建模序列的联合概率。
- en: '![06_05](../Images/06_05.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![06_05](../Images/06_05.png)'
- en: Figure 6.5 Backward language model equation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 反向语言模型方程。
- en: Again, consider the short sentence, “*You can be.*” According to the formula
    in figure 6.5, the backward language model computes the probability of the sentence
    as the probability of the last word in a sentence being “*be*,” times the probability
    of the second word being “*can*,” given that the last word is “*be,*” times the
    probability of the first word being “*You*,” given that the other two are “*can
    be*.”
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑这个简短的句子，“*你可以*”。根据图6.5中的公式，反向语言模型计算句子的概率为最后一个词在句子中是“*是*”的概率乘以第二个词是“*可以*”的概率，假设最后一个词是“*是*”，再乘以第一个词是“*你*”的概率，假设其他两个词是“*可以是*”。
- en: A bidirectional language model combines the forward and backward models. The
    ELMo model specifically looks to maximize the joint log likelihood of the two
    directions—the quantity shown in figure 6.6\. Note that although separate parameters
    are maintained for the forward and backward language models, the token vectors
    and the final layer parameters are shared between the two. This is an example
    of the soft-parameter sharing multitask learning scenario discussed in chapter
    4.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个双向语言模型结合了前向和后向模型。ELMo模型特别寻求最大化两个方向的联合对数似然——在图6.6中显示的量。请注意，尽管为前向和后向语言模型保留了单独的参数，但令牌向量和最终层参数在两者之间是共享的。这是第4章讨论的软参数共享多任务学习场景的一个例子。
- en: '![06_06](../Images/06_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![06_06](../Images/06_06.png)'
- en: Figure 6.6 The joint bidirectional language modeling (LM) objective equation
    that ELMo uses to build bidirectional context for any given token in a sequence
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 ELMo用于为序列中的任何给定令牌构建双向上下文的联合双向语言建模（LM）目标方程
- en: The ELMo representation for each token is derived from the internal states of
    the bidirectional LSTM language model. For any given task, it is a linear combination
    of the internal states of all LSTM layers (in both directions) corresponding to
    the target token.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 每个令牌的ELMo表示来自双向LSTM语言模型的内部状态。对于任何给定任务，它是与目标令牌对应的所有LSTM层（两个方向上的）的内部状态的线性组合。
- en: Combining all internal states, versus using just the top layer, as, for instance,
    in SIMOn, offers significant advantages. Although the lower layers of the LSTM
    enable good performance on syntax-based tasks, such as part-of-speech tagging,
    the higher layers enable context-dependent disambiguation in meaning. Learning
    a linear combination for each task across both types of representations allows
    the final model to select the kind of signal it needs for the task at hand.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内部状态组合在一起，与仅使用顶层不同，例如在SIMOn中，具有显著的优势。尽管LSTM的较低层使得在基于句法的任务（如词性标注）上具有良好的性能，但较高层使得在含义上进行上下文相关的消歧。学习每个任务在这两种表示类型之间的线性组合，允许最终模型选择它需要的任务类型的信号。
- en: 6.2.2 Application to fake news detection
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 应用于假新闻检测的模型
- en: Now let’s proceed to build an ELMo model for the fake news classification dataset
    we assembled in section 5.2\. For readers who have already worked through chapters
    3 and 4, this is our second application of the ELMo modeling framework to a practical
    example.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续构建一个ELMo模型，用于我们在第5.2节中组装的假新闻分类数据集。对于已经阅读过第3章和第4章的读者来说，这是ELMo建模框架对实际示例的第二个应用。
- en: 'Because we have already built the ELMo model, we will be able to reuse some
    of the functions that we already defined in chapter 3\. Refer to listing 3.4,
    which employs the TensorFlow Hub platform to load the weights made available by
    ELMo’s authors and builds a Keras-ready model using them via the class `ElmoEmbeddingLayer`.
    Having defined this class, we can train our required ELMo model for fake news
    detection via the following code (slightly modified from listing 3.6):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经构建了ELMo模型，我们将能够重用一些在第3章中已经定义的函数。请参考第3.4节的代码，该代码利用TensorFlow Hub平台加载了ELMo作者提供的权重，并使用`ElmoEmbeddingLayer`类构建了一个适用于Keras的模型。定义了这个类之后，我们可以通过以下代码训练我们所需的用于假新闻检测的ELMo模型（与第3.6节稍作修改的代码）：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ A new layer outputting 256-dimensional feature vectors
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输出256维特征向量的新层
- en: ❷ The classification layer
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分类层
- en: ❸ Loss, metric, and optimizer choices
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 损失、度量和优化器的选择
- en: ❹ Shows the model architecture for inspection
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 显示用于检查的模型架构
- en: ❺ Fits the model for 10 epochs
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将模型拟合10个epochs
- en: 'Let’s look at the model structure closer, which is output by the `model.summary()`
    statement in the previous code snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地查看模型结构，该结构由前述代码片段中的`model.summary()`语句输出：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The layers `dense_1` and `dense_2` are the new fully connected layers added
    on top of the pretrained embedding produced by listing 3.4\. The pretrained embedding
    is the `elmo_embedding_layer_1`. Note that it has four trainable parameters, as
    shown by the printed model summary. The four parameters are the weights in the
    linear combination of internal bi-LSTM states described in the previous subsection.
    If you use the TensorFlow Hub approach to using the pretrained ELMo model as we
    have done here, the rest of the ELMo model is not trainable. It is possible, however,
    to build a fully trainable TensorFlow-based ELMo model using another version of
    the model repository.[²](#pgfId-1099942)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`dense_1`和`dense_2`层是添加到第3.4节产生的预训练嵌入之上的新的全连接层。预训练嵌入是`elmo_embedding_layer_1`。请注意，打印的模型摘要显示它有四个可训练参数。这四个参数是前面子节中描述的内部双向LSTM状态的线性组合中的权重。如果您像我们这样使用TensorFlow
    Hub方法使用预训练的ELMo模型，则ELMo模型的其余部分不可训练。然而，可以使用模型库的另一个版本构建一个完全可训练的基于TensorFlow的ELMo模型。'
- en: The convergence result achieved when we executed the previous code on the fake
    news dataset is shown in figure 6.7\. We see that an accuracy over 98% is achieved.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在假新闻数据集上执行前述代码时所达到的收敛结果如图6.7所示。我们看到，达到了超过98%的准确率。
- en: '![06_07](../Images/06_07.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![06_07](../Images/06_07.png)'
- en: Figure 6.7 Convergence results for the ELMo model trained on the fake news dataset
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 ELMO模型在假新闻数据集上训练的收敛结果
- en: 6.3 Universal Language Model Fine-Tuning (ULMFiT)
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 通用语言模型微调（ULMFiT）
- en: Around the time techniques like ELMo were emerging, it was recognized that NLP
    language models were different from computer vision models in various ways. Applying
    the same techniques from computer vision to the fine-tuning of NLP language models
    came with drawbacks. For instance, the process often suffered from catastrophic
    forgetting of pretrained knowledge, as well as overfitting on the new data. The
    impact of this was the loss of any existing pretrained knowledge during training,
    as well as poor generalizability of the resulting model on any data outside the
    training set. The method known as Universal Language Model Fine-Tuning (ULMFiT)
    developed a set of techniques for fine-tuning NLP language models to alleviate
    some of these drawbacks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在ELMo等技术出现的时候，人们意识到NLP语言模型在各种方面与计算机视觉模型不同。将计算机视觉的相同技术应用于微调NLP语言模型会带来一些不利之处。例如，这个过程常常遭受到预训练知识的灾难性遗忘，以及在新数据上的过度拟合。这导致的后果是在训练期间失去了任何现存的预训练知识，以及在训练集之外的任何数据上的模型通用性差。名为通用语言模型微调（ULMFiT）的方法开发了一套技术，用于微调NLP语言模型以减轻这些不利之处。
- en: More specifically, the method stipulates some variable learning-rate schedules
    for the various layers of the general pretrained language model during fine-tuning.
    It also specifies a set of techniques for fine-tuning the task-specific layers
    of the language model for more efficient transfer. Although these techniques were
    demonstrated by the authors in the context of classification and LSTM-based language
    models, the techniques are meant to be more general.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，该方法规定了在微调过程中对一般预训练语言模型的各层使用一些可变的学习率安排。它还为微调语言模型的任务特定层提供了一套技术，以实现更高效的迁移。尽管这些技术是作者在分类和基于LSTM的语言模型的背景下演示的，但这些技术意在更一般的情况下使用。
- en: We touch on the various techniques introduced by this method in this section.
    However, we do not actually implement it in the code in this section. We delay
    the numerical investigation of ULMFiT until chapter 9, where we explore various
    pretrained model adaptation techniques for new scenarios. We will do that using
    the fast.ai library,[³](#pgfId-1099961) which was written by the ULMFiT authors.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们会涉及到该方法引入的各种技术。但是，我们并没有在本节中实际实现它的代码。我们将延迟对ULMFiT的数值研究，直到第9章，在那里我们将探讨各种预训练模型适应新场景的技术。我们将使用由ULMFiT作者编写的fast.ai库,[³](#pgfId-1099961)来进行这项工作。
- en: For the procedures discussed next, it is assumed that we have a language model
    pretrained on a large, general text corpus, such as Wikipedia.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论接下来的程序，我们假定我们有一个在大型普通文本语料库（如维基百科）上预训练的语言模型。
- en: 6.3.1 Target task language model fine-tuning
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 目标任务语言模型微调
- en: No matter how general the initial pretrained model is, the final deployment
    stage will likely involve data from a different distribution. This motivates us
    to fine-tune the general pretrained model on a new, smaller dataset from the new
    distribution to adapt to the new scenario. The authors of ULMFiT found that the
    techniques of *discriminative fine-tuning* and *slanted learning rates* alleviate
    the twin problems of overfitting and catastrophic forgetting experienced by researchers
    when doing this.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 无论最初的预训练模型有多普通，最后的部署阶段可能会涉及来自不同分布的数据。这促使我们在新分布的小型数据集上对一般预训练模型进行微调，以适应新场景。ULMFiT的作者发现，*辨别性微调*和*倾斜学习率*的技术减轻了研究人员在此过程中遇到的过拟合和灾难性遗忘的双重问题。
- en: Discriminative fine-tuning stipulates that because different layers of the language
    model capture different information, they should be fine-tuned at different rates.
    Particularly, the authors found empirically that it was beneficial to first fine-tune
    the very last layer and note its optimal learning rate. Once they obtain that
    base rate, they divide this optimal rate by the number 2.6, which yields the suggested
    rate for the layer right below it. Successively dividing by the same factor yields
    progressively lower rates for each of the lower layers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 辨别性微调规定，由于语言模型的不同层捕捉了不同的信息，因此它们应该以不同的速率进行微调。特别是，作者们经验性地发现，首先微调最后一层并注意其最佳学习率是有益的。一旦他们得到了这个基本速率，他们将这个最佳速率除以2.6，这样就得到了以下层所建议的速率。通过以相同的因数进行逐步除法，可以得到越来越低的下层速率。
- en: When adapting the language model, we want the model to converge quickly in the
    beginning, followed by a slower refinement stage. The authors found that the best
    way to achieve this is to use a slanted triangular learning rate, which linearly
    increases the learning rate and then linearly decays it. In particular, they increase
    the rate linearly for the initial 10% of the iterations, up to a maximum value
    of 0.01\. Their suggested rate schedule is illustrated in figure 6.8 for the case
    of 10,000 total iterations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在适应语言模型时，我们希望模型在开始阶段快速收敛，然后进入较慢的细化阶段。作者发现，实现这一点的最佳方法是使用倾斜三角形学习率，该学习率线性增加，然后线性衰减。特别地，他们在迭代的初始10%期间线性增加速率，直到最大值为0.01。他们建议的速率时间表如图6.8所示，针对总迭代次数为10,000的情况。
- en: '![06_08](../Images/06_08.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![06_08](../Images/06_08.png)'
- en: Figure 6.8 Suggested ULMFiT rate schedule for the case of 10,000 total iterations.
    The rate increases linearly for 10% of the total number of iterations (i.e., 1,000),
    up to a maximum of 0.01, and then decreases linearly afterward to 0.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 建议的ULMFiT速率时间表，适用于总迭代次数为10,000的情况。速率线性增加了总迭代次数的10%（即1,000），最高值为0.01，然后线性下降至0。
- en: 6.3.2 Target task classifier fine-tuning
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 目标任务分类器微调
- en: 'In addition to techniques for fine-tuning the language model on a small dataset
    representing the data distribution for the new scenario, ULMFiT provides two techniques
    for refining the task-specific layers: *concat pooling* and *gradual unfreezing*.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在小型数据集上微调语言模型以表示新场景的数据分布的技术外，ULMFiT还提供了两种用于优化任务特定层的技术：*concat pooling*和*gradual
    unfreezing*。
- en: At the time ULMFiT was developed, it was standard practice to pass the hidden
    state of the final unit of an LSTM-based language model to the task-specific layer.
    The authors instead recommend concatenating these final hidden states with the
    max-pooled and mean-pooled hidden states of all time steps (as many of them as
    can fit in memory). In the bidirectional context, they do this separately for
    forward and backward language models and average predictions. This process, which
    they call *concat pooling*, performs a similar function to the bidirectional language
    modeling approach described for ELMo.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在ULMFiT开发时，将基于LSTM的语言模型的最终单元的隐藏状态传递给任务特定层是标准做法。作者建议将这些最终隐藏状态与所有时间步的最大池化和平均池化隐藏状态串联起来（尽可能多地适应内存）。在双向上下文中，他们分别为前向和后向语言模型执行此操作，并平均预测结果。他们称之为*concat
    pooling*的过程与ELMo描述的双向语言建模方法执行类似的功能。
- en: In order to reduce the risks of catastrophic forgetting when fine-tuning, the
    authors suggest unfreezing and tuning gradually. This process starts with the
    last layer, which contains the least general knowledge and is the only one unfrozen
    and refined at the first epoch. At the second epoch, an additional layer is unfrozen,
    and the process is repeated. The process continues until all task-specific layers
    are unfrozen and fine-tuned at the last iteration of this gradual unfreezing process.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少微调时灾难性遗忘的风险，作者建议逐渐解冻和调整。这个过程从最后一层开始，该层包含最少的通用知识，并且在第一个epoch时是唯一解冻和精炼的层。在第二个epoch中，将解冻一个额外的层，并重复该过程。该过程持续到所有任务特定层都在该渐进解冻过程的最后迭代中解冻和微调。
- en: As a reminder, these techniques will be explored in the code in chapter 9, which
    will cover various adaptation strategies.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这些技术将在第9章的代码中探讨，该章节将涵盖各种适应策略。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Character-level models, as opposed to word-level models, can handle misspellings
    and other social media features, such as emoticons and niche vernacular.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与词级模型相反，字符级模型可以处理拼写错误和其他社交媒体特征，例如表情符号和小众俚语。
- en: Bidirectional language modeling is key for building word embeddings that are
    aware of their local context.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向语言建模对于构建具有意识到其局部上下文的词嵌入至关重要。
- en: SIMOn and ELMo both employ character-level CNNs and bi-LSTMs, with the latter
    helping to achieve bidirectional context-building.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SIMOn和ELMo都使用字符级CNN和双向LSTM，后者有助于实现双向上下文建模。
- en: Adapting a pretrained language model to a new scenario may benefit from fine-tuning
    the different layers of the model at different rates, which should initially increase
    and then decrease according to a slanted triangular schedule.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预训练语言模型适应新场景可能会受益于对模型的不同层进行不同速率的微调，这应根据倾斜三角形时间表首先增加然后减少。
- en: Adapting a task-specific layer to a new scenario may benefit from unfreezing
    and fine-tuning the different layers gradually, starting from the last layer and
    unfreezing increasingly more until all layers are refined.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任务特定的层适应新情境可能会受益于逐渐解冻和微调不同的层，从最后一层开始解冻，逐渐解冻更多层，直到所有层都被精细调整。
- en: ULMFiT employs discriminative fine-tuning, slanted triangular learning rates,
    and gradual unfreezing to alleviate overfitting and catastrophic forgetting when
    fine-tuning language models.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ULMFiT采用辨别微调，倾斜三角形学习率和渐进解冻来缓解微调语言模型时的过拟合和灾难性遗忘。
- en: 1. N. Dhamani et al., “Using Deep Networks and Transfer Learning to Address
    Disinformation,” AI for Social Good ICML Workshop (2019).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 1. N. Dhamani 等人，“使用深度网络和迁移学习解决虚假信息问题”，AI for Social Good ICML Workshop（2019年）。
- en: 2. [https://github.com/allenai/bilm-tf](https://github.com/allenai/bilm-tf)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 2. [https://github.com/allenai/bilm-tf](https://github.com/allenai/bilm-tf)
- en: 3. [http://nlp.fast.ai/ulmfit](http://nlp.fast.ai/ulmfit)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 3. [http://nlp.fast.ai/ulmfit](http://nlp.fast.ai/ulmfit)
