- en: Appendix B. Survey of existing solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B. 现有解决方案调查
- en: Deep learning systems are huge undertakings when implemented from scratch. In
    some situations, special requirements may warrant spending the extra effort to
    build a deep learning system from scratch. In other cases, given finite resources
    and time, it may make sense to use existing components, or even systems as a whole,
    and tailor them to your own needs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始实施深度学习系统是一项庞大的工作。在某些情况下，特殊需求可能需要额外的努力来从头开始构建深度学习系统。在其他情况下，鉴于有限的资源和时间，使用现有组件，甚至整个系统，并将其定制为符合自己需求可能是有意义的。
- en: The purpose of this appendix is to examine a few deep learning systems that
    have been implemented by different cloud vendors and open source communities.
    These operations range from serverless deployment to custom service container
    deployment. You will gain a sense of which pieces of these operations you can
    use to design your own project by comparing them with our reference architecture
    and highlighting their similarities and differences.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录的目的是审查几个由不同云供应商和开源社区实施的深度学习系统。这些操作范围从无服务器部署到自定义服务容器部署。通过将它们与我们的参考架构进行比较，并突出它们的相似之处和不同之处，你将了解到哪些操作可以用来设计自己的项目。
- en: If you want to see a quick summarized comparison of every solution that we will
    cover, feel free to skip ahead to section B.5\. Also, for your convenience, the
    reference architecture introduced in section 1.2.1 is reposted in figure B.1.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想看到我们将要涵盖的每个解决方案的快速摘要比较，请随意跳转到 B.5 节。另外，为了方便起见，在第 1.2.1 节介绍的参考架构在图 B.1 中重新发布。
- en: '![](../Images/B-1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-1.png)'
- en: Figure B.1 An overview of a typical deep learning system that includes basic
    components to support a deep learning development cycle. This reference architecture
    can be used as a starting point and further tailored.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.1 典型深度学习系统概述，包括支持深度学习开发周期的基本组件。这个参考架构可以作为一个起点，并进一步定制。
- en: B.1 Amazon SageMaker
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 亚马逊 SageMaker
- en: Amazon SageMaker is the umbrella term for its collection of artificial intelligence
    products that can be used together to form a complete deep learning system. In
    this section, we will review the suite of products and see how they compare with
    our key components. As mentioned at the beginning of this section, we make these
    comparisons so that you will learn what product will best help to build your own
    system.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 是其人工智能产品系列的总称，可以一起使用形成完整的深度学习系统。在本节中，我们将回顾产品套件，并查看它们与我们的关键组件的比较。正如本节开头所提到的，我们进行这些比较是为了让你了解哪种产品能最好地帮助构建你自己的系统。
- en: B.1.1 Dataset management
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.1 数据集管理
- en: Amazon SageMaker does not offer a dataset management component that provides
    a unified interface to help manage the complex interaction of data preparation
    with the different types of users of a deep learning system. Amazon, however,
    does provide a collection of data storage, transformation, and querying solutions
    that can be used to build a data management component.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 没有提供数据集管理组件，该组件提供统一的接口来帮助管理数据准备与深度学习系统不同类型用户的复杂交互。然而，亚马逊提供了一系列数据存储、转换和查询解决方案，可以用来构建数据管理组件。
- en: It is possible to build a data management component that collects raw data for
    Amazon S3, an object storage product. Metadata tagging can be backed by AWS Glue
    Data Catalog, which can be used by AWS Glue ETL for further processing into datasets
    that can be used for training. After reading chapter 2, you should be able to
    identify how you can use these Amazon products to build your own data management
    component.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可以构建一个数据管理组件，用于收集 Amazon S3 的原始数据，这是一款对象存储产品。元数据标记可以由 AWS Glue 数据目录支持，该目录可以由
    AWS Glue ETL 用于进一步处理成可用于训练的数据集。阅读第二章后，你应该能够确定如何使用这些亚马逊产品构建自己的数据管理组件。
- en: B.1.2 Model training
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.2 模型训练
- en: Amazon SageMaker supports both built-in algorithms and externally provided custom
    code for training deep learning models. It also supports containers for training
    runs. It exposes an API that can be called to launch training jobs on demand.
    This is largely similar to the compute backend that powers the training component
    of a deep learning system, which is covered in this book. To implement the resource
    management portion of the training component, you may use the existing tools provided
    by Amazon, such as assigning resource limits and policies to different AWS Identity
    and Access Management (IAM) users or roles. If your organization requires extra
    control or sophistication or already has an identity provider implementation,
    you may need to spend more time building a custom solution. After reading chapters
    3 and 4, you should be able to figure out how you can build your own training
    component with existing Amazon tools.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker支持内置算法和提供外部自定义代码进行深度学习模型训练。它还支持用于训练运行的容器。它公开了一个API，可以通过调用来启动按需训练作业。这与驱动深度学习系统培训组件的计算后端非常相似，本书也对此进行了介绍。要实现训练组件的资源管理部分，您可以使用Amazon提供的现有工具，例如为不同的AWS身份和访问管理（IAM）用户或角色分配资源限制和策略。如果您的组织需要额外的控制或复杂性，或者已经有身份提供者实现，您可能需要花费更多的时间构建自定义解决方案。阅读第3章和第4章后，您应该能够弄清楚如何使用现有的Amazon工具来构建自己的训练组件。
- en: B.1.3 Model serving
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.3模型服务
- en: Amazon SageMaker, in its most basic form, supports deploying trained models
    as web services that are accessible over the internet. For scaling out to host
    multiple models without deploying each of them to separate endpoints, SageMaker
    provides a multimodel endpoint that also comes with configurable model-caching
    behavior. These tools can come in handy if they fit your bill. As of the time
    of this writing, SageMaker supports multi-container endpoints and serial inference
    pipelines, which are similar to the serving architecture and DAG support described
    in this book. Chapters 6 and 7 review model-serving principles so that you will
    understand what existing tools you can use and how you can build your own when
    you encounter limitations with existing tools.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最基本的形式下，Amazon SageMaker能够支持将训练后的模型部署为可以通过互联网访问的Web服务。为了扩展多个模型的部署而无需将它们部署到单独的端点，SageMaker提供了一个多模型端点，同时也配备了可配置的模型缓存行为。如果这些工具符合您的需求，它们会很有用。截至本文撰写时，SageMaker支持多容器端点和串行推理流水线，这与本书中描述的服务架构和DAG支持类似。第6章和第7章回顾了模型服务原则，以便您了解现有工具和在遇到现有工具的限制时如何构建自己的工具。
- en: B.1.4 Metadata and artifacts store
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.4元数据和工件存储
- en: As the component that centers around trained models, it is not surprising to
    see cloud vendors make products that do just that. The SageMaker Model Registry
    provides functionalities that map to many key concepts of the metadata and artifacts
    store of a deep learning system. For example, metadata, such as training metrics
    of a model and model versions, can be tracked using Model Registry. It does not,
    however, provide a storage solution for artifacts in the same component. You can
    easily build an interface on top of Model Registry and other Amazon storage products
    to provide the artifact storage aspect of this component.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为以受过训练的模型为中心的组件，云供应商推出了专门的产品是不足为奇的。SageMaker模型注册表提供了许多元数据和深度学习系统工件存储的关键概念相对应的功能。例如，可以使用模型注册表跟踪模型的训练度量和模型版本等元数据。然而，它并不提供同一组件中的工件存储解决方案。您可以轻松地在模型注册表和其他Amazon存储产品之上构建接口，以提供这个组件的工件存储方面。
- en: Another important type of metadata that is tracked between artifacts is their
    lineage information. SageMaker provides ML Lineage Tracking as a separate feature
    that keeps tabs on this information automatically.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在工件之间跟踪的另一种重要元数据类型是它们的谱系信息。SageMaker提供了ML谱系跟踪作为一个独立的功能，它会自动跟踪这些信息。
- en: In chapter 8, we will discuss key concerns in building the metadata and artifacts
    store. After reading the chapter, you will understand the design principles behind
    this component and how existing products can help you to build your own quickly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章中，我们将讨论构建元数据和工件存储的关键问题。阅读本章后，您将了解这个组件背后的设计原则以及现有的产品如何帮助您快速构建自己的组件。
- en: B.1.5 Workflow orchestration
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.5工作流编排
- en: On Amazon SageMaker, you can use the Model Building Pipelines product to manage
    your workflows, or *pipelines* (which is SageMaker terminology). Using this product,
    you can execute a set of actions, such as data preparation steps, training steps,
    and model validation steps, as one unit with a predefined order in an arbitrary
    fashion. To allow multiple types of users to work on the same problem, SageMaker
    also provides a Project product to help organize relationships between workflows,
    code versions, lineage information, and different access permissions for each
    user type.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在亚马逊 SageMaker 上，您可以使用 Model Building Pipelines 产品来管理工作流程或*管道*（这是 SageMaker
    的术语）。使用此产品，您可以按照预定义的顺序以任意方式执行一组操作，例如数据准备步骤、训练步骤和模型验证步骤，作为一个单元。为了允许多种类型的用户共同解决同一个问题，SageMaker
    还提供了一个 Project 产品，帮助组织工作流程、代码版本、血统信息以及每种用户类型的不同访问权限之间的关系。'
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system, as well
    as its role in an enterprise environment.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在第 9 章中，我们将介绍如何使用工作流管理器启用不同的训练模式。阅读本章后，您将了解到在深度学习系统中设计和实用工作流管理器的原因，以及其在企业环境中的作用。'
- en: B.1.6 Experimentation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   B.1.6 实验'
- en: Amazon SageMaker provides a feature called Experiments that tags experimental
    runs with relevant tracking information and metrics. Indeed, this type of tracking
    information is also a kind of metadata, which is important to users of a deep
    learning system who need to evaluate the performance of different combinations
    of data input, training algorithms, and hyperparameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '-   亚马逊 SageMaker 提供了一个名为 Experiments 的功能，它使用相关的跟踪信息和指标标记实验运行。事实上，这种跟踪信息也是一种元数据，对于需要评估不同数据输入、训练算法和超参数组合性能的深度学习系统用户来说，这种元数据是很重要的。'
- en: B.2 Google Vertex AI
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   B.2 Google Vertex AI'
- en: Google Vertex AI, a combination of Google’s AI platform offering and its AutoML
    product, provides a collection of tools and services that can be used as a deep
    learning system. In this section, we will review its offerings and compare them
    with key components introduced in this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '-   Google Vertex AI 是 Google AI 平台提供的一项功能与其 AutoML 产品相结合的产品，提供了一系列可用作深度学习系统的工具和服务。在本节中，我们将回顾其提供的功能，并将其与本书介绍的关键组件进行比较。'
- en: B.2.1 Dataset management
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   B.2.1 数据集管理'
- en: Google Vertex AI provides a simple API to manage datasets, though you must first
    upload your object data to Google Cloud Storage and then upload metadata and annotation
    files that reference object data in Google Cloud Storage via the Vertex AI API.
    The dataset API is similar across different types of datasets (images, text, video,
    etc.) that provide a unified experience to developers. The API, however, does
    not provide versioning information and other lineage tracking information. In
    chapter 2, we explore core data management principles. After reading the chapter,
    you will be able to compare existing solutions and extend them or build from scratch
    for your own needs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '-   Google Vertex AI 提供了一个简单的 API 来管理数据集，尽管您必须首先将对象数据上传到 Google Cloud Storage，然后通过
    Vertex AI API 上传引用 Google Cloud Storage 中对象数据的元数据和注释文件。数据集 API 在提供给开发人员时提供了统一的体验，尽管不同类型的数据集（图像、文本、视频等）之间的
    API 类似。然而，该 API 并未提供版本信息和其他血统跟踪信息。在第 2 章中，我们探讨了核心数据管理原则。阅读本章后，您将能够比较现有解决方案，并针对自己的需求扩展它们或从头开始构建。'
- en: B.2.2 Model training
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   B.2.2 模型训练'
- en: Google Vertex AI supports training with Docker containers. It provides prebuilt
    training containers for those who do not need further customization and also supports
    custom-built training containers for those who require more than what the prebuilt
    flavor provides. Its training service exposes an interface that allows the launching
    of training runs on either a single node or on multiple nodes for distributed
    training. When running distributed training, Vertex AI provides additional support
    with reduction to accelerate training. In chapters 3 and 4, we explore these features
    and the principles behind them. After reading these chapters, you will be able
    to determine what existing offerings you can use, how to extend them if you need
    more, and how to build it from scratch if you have more specific requirements.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI 支持使用 Docker 容器进行训练。它为那些不需要进一步定制的用户提供预构建的训练容器，并支持为那些需要比预构建版本提供的内容更多的用户提供自定义构建的训练容器。其训练服务公开了一个接口，允许在单个节点或多个节点上启动训练运行以进行分布式训练。在运行分布式训练时，Vertex
    AI 提供额外的支持，以加速训练过程。在第 3 章和第 4 章中，我们探讨了这些功能及其背后的原理。阅读完这些章节后，您将能够确定可以使用哪些现有产品，如何在需要更多功能时扩展它们，以及如何根据具体需求从头开始构建它们。
- en: B.2.3 Model serving
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.3 模型服务
- en: Google Vertex AI supports serving online inference requests to trained models,
    either with prebuilt inference containers or custom inference containers. Trained
    models are decoupled from containers and must be deployed with compute resources
    to form an endpoint that can serve online inference requests. Vertex AI supports
    deploying one model to multiple endpoints and supports deploying multiple models
    to a single endpoint. Different from other solutions that support various model
    types, deploying multiple models to a single endpoint in Vertex AI is primarily
    used for canarying new model versions using split traffic patterns. In Vertex
    AI, if you train a Vertex AI video model, it cannot be made to serve online inference
    requests.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI 支持向经过训练的模型提供在线推断请求服务，可以使用预构建推断容器或自定义推断容器。训练过的模型与容器分离，并且必须使用计算资源部署，以形成可以提供在线推断请求服务的端点。Vertex
    AI 支持将一个模型部署到多个端点，并支持将多个模型部署到单个端点。与支持各种模型类型的其他解决方案不同，在 Vertex AI 中，将多个模型部署到单个端点主要用于使用分流流量模式来执行新模型版本的金丝雀发布。在
    Vertex AI 中，如果您训练了一个 Vertex AI 视频模型，则无法使其提供在线推断请求服务。
- en: In chapters 6 and 7, we learn the fundamental principles behind model serving.
    After completing these chapters, you will have a good understanding of model serving
    and will be able to decide whether existing solutions are sufficient for your
    needs. You will be able to build your own, as well as understand how to operate
    a model server efficiently and at scale.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 6 章和第 7 章中，我们学习了模型服务背后的基本原理。完成这些章节后，您将对模型服务有很好的理解，并能够决定现有解决方案是否满足您的需求。您将能够构建自己的解决方案，并了解如何高效且规模化地运行模型服务器。
- en: B.2.4 Metadata and artifacts store
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.4 元数据和工件存储
- en: Vertex ML Metadata is Google’s metadata store solution that can be used in a
    deep learning system. It uses a graph to describe relationships between artifacts
    such as datasets, training runs, and trained models. Each node and edge in the
    graph can be tagged with a list of key-value pairs to describe any metadata. When
    used properly, this can provide comprehensive lineage information for everything
    in a deep learning system.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex ML Metadata 是谷歌的元数据存储解决方案，可用于深度学习系统中。它使用图来描述诸如数据集、训练运行和训练模型等工件之间的关系。图中的每个节点和边都可以用一系列键值对标记，以描述任何元数据。当正确使用时，这可以为深度学习系统中的所有内容提供全面的血统信息。
- en: Artifacts are not stored directly in Vertex ML Metadata. Artifacts are stored
    in Google Cloud Storage. Vertex ML Metadata uses a URI reference to point to these
    artifacts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 工件不直接存储在 Vertex ML Metadata 中。工件存储在 Google Cloud Storage 中。Vertex ML Metadata
    使用 URI 引用指向这些工件。
- en: In chapter 8, we will explore a similar approach in building a metadata and
    artifacts store, where both can be managed through a single, unified interface.
    After reading the chapter, you will be able to tell how to leverage and extend
    existing solutions for your needs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 章中，我们将探讨一种类似的方法，即构建元数据和工件存储，通过单一统一的界面可以管理两者。阅读完本章后，您将能够了解如何利用和扩展现有解决方案以满足您的需求。
- en: B.2.5 Workflow orchestration
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.5 工作流编排
- en: With Google, you can use Vertex Pipelines to manage and operate your deep learning
    workflows. You can represent data preparation and training operations as steps
    in a pipeline. In Vertex Pipelines, steps are organized as nodes in a directed
    acyclic graph. Each step of the pipeline is implemented by a container. A run
    of a pipeline is in fact an orchestration of executions of containers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Google，您可以使用 Vertex Pipelines 来管理和操作您的深度学习工作流程。您可以将数据准备和训练操作表示为管道中的步骤。在 Vertex
    Pipelines 中，步骤被组织为有向无环图中的节点。管道的每个步骤由一个容器实现。管道的运行实际上是对容器执行的编排。
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system and its
    role in an enterprise environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 9 章中，我们将介绍如何使用工作流管理器来启用不同的训练模式。阅读该章后，您将了解到在深度学习系统中设计工作流管理器的原因和效用以及它在企业环境中的作用。
- en: B.2.6 Experimentation
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验
- en: Google Vertex AI Experiments provides a unified UI to create, track, and manage
    experiments. The Vertex AI SDK provides autologging support for model training
    code to record hyperparameters, metrics, and data lineage. When paired with Vertex
    ML Metadata, you can get a complete overview of all your model training experiment
    runs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI Experiments 提供了统一的用户界面来创建、跟踪和管理实验。Vertex AI SDK 提供了用于模型训练代码的自动记录支持，以记录超参数、指标和数据衍生关系。与
    Vertex ML Metadata 结合使用时，您可以获得所有模型训练实验运行的完整概述。
- en: B.3 Microsoft Azure Machine Learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微软 Azure 机器学习
- en: Different from the classic ML Studio offering from Microsoft that focuses on
    a GUI approach to machine learning, Azure Machine Learning is a new suite of tools
    and services that also supports a wide variety of customization using code and
    established open source frameworks. In this section, we will compare their offerings
    to key components that are described in this book. After completing this section,
    you will gain a sense of what you can use as is, what you can extend, and what
    you need to build from scratch to fulfill your requirements.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于微软经典的 ML Studio 方案，该方案专注于机器学习的图形用户界面方法，Azure 机器学习是一套新的工具和服务，还支持使用代码和已建立的开源框架进行广泛定制。在本节中，我们将比较它们的功能与本书中描述的关键组件。完成本节后，您将对可直接使用的内容、可扩展的内容以及需要从头开始构建以满足您需求的内容有所了解。
- en: B.3.1 Dataset management
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集管理
- en: On Azure Machine Learning, datasets are first-class objects that are inputs
    and outputs of data processing and training tasks. Datasets are defined as a collection
    of metadata associated with the raw data of the dataset. The dataset references
    its raw data via a URI reference to underlying data storage. Once a dataset is
    created, it becomes immutable. The underlying data, however, does not have the
    same guarantee, and it is up to you to manage its immutability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 机器学习中，数据集是第一类对象，是数据处理和训练任务的输入和输出。数据集被定义为与数据集的原始数据相关联的元数据集合。数据集通过 URI
    引用指向底层数据存储的原始数据。一旦数据集被创建，它就变成了不可变的。然而，底层数据并没有同样的保证，您需要自己管理其不可变性。
- en: Once datasets are defined, data processing and training codes can access them
    through a unified client API. Data can either be downloaded for local access or
    mounted as network storage for direct access. After reading chapter 2, you will
    be able to identify similarities between this paradigm and the one that is described
    in the book. You will learn how you can use this existing product as is and how
    you can extend it for your own needs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集被定义，数据处理和训练代码可以通过统一的客户端 API 访问它们。数据可以被下载以供本地访问，也可以被挂载为网络存储以供直接访问。阅读第 2
    章后，您将能够识别出这种范例与本书中描述的范例之间的相似之处。您将学习如何直接使用这个现有产品以及如何根据自己的需求进行扩展。
- en: B.3.2 Model training
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练
- en: Azure Machine Learning supplies prebuilt containers with Python distributions
    and allows users to define a custom-base image that conforms to specific requirements.
    As of this writing, only Python is supported for defining custom training code.
    To launch a training run, you need to specify a runtime container and a reference
    to training code that conforms to a certain convention. If you need something
    other than this setup, you will need to build your own training service. Chapters
    3 and 4 will show you the key principles of a training service and an example
    that you can use as a starting point for your own training service.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning 提供预构建的带有 Python 分发的容器，并允许用户定义符合特定要求的自定义基础镜像。在我们撰写本文时，仅支持使用
    Python 定义自定义的训练代码。要启动一个训练过程，您需要指定运行时容器和符合特定约定的训练代码的引用。如果您需要其他设置，则需要构建自己的训练服务。第
    3 和第 4 章将向您展示训练服务的关键原则和一个示例，作为自己训练服务的起点。
- en: B.3.3 Model serving
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.3 模型服务
- en: On Azure Machine Learning v2, an endpoint can be created to serve online inference
    requests. The endpoint can either be configured to load a certain model and use
    a Python script to produce inferences or be configured to use a completely custom
    container image—such as TensorFlow Serving—to produce inferences. Azure Machine
    Learning also integrates with NVIDIA Triton Inference Server, which provides additional
    performance gain when GPU is used to produce inferences.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure Machine Learning v2 上，可以创建端点来提供在线推断请求。端点可以配置为加载某个模型并使用 Python 脚本产生推断结果，或者配置为使用完全自定义的容器镜像（如
    TensorFlow Serving）产生推断结果。Azure Machine Learning 还与 NVIDIA Triton Inference Server
    集成，当 GPU 用于产生推断结果时，可以提供额外的性能提升。
- en: If you need to deploy multiple models to a single endpoint or manage models
    and inference production on edge devices, you will need to build your own. In
    chapters 6 and 7, we discuss model serving in depth. After completing these chapters,
    you will be able to build your own model server should you require additional
    features that existing offerings do not support.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要将多个模型部署到一个端点或在边缘设备上管理模型和推断产量，则需要构建自己的服务。在第 6 和第 7 章中，我们深入讨论了模型服务。完成这些章节后，您将能够构建自己的模型服务器，以便支持现有提供的功能无法支持的其他功能。
- en: B.3.4 Metadata and artifacts store
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.4 元数据和工件存储
- en: In Azure Machine Learning, metadata can be tagged along many objects, such as
    models, training runs, etc., in the form of tags. While not a standalone product,
    the model registration capability supports additional metadata when registering
    a model. The interface receives the metadata as well as the model file (artifact)
    at the same time during registration, taking one less step when compared to other
    solutions that require models to be registered to already exist in their cloud
    storage.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure Machine Learning 中，元数据可以以标签的形式附加到许多对象上，例如模型、训练运行等等。虽然不是一个独立的产品，但模型注册功能支持在注册模型时添加额外的元数据。在注册期间，接口同时接收元数据和模型文件（工件），相比其他需要先将模型注册到其云存储中的解决方案，需要少一步操作。
- en: As of this writing, a preview feature called registry can be used to centralize
    ML related metadata to one place. If you want to track lineage between different
    artifacts, though you may need to build your own solution.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们撰写本文时，一个称为注册表的预览功能可用于将与 ML 相关的元数据集中存储到一个位置。但是，如果您想要跟踪不同工件之间的传承关系，可能需要构建自己的解决方案。
- en: After reading chapter 8, you will gain an in-depth understanding of the metadata
    and artifacts store. You will learn its fundamentals and will be able to quickly
    build one yourself.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读第 8 章后，您将深入了解元数据和工件存储。您将学习其基础知识，并能够快速构建自己的存储。
- en: B.3.5 Workflow orchestration
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.5 工作流编排
- en: Azure Machine Learning provides a feature called *ML pipelines* that allows
    you to define data, training, and other tasks as steps. These steps are put together
    programmatically to form a pipeline that can be executed periodically based on
    a schedule or trigger or be launched once manually. Compute resources, execution
    environment, and access permissions can be configured programmatically when the
    pipeline is defined.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning 提供了一个称为 *ML pipelines* 的功能，允许您将数据、训练和其他任务定义为步骤。这些步骤可以通过编程方式组合成管道，并可以根据时间表或触发器定期执行，或仅可手动启动。管道定义时，可以编程配置计算资源、执行环境和访问权限。
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system and its
    role in an enterprise environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，我们将回顾如何使用工作流管理器启用不同的训练模式。阅读完本章后，你将理解深度学习系统中工作流管理器的设计理念和实用性，以及它在企业环境中的作用。
- en: B.3.6 Experimentation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.6 实验
- en: Azure Machine Learning provides a feature for defining and tracking experiments.
    When model training is being performed as part of an experiment, metrics can be
    logged from the training code and visualized through the web interface. It also
    supports arbitrary tagging and parent-child relationships between experiment runs
    for a hierarchical organization and lookup.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习提供了一个用于定义和跟踪实验的功能。在实验的一部分进行模型训练时，可以从训练代码中记录指标，并通过 Web 界面可视化。它还支持任意标记和实验运行之间的父子关系，以进行层次化组织和查找。
- en: B.4 Kubeflow
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 Kubeflow
- en: Kubeflow is an open source suite of tools that provides many useful components
    for building a deep learning system without being locked into a particular cloud
    vendor. In this section, we walk through the list of key components that are introduced
    in this book and compare them with similar components provided by Kubeflow.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 是一个开源工具套件，为构建深度学习系统提供了许多有用的组件，而不会被锁定到特定的云供应商。在本节中，我们列出了本书介绍的关键组件，并将它们与
    Kubeflow 提供的类似组件进行了比较。
- en: B.4.1 Dataset management
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.1 数据集管理
- en: Kubeflow’s vision is to not reinvent any existing tools, so it should not be
    a surprise that it does not come with a data management component, as other open
    source solutions exist. In chapter 2, we review some open source data management
    solutions and explore how they can be further extended to implement key principles
    described in that chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 的愿景是不重复造轮子，因此不会有数据管理组件也不足为奇，因为其他开源解决方案已经存在。在第2章中，我们审查了一些开源数据管理解决方案，并探讨了它们如何进一步扩展以实现该章节描述的关键原则。
- en: B.4.2 Model training
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.2 模型训练
- en: Kubeflow, being a suite of tools that is based on Kubernetes, has the luxury
    of being backed by a sophisticated resource scheduler. Unlike cloud vendors that
    provide prebuilt model training containers, you must build your own and manage
    their launches. In chapters 3 and 4, we talk about the principles of a training
    service and how it helps to abstract the complexity in resource assignment and
    scheduling. We go over a reference training service, and you learn how to build
    one yourself for your requirements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 是一个基于 Kubernetes 的工具套件，具有复杂的资源调度器支持。不像云供应商提供预构建的模型训练容器，你必须构建自己的容器并管理它们的启动。在第3章和第4章中，我们讨论了训练服务的原则以及它如何帮助抽象出资源分配和调度中的复杂性。我们将介绍一个参考训练服务，你将学会如何根据自己的需求构建一个。
- en: B.4.3 Model serving
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.3 模型服务
- en: As of this writing, Kubeflow provides a KServe component that can be used to
    deploy trained models as an inference service, which serves inference requests
    over the network. It is an interface that sits on top of existing serving frameworks
    such as TensorFlow Serving, PyTorch TorchServe, and NVIDIA Triton Inference Server.
    The main benefit of using KServe is the additional abstraction of operational
    complexity such as autoscaling, health checks, and auto-recovery. Because it is
    an open source solution, it is possible to host either one model or multiple models
    with the same endpoint. In chapters 6 and 7, we will go through model serving
    principles so that you will understand the reason behind the design of popular
    serving interfaces and how you can customize them to fit your own needs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，Kubeflow 提供了一个名为 KServe 的组件，可用于将训练好的模型部署为推断服务，通过网络提供推断请求。它是一个接口，位于现有的服务框架之上，如
    TensorFlow Serving、PyTorch TorchServe 和 NVIDIA Triton 推断服务器。使用 KServe 的主要好处在于额外抽象了操作复杂性，如自动缩放、健康检查和自动恢复。由于它是一个开源解决方案，可以在同一个端点上托管一个或多个模型。在第6章和第7章中，我们将介绍模型服务原理，以便你理解设计流行服务接口的原因以及如何自定义它们以适应自己的需求。
- en: B.4.4 Metadata and artifacts store
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.4 元数据和工件存储
- en: Starting with Kubeflow version 1.3, metadata and artifacts become an integral
    part of Kubeflow Pipelines. Kubeflow Pipelines consist of a graph of pipeline
    components. Between each component, parameters and artifacts can be passed along.
    Similar to the description in this book, artifacts encapsulate any kind of data
    that is the side effect of a deep learning system, such as the model itself, training
    metrics, and data distribution metrics. Metadata is any data that describes pipeline
    components and artifacts. With these constructs, you can deduce the lineage between
    input training datasets, trained models, experiment results, and served inferences.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Kubeflow 版本 1.3 开始，元数据和工件成为 Kubeflow Pipelines 的一个组成部分。Kubeflow Pipelines
    由一组管道组件图组成。在每个组件之间，参数和工件可以传递。类似于本书中的描述，工件封装了深度学习系统的任何类型的数据，例如模型本身、训练指标和数据分布指标等。元数据是描述管道组件和工件的任何数据。有了这些构造，你可以推断出输入训练数据集、训练模型、实验结果和服务推理之间的衍生关系。
- en: In chapter 8, we discuss key concerns for building the metadata and artifacts
    store. After reading the chapter, you will understand the design principles behind
    this component and how existing products can help you build your own quickly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 章中，我们讨论了构建元数据和工件存储的关键问题。阅读完本章后，你将理解该组件背后的设计原则以及现有产品如何帮助你快速构建自己的存储。
- en: B.4.5 Workflow orchestration
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.5 工作流编排
- en: Also described in the previous section, Kubeflow Pipelines can be used to help
    manage deep learning data preparation and training workflows. Metadata and versioning
    are built into pipelines, and native users and access permissions from Kubernetes
    can be used to restrict access.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此前的章节中也有描述，Kubeflow Pipelines 可以用于帮助管理深度学习数据准备和训练工作流程。元数据和版本控制已集成到管道中，并且可以使用
    Kubernetes 的本机用户和访问权限来限制访问。
- en: In chapter 9, we review how a workflow manager enables different modes of training.
    After reading the chapter, you will understand the reasoning behind the design
    and utility of a workflow manager in a deep learning system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 9 章中，我们将回顾工作流程管理器如何实现不同的训练模式。阅读完本章后，你将理解深度学习系统中工作流程管理器设计和实用性背后的原理。
- en: B.4.6 Experimentation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.6 实验
- en: Kubeflow Pipelines provides the Experiment construct in which multiple training
    runs can be organized into a logical group, where it provides additional visualization
    tools for differences between each experimental run. This fits well with offline
    experimentation. If you need to perform online experimentation, you will need
    to roll your own solution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 提供了实验结构，其中多个训练运行可以组织成一个逻辑组，在其中为每个实验运行提供了额外的可视化工具以显示差异。这非常适用于离线实验。如果需要进行在线实验，则需要自行解决方案。
- en: B.5 Side-by-side comparison
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.5 旁边对比
- en: We think it will be handy to provide a summarized overview table of every solution
    grouped by components that we have covered previously. We hope that table B.1
    will make it easier for you to pick the right solution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，提供一个按照我们先前涵盖的组件分组的所有解决方案的摘要概述表格将会很方便。我们希望表格 B.1 能够让你更轻松地选择合适的解决方案。
- en: Table B.1 Side-by-side comparison
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 B.1 旁边对比
- en: '|  | Amazon SageMaker | Google Vertex AI | Microsoft Azure Machine Learning
    | Kubeflow |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 亚马逊 SageMaker | 谷歌 Vertex AI | 微软 Azure 机器学习 | Kubeflow |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Comparing dataset management solutions | AWS components, such as S3, Glue
    Data Catalog, and Glue ETL, can be used to build a dataset management component.
    | APIs for managing datasets are ready to use. Data content upload and metadata
    tagging are separate operations. | Datasets are first-class objects and are immutable
    once created. A unified client API is provided for training jobs to access training
    datasets. | Does not provide a dataset management solution. Other open source
    alternatives are readily available. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 比较数据集管理解决方案 | AWS 组件，如 S3、Glue 数据目录和 Glue ETL，可用于构建数据集管理组件。 | 用于管理数据集的 API
    已经准备就绪。数据内容上传和元数据标记是分开的操作。 | 数据集是一级对象，一旦创建就不可变。提供了统一的客户端 API 用于训练作业访问训练数据集。 |
    不提供数据集管理解决方案。其他开源替代方案随时可用。 |'
- en: '| Comparing model training solutions | Supports built-in algorithms, externally
    provided custom code, and custom containers for training. Exposes an API for launching
    training jobs on demand. | Provides prebuilt training containers that can be used
    as is. Supports custom training containers. Exposes an API that supports launching
    training containers on multiple nodes. | Provides prebuilt training containers
    with Python that can be customized. Training containers must conform to a certain
    convention. | Has native access to Kubernetes scheduling capabilities. No prebuilt
    training containers are provided. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 比较模型训练解决方案 | 支持内置算法、外部提供的自定义代码和用于训练的自定义容器。提供一个 API 用于按需启动训练作业。 | 提供可以直接使用的预构建训练容器。支持自定义训练容器。提供支持在多个节点上启动训练容器的
    API。 | 提供具有 Python 的预构建训练容器，可进行定制。训练容器必须符合某种约定。 | 具有对 Kubernetes 调度能力的本地访问。不提供预构建的训练容器。
    |'
- en: '| Comparing model serving solutions | Models can be deployed as web endpoints.
    Multiple models can be deployed to the same endpoint for better utilization, with
    some limitations when GPUs are used. Configurable model caching behavior. | Models
    and inference containers are decoupled. They must be deployed together to form
    a web endpoint for serving. Custom inference containers are supported. Multiple
    models per endpoint are primarily used for canarying new versions of models. Video
    models are not supported. | Endpoints can be deployed to serve models over the
    web. Endpoints are configured to use a particular model with a custom Python script
    for producing inferences. NVIDIA Triton Inference Server integration is available.
    | KServe is the Kubeflow component that serves models. It provides a serverless
    inferencing abstraction on top of popular serving frameworks such as TensorFlow
    Serving, PyTorch TorchServe, and NVIDIA Triton Inference Server. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 比较模型服务解决方案 | 模型可以部署为 Web 端点。可以将多个模型部署到同一端点以实现更好的利用，但使用 GPU 时存在一些限制。可配置的模型缓存行为。
    | 模型和推断容器是分离的。它们必须一起部署以形成用于服务的 Web 端点。支持自定义推断容器。主要用于金丝雀测试新版本模型的单个端点上使用多个模型。不支持视频模型。
    | 可以部署端点以通过 Web 提供模型服务。端点配置为使用特定模型，使用自定义 Python 脚本生成推断。支持 NVIDIA Triton 推理服务器集成。
    | KServe 是 Kubeflow 的组件，用于提供模型服务。它在流行的服务框架（如 TensorFlow Serving、PyTorch TorchServe
    和 NVIDIA Triton 推理服务器）之上提供了一个无服务器推断抽象。 |'
- en: '| Comparing metadata and artifacts store solutions | SageMaker Model Registry
    provides a central metadata store solution. Artifacts are stored separately in
    Amazon’s object store. | Vertex ML Metadata provides a central metadata store
    solution. Metadata is stored as graphs that can describe complex relationships.
    Artifacts are stored in Google’s object store. | A preview feature called registry
    can be used to centralize ML metadata. Metadata exists as tags of different objects
    (training runs, models, etc.), and objects can be artifacts. Lineage information
    can be deduced using these object tags. | Does not have a central repository of
    metadata or artifacts. Metadata and artifacts are integral parts of Kubeflow Pipelines.
    Each stage in the pipeline can be annotated with metadata and produce artifacts
    that can be tracked. Lineage information can be deduced from this information
    that can be retrieved from the Pipelines API. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 比较元数据和工件存储解决方案 | SageMaker 模型注册表提供了一个中心化的元数据存储解决方案。工件单独存储在亚马逊的对象存储中。 | Vertex
    ML 元数据提供了一个中心化的元数据存储解决方案。元数据以能描述复杂关系的图形形式存储。工件存储在谷歌的对象存储中。 | 一个称为注册表的预览功能可用于集中管理
    ML 元数据。元数据存在于不同对象（训练运行、模型等）的标签中，并且对象可以是工件。可以使用这些对象标签推断出血统信息。 | 没有中心化的元数据或工件存储库。元数据和工件是
    Kubeflow Pipelines 的组成部分。管道中的每个阶段都可以用元数据注释，并且可以跟踪生成的工件。可以从 Pipelines API 检索到的信息中推断出血统信息。
    |'
- en: '| Comparing workflow orchestration solutions | Model Building Pipelines can
    be used to build and manage deep learning workflows. | Vertex ML Metadata provides
    a central metadata store solution. Metadata is stored as graphs that can describe
    complex relationships. Artifacts are stored in Google’s object store. | A preview
    feature called registry can be used to centralize ML metadata. Metadata exists
    as tags of different objects (training runs, models, etc.), and objects can be
    artifacts. Lineage information can be deduced using these object tags. | Does
    not have a central repository of metadata or artifacts. Metadata and artifacts
    are integral parts of Kubeflow Pipelines. Each stage in the pipeline can be annotated
    with metadata and produce artifacts that can be tracked. Lineage information can
    be deduced from the details that can be retrieved from the Pipelines API. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 比较工作流编排解决方案 | 模型构建流水线可用于构建和管理深度学习工作流。 | Vertex ML Metadata 提供了一个集中的元数据存储解决方案。元数据存储为可以描述复杂关系的图形。工件存储在谷歌的对象存储中。
    | 一个名为注册表的预览功能可用于集中化 ML 元数据。元数据存在于不同对象的标签中（训练运行、模型等），对象可以是工件。可以使用这些对象标签推断出谱系信息。
    | 没有元数据或工件的中央存储库。元数据和工件是 Kubeflow 流水线的组成部分。管道中的每个阶段都可以用元数据注释，并产生可以被跟踪的工件。可以从可以从流水线
    API 中检索的详细信息中推断出谱系信息。 |'
- en: '| Comparing experimentation solutions | The Experiments feature provides grouping
    and tracking for training runs. | Provides Vertex AI Experiments for tracking
    and visualizing experiment setups and run results. | Provides features for defining
    and tracking experiments. Experiments can be associated with a parent-child relationship.
    The web interface supports visualization. | Provides an Experiment construct for
    logical grouping of Kubeflow Pipelines that belong to the same experiment group.
    Visualization tools are provided to highlight differences between each pipeline
    run in the same experiment. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 比较实验解决方案 | 实验功能提供了对训练运行的分组和跟踪。 | 为 Vertex AI 实验提供跟踪和可视化实验设置和运行结果的功能。 | 提供了定义和跟踪实验的功能。实验可以关联为父子关系。Web
    接口支持可视化。 | 提供了一个实验构造，用于逻辑分组 Kubeflow 流水线，这些流水线属于同一个实验组。提供了可视化工具，用于突出显示同一实验中每个流水线运行之间的差异。
    |'
