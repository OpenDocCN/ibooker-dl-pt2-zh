- en: '4 Getting started with neural networks: Classification and regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 从神经网络开始：分类与回归
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Your first examples of real-world machine learning workflows
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的第一个真实世界机器学习工作流程的例子
- en: Handling classification problems over vector data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理基于向量数据的分类问题
- en: Handling continuous regression problems over vector data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理基于向量数据的连续回归问题
- en: 'This chapter is designed to get you started using neural networks to solve
    real problems. You’ll consolidate the knowledge you gained from chapters 2 and
    3, and you’ll apply what you’ve learned to the following three new tasks covering
    the three most common use cases of neural networks:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在帮助你开始使用神经网络解决实际问题。你将巩固从第2章和第3章中获得的知识，并将你所学的知识应用于以下三个新任务，涵盖神经网络最常见的三个用例：
- en: Classifying movie reviews as positive or negative (binary classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将电影评论分类为积极或消极（二元分类
- en: Classifying news wires by topic (multiclass classification)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按主题对新闻线索进行分类（多类分类）
- en: Estimating the price of a house, given real estate data (scalar regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定房地产数据的情况下估算房屋价格（标量回归
- en: 'These examples will be your first contact with end-to-end machine learning
    workflows: you’ll be introduced to data preprocessing, basic model architecture
    principles, and model evaluation.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子将是你与端到端机器学习工作流程的第一次接触：你将了解数据预处理、基本模型架构原则和模型评估。
- en: Classification and regression glossary
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与回归词汇表
- en: 'Classification and regression involve many specialized terms. You’ve come across
    some of them in earlier examples, and you’ll see more of them in future chapters.
    They have the following precise, machine learning–specific definitions, and you
    should be familiar with them:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与回归涉及许多专业术语。你已经在早期的例子中遇到了一些，未来的章节中你还将看到更多。它们有以下精确的、特定于机器学习的定义，你应该熟悉它们：
- en: '*Sample or input*—One data point that goes into your model.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*样本或输入*——进入模型的一个数据点。'
- en: '*Prediction or output*—What comes out of your model.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测或输出*——从你的模型中输出的结果。'
- en: '*Target*—The truth. What your model should ideally have predicted, according
    to an external source of data.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*目标*——真相。根据外部数据源，你的模型理想情况下应该预测的结果。'
- en: '*Prediction error or loss value*—A measure of the distance between your model’s
    prediction and the target.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测错误或损失值*——衡量模型预测与目标之间距离的指标。'
- en: '*Classes*—A set of possible labels to choose from in a classification problem.
    For example, when classifying cat and dog pictures, “dog” and “cat” are the two
    classes.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*类*——在分类问题中可以选择的一组可能的标签。例如，在分类猫和狗的图片时，“狗”和“猫”是两个类。'
- en: '*Label*—A specific instance of a class annotation in a classification problem.
    For instance, if picture #1234 is annotated as containing the class “dog,” then
    “dog” is a label of picture #1234.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签*——在分类问题中，某一类注释的具体实例。例如，如果图片#1234被标注包含“狗”这一类，则“狗”是图片#1234的一个标签。'
- en: '*Ground-truth or annotations*—All targets for a dataset, typically collected
    by humans.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真实值或注释*——数据集的所有目标，通常由人类收集。'
- en: '*Binary classification*—A classification task where each input sample should
    be categorized into two exclusive categories.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二元分类*——一种分类任务，每个输入样本需要被归类到两个互斥的类别中。'
- en: '*Multiclass classification*—A classification task where each input sample should
    be categorized into more than two categories, for instance, classifying handwritten
    digits.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多类分类*——一种分类任务，每个输入样本需要被归类到两个以上的类别中，例如，对手写数字的分类。'
- en: '*Multilabel classification*—A classification task where each input sample can
    be assigned multiple labels. For instance, a given image may contain both a cat
    and a dog and should be annotated with both the “cat” label and the “dog” label.
    The number of labels per image is usually variable.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多标签分类*——一种分类任务，每个输入样本可以被分配多个标签。例如，给定的图像可能同时包含一只猫和一只狗，应该用“猫”标签和“狗”标签进行标注。每张图片的标签数量通常是可变的。'
- en: '*Scalar regression*—A task where the target is a continuous scalar value. Predicting
    house prices is a good example: the different target prices form a continuous
    space.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标量回归*——一个目标是连续标量值的任务。预测房价就是一个很好的例子：不同的目标价格构成了一个连续的空间。'
- en: '*Vector regression*—A task where the target is a set of continuous values,
    for example, a continuous vector. If you’re doing regression against multiple
    values (such as the coordinates of a bounding box in an image), then you’re doing
    vector regression.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*矢量回归*——目标是一组连续值的任务，例如，连续矢量。如果你对多个值进行回归（例如图像中边界框的坐标），那么你在进行矢量回归。'
- en: '*Mini-batch or batch*—A small set of samples (typically between 8 and 128)
    that are processed simultaneously by the model. The number of samples is often
    a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch
    is used to compute a single gradient descent update applied to the weights of
    the model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*小批量或批处理*——一组较小的样本（通常介于 8 到 128 之间）由模型同时处理。样本数量通常是 2 的幂，以便在 GPU 上分配内存。在训练过程中，小批量用于计算对模型权重应用单次梯度下降更新。'
- en: By the end of this chapter, you’ll be able to use neural networks to handle
    simple classification and regression tasks over vector data. You’ll then be ready
    to start building a more principled, theory-driven understanding of machine learning
    in chapter 5.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够使用神经网络处理矢量数据上的简单分类和回归任务。然后你将准备好在第 5 章开始构建更加系统、基于理论的机器学习理解。
- en: '4.1 Classifying movie reviews: A binary classification example'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 电影评论分类：一个二元分类的例子
- en: Two-class classification, or binary classification, is one of the most common
    kinds of machine learning problems. In this example, you’ll learn to classify
    movie reviews as positive or negative, based on the text content of the reviews.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类问题，或者二元分类，是机器学习问题中最常见的一类。在这个例子中，你将学习如何根据评论的文本内容将电影评论分类为正面或负面。
- en: 4.1.1 The IMDB dataset
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 IMDB 数据集
- en: 'You’ll work with the *IMDB dataset*: a set of 50,000 highly polarized reviews
    from the Internet Movie Database. They’re split into 25,000 reviews for training
    and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive
    reviews.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 *IMDB 数据集*：一组来自互联网电影数据库的 50,000 条高度极化的评论。它们被分为 25,000 条用于训练和 25,000 条用于测试，每组包含
    50% 的负面评论和 50% 的正面评论。
- en: 'Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It
    has already been preprocessed: the reviews (sequences of words) have been turned
    into sequences of integers, where each integer stands for a specific word in a
    dictionary. This enables us to focus on model building, training, and evaluation.
    In chapter 11, you’ll learn how to process raw text input from scratch.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 和 MNIST 数据集一样，IMDB 数据集也随 Keras 一起打包。它已经经过预处理：评论（单词序列）已被转换为整数序列，其中每个整数代表词典中的特定单词。这让我们可以专注于模型构建、训练和评估。在第
    11 章中，你将学习如何从头开始处理原始文本输入。
- en: The following code will load the dataset (when you run it the first time, about
    80 MB of data will be downloaded to your machine).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将加载数据集（当你第一次运行时，约 80 MB 的数据将被下载到你的机器上）。
- en: Listing 4.1 Loading the IMDB dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 4.1：加载 IMDB 数据集
- en: library(keras)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`library(keras)`'
- en: imdb <- dataset_imdb(num_words = 10000)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`imdb <- dataset_imdb(num_words = 10000)`'
- en: c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb`'
- en: Using the multiassignment (%<-%) operator
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多重赋值（`%<-%`）运算符
- en: The argument num_words = 10000 means you’ll keep only the top 10,000 most frequently
    occurring words in the training data. Rare words will be discarded. This allows
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `num_words = 10000` 表示你只会保留训练数据中最常出现的前 10,000 个单词。稀有单词将被丢弃。这使得
- en: imdb <- dataset_imdb(num_words = 10000)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`imdb <- dataset_imdb(num_words = 10000)`'
- en: train_data <- imdb$train$x
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_data <- imdb$train$x`'
- en: train_labels <- imdb$train$y
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_labels <- imdb$train$y`'
- en: test_data <- imdb$test$x
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_data <- imdb$test$x`'
- en: test_labels <- imdb$test$y
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_labels <- imdb$test$y`'
- en: 'The datasets built into Keras are all nested lists of training and test data.
    Here, we use the multiassignment operator (%<-%) from the zeallot package to unpack
    the list into a set of distinct variables. This could equally be written as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 内置的数据集都是包含训练和测试数据的嵌套列表。这里，我们使用 zeallot 包的多重赋值运算符（`%<-%`）将列表解包为一组不同的变量。同样，这也可以写成如下形式：
- en: The multiassignment version is preferable because it’s more compact. The %<-%
    operator is automatically available whenever the R Keras package is attached.
    us to work with vector data of manageable size. If we didn’t set this limit, we’d
    be working with 88,585 unique words in the training data, which is unnecessarily
    large. Many of these words occur only in a single sample and thus can’t be meaningfully
    used for classification.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '多赋值版本更好，因为它更简洁。%<-% 操作符在 R Keras 包附加时会自动可用。我们可以处理可管理大小的向量数据。如果我们没有设置这个限制，我们将使用训练数据中的
    88,585 个唯一单词，这是不必要的庞大数量。其中许多单词仅出现在单个样本中，因此不能用于分类。 '
- en: 'The variables train_data and test_data are lists of reviews; each review is
    a list of word indices (encoding a sequence of words). train_labels and test_labels
    are lists of 0s and 1s, where 0 stands for *negative* and 1 stands for *positive*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 train_data 和 test_data 是评论列表；每个评论是一个单词索引列表（编码为一系列单词）。train_labels 和 test_labels
    是 0 和 1 的列表，其中 0 代表 *负面*，1 代表 *正面*：
- en: str(train_data)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`str(train_data)`'
- en: List of 25000
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 25000 的列表
- en: '$ : int [1:218] 1 14 22 16 43 530 973 1622 1385 65 …'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:218] 1 14 22 16 43 530 973 1622 1385 65 …`'
- en: '$ : int [1:189] 1 194 1153 194 8255 78 228 5 6 1463 …'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:189] 1 194 1153 194 8255 78 228 5 6 1463 …`'
- en: '$ : int [1:141] 1 14 47 8 30 31 7 4 249 108 …'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:141] 1 14 47 8 30 31 7 4 249 108 …`'
- en: '$ : int [1:550] 1 4 2 2 33 2804 4 2040 432 111 …'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:550] 1 4 2 2 33 2804 4 2040 432 111 …`'
- en: '$ : int [1:147] 1 249 1323 7 61 113 10 10 13 1637 …'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:147] 1 249 1323 7 61 113 10 10 13 1637 …`'
- en: '$ : int [1:43] 1 778 128 74 12 630 163 15 4 1766 …'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:43] 1 778 128 74 12 630 163 15 4 1766 …`'
- en: '$ : int [1:123] 1 6740 365 1234 5 1156 354 11 14 5327 …'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:123] 1 6740 365 1234 5 1156 354 11 14 5327 …`'
- en: '$ : int [1:562] 1 4 2 716 4 65 7 4 689 4367 …'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ : int [1:562] 1 4 2 716 4 65 7 4 689 4367 …`'
- en: '[list output truncated]'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`[list output truncated]`'
- en: str(train_labels)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`str(train_labels)`'
- en: int [1:25000] 1 0 0 1 0 0 1 0 1 0 …
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`int [1:25000] 1 0 0 1 0 0 1 0 1 0 …`'
- en: 'Because we’re restricting ourselves to the top 10,000 most frequent words,
    no word index will exceed 10,000:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们限制在最常见的前 10,000 个单词范围内，所以没有单词索引会超过 10,000：
- en: max(sapply(train_data, max))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`max(sapply(train_data, max))`'
- en: '[1] 9999'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`[1] 9999`'
- en: For kicks, here’s how you can quickly decode one of these reviews back to English
    words.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，下面是如何快速将其中一个评论解码回英文单词。
- en: Listing 4.2 Decoding reviews back to text
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 将评论解码回文本
- en: word_index <- dataset_imdb_word_index()➊
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`word_index <- dataset_imdb_word_index()➊`'
- en: reverse_word_index <- names(word_index)➋
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`reverse_word_index <- names(word_index)➋`'
- en: names(reverse_word_index) <- as.character(word_index)➋
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`names(reverse_word_index) <- as.character(word_index)➋`'
- en: decoded_words <- train_data[[1]] %>%
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoded_words <- train_data[[1]] %>%'
- en: sapply(function(i) {
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`sapply(function(i) {`'
- en: if (i > 3) reverse_word_index[[as.character(i - 3)]]➌
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 (i > 3) reverse_word_index[[as.character(i - 3)]]➌
- en: else "?"
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`else "?"`'
- en: '})'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '})`'
- en: decoded_review <- paste0(decoded_words, collapse = " ")
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`decoded_review <- paste0(decoded_words, collapse = " ")`'
- en: cat(decoded_review, "\n")
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`cat(decoded_review, "\n")`'
- en: '? this film was just brilliant casting location scenery story direction everyone''s
    really suited the part they played and you could just imagine being there robert
    ? is an amazing actor and now the same being director …'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '? 这部电影简直太棒了，演员阵容、拍摄地点、场景、故事情节、导演都非常适合他们所扮演的角色，你可以想象自己就在那里，罗伯特?是一位了不起的演员，而且现在也是导演……'
- en: ➊ **word_index is a named vector mapping words to an integer index.**
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **word_index 是一个将单词映射到整数索引的命名向量。**
- en: ➋ **Reverses it, mapping integer indices to words**
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将其反转，将整数索引映射到单词**
- en: ➌ **Decodes the review. Note that the indices are offset by 3 because 0, 1,
    and 2 are reserved indices for "padding," "start of sequence," and "unknown."**
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **解码评论。请注意，索引偏移了 3，因为 0、1 和 2 分别保留为 "填充"、"序列开始" 和 "未知" 的索引。**
- en: 4.1.2 Preparing the data
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 准备数据
- en: 'You can’t directly feed lists of integers into a neural network. They all have
    different lengths, but a neural network expects to process contiguous batches
    of data. You have to turn your lists into tensors. You can do that in the following
    two ways:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能直接将整数列表馈送到神经网络中。它们的长度各不相同，但神经网络期望处理连续的数据批次。你必须将列表转换为张量。你可以通过以下两种方式做到这一点：
- en: Pad your lists so that they all have the same length, turn them into an integer
    tensor of shape (samples, max_length), and start your model with a layer capable
    of handling such integer tensors (the Embedding layer, which we’ll cover in detail
    later in the book).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对列表进行填充，使它们具有相同的长度，将它们转换为形状为 (samples, max_length) 的整数张量，并从能处理这种整数张量的层开始构建你的模型（嵌入层，我们将在本书后面详细介绍）。
- en: '*Multi-hot encode* your lists to turn them into vectors of 0s and 1s. This
    would mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional
    vector that would be all 0s except for indices 8 and 5, which would be 1s. Then
    you could use a layer_dense(), capable of handling floating-point vector data,
    as the first layer in your model'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多热编码* 你的列表，将它们转换为 0 和 1 的向量。这意味着，例如，将序列 [8, 5] 转换为一个 10,000 维的向量，除了索引 8 和
    5 外全部为 0，索引 8 和 5 为 1。然后你可以使用一个 layer_dense()，它能处理浮点向量数据，作为你模型中的第一层'
- en: Let’s go with the latter solution to vectorize the data, which you’ll do manually
    for maximum clarity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择后一种解决方案来向量化数据，这样你就可以手动进行，以获得最大的清晰度。
- en: '**Listing 4.3 Encoding the integer sequences via multi-hot encoding**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 4.3 通过多热编码对整数序列进行编码**'
- en: vectorize_sequences <- function(sequences, dimension = 10000)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: vectorize_sequences <- function(sequences, dimension = 10000)
- en: '{ results <- array(0, dim = c(length(sequences), dimension))➊'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '{ results <- array(0, dim = c(length(sequences), dimension))➊'
- en: for (i in seq_along(sequences)) {
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq_along(sequences)) {
- en: sequence <- sequences[[i]]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: sequence <- sequences[[i]]
- en: for (j in sequence)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: for (j in sequence)
- en: results[i, j] <- 1➋
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: results[i, j] <- 1➋
- en: '}'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: results
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: results
- en: '}'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: x_train <- vectorize_sequences(train_data)➌
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: x_train <- vectorize_sequences(train_data)➌
- en: x_test <- vectorize_sequences(test_data)➍
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: x_test <- vectorize_sequences(test_data)➍
- en: ➊ **Create an all-zero matrix of shape (length(sequences), dimension).**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **创建形状为 (length(sequences), dimension) 的全零矩阵。**
- en: ➋ **Set specific indices of results to 1s.**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将结果的特定索引设置为 1。**
- en: ➌ **Vectorize training data.**
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **向量化训练数据。**
- en: ➍ **Vectorize test data.**
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **向量化测试数据。**
- en: 'Here’s what the samples look like now:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在样本看起来是这样的：
- en: str(x_train)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: str(x_train)
- en: num [1:25000, 1:10000] 1 1 1 1 1 1 1 1 1 1 …
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:25000, 1:10000] 1 1 1 1 1 1 1 1 1 1 …
- en: 'You should also vectorize your labels, which is a straightforward cast of integers
    to floats:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该将标签向量化，这是将整数转换为浮点数的直接转换：
- en: y_train <- as.numeric(train_labels)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: y_train <- as.numeric(train_labels)
- en: y_test <- as.numeric(test_labels)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: y_test <- as.numeric(test_labels)
- en: Now the data is ready to be fed into a neural network.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备好输入神经网络了。
- en: 4.1.3 Building your model
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 构建你的模型
- en: 'The input data is vectors, and the labels are scalars (1s and 0s): this is
    one of the simplest problem setups you’ll ever encounter. A type of model that
    performs well on such a problem is a plain stack of densely connected layers (layer_dense())
    with relu activations.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据是向量，标签是标量（1 和 0）：这是你可能会遇到的最简单的问题设置之一。在这种问题上表现良好的模型类型是一个简单的密集连接层堆叠（layer_dense()），使用
    relu 激活。
- en: 'You need to make two key architecture decisions about such a stack of dense
    layers:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做两个关键的架构决策来设计这样一堆密集层：
- en: How many layers to us
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多少层
- en: How many units to choose for each laye
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个层选择多少个单元
- en: '![Image](../images/f0108-01.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0108-01.jpg)'
- en: '**Figure 4.1 The three-layer model**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.1 三层模型**'
- en: 'In chapter 5, you’ll learn formal principles to guide you in making these choices.
    For the time being, you’ll have to trust me with the following architecture choices:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章，你将学习形式原理，指导你进行这些选择。目前，你将不得不相信我做出以下架构选择：
- en: Two intermediate layers with 16 units each
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个中间层，每个有 16 个单元
- en: A third layer that will output the scalar prediction regarding the sentiment
    of the current revie
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个层将输出关于当前评论情感的标量预测
- en: '[Figure 4.1](#fig4-1) shows what the model looks like. And the following listing
    shows the Keras implementation, similar to the MNIST example you saw previously.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.1](#fig4-1) 展示了模型的样子。下面的列表展示了 Keras 实现，与之前你看到的 MNIST 示例类似。'
- en: '**Listing 4.4 Model definition**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 4.4 模型定义**'
- en: model <- keras_model_sequential() %>%
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: 'The first argument being passed to each layer_dense() is the number of *units*
    in the layer: the dimensionality of representation space of the layer. Remember
    from chapters 2 and 3 that each such layer_dense() with a relu activation implements
    the following chain of tensor operations:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给每个 layer_dense() 的第一个参数是层中的 *单元* 数：层的表示空间的维度。从第 2 章和第 3 章记得，每个带有 relu 激活的
    layer_dense() 实现以下张量操作链：
- en: output <- relu(dot(input, W) + b)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: output <- relu(dot(input, W) + b)
- en: 'Having 16 units means the weight matrix W will have shape (input_dimension,
    16): the dot product with W will project the input data onto a 16-dimensional
    representation space (and then you’ll add the bias vector b and apply the relu
    operation). You can intuitively understand the dimensionality of your representation
    space as “how much freedom you’re allowing the model to have when learning internal
    representations.” Having more units (a higher-dimensional representation space)
    allows your model to learn more complex representations, but it makes the model
    more computationally expensive and may lead to learning unwanted patterns (patterns
    that will improve performance on the training data but not on the test data).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有16个单元意味着权重矩阵W的形状为（输入维度，16）：与W的点积将将输入数据投影到16维表示空间（然后您将添加偏置向量b并应用relu操作）。您可以直观地将表示空间的维数理解为“在学习内部表示时为模型提供多少自由度”。具有更多单元（更高维的表示空间）允许您的模型学习更复杂的表示，但会使模型更加计算密集，并可能导致学习不需要的模式（这些模式会提高在训练数据上的性能，但不会提高在测试数据上的性能）。
- en: 'The intermediate layers use relu as their activation function, and the final
    layer uses a sigmoid activation so as to output a probability (a score between
    0 and 1 indicating how likely the sample is to have the target “1”: how likely
    the review is to be positive). A relu (rectified linear unit) is a function meant
    to zero out negative values (see [figure 4.2](#fig4-2)), whereas a sigmoid “squashes”
    arbitrary values into the [0, 1] interval (see [figure 4.3](#fig4-3)), outputting
    something that can be interpreted as a probability.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层使用relu作为它们的激活函数，最终层使用sigmoid激活，以便输出一个概率（介于0和1之间的分数，指示样本有多大可能性具有目标“1”：评论有多可能是积极的）。relu（修正线性单元）是一个将负值归零的函数（见[图4.2](#fig4-2)），而sigmoid“挤压”任意值到[0,
    1]区间（见[图4.3](#fig4-3)），输出可解释为概率的东西。
- en: '![Image](../images/f0109-01.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0109-01.jpg)'
- en: '**Figure 4.2 The relu function**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.2 Relu函数**'
- en: '![Image](../images/f0109-02.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0109-02.jpg)'
- en: '**Figure 4.3 The sigmoid function**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.3 Sigmoid函数**'
- en: What are activation functions, and why are they necessary?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是什么，为什么它们是必需的？
- en: 'Without an activation function like relu (also called a *nonlinearity*), layer_dense
    would consist of two linear operations—a dot product and an addition:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有像relu这样的激活函数（也称为*非线性*），layer_dense将由两个线性操作组成：点积和加法：
- en: output <- dot(input, W) + b
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <- 点积（输入，W）+ b
- en: 'The layer could learn only *linear transformations* (affine transformations)
    of the input data: the *hypothesis space* of the layer would be the set of all
    possible linear transformations of the input data into a 16-dimensional space.
    Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers
    of representations, because a deep stack of linear layers would still implement
    a linear operation: adding more layers wouldn’t extend the hypothesis space (as
    you saw in chapter 2).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该层只能学习输入数据的*线性转换*（仿射转换）：该层的*假设空间*将是将输入数据转换为16维空间的所有可能线性转换的集合。这样的假设空间过于受限，不会从多层表示中受益，因为深层线性层的堆叠仍然实现线性操作：添加更多层不会扩展假设空间（正如您在第2章中看到的）。
- en: 'To get access to a much richer hypothesis space that will benefit from deep
    representations, you need a nonlinearity, or activation function. relu is the
    most popular activation function in deep learning, but many other candidates exist,
    which all come with similarly strange names: prelu, elu, and so on.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了访问一个更丰富的假设空间，从而受益于深度表示，您需要一个非线性或激活函数。relu是深度学习中最流行的激活函数，但还有许多其他候选激活函数，它们都具有类似奇怪的名称：prelu、elu等等。
- en: 'Finally, you need to choose a loss function and an optimizer. Because you’re
    facing a binary classification problem and the output of your model is a probability
    (you end your model with a single-unit layer with a sigmoid activation), it’s
    best to use the binary_crossentropy loss. It isn’t the only viable choice: for
    instance, you could use mean_squared_error. But cross-entropy is usually the best
    choice when you’re dealing with models that output probabilities. *Cross-entropy*
    is a quantity from the field of information theory that measures the distance
    between probability distributions or, in this case, between the ground-truth distribution
    and your predictions. As for the choice of the optimizer, we’ll go with rmsprop,
    which is a usually a good default choice for virtually any problem.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要选择损失函数和优化器。因为您面临的是二元分类问题，您的模型的输出是概率（您的模型以带有 sigmoid 激活的单单元层结束），所以最好使用
    binary_crossentropy 损失。这不是唯一可行的选择：例如，您可以使用 mean_squared_error。但是当您处理输出概率的模型时，交叉熵通常是最佳选择。*交叉熵*是信息论领域的一种量，它衡量概率分布之间的距离，或者在这种情况下，衡量真实分布与您的预测之间的距离。至于优化器的选择，我们将选择
    rmsprop，这通常是几乎任何问题的良好默认选择。
- en: Here’s the step where we configure the model with the rmsprop optimizer and
    the binary_crossentropy loss function. Note that we’ll also monitor accuracy during
    training.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置模型的步骤，其中使用了 rmsprop 优化器和 binary_crossentropy 损失函数。请注意，在训练过程中我们也会监视准确性。
- en: '**Listing 4.5 Compiling the model**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 4.5 编译模型**'
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: 4.1.4 Validating your approach
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 验证你的方法
- en: As you learned in chapter 3, a deep learning model should never be evaluated
    on its training data—it’s standard practice to use a validation set to monitor
    the accuracy of the model during training. Here, we’ll create a validation set
    by setting apart 10,000 samples from the original training data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第 3 章学到的，深度学习模型永远不应该在其训练数据上进行评估——在训练过程中使用验证集来监视模型的准确性是标准做法。在这里，我们将通过从原始训练数据中分离出
    10,000 个样本来创建一个验证集。
- en: '**Listing 4.6 Setting aside a validation set**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 4.6 设置验证集**'
- en: x_val <- x_train[seq(10000), ]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: x_val <- x_train[seq(10000), ]
- en: partial_x_train <- x_train[-seq(10000), ]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: partial_x_train <- x_train[-seq(10000), ]
- en: y_val <- y_train[seq(10000)]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: y_val <- y_train[seq(10000)]
- en: partial_y_train <- y_train[-seq(10000)]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: partial_y_train <- y_train[-seq(10000)]
- en: We will now train the model for 20 epochs (20 iterations over all samples in
    the training data) in mini-batches of 512 samples. At the same time, we will monitor
    loss and accuracy on the 10,000 samples that we set apart. We do so by passing
    the validation data as the validation_data argument.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 20 个时期（20 次对训练数据中的所有样本进行迭代）以 512 个样本一组的小批量进行模型训练。同时，我们将监视我们分离出来的 10,000
    个样本上的损失和准确性。我们通过将验证数据作为验证数据参数来实现这一点。
- en: '**Listing 4.7 Training your model**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 4.7 训练你的模型**'
- en: history <- model %>% fit(
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(
- en: partial_x_train,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: partial_x_train，
- en: partial_y_train,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: partial_y_train，
- en: epochs = 20,
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20，
- en: batch_size = 512,
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 512，
- en: validation_data = list(x_val, y_val)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(x_val, y_val)
- en: )
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: On a CPU, this will take less than 2 seconds per epoch—training is over in 20
    seconds. At the end of every epoch is a slight pause as the model computes its
    loss and accuracy on the 10,000 samples of the validation data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上，每个时期不到 2 秒——在 20 秒内完成训练。每个时期结束时会稍作停顿，因为模型会计算验证数据的 10,000 个样本上的损失和准确性。
- en: 'Note that the call to model %>% fit() returns a history object, as you saw
    in chapter 3\. This object has a member metrics, which is a named list containing
    data about everything that happened during training. Let’s look at it:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对模型的 fit() 调用会返回一个历史对象，就像你在第 3 章中看到的那样。此对象具有一个名为 metrics 的成员，它是一个包含训练过程中发生的所有事情的数据的命名列表。让我们来看一下：
- en: str(history$metrics)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: str(history$metrics)
- en: List of 4
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 4 个列表
- en: '$ loss : num [1:20] 0.526 0.326 0.241 0.191 0.154 …'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '$ loss : num [1:20] 0.526 0.326 0.241 0.191 0.154 …'
- en: '$ accuracy : num [1:20] 0.799 0.899 0.921 0.937 0.951 …'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '$ accuracy : num [1:20] 0.799 0.899 0.921 0.937 0.951 …'
- en: '$ val_loss : num [1:20] 0.415 0.327 0.286 0.276 0.285 …'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '$ val_loss : num [1:20] 0.415 0.327 0.286 0.276 0.285 …'
- en: '$ val_accuracy: num [1:20] 0.857 0.876 0.891 0.89 0.886 …'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '$ val_accuracy: num [1:20] 0.857 0.876 0.891 0.89 0.886 …'
- en: 'The metrics list contains four entries: one per metric that was being monitored
    during training and during validation. We’ll use the plot() method for the history
    object to plot the training and validation loss side by side, as well as the training
    and validation accuracy (see [figure 4.4](#fig4-4)). Note that your own results
    may vary slightly due to a different random initialization of your model.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 指标列表包含四个条目：每个条目分别监视训练和验证期间的指标。 我们将使用历史对象的plot()方法将训练和验证损失并排绘制出来，以及训练和验证准确性（参见[图4.4](#fig4-4)）。
    请注意，由于模型的不同随机初始化，您自己的结果可能会略有不同。
- en: plot(history)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history)
- en: '![Image](../images/f0111-01.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0111-01.jpg)'
- en: '**Figure 4.4 Training and validation loss and accuracy metrics**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.4训练和验证损失以及准确性指标**'
- en: 'As you can see, the training loss decreases with every epoch, and the training
    accuracy increases with every epoch. That’s what you would expect when running
    gradient descent optimization—the quantity you’re trying to minimize should be
    less with every iteration. But that isn’t the case for the validation loss and
    accuracy: they seem to peak at the fourth epoch. This is an example of what we
    warned against earlier: a model that performs better on the training data isn’t
    necessarily a model that will do better on data it has never seen before. In precise
    terms, what you’re seeing is *overfitting*: after the fourth epoch, you’re overoptimizing
    on the training data, and you end up learning representations that are specific
    to the training data and don’t generalize to data outside of the training set.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，训练损失随每个时期减少，而训练准确性随每个时期增加。 这是当运行梯度下降优化时您可以预期的情况 - 您试图最小化的数量应该在每次迭代中减少。
    但是，验证损失和准确性并非如此：它们似乎在第四个时期达到顶峰。 这是我们早些时候警告过的一个例子：在训练数据上表现更好的模型不一定是在以前从未见过的数据上表现更好的模型。
    在精确的术语中，您看到的是*过度拟合*：在第四个时期之后，您过度优化了训练数据，并且最终学习了特定于训练数据的表示，而这些表示不能推广到训练集之外的数据。
- en: Training history plot() method
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 训练历史plot()方法
- en: 'The plot() method for training history objects uses ggplot2 for plotting if
    it’s available (if it isn’t, base graphics are used). The plot includes all specified
    metrics as well as the loss; it draws a smoothing line if there are 10 or more
    epochs. You can customize all of this behavior via various arguments to the plot()
    method. If you want to create a custom visualization, call the as.data.frame()
    method on history to obtain a data frame with factors for each metric as well
    as training versus validation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可用（如果不可用，则使用基本图形），则用于绘制训练历史对象的plot()方法使用ggplot2进行绘制。 绘图包括所有指定的指标以及损失; 如果有10个或更多时期，则绘制平滑线。
    您可以通过plot()方法的各种参数自定义所有这些行为。 如果要创建自定义可视化，请调用history上的as.data.frame()方法以获得每个指标以及训练与验证的因子的数据框：
- en: history_df <- as.data.frame(history)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: history_df <- as.data.frame(history)
- en: str(history_df)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: str(history_df)
- en: '''data.frame'': 80 obs. of 4 variables:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '''data.frame'': 80 obs. of 4 variables:'
- en: '$ epoch : int 1 2 3 4 5 6 7 8 9 10 …'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '$ epoch : int 1 2 3 4 5 6 7 8 9 10 …'
- en: '$ value : num 0.526 0.326 0.241 0.191 0.154 …'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '$ value : num 0.526 0.326 0.241 0.191 0.154 …'
- en: '$ metric: Factor w/ 2 levels "loss","accuracy": 1 1 1 1 1 1 1 1 1 1 …'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '$ metric: 因子，带有2个级别"损失"，"准确性"：1 1 1 1 1 1 1 1 1 1 …'
- en: '$ data : Factor w/ 2 levels "training","validation": 1 1 1 1 1 1 1 …'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '$ data : 因子，带有2个级别"training"，"validation"：1 1 1 1 1 1 1 …'
- en: In this case, to prevent overfitting, you could stop training after four epochs.
    In general, you can use a range of techniques to mitigate overfitting, which we’ll
    cover in chapter 5\. Let’s train a new model from scratch for four epochs and
    then evaluate it on the test data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，为了防止过度拟合，您可以在四个时期后停止训练。 一般来说，您可以使用一系列技术来减轻过度拟合，我们将在第5章中介绍。 让我们从头开始为四个时期训练一个新模型，然后在测试数据上评估它。
- en: Listing 4.8 Retraining a model from scratch
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.8从头开始重新训练模型
- en: model <- keras_model_sequential() %>%
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 损失= "二进制交叉熵",
- en: metrics = "accuracy")
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 指标= "准确性")
- en: model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)
- en: results <- model %>% evaluate(x_test, y_test)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 结果<- model %>% evaluate(x_test, y_test)
- en: 'The final results are as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下：
- en: results
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 结果
- en: loss  accuracy
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 损失  准确性
- en: 0.2999835 0.8819600➊
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 0.2999835 0.8819600➊
- en: ➊ **The first number, 0.29, is the test loss, and the second number, 0.88, is
    the test accuracy.**
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **第一个数字0.29是测试损失，第二个数字0.88是测试准确性。**
- en: This fairly naive approach achieves an accuracy of 88%. With state-of-the-art
    approaches, you should be able to get close to 95%.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相当幼稚的方法可以达到88%的准确度。采用先进的方法，你应该能够接近95%。
- en: 4.1.5 Using a trained model to generate predictions on new data
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.5 使用训练好的模型在新数据上生成预测
- en: 'After having trained a model, you’ll want to use it in a practical setting.
    You can generate the likelihood of reviews being positive by using the predict()
    method, as you learned in chapter 3:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完一个模型后，你会希望在实际环境中使用它。你可以使用predict()方法生成评论是积极的概率，就像你在第3章中所学到的那样：
- en: model %>% predict(x_test)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% predict(x_test)
- en: '[,1]'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[,1]'
- en: '[1,] 0.20960191'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[1,] 0.20960191'
- en: '[2,] 0.99959260'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[2,] 0.99959260'
- en: '[3,] 0.93098557'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[3,] 0.93098557'
- en: '[4,] 0.83782458'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[4,] 0.83782458'
- en: '[5,] 0.94010764'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[5,] 0.94010764'
- en: '[6,] 0.79225385'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[6,] 0.79225385'
- en: '[7,] 0.99964178'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[7,] 0.99964178'
- en: '[8,] 0.01294626'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[8,] 0.01294626'
- en: …
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: As you can see, the model is confident for some samples (0.99 or more, or 0.01
    or less) but less confident for others (0.6, 0.4).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型对某些样本非常自信（0.99或更高，或0.01或更低），但对其他样本不太自信（0.6，0.4）。
- en: 4.1.6 Further experiments
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.6 更多实验
- en: 'The following experiments will help convince you that the architecture choices
    you’ve made are all fairly reasonable, although there’s still room for improvement:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实验将有助于确信你所做的架构选择都相当合理，尽管仍有改进的空间：
- en: You used two representation layers before the final classification layer. Try
    using one or three representation layers, and see how doing so affects validation
    and test accuracy.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最终分类层之前，你使用了两个表示层。尝试使用一个或三个表示层，看看这样做对验证和测试准确性的影响。
- en: 'Try using layers with more units or fewer units: 32 units, 64 units, and so
    on.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用更多单元或更少单元的层：32个单元，64个单元，等等。
- en: Try using the mse loss function instead of binary_crossentropy.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用均方误差损失函数而不是二元交叉熵。
- en: Try using the tanh activation (an activation that was popular in the early days
    of neural networks) instead of relu
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用tanh激活函数（在神经网络的早期比较流行）而不是relu。
- en: 4.1.7 Wrapping up
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.7 总结
- en: 'Here’s what you should take away from this example:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你从这个例子中应该得到的结论：
- en: You usually need to do quite a bit of preprocessing on your raw data to be able
    to feed it—as tensors—into a neural network. Sequences of words can be encoded
    as binary vectors, but other encoding options are also available
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，你需要对原始数据进行相当多的预处理才能将其作为张量输入到神经网络中。单词序列可以编码为二进制向量，但也有其他编码选项。
- en: Stacks of layer_dense() with relu activations can solve a wide range of problems
    (including sentiment classification), and you’ll likely use them frequently.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有relu激活的layer_dense()堆叠层可以解决各种问题（包括情感分类），你很可能经常使用它们。
- en: 'In a binary classification problem (two output classes), your model should
    end with a layer_dense() with one unit and a sigmoid activation: the output of
    your model should be a scalar between 0 and 1, encoding a probability.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二分类问题（两个输出类别）中，你的模型应该以一个具有一个单元和sigmoid激活函数的layer_dense()层结束：模型的输出应该是一个介于0和1之间的标量，代表概率。
- en: With such a scalar sigmoid output on a binary classification problem, the loss
    function you should use is binary_crossentropy
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二分类问题上，由于输出为标量sigmoid值，你应该使用二元交叉熵损失函数。
- en: The rmsprop optimizer is generally a good enough choice, whatever your problem.
    That’s one less thing for you to worry about.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rmsprop优化器通常是一个足够好的选择，无论你的问题是什么。这样你就少了一件要担心的事情。
- en: As they get better on their training data, neural networks eventually start
    over-fitting and end up obtaining increasingly worse results on data they’ve never
    seen before. Be sure to always monitor performance on data that is outside of
    the training set
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着神经网络在训练数据上表现越来越好，它们最终会开始过拟合，并且在从未见过的数据上获得越来越差的结果。一定要始终监控在训练集之外的数据上的性能。
- en: '4.2 Classifying newswires: A multiclass classification example'
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2. 对新闻线进行分类：一个多类别分类示例。
- en: In the previous section, you saw how to classify vector inputs into two mutually
    exclusive classes using a densely connected neural network. But what happens when
    you have more than two classes?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，你看到了如何使用全连接神经网络将向量输入分类为两个互斥的类别。但是当你有超过两个类别时会发生什么呢？
- en: In this section, we’ll build a model to classify Reuters newswires into 46 mutually
    exclusive topics. Because we have many classes, this problem is an instance of
    *multi-class classification*, and because each data point should be classified
    into only one category, the problem is more specifically an instance of *single-label
    multiclass classification*. If each data point could belong to multiple categories
    (in this case, topics), we’d be facing a *multilabel multiclass classification*
    problem.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在这一部分中，我们将构建一个模型来将路透社新闻稿分类到46个互斥的主题中。因为我们有很多类别，这个问题是*多类别分类*的一个实例，并且因为每个数据点应该被分类到只有一个类别，所以这个问题更具体地说是*单标签多类别分类*的一个实例。如果每个数据点可以属于多个类别（在本例中，主题），我们将面临一个*多标签多类别分类*问题。'
- en: 4.2.1 The Reuters dataset
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   4.2.1 路透社数据集'
- en: You’ll work with the *Reuters dataset*, a set of short newswires and their topics,
    published by Reuters in 1986\. It’s a simple, widely used toy dataset for text
    classification. The dataset contains 46 different topics; some topics are more
    represented than others, but each topic has at least 10 examples in the training
    set. Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras.
    Let’s take a look.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '-   你将使用*路透社数据集*，这是由路透社于1986年发布的一组短新闻和它们的主题。它是一个简单、广泛使用的文本分类玩具数据集。数据集包含46个不同的主题；一些主题比其他主题更具代表性，但每个主题在训练集中至少有10个示例。和IMDB以及MNIST一样，路透社数据集作为Keras的一部分打包提供。让我们来看看。'
- en: Listing 4.9 Loading the Reuters dataseta
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '-   列表4.9 加载路透社数据集'
- en: reuters <- dataset_reuters(num_words = 10000)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '-   reuters <- dataset_reuters(num_words = 10000)'
- en: c(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '-   c(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters'
- en: 'As with the IMDB dataset, the argument num_words = 10000 restricts the data
    to the 10,000 most frequently occurring words found in the data. You have 8,982
    training examples and 2,246 test examples:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '-   与IMDB数据集一样，参数num_words = 10000将数据限制在数据中出现频率最高的10000个单词中。你有8982个训练示例和2246个测试示例：'
- en: length(train_data)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '-   length(train_data)'
- en: '[1] 8982'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [1] 8982'
- en: length(test_data)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '-   length(test_data)'
- en: '[1] 2246'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [1] 2246'
- en: 'As with the IMDB reviews, each example is a list of integers (word indices):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '-   与IMDB评论一样，每个示例都是一个整数列表（单词索引）：'
- en: str(train_data)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '-   str(train_data)'
- en: List of 8982
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '-   列表8982的列表'
- en: '$ : int [1:87] 1 2 2 8 43 10 447 5 25 207 …'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:87] 1 2 2 8 43 10 447 5 25 207 …'
- en: '$ : int [1:56] 1 3267 699 3434 2295 56 2 7511 9 56 …'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:56] 1 3267 699 3434 2295 56 2 7511 9 56 …'
- en: '$ : int [1:139] 1 53 12 284 15 14 272 26 53 959 …'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:139] 1 53 12 284 15 14 272 26 53 959 …'
- en: '$ : int [1:224] 1 4 686 867 558 4 37 38 309 2276 …'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:224] 1 4 686 867 558 4 37 38 309 2276 …'
- en: '$ : int [1:101] 1 8295 111 8 25 166 40 638 10 436 …'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:101] 1 8295 111 8 25 166 40 638 10 436 …'
- en: '$ : int [1:116] 1 4 37 38 309 213 349 1632 48 193 …'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:116] 1 4 37 38 309 213 349 1632 48 193 …'
- en: '$ : int [1:100] 1 56 5539 925 149 8 16 23 931 3875 …'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:100] 1 56 5539 925 149 8 16 23 931 3875 …'
- en: '$ : int [1:100] 1 53 648 26 14 749 26 39 6207 5466 …'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '-   $ : int [1:100] 1 53 648 26 14 749 26 39 6207 5466 …'
- en: '[list output truncated]'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [list output truncated]'
- en: Here’s how you can decode it back to words, in case you’re curious.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在你好奇的情况下，这里是如何将它解码回单词的方法。'
- en: Listing 4.10 Decoding newswires back to text
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '-   列表4.10 将新闻线路解码回文本'
- en: word_index <- dataset_reuters_word_index()
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '-   word_index <- dataset_reuters_word_index()'
- en: reverse_word_index <- names(word_index)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '-   reverse_word_index <- names(word_index)'
- en: names(reverse_word_index) <- as.character(word_index)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '-   names(reverse_word_index) <- as.character(word_index)'
- en: decoded_words <- train_data[[1]] %>%
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '-   decoded_words <- train_data[[1]] %>%'
- en: sapply(function(i) {
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '-   sapply(function(i) {'
- en: if (i > 3) reverse_word_index[[as.character(i - 3)]]➊
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '-   if (i > 3) reverse_word_index[[as.character(i - 3)]]➊'
- en: else "?"
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '-   else "?"'
- en: '})'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '-   })'
- en: decoded_review <- paste0(decoded_words, collapse = " ")
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '-   decoded_review <- paste0(decoded_words, collapse = " ")'
- en: decoded_review
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '-   decoded_review'
- en: '[1] "? ? ? said as a result of its december acquisition of space co it'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '-   [1] "? ? ? said as a result of its december acquisition of space co it'
- en: '![Image](../images/common01.jpg) expects …"'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '-   ![图像](../images/common01.jpg) expects …"'
- en: ➊ **Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices
    for "padding," "start of sequence," and "unknown."**
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '-   ➊ **请注意，索引偏移了3，因为0、1和2是为“填充”、“序列开始”和“未知”保留的索引。**'
- en: 'The label associated with an example is an integer between 0 and 45—a topic
    index:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '-   与示例相关联的标签是介于0和45之间的整数—一个主题索引：'
- en: str(train_labels)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '-   str(train_labels)'
- en: int [1:8982] 3 4 3 4 4 4 4 3 3 16 …
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '-   int [1:8982] 3 4 3 4 4 4 4 3 3 16 …'
- en: 4.2.2 Preparing the data
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   4.2.2 准备数据'
- en: You can vectorize the data with the same approach as in the previous example.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '-   你可以用和前面例子中一样的方法对数据进行向量化。'
- en: Listing 4.11 Encoding the input data
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '-   列表4.11 对输入数据进行编码'
- en: vectorize_sequences <- function(sequences, dimension = 10000) {
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '-   vectorize_sequences <- function(sequences, dimension = 10000) {'
- en: results <- matrix(0, nrow = length(sequences), ncol = dimension)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '-   results <- matrix(0, nrow = length(sequences), ncol = dimension)'
- en: for (i in seq_along(sequences))
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq_along(sequences))
- en: results[i, sequences[[i]]] <- 1
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: results[i, sequences[[i]]] <- 1
- en: results
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: results
- en: '}'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: x_train <- vectorize_sequences(train_data)➊
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: x_train <- vectorize_sequences(train_data)➊
- en: x_test <- vectorize_sequences(test_data)➋
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: x_test <- vectorize_sequences(test_data)➋
- en: ➊ **Vectorized training data**
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **向量化的训练数据**
- en: ➋ **Vectorized test data**
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **向量化的测试数据**
- en: 'To vectorize the labels, you have two possibilities: you can cast the label
    list as an integer tensor, or you can use *one-hot encoding*. One-hot encoding
    is a widely used format for categorical data, also called *categorical encoding*.
    In this case, one-hot encoding of the labels consists of embedding each label
    as an all-zero vector with a 1 in the place of the label index. The following
    listing shows an example.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要对标签进行向量化，你有两种选择：你可以将标签列表转换为整数张量，或者你可以使用 *独热编码*。独热编码是一种广泛使用的类别数据格式，也被称为 *类别编码*。在这种情况下，标签的独热编码包括将每个标签嵌入为一个全零向量，在标签索引的位置上有一个
    1。下面的列表显示了一个示例。
- en: Listing 4.12 Encoding the labels
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.12 编码标签
- en: to_one_hot <- function(labels, dimension = 46) {
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: to_one_hot <- function(labels, dimension = 46) {
- en: results <- matrix(0, nrow = length(labels), ncol = dimension)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: results <- matrix(0, nrow = length(labels), ncol = dimension)
- en: labels <- labels + 1➊
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: labels <- labels + 1➊
- en: for(i in seq_along(labels)) {
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in seq_along(labels)) {
- en: j <- labels[[i]]
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: j <- labels[[i]]
- en: results[i, j] <- 1
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: results[i, j] <- 1
- en: '}'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: results
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: results
- en: '}'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: y_train <- to_one_hot(train_labels)➋
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: y_train <- to_one_hot(train_labels)➋
- en: y_test <- to_one_hot(test_labels)➌
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: y_test <- to_one_hot(test_labels)➌
- en: ➊ **Vectorized training labels**
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **向量化的训练标签**
- en: ➋ **Vectorized test labels**
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **向量化的测试标签**
- en: ➌ **Labels are 0-based**a
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **标签是从0开始的**
- en: 'Note that there is a built-in way to do this in Keras:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 Keras 中有一种内置的方法可以做到这一点：
- en: y_train <- to_categorical(train_labels)
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: y_train <- to_categorical(train_labels)
- en: y_test <- to_categorical(test_labels)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: y_test <- to_categorical(test_labels)
- en: 4.2.3 Building your model
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 构建你的模型
- en: 'This topic-classification problem looks similar to the previous movie-review
    classification problem: in both cases, we’re trying to classify short snippets
    of text. But there is a new constraint here: the number of output classes has
    gone from 2 to 46\. The dimensionality of the output space is much larger.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题分类问题看起来与之前的电影评论分类问题相似：在这两种情况下，我们都在尝试对短文本片段进行分类。但是这里有一个新的约束：输出类别的数量从2增加到46。输出空间的维度要大得多。
- en: 'In a stack of layer_dense()s like those we’ve been using, each layer can access
    only information present in the output of the previous layer. If one layer drops
    some information relevant to the classification problem, this information can
    never be recovered by later layers: each layer can potentially become an information
    bottleneck. In the previous example, we used 16-dimensional intermediate layers,
    but a 16-dimensional space may be too limited to learn to separate 46 different
    classes: such small layers may act as information bottlenecks, permanently dropping
    relevant information. For this reason we’ll use larger layers. Let’s go with 64
    units.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在像我们一直使用的 layer_dense() 堆栈中，每一层只能访问前一层输出中的信息。如果一层丢失了与分类问题相关的信息，这些信息将无法被后续层恢复：每一层都可能成为信息瓶颈。在前一个例子中，我们使用了
    16 维的中间层，但 16 维空间可能太小，无法学习将 46 个不同类别分开：如此小的层可能会成为信息瓶颈，永久丢弃相关信息。出于这个原因，我们将使用较大的层。让我们使用
    64 个单元。
- en: Listing 4.13 Model definition
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.13 模型定义
- en: model <- keras_model_sequential() %>%
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: layer_dense(46, activation = "softmax")
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(46, activation = "softmax")
- en: You should note two other things about this architecture. First, we end the
    model with a layer_dense() of size 46\. This means for each input sample, the
    network will output a 46-dimensional vector. Each entry in this vector (each dimension)
    will encode a different output class.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意这个架构的另外两件事。首先，我们以大小为 46 的 layer_dense() 结束模型。这意味着对于每个输入样本，网络会输出一个 46 维向量。该向量中的每个条目（每个维度）将编码不同的输出类别。
- en: Second, the last layer uses a softmax activation. You saw this pattern in the
    MNIST example. It means the model will output a *probability distribution* over
    the 46 different output classes—for every input sample, the model will produce
    a 46-dimensional output vector, where output[i] is the probability that the sample
    belongs to class i. The 46 scores will sum to 1.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，最后一层使用了softmax激活函数。你在MNIST示例中看到过这种模式。它意味着模型将对46个不同的输出类别输出一个*概率分布*，对于每个输入样本，模型将产生一个46维的输出向量，其中output[i]是样本属于类别i的概率。这46个分数将总和为1。
- en: 'The best loss function to use in this case is categorical_crossentropy. It
    measures the distance between two probability distributions: here, between the
    probability distribution output by the model and the true distribution of the
    labels. By minimizing the distance between these two distributions, you train
    the model to output something as close as possible to the true labels.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最好使用categorical_crossentropy作为损失函数。它衡量了模型输出的概率分布与标签的真实概率分布之间的距离。通过最小化这两个分布之间的距离，训练模型输出尽可能接近真实标签。
- en: Listing 4.14 Compiling the model
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.14 编译模型
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "categorical_crossentropy",
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = "categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: 4.2.4 Validating your approach
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 验证你的方法
- en: Let’s set apart 1,000 samples in the training data to use as a validation set.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在训练数据中留出1000个样本作为验证集。
- en: Listing 4.15 Setting aside a validation set
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.15 设置验证集
- en: val_indices <- 1:1000
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: val_indices <- 1:1000
- en: x_val <- x_train[val_indices, ]
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: x_val <- x_train[val_indices, ]
- en: partial_x_train <- x_train[-val_indices, ]
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 部分_x_train <- x_train[-val_indices, ]
- en: y_val <- y_train[val_indices, ]
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: y_val <- y_train[val_indices, ]
- en: partial_y_train <- y_train[-val_indices, ]
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 部分_y_train <- y_train[-val_indices, ]
- en: Now let’s train the model for 20 epochs.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练20个epochs的模型。
- en: Listing 4.16 Training the model
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.16 训练模型
- en: history <- model %>% fit(
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 历史 <- model %>% fit(
- en: partial_x_train,
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 部分_x_train,
- en: partial_y_train,
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 部分_y_train,
- en: epochs = 20,
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20,
- en: batch_size = 512,
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 512,
- en: validation_data = list(x_val, y_val)
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(x_val, y_val)
- en: )
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: And finally, let’s display its loss and accuracy curves (see [figure 4.5](#fig4-5)).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们显示它的损失和准确率曲线（请参见[图4.5](#fig4-5)）。
- en: Listing 4.17 Plotting the training and validation loss and accuracy
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.17 绘制训练和验证损失和准确率
- en: plot(history)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history)
- en: '![Image](../images/f0118-01.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0118-01.jpg)'
- en: '**Figure 4.5 Loss and accuracy curves of the validation set**'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.5 验证集的损失和准确率曲线**'
- en: The model begins to overfit after nine epochs. Let’s train a new model from
    scratch for nine epochs and then evaluate it on the test set.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在九个epochs之后，模型开始过拟合。让我们从头开始训练一个新模型进行九个epochs，然后在测试集上评估它。
- en: Listing 4.18 Retraining a model from scratch
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.18 从零开始重新训练模型
- en: model <- keras_model_sequential() %>%
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: layer_dense(46, activation = "softmax")
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(46, activation = "softmax")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "categorical_crossentropy",
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = "categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model %>% fit(x_train, y_train, epochs = 9, batch_size = 512)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(x_train, y_train, epochs = 9, batch_size = 512)
- en: results <- model %>% evaluate(x_test, y_test)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 <- model %>% evaluate(x_test, y_test)
- en: 'Here are the final results:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终的结果：
- en: results
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 结果
- en: loss  accuracy
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 损失  准确率
- en: 0.9562974 0.7898486
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 0.9562974 0.7898486
- en: 'This approach reaches an accuracy of ~80%. With a balanced binary classification
    problem, the accuracy reached by a purely random classifier would be 50%. But
    in this case, we have 46 classes, and they may not be equally represented. What
    would be the accuracy of a random baseline? We could try quickly checking this
    empirically:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法达到了约80%的准确率。对于一个平衡的二分类问题，纯随机分类器达到的准确率将为50%。但是在这种情况下，我们有46个类别，并且它们可能不是平均表示的。一个随机基准的准确率是多少？我们可以快速通过经验来检查一下：
- en: mean(test_labels == sample(test_labels))
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: mean(test_labels == sample(test_labels))
- en: '[1] 0.190561'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 0.190561'
- en: As you can see, a random classifier would score around 19% classification accuracy,
    so the results of our model seem pretty good in that light.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，随机分类器的分类准确率约为19%，所以我们的模型在这个方面看起来相当不错。
- en: 4.2.5 Generating predictions on new data
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.5 在新数据上生成预测
- en: 'Calling the model’s predict() method on new samples returns a class probability
    distribution over all 46 topics for each sample. Let’s generate topic predictions
    for all of the test data:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在新样本上调用模型的 predict() 方法会返回每个样本的所有 46 个主题的类别概率分布。让我们为所有的测试数据生成主题预测：
- en: predictions <- model %>% predict(x_test)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 预测 <- model %>% 预测(x_test)
- en: 'Each entry (row) in “predictions” is a vector of length 46:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: “预测”中的每个条目（行）都是长度为 46 的向量：
- en: str(predictions)
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: str(predictions)
- en: num [1:2246, 1:46] 0.0000873 0.0013171 0.0094679 0.0001123 0.0001032 …
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:2246, 1:46] 0.0000873 0.0013171 0.0094679 0.0001123 0.0001032 …
- en: 'The coefficients in this vector sum to 1, because they form a probability distribution:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量中的系数之和为 1，因为它们形成了概率分布：
- en: sum(predictions[1, ])
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: sum(predictions[1, ])
- en: '[1] 1'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 1'
- en: 'The largest entry is the predicted class—the class with the highest probability:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的条目是预测类别——具有最高概率的类别：
- en: which.max(predictions[1, ])
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: which.max(predictions[1, ])
- en: '[1] 5'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 5'
- en: 4.2.6 A different way to handle the labels and the loss
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.6 处理标签和损失的另一种方式
- en: 'We mentioned earlier that another way to encode the labels would be to preserve
    their integer values, like this:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，编码标签的另一种方式是保留它们的整数值，就像这样：
- en: y_train <- train_labels
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: y_train <- 训练标签
- en: y_test <- test_labels
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: y_test <- 测试标签
- en: 'The only thing this approach would change is the choice of the loss function.
    The loss function used in listing 4.18, categorical_crossentropy, expects the
    labels to follow a categorical encoding. With integer labels, you should use sparse_categorical_
    crossentropy:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法唯一会改变的是损失函数的选择。列表 4.18 中使用的损失函数 categorical_crossentropy，期望标签遵循分类编码。对于整数标签，你应该使用
    sparse_categorical_ crossentropy：
- en: model %>% compile(
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% 编译(
- en: optimizer = "rmsprop",
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "accuracy")
- en: This new loss function is still mathematically the same as categorical_crossentropy;
    it just has a different interface.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的损失函数在数学上与 categorical_crossentropy 相同；它只是具有不同的接口。
- en: 4.2.7 The importance of having sufficiently large intermediate layers
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.7 拥有足够大的中间层的重要性
- en: 'We mentioned earlier that because the final outputs are 46-dimensional, you
    should avoid intermediate layers with many fewer than 46 units. Now let’s see
    what happens when we introduce an information bottleneck by having intermediate
    layers that are significantly less than 46-dimensional: for example, 4-dimensional.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，因为最终输出是 46 维的，所以应该避免中间层的单位少于 46 个。现在让我们看看当我们通过引入一个信息瓶颈，使中间层明显少于 46 维时会发生什么：例如，4
    维。
- en: Listing 4.19 A model with an information bottleneck
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.19 具有信息瓶颈的模型
- en: model <- keras_model_sequential() %>%
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, 激活 = "relu") %>%
- en: layer_dense(4, activation = "relu") %>%
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(4, 激活 = "relu") %>%
- en: layer_dense(46, activation = "softmax")
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(46, 激活 = "softmax")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% 编译(optimizer = "rmsprop",
- en: loss = "categorical_crossentropy",
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "accuracy")
- en: model %>% fit(
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(
- en: partial_x_train,
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: partial_x_train,
- en: partial_y_train,
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: partial_y_train,
- en: epochs = 20,
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20,
- en: batch_size = 128,
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128,
- en: validation_data = list(x_val, y_val)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据 = list(x_val, y_val)
- en: )
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: The model now peaks at ~71% validation accuracy, an 8% absolute drop. This drop
    is mostly because we’re trying to compress a lot of information (enough information
    to recover the separation hyperplanes of 46 classes) into an intermediate space
    that is too low-dimensional. The model is able to cram *most* of the necessary
    information into these 4-dimensional representations, but not all of it.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型的验证准确率达到了约 71%，绝对下降了 8%。这个下降主要是因为我们试图将大量信息（足以恢复 46 个类别的分离超平面的信息）压缩到一个维度太低的中间空间中。模型能够将*大部分*必要的信息压缩到这些
    4 维表示中，但不是全部。
- en: 4.2.8 Further experiments
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.8 进一步的实验
- en: 'Like in the previous example, I encourage you to try out the following experiments
    to train your intuition about the kind of configuration decisions you have to
    make with such models:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面的例子一样，我鼓励你尝试以下实验来培养你对这种模型配置决策的直觉：
- en: 'Try using larger or smaller layers: 32 units, 128 units, and so on.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用更大或更小的层：32 个单元、128 个单元等等。
- en: You used two intermediate layers before the final softmax classification layer.
    Now try using a single intermediate layer, or three intermediate layers.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最终的 softmax 分类层之前，你使用了两个中间层。现在尝试使用一个单独的中间层，或者三个中间层。
- en: 4.2.9 Wrapping up
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.9 总结
- en: 'Here’s what you should take away from this example: if you’re trying to classify
    data points among *N* classes, your model should end with a layer_dense() of size
    *N*.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子告诉我们以下几点：如果你试图将数据点分类到*N*个类别中，你的模型应该以大小为*N*的layer_dense()结尾。
- en: In a single-label, multiclass classification problem, your model should end
    with a softmax activation so that it will output a probability distribution over
    the *N* output classes.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在单标签、多类别分类问题中，你的模型应该以softmax激活结束，以便输出*N*个输出类别的概率分布。
- en: categorical_crossentropy is almost always the loss function you should use for
    such problems. It minimizes the distance between the probability distributions
    output by the model and the true distribution of the targets.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: categorical_crossentropy几乎总是你应该在这些问题中使用的损失函数。它最小化了模型输出的概率分布与目标的真实分布之间的距离。
- en: 'You can handle labels in multiclass classification in the following two ways:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类别分类中，你可以通过以下两种方式处理标签：
- en: Encode the labels via categorical encoding (also known as one-hot encoding)
    and use categorical_crossentropy as a loss function.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分类编码（也称为独热编码）对标签进行编码，并使用categorical_crossentropy作为损失函数。
- en: Encode the labels as integers and use the sparse_categorical_crossentropy loss
    function
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标签编码为整数，并使用sparse_categorical_crossentropy损失函数
- en: If you need to classify data into a large number of categories, you should avoid
    creating information bottlenecks in your model due to intermediate layers that
    are too small.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要将数据分类到大量的类别中，你应该避免因中间层太小而在模型中创建信息瓶颈。
- en: '4.3 Predicting house prices: A regression example'
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 预测房价：一个回归示例
- en: The two previous examples were considered classification problems, where the
    goal was to predict a single discrete label of an input data point. Another common
    type of machine learning problem is *regression*, which consists of predicting
    a continuous value instead of a discrete label, for instance, predicting the temperature
    tomorrow, given meteorological data, or predicting the time that a software project
    will take to complete, given its specifications.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的两个例子都被视为分类问题，其目标是预测输入数据点的单个离散标签。另一种常见的机器学习问题是*回归*，它包括预测连续值而不是离散标签，例如，根据气象数据预测明天的温度，或者根据软件项目的规格预测完成所需的时间。
- en: Don’t confuse *regression* with the *logistic regression* algorithm. Confusingly,
    logistic regression isn’t a regression algorithm—it’s a classification algorithm.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆*回归*和*逻辑回归*算法。令人困惑的是，逻辑回归并不是一个回归算法，而是一个分类算法。
- en: 4.3.1 The Boston housing price dataset
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 波士顿房价数据集
- en: 'In this section, we’ll attempt to predict the median price of homes in a given
    Boston suburb in the mid-1970s, given data points about the suburb at the time,
    such as the crime rate, the local property tax rate, and so on. The dataset we’ll
    use has an interesting difference from the two previous examples. It has relatively
    few data points: only 506, split between 404 training samples and 102 test samples.
    And each *feature* in the input data (e.g., the crime rate) has a different scale.
    For instance, some values are proportions, which take values between 0 and 1,
    others take values between 1 and 12, others between 0 and 100, and so on.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将尝试预测20世纪70年代中期波士顿某郊区房屋的中位价格，给定当时有关郊区的数据点，例如犯罪率、当地房产税率等。我们将使用的数据集与前两个例子有一个有趣的区别。它的数据点相对较少：只有506个，分为404个训练样本和102个测试样本。而且输入数据中的每个*特征*（例如，犯罪率）都有不同的比例。例如，一些值是比例，取值介于0和1之间，其他取值介于1和12之间，还有其他取值介于0和100之间，依此类推。
- en: Listing 4.20 Loading the Boston housing dataset
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.20 加载波士顿房屋数据集
- en: boston <- dataset_boston_housing()
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: boston <- dataset_boston_housing()
- en: c(c(train_data, train_targets), c(test_data, test_targets)) %<-% boston
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: c(c(train_data, train_targets), c(test_data, test_targets)) %<-% boston
- en: 'Let’s look at the data:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看数据：
- en: str(train_data)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: str(train_data)
- en: num [1:404, 1:13] 1.2325 0.0218 4.8982 0.0396 3.6931 …
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:404, 1:13] 1.2325 0.0218 4.8982 0.0396 3.6931 …
- en: str(test_data)
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: str(test_data)
- en: num [1:102, 1:13] 18.0846 0.1233 0.055 1.2735 0.0715 …
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:102, 1:13] 18.0846 0.1233 0.055 1.2735 0.0715 …
- en: 'As you can see, we have 404 training samples and 102 test samples, each with
    13 numerical features, such as per capita crime rate, average number of rooms
    per dwelling, accessibility to highways, and so on. The targets are the median
    values of owner-occupied homes, in thousands of dollars:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们有404个训练样本和102个测试样本，每个样本都有13个数值特征，例如人均犯罪率、每个住宅的平均房间数、高速公路的可达性等。目标是自住房屋的中位数价值，以千美元计算：
- en: str(train_targets)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: str(train_targets)
- en: num [1:404(1d)] 15.2 42.3 50 21.1 17.7 18.5 11.3 15.6 15.6 14.4 …
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:404(1d)] 15.2 42.3 50 21.1 17.7 18.5 11.3 15.6 15.6 14.4 …
- en: The prices are typically between $10,000 and $50,000\. If that sounds cheap,
    remember that this was the mid-1970s, and these prices aren’t adjusted for inflation.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 价格通常在1万到5万美元之间。如果听起来很便宜，记住这是上世纪70年代中期，这些价格并未经过通货膨胀调整。
- en: 4.3.2 Preparing the data
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 准备数据
- en: 'It would be problematic to feed into a neural network values that all take
    wildly different ranges. The model might be able to automatically adapt to such
    heterogeneous data, but it would definitely make learning more difficult. A widespread
    best practice for dealing with such data is to do feature-wise normalization:
    for each feature in the input data (a column in the input data matrix), we subtract
    the mean of the feature and divide by the standard deviation, so that the feature
    is centered around 0 and has a unit standard deviation. This is easily done in
    R using the scale() function.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 把各种取值范围差异巨大的值喂给神经网络是有问题的。模型可能能够自动适应这样的异构数据，但这肯定会使学习变得更加困难。处理这种数据的广泛最佳实践是进行特征归一化：对于输入数据中的每个特征（输入数据矩阵中的一列），我们减去该特征的平均值，并除以标准差，使得该特征以0为中心，具有单位标准差。在R中使用scale()函数可以轻松实现这一点。
- en: Listing 4.21 Normalizing the data
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.21 归一化数据
- en: mean <- apply(train_data, 2, mean)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: mean <- apply(train_data, 2, mean)
- en: sd <- apply(train_data, 2, sd)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: sd <- apply(train_data, 2, sd)
- en: train_data <- scale(train_data, center = mean, scale = sd)
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: train_data <- scale(train_data, center = mean, scale = sd)
- en: test_data <- scale(test_data, center = mean, scale = sd)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: test_data <- scale(test_data, center = mean, scale = sd)
- en: Note that the quantities used for normalizing the test data are computed using
    the training data. You should never use any quantity computed on the test data
    in your workflow, even for something as simple as data normalization.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，用于归一化测试数据的量是使用训练数据计算的。你绝对不应该在工作流程中使用在测试数据上计算的任何量，即使是像数据归一化这样简单的事情也不行。
- en: 4.3.3 Building your model
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 构建你的模型
- en: Because so few samples are available, we’ll use a very small model with two
    intermediate layers, each with 64 units. In general, the less training data you
    have, the worse overfitting will be, and using a small model is one way to mitigate
    overfitting.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可用的样本非常少，我们将使用一个非常小的模型，其中包含两个中间层，每个层都有64个单元。一般来说，你拥有的训练数据越少，过拟合就会越严重，使用一个小模型是缓解过拟合的一种方式。
- en: Listing 4.22 Model definition
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.22 模型定义
- en: build_model <- function() {➊
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: build_model <- function() {➊
- en: model <- keras_model_sequential() %>%
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: layer_dense(64, activation = "relu") %>%
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(64, activation = "relu") %>%
- en: layer_dense(1)
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1)
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "mse",
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "mse",
- en: metrics = "mae")
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "mae")
- en: model
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '}'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Because we need to instantiate the same model multiple times, we use a function
    to construct it.**
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **因为我们需要多次实例化相同的模型，所以我们使用一个函数来构造它。**
- en: 'The model ends with a single unit and no activation (it will be a linear layer).
    This is a typical setup for scalar regression (a regression where you’re trying
    to predict a single continuous value). Applying an activation function would constrain
    the range the output can take: for instance, if you applied a sigmoid activation
    function to the last layer, the model could learn to predict values only between
    0 and 1\. Here, because the last layer is purely linear, the model is free to
    learn to predict values in any range.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以一个单元和无激活函数（它将是一个线性层）结束。这是标量回归的典型设置（一种回归，你试图预测一个单一的连续值）。应用激活函数会限制输出的范围：例如，如果你在最后一层应用了sigmoid激活函数，模型只能学习预测0到1之间的值。在这里，因为最后一层是纯线性的，模型可以自由地学习预测任何范围内的值。
- en: Note that we compile the model with the mse loss function—*mean squared error*
    (MSE), the square of the difference between the predictions and the targets. This
    is a widely used loss function for regression problems.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用mse损失函数进行模型编译——*均方误差*（MSE），即预测值与目标值之间的差的平方。这是解决回归问题时广泛使用的损失函数。
- en: 'We’re also monitoring a new metric during training: *mean absolute error* (MAE).
    It’s the absolute value of the difference between the predictions and the targets.
    For instance, an MAE of 0.5 on this problem would mean your predictions are off
    by $500 on average.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在训练过程中监控一个新的指标：*平均绝对误差*（MAE）。它是预测值与目标值之间差值的绝对值。例如，在这个问题上的MAE为0.5，意味着您的预测平均偏差为500美元。
- en: 4.3.4 Validating your approach using K-fold validation
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 使用K倍验证验证您的方法
- en: 'To evaluate our model while we keep adjusting its parameters (such as the number
    of epochs used for training), we could split the data into a training set and
    a validation set, as we did in the previous examples. But because we have so few
    data points, the validation set would end up being very small (e.g., about 100
    examples). As a consequence, the validation scores might change a lot depending
    on which data points we chose for validation and which we chose for training:
    the validation scores might have a high *variance* with regard to the validation
    split. This would prevent us from reliably evaluating our model.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们不断调整模型参数的同时评估我们的模型（例如用于训练的轮次数），我们可以将数据分割为训练集和验证集，就像我们在之前的例子中所做的那样。但是由于我们的数据点很少，验证集最终会变得非常小（例如，大约100个示例）。因此，验证分数可能会因我们选择哪些数据点用于验证和哪些用于训练而大不相同：验证分数可能在验证分割方面具有很高的*方差*。这会阻止我们可靠地评估我们的模型。
- en: The best practice in such situations is to use *K*-fold cross-validation (see
    [figure 4.6](#fig4-6)).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下的最佳实践是使用*K*倍交叉验证（参见[图 4.6](#fig4-6)）。
- en: '![Image](../images/f0124-01.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0124-01.jpg)'
- en: '**Figure 4.6 *K*-fold cross-validation with K = 3**'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.6 *K*倍交叉验证，K = 3**'
- en: It consists of splitting the available data into *K* partitions (typically *K*
    = 4 or 5), instantiating *K* identical models, and training each one on *K*– 1
    partitions while evaluating on the remaining partition. The validation score for
    the model used is then the average of the *K* validation scores obtained. In terms
    of code, this is straightforward.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括将可用数据分成*K*个分区（通常*K* = 4或5），实例化*K*个相同的模型，并在训练每个模型时评估剩余分区。然后使用的模型的验证分数是所获得的*K*个验证分数的平均值。从代码的角度来看，这很简单。
- en: Listing 4.23 *K*-fold validation
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.23 *K*倍验证
- en: k <- 4
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: k <- 4
- en: fold_id <- sample(rep(1:k, length.out = nrow(train_data)))
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: fold_id <- sample(rep(1:k, length.out = nrow(train_data)))
- en: num_epochs <- 100
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: num_epochs <- 100
- en: all_scores <- numeric()
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: all_scores <- numeric()
- en: for (i in 1:k) {
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:k) {
- en: 'cat("Processing fold #", i, "\n")'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: cat("处理第", i, "折\n")
- en: val_indices <- which(fold_id == i)➊
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: val_indices <- which(fold_id == i)➊
- en: val_data <- train_data[val_indices, ]➊
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: val_data <- train_data[val_indices, ]➊
- en: val_targets <- train_targets[val_indices]➊
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: val_targets <- train_targets[val_indices]➊
- en: partial_train_data <- train_data[-val_indices, ]➋
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: partial_train_data <- train_data[-val_indices, ]➋
- en: partial_train_targets <- train_targets[-val_indices]➋
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: partial_train_targets <- train_targets[-val_indices]➋
- en: model <- build_model()➌
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: model <- build_model()➌
- en: model %>% fit (➍
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit (➍
- en: partial_train_data,
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: partial_train_data,
- en: partial_train_targets,
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: partial_train_targets,
- en: epochs = num_epochs,
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = num_epochs,
- en: batch_size = 16,
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 16,
- en: verbose = 0
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: verbose = 0
- en: )
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: results <- model %>%
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: results <- model %>%
- en: evaluate(val_data, val_targets, verbose = 0)➎
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: evaluate(val_data, val_targets, verbose = 0)➎
- en: all_scores[[i]] <- results[['mae']]
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: all_scores[[i]] <- results[['mae']]
- en: '}'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Processing fold # 1'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 处理第 1 折
- en: 'Processing fold # 2'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 处理第 2 折
- en: 'Processing fold # 3'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 处理第 3 折
- en: 'Processing fold # 4'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 处理第 4 折
- en: '➊ **Prepare the validation data: data from partition #k.**'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **准备验证数据：来自第k个分区的数据。**
- en: '➋ **Prepare the training data: data from all other partitions.**'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **准备训练数据：来自所有其他分区的数据。**
- en: ➌ **Build the Keras model (already compiled).**
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **构建Keras模型（已编译）。**
- en: ➍ **Train the model (in silent mode, verbose = 0).**
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **训练模型（静默模式，verbose = 0）。**
- en: ➎ **Evaluate the model on the validation data.**
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **在验证数据上评估模型。**
- en: 'Running this with num_epochs = 100 yields the following results:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个，设定 `num_epochs = 100`，得到以下结果：
- en: all_scores
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: all_scores
- en: '[1] 2.435980 2.165334 2.252230 2.362636'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 2.435980 2.165334 2.252230 2.362636'
- en: mean(all_scores)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: mean(all_scores)
- en: '[1] 2.304045'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 2.304045'
- en: The different runs do indeed show rather different validation scores, from 2.1
    to 2.4\. The average (2.3) is a much more reliable metric than any single score—that’s
    the entire point of *K*-fold cross-validation. In this case, we’re off by $2,300
    on average, which is significant considering that the prices range from $10,000
    to $50,000.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的运行确实显示了相当不同的验证分数，从2.1到2.4不等。平均值（2.3）比任何单个分数更可靠——这正是*K*倍交叉验证的全部意义所在。在这种情况下，我们的平均偏差为2,300美元，考虑到价格范围从10,000美元到50,000美元，这是相当显著的。
- en: 'Let’s try training the model a bit longer: 500 epochs. To keep a record of
    how well the model does at each epoch, we’ll modify the training loop to save
    the per-epoch validation score log for each fold.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试更长时间地训练模型：500 个时代。为了记录模型在每个时代的表现如何，我们将修改训练循环以保存每个折叠的每时代验证分数日志。
- en: Listing 4.24 Saving the validation logs at each fold
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.24 在每个折叠保存验证日志
- en: num_epochs <- 500
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: num_epochs <- 500
- en: all_mae_histories <- list()
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: all_mae_histories <- list()
- en: for (i in 1:k) {
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 (i 在 1:k) {
- en: 'cat("Processing fold #", i, "\n")'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 'cat("处理折叠 #", i, "\n")'
- en: val_indices <- which(fold_id == i)➊
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: val_indices <- which(fold_id == i)➊
- en: val_data <- train_data[val_indices, ]➊
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: val_data <- train_data[val_indices, ]➊
- en: val_targets <- train_targets[val_indices]➊
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: val_targets <- train_targets[val_indices]➊
- en: partial_train_data <- train_data[-val_indices, ]➋
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: partial_train_data <- train_data[-val_indices, ]➋
- en: partial_train_targets <- train_targets[-val_indices]➋
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: partial_train_targets <- train_targets[-val_indices]➋
- en: model <- build_model()➌
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: model <- build_model()➌
- en: history <- model %>% fit(➍
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(➍
- en: partial_train_data, partial_train_targets,
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: partial_train_data, partial_train_targets，
- en: validation_data = list(val_data, val_targets),
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = list(val_data, val_targets),
- en: epochs = num_epochs, batch_size = 16, verbose = 0
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = num_epochs, batch_size = 16, verbose = 0
- en: )
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ）
- en: mae_history <- history$metrics$val_mae
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: mae_history <- history$metrics$val_mae
- en: all_mae_histories[[i]] <- mae_history
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: all_mae_histories[[i]] <- mae_history
- en: '}'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Processing fold # 1'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '处理折叠 # 1'
- en: 'Processing fold # 2'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '处理折叠 # 2'
- en: 'Processing fold # 3'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '处理折叠 # 3'
- en: 'Processing fold # 4'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '处理折叠 # 4'
- en: all_mae_histories <- do.call(cbind, all_mae_histories)
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: all_mae_histories <- do.call(cbind, all_mae_histories)
- en: '➊ **Prepare the validation data: data from partition #k.**'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ **准备验证数据：来自第 #k 分区的数据。**'
- en: '➋ **Prepare the training data: data from all other partitions.**'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **准备训练数据：来自所有其他分区的数据。**
- en: ➌ **Build the Keras model (already compiled).**
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **构建 Keras 模型（已编译）。**
- en: ➍ **Train the model (in silent mode, verbose = 0).**
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **以静默模式训练模型（verbose = 0）。**
- en: We can then compute the average of the per-epoch MAE scores for all folds.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算所有折叠的每时代 MAE 分数的平均值。
- en: Listing 4.25 Building the history of successive mean *K*-fold validation scores
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 4.25 构建连续均值 *K*-折验证分数的历史**'
- en: average_mae_history <- rowMeans(all_mae_histories)
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: average_mae_history <- rowMeans(all_mae_histories)
- en: Let’s plot this; see [figure 4.7](#fig4-7).
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来绘制一下；请参阅 [图 4.7](#fig4-7)。
- en: Listing 4.26 Plotting validation scores
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.26 绘制验证分数
- en: plot(average_mae_history, xlab = "epoch", type = 'l')
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: plot(average_mae_history, xlab = "时代", type = 'l')
- en: '![Image](../images/f0126-01.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0126-01.jpg)'
- en: '**Figure 4.7 Validation MAE by epoch**'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.7 通过时代的验证 MAE**'
- en: 'It may be a little difficult to read the plot, due to a scaling issue: the
    validation MAE for the first few epochs is dramatically higher than the values
    that follow. Let’s omit the first 10 data points, which are on a different scale
    than the rest of the curve.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缩放问题，读取图可能有点困难：最初几个时代的验证 MAE 显著高于后面的值。让我们省略前 10 个数据点，这些数据点与曲线的其余部分在不同的比例上。
- en: Listing 4.27 Plotting validation scores, excluding the first 10 data points
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.27 绘制验证分数，排除前 10 个数据点
- en: truncated_mae_history <- average_mae_history[-(1:10)]
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: truncated_mae_history <- average_mae_history[-(1:10)]
- en: plot(average_mae_history, xlab = "epoch", type = 'l',
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: plot(average_mae_history, xlab = "时代", type = 'l',
- en: ylim = range(truncated_mae_history))
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: ylim = range(truncated_mae_history))
- en: As you can see in [figure 4.8](#fig4-8), validation MAE stops improving significantly
    after 100– 140 epochs (this number includes the 10 epochs we omitted). Past that
    point, we start overfitting.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[图 4.8](#fig4-8)中所看到的，验证 MAE 在 100-140 个时代后停止显著改善（这个数字包括我们省略的 10 个时代）。在那之后，我们开始过拟合。
- en: '![Image](../images/f0127-01.jpg)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0127-01.jpg)'
- en: '**Figure 4.8 Validation MAE by epoch, excluding the first 10 data points**'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.8 通过时代排除前 10 个数据点的验证 MAE**'
- en: Once you’re finished tuning other parameters of the model (in addition to the
    number of epochs, you could also adjust the size of the intermediate layers),
    you can train a final production model on all of the training data, with the best
    parameters, and then look at its performance on the test data.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成了模型的其他参数调整（除了时代数量，您还可以调整中间层的大小），您可以使用最佳参数在所有训练数据上训练最终的生产模型，然后查看其在测试数据上的性能。
- en: Listing 4.28 Training the final model
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.28 训练最终模型
- en: model <- build_model()➊
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: model <- build_model()➊
- en: model %>% fit(train_data, train_targets,➋
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(train_data, train_targets,➋
- en: epochs = 120, batch_size = 16, verbose = 0)
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 120, batch_size = 16, verbose = 0)
- en: result <- model %>% evaluate(test_data, test_targets)
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: result <- model %>% evaluate(test_data, test_targets)
- en: ➊ **Get a fresh, compiled model.**
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **获得一个新的、已编译的模型。**
- en: ➋ **Train it on the entirety of the data.**
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在所有数据上训练它。**
- en: 'Here’s the final result:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终结果：
- en: result["mae"]
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: result["mae"]
- en: mae
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: mae
- en: '2.476283'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '2.476283'
- en: We’re still off by a bit under $2,500\. It’s an improvement! Just like with
    the two previous tasks, you can try varying the number of layers in the model,
    or the number of units per layer, to see if you can squeeze out a lower test error.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有少了一点的$2,500。这是一个进步！就像前两个任务一样，你可以尝试改变模型中的层数或每层的单位数，看看是否能够减少测试错误。
- en: 4.3.5 Generating predictions on new data
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 生成新数据的预测
- en: 'When calling predict() on our binary classification model, we retrieved a scalar
    score between 0 and 1 for each input sample. With our multiclass classification
    model, we retrieved a probability distribution over all classes for each sample.
    Now, with this scalar regression model, predict() returns the model’s guess for
    the sample’s price in thousands of dollars:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 当在我们的二元分类模型上调用 predict() 时，我们得到了每个输入样本的0到1之间的标量分数。对于我们的多类分类模型，我们得到了每个样本上所有类别的概率分布。现在，对于这个标量回归模型，predict()
    返回模型对样本价格的猜测，单位是千美元：
- en: predictions <- model %>% predict(test_data)
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- model %>% predict(test_data)
- en: predictions[1, ]
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: predictions[1, ]
- en: '[1] 10.27619'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 10.27619'
- en: The first house in the test set is predicted to have a price of about $10,000.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集中的第一套房子预计价格约为10,000美元。
- en: 4.3.6 Wrapping up
  id: totrans-558
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.6 总结
- en: 'Here’s what you should take away from this scalar regression example:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个标量回归示例中你应该了解到：
- en: Regression is done using different loss functions than we used for classification.
    Mean squared error (MSE) is a loss function commonly used for regression.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归是使用不同的损失函数进行的，而我们用于分类的损失函数不同。均方误差（MSE）是常用于回归的损失函数。
- en: Similarly, evaluation metrics to be used for regression differ from those used
    for classification; naturally, the concept of accuracy doesn’t apply for regression.
    A common regression metric is mean absolute error (MAE).
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，用于回归的评估指标与用于分类的指标不同；自然地，准确度的概念不适用于回归。常见的回归指标是平均绝对误差（MAE）。
- en: When features in the input data have values in different ranges, you should
    scale each feature independently as a preprocessing step.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输入数据中的特征具有不同范围的值时，你应该独立地对每个特征进行缩放作为预处理步骤。
- en: When there is little data available, using *K*-fold validation is a great way
    to reliably evaluate a model.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据很少时，使用*K*折验证是可靠评估模型的好方法。
- en: When little training data is available, it’s preferable to use a small model
    with few intermediate layers (typically only one or two), in order to avoid severe
    overfitting.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据很少时，最好使用只有一两个中间层（通常只有一两个）的小型模型，以避免严重过拟合。
- en: Summary
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The three most common kinds of machine learning tasks on vector data are binary
    classification, multiclass classification, and scalar regression.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据上的三种最常见的机器学习任务是二元分类、多类分类和标量回归。
- en: The “Wrapping up” sections earlier in the chapter summarize the important points
    you’ve learned regarding each task.
  id: totrans-567
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 章节中的“总结”部分总结了你对每个任务学到的重要知识点。
- en: Regression uses different loss functions and different evaluation metrics than
    classification
  id: totrans-568
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归使用不同的损失函数和不同的评估指标，而不是分类
- en: You’ll usually need to preprocess raw data before feeding it into a neural network.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将原始数据输入神经网络之前，通常需要对其进行预处理。
- en: When your data has features with different ranges, scale each feature independently
    as part of preprocessing
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的数据具有不同范围的特征时，作为预处理的一部分，应该独立地对每个特征进行缩放。
- en: As training progresses, neural networks eventually begin to overfit and obtain
    worse results on never-before-seen data.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着训练的进行，神经网络最终开始过拟合，并在以前未见过的数据上获得更差的结果。
- en: If you don’t have much training data, use a small model with only one or two
    intermediate layers, to avoid severe overfitting.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你没有太多的训练数据，使用只有一两个中间层的小型模型，以避免严重过拟合。
- en: If your data is divided into many categories, you may cause information bottlenecks
    if you make the intermediate layers too small.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据被分成许多类别，如果使中间层太小，可能会造成信息瓶颈。
- en: When you’re working with little data, *K*-fold validation can help reliably
    evaluate your model.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你处理少量数据时，*K*折验证可以帮助可靠地评估你的模型。
