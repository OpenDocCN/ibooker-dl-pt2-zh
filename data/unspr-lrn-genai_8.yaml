- en: '8 Deep Learning: the foundational concepts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 深度学习：基本概念
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章介绍
- en: Deep learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习
- en: Building blocks of deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的构建模块
- en: Layers in a neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的层
- en: Activation functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Supervised learning using deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习的监督学习
- en: Unsupervised learning using deep learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习使用深度学习
- en: Python code using tensorflow and keras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用tensorflow和keras的Python代码
- en: Deep learning libraries
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习库
- en: “Life is really simple, but we insist on making it complicated – Confucius”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “生活真的很简单，但我们一直坚持要把它弄复杂 - 孔子”
- en: Welcome to the third part of the book. So far, you have covered a lot of concepts
    and case studies and Python code. From this chapter onwards, the level of complexity
    will be even higher.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到本书的第三部分。到目前为止，你已经学习了很多概念、案例研究和Python代码。从本章开始，复杂性水平将更高。
- en: In the first two parts of the book, we covered various unsupervised learning
    algorithms like clustering, dimensionality reduction etc. We discussed both simpler
    and advanced algorithms. We also covered working on text data in the last part
    of the book. Starting from this third part of the book, we will start our journey
    on deep learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前两部分，我们涵盖了各种无监督学习算法，如聚类，降维等等。我们讨论了简单和高级算法。我们还在本书的最后一部分讨论了处理文本数据的方法。从本书的第三部分开始，我们将开始深度学习的旅程。
- en: Deep learning and neural networks have changed the world and the business domains.
    You must have heard about deep learning and neural networks. Their implementations
    and sophistication results in better cancer detection, autonomous driving cars,
    improved disaster management systems, better pollution control systems, reduced
    fraud in transactions and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和神经网络改变了世界和商业领域。你一定听说过深度学习和神经网络。它们的实施和复杂性导致更好的癌症检测，自动驾驶汽车，改进的灾害管理系统，更好的污染控制系统，减少交易欺诈等等。
- en: In the third part of the book, we will be exploring unsupervised learning using
    deep learning. We will be studying what is deep learning and the basics of neural
    networks to start with. We will study what are the layers in a neural network,
    activation functions, process of deep learning and various libraries. Then we
    will move to Autoencoders and Generative Adversarial Networks and Deep Belief
    Networks. The topics are indeed complex and sometime quite mathematically heavy.
    We will using different kinds of datasets for working on the problems, but primarily
    the datasets will be unstructured in nature. As always, Python will be used to
    generate the solutions. We are also sharing a lot of external resources to complement
    the concepts. Please note that these are pretty advanced topics and a lot of research
    is still undergoing for these topics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第三部分，我们将探讨使用深度学习的无监督学习。我们将学习什么是深度学习和神经网络的基础知识。我们将研究神经网络中的层，激活函数，深度学习的过程和各种库。然后我们将转向自动编码器、生成对抗网络和深度信念网络。这些话题确实很复杂，有时候相当数学密集。我们将使用不同类型的数据集来解决问题，但主要是非结构化的数据集。和往常一样，Python
    将用于生成解决方案。我们还分享了许多外部资源来补充这些概念。请注意，这些都是非常先进的主题，对于这些主题仍在进行大量的研究。
- en: We have divided the third part of the book in three chapters. The eighth chapter
    covers the foundation concepts of deep learning and neural networks required.
    The next two chapters will focus on autoencoders, GAN and Deep belief networks.
    The final chapter of the book talks about the deployment of these models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将第三部分分为三章。第八章介绍了所需的深度学习和神经网络的基础概念。接下来的两章将专注于自动编码器，GAN 和深度信念网络。本书的最后一章讨论了这些模型的部署。
- en: This chapter 8 discusses the concepts of neural networks and deep learning.
    We will be discussing what is a neural network, what are activation functions,
    what are different optimization functions, neural network training process etc.
    It is vital for you to know these concepts of deep learning before you can understand
    the deeper concepts of auto encoders and GAN. The concepts covered in this chapter
    form the base of neural networks and deep learning and subsequent learning in
    the next two chapters. Hence, it is vital that you are clear about these concepts.
    The best external resource to get these concepts in more details are given at
    the end of the chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了神经网络和深度学习的概念。我们将讨论什么是神经网络，什么是激活函数，什么是不同的优化函数，神经网络训练过程等。在你理解自动编码器和生成对抗网络的更深层次概念之前，了解这些深度学习概念对你来说至关重要。本章涵盖的概念是神经网络和深度学习的基础，也是下两章进一步学习的基础。因此，你清楚这些概念非常重要。在本章末尾还有更详细的外部资源可供获取这些概念。
- en: Welcome to the eighth chapter and all the very best!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第八章，祝一切顺利！
- en: 8.1 Technical toolkit
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 技术工具包
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at this location.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用迄今为止使用的相同版本的Python和Jupyter笔记本。本章中使用的代码和数据集已经保存到了此位置。
- en: You would need to install a few Python libraries in this chapter which are –
    tensorflow and keras.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在本章中安装一些Python库，它们是 - tensorflow 和 keras。
- en: Let’s get started with Chapter 8 of the book!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始学习第8章吧！
- en: '8.2 Deep Learning: What is it? What does it do?'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 深度学习：是什么？它是做什么的？
- en: Deep learning has gathered a lot of momentum in the last few years. Neural networks
    are pushing the boundaries of machine learning solutions. Deep learning is machine
    learning only. Deep learning is based on neural networks. It utilizes the similar
    concept i.e. using the historical data, understanding the attributes and the intelligence
    gathered can be then used for finding the patterns or predicting the future, albeit
    deep learning is more complex than the algorithms we have covered so far.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 近几年来，深度学习已经积累了很多动力。神经网络正在推动机器学习解决方案的边界。深度学习只是机器学习。深度学习是基于神经网络的。它利用了相似的概念，即利用历史数据，理解属性和收集的智能可以用于找到模式或预测未来，尽管深度学习比我们迄今为止涵盖的算法更复杂。
- en: Recall from Chapter 1 where we covered concepts of structured and unstructured
    datasets. Unstructured datasets are the text, images, audio, video etc. In Figure
    8-1 we describe the major sources of text, images, audio, video datasets.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下第1章，我们在那里介绍了结构化和非结构化数据集的概念。非结构化数据集包括文本、图像、音频、视频等。在图8-1中，我们描述了文本、图像、音频、视频数据集的主要来源。
- en: Figure 8-1 Unstructured datasets like text, audio, images, videos can be analyzed
    using deep learning. There are multiple sources of such datasets.
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-1 非结构化数据集如文本、音频、图像、视频可以使用深度学习进行分析。这样的数据集有多个来源。
- en: '![A screenshot of a cell phone Description automatically generated](images/08image004.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图 描述自动生成的](images/08image004.png)'
- en: 'Whilst deep learning can be implemented for structured datasets too, it is
    mostly working wonders on unstructured datasets. One of the prime reasons is that
    the classical machine learning algorithms are sometimes not that effective on
    unstructured datasets like that of images, text, audios and videos. We are listing
    a few of the path breaking solutions delivered by deep learning across various
    domains:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习也可以用于结构化数据集，但它在非结构化数据集上的表现却是非常出色的。其主要原因之一是，经典的机器学习算法在像图像、文本、音频和视频这样的非结构化数据集上有时并不那么有效。我们列举了深度学习在各个领域取得的一些突破性解决方案。
- en: '**Medical and pharmaceuticals**: Deep learning sees application in areas such
    as the identification of bones and joint problems, or in determining if there
    are any clots in arteries or veins. In the pharmaceuticals field, it expedites
    clinical trials and helps to reach the target drug faster.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**医学和制药**: 深度学习在骨骼和关节问题的识别，或者在确定动脉或静脉中是否有血栓等领域发挥作用。在制药领域，它可以加快临床试验的进程，并帮助更快地找到目标药物。'
- en: '**Banking and financial sector**: Deep learning-based algorithms are used to
    detect potential fraud in transactions. Using image recognition-based algorithms,
    we can also distinguish fake signatures on cheques.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**银行和金融业**: 基于深度学习的算法可用于检测交易中潜在的欺诈行为。利用基于图像识别的算法，我们还可以区分支票上的伪造签名。'
- en: '**Automobile sector:** You must have heard about autonomous driving aka self-driving
    cars. Using deep learning, the algorithms are able to detect traffic signals,
    pedestrians, other vehicles on the road, their respective distances and so on.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**汽车行业：** 你一定听说过自动驾驶，即自动驾驶汽车。使用深度学习，算法能够检测道路上的交通信号、行人、其他车辆、它们之间的距离等。'
- en: '**Automatic speech recognition** is possible with deep learning. Using the
    sophisticated neural networks, humans are able to create speech recognition algorithms.
    These solutions are being used across Siri, Alexa, Translator, Baidu etc.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用深度学习技术可以实现**自动语音识别**。通过复杂的神经网络，人类能够创建语音识别算法。这些解决方案被应用于 Siri、Alexa、Translator、百度等产品中。
- en: '**Retail:** In the retail sector, using deep learning-based algorithms, humans
    are able to improve the customer targeting and develop advanced and customized
    marketing tactics. The recommendation models to provide next-best products to
    the customers have been improved using deep learning. We are able to get better
    ROI (return-on-investments) and able to improve cross-sell and upsell strategies.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**零售业：** 在零售行业中，使用基于深度学习的算法，人类能够改善客户定位，并开发先进和定制的营销策略。使用深度学习改进了提供下一优产品推荐的模型。我们能够获得更好的投资回报率（ROI），并改善交叉销售和上销售策略。'
- en: '**Image recognition**: Neural networks are improving our image recognition
    techniques: It can be done using convolutional neural networks which is improving
    computer vision. The use cases are many like:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像识别：** 神经网络正在改进我们的图像识别技术：可以使用卷积神经网络来实现，这种技术正在改进计算机视觉。使用案例有很多，比如：'
- en: Deep learning is quite effective for differentiation between cancerous cells
    and benign cells. It can be achieved by using the images of cancerous cells and
    benign cells.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习对于区分癌细胞和良性细胞非常有效。可以通过使用癌细胞和良性细胞的图像来实现。
- en: Automated number plate reading system are already developed using neural networks
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用神经网络已经开发了自动车牌识别系统。
- en: Object detection methods can be developed using deep learning.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用深度学习可以开发目标检测方法。
- en: Motion sensing and tracking systems can be developed using deep learning
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用深度学习可以开发运动感知和跟踪系统。
- en: In disaster management systems, deep learning can detect the presence of humans
    in the impacted areas. Just imagine how precious moments and eventually human
    lives can be saved using better detection.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在灾难管理系统中，深度学习可以检测受影响区域的人员存在。想象一下，使用更好的检测技术可以拯救多么宝贵的时间，最终拯救人的生命。
- en: The use cases listed are certainly not exhaustive. Using deep learning, we are
    able to improve natural language processing (NLP) solutions used to measure customer’s
    sentiments, language translation, text classification, named-entity-recognition
    etc. Across use cases in bioinformatics, military, mobile advertising, telecom,
    technology, supply chain and so on – deep learning is paving the path for the
    future.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所列举的使用案例并不详尽。通过深度学习，我们能够改进用于测量客户情感、语言翻译、文本分类、命名实体识别等的自然语言处理（NLP）解决方案。在生物信息学、军事、移动广告、电信、科技、供应链等领域的使用案例中，深度学习正在为未来铺平道路。
- en: We have now covered how powerful deep learning is. We will now start with the
    building blocks of neural networks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了深度学习的强大之处。现在我们将开始讲述神经网络的构建模块。
- en: 8.3 Building blocks of a neural network
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 神经网络的构建模块
- en: Artificial Neural Networks (ANNs) are said to be inspired by the way a human
    brain works. The human brain is the best machine we currently have access to.
    When we see a picture or a face or hear a tune, we associate a label or a name
    against it. That allows us to train our brain and senses to recognize a picture
    or a face or a tune when we see/hear it again.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）据说受到了人脑工作方式的启发。人脑是我们目前能够接触到的最好的机器。当我们看到一张图片、一张脸或听到一首曲子时，我们会为其贴上一个标签或名称。这使我们能够训练我们的大脑和感官，以便在再次看到/听到时识别出一张图片、一张脸或一首曲子。
- en: ANNs learn to perform similar tasks by learning or getting trained.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络通过学习或接受训练来执行类似的任务。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 小测验 - 回答这些问题来检查你的理解。本书末尾附有答案。'
- en: 1.  What is the meaning of deep learning?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1.  深度学习的含义是什么？
- en: 2.  Neural networks cannot be used for unsupervised learning. True or False.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2.  神经网络不能用于无监督学习。真或假？
- en: 3.  Explore more use cases for deep learning in non-conventional business domains.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 3.  探索深度学习在非传统业务领域的更多应用案例。
- en: We will now move to explore how neural network helps with some business solutions
    in the next section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在下一节中探讨神经网络如何帮助解决商业问题。
- en: 8.3.1 Neural Networks for solutions
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 解决方案的神经网络
- en: 'In deep learning too, the concepts of supervised and unsupervised learning
    are applicable. We are going to cover both types of trainings of the network:
    supervised and unsupervised. It gives you the complete picture. At the same time,
    to fully appreciate the unsupervised deep learning, it is suggested to be clear
    on supervised deep learning process.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，监督和非监督学习的概念也适用。我们将介绍网络的两种训练类型：有监督和无监督。这为您提供了完整的图片。同时，为了充分欣赏无监督的深度学习，建议首先了解有监督深度学习的过程。
- en: Let’s understand the deep learning process using an example. Consider this,
    we wish to create a solution which can identify faces, that means a solution which
    can distinguish faces and also identify the person by allocating a name to the
    face. For training the model, we will have a dataset which will have images of
    people’s face and corresponding names. ANN will start with no prior understanding
    of the image’s dataset or the attributes. The ANN during the process of training,
    will learn the attributes and the identification characteristics from the training
    data and learn them. These learned attributes are then used to distinguish between
    faces. At this moment, we are only covering the process at a high level; we will
    cover this process in much more detail in subsequent sections.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们了解深度学习的过程。例如，我们希望创建一个可以识别人脸的解决方案，也就是说，一个可以区分面部并通过给面部分配一个名称来识别人物的解决方案。为了训练模型，我们将准备一个包含人脸图像和相应名称的数据集。在训练过程中，人工神经网络（ANN）不具有先前对图像数据集或属性的了解。在训练过程中，ANN会从训练数据中学习属性和识别特征，并利用这些学习到的属性来区分面部。此时，我们只涵盖了高层次的过程，后续章节将更详细地介绍这个过程。
- en: You can see a representation of a neural network in Figure 8-2.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图8-2中看到神经网络的表示形式。
- en: Figure 8-2 A typical neural network with neurons and various layers
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-2是一个典型的神经网络，有神经元和各种层。
- en: '![](images/08image006.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image006.png)'
- en: The process in a neural network is quite complex. We will first cover all the
    building blocks of a neural network – like neuron, activation functions, weights,
    bias terms etc. and then move to the process followed in a neural network. We
    will start with the protagonist – a neuron.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的过程相当复杂。我们将首先介绍神经网络中所有构建块的内容，例如神经元、激活函数、权重、偏置项等，并然后讨论神经网络中的过程。我们将从主角-神经元开始。
- en: 8.3.2 Artificial neuron and perceptron
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 人工神经元和感知器
- en: A human brain contains billions of neurons. The neurons are interconnected cells
    in our brains. These neurons receive signals, process them and generate results.
    Artificial neurons are based on biological neurons only and can be said as simplified
    computational models of biological neurons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑包含数十亿个神经元。神经元是我们大脑中相互连接的细胞。这些神经元接收信号，处理它们并产生结果。人工神经元基于生物神经元，并可以被认为是生物神经元的简化计算模型。
- en: In the year 1943, researchers Warren McCullock and Walter Pitts proposed the
    concept of a simplified brain cells calls McCullock-Pitts (MCP) neuron. It can
    be thought as a simple logic gate with binary outputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 1943年，研究人员Warren McCullock和Walter Pitts提出了一个简化的脑细胞概念，称为McCullock-Pitts（MCP）神经元。它可以被认为是一个具有二进制输出的简单逻辑门。
- en: The working methodology for artificial neurons is similar to biological neurons,
    albeit they are far simpler than biological neurons. The perceptron is a mathematical
    model of a biological neuron. In the actual biological neurons, dendrites receive
    electrical signals from the axons of other neurons. In the perceptron, these electrical
    signals are represented as numerical values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元的工作方法类似于生物神经元，尽管它们比生物神经元简单得多。感知器是生物神经元的数学模型。在实际的生物神经元中，树突从其他神经元的轴突中接收电信号。在感知器中，这些电信号被表示为数字值。
- en: The artificial neuron receives inputs from the previous neurons or can receive
    the input data. It then processes that input information and shares an output.
    The input can be the raw data or processed information from a preceding neuron.
    The neuron then combines the input with their own internal state, weighs them
    separately and passes the output received through a non-linear function to generate
    output. These non-linear functions are also called as activation functions (we
    will cover them later). You can consider an activation function as a mathematical
    funcation. A neuron can be represented as in Figure 8-3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元接收来自前一神经元的输入，或者可以接收输入数据。然后处理输入信息并输出结果。输入可以是原始数据或来自前一神经元的处理信息。神经元将输入与其自身的内部状态结合起来，分别加权并通过非线性函数传递接收到的输出以生成输出。这些非线性函数也被称为激活函数（我们稍后会讨论它们）。您可以将激活函数视为数学函数。可以将神经元表示如图8-3所示。
- en: Figure 8-3 A neuron gets the inputs, processes it using mathematical functions
    and then generates the output
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-3 神经元获得输入，使用数学函数处理输入，然后生成输出
- en: '![](images/08image007.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image007.png)'
- en: In simpler terms, a neuron can be termed as a mathematical function which computes
    the weighted average of its input datasets, then this sum is passed through activation
    functions. The output of the neuron can then be the input to the next neuron which
    will again process the input received. Let’s go a bit deeper.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，神经元可以称为一个计算其输入数据集的加权平均值的数学函数，然后将这个总和通过激活函数进行处理。然后神经元的输出可以成为下一个神经元的输入，该神经元再次处理接收到的输入。让我们再深入一点。
- en: In a perceptron, each input value is multiplied by a factor called the *weight*.
    Biological neuron fires once the total strength of the input signals exceed a
    certain threshold. The similar format is followed in a perceptron too. In a perceptron,
    a weighted sum of the inputs is calculated to get the total strength of the input
    data and then an activation function is applied to each of the outputs. Each output
    can then be fed to the next layerof perceptron.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知器中，每个输入值都乘以一个称为*权重*的因子。生物神经元在总强度的输入信号超过一定阈值时激活。在感知器中也遵循类似的格式。在感知器中，计算输入数据的加权总和，然后对每个输出应用激活函数。然后每个输出可以馈送到感知器的下一层。
- en: 'Let’s us assume that there are two input values “a” and “b” for a perceptron
    X which for the sake of simplicity has only one output. Let the respective weights
    for a and b are P and Q. So, the weighted sum can be said as : P*x + Q*b. The
    perceptron will only fire or will have a non-zero output, only if the weighted
    sum exceeds a certain threshold. Let’s call the threshold as “C”. So, we can say
    that:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设感知器X有两个输入值“a”和“b”，为了简单起见，它只有一个输出。让a和b分别的权重为P和Q。所以，加权总和可以表示为：P*x + Q*b。只有当加权总和超过一定阈值时，感知器才会激活或产生非零输出。让我们称阈值为“C”。因此，我们可以说：
- en: Output of X will be 0 if P*x + Q*y <= C
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果P*x + Q*y <= C，则X的输出将为0
- en: Output of X will be 1 if P*x + Q*y > C
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果P*x + Q*y > C，则X的输出将为1
- en: If we generalize this understanding we can represent as shown below. If we represent
    a perceptron as a function, which maps input “x” as the function below
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们概括这一理解，我们可以表示如下。如果我们将感知器表示为一个函数，该函数将输入"x"映射为如下函数
- en: f(x) = 1 if w*x + b > 0
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = 1 如果 w*x + b > 0
- en: 0 otherwise
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 否则为0
- en: 'where:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: x is vector of input values
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: x是输入值的向量
- en: w represents the vector of weights
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: w表示权重向量
- en: b is the bias term
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: b是偏置项
- en: We will explain the bias and the weight terms now.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此处解释偏差和权重项。
- en: 'Recall the linear equation: y = mx+ c where m is the slope of the straight
    line and c is the constant term. Both bias and weight can be defined using the
    same linear equation.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下线性方程：y = mx+ c其中m是直线的斜率，c是常数项。 偏置和权重都可以使用相同的线性方程定义。
- en: 'Weight: the role of weight is similar to the slope of the line in linear equation.
    It defines what is the change in the value of f(x) by a unit change in the value
    of x.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：权重的作用类似于线性方程中直线的斜率。它定义了f(x)的值在x的值单位变化时的变化。
- en: 'Bias: the role of the bias is similar to the role of a constant in a linear
    function. In case there is no bias, the input to the activation function is x
    multiplied by the weight.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置：偏置的作用类似于线性函数中常数的作用。如果没有偏差，激活函数的输入就是x乘以权重。
- en: Weights and bias terms are the parameters which get trained in a network.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏置项是网络中进行训练的参数。
- en: The output of the function will depend on the activation function which is used.
    We will cover various types of activation functions in the next section after
    we have covered different layers in a network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输出将取决于所使用的激活函数。在我们讨论完网络中不同层之后，我们将在下一节介绍各种类型的激活函数。
- en: 8.3.3 Different layers in a network
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 网络中的不同层
- en: A simple and effective way of organizing neurons is the following. Rather than
    allowing arbitrary neurons connected with arbitrary others, neurons are organized
    in layers. A neuron in a layer has all its inputs coming *only* from the previous
    layer, and all its output going *only* to the next. There are no other connections,
    for example between neurons of the same layer or between neurons in neurons belonging
    in distant layers (with a small exception for a pretty specialized case which
    is beyond the scope of this book).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单而有效的组织神经元的方式如下。与其允许任意神经元连接到任意其他神经元，不如将神经元组织成层次结构。一层中的神经元的所有输入仅来自上一层，并且所有输出仅传递到下一层。没有其他连接，例如同一层中的神经元之间的连接，或者属于远程层的神经元之间的连接（对于一个相当特殊的情况有一个小小的例外，但超出了本书的范围）。
- en: 'We know that information flows through a neural network. That information is
    processed and passed on from one layer to another layer in a network. There are
    three layers in a neural network as shown in Figure 8-4\. A typical neural network
    looks like the figure below:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道信息通过神经网络流动。这些信息在网络中的一层又一层地被处理和传递。如图8-4所示，神经网络有三层。典型的神经网络如下图所示：
- en: Figure 8-4 A typical neural network with neurons and input, hidden and output
    layers
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-4 一个具有神经元和输入、隐藏和输出层的典型神经网络
- en: '![](images/08image008.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image008.png)'
- en: The neural network shown in Figure 8-4 has 3 input units, 2 hidden layers with
    4 neurons each and one final output layer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-4所示的神经网络有3个输入单元，2个每个有4个神经元的隐藏层和一个最终的输出层。
- en: '**Input Layer**: as the name signifies it receives the input data and shares
    it with the hidden layers.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层**：顾名思义，它接收输入数据并与隐藏层共享。'
- en: '**Hidden Layer**: it is the heart and soul of the network. The number of hidden
    layers depends on the problem at hand, the number of layers can range from a few
    to hundredsAll the processing, feature extraction, learning of the attributes
    is done in these layers. In the hidden layers, all the input raw data is broken
    into attributes and features. This learning is useful for decision making at a
    later stage.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏层**：它是网络的核心和灵魂。隐藏层的数量取决于手头的问题，层数可以从几层到数百层不等。在这些层中进行所有的处理、特征提取和属性学习。在隐藏层中，所有的输入原始数据都被分解成属性和特征。这种学习对以后的决策有用。'
- en: '**Output Layer**: the decision layer and final piece in a network. It accepts
    the outputs from the preceding hidden layers and then makes a prediction.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出层**：决策层和网络中的最后一块拼图。它接受来自前面隐藏层的输出，然后做出预测。'
- en: For example, the input training data will have raw images or processed images.
    These images will be fed to the input layer. The data now travels to the hidden
    layers where all the calculations are done. These calculations are done by neurons
    in each layer. The output is the task that needs to be accomplished for example
    identification of an object or if we want to classify an image etc.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，输入的训练数据可能是原始图像或处理过的图像。这些图像将被馈送到输入层。数据现在传输到隐藏层，其中所有的计算都是由每一层的神经元完成的。输出是需要完成的任务，例如识别一个对象或者如果我们想要对图像进行分类等。
- en: The ANN consists of various connections. Each of the connection aims to receive
    the input and provide the output to the next neuron. This output to the next neuron
    will serve as an input to it. Also, as discussed earlier, each connection is assigned
    a weight which is a representative of its respective importance. It is important
    to note that a neuron can have multiple input and output connections which means
    it can receive inputs and deliver multiple outputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ANN 由各种连接组成。每个连接的目标是接收输入并将输出提供给下一个神经元。这个输出将作为下一个神经元的输入。另外，正如之前讨论的，每个连接都被赋予一个权重，这代表了其相应的重要性。值得注意的是，一个神经元可以有多个输入和输出连接，这意味着它可以接收多个输入并提供多个输出。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) **小测验** - 回答这些问题来检查你的理解。答案在书的末尾。'
- en: 1.  The input data is fed to the hidden layers in a neural network. True or
    False.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 1.  输入数据被馈送到神经网络的隐藏层中。是或否？
- en: 2.  Bias term is similar to the slop of a linear equation. True or False.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 偏置项类似于线性方程的斜率。对或错。
- en: 3.  Find and explore the deepest neural network ever trained.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 查找并探索有史以来训练最深的神经网络。
- en: So, what is the role of a layer? A layer receives inputs, processes them and
    passes the output to the next layer. Technically, it is imperative that the transformation
    implemented by a layer is parameterized by its weights which are also referred
    to as parameters of a layer. To simplify, to make it possible that a neural network
    is "trained" to a specific task, something must be changed in the network. It
    turns out that changing the architecture of the network (i.e. how neurons are
    connected with each other) have only a small effect. On the other hand, as we
    will see later in this chapter, changing the weights is key to the "learning"
    process.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，一个层的作用是什么呢？ 一个层接收输入，处理它们，并将输出传递给下一个层。 从技术上讲，层实现的转换必须由它的权重参数化，这也被称为层的参数。 为了简化，为了使神经网络能够“训练”到一个特定的任务，必须更改网络中的某些内容。
    结果证明，改变网络的架构（即神经元如何彼此连接）只有很小的影响。 另一方面，正如我们将在本章后面看到的，改变权重是“学习”过程的关键。
- en: We will now move to the very important topic of activation functions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转移到非常重要的激活函数主题。
- en: 8.3.4 Activation Functions
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 激活函数
- en: Recall in the last sections, we introduced activation functions. The primary
    role of an activation function is to decide whether a neuron/perceptron should
    fire or not. They play a central role in training of the network at a later stage.
    They are sometimes referred as *Transfer Functions*. It is also important to know
    that why we need non-linear activation functions. If we use only linear activation
    functions, the output will also be linear. At the same time, the derivative of
    a linear function will be constant. Hence, there will not be much learning possible.
    Because of such reasons, we prefer to have non-linear activation functions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在最后几节中，我们介绍了激活函数。 激活函数的主要作用是决定神经元/感知器是否应该发射。 它们在后期的网络训练中起着核心作用。 有时它们被称为*传输函数*。
    还重要的是要知道为什么我们需要非线性激活函数。 如果我们仅使用线性激活函数，输出也将是线性的。 同时，线性函数的导数将是常数。 因此，学习的可能性将不大。
    因此，出于这样的原因，我们更喜欢具有非线性激活函数。
- en: We will study the most common activation functions now.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将研究最常见的激活函数。
- en: Sigmoid Function
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: The Sigmoid is a bounded monotonic mathematical function. The Sigmoid is a mathetical
    function which always increase its output value when the input values increases.
    It output value is always between -1 and 1.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是一个有界的单调数学函数。 Sigmoid 是一个数学函数，当输入值增加时，其输出值总是增加的。 其输出值始终在 -1 和 1 之间。
- en: It is a differentiable function with an S-shaped curve and its first derivative
    function is bell-shaped. It has a non-negative derivative function and is defined
    for all real input values. The Sigmoid function is used if the output value of
    a neuron is between 0 and 1.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个可微函数，具有 S 形曲线，其一阶导函数呈钟形。 它具有非负的导函数，并且对所有实数输入值定义。 如果神经元的输出值在 0 和 1 之间，则使用
    Sigmoid 函数。
- en: 'Mathematically, a sigmoid function is:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，sigmoid 函数是：
- en: (Equation 8.1)
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程式 8.1）
- en: S(x) = 1 = e^x
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: S(x) = 1 = e^x
- en: 1 + e^(-x) e^x +1
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 1 + e^(-x) e^x +1
- en: Graph of a sigmoid function can be shown in Figure 8-5.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid 函数的图形可以在图 8-5 中显示。
- en: Figure 8-5 A sigmoid function is shown here. Note the shape of the function
    and the min/max values.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8-5 显示了一个 sigmoid 函数。请注意函数的形状和最小/最大值。
- en: '![page9image55589296](images/08image009.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![page9image55589296](images/08image009.png)'
- en: Sigmoid function finds its applications in complex learning systems. It is usually
    used for binary classification and in the final output layer of the network.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数在复杂学习系统中找到其应用。 通常用于二进制分类和网络的最终输出层。
- en: tanh Function
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Tanh 函数
- en: In mathematics, Tangent Hyperbolic Function or tanh is a differentiable Hyperbolic
    Function. It is a smooth function and its input values are in the range of -1
    to +1.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，双曲正切函数或 tanh 是一个可微的双曲函数。它是一个平滑函数，其输入值在 -1 到 +1 的范围内。
- en: A tanh function is written as Equation 8.2.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh 函数被写成方程式 8.2。
- en: (Equation 8.2)
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程式 8.2）
- en: Tanh(x) = ex – e-x
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh(x) = ex – e-x
- en: ex + e-x
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ex + e-x
- en: Graphical representation of tanh is shown in Figure 8-6\. It is a scaled version
    of Sigmoid function and hence a tanh function can be derived from Sigmoid function
    and vice versa.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲线的图形表示如图 8-6 所示。它是 Sigmoid 函数的缩放版本，因此可以从 Sigmoid 函数导出 tanh 函数，反之亦然。
- en: Figure 8-6 A tanh function is shown here which is scaled version of sigmoid
    function
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-6 这里显示了一个tanh函数，它是sigmoid函数的缩放版本
- en: '![page10image55837712](images/08image010.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![page10image55837712](images/08image010.png)'
- en: A tanh function is generally used in the hidden layers. It makes the mean closer
    to zero which makes the training easier for the next layer in the network. This
    is also referred as *Centering the data*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: tanh函数通常用于隐藏层。它使均值接近于零，从而使网络中的下一层训练更容易。这也被称为*数据居中*。
- en: Rectified Linear Unit or ReLU
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Rectified Linear Unit或ReLU
- en: Rectified Linear Unit or ReLU is an activation function that defines the positives
    of an argument. We are showing the ReLU function below. Note that the value is
    0 even for the negative values and from 0 the value starts to incline.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Rectified Linear Unit或ReLU是一个定义参数的正部分的激活函数。我们在下面展示了ReLU函数。注意，即使对于负值，值也为0，并且从0开始倾斜。
- en: (Equation 8.3)
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程8.3）
- en: F(x) = max (0, x)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: F(x) = max (0, x)
- en: i.e., will give output as x if positive else 0
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 即，如果是正数，则输出为x，否则为0
- en: A ReLU function is shown as Figure 8-7
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-7 显示了ReLU函数
- en: Figure 8-7 A ReLU function is shown here, it is one of the favored activation
    function in the hidden layers of a neural network. ReLU is simple to use and less
    expensive to train.
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-7 这里显示了ReLU函数，它是神经网络隐藏层中受欢迎的激活函数之一。ReLU易于使用且训练成本较低。
- en: '![page11image55974400](images/08image011.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![page11image55974400](images/08image011.png)'
- en: It is a simple function and hence less expensive to compute and much faster.
    It is unbounded and not centred at zero. It is differentiable at all the places
    except zero. Since ReLU function in less complex, is computationally less expensive
    and hence is widely used in the hidden layers to train the networks faster.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个简单的函数，因此计算成本较低，速度更快。它是无界的，并且不以零为中心。除零点外，它在所有位置上都可微。由于ReLU函数较不复杂，计算成本较低，因此在隐藏层中被广泛用于更快地训练网络。
- en: Softmax Function
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Softmax函数
- en: The softmax function is used in the final layer of the neural network to generate
    the output from the network. It is the activation functionThe output can be a
    final classification of an image for distinct categories. It is an activation
    function that is useful for multi-class classification problems and forces the
    neural network to output the sum of 1.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数用于神经网络的最后一层，以从网络中生成输出。它是激活函数，输出可以是图像的最终分类，用于不同的类别。它是一个对于多类分类问题有用的激活函数，并强制神经网络输出总和为1。
- en: As an example, if the number of distinct class for an image are cars, bikes
    or trucks, the softmax function will generate three probabilities for each category.
    The category which has received the highest probability will be the predicted
    category.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果图像的不同类别是汽车、摩托车或卡车，则Softmax函数将为每个类别生成三个概率。获得最高概率的类别将是预测的类别。
- en: There are other activation functions too like ELU, PeLU etc. which are beyond
    the scope of this book. We are providing the summary of various activation functions
    at the end of this chapter.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他激活函数，如ELU、PeLU等，超出了本书的范围。我们在本章末尾提供了各种激活函数的摘要。
- en: We will now cover hyperparameters in the next section. They are the control
    levers we have while the network is trained.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中介绍超参数。它们是我们在训练网络时拥有的控制杆。
- en: 8.3.5 Hyperparameters
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.5 超参数
- en: During training a network, the algorithm is constantly learning the attributes
    of the raw input data. At the same time, the network cannot learn everything itself,
    there are a few parameters which require initial settings to be provided. These
    are the variables that determine the structure of the neural network and the respective
    variables which are useful to train the network.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络时，算法不断学习原始输入数据的属性。同时，网络无法自行学习所有内容，有些参数需要提供初始设置。这些变量决定了神经网络的结构以及训练网络时有用的相应变量。
- en: We provide the hyperparameters before we start the network training. A few examples
    of hyperparameters are number of hidden layers in the network, number of neurons
    in each layer, activation functions used in layer, weight initialization etc.
    We have to pick the best values of the hyperparameters. To do so, weselect some
    reasonable values for the hyperparameters, train the network, then measure the
    performance of the network and then tweak the Hyperparameters and then re-train
    the network, re-evaluate and re- tweak and this process continues.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开始网络训练之前提供超参数。超参数的一些例子包括网络中隐藏层数的数量、每个层中神经元的数量、层中使用的激活函数、权重初始化等。我们必须选择超参数的最佳值。为此，我们选择一些合理的超参数值，训练网络，然后测量网络的性能，然后调整超参数，然后重新训练网络，重新评估和重新调整，这个过程继续进行。
- en: Hyperparameters are controlled by us as we input hyperparameters to improve
    the performance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是由我们控制的，因为我们输入超参数来提高性能。
- en: Gradient Descent and Stochastic Gradient Descent
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度下降和随机梯度下降
- en: In any prediction-based solution, we would want to predict as best as we can
    or in other words, we wish to reduce the error as much as we can. Error is the
    difference between the actual values and the predicted values. The purpose of
    a Machine Learning solution is to find the most optimum value for our functions.
    We want to decrease the error or maximize the accuracy. Gradient Descent can help
    to achieve this purpose.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何基于预测的解决方案中，我们希望尽可能好地预测，换句话说，我们希望尽可能减少错误。误差是实际值和预测值之间的差异。机器学习解决方案的目的是找到我们函数的最优值。我们想减少错误或最大化准确性。梯度下降可以帮助实现这一目的。
- en: Gradient Descent technique is an optimization technique used to find the global
    minimaof a function. We proceed in the direction of the steepest descent iteratively
    which is defined by the negative of the gradient.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降技术是一种优化技术，用于找到函数的全局最小值。我们迭代地沿着最陡的下降方向前进，这个方向是由负梯度定义的。
- en: But gradient descent can be slow to run on very large datasets or the datasets
    with a very high number of dimensions. It is due to the fact that one iteration
    of the gradient descent algorithm predicts for every instance in the training
    dataset. Hence, it is obvious that it will take a lot of time if we have thousands
    of records. For such a situation, we have Stochastic Gradient Descent.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但梯度下降在运行非常大的数据集或具有非常高维数的数据集时可能运行速度较慢。这是由于梯度下降算法的一次迭代会针对训练数据集中的每个实例进行预测。因此，很显然，如果我们有成千上万条记录，它将花费很长时间。对于这种情况，我们有随机梯度下降。
- en: In Stochastic Gradient Descent, rather than at the end of the batch of the data,
    the coefficients are updated for each training instance and hence it takes less
    time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机梯度下降中，系数不是在数据批次结束时更新，而是对每个训练实例进行更新，因此需要较少的时间。
- en: The image below shows the way a Gradient Descent works. Notice how we can progress
    downwards towards the Global Minimum.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了梯度下降的工作方式。注意我们如何朝向全局最小值进展。
- en: Figure 8-8 The concept of gradient descent. It is the mechanism to minimize
    the loss function
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-8 梯度下降的概念。这是最小化损失函数的机制
- en: '![page14image55583056](images/08image012.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![page14image55583056](images/08image012.png)'
- en: Learning and Learning Rate
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习和学习率
- en: For a network, we take a various steps to improve the performance of the solution,
    learning rate is one of them. The learning rate will define the size of the corrective
    steps which a model takes to reduce the errors. Learning rate defines the amount
    by which we should adjust the values of weights of the network with respect the
    loss gradients (more on this process will come in the next section). If we have
    a higher learning rate, the accuracy will be lower. If we have a very low learning
    rate, the training time will increase a lot.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络，我们采取各种步骤来提高解决方案的性能，学习率就是其中之一。学习率将定义模型减少错误所采取的纠正步骤的大小。学习率定义了我们应该如何调整网络权重的值，以使其相对于损失梯度（这个过程的更多信息将在下一节提供）调整的数量。如果我们采用较高的学习率，准确性会降低。如果我们采用非常低的学习率，训练时间将大大增加。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 小测验-回答这些问题以检验您的理解。答案在本书结尾处。'
- en: 1.  Compare and contrast between sigmoid and tanh function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 1.  比较并对比Sigmoid和tanh函数。
- en: 2.  ReLU is generally used in output layer of the network. True or False.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 2.  ReLU通常用于网络的输出层。是或否？
- en: 3.  Gradient descent is an optimization technique. True or False.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 梯度下降是一种优化技术。真或假。
- en: We have now examined the main concepts of Deep Learning. Now let us study how
    a Neural Network works. We will understand how the various layers interact with
    each other and how information is passed from one layer to another.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了深度学习的主要概念。现在让我们研究一下神经网络是如何工作的。我们将了解各个层是如何相互作用的，以及信息是如何从一层传递到另一层的。
- en: 8.4 How Does Deep Learning Work in a supervised manner?
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 深度学习如何在监督方式下工作？
- en: We have now covered the major components of a neural network. It is the time
    for all the pieces to come together and orchestrate the entire learning. The training
    of a neural network is quite a complex process. The entire process can be examined
    as below in a step-by-step fashion.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经介绍了神经网络的主要组成部分。现在是所有部分汇聚在一起，组织整个学习的时候了。神经网络的训练是一个非常复杂的过程。整个过程可以按照以下步骤一步步地进行检查。
- en: You might be wondering about what is meant by learning of a neural network.
    Learning is a process to find the best and most optimized values for weights and
    bias for all the layers of the network so that we can achieve the best accuracy.
    As deep neural networks can have practically infinite possibilities for weights
    and bias terms, we have to find the most optimum value for all the parameters.
    This seems like a herculean task considering that changing one value impacts the
    other values and indeed it is process where the various parameters of the networks
    are changing.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想知道神经网络的学习是什么意思。学习是一个过程，为了所有网络层找到最佳和最优化的权重和偏置值，以便我们可以达到最佳准确度。由于深度神经网络在权重和偏置项方面实际上具有无限的可能性，我们必须找到所有参数的最优值。考虑到改变一个值会影响其他值，这似乎是一项艰巨的任务，而实际上它是一个各种网络参数在变化的过程。
- en: Recall in the first chapter we covered the basics of supervised learning. We
    will refresh that understanding here. The reason to refresh supervised learning
    is to ensure that you are fully able to appreciate the process of training the
    neural network.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在第一章中我们介绍了监督学习的基础知识。我们将在这里刷新对此的理解。刷新监督学习的原因是确保您能够充分理解训练神经网络的过程。
- en: 8.4.1 Supervised learning algorithms
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 监督学习算法
- en: A quick definition is - supervised learning algorithms have a “guidance” or
    “supervision” to direct toward the business goal of making predictions for the
    future.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速的定义是 - 监督学习算法具有“指导”或“监督”，以指导向着实现未来预测的业务目标。
- en: Formally put, supervised models are statistical models which use both the input
    data and the desired output to predict for the future. The output is the value
    which we wish to predict and is referred as the *target variable* and the data
    used to make that prediction is called as *training data*. Target variable is
    sometimes referred as the *label*. The various attributes or variables present
    in the data are called as *independent variables*. Each of the historical data
    point or a *training example* contains these independent variables and corresponding
    target variable. Supervised learning algorithms make a prediction for the unseen
    future data. The accuracy of the solution depends on the training done and patterns
    learned from the labelled historical data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，监督模型是使用输入数据和期望输出来预测未来的统计模型。输出是我们希望预测的值，被称为*目标变量*，用于进行预测的数据称为*训练数据*。目标变量有时被称为*标签*。数据中存在的各种属性或变量称为*独立变量*。每个历史数据点或*训练示例*都包含这些独立变量和相应的目标变量。监督学习算法对未来看不见的数据进行预测。解决方案的准确性取决于所进行的训练以及从标记历史数据中学到的模式。
- en: Most of the deep learning solutions are based on supervised learning. Unsupervised
    deep learning is rapidly gaining traction, however, as unlabelled datasets are
    far more abundant than labelled ones.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习解决方案都基于监督学习。然而，无监督深度学习正在迅速获得认可，因为未标记的数据集比标记的数据集更加丰富。
- en: 'For example, if we wish to create a solution which can identify faces of people.
    In this case, we will have:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们希望创建一个可以识别人脸的解决方案。在这种情况下，我们将有：
- en: 'Training data: different images of faces of the people from various angles.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据：来自不同角度的人脸图像。
- en: 'Target variable: name of person.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量：人的姓名。
- en: This training dataset can be fed to the algorithm. The algorithm will then understand
    the attributes of various faces, or in other words, **learn** the various attributes.
    Based on the training done, the algorithm can then make a prediction on the faces.
    The prediction will be a probability score if the face belongs to Mr. X. If the
    probability is high enough, we can safely say that the face belongs to Mr.X.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练数据集可以输入到算法中。然后算法将理解各种面部的属性，或者换句话说，*学习*各种属性。基于所进行的训练，算法可以对面部进行预测。如果概率得分表明面部属于某人X，那么我们可以安全地说该面部属于某人X。
- en: Supervised learning problems are used in demand prediction, credit card fraud
    detection, customer churn prediction, premium estimation etc. They are heavily
    used across retail, telecom, banking and finance, aviation, insurance etc.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题用于需求预测、信用卡欺诈检测、客户流失预测、保费估算等。它们在零售、电信、银行和金融、航空、保险等行业被广泛应用。
- en: We have now refreshed the concepts of supervised learning for you. We will now
    move to the first step in the training of the neural network which is feed-forward
    propagation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为您刷新了监督学习的概念。我们将现在转移到神经网络训练的第一步，即前馈传播。
- en: '8.4.2 Step 1: feed-forward propagation'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 第1步：前馈传播
- en: Let us start the the process done in a Neural Network. We have tried to create
    an illustrative diagram in Figure 8-9\. This is the basic skeleton of a network
    we have created to explain the process. Let’s consider we have some input data
    points and we will have the input data layer, which will consume the input data.
    The information is flowing from the input layer to the data transformation layers
    (hidden layers). In the hidden layers, the data is processed using the activation
    functions and based on the weights and bias terms. And then a prediction is made
    on the data set. It is called *feed-forward propagation* as during this process
    the input variables are calculated in a sequence from the input layer to the output
    layer.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始神经网络中的过程。我们尝试在图8-9中创建一个说明性的图表。这是我们为了解释该过程而创建的网络的基本骨架。我们假设有一些输入数据点，我们将有输入数据层，该层将消耗输入数据。信息从输入层流向数据转换层（隐藏层）。在隐藏层中，使用激活函数处理数据，并基于权重和偏置项进行处理。然后对数据集进行预测。这称为*前馈传播*，因为在此过程中，输入变量按顺序从输入层到输出层计算。
- en: Let’s take the same problem we used to explain the process in a supervised algorithm
    in section 8.3\. For example, if we wish to create a solution which can identify
    faces of people. In this case, we will have the
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拿同样的问题来解释8.3节中监督算法中的过程。例如，如果我们希望创建一个能够识别人脸的解决方案。在这种情况下，我们将有
- en: 'Training data: different images of faces of the people from various angles.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据：来自各个角度的不同人脸图像。
- en: 'Target variable: name of person.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量：人的姓名。
- en: This training dataset can be fed to the algorithm. The network will then understand
    the attributes of various faces, or in other words, **learn** the various attributes
    of a facial data. Based on the training done, the algorithm can then make a prediction
    on the faces. The prediction will be a probability score if the face belongs to
    Mr. X. If the probability is high enough, we can safely say that the face belongs
    to Mr.X.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练数据集可以输入到算法中。然后网络将理解各种面孔的属性，或者换句话说，*学习*面部数据的各种属性。基于所进行的训练，算法可以对面部进行预测。如果概率得分表明面部属于某人X，那么我们可以安全地说该面部属于某人X。
- en: Figure 8-9 The basic skeleton of a neural network training process, we have
    the input layers and data transformation layers.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-9 神经网络训练过程的基本骨架，我们有输入层和数据转换层。
- en: '![](images/08image014.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image014.png)'
- en: Once the data processing is done in the hidden layers, a prediction is generated,
    which is the probability if the face belongs to Mr. X.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在隐藏层中完成数据处理，就会生成一个预测，即面部属于某人X的概率。
- en: '8.4.3 Step 2: adding the loss function'
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 第2步：添加损失函数
- en: The output is generated in step 1\. Now we have to gauge the accuracy of this
    network. We want out network to have the best possible accuracy in identifying
    the faces. And using the prediction made by the algorithm, we will control and
    improve the accuracy of the network.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步生成输出。现在我们必须评估该网络的准确性。我们希望我们的网络在识别面部时具有尽可能高的准确性。并使用算法生成的预测，我们将控制并提高网络的准确性。
- en: Accuracy measurement in the network can be achieved by the loss function, also
    called the *objective function*. The loss function compares the actual values
    and the predicted values. The loss function computes the difference score, and
    hence is able to measure how well the network has done and what are the error
    rates.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的准确度测量可以通过损失函数来实现，也被称为*目标函数*。损失函数比较实际值和预测值。损失函数计算差异分数，因此能够测量网络的表现如何以及错误率是多少。
- en: Let’s update the diagram we created in Step 1 by adding a Loss Function and
    corresponding Loss Score, used to measure the accuracy for the network as shown
    in Figure 8-10.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们在第1步创建的图表，通过添加一个损失函数和相应的损失分数，来测量网络的准确性，如图8-10所示。
- en: Figure 8-10 A loss function has been added to measure the accuracy.
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-10 添加了一个损失函数来测量准确性。
- en: '![](images/08image015.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image015.png)'
- en: '8.4.4 Step 3: calculating the error'
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.4 第3步：计算错误
- en: We generated the predictions in Step 1 of the network. In step 2, we compared
    the output with the actual values to get the error in prediction. The objective
    of our solution is to minimize this error which means the same as maximizing the
    accuracy.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在网络的第1步生成了预测。在第2步，我们将输出与实际值进行比较，以得到预测误差。我们解决方案的目标是最小化这个错误，即最大化准确性。
- en: In order to constantly lower the error, the loss score (Predictions – Actual)
    is then used as a feedback to adjust the value of the weights. This task is done
    by the Backpropagation algorithm.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不断降低误差，损失分数（预测-实际）被用作反馈来调整权重的值。这个任务是由反向传播算法完成的。
- en: 8.5 Backpropagation
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 反向传播
- en: In the step 3 of the last section, we said we use an optimizer to constantly
    update the weights to reduce the error. While the learning rate defines the size
    of the corrective steps to reduce the error, backpropagation is used to adjust
    the connection weights. These weights are updated backward basis the error. Following
    it, the errors are recalculated, gradient descent is calculated and the respective
    weights are adjusted. And hence, back-propagation is sometimes called the central
    algorithm in deep learning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在上个部分的第3步中，我们说我们使用优化器不断更新权重以减小错误。学习速率定义了减小错误的修正步长的大小，反向传播用于调整连接权重。这些权重是根据错误向后更新的。随后，重新计算错误，计算梯度下降，并调整相应的权重。因此，反向传播有时被称为深度学习中的中心算法。
- en: The back-propagation was originally suggested in 1970s. Then in 1986, David
    Rumelhartm Geoffrey Hinton and Ronald Williams’s paper got a lot of appreciation.
    In the modern days, back-propagation is the backbone of deep learning solutions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播最初是在20世纪70年代提出的。然后在1986年，大卫·鲁梅尔哈特，杰弗里·辛顿和罗纳德·威廉斯的论文得到了很多赞赏。在现代，反向传播是深度学习解决方案的支柱。
- en: The image below shows the process for Backpropagation where the information
    flows from the output layer back to the hidden layers. Note that the flow of information
    is backwards as compared to forward propagation where the information flows from
    left to right. The process for a backpropagation is shown in (Figure 8-11).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了反向传播的过程，信息从输出层向隐藏层反向流动。注意，信息的流动是与正向传播相反的，正向传播中信息从左到右流动。反向传播的过程如图8-11所示。
- en: Figure 8-11 Backpropagation as a process- the information flows from the final
    layers to the initial layers
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-11 反向传播作为一个过程- 信息从最终层向初始层流动
- en: '![page12image55891232](images/08image016.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![page12image55891232](images/08image016.png)'
- en: First, we will describe the process at very high level. Remember that in step
    1, at the start of the training process, some random values are assigned to the
    weights. Using these random values, an initial output is generated. Since it is
    the very first attempt, the output received can be quite different from the real
    values and the loss score is accordingly very high. But this is going to improve.
    While training the Neural Network, the weights (and biases) are adjusted a little
    in the correct direction, and subsequently, the loss score decreases. We iterate
    this training loop many times and it results in most optimum weight values that
    minimize the loss function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将以非常高层次描述过程。记住，在训练过程的开始，在步骤1中，权重被分配了一些随机值。使用这些随机值，生成了初始输出。由于这是第一次尝试，收到的输出可能与实际值相差很大，相应的损失分数也很高。但这将要改善。在训练神经网络时，权重（和偏置）会在正确的方向上稍微调整，随后，损失分数减少。我们迭代这个训练循环很多次，最终得到最优的最小化损失函数的权重值。
- en: Back-propagation allows us to iteratively reduce the error during the network
    training process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播允许我们在网络训练过程中迭代地减少误差。
- en: The following section is going to be mathematically heavy. If you are not keen
    to understand the mathematics behind the process, you can skip it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的部分将涉及大量数学内容。如果你对过程背后的数学不感兴趣，可以跳过它。
- en: 8.5.1 Mathematics behind back-propagation
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 反向传播背后的数学
- en: To we train a neural network, we calculate a loss function. The loss function
    tells us how different are the predictions from the actual values. Backpropagation
    calculates the gradient of the loss function with respect to the each of the weights.
    And with this information, each weight can be updated individually over iterations
    which reduces the loss gradually.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练神经网络，我们计算一个损失函数。损失函数告诉我们预测值与实际值之间的差异有多大。反向传播计算损失函数相对于每个权重的梯度。有了这些信息，每个权重都可以在迭代中逐渐更新，从而逐渐减少损失。
- en: In back-prop (back-propagation is also called as back-prop), the gradient is
    calculated backwards i.e., from the last layer of the network through the hidden
    layers to the very first layer. The gradients of all the layers are combined using
    calculus chain rule to get the gradient of any particular layer.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，梯度是从网络的最后一层通过隐藏层到第一层计算的。所有层的梯度使用微积分链式法则组合在一起，以获得任何特定层的梯度。
- en: 'We will now go into more details of the process. Let’s denote a few mathematical
    symbols first:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将更详细地介绍这个过程。首先让我们表示一些数学符号：
- en: h^(i)) – output of the hidden layer i
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: h^(i)) – 隐藏层i的输出
- en: g^(i))- activation function of hidden layer i
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: g^(i))- 隐藏层i的激活函数
- en: w^(i))- hidden weights matrix in the layer i
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: w^(i))- 层i中的隐藏权重矩阵
- en: b^(i))- the bias in layer i
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b^(i))- 层i中的偏置
- en: x- the input vector
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: x- 输入向量
- en: N – the total number of layers in the network
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N – 网络中的总层数
- en: W^(i))[jk]- weight of the network from node j in layer (i-1) too node k in layer
    i
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: W^(i))[jk]- 网络中从第(i-1)层节点j到第i层节点k的权重
- en: '∂A/∂B: it is partial derivative of A with respect to B'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ∂A/∂B：它是A相对于B的偏导数
- en: 'During the training of the network, the input x is fed to the network and it
    passes through the layers to generate an output ŷ. The expected output is y. Hence,
    the cost function or the loss function to compare y and ŷ is going to be C(y,
    ŷ). Also, the output for any hidden layer of the network can be represented as:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络训练过程中，输入x被馈送到网络，并通过各层生成输出ŷ。期望的输出是y。因此，比较y和ŷ的成本函数或损失函数将是C(y, ŷ)。此外，网络的任何隐藏层的输出都可以表示为：
- en: (Equation 8.4)
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程式8.4）
- en: h^((i)) = g^((i)) (W^((i)T) x + b^((i)))
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: h^((i)) = g^((i)) (W^((i)T) x + b^((i)))
- en: where i (index) can be any layer in the network
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，i（索引）可以是网络中的任意层
- en: 'The final layer’s output is:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层的输出是：
- en: (Equation 8.5)
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程8.5）
- en: y(x) = W^((N)T) h^((N-1)) + b^((N))
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: y(x) = W^((N)T) h^((N-1)) + b^((N))
- en: During the training of the network, we adjust the network’s weights so that
    C is reduced. And hence, we calculate the derivative of C with respect to every
    weight in the network. The following is the derivative of C with respect to every
    weight in the network
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络训练过程中，我们调整网络的权重以减少C。因此，我们计算C相对于网络中每个权重的导数。以下是网络中每个权重的C导数
- en: '![](images/08image017.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image017.png)'
- en: Now we know that a neural network has many layers. The back-propagation algorithm
    starts at calculating the derivatives at the last layer of the network, which
    is the N^(th) layer. And then these derivatives are few backwards. So, the derivatives
    at the Nth layers will be fed to (N-1) layer of the network and so on.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道神经网络有很多层。反向传播算法从网络的最后一层开始计算导数，即第N^(th)层。然后这些导数向后传播。因此，N层的导数将被馈送到网络的(N-1)层，依此类推。
- en: Each component of the derivatives of C are calculated individually using the
    calculus chain rule.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微积分链式法则分别计算C的导数的每个分量。
- en: As per the chain rule, for a function c depending of b, where b depends on a,
    the derivative of c with respect to a can be written as
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 根据链式法则，对于一个依赖于b的函数c，其中b依赖于a，c相对于a的导数可以写成
- en: '![](images/08image018.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image018.png)'
- en: Hence, in the back-propagation the derivatives of the layer N are used in the
    layer (N-1) so that they are saved and again used in (N-2) layer. We start with
    last layer of the network, through all the layers to the first layer, and each
    of the times, we use the derivatives of the last calculations made to get the
    derivatives of the current layers. And hence, back-prop turns out to be an extremely
    efficient as compared to a normal approach where we would have calculated each
    weight in the network individually.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在反向传播中，第 N 层的导数被用于第 (N-1) 层，以便保存它们然后在第 (N-2) 层再次使用。我们从网络的最后一层开始，遍历所有层直到第一层，并且每次，我们使用最后一次计算的导数来得到当前层的导数。因此，与普通方法相比，反向传播变得极其高效，因为我们不需要分别计算网络中的每个权重。
- en: Once we have calculated the gradients, we would update all the weights in the
    network. The objective being to minimize the cost function. We have already studied
    methods like Gradient Descent in the last section.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出梯度，我们将更新网络中的所有权重。目标是最小化成本函数。我们已经在上一节中学习了类似梯度下降的方法。
- en: We will now continue to the next step in the neural network training process.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将继续进行神经网络训练过程的下一步。
- en: 'Step 4: OPTIMIZATION'
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 4：优化
- en: We studied the back propagation in the last section. It allows us to optimize
    our network and achieve the best accuracy. And hence, we have updated the figure
    too in Figure 8-12\. Notice the optimizer which provides regular and continuous
    feedback to reach the best solution.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中我们学习了反向传播。它让我们能够优化我们的网络并实现最佳准确度。因此，我们也更新了图中的图 8-12。请注意优化器提供了定期和连续的反馈以达到最佳解决方案。
- en: Figure 8-12 Optimization is the process to minimize the loss function
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8-12 优化是最小化损失函数的过程
- en: '![](images/08image019.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](images/08image019.png)'
- en: Once we have achieved the best values of the weights and biases for our network,
    we call that our network istrained. We can now use to make predictions on unseen
    dataset which has not been used for training the network.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为我们的网络获得了权重和偏差的最佳值，我们称之为我们的网络已经训练完成。我们现在可以用它对未经训练的数据集进行预测。
- en: Now you have understood what the various components of Deep Learning are and
    how they work together in supervised manner. We will now briefly describe the
    unsupervised deep learning.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了深度学习的各个组件以及它们在监督方式下如何协同工作。我们现在将简要描述无监督深度学习。
- en: 8.6 How deep learning works in unsupervised manner
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 无监督深度学习的工作原理
- en: We know that unsupervised learning solutions work on unlabeled datasets. For
    deep learning in unsupervised settings, the training dataset is unlabeled
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道无监督学习解决方案用于未标记的数据集。对于无监督设置中的深度学习，训练数据集是未标记的。
- en: As compared to supervised datasets where we have tags, unsupervised methods
    have to self-organize themselves to get densities, probabilities distributions,
    preferences and groupings. We can solve a similar problem using supervised and
    unsupervised methods. For example, a supervised deep learning method can be used
    to identify dogs vs cats while an unsupervised deep learning method might be used
    to cluster the pictures of dogs and cats into different groups. It is also observed
    in machine learning that a lot of solutions which were initially conceived as
    supervised learning ones, over the period of time employed unsupervised learning
    methods to enrich the data and hence improve the supervised learning solution.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与有标签的监督数据集相比，无监督方法必须自我组织以获取密度、概率分布、偏好和分组。我们可以使用监督和无监督方法来解决类似问题。例如，监督深度学习方法可用于识别狗和猫，而无监督深度学习方法可能用于将狗和猫的图片聚类到不同的组中。在机器学习中也观察到，许多最初构想为监督学习的解决方案，随着时间的推移采用了无监督学习方法来丰富数据，从而改进了监督学习解决方案。
- en: 'During the learning phase in unsupervised deep learning, it is expected that
    the network will mimic the data and will then improve itself based on the errors.
    In supervised learning algorithm, there are other methods which play the same
    part as back-prop algorithm. They are:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督深度学习的学习阶段，预期网络将模拟数据，然后根据错误进行自我改进。在监督学习算法中，还有其他方法起到与反向传播算法相同的作用。它们是：
- en: Boltzmann learning rule
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 波兹曼学习规则
- en: Contrastive divergence
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对比散度
- en: Maximum likelihood
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大似然
- en: Hopfield learning rule
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 霍普菲尔德学习规则
- en: Gibbs sampling
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gibbs 采样
- en: Deep belief networks and so on
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度信念网络等等
- en: In this book, we are going to cover autoencoders and deep belief networks. We
    are also going to explore GAN (Generative Adversarial Networks). It is time for
    you to now examine all the tools and libraries for Deep Learning.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将讨论自动编码器和深度信念网络。我们还将探索 GAN（生成对抗网络）。现在是时候让你检查所有用于深度学习的工具和库了。
- en: '![](images/tgt.png) POP QUIZ – answer these question to check your understanding..
    Answers at the end of the book'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) - 答题测验 - 回答这些问题以检查你的理解。答案在书的末尾。'
- en: 1.  Write in a simple form, the major steps in a back-prop technique.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 以简单形式写出反向传播技术的主要步骤。
- en: 2.  Back-prop is preferred in unsupervised learning. True or False.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 反向传播在无监督学习中更受欢迎。正确还是错误？
- en: 3.  The objective of deep learning is to maximise the loss function. True or
    False.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 深度学习的目标是最大化损失函数。正确还是错误？
- en: 8.7 Popular Deep learning libraries
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 流行的深度学习库
- en: Over the last few chapters, we have used a lot of libraries and packages for
    implementing solutions. There are quite a few libraries which are available in
    the industry for deep learning. These packages expedite the solution building
    and reduce the efforts as most of the heavy-lifting is done by these libraries.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们使用了许多库和包来实现解决方案。在行业中有很多用于深度学习的库。这些包加快了解决方案的构建，并减少了工作量，因为大部分繁重的工作都是由这些库完成的。
- en: 'We are discussing the most popular deep learning libraries here:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论最流行的深度学习库：
- en: '**TensorFlow**: TensorFlow (TF) developed by Google is arguably one of the
    most popular and widely used Deep Learning frameworks. It was launched in 2015
    and since is being used by a number of businesses and brands across the globe.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow**：Google 开发的 TensorFlow（TF）可以说是最流行和广泛使用的深度学习框架之一。它于 2015 年推出，自那以后被全球许多企业和品牌使用。'
- en: Python is mostly used for TF but C++, Java, C#, Javascript, Julia can also be
    used. You have to install TF library on your system and import the library.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Python 主要用于 TF，但也可以使用 C++、Java、C#、Javascript、Julia。你需要在系统上安装 TF 库并导入该库。
- en: Go to [www.tensorflow.org/install](www.tensorflow.org.html) and follow the instructions
    to install TensorFlow.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 [www.tensorflow.org/install](www.tensorflow.org.html) 并按照说明安装 TensorFlow。
- en: TensorFlow is one of the most popular libraries and can work on mobile devices
    like iOS and Android too.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是最流行的库之一，也可以在 iOS 和 Android 等移动设备上运行。
- en: '**Keras**: Keras is a mature API driven solution and quite easy to use. It
    is one of the best choices for starters and amongst the best for prototyping simple
    concepts in an easy and fast manner. Keras was initially released in 2015 and
    is one of the most recommended libraries.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**Keras**：Keras 是一个成熟的 API 驱动解决方案，非常易于使用。它是初学者的最佳选择之一，也是在易于理解和快速原型设计简单概念方面最好的选择之一。Keras
    最初于 2015 年发布，是最受推荐的库之一。'
- en: Go to [https://keras.io](.html) and follow the instructions to install Keras.
    Tf.keras can be used as an API.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 [https://keras.io](.html) 并按照说明安装 Keras。可以使用 Tf.keras 作为 API。
- en: Serialization/Deserialization APIs, call backs, and data streaming using Python
    generators are very mature. Massive models in Keras are reduced to single-line
    functions which makes it a less configurable environment and hence very convenient
    and easy to use.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 生成器进行序列化/反序列化 API、回调和数据流传输非常成熟。在 Keras 中，庞大的模型被简化为单行函数，这使得它成为一个 less
    configurable 环境，因此非常方便和易于使用。
- en: '**PyTorch**: Facebook’s brain-child PyTorch was released in 2016 and is another
    popular framework. PyTorch operates with dynamically updated graphs and allows
    data parallelism and distributed learning model. There are debuggers like pdb
    or PyCharm available in PyTorch. For small projects and prototyping, PyTorch can
    be a good choice.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch**：Facebook 的心头宝贝 PyTorch 于 2016 年发布，是另一个流行的框架。PyTorch 使用动态更新的图形，并允许数据并行和分布式学习模型。在
    PyTorch 中有像 pdb 或 PyCharm 这样的调试器可用。对于小型项目和原型设计，PyTorch 可能是一个不错的选择。'
- en: '**Sonnet**: DeepMind’s Sonnet is developed using and on top of TF. Sonnet is
    designed for complex Neural Network applications and architectures. It works by
    creating primary Python objects corresponding to a particular part of the neural
    network. Then these Python objects are independently connected to the computational
    TF graph. Because of this separation (creating Python objects and associating
    them to a graph), the design is simplified.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sonnet**：DeepMind的Sonnet是使用和基于TF开发的。Sonnet专为复杂的神经网络应用和架构设计而设计。它通过创建与神经网络特定部分相对应的主要Python对象来工作。然后这些Python对象独立于计算TF图形地连接。由于这种分离（创建Python对象并将其与图形关联），设计得到了简化。'
- en: Having high-level object-oriented libraries is very helpful as the abstraction
    is allowed when we develop machine learning solutions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有高级面向对象类库非常有帮助，因为在我们开发机器学习解决方案时，它允许我们抽象化。
- en: '**MXNet**: Apache''s MXNet is a highly scalable deep learning tool that is
    easy to use and has detailed documentation. A large number of languages like C
    ++, Python, R, Julia, JavaScript, Scala, Go, Perl are supported by MXNet.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**MXNet**：Apache的MXNet是一个高度可扩展的深度学习工具，易于使用并具有详细的文档。MXNet支持许多语言，如C ++、Python、R、Julia、JavaScript、Scala、Go和Perl。'
- en: There are other frameworks too like Swift, Gluon, Chainer, DL4J, etc, however,
    we've only discussed the popular ones here.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他框架，如Swift、Gluon、Chainer、DL4J等，但我们只讨论了流行的框架。
- en: We will now examine a short code in tensorflow and keras. It is just to test
    that you have installed these libraries correctly. You can learn more about tensorflow
    at https://www.tensorflow.org and keras at https://keras.io.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将检查tensorflow和keras中的一个简短的代码。这只是为了测试您是否已正确安装这些库。您可以在https://www.tensorflow.org和https://keras.io上了解更多关于tensorflow和keras的信息。
- en: 8.7.1 Python code for keras and tensorflow
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1 keras和tensorflow的Python代码
- en: We are implementing a very simple code in tensorflow. We simply import the tensor
    flow library and print “hello”. We also check the version of tensorflow.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在tensorflow中实现了一个非常简单的代码。我们只是导入了tensor flow库并打印“hello”。我们还会检查tensorflow的版本。
- en: '[PRE0]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If this code runs for you and prints the version of tensorflow for you it means
    that you have installed `tensorflow` correctly.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此代码可以在您的计算机上运行并为您打印出tensorflow的版本，则意味着您已经正确安装了`tensorflow`。
- en: '[PRE1]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If this code runs for you and prints the version of keras, it means that you
    have installed `keras` correctly.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该代码对您运行并打印出keras的版本，那么这意味着您已正确安装了`keras`。
- en: 8.8 Closing words
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 结束语
- en: Deep learning is changing the world we live in. It is enabling us to train and
    create really complex solutions which were a mere thought earlier. The impact
    of deep learning can be witnessed across multiple domains and industries. Perhaps
    there are no industries which have been left unimpacted by the marvels of deep
    learning.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习正在改变我们生活的世界。它使我们能够训练和创建真正复杂的解决方案，这些只是以前的想法。深度学习的影响可以在多个领域和行业中看到。也许没有一个行业不受深度学习奇迹的影响。
- en: Deep learning is one of the most-sought after field for research and development.
    Every year there are many journals and papers which are published on deep learning.
    Researchers across the prominent institutions and universities (like Oxford, Stanford
    etc.) of the world are engrossed in finding improved neural network architectures.
    At the same time, professionals and engineers in the reputed organizations (like
    Google, Facebook etc.) are working hard to create sophisticated architectures
    to improve our performance.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是研究和开发中最受追捧的领域之一。每年都有许多关于深度学习的期刊和论文发表。世界上重要机构和大学（如牛津、斯坦福等）的研究人员正全神贯注于寻找改进的神经网络架构。与此同时，世界知名机构（如谷歌、Facebook等）的专业人员和工程师正在努力创建复杂的架构来提高我们的性能。
- en: Deep learning is making our systems and machines able to solve problems typically
    assumed to be realm of humans . Humans have improved clinical trials process for
    the pharma sector, we have improved fraud detection softwares, automatic speech
    detection systems, various image recognition solution, more robust natural language
    processing solutions, targeted marketing solutions improving customer relationship
    managements and recommendation systems, better safety processes and so on. The
    list is quite long and growing day-by-day.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习使得我们的系统和机器能够解决通常被认为是人类领域的问题。人们改进了药物行业的临床试验流程，改进了欺诈检测软件、自动语音检测系统、各种图像识别解决方案、更强大的自然语言处理解决方案、改进客户关系管理和推荐系统的定向营销解决方案、更好的安全流程等等。这个列表相当长，并且每天都在增长。
- en: At the same time, there are still a few challenges. The expectations from deep
    learning continue to increase. Deep learning is not a silver bullet or a magic
    wand to resolve all the issues. It is surely one of the more sophisticated solutions
    but it is certainly not the 100% solution to all the business problems. At the
    same time, the dataset we need to feed the algorithms is not always available.
    There is a dearth of good quality datasets which are representatives of the business
    problem. Often it is observed that, big organizations like Google or Meta or Amazon
    can afford to collect such massive datasets. And many times, we do find a lot
    of quality issues in the data. Having processing power to train these complex
    algorithms is also a challenge. With the advent of cloud computing, though this
    problem has been resolved to a certain extent.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，仍然存在一些挑战。对深度学习的期望不断增加。深度学习不是解决所有问题的灵丹妙药或魔法棒。它确实是更为复杂的解决方案之一，但绝对不是所有业务问题的100%解决方案。与此同时，我们需要为算法提供的数据集并不总是可用的。缺乏代表业务问题的高质量数据集。经常可以观察到，像谷歌、Meta或亚马逊这样的大型组织可以负担得起收集如此大规模的数据集。而且很多时候，我们在数据中发现了很多质量问题。拥有处理这些复杂算法的处理能力也是一个挑战。随着云计算的出现，尽管这个问题在一定程度上已经得到了解决。
- en: In this chapter, we explored the basics of neural networks and deep learning.
    We covered the details around neurons, activation function, different layers of
    a network and loss function. We also covered in detail the back-propagation algorithm
    – the central algorithm used to train a supervised deep learning solution. Then,
    we briefly went through unsupervised deep learning algorithms. We will be covering
    these unsupervised deep learning solutions in a great details in the next chapters.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了神经网络和深度学习的基础知识。我们涵盖了神经元、激活函数、网络不同层次和损失函数的细节。我们还详细介绍了反向传播算法——用于训练监督深度学习解决方案的核心算法。然后，我们简要介绍了无监督深度学习算法。我们将在接下来的章节中详细介绍这些无监督深度学习解决方案。
- en: We are also showing the major activation functions in Figure 8-13.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了主要的激活函数在图8-13中。
- en: 'Figure 8-13 Major activation functions at a glance (image: towardsdatascience)'
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8-13 主要激活函数一览（图片来源：towardsdatascience）
- en: '![Activation Functions in Neural Networks | by SAGAR SHARMA | Towards Data
    Science](images/08image020.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络中的激活函数 | 作者：SAGAR SHARMA | Towards Data Science](images/08image020.png)'
- en: You can now move to the practice questions.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以转向练习问题。
- en: Practical next steps and suggested readings
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实用的下一步和建议阅读
- en: This book Deep Learning with Python by François Chollet is one of the best resources
    to clarify the concepts of deep learning. It covers all the concepts of deep learning
    and neural networks and is written by the creator of Keras.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 《[深度学习与Python](https://www.amazon.cn/dp/B0785Q7GSY)》这本由François Chollet编写的书籍是澄清深度学习概念的最佳资源之一。它涵盖了深度学习和神经网络的所有概念，并由Keras的创作者编写。
- en: 'Read the following research papers:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读以下研究论文：
- en: Distilling the Knowledge in a Neural Network by G.Hinton et al ([https://arxiv.org/pdf/1503.02531.pdf](pdf.html))
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过G.Hinton等人的论文《[在神经网络中提炼知识](https://arxiv.org/pdf/1503.02531.pdf)》
- en: Training very deep networks by R. Srivastava et al ([https://arxiv.org/pdf/1503.02531.pdf](pdf.html))
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过R. Srivastava等人的论文《[训练非常深的网络](https://arxiv.org/pdf/1503.02531.pdf)》
- en: Distributed Representations of Words and Phrases and their compositionality
    (Word2Vec) by Tomas Mikolov et al ([https://arxiv.org/abs/1310.4546](abs.html))
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Tomas Mikolov等人的论文《[词和短语的分布式表示及其组合性（Word2Vec）](https://arxiv.org/abs/1310.4546)》
- en: Generative Adversarial Networks (GANs) by Ian J. Goodfellow et al ([https://arxiv.org/abs/1406.2661](abs.html))
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Ian J. Goodfellow等人的论文《[生成对抗网络（GANs）](https://arxiv.org/abs/1406.2661)》
- en: Deep Residual Learning for Image Recognition (ResNet) by Kaining He et al ([https://arxiv.org/abs/1512.03385](abs.html))
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Kaining He等人的论文《[用于图像识别的深度残差学习（ResNet）](https://arxiv.org/abs/1512.03385)》
- en: 8.9 Summary
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.9 总结
- en: Deep learning and neural networks
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习和神经网络
- en: Definition of neurons
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元的定义
- en: Different types of activation functions
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的激活函数
- en: Different types of optimization functions
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的优化函数
- en: Neural network training process
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络训练过程
- en: Various libraries for deep learning
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的各种库
