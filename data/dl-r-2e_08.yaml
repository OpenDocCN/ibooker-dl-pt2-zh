- en: 5 *Fundamentals of machine learning*
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 *机器学习基础*
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章内容*'
- en: Understanding the tension between generalization and optimization, the fundamental
    issue in machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解泛化和优化之间的紧张关系，这是机器学习中的基本问题
- en: Evaluation methods for machine learning models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的评估方法
- en: Best practices to improve model fitting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善模型拟合的最佳实践
- en: Best practices to achieve better generalization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实现更好的泛化而采用的最佳实践
- en: 'After the three practical examples in chapter 4, you should be starting to
    feel familiar with how to approach classification and regression problems using
    neural networks, and you’ve witnessed the central problem of machine learning:
    overfitting. This chapter will formalize some of your new intuition about machine
    learning into a solid conceptual framework, highlighting the importance of accurate
    model evaluation and the balance between training and generalization.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章的三个实际示例之后，你应该开始感到熟悉如何使用神经网络解决分类和回归问题，并且你已经目睹了机器学习的核心问题：过拟合。本章将会将一些关于机器学习的新直觉正式化为一个坚实的概念框架，强调准确的模型评估和训练与泛化之间的平衡的重要性。
- en: '5.1 Generalization: The goal of machine learning'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 泛化：机器学习的目标
- en: 'In the three examples presented in chapter 4—predicting movie reviews, topic
    classification, and house-price regression—we split the data into a training set,
    a validation set, and a test set. The reason not to evaluate the models on the
    same data they were trained on quickly became evident: after just a few epochs,
    performance on never-before-seen data started diverging from performance on the
    training data, which always improves as training progresses. The models started
    to *overfit*. Overfitting happens in every machine learning problem.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章中介绍的三个示例——预测电影评论、主题分类和房价回归——我们将数据分为训练集、验证集和测试集。很快就能明显看到不要在训练模型的相同数据上评估模型的原因：在几个周期后，从未见过的数据上的性能开始与训练数据上的性能分歧，而训练数据的性能随着训练的进行而改善。模型开始*过拟合*。在每个机器学习问题中都会发生过拟合。
- en: The fundamental issue in machine learning is the tension between optimization
    and generalization. *Optimization* refers to the process of adjusting a model
    to get the best performance possible on the training data (the *learning* in *machine
    learning*), whereas *generalization* refers to how well the trained model performs
    on data it has never seen before. The goal of the game is to get good generalization,
    of course, but you don’t control generalization; you can only fit the model to
    its training data. If you do that *too well*, overfitting kicks in and generalization
    suffers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的基本问题是优化和泛化之间的紧张关系。*优化* 是指调整模型以在训练数据上获得最佳性能的过程（*机器学习* 中的*学习*），而*泛化* 是指训练模型在以前从未见过的数据上的表现如何。游戏的目标当然是获得良好的泛化，但你无法控制泛化；你只能使模型适应其训练数据。如果你做得*太好*，过拟合就会发生，泛化就会受到影响。
- en: But what causes overfitting? How can we achieve good generalization?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但是导致过拟合的原因是什么？我们如何获得良好的泛化？
- en: 5.1.1 Underfitting and overfitting
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 欠拟合和过拟合
- en: For the models you saw in the previous chapter, performance on the held-out
    validation data started improving as training went on and then inevitably peaked
    after a while. This pattern (illustrated in [figure 5.1](#fig5-1)) is universal.
    You’ll see it with any model type and any dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你在上一章中看到的模型，随着训练的进行，验证数据的性能开始提高，然后不可避免地在一段时间后达到峰值。这种模式（在[图 5.1](#fig5-1)中说明）是普遍存在的。你会在任何模型类型和任何数据集中都看到它。
- en: '![Image](../images/f0131-01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0131-01.jpg)'
- en: '**Figure 5.1 Canonical overfitting behavior**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.1 典型的过拟合行为**'
- en: 'At the beginning of training, optimization and generalization are correlated:
    the lower the loss on training data, the lower the loss on test data. While this
    is happening, your model is said to be *underfit*: there is still progress to
    be made; the network hasn’t yet modeled all relevant patterns in the training
    data. But after a certain number of iterations on the training data, generalization
    stops improving, validation metrics stall, and then begin to degrade: the model
    is starting to overfit. That is, it’s beginning to learn patterns that are specific
    to the training data but that are misleading or irrelevant when it comes to new
    data.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始阶段，优化和泛化是相关的：在训练数据上损失越低，测试数据上的损失也越低。当这种情况发生时，你的模型被称为是*欠拟合*的：仍然有进步空间；网络尚未对训练数据中的所有相关模式建模。但是在对训练数据进行了一定数量的迭代后，泛化停止改善，验证指标停滞不前，然后开始恶化：模型开始过拟合。也就是说，它开始学习对训练数据特定的模式，但这些模式在新数据方面是误导性的或无关紧要的。
- en: Overfitting is particularly likely to occur when your data is noisy, if it involves
    uncertainty, or if it includes rare features. Let’s look at some concrete examples.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据嘈杂、涉及不确定性或包含罕见特征时，过拟合特别容易发生。让我们看一些具体的例子。
- en: '**NOISY TRAINING DATA**'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**嘈杂的训练数据**'
- en: In real-world datasets, it’s fairly common for some inputs to be invalid. Perhaps
    a MNIST digit could be an all-black image, for instance, or something like [figure
    5.2](#fig5-2).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的数据集中，一些输入无效是相当常见的。例如，MNIST 数字可能是一张全黑的图片，或者类似于[图5.2](#fig5-2)的东西。
- en: '![Image](../images/f0132-01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0132-01.jpg)'
- en: '**Figure 5.2 Some pretty weird MNIST training samples**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.2 一些相当奇怪的 MNIST 训练样本**'
- en: What are these? I don’t know, either. But they’re all part of the MNIST training
    set. What’s even worse, however, is having perfectly valid inputs that end up
    mislabeled, like those in [figure 5.3](#fig5-3).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是什么？我也不知道。但它们都是MNIST训练集的一部分。更糟糕的是，存在完全有效的输入最终被错误标记，就像[图5.3](#fig5-3)中的那些一样。
- en: '![Image](../images/f0132-02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0132-02.jpg)'
- en: '**Figure 5.3 Mislabeled MNIST training samples**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.3 错误标记的 MNIST 训练样本**'
- en: If a model goes out of its way to incorporate such outliers, its generalization
    performance will degrade, as shown in [figure 5.4](#fig5-4). For instance, a 4
    that looks very close to the mislabeled 4 in [figure 5.3](#fig5-3) may end up
    getting classified as a 9.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型极力将这些离群值纳入考虑范围，其泛化性能将下降，如[图5.4](#fig5-4)所示。例如，一个看起来与[图5.3](#fig5-3)中错误标记的4非常接近的4可能最终被分类为9。
- en: '![Image](../images/f0132-03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0132-03.jpg)'
- en: '**Figure 5.4 Dealing with outliers: Robust fit vs. overfitting**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.4 处理离群值：稳健拟合 vs. 过拟合**'
- en: '**AMBIGUOUS FEATURES**'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**模糊特征**'
- en: Not all data noise comes from inaccuracies—even perfectly clean and neatly labeled
    data can be noisy when the problem involves uncertainty and ambiguity. In classification
    tasks, it is often the case that some regions of the input feature space are associated
    with multiple classes at the same time. Let’s say you’re developing a model that
    takes an image of a banana and predicts whether the banana is unripe, ripe, or
    rotten. These categories have no objective boundaries, so the same picture might
    be classified as either unripe or ripe by different human labelers. Similarly,
    many problems involve randomness. You could use atmospheric pressure data to predict
    whether it will rain tomorrow, but the exact same measurements may be followed
    sometimes by rain and sometimes by a clear sky, with some probability.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的数据噪声都来自于不准确性——即使是完全干净且标记整齐的数据，在涉及不确定性和模棱两可的问题时也可能是嘈杂的。在分类任务中，通常情况下，输入特征空间的某些区域同时与多个类相关联。比如说，你正在开发一个模型，它接收香蕉的图像并预测香蕉是未成熟、成熟还是腐烂的。这些类别没有客观的界限，所以同一张图片可能会被不同的人类标记者分类为未成熟或成熟。同样地，许多问题涉及随机性。你可以利用大气压力数据来预测明天是否会下雨，但完全相同的测量结果有时可能会导致雨天，有时可能导致晴天，具有一定的概率。
- en: A model could overfit to such probabilistic data by being too confident about
    ambiguous regions of the feature space, like in [figure 5.5](#fig5-5). A more
    robust fit would ignore individual data points and look at the bigger picture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型可能会对这种概率性数据过拟合，过于自信于特征空间的模糊区域，就像[图5.5](#fig5-5)中一样。一个更健壮的拟合会忽略个别数据点，而着眼于更大的视角。
- en: '![Image](../images/f0133-01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0133-01.jpg)'
- en: '**Figure 5.5 Robust fit vs. overfitting given an ambiguous area of the feature
    space**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.5 在特征空间的模糊区域中的稳健拟合 vs. 过拟合**'
- en: '**RARE FEATURES AND SPURIOUS CORRELATIONS**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**罕见特征和伪相关性**'
- en: 'If you’ve only ever seen two orange tabby cats in your life, and they both
    happened to be terribly antisocial, you might infer that orange tabby cats are
    generally likely to be antisocial. That’s overfitting: if you had been exposed
    to a wider variety of cats, including more orange ones, you’d have learned that
    cat color is not well correlated with character.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只见过两只橘色虎斑猫，并且它们都很反社会，你可能会推断橘色虎斑猫大致上都倾向于反社会。这就是过拟合：如果你接触到了更多种类的猫，包括更多橘色的猫，你会发现猫的颜色与性格并没有很强的关联。
- en: Likewise, machine learning models trained on datasets that include rare feature
    values are highly susceptible to overfitting. In a sentiment classification task,
    if the word *cherimoya* (a fruit native to the Andes) appears in only one text
    in the training data, and this text happens to be negative in sentiment, a poorly
    regularized model might put a very high weight on this word and always classify
    new texts that mention cherimoyas as negative, whereas, objectively, there’s nothing
    negative about the cherimoya.^([1](#Rendnote1))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，训练在包含罕见特征值的数据集上的机器学习模型非常容易过拟合。在情感分类任务中，如果词 *cherimoya*（一种生长在安第斯山脉的水果）只出现在训练数据中的一篇文本中，并且这篇文本恰好是消极情绪，一个没有良好正则化的模型可能会对这个词放很高的权重，并且总是将提到
    cherimoya 的新文本分类为消极，然而，客观上讲，cherimoya 并没有任何消极的含义。 [^1]
- en: Importantly, a feature value doesn’t need to occur only a couple of times to
    lead to spurious correlations. Consider a word that occurs in 100 samples in your
    training data that’s associated with a positive sentiment 54% of the time and
    with a negative sentiment 46% of the time. That difference may well be a complete
    statistical fluke, yet your model is likely to learn to leverage that feature
    for its classification task. This is one of the most common sources of overfitting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，一个特征值不必只出现几次就会导致错误的相关性。考虑一个在训练数据中出现在100个样本中的词，54%的情况下与积极情绪相关，46%的情况下与消极情绪相关。这个差异很可能只是一个完全的统计故障，然而你的模型很可能会学习利用这个特征进行分类任务。这是过拟合的最常见来源之一。
- en: 'Here’s a striking example. Using MNIST, create a new training set by concatenating
    784 white noise dimensions to the existing 784 dimensions of the data, so half
    of the data is now noise. For comparison, also create an equivalent dataset by
    concatenating 784 all-zeros dimensions. Our concatenation of meaningless features
    does not at all affect the information content of the data: we’re only adding
    something. Human classification accuracy wouldn’t be affected by these transformations
    at all.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个惊人的例子。使用MNIST数据集，通过在现有的784维数据上连续添加784个白噪声维度，创建一个新的训练集，使得一半的数据现在是噪声。为了比较，还创建一个等效的数据集，通过在现有的784维数据上连续添加784个全零维度。我们的无意义特征的组合对数据的信息内容没有影响：我们只是添加了一些东西。这些转换对人类的分类准确性不会产生影响。
- en: '**Listing 5.1 Adding white noise channels or all-zeros channels to MNIST**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.1 向MNIST数据集添加白噪声通道或全零通道**'
- en: library(keras)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: library(keras)
- en: mnist <- dataset_mnist()
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: mnist <- dataset_mnist()
- en: train_labels <- mnist$train$y
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: train_labels <- mnist$train$y
- en: train_images <- array_reshape(mnist$train$x / 255,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- array_reshape(mnist$train$x / 255,
- en: c(60000, 28 * 28))
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: c(60000, 28 * 28))
- en: random_array <- function(dim) array(runif(prod(dim)), dim)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: random_array <- function(dim) array(runif(prod(dim)), dim)
- en: noise_channels <- random_array(dim(train_images))
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: noise_channels <- random_array(dim(train_images))
- en: train_images_with_noise_channels <- cbind(train_images, noise_channels)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: train_images_with_noise_channels <- cbind(train_images, noise_channels)
- en: zeros_channels <- array(0, dim(train_images))
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: zeros_channels <- array(0, dim(train_images))
- en: train_images_with_zeros_channels <- cbind(train_images, zeros_channels)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: train_images_with_zeros_channels <- cbind(train_images, zeros_channels)
- en: Now let’s train the model from chapter 2 on both of these training sets.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在这两个训练集上训练第2章的模型。
- en: '**Listing 5.2 Training the same model with noise channels or all-zeros channels**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.2 使用带有噪声通道或全零通道的相同模型进行训练**'
- en: get_model <- function()
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: get_model <- function()
- en: '{ model <- keras_model_sequential() %>%'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '{ model <- keras_model_sequential() %>%'
- en: layer_dense(512, activation = "relu") %>%
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(512, activation = "relu") %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: model %>% compile(
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(
- en: optimizer = "rmsprop",
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '}'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: model <- get_model()
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model()
- en: history_noise <- model %>% fit(
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: history_noise <- model %>% fit(
- en: train_images_with_noise_channels, train_labels,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: train_images_with_noise_channels, train_labels,
- en: epochs = 10,
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: batch_size = 128,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128,
- en: validation_split = 0.2)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: validation_split = 0.2)
- en: model <- get_model()
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model()
- en: history_zeros <- model %>% fit(
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: history_zeros <- model %>% fit(
- en: train_images_with_zeros_channels, train_labels,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: train_images_with_zeros_channels, train_labels,
- en: epochs = 10,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: batch_size = 128,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128,
- en: validation_split = 0.2)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: validation_split = 0.2)
- en: Let’s compare how the validation accuracy of each model evolves over time. The
    result is shown in [figure 5.6](#fig5-6).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较每个模型的验证准确性随时间的演变。结果显示在 [图 5.6](#fig5-6) 中。
- en: '**Listing 5.3 Plotting a validation accuracy comparison**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 5.3 绘制验证准确性比较**'
- en: plot(NULL,
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: plot(NULL,
- en: main = "Effect of Noise Channels on Validation Accuracy",
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 主题 = "噪声通道对验证准确性的影响",
- en: xlab = "Epochs", xlim = c(1, history_noise$params$epochs),
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: xlab = "时期", xlim = c(1, history_noise$params$epochs),
- en: ylab = "Validation Accuracy", ylim = c(0.9, 1), las = 1)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ylab = "验证准确性", ylim = c(0.9, 1), las = 1)
- en: lines(history_zeros$metrics$val_accuracy, lty = 1, type = "o")
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_zeros$metrics$val_accuracy, lty = 1, type = "o")
- en: lines(history_noise$metrics$val_accuracy, lty = 2, type = "o")
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_noise$metrics$val_accuracy, lty = 2, type = "o")
- en: legend("bottomright", lty = 1:2,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: legend("bottomright", lty = 1:2,
- en: legend = c("Validation accuracy with zeros channels",
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图例 = c("带零通道的验证准确性",
- en: '"Validation accuracy with noise channels"))'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '"带噪声通道的验证准确性"))'
- en: '![Image](../images/f0135-01.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0135-01.jpg)'
- en: '**Figure 5.6 Effect of noise channels on validation accuracy**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.6 噪声通道对验证准确性的影响**'
- en: Despite the data holding the same information in both cases, the validation
    accuracy of the model trained with noise channels ends up about one percentage
    point lower—purely through the influence of spurious correlations. The more noise
    channels you add, the further accuracy will degrade.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据在两种情况下都持有相同的信息，但使用噪声通道训练的模型的验证准确性最终约低了一个百分点—纯粹是通过虚假相关性的影响。您添加的噪声通道越多，准确性就会进一步下降。
- en: Noisy features inevitably lead to overfitting. As such, in cases where you aren’t
    sure whether the features you have are informative or distracting, it’s common
    to do *feature selection* before training. Restricting the IMDB data to the top
    10,000 most common words was a crude form of feature selection, for instance.
    The typical way to do feature selection is to compute some usefulness score for
    each feature available—a measure of how informative the feature is with respect
    to the task, such as the mutual information between the feature and the labels—and
    keep only features that are above some threshold. Doing this would filter out
    the white noise channels in the preceding example.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 嘈杂的特征不可避免地导致过度拟合。因此，在不确定您拥有的特征是信息性的还是干扰性的情况下，在训练之前进行 *特征选择* 是很常见的。例如，将 IMDB
    数据限制为前 10,000 个最常见的单词是一种粗略的特征选择形式。进行特征选择的典型方式是为每个可用特征计算一些有用性分数—即特征相对于任务的信息量，例如特征与标签之间的互信息—并仅保留高于某个阈值的特征。这样做会过滤掉前面示例中的白噪声通道。
- en: 5.1.2 The nature of generalization in deep learning
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 深度学习中泛化性质
- en: A remarkable fact about deep learning models is that they can be trained to
    fit anything, as long as they have enough representational power.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的一个显著事实是，只要具有足够的表征能力，它们就可以被训练来适应任何东西。
- en: Don’t believe me? Try shuffling the MNIST labels and train a model on that.
    Even though there is no relationship whatsoever between the inputs and the shuffled
    labels, the training loss goes down just fine, even with a relatively small model.
    Naturally, the validation loss does not improve at all over time, because there
    is no possibility of generalization in this setting (see the following graph).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不信？试着洗牌 MNIST 标签并在其上训练一个模型。即使输入与洗牌后的标签之间完全没有关系，训练损失也会下降得很好，即使是一个相对较小的模型。自然，随着时间的推移，验证损失根本不会改善，因为在这种情况下没有可能进行泛化（请参见以下图表）。
- en: '**Listing 5.4 Fitting an MNIST model with randomly shuffled labels**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 5.4 使用随机洗牌标签拟合 MNIST 模型**'
- en: c(c(train_images, train_labels), .) %<-% dataset_mnist()➊
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: c(c(train_images, train_labels), .) %<-% dataset_mnist()➊
- en: train_images <- array_reshape(train_images / 255,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- array_reshape(train_images / 255,
- en: c(60000, 28 * 28))
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: c(60000, 28 * 28))
- en: random_train_labels <- sample(train_labels)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: random_train_labels <- sample(train_labels)
- en: model <- keras_model_sequential() %>%
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(512, activation = "relu") %>%
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(512, activation = "relu") %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: history <- model %>% fit(train_images, random_train_labels,
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(train_images, random_train_labels,
- en: epochs = 100,
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 100,
- en: batch_size = 128,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128,
- en: validation_split = 0.2)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 验证分割 = 0.2)
- en: plot(history)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制(history)
- en: ➊ **Use . in a %<-% multi-assign call to ignore some elements. Here, we're ignoring
    the test portion of MNIST.**
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在 %<-% 多重赋值调用中使用.来忽略一些元素。在这里，我们忽略了MNIST的测试部分。**
- en: '![Image](../images/f0137-01.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0137-01.jpg)'
- en: In fact, you don’t even need to do this with MNIST data—you could just generate
    white noise inputs and random labels. You could fit a model on that, too, as long
    as it has enough parameters. It would just end up memorizing specific inputs,
    much like a hash table.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你甚至不需要用MNIST数据做这个——你可以只是生成白噪声输入和随机标签。只要它有足够的参数，你也可以对其进行模型拟合。它最终只会记住特定的输入，就像一个哈希表一样。
- en: If this is the case, then how come deep learning models generalize at all? Shouldn’t
    they just learn an ad hoc mapping between training inputs and targets, like a
    fancy hash table? What expectation can we have that this mapping will work for
    new inputs?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样的话，那么深度学习模型怎么会有泛化能力呢？它们难道不应该只是学会了训练输入和目标之间的一种特定映射，就像一个花哨的哈希表吗？我们对于这种映射能够适用于新输入有什么期望呢？
- en: As it turns out, the nature of generalization in deep learning has rather little
    to do with deep learning models themselves and much to do with the structure of
    information in the real world. Let’s take a look at what’s really going on here.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，深度学习中的泛化性质与深度学习模型本身关系不大，而与现实世界中信息的结构有很大关系。让我们来看看这里真正发生了什么。
- en: '**THE MANIFOLD HYPOTHESIS**'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**流形假设**'
- en: 'The input to an MNIST classifier (before preprocessing) is a 28 × 28 array
    of integers between 0 and 255\. The total number of possible input values is thus
    256 to the power of 784—much greater than the number of atoms in the universe.
    However, very few of these inputs would look like valid MNIST samples: actual
    handwritten digits occupy only a tiny *subspace* of the parent space of all possible
    28 × 28 integer arrays. What’s more, this subspace isn’t just a set of points
    sprinkled at random in the parent space: it is highly structured.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST分类器的输入（在预处理之前）是一个28 × 28的整数数组，其值介于0和255之间。因此，可能的输入值的总数是256的784次方——远远大于宇宙中的原子数。然而，这些输入中很少有看起来像是有效的MNIST样本：实际的手写数字只占据了所有可能的28
    × 28整数数组的父空间中的一小部分*子空间*。而且，这个子空间不仅仅是在父空间中随机分布的一组点：它具有高度结构化的特性。
- en: 'First, the subspace of valid handwritten digits is *continuous*: if you take
    a sample and modify it a little, it will still be recognizable as the same handwritten
    digit. Further, all samples in the valid subspace are *connected* by smooth paths
    that run through the subspace. This means that if you take two random MNIST digits
    A and B, there exists a sequence of “intermediate” images that morph A into B,
    such that two consecutive digits are very close to each other (see [figure 5.7](#fig5-7)).
    Perhaps there will be a few ambiguous shapes close to the boundary between two
    classes, but even these shapes would still look very digit-like.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有效手写数字的子空间是*连续*的：如果你对一个样本进行一点点修改，它仍然可以被识别为相同的手写数字。此外，所有有效子空间中的样本都是通过在子空间中穿过的平滑路径*连接*起来的。这意味着如果你取两个随机的MNIST数字A和B，存在一个“中间”图像序列，可以将A变形为B，使得两个连续的数字非常接近（参见[图5.7](#fig5-7)）。也许在两个类之间的边界上会有一些模糊的形状，但即使这些形状看起来仍然非常像数字。
- en: '![Image](../images/f0138-01.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0138-01.jpg)'
- en: '**Figure 5.7 Different MNIST digits gradually morphing into one another, showing
    that the space of handwritten digits forms a manifold. This image was generated
    using code from chapter 12.**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.7 不同的MNIST数字逐渐转变成彼此，显示手写数字的空间形成一个流形。此图是使用第12章的代码生成的。**'
- en: In technical terms, you would say that handwritten digits form a *manifold*
    within the space of possible 28 × 28 integer arrays. That’s a big word, but the
    concept is pretty intuitive. A manifold is a lower-dimensional subspace of some
    parent space that is locally similar to a linear (Euclidian) space. For instance,
    a smooth curve in the plane is a 1D manifold within a 2D space, because for every
    point on the curve, you can draw a tangent (the curve can be approximated by a
    line at every point). A smooth surface within a 3D space is a 2D manifold. And
    so on.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，你会说手写数字在可能的28 × 28整数数组空间内形成了一个*流形*。这是一个大词，但概念上相当直观。流形是某些父空间的低维子空间，局部类似于线性（欧几里得）空间。例如，在平面上的平滑曲线是2D空间内的1D流形，因为对于曲线上的每一点，你都可以画出一个切线（曲线在每一点都可以用一条直线来近似）。在3D空间中的平滑曲面是2D流形。等等。
- en: More generally, the *manifold hypothesis* posits that all natural data lies
    on a low-dimensional manifold within the high-dimensional space where it is encoded.
    That’s a pretty strong statement about the structure of information in the universe.
    As far as we know, it’s accurate, and it’s the reason deep learning works. It’s
    true for MNIST digits, but also for human faces, tree morphology, the sounds of
    the human voice, and even natural language.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地，*流形假设*认为所有自然数据都位于其编码的高维空间内的低维流形上。这是关于宇宙信息结构的一个非常强有力的陈述。据我们所知，这是准确的，也是深度学习有效的原因。对于MNIST数字是正确的，但对于人脸、树形态、人类声音以及自然语言也是如此。
- en: The manifold hypothesis implies that
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 流形假设意味着
- en: Machine learning models only have to fit relatively simple, low-dimensional,
    highly-structured subspaces within their potential input space (latent manifolds)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型只需适应其潜在输入空间（潜在流形）内的相对简单、低维、高度结构化的子空间
- en: Within one of these manifolds, it’s always possible to *interpolate* between
    two inputs, that is to say, morph one into another via a continuous path along
    which all points fall on the manifold
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这些流形之一中，总是可以在两个输入之间进行*插值*，也就是说，通过沿着所有点都落在流形上的连续路径将一个输入变形为另一个输入
- en: The ability to interpolate between samples is the key to understanding generalization
    in deep learning.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本之间进行插值的能力是理解深度学习中泛化的关键。
- en: '**INTERPOLATION AS A SOURCE OF GENERALIZATION**'
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**插值作为泛化的源头**'
- en: If you work with data points that can be interpolated, you can start making
    sense of points you’ve never seen before by relating them to other points that
    lie close on the manifold. In other words, you can make sense of the *totality*
    of the space using only a *sample* of the space. You can use interpolation to
    fill in the blanks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理可以插值的数据点，你可以通过将它们与流形上接近的其他点相关联来开始理解以前从未见过的点。换句话说，你可以只使用空间的一个*样本*来理解*整体*空间。你可以使用插值来填补空白。
- en: Note that interpolation on the latent manifold is different from linear interpolation
    in the parent space, as illustrated in [figure 5.8](#fig5-8). For instance, the
    average of pixels between two MNIST digits is usually not a valid digit.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，潜在流形上的插值与父空间中的线性插值不同，如[图5.8](#fig5-8)所示。例如，在两个MNIST数字之间的像素的平均通常不是有效的数字。
- en: '![Image](../images/f0139-01.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0139-01.jpg)'
- en: '**Figure 5.8 The difference between linear interpolation and interpolation
    on the latent manifold. Every point on the latent manifold of digits is a valid
    digit, but the average of two digits usually isn’t.**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.8 线性插值与潜在流形上的插值之间的区别。数字的潜在流形上的每一点都是有效的数字，但两个数字的平均值通常不是。**'
- en: 'Crucially, although deep learning achieves generalization via interpolation
    on a learned approximation of the data manifold, it would be a mistake to assume
    that interpolation is *all* there is to generalization. It’s the tip of the iceberg.
    Interpolation can only help you make sense of things that are very close to what
    you’ve seen before: it enables *local generalization*. But remarkably, humans
    deal with extreme novelty all the time, and we do just fine. You don’t need to
    be trained in advance on countless examples of every situation you’ll ever have
    to encounter. Every single one of your days is different from any day you’ve experienced
    before, and different from any day experienced by anyone since the dawn of humanity.
    You can switch between spending a week in New York City, a week in Shanghai, and
    a week in Bangalore without requiring thousands of lifetimes of learning and rehearsal
    for each city.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，尽管深度学习通过对数据流形的学习近似进行插值来实现泛化，但认为插值就是泛化的全部是错误的。这只是冰山一角。插值只能帮助你理解与之前见过的东西非常接近的事物：它实现了*局部泛化*。但值得注意的是，人类经常处理极端的新奇事物，而且我们做得很好。你不需要事先接受每种情况的无数示例的训练和排练。你每一天的生活都不同于以往任何一天，也不同于人类历史的任何一天。你可以在纽约市待上一周，然后在上海待一周，再在班加罗尔待上一周，而无需为每个城市需要成千上万次的学习和排练。
- en: 'Humans are capable of *extreme generalization*, which is enabled by cognitive
    mechanisms other than interpolation: abstraction, symbolic models of the world,
    reasoning, logic, common sense, innate priors about the world—what we generally
    call *reason*, as opposed to intuition and pattern recognition. The latter are
    largely interpolative in nature, but the former isn’t. Both are essential to intelligence.
    We’ll talk more about this in chapter 14.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够进行*极端泛化*，这是由于与插值不同的认知机制的启用：抽象、世界的符号模型、推理、逻辑、常识、关于世界的先验知识——我们通常称之为*理性*，与直觉和模式识别相对。后者在性质上主要是插值的，而前者不是。这两者对智力都是至关重要的。我们将在第14章中更详细地讨论这个问题。
- en: '**WHY DEEP LEARNING WORKS**'
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**深度学习为何有效**'
- en: Remember the crumpled paper ball metaphor from chapter 2? A sheet of paper represents
    a 2D manifold within 3D space (see [figure 5.9](#fig5-9)). A deep learning model
    is a tool for uncrumpling paper balls, that is, for disentangling latent manifolds.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得第2章中的揉皱纸团的比喻吗？一张纸代表了三维空间内的二维流形（见[图5.9](#fig5-9)）。深度学习模型是一种解开纸团的工具，也就是说，是解开潜在流形的工具。
- en: A deep learning model is basically a very high-dimensional curve—a curve that
    is smooth and continuous (with additional constraints on its structure, originating
    from model architecture priors), because it needs to be differentiable. And that
    curve is fitted to data points via gradient descent, smoothly and incrementally.
    By its very nature, deep learning is about taking a big, complex curve—a manifold—and
    incrementally adjusting its parameters until it fits some training data points.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型基本上是一个非常高维的曲线——一个平滑连续的曲线（具有来自模型架构先验的结构附加约束），因为它需要可微性。而且，该曲线通过梯度下降平稳地逐渐拟合到数据点。由于其本质，深度学习是关于采取一个大的、复杂的曲线——一个流形——并逐渐调整其参数，直到它适合一些训练数据点。
- en: '![Image](../images/f0140-01.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0140-01.jpg)'
- en: '**Figure 5.9 Uncrumpling a complicated manifold of data**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.9 展开复杂的数据流形**'
- en: The curve involves enough parameters that it could fit anything—indeed, if you
    let your model train for long enough, it will effectively end up purely memorizing
    its training data and won’t generalize at all. However, the data you’re fitting
    to isn’t made of isolated points sparsely distributed across the underlying space.
    Your data forms a highly structured, low-dimensional manifold within the input
    space—that’s the manifold hypothesis. And because fitting your model curve to
    this data happens gradually and smoothly over time as gradient descent progresses,
    there will be an intermediate point during training at which the model roughly
    approximates the natural manifold of the data, as you can see in [figure 5.10](#fig5-10).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线涉及足够多的参数，可以适合任何东西——事实上，如果你让你的模型训练足够长的时间，它将有效地纯粹记住它的训练数据，而不会完全推广。然而，你拟合的数据并不是由稀疏分布在底层空间中的孤立点组成的。你的数据在输入空间内形成了一个高度结构化的低维流形——这就是流形假设。由于拟合模型曲线到这些数据是随着梯度下降的进行逐渐平滑地进行的，因此在训练过程中将会有一个中间点，此时模型大致近似于数据的自然流形，就像你在[图5.10](#fig5-10)中所看到的那样。
- en: '![Image](../images/f0140-02.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0140-02.jpg)'
- en: '**Figure 5.10 Going from a random model to an overfit model and achieving a
    robust fit as an intermediate state**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.10 从随机模型到过度拟合模型并实现鲁棒拟合作为中间状态**'
- en: Moving along the curve learned by the model at that point will come close to
    moving along the actual latent manifold of the data. As such, the model will be
    capable of making sense of never-before-seen inputs via interpolation between
    training inputs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着模型学习的曲线在那一点移动将接近于沿着数据的实际潜在流形移动。因此，模型将能够通过在训练输入之间进行插值来理解以前未曾见过的输入。
- en: 'Besides the trivial fact that they have sufficient representational power,
    deep learning models have the following properties that make them particularly
    well-suited to learning latent manifolds:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了它们具有足够的表示能力这一平凡事实之外，深度学习模型具有以下特性，使它们特别适合学习潜在流形：
- en: Deep learning models implement a smooth, continuous mapping from their inputs
    to their outputs. It has to be smooth and continuous because it must be differentiable,
    by necessity (you couldn’t do gradient descent otherwise). This smoothness helps
    approximate latent manifolds, which follow the same properties.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型实现了从输入到输出的平滑连续映射。它必须是平滑连续的，因为它必须是可微的，必须如此（否则你无法进行梯度下降）。这种平滑性有助于逼近潜在流形，其遵循相同的性质。
- en: Deep learning models tend to be structured in a way that mirrors the “shape”
    of the information in their training data (via architecture priors). This is particularly
    the case for image-processing models (discussed in chapters 8 and 9) and sequence-processing
    models (chapter 10). More generally, deep neural networks structure their learned
    representations in a hierarchical and modular way, which echoes the way natural
    data is organized
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型往往以与其训练数据中信息“形状”相似的方式进行结构化（通过体系结构先验）。这在图像处理模型（在第8章和第9章中讨论）和序列处理模型（第10章中）中尤为明显。更一般地说，深度神经网络以层次化和模块化的方式组织其学习到的表示，这与自然数据组织方式相呼应。
- en: '**TRAINING DATA IS PARAMOUNT**'
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**训练数据至关重要**'
- en: Although deep learning is indeed well suited to manifold learning, the power
    to generalize is more a consequence of the natural structure of your data than
    a consequence of any property of your model. You’ll be able to generalize only
    if your data forms a manifold where points can be interpolated. The more informative
    and the less noisy your features are, the better you will be able to generalize,
    because your input space will be simpler and better structured. Data curation
    and feature engineering are essential to generalization.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习确实非常适合流形学习，但泛化的能力更多地是由你的数据的自然结构决定而不是你的模型的任何属性。只有在数据形成了可以进行插值的流形时，你才能进行泛化。特征越具有信息性且噪声越少，你就越能够进行泛化，因为你的输入空间将更简单、更有结构。数据整理和特征工程对泛化至关重要。
- en: 'Further, because deep learning is curve fitting, for a model to perform well,
    *it needs to be trained on a dense sampling of its input space*. A dense sampling
    in this context means that the training data should densely cover the entirety
    of the input data manifold (see [figure 5.11](#fig5-11)). This is especially true
    near decision boundaries. With a sufficiently dense sampling, it becomes possible
    to make sense of new inputs by interpolating between past training inputs without
    having to use common sense, abstract reasoning, or external knowledge about the
    world—all things that machine learning models have no access to. As such, you
    should always keep in mind that the best way to improve a deep learning model
    is to train it on more data or better data (of course, adding overly noisy or
    inaccurate data will harm generalization). A denser coverage of the input data
    manifold will yield a model that generalizes better. You should never expect a
    deep learning model to perform anything more than crude interpolation between
    its training samples, and thus you should do everything you can to make interpolation
    as easy as possible. The only thing you will find in a deep learning model is
    what you put into it: the priors encoded in its architecture and the data it was
    trained on.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于深度学习是曲线拟合，因此要使模型表现出色，*必须在输入空间上进行稠密采样训练*。在这种情况下，稠密采样意味着训练数据应该密集地覆盖整个输入数据曲面（参见[图5.11](#fig5-11)）。这在决策边界附近尤为重要。通过足够稠密的采样，就能够通过在过去的训练输入之间插值来理解新的输入，而无需使用常识、抽象推理或对世界的外部知识——这些是机器学习模型无法获得的。因此，你应该始终记住，改善深度学习模型的最佳方法是让其在更多数据或更好的数据上进行训练（当然，添加过于嘈杂或不准确的数据将损害泛化能力）。输入数据曲面的更密集覆盖将产生更好泛化性能的模型。你永远不应该指望深度学习模型能够执行比其训练样本之间的粗略插值更多的任务，因此你应该尽一切可能使插值变得更容易。你在深度学习模型中找到的唯一内容就是你输入的内容：其体系结构中编码的先验知识和其训练数据。
- en: '![Image](../images/f0141-01.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0141-01.jpg)'
- en: '**Figure 5.11 A dense sampling of the input space is necessary to learn a model
    capable of accurate generalization.**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.11 在输入空间上进行密集采样是学习一个能够准确泛化的模型所必需的。**'
- en: When getting more data isn’t possible, the next best solution is to modulate
    the quantity of information that your model is allowed to store or to add constraints
    on the smoothness of the model curve. If a network can afford to memorize only
    a small number of patterns, or very regular patterns, the optimization process
    will force it to focus on the most prominent patterns, which have a better chance
    of generalizing well. The process of fighting overfitting this way is called *regularization*.
    We’ll review regularization techniques in depth in section 5.4.4.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当无法获得更多数据时，下一个最好的解决方案是调节模型被允许存储的信息量或者对模型曲线的平滑性添加约束。如果一个网络只能记住少量模式或者非常规则的模式，那么优化过程将迫使它集中于最突出的模式，这些模式有更好的泛化能力。以这种方式抵抗过拟合的过程称为*正则化*。我们将在第
    5.4.4 节深入介绍正则化技术。
- en: 'Before you can start tweaking your model to help it generalize better, you’ll
    need a way to assess how your model is currently doing. In the following section,
    you’ll learn how you can monitor generalization during model development: model
    evaluation.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始调整模型以帮助其更好地推广之前，您需要一种评估您当前模型表现的方法。在接下来的部分中，您将学习如何在模型开发过程中监控泛化：模型评估。
- en: 5.2 Evaluating machine learning models
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 评估机器学习模型
- en: You can control only what you can observe. Because your goal is to develop models
    that can successfully generalize to new data, it’s essential to be able to reliably
    measure the generalization power of your model. In this section, I’ll formally
    introduce the different ways you can evaluate machine learning models. You’ve
    already seen most of them in action in the previous chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你只能控制你能观察到的东西。因为你的目标是开发能够成功推广到新数据的模型，所以能够可靠地衡量模型的泛化能力至关重要。在本节中，我将正式介绍评估机器学习模型的不同方法。您已经在上一章中看到了大部分方法的实际应用。
- en: 5.2.1 Training, validation, and test sets
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 训练、验证和测试集
- en: 'Evaluating a model always boils down to splitting the available data into three
    sets: training, validation, and test. You train on the training data and evaluate
    your model on the validation data. Once your model is ready for prime time, you
    test it one final time on the test data, which is meant to be as similar as possible
    to production data. Then you can deploy the model in production.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型总归要将可用数据分为三组：训练集、验证集和测试集。您在训练数据上训练模型，并在验证数据上评估模型。一旦您的模型准备投入使用，您可以最后一次在测试数据上对其进行测试，这些数据应尽可能与生产数据相似。然后您可以将模型部署到生产环境中。
- en: 'You may ask, why not have two sets: a training set and a test set? You’d train
    on the training data and evaluate on the test data. Much simpler!'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么不设置两个集合：一个训练集和一个测试集？你可以在训练数据上进行训练，然后在测试数据上进行评估。简单多了！
- en: 'The reason is that developing a model always involves tuning its configuration,
    for example, choosing the number of layers or the size of the layers (called the
    *hyper-parameters* of the model, to distinguish them from the *parameters*, which
    are the network’s weights). You do this tuning by using, as a feedback signal,
    the performance of the model on the validation data. In essence, this tuning is
    a form of *learning*: a search for a good configuration in some parameter space.
    As a result, tuning the configuration of the model based on its performance on
    the validation set can quickly result in *over-fitting to the validation set*,
    even though your model is never directly trained on it.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为开发模型总是涉及调整其配置，例如选择层数或层大小（称为模型的*超参数*，以区别于*参数*，即网络的权重）。您通过使用模型在验证数据上的性能作为反馈信号来进行此调整。本质上，这种调整是一种*学习*：在某个参数空间中寻找良好配置的搜索。因此，基于模型在验证集上的性能调整模型的配置可能会很快导致*过度拟合验证集*，即使您的模型从未直接在其上训练过。
- en: Central to this phenomenon is the notion of *information leaks*. Every time
    you tune a hyperparameter of your model based on the model’s performance on the
    validation set, some information about the validation data leaks into the model.
    If you do this only once, for one parameter, then very few bits of information
    will leak, and your validation set will remain reliable for evaluating the model.
    But if you repeat this many times—running one experiment, evaluating on the validation
    set, and modifying your model as a result—then you’ll leak an increasingly significant
    amount of information about the validation set into the model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象的核心是*信息泄漏*的概念。每当你根据模型在验证集上的性能调整模型的超参数时，一些关于验证数据的信息就会泄漏到模型中。如果你只做一次这样的操作，对一个参数，那么很少的信息会泄漏，你的验证集仍然可以可靠地用来评估模型。但如果你多次重复这个过程——进行一次实验，评估验证集，然后根据结果修改模型——那么越来越多的验证集信息将泄漏到模型中。
- en: 'At the end of the day, you’ll end up with a model that performs artificially
    well on the validation data, because that’s what you optimized it for. You care
    about performance on completely new data, not on the validation data, so you need
    to use a completely different, never-before-seen dataset to evaluate the model:
    the test dataset. Your model shouldn’t have had access to *any* information about
    the test set, even indirectly. If anything about the model has been tuned based
    on test set performance, then your measure of generalization will be flawed.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你将得到一个在验证数据上表现良好的模型，因为这是你为其优化的。你关心的是在全新数据上的表现，而不是在验证数据上的表现，因此，你需要使用一个完全不同的，之前从未见过的数据集来评估模型：测试数据集。你的模型不应该有关于测试集的*任何*信息，即使是间接的。如果模型的任何信息都是基于测试集的性能来调整的，那么你的泛化度量将是有缺陷的。
- en: 'Splitting your data into training, validation, and test sets may seem straightforward,
    but we have a few advanced ways to do it that can come in handy when little data
    is available. Let’s review three classic evaluation recipes: simple holdout validation,
    *K*-fold validation, and iterated *K*-fold validation with shuffling. We’ll also
    talk about the use of common-sense baselines to check that your training is going
    somewhere.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为训练、验证和测试集可能看起来很简单，但当数据很少时，我们有一些可以派上用场的高级方法。让我们回顾三种经典的评估方法：简单留出验证、*K*-折交叉验证和带洗牌的重复*K*-折交叉验证。我们还将谈论使用常识基准线来检查你的训练是否是有效的。
- en: '**SIMPLE HOLDOUT VALIDATION**'
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**简单的留出验证**'
- en: Set apart some fraction of your data as your test set. Train on the remaining
    data, and evaluate on the test set. As you saw in the previous sections, to prevent
    information leaks, you shouldn’t tune your model based on the test set, and, therefore,
    you should *also* reserve a validation set.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将一部分数据作为测试集。在剩余数据上训练，并在测试集上评估。正如前面所看到的，为了防止信息泄漏，你不应该基于测试集来调整模型，因此，你也应该*保留一个验证集*。
- en: Schematically, holdout validation looks like [figure 5.12](#fig5-12). Listing
    5.5 shows a simple implementation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 简略地说，留出验证看起来像[图 5.12](#fig5-12)。列表 5.5 展示了一个简单的实现。
- en: '![Image](../images/f0143-01.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0143-01.jpg)'
- en: '**Figure 5.12 Simple holdout validation split**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.12 简单的留出验证切分**'
- en: '**Listing 5.5 Holdout validation (note that labels are omitted for simplicity)**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 5.5 留出验证(为简单起见，标签被省略)**'
- en: num_validation_samples <- 10000
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: num_validation_samples <- 10000
- en: val_indices <- sample.int(num_validation_samples, nrow(data))➊
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: val_indices <- sample.int(num_validation_samples, nrow(data))➊
- en: validation_data <- data[val_indices, ]➋
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data <- data[val_indices, ]➋
- en: training_data <- data[-val_indices, ]➌
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: training_data <- data[-val_indices, ]➌
- en: model <- get_model()
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model()
- en: fit(model, training_data, …)➍
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: fit(model, training_data, …)➍
- en: validation_score <- evaluate(model, validation_data, …)➎
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: validation_score <- evaluate(model, validation_data, …)➎
- en: …
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: model <- get_model()
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model()
- en: fit(model, data, …)➏
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: fit(model, data, …)➏
- en: test_score <- evaluate(model, test_data, …)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: test_score <- evaluate(model, test_data, …)
- en: ➊ **Assembling the validation set from a random sampling of the data is usually
    appropriate.**
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **从数据的随机抽样中组装验证集通常是合适的。**
- en: ➋ **Define the validation set.**
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **定义验证集。**
- en: ➌ **Define the training set.**
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **定义训练集。**
- en: ➍ **Train a model on the training data, and evaluate it on the validation data.**
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **在训练数据上训练模型，并在验证数据上评估。**
- en: ➎ **At this point you can tune your model, retrain it, evaluate it, tune it
    again.**
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **这时你可以调整你的模型，重新训练它，评估它，再次调整它。**
- en: ➏ **Once you've tuned your hyperparameters, it's common to train your final
    model from scratch on all non-test data available (the training_data and validation_data
    combined).**
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **一旦调整好超参数，通常会从所有非测试数据（即结合了训练数据和验证数据）重新训练最终模型。**
- en: NOTE In these examples, we assume data is a rank 2 tensor. Add commas to the
    [ call as needed for higher rank data. For example, data[idx, , ] if data is rank
    3, data[idx, , , ] for rank 4, and so on.
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意 在这些示例中，我们假设数据是一个二阶张量。根据需要为更高阶的数据添加逗号。例如，如果数据是三阶的，则是 data[idx, , ]，如果是四阶的，则是
    data[idx, , , ]，以此类推。
- en: 'This is the simplest evaluation protocol, and it suffers from one flaw: if
    little data is available, then your validation and test sets may contain too few
    samples to be statistically representative of the data at hand. This is easy to
    recognize: if different random shuffling rounds of the data before splitting end
    up yielding very different measures of model performance, then you’re having this
    issue. *K*-fold validation and iterated *K*-fold validation are two ways to address
    this, as discussed next.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的评估协议，但有一个缺点：如果可用的数据很少，那么您的验证和测试集可能包含的样本太少，无法代表手头的数据。这很容易识别：如果在分割之前对数据进行不同的随机洗牌会得到非常不同的模型性能度量，则会出现这个问题。
    *K* 折验证和迭代 *K* 折验证是解决此问题的两种方式，下面会讨论。
- en: '**K-FOLD VALIDATION**'
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**K 折验证**'
- en: With this approach, you split your data into K partitions of equal size. For
    each partition i, train a model on the remaining K - 1 partitions, and evaluate
    it on partition i. Your final score is then the averages of the *K* scores obtained.
    This method is helpful when the performance of your model shows significant variance
    based on your train-test split. Like holdout validation, this method doesn’t exempt
    you from using a distinct validation set for model calibration.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，将数据分成 K 个相等大小的分区。对于每个分区 i，训练一个模型，使用其余的 K - 1 个分区进行训练，并在分区 i 上评估它。然后，您的最终得分是获得的
    *K* 个得分的平均值。当您的模型的性能基于训练-测试分割表现出显著差异时，这种方法很有帮助。与留出验证类似，这种方法并不免除您使用不同的验证集进行模型校准。
- en: Schematically, *K*-fold cross-validation looks like [figure 5.13](#fig5-13).
    Listing 5.6 shows a simple implementation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 5.13（#fig5-13）可以看出，*K* 折交叉验证的示意图如下。列表 5.6 显示了一个简单的实现。
- en: '![Image](../images/f0144-01.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0144-01.jpg)'
- en: '**Figure 5.13 *K*-fold cross-validation with *K* = 3**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.13 *K* 折交叉验证，其中 *K* = 3**'
- en: '**Listing 5.6 *K*-fold cross-validation (note that labels are omitted for simplicity)**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 5.6 *K* 折交叉验证（为简单起见，标签被省略）**'
- en: k <- 3
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: k <- 3
- en: fold_id <- sample(rep(1:k, length.out = nrow(data)))
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: fold_id <- sample(rep(1:k, length.out = nrow(data)))
- en: validation_scores <- numeric()
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: validation_scores <- numeric()
- en: for (fold in seq_len(k)) {
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: for (fold in seq_len(k)) {
- en: validation_idx <- which(fold_id == fold)➊
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: validation_idx <- which(fold_id == fold)➊
- en: validation_data <- data[validation_idx, ]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data <- data[validation_idx, ]
- en: training_data <- data[-validation_idx, ]➋
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: training_data <- data[-validation_idx, ]➋
- en: model <- get_model()➌
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model()➌
- en: fit(model, training_data, …)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: fit(model, training_data, …)
- en: validation_score <- evaluate(model, validation_data, …)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: validation_score <- evaluate(model, validation_data, …)
- en: validation_scores[[fold]] <- validation_score
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: validation_scores[[fold]] <- validation_score
- en: '}'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: validation_score <- mean(validation_scores)➍
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: validation_score <- mean(validation_scores)➍
- en: model <- get_model()
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model()
- en: fit(model, data, …)➎
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: fit(model, data, …)➎
- en: test_score <- evaluate(model, test_data, …)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: test_score <- evaluate(model, test_data, …)
- en: ➊ **Select the validation-data partition.**
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **选择验证数据分区。**
- en: ➋ **Use the remainder of the data as training data.**
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将剩余的数据用作训练数据。**
- en: ➌ **Create a brand-new instance of the model (untrained).**
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **创建一个全新的模型实例（未训练）。**
- en: '➍ **Validation score: average of the validation scores of the K-folds**'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **验证分数：K 折验证的验证分数的平均值**
- en: ➎ **Train the final model on all non-test data available.**
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **在所有非测试数据上训练最终模型。**
- en: '**ITERATED K-FOLD VALIDATION WITH SHUFFLING**'
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**迭代 K 折验证与洗牌**'
- en: This one is for situations in which you have relatively little data available
    and you need to evaluate your model as precisely as possible. I’ve found it to
    be extremely helpful in Kaggle competitions. It consists of applying *K*-fold
    validation multiple times, shuffling the data every time before splitting it K
    ways. The final score is the average of the scores obtained at each run of *K*-fold
    validation. Note that you end up training and evaluating P * K models (where P
    is the number of iterations you use), which can be very expensive.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了在您可用的数据相对较少且需要尽可能准确地评估模型的情况下。我发现它在Kaggle竞赛中非常有帮助。它包括多次应用*K*折叠验证，在每次将数据分成K份之前都会对数据进行洗牌。最终得分是*K*折叠验证每次运行获得的得分的平均值。请注意，最终您将训练和评估P
    * K个模型（其中P是您使用的迭代次数），这可能非常昂贵。
- en: 5.2.2 Beating a common-sense baseline
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 超越常识基线
- en: Besides the different evaluation protocols you have available, one last thing
    you should know about is the use of common-sense baselines.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 除了您可用的不同评估协议之外，您还应该了解的一件事是使用常识基线。
- en: Training a deep learning model is a bit like pressing a button that launches
    a rocket in a parallel world. You can’t hear it or see it. You can’t observe the
    manifold learning process—it’s happening in a space with thousands of dimensions,
    and even if you projected it to 3D, you couldn’t interpret it. The only feedback
    you have is your validation metrics—like an altitude meter on your invisible rocket.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型有点像按下启动平行世界火箭的按钮。你听不到也看不到它。你无法观察到流形学习过程—它发生在一个具有成千上万维度的空间中，即使你将其投影到3D中，你也无法解释它。你唯一的反馈是你的验证指标—就像你看不见的火箭上的高度计。
- en: 'It’s particularly important to be able to tell whether you’re getting off the
    ground at all. What was the altitude you started at? Your model seems to have
    an accuracy of 15%—is that any good? Before you start working with a dataset,
    you should always pick a trivial baseline that you’ll try to beat. If you cross
    that threshold, you’ll know you’re doing something right: your model is actually
    using the information in the input data to make predictions that generalize, and
    you can keep going. This baseline could be the performance of a random classifier
    or the performance of the simplest non– machine learning technique you can imagine.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 能够判断自己是否有起色尤为重要。你起飞的高度是多少？你的模型似乎有15%的准确率—这算好吗？在你开始处理数据集之前，你应该始终选择一个微不足道的基准，你会尝试超越它。如果你超过了这个阈值，你就知道你做对了：你的模型实际上正在使用输入数据中的信息来做出概括性的预测，你可以继续前进。这个基准可以是随机分类器的性能，也可以是你能想象到的最简单的非机器学习技术的性能。
- en: For instance, in the MNIST digit-classification example, a simple baseline would
    be a validation accuracy greater than 0.1 (random classifier); in the IMDB example,
    it would be a validation accuracy greater than 0.5\. In the Reuters example, it
    would be around 0.18–0.19, due to class imbalance. If you have a binary classification
    problem where 90% of samples belong to class A and 10% belong to class B, then
    a classifier that always predicts A already achieves 0.9 in validation accuracy,
    and you’ll need to do better than that.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在MNIST数字分类示例中，一个简单的基准是验证准确度大于0.1（随机分类器）；在IMDB示例中，它将是验证准确度大于0.5；在Reuters示例中，由于类别不平衡，它将在0.18–0.19左右。如果您有一个二分类问题，其中90%的样本属于A类，而10%属于B类，那么始终预测A的分类器已经达到了0.9的验证准确度，您需要做得比这更好。
- en: Having a common-sense baseline you can refer to is essential when you’re getting
    started on a problem no one has solved before. If you can’t beat a trivial solution,
    your model is worthless—perhaps you’re using the wrong model, or perhaps the problem
    you’re tackling can’t even be approached with machine learning in the first place.
    Time to go back to the drawing board.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在解决以前没有人解决过的问题时，拥有一个常识基准是至关重要的。如果您无法击败一个微不足道的解决方案，那么您的模型毫无价值—也许您正在使用错误的模型，或者您正在处理的问题根本不能用机器学习来解决。是时候回到起点了。
- en: 5.2.3 Things to keep in mind about model evaluation
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 模型评估要记住的事项
- en: 'Keep an eye out for the following when you’re choosing an evaluation protocol:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择评估协议时，请注意以下事项：
- en: '*Data representativeness*—You want both your training set and test set to be
    representative of the data at hand. For instance, if you’re trying to classify
    images of digits, and you’re starting from an array of samples where the samples
    are ordered by their class, taking the first 80% of the array as your training
    set and the remaining 20% as your test set will result in your training set containing
    only classes 0–7, whereas your test set will contain only classes 8–9\. This seems
    like a ridiculous mistake, but it’s surprisingly common. For this reason, you
    usually should *randomly shuffle* your data before splitting it into training
    and test sets.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据的代表性* —— 你希望你的训练集和测试集都能代表手头的数据。例如，如果你试图对数字图像进行分类，并且你从一个按类别排序的样本数组开始，那么将数组的前80%作为你的训练集，剩下的20%作为测试集将导致你的训练集只包含类别0-7，而你的测试集将只包含类别8-9。这似乎是一个荒谬的错误，但这是令人惊讶地常见。因此，你通常应该在将数据拆分为训练集和测试集之前对数据进行*随机洗牌*。'
- en: '*The arrow of time*—If you’re trying to predict the future given the past (e.g.,
    tomorrow’s weather, stock movements, and so on), you should not randomly shuffle
    your data before splitting it, because doing so will create a *temporal leak*:
    your model will effectively be trained on data from the future. In such situations,
    you should always make sure all data in your test set is *posterior* to the data
    in the training set.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间箭头* —— 如果你试图根据过去来预测未来（例如，明天的天气，股票走势等），在将数据拆分之前不应该随机洗牌你的数据，因为这样做会产生*时间泄漏*：你的模型实际上是在未来的数据上训练的。在这种情况下，你应该始终确保测试集中的所有数据都*后置于*训练集中的数据。'
- en: '*Redundancy in your data*—If some data points in your data appear twice (fairly
    common with real-world data), then shuffling the data and splitting it into a
    training set and a validation set will result in redundancy between the training
    and validation sets. In effect, you’ll be testing on part of your training data,
    which is the worst thing you can do! Make sure your training set and validation
    set are disjoint.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据中的冗余* —— 如果你的数据中有一些数据点出现了两次（在真实世界的数据中相当常见），那么对数据进行洗牌并将其分为训练集和验证集将导致训练集和验证集之间存在冗余。实际上，你将在部分训练数据上进行测试，这是你可以做的最糟糕的事情！确保你的训练集和验证集是不相交的。'
- en: Having a reliable way to evaluate the performance of your model is how you’ll
    be able to monitor the tension at the heart of machine learning—between optimization
    and generalization, underfitting and overfitting.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个可靠的评估模型性能的方法是你能够监控机器学习中核心的紧张关系 —— 在优化和泛化之间，在欠拟合和过拟合之间。
- en: 5.3 Improving model fit
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 改善模型拟合
- en: To achieve the perfect fit, you must first overfit. Because you don’t know in
    advance where the boundary lies, you must cross it to find it. Thus, your initial
    goal as you start working on a problem is to achieve a model that shows some generalization
    power and that is able to overfit. Once you have such a model, you’ll focus on
    refining generalization by fighting overfitting.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到完美的拟合，你必须首先过拟合。因为你事先不知道边界在哪里，所以必须跨越它才能找到它。因此，当你开始解决一个问题时，你的初始目标是实现一个表现出一定泛化能力并且能够过拟合的模型。一旦你有了这样一个模型，你将专注于通过抗击过拟合来精炼泛化能力。
- en: 'There are three common problems you’ll encounter at this stage:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段你会遇到三个常见的问题：
- en: 'Training doesn’t get started: your training loss doesn’t go down over time.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练没有开始：你的训练损失随时间没有下降。
- en: 'Training gets started just fine, but your model doesn’t meaningfully generalize:
    you can’t beat the common-sense baseline you set.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练开始得很顺利，但你的模型没有有意义的泛化：你无法打败你设定的常识基线。
- en: Training and validation loss both go down over time, and you can beat your baseline,
    but you don’t seem to be able to overfit, which indicates you’re still underfitting
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失和验证损失随时间都在下降，你可以打败基线，但似乎无法过拟合，这表明你仍然是欠拟合的。
- en: 'Let’s see how you can address these issues to achieve the first big milestone
    of a machine learning project: getting a model that has some generalization power
    (it can beat a trivial baseline) and that is able to overfit.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看你如何解决这些问题，以实现机器学习项目的第一个重要里程碑：获得具有一定泛化能力（能够击败一个琐碎的基线）并且能够过拟合的模型。
- en: 5.3.1 Tuning key gradient descent parameters
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 调整关键的梯度下降参数
- en: 'Sometimes training doesn’t get started, or it stalls too early. Your loss is
    stuck. This is *always* something you can overcome: remember that you can fit
    a model to random data. Even if nothing about your problem makes sense, you should
    *still* be able to train something, if only by memorizing the training data.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候训练无法开始，或者开始后很快停止。损失值停滞不变。这总是可以克服的：记住，你可以使用随机数据来拟合模型。即使问题似乎毫无意义，你仍然应该可以训练出一些东西，即使只是记忆训练数据。
- en: 'When this happens, it’s always a problem with the configuration of the gradient
    descent process: your choice of optimizer, the distribution of initial values
    in the weights of your model, your learning rate, or your batch size. All these
    parameters are interdependent, and as such, it is usually sufficient to tune the
    learning rate and the batch size while keeping the rest of the parameters constant.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当此情况发生时，通常问题出在梯度下降过程的配置上：优化器的选择、模型权重的初始值分布、学习率或批量大小。所有这些参数都是相互依赖的，因此通常只需调整学习率和批量大小，同时保持其他参数不变即可。
- en: 'Let’s look at a concrete example: let’s train the MNIST model from chapter
    2 with an inappropriately large learning rate of value 1.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子：使用值为1的不恰当大学习率训练第2章的MNIST模型。
- en: '**Listing 5.7 Training an MNIST model with an incorrectly high learning rate**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.7 使用过高的学习率训练MNIST模型**'
- en: c(c(train_images, train_labels), .) %<-% dataset_mnist()
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: c(c(train_images, train_labels), .) %<-% dataset_mnist()
- en: train_images <- array_reshape(train_images / 255,
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: train_images <- array_reshape(train_images / 255,
- en: c(60000, 28 * 28))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: c(60000, 28 * 28))
- en: model <- keras_model_sequential() %>%
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(units = 512, activation = "relu") %>%
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 512, activation = "relu") %>%
- en: layer_dense(units = 10, activation = "softmax")
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 10, activation = "softmax")
- en: model %>% compile(optimizer = optimizer_rmsprop(1),
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = optimizer_rmsprop(1),
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: history <- model %>% fit(train_images, train_labels,
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: history <- model %>% fit(train_images, train_labels,
- en: epochs = 10, batch_size = 128,
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10, batch_size = 128,
- en: validation_split = 0.2)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: validation_split = 0.2)
- en: plot(history)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history)
- en: '![Image](../images/f0148-01.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0148-01.jpg)'
- en: The model quickly reaches a training and validation accuracy in the 20%–30%
    range, but cannot get past that. Let’s try to lower the learning rate to a more
    reasonable value of 1e-2.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 模型很快达到了20% - 30%的训练和验证准确率，但无法超过这个范围。让我们尝试将学习率降低到1e-2这样更合理的值。
- en: '**Listing 5.8 The same model with a more appropriate learning rate**'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.8 具有更合适学习率的相同模型**'
- en: model <- keras_model_sequential() %>%
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(units = 512, activation = "relu") %>%
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 512, activation = "relu") %>%
- en: layer_dense(units = 10, activation = "softmax")
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(units = 10, activation = "softmax")
- en: model %>% compile(optimizer = optimizer_rmsprop(1e-2),
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = optimizer_rmsprop(1e-2),
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model %>%
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: fit(train_images, train_labels,
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_images, train_labels,
- en: epochs = 10, batch_size = 128,
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10, batch_size = 128,
- en: validation_split = 0.2) ->
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: validation_split = 0.2) ->
- en: history
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: history
- en: plot(history)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history)
- en: '![Image](../images/f0149-01.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0149-01.jpg)'
- en: The model is now able to train.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型可以进行训练了。
- en: 'If you find yourself in a similar situation, try the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到类似的情况，请尝试以下操作：
- en: '*Lowering or increasing the learning rate*—A learning rate that is too high
    may lead to updates that vastly overshoot a proper fit, like in the preceding
    example, and a learning rate that is too low may make training so slow that it
    appears to stall.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降低或增加学习率* - 过高的学习率可能导致更新远超过合适的拟合，就像前面的例子一样，学习率太低可能会导致训练过程非常缓慢。'
- en: '*Increasing the batch size*—A batch with more samples will lead to gradients
    that are more informative and less noisy (lower variance).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*增加批量大小* - 具有更多样本的批次会产生更有信息量且噪音较小（方差较低）的梯度。'
- en: You will, eventually, find a configuration that gets training started.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你将找到一个可以进行训练的配置。
- en: 5.3.2 Leveraging better architecture priors
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 利用更好的架构先验
- en: 'You have a model that fits, but for some reason your validation metrics aren’t
    improving at all. They remain no better than what a random classifier would achieve:
    your model trains but doesn’t generalize. What’s going on?'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个适合的模型，但是由于某种原因你的验证指标一直没有改善。它们的表现不会比随机分类器好：模型可以训练但无法推广。发生了什么？
- en: This is perhaps the worst machine learning situation you can find yourself in.
    It indicates that *something is fundamentally wrong with your approach*, and it
    may not be easy to tell what. Here are some tips.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这也许是你可能会遇到的最糟糕的机器学习情况。这表明*你的方法存在根本性问题*，而且可能不容易找出问题所在。以下是一些提示。
- en: 'First, it may be that the input data you’re using simply doesn’t contain sufficient
    information to predict your targets: the problem as formulated is not solvable.
    This is what happened earlier when we tried to fit an MNIST model where the labels
    were shuffled: the model would train just fine, but validation accuracy would
    stay stuck at 10%, because it was plainly impossible to generalize with such a
    dataset.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可能是你正在使用的输入数据简单地不包含足够的信息来预测你的目标：所描述的问题是不可解的。这就是早些时候当我们试图拟合一个MNIST模型而标签被洗牌时发生的情况：模型可以训练得很好，但验证准确率会停留在10%，因为使用这样的数据集明显是无法泛化的。
- en: 'It may also be that the kind of model you’re using is not suited for the problem
    at hand. For instance, in chapter 10, you’ll see an example of a time-series prediction
    problem where a densely connected architecture isn’t able to beat a trivial baseline,
    whereas a more appropriate recurrent architecture does manage to generalize well.
    Using a model that makes the right assumptions about the problem is essential
    to achieve generalization: you should leverage the right architecture priors.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能是你正在使用的模型类型不适合手头的问题。例如，在第10章中，你将看到一个时间序列预测问题的例子，其中一个密集连接的架构无法击败一个微不足道的基线，而一个更合适的循环架构确实成功泛化。使用对问题做出正确假设的模型对于实现泛化至关重要：你应该利用正确的架构先验。
- en: In the following chapters, you’ll learn about the best architectures to use
    for a variety of data modalities—images, text, time series, and so on. In general,
    you should always make sure to read up on architecture best practices for the
    kind of task you’re attacking—chances are you’re not the first person to attempt
    it.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习如何针对各种数据模态（图像、文本、时间序列等）选择最佳的架构。通常情况下，你应该确保阅读关于你攻击的任务类型的架构最佳实践——很有可能你不是第一个尝试这个任务的人。
- en: 5.3.3 Increasing model capacity
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 增加模型容量
- en: 'If you manage to get to a model that fits, where validation metrics are going
    down, and that seems to achieve at least some level of generalization power, congratulations:
    you’re almost there. Next, you need to get your model to start overfitting. Consider
    the following small model—a simple logistic regression—trained on MNIST pixels.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你设法得到一个拟合的模型，其中验证指标下降，并且似乎至少具有一定程度的泛化能力，恭喜你：你几乎成功了。接下来，你需要让你的模型开始过拟合。考虑下面这个小模型——一个简单的逻辑回归——在MNIST像素上训练。
- en: '**Listing 5.9 A simple logistic regression on MNIST**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 5.9 在MNIST上的简单逻辑回归**'
- en: model <- keras_model_sequential() %>%
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: history_small_model <- model %>%
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: history_small_model <- model %>%
- en: fit(train_images, train_labels,
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_images, train_labels,
- en: epochs = 20,
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20,
- en: batch_size = 128,
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128,
- en: validation_split = 0.2)
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: validation_split = 0.2)
- en: plot(history_small_model$metrics$val_loss, type = 'o',
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history_small_model$metrics$val_loss, type = 'o',
- en: main = "Effect of Insufficient Model Capacity on Validation Loss",
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: main = "模型容量不足对验证损失的影响",
- en: xlab = "Epochs", ylab = "Validation Loss")
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: xlab = "Epochs", ylab = "Validation Loss")
- en: You get loss curves that look like [figure 5.14](#fig5-14).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到的损失曲线看起来像[图5.14](#fig5-14)。
- en: '![Image](../images/f0151-01.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0151-01.jpg)'
- en: '**Figure 5.14 Effect of insufficient model capacity on validation loss**'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.14 模型容量不足对验证损失的影响**'
- en: Validation metrics seem to stall, or to improve very slowly, instead of peaking
    and reversing course. The validation loss goes to 0.26 and just stays there. You
    can fit, but you can’t clearly overfit, even after many iterations over the training
    data. You’re likely to encounter similar curves often in your career.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 验证指标似乎停滞不前，或者改善得非常缓慢，而不是达到峰值然后反转方向。验证损失达到0.26后就停在那里。你可以拟合，但明显无法过拟合，即使在训练数据上进行了多次迭代后也是如此。在你的职业生涯中，你可能经常遇到类似的曲线。
- en: 'Remember that it should always be possible to overfit. Much like the problem
    where the training loss doesn’t go down, this is an issue that can always be solved.
    If you can’t seem to be able to overfit, it’s likely a problem with the *representational
    power* of your model: you’re going to need a bigger model, one with more *capacity*,
    that is, one able to store more information. You can increase representational
    power by adding more layers, using bigger layers (layers with more parameters),
    or using kinds of layers that are more appropriate for the problem at hand (better
    architecture priors).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，总是应该能够过度拟合。就像训练损失不下降的问题一样，这是一个总是可以解决的问题。如果你似乎无法过度拟合，那很可能是你的模型的*表示能力*有问题：你需要一个更大的模型，一个能够存储更多信息的模型。你可以通过添加更多层，使用更大的层（参数更多的层），或者使用更适合手头问题的层类型（更好的架构先验）来增加表示能力。
- en: 'Let’s try training a bigger model, one with two intermediate layers with 96
    units each:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试训练一个更大的模型，一个有两个中间层，每个层有 96 个单元：
- en: model <- keras_model_sequential() %>%
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(96, activation = "relu") %>%
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(96, activation = "relu") %>%
- en: layer_dense(96, activation = "relu") %>%
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(96, activation = "relu") %>%
- en: layer_dense(10, activation = "softmax")
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(10, activation = "softmax")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: history_large_model <- model %>%
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: history_large_model <- model %>%
- en: fit(train_images, train_labels,
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_images, train_labels,
- en: epochs = 20,
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20,
- en: batch_size = 128,
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128,
- en: validation_split = 0.2)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: validation_split = 0.2)
- en: 'The validation curve now looks exactly like it should: the model fits fast
    and starts overfitting after eight epochs (see [figure 5.15](#fig5-15)):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，验证曲线看起来正是应该的样子：模型拟合迅速，并在八个时期后开始过度拟合（见[图 5.15](#fig5-15)）：
- en: plot(history_large_model$metrics$val_loss, type = 'o',
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history_large_model$metrics$val_loss, type = 'o',
- en: main = "Validation Loss for a Model with Appropriate Capacity",
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 主要 = "适当容量模型的验证损失",
- en: xlab = "Epochs", ylab = "Validation Loss")
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: xlab = "时期", ylab = "验证损失")
- en: '![Image](../images/f0152-01.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0152-01.jpg)'
- en: '**Figure 5.15 Validation loss for a model with appropriate capacity**'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.15 适当容量模型的验证损失**'
- en: 5.4 Improving generalization
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 改善泛化
- en: Once your model has shown itself to have some generalization power and to be
    able to overfit, it’s time to switch your focus to maximizing generalization.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型已经表现出一些泛化能力并且能够过度拟合，就是时候把重点转移到最大化泛化上了。
- en: 5.4.1 Dataset curation
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 数据集策划
- en: You’ve already learned that generalization in deep learning originates from
    the latent structure of your data. If your data makes it possible to smoothly
    interpolate between samples, you will be able to train a deep learning model that
    generalizes. If your problem is overly noisy or fundamentally discrete, like,
    say, list sorting, deep learning will not help you. Deep learning is curve fitting,
    not magic.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学到了，深度学习中的泛化源于你的数据的潜在结构。如果你的数据使得在样本之间平滑插值成为可能，你就能训练一个泛化的深度学习模型。如果你的问题过于嘈杂或者本质上是离散的，比如说，列表排序，深度学习将无法帮助你。深度学习是曲线拟合，不是魔法。
- en: As such, it is essential that you make sure that you’re working with an appropriate
    dataset. Spending more effort and money on data collection almost always yields
    a much greater return on investment than spending the same on developing a better
    model.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确保你正在使用适当的数据集至关重要。在数据收集上花费更多的精力和金钱几乎总是比在开发更好的模型上花费相同的时间和金钱收益更大。
- en: Make sure you have enough data. Remember that you need a *dense sampling* of
    the input-cross-output space. More data will yield a better model. Sometimes,
    problems that seem impossible at first become solvable with a larger dataset.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你有足够的数据。记住，你需要对输入-输出空间进行*密集采样*。更多的数据将产生更好的模型。有时，一开始看似不可能的问题在有了更大的数据集后变得可解。
- en: Minimize labeling errors—visualize your inputs to check for anomalies, and proofread
    your labels
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化标记错误 —— 可视化你的输入以检查异常，并校对你的标签
- en: Clean your data and deal with missing values (we’ll cover this in the next chapter).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理你的数据并处理缺失值（我们将在下一章中涵盖这个问题）。
- en: If you have many features and you aren’t sure which ones are actually useful,
    do feature selection.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有很多特征，并且不确定哪些是真正有用的，请进行特征选择。
- en: A particularly important way to improve the generalization potential of your
    data is feature engineering. For most machine learning problems, feature engineering
    is a key ingredient for success. Let’s take a look.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 改进数据的泛化能力的一个特别重要的方式是特征工程。对于大多数机器学习问题，特征工程是成功的关键因素。让我们来看一下。
- en: 5.4.2 Feature engineering
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 特征工程
- en: '*Feature engineering* is the process of using your own knowledge about the
    data and about the machine learning algorithm at hand (in this case, a neural
    network) to make the algorithm work better by applying hardcoded (nonlearned)
    transformations to the data before it goes into the model. In many cases, it isn’t
    reasonable to expect a machine learning model to be able to learn from completely
    arbitrary data. The data needs to be presented to the model in a way that will
    make the model’s job easier.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征工程*是利用你对数据和手头的机器学习算法（在本例中是神经网络）的了解，通过在数据输入模型之前应用硬编码（非学习）的转换来使算法更好地工作的过程。在许多情况下，期望机器学习模型能够从完全任意的数据中学习是不合理的。数据需要以一种使模型工作更轻松的方式呈现给模型。'
- en: Let’s look at an intuitive example. Suppose you’re trying to develop a model
    that can take as input an image of a clock and can output the time of day (see
    [figure 5.16](#fig5-16)).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个直观的例子。假设你正在尝试开发一个模型，该模型可以将时钟的图像作为输入，并能够输出当天的时间（见[图5.16](#fig5-16)）。
- en: '![Image](../images/f0153-01.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0153-01.jpg)'
- en: '**Figure 5.16 Feature engineering for reading the time on a clock**'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.16 读取时钟时间的特征工程**'
- en: If you choose to use the raw pixels of the image as input data, you have a difficult
    machine learning problem on your hands. You’ll need a convolutional neural network
    to solve it, and you’ll have to expend quite a bit of computational resources
    to train the network.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择使用图像的原始像素作为输入数据，你将面临一个困难的机器学习问题。你将需要一个卷积神经网络来解决它，并且你将不得不耗费相当多的计算资源来训练网络。
- en: 'But if you already understand the problem at a high level (you understand how
    humans read time on a clock face), you can come up with much better input features
    for a machine learning algorithm: for instance, it’s easy to write a five-line
    R script to follow the black pixels of the clock hands and output the (x, y) coordinates
    of the tip of each hand. Then a simple machine learning algorithm can learn to
    associate these coordinates with the appropriate time of day.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你已经在高层次上理解了问题（你了解人类如何在时钟表盘上读取时间），那么你可以为机器学习算法提供更好的输入特征：例如，编写一个五行的 R 脚本来跟踪时钟指针的黑色像素，并输出每个指针尖的（x，y）坐标。然后，一个简单的机器学习算法可以学会将这些坐标与适当的当天时间相关联。
- en: 'You can go even further: do a coordinate change and express the (x, y) coordinates
    as polar coordinates with regard to the center of the image. Your input will become
    the angle theta of each clock hand. At this point, your features are making the
    problem so easy that no machine learning is required; a simple rounding operation
    and dictionary lookup are enough to recover the approximate time of day.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以走得更远：进行坐标变换，并将（x，y）坐标表示为相对于图像中心的极坐标。你的输入将成为每个时钟指针的角度θ。在这一点上，你的特征使得问题变得如此简单，以至于不需要任何机器学习；一个简单的取整操作和字典查找就足以恢复大致的当天时间。
- en: 'That’s the essence of feature engineering: making a problem easier by expressing
    it in a simpler way. Make the latent manifold smoother, simpler, and better organized.
    Doing so usually requires understanding the problem in depth.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是特征工程的本质：通过以更简单的方式表达问题来简化问题。使潜在的流形更加平滑、简单和组织良好。通常，这需要深入理解问题。
- en: Before deep learning, feature engineering used to be the most important part
    of the machine learning workflow, because classical shallow algorithms didn’t
    have hypothesis spaces rich enough to learn useful features by themselves. The
    way you presented the data to the algorithm was absolutely critical to its success.
    For instance, before convolutional neural networks became successful on the MNIST
    digit-classification problem, solutions were typically based on hardcoded features
    such as the number of loops in a digit image, the height of each digit in an image,
    a histogram of pixel values, and so on.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习出现之前，特征工程曾经是机器学习工作流程中最重要的部分，因为经典的浅层算法没有足够丰富的假设空间来自行学习有用的特征。你向算法呈现数据的方式对其成功至关重要。例如，在卷积神经网络在MNIST数字分类问题上取得成功之前，解决方案通常基于硬编码特征，如数字图像中的循环次数、图像中每个数字的高度、像素值的直方图等。
- en: 'Fortunately, modern deep learning removes the need for most feature engineering,
    because neural networks are capable of automatically extracting useful features
    from raw data. Does this mean you don’t have to worry about feature engineering
    as long as you’re using deep neural networks? No, for the following two reasons:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，现代深度学习消除了大部分特征工程的需求，因为神经网络能够从原始数据中自动提取有用的特征。这是否意味着只要使用深度神经网络，你就不必担心特征工程了？不，有以下两个原因：
- en: Good features still allow you to solve problems more elegantly while using fewer
    resources. For instance, it would be ridiculous to solve the problem of reading
    a clock face using a convolutional neural network.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好的特征仍然可以让你更优雅地解决问题，同时使用更少的资源。例如，使用卷积神经网络解决读取钟面的问题就是荒谬的。
- en: Good features let you solve a problem with far less data. The ability of deep
    learning models to learn features on their own relies on having lots of training
    data available; if you have only a few samples, the information value in their
    features becomes critical
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好的特征让你能够用更少的数据解决问题。深度学习模型自行学习特征的能力依赖于有大量的训练数据可用；如果你只有少量样本，那么样本特征中的信息价值就变得至关重要了。
- en: 5.4.3 Using early stopping
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 使用提前停止
- en: 'In deep learning, we always use models that are vastly overparameterized: they
    have way more degrees of freedom than the minimum necessary to fit to the latent
    manifold of the data. This overparameterization is not an issue, because *you
    never fully fit a deep learning model*. Such a fit wouldn’t generalize at all.
    You will always interrupt training long before you’ve reached the minimum possible
    training loss.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们总是使用大大超参数化的模型：它们的自由度比拟合数据的潜在流形所需的最小自由度要多得多。这种超参数化不是问题，因为*你永远不会完全拟合一个深度学习模型*。这样的拟合根本无法泛化。在你达到最小可能的训练损失之前，你总是会在训练之前中断。
- en: Finding the exact point during training where you’ve reached the most generalizable
    fit—the exact boundary between an underfit curve and an overfit curve—is one of
    the most effective things you can do to improve generalization.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 找到在训练过程中达到最可泛化拟合的确切点——欠拟合曲线和过拟合曲线之间的确切边界——是你可以做的最有效的事情之一，以提高泛化能力。
- en: In the examples in the previous chapter, we would start by training our models
    for longer than needed to figure out the number of epochs that yielded the best
    validation metrics, and then we would retrain a new model for exactly that number
    of epochs. This is pretty standard, but it requires you to do redundant work,
    which can sometimes be expensive. Naturally, you could just save your model at
    the end of each epoch, and once you’ve found the best epoch, reuse the closest
    saved model you have. In Keras, it’s typical to do this with callback_early_stopping,
    which will interrupt training as soon as validation metrics have stopped improving,
    while remembering the best known model state. You’ll learn to use callbacks in
    chapter 7.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的示例中，我们会先训练我们的模型比需要的时间更长，以找出产生最佳验证指标的周期数，然后我们会为确切的周期数重新训练一个新模型。这是相当标准的做法，但它需要你做冗余工作，有时可能是昂贵的。自然地，你可以在每个周期结束时保存你的模型，一旦找到最佳周期，就重新使用你最接近的保存模型。在Keras中，使用callback_early_stopping这个回调函数是很典型的做法，它会在验证指标停止改善时立即中断训练，同时记住已知的最佳模型状态。你将在第7章学习如何使用回调函数。
- en: 5.4.4 Regularizing your model
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 正则化你的模型
- en: '*Regularization techniques* are a set of best practices that actively impede
    the model’s ability to fit perfectly to the training data, with the goal of making
    the model perform better during validation. This is called *regularizing* the
    model, because it tends to make the model simpler, more “regular,” and its curve
    smoother, more “generic”; thus it is less specific to the training set and better
    able to generalize by more closely approximating the latent manifold of the data.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化技术*是一组最佳实践，积极阻碍模型完美拟合训练数据的能力，其目标是使模型在验证期间表现更好。这被称为*正则化*模型，因为它倾向于使模型更简单、更“规则”，其曲线更平滑、更“通用”；因此，它不太具体于训练集，并且更能够通过更紧密地逼近数据的潜在流形进行泛化。'
- en: Keep in mind that regularizing a model is a process that should always be guided
    by an accurate evaluation procedure. You will achieve generalization only if you
    can measure it.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对模型进行正则化是一个应该始终由准确的评估程序指导的过程。只有当您能够测量到时，您才能实现泛化。
- en: Let’s review some of the most common regularization techniques and apply them
    in practice to improve the movie-classification model from chapter 4.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一些最常见的正则化技术，并在实践中应用它们来改进第 4 章的电影分类模型。
- en: '**REDUCING THE NETWORK’S SIZE**'
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**减小网络的规模**'
- en: 'You’ve already learned that a model that is too small will not overfit. The
    simplest way to mitigate overfitting is to reduce the size of the model (the number
    of learnable parameters in the model, determined by the number of layers and the
    number of units per layer). If the model has limited memorization resources, it
    won’t be able to simply memorize its training data; thus, to minimize its loss,
    it will have to resort to learning compressed representations that have predictive
    power regarding the targets— precisely the type of representations we’re interested
    in. At the same time, keep in mind that you should use models that have enough
    parameters that they don’t under-fit: your model shouldn’t be starved for memorization
    resources. There is a compromise to be found between *too much capacity* and *not
    enough capacity*.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经学会了，模型太小将不会过度拟合。缓解过拟合的最简单方法是减小模型的大小（模型中可学习参数的数量，由层数和每层单元的数量确定）。如果模型具有有限的记忆资源，它将无法简单地记住其训练数据；因此，为了最小化其损失，它将不得不诉诸于学习具有关于目标的预测能力的压缩表示——这恰好是我们感兴趣的表示类型。与此同时，请记住，您应该使用具有足够参数的模型，使其不会欠拟合：您的模型不应该缺乏记忆资源。需要在*过多容量*和*不足容量*之间找到一个折衷。
- en: Unfortunately, there is no magical formula to determine the right number of
    layers or the right size for each layer. You must evaluate an array of different
    architectures (on your validation set, not on your test set, of course) to find
    the correct model size for your data. The general workflow for finding an appropriate
    model size is to start with relatively few layers and parameters and increase
    the size of the layers or add new layers until you see diminishing returns with
    regard to validation loss.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，并没有一个神奇的公式来确定正确的层数或每个层的正确大小。您必须评估一系列不同的架构（当然是在验证集上，而不是在测试集上）以找到适合您数据的正确模型大小。找到合适模型大小的一般工作流程是从相对较少的层和参数开始，并增加层的大小或添加新层，直到您看到验证损失的减小收益。
- en: Let’s try this on the movie-review classification model. The following listing
    shows our original model.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在电影评论分类模型上尝试一下这个。下面的列表显示了我们的原始模型。
- en: '**Listing 5.10 Original model**'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 5.10 原始模型**'
- en: c(c(train_data, train_labels), .) %<-% dataset_imdb(num_words = 10000)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: c(c(train_data, train_labels), .) %<-% dataset_imdb(num_words = 10000)
- en: vectorize_sequences <- function(sequences, dimension = 10000) {
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: vectorize_sequences <- function(sequences, dimension = 10000) {
- en: results <- matrix(0, nrow = length(sequences), ncol = dimension)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: results <- matrix(0, nrow = length(sequences), ncol = dimension)
- en: for(i in seq_along(sequences))
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: for(i in seq_along(sequences))
- en: results[i, sequences[[i]]] <- 1
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: results[i, sequences[[i]]] <- 1
- en: results
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: results
- en: '}'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: train_data <- vectorize_sequences(train_data)
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: train_data <- vectorize_sequences(train_data)
- en: model <- keras_model_sequential() %>%
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: history_original <- model %>%
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: history_original <- model %>%
- en: fit(train_data, train_labels,
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_data, train_labels,
- en: epochs = 20, batch_size = 512, validation_split = 0.4)
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20, batch_size = 512, validation_split = 0.4)
- en: Now let’s try to replace it with this smaller model.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试用这个较小的模型来替换它。
- en: '**Listing 5.11 Version of the model with lower capacity**'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 5.11 容量较低的模型版本**'
- en: model <- keras_model_sequential() %>%
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(4, activation = "relu") %>%
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(4, activation = "relu") %>%
- en: layer_dense(4, activation = "relu") %>%
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(4, activation = "relu") %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "准确率")
- en: history_smaller_model <- model %>%
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: history_smaller_model <- model %>%
- en: fit(train_data, train_labels,
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_data, train_labels,
- en: epochs = 20, batch_size = 512, validation_split = 0.4)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20, batch_size = 512, validation_split = 0.4)
- en: 'Let’s generate a plot ([figure 5.17](#fig5-17)) to compare the validation losses
    of the original model and the smaller model:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一个图表（[图 5.17](#fig5-17)）来比较原始模型和较小模型的验证损失：
- en: plot(
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图(
- en: NULL,➊
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: NULL,➊
- en: main = "Original Model vs. Smaller Model on IMDB Review Classification",
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 主题 = "IMDB 评论分类的原始模型与较小模型比较",
- en: xlab = "Epochs",
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: xlab = "周期",
- en: xlim = c(1, history_original$params$epochs),
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: xlim = c(1, history_original$params$epochs),
- en: ylab = "Validation Loss",
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ylab = "验证损失",
- en: ylim = extendrange(history_original$metrics$val_loss),
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ylim = extendrange(history_original$metrics$val_loss),
- en: panel.first = abline(v = 1:history_original$params$epochs,➋
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: panel.first = abline(v = 1:history_original$params$epochs,➋
- en: lty = "dotted", col = "lightgrey")
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: lty = "dotted", col = "lightgrey")
- en: )
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: lines(history_original  $metrics$val_loss, lty = 2)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_original  $metrics$val_loss, lty = 2)
- en: lines(history_smaller_model$metrics$val_loss, lty = 1)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_smaller_model$metrics$val_loss, lty = 1)
- en: legend("topleft", lty = 2:1,
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: legend("topleft", lty = 2:1,
- en: legend = c("Validation loss of original model",
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: legend = c("原始模型的验证损失",
- en: '"Validation loss of smaller model"))'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '"验证损失较小模型"))'
- en: ➊ **NULL tells plot() to set up the plot region but not draw any data yet.**
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **NULL 告诉 plot() 设置绘图区域但不绘制任何数据。**
- en: ➋ **Draw grid lines.**
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **绘制网格线。**
- en: '![Image](../images/f0157-01.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0157-01.jpg)'
- en: '**Figure 5.17 Original model vs. smaller model on IMDB review classification**'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.17 IMDB 评论分类的原始模型与较小模型**'
- en: As you can see, the smaller model starts overfitting later than the reference
    model (after six epochs rather than four), and its performance degrades more slowly
    once it starts overfitting.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，较小的模型开始过拟合比参考模型晚（在六个周期后而不是四个周期后），一旦开始过拟合，其性能下降得更慢。
- en: Now let’s add to our benchmark a model that has much more capacity—far more
    than the problem warrants. Although it is standard to work with models that are
    significantly overparameterized for what they’re trying to learn, there can definitely
    be such a thing as *too much* memorization capacity. You’ll know your model is
    too large if it starts overfitting right away and if its validation loss curve
    looks choppy with high variance (although choppy validation metrics could also
    be a symptom of using an unreliable validation process, such as a validation split
    that’s too small).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们添加一个具有更大容量的基准模型——远远超出了问题所需的容量。尽管标准工作于对于他们要学习的内容明显过度参数化的模型，但确实存在过多记忆能力的情况。如果您的模型立即开始过拟合，并且其验证损失曲线看起来波动大（尽管波动的验证指标也可能是使用不可靠的验证过程的症状，例如验证拆分太小），那么您将知道您的模型太大。
- en: '**Listing 5.12 Version of the model with higher capacity**'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 5.12 容量更高的模型版本**'
- en: model <- keras_model_sequential() %>%
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(512, activation = "relu") %>%
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(512, activation = "relu") %>%
- en: layer_dense(512, activation = "relu") %>%
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(512, activation = "relu") %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "准确率")
- en: history_larger_model <- model %>%
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: history_larger_model <- model %>%
- en: fit(train_data, train_labels,
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_data, train_labels,
- en: epochs = 20, batch_size = 512, validation_split = 0.4)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20, batch_size = 512, validation_split = 0.4)
- en: plot(
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图(
- en: NULL,
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: NULL,
- en: main =
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 主题 =
- en: '"Original Model vs. Much Larger Model on IMDB Review Classification",'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '"IMDB 评论分类的原始模型与更大模型的比较",'
- en: xlab = "Epochs", xlim = c(1, history_original$params$epochs),
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: xlab = "周期", xlim = c(1, history_original$params$epochs),
- en: ylab = "Validation Loss",
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ylab = "验证损失",
- en: ylim = range(c(history_original$metrics$val_loss,
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ylim = range(c(history_original$metrics$val_loss,
- en: history_larger_model$metrics$val_loss)),
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: history_larger_model$metrics$val_loss)),
- en: panel.first = abline(v = 1:history_original$params$epochs,
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: panel.first = abline(v = 1:history_original$params$epochs,
- en: lty = "dotted", col = "lightgrey")
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: lty = "dotted", col = "lightgrey")
- en: )
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: lines(history_original $metrics$val_loss, lty = 2)
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_original $metrics$val_loss, lty = 2)
- en: lines(history_larger_model$metrics$val_loss, lty = 1)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_larger_model$metrics$val_loss, lty = 1)
- en: legend("topleft", lty = 2:1,
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: legend("左上角", lty = 2:1,
- en: legend = c("Validation loss of original model",
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: legend = c("原始模型的验证损失",
- en: '"Validation loss of larger model"))'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '"更大模型的验证损失"))'
- en: '[Figure 5.18](#fig5-18) shows how the bigger model fares compared with the
    reference model.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.18](#fig5-18) 显示了更大模型与参考模型的对比情况。'
- en: '![Image](../images/f0158-01.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0158-01.jpg)'
- en: '**Figure 5.18 Original model vs. much larger model on IMDB review classification**'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.18 IMDB评论分类的原始模型和更大模型对比**'
- en: The bigger model starts overfitting almost immediately, after just one epoch,
    and it overfits much more severely. Its validation loss is also noisier. It gets
    training loss near zero very quickly. The more capacity the model has, the more
    quickly it can model the training data (resulting in a low training loss), but
    the more susceptible it is to overfitting (resulting in a large difference between
    the training and validation loss).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的模型几乎立即开始过拟合，仅经过一个时期，它的过拟合情况更为严重。其验证损失也更加嘈杂。它非常快地获得接近零的训练损失。模型的容量越大，就越能快速建模训练数据（导致较低的训练损失），但也越容易过拟合（导致训练和验证损失之间的差异很大）。
- en: '**ADDING WEIGHT REGULARIZATION**'
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**添加权重正则化**'
- en: 'You may be familiar with the principle of *Occam’s razor*: given two explanations
    for something, the explanation most likely to be correct is the simplest one—the
    one that makes fewer assumptions. This idea also applies to the models learned
    by neural networks: given some training data and a network architecture, multiple
    sets of weight values (multiple *models*) could explain the data. Simpler models
    are less likely to over-fit than complex ones.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能熟悉*奥卡姆剃刀*原则：对于某事物的两种解释，最有可能正确的解释是最简单的解释——即做出较少假设的解释。这个想法也适用于神经网络学习的模型：对于一些训练数据和网络架构，多组权重值（多个*模型*）可能可以解释数据。较简单的模型不太可能出现过拟合，
    高复杂模型则相反。
- en: 'A *simple model* in this context is a model where the distribution of parameter
    values has less entropy (or a model with fewer parameters, as you saw in the previous
    section). Thus, a common way to mitigate overfitting is to put constraints on
    the complexity of a model by forcing its weights to take only small values, which
    makes the distribution of weight values more *regular*. This is called *weight
    regularization*, and it’s done by adding to the loss function of the model a cost
    associated with having large weights. This cost comes in two flavors:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，*简单模型*是指参数值分布熵较低（或参数较少的模型，正如在前一节中所看到的）。因此，减轻过拟合的常用方法是通过对模型的复杂性施加约束，强制其权重只取小值，从而使权重值的分布更加*规则*。这称为*权重正则化*，它通过在模型的损失函数中增加与大权重相关的代价来实现。这种代价有两种类型：
- en: '*L1 regularization*—The cost added is proportional to the *absolute value of
    the weight coefficients* (the *L1 norm* of the weights).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1正则化*—增加的成本与权重系数的*绝对值*成正比（权重的*L1范数*）。'
- en: '*L2 regularization*—The cost added is proportional to the *square of the value
    of the weight coefficients* (the *L2 norm* of the weights). L2 regularization
    is also called *weight decay* in the context of neural networks. Don’t let the
    different name confuse you: weight decay is mathematically the same as L2 regularization.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L2正则化*—增加的成本与权重系数的*平方值*成正比（权重的*L2范数*）。在神经网络环境中，L2正则化也称为*权重衰减*。不要让不同的名字让你困惑：数学上，权重衰减与L2正则化是一样的。'
- en: In Keras, weight regularization is added by passing *weight regularizer instances*
    to layers as keyword arguments. Let’s add L2 weight regularization to our initial
    movie-review classification model.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，通过将*权重正则化器实例*作为关键字参数加入到层中，可添加权重正则化。让我们向初始的电影评论分类模型中添加L2权重正则化。
- en: '**Listing 5.13 Adding L2 weight regularization to the model**'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 5.13 向模型添加L2权重正则化**'
- en: model <- keras_model_sequential() %>%
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(16, activation = "relu",
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu",
- en: kernel_regularizer = regularizer_l2(0.002)) %>%
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: kernel_regularizer = regularizer_l2(0.002)) %>%
- en: layer_dense(16, activation = "relu",
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu",
- en: kernel_regularizer = regularizer_l2(0.002)) %>%
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: kernel_regularizer = regularizer_l2(0.002)) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "二元交叉熵",
- en: metrics = "accuracy")
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "准确性")
- en: history_l2_reg <- model %>% fit(
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: history_l2_reg <- model %>% fit(
- en: train_data, train_labels,
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: train_data, train_labels,
- en: epochs = 20, batch_size = 512, validation_split = 0.4)
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20, batch_size = 512, validation_split = 0.4)
- en: plot(history_l2_reg)
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history_l2_reg)
- en: '![Image](../images/f0160-01.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0160-01.jpg)'
- en: In the preceding listing, regularizer_l2(0.002) means every coefficient in the
    weight matrix of the layer will add 0.002 * weight_coefficient_value ^ 2 to the
    total loss of the model. Note that because this penalty is *added only at training
    time*, the loss for this model will be much higher at training than at test time.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的清单中，regularizer_l2(0.002)表示层中权重矩阵中的每个系数将会添加 0.002 * weight_coefficient_value
    ^ 2 到模型的总损失中。请注意，因为这个惩罚只在训练时*添加*，所以该模型的损失在训练时会比在测试时高得多。
- en: '[Figure 5.19](#fig5-19) shows the impact of the L2 regularization penalty.
    As you can see, the model with L2 regularization has become much more resistant
    to overfitting than the reference model, even though both models have the same
    number of parameters.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.19](#fig5-19) 显示了 L2 正则化惩罚的影响。正如你所见，具有 L2 正则化的模型比参考模型更能抵抗过拟合，即使两个模型具有相同数量的参数。'
- en: '**Listing 5.14 Generating a plot to demonstrate the effect of L2 weight regularization**'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 5.14 生成图表以演示 L2 权重正则化的效果**'
- en: plot(NULL,
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: plot(NULL,
- en: main = "Effect of L2 Weight Regularization on Validation Loss",
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: main = "L2 权重正则化对验证损失的影响",
- en: xlab = "Epochs",
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: xlab = "Epochs",
- en: xlim = c(1, history_original$params$epochs),
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: xlim = c(1, history_original$params$epochs),
- en: ylab = "Validation Loss",
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: ylab = "验证损失",
- en: ylim = range(c(history_original$metrics$val_loss,
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: ylim = range(c(history_original$metrics$val_loss,
- en: history_l2_reg $metrics$val_loss)),
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: history_l2_reg $metrics$val_loss)),
- en: panel.first = abline(v = 1:history_original$params$epochs,
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: panel.first = abline(v = 1:history_original$params$epochs,
- en: lty = "dotted", col = "lightgrey"))
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: lty = "dotted", col = "lightgrey"))
- en: lines(history_original$metrics$val_loss, lty = 2)
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_original$metrics$val_loss, lty = 2)
- en: lines(history_l2_reg $metrics$val_loss, lty = 1)
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_l2_reg $metrics$val_loss, lty = 1)
- en: legend("topleft", lty = 2:1,
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: legend("左上角", lty = 2:1,
- en: legend = c("Validation loss of original model",
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: legend = c("原始模型的验证损失",
- en: '"Validation loss of L2-regularized model"))'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '"L2 正则化模型的验证损失"))'
- en: '![Image](../images/f0161-01.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0161-01.jpg)'
- en: '**Figure 5.19 Effect of L2 weight regularization on validation loss**'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.19 L2 权重正则化对验证损失的影响**'
- en: As an alternative to L2 regularization, you can use one of the following Keras
    weight regularizers.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 L2 正则化的替代方案，你可以使用以下 Keras 权重正则化器之一。
- en: '**Listing 5.15 Different weight regularizers available in Keras**'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 5.15 Keras 中可用的不同权重正则化器**'
- en: regularizer_l1(0.001)➊
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: regularizer_l1(0.001)➊
- en: regularizer_l1_l2(l1 = 0.001, l2 = 0.001)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: regularizer_l1_l2(l1 = 0.001, l2 = 0.001)
- en: <keras.regularizers.L1 object at 0x7f81cc3df340>➋
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: <keras.regularizers.L1 object at 0x7f81cc3df340>➋
- en: <keras.regularizers.L1L2 object at 0x7f81cc651c40>
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: <keras.regularizers.L1L2 object at 0x7f81cc651c40>
- en: ➊ **L1 regularization**
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **L1 正则化**
- en: ➋ **Simultaneous L1 and L2 regularization**
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **同时使用 L1 和 L2 正则化**
- en: 'Note that weight regularization is more typically used for smaller deep learning
    models. Large deep learning models tend to be so overparameterized that imposing
    constraints on weight values doesn’t have much impact on model capacity and generalization.
    In these cases, a different regularization technique is preferred: dropout.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，权重正则化更常用于较小的深度学习模型。大型深度学习模型往往过度参数化，对权重值施加约束并不会对模型容量和泛化性产生太大影响。在这些情况下，更喜欢使用不同的正则化技术：dropout。
- en: '**ADDING DROPOUT**'
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**添加 dropout**'
- en: '*Dropout* is one of the most effective and most commonly used regularization
    techniques for neural networks; it was developed by Geoff Hinton and his students
    at the University of Toronto. Dropout, applied to a layer, consists of randomly
    *dropping out* (setting to zero) a number of output features of the layer during
    training. Let’s say a given layer would normally return a vector c(0.2, 0.5, 1.3,
    0.8, 1.1) for a given input sample during training. After applying dropout, this
    vector will have a few zero entries distributed at random: for example, c(0, 0.5,
    1.3, 0, 1.1). The *dropout rate* is the fraction of the features that are zeroed
    out; it’s usually set between 0.2 and 0.5\. At test time, no units are dropped
    out; instead, the layer’s output values are scaled down by a factor equal to the
    dropout rate, to balance for the fact that more units are active than at training
    time.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout*是神经网络中最有效、最常用的正则化技术之一；它是由杰弗·欣顿和他在多伦多大学的学生们开发的。应用于一层的Dropout在训练期间包括随机地*退出*（设为零）一定数量的该层输出特征。假设给定的层在训练期间为给定的输入样本返回一个向量c(0.2,
    0.5, 1.3, 0.8, 1.1)。应用Dropout后，该向量将随机分布一些零条目，例如，c(0, 0.5, 1.3, 0, 1.1)。*退出率*是被置零的特征的比例；通常设置在0.2和0.5之间。在测试时，不会退出任何单元；相反，该层的输出值会按退出率缩小，以平衡更多单元处于活动状态的事实。'
- en: 'Consider a matrix containing the output of a layer, layer_output, of shape
    (batch_size, features). At training time, we zero out at random a fraction of
    the values in the matrix:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含一层输出的矩阵，layer_output，形状为(batch_size, features)。在训练时，我们随机将矩阵中的一部分值置零：
- en: zero_out <- random_array(dim(layer_output)) < .5
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '`zero_out <- random_array(dim(layer_output)) < .5`'
- en: layer_output[zero_out] <- 0➊
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_output[zero_out] <- 0`➊'
- en: ➊ **At training time, drops out 50% of the units in the output**
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在训练时，退出输出中的50%单元**
- en: 'At test time, we scale down the output by the dropout rate. Here, we scale
    by 0.5 (because we previously dropped half the units):'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时，我们通过退出率来缩放输出。在这里，我们以0.5的比例缩放（因为我们之前退出了一半的单元）：
- en: layer_output <- layer_output * .5➊
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_output <- layer_output * .5`➊'
- en: ➊ **At test time**
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在测试时**
- en: 'Note that this process can be implemented by doing both operations at training
    time and leaving the output unchanged at test time, which is often the way it’s
    implemented in practice (see [figure 5.20](#fig5-20)):'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个过程可以通过在训练时执行两个操作并在测试时保持输出不变来实现，这通常是实践中的实现方式（参见[图5.20](#fig5-20)）：
- en: layer_output[random_array(dim(layer_output)) < dropout_rate] <- 0
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_output[random_array(dim(layer_output)) < dropout_rate] <- 0`'
- en: layer_output <- layer_output / .5➊
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`layer_output <- layer_output / .5`➊'
- en: ➊ **At training time. Note that we're scaling up rather than scaling down in
    this case.**
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **在训练时。请注意，在这种情况下，我们是放大而不是缩小。**
- en: '![Image](../images/f0162-01.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0162-01.jpg)'
- en: '**Figure 5.20 Dropout applied to an activation matrix at training time, with
    rescaling happening during training. At test time, the activation matrix is unchanged.**'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.20 Dropout应用于训练期间的激活矩阵，训练期间发生重新缩放。在测试时，激活矩阵保持不变。**'
- en: This technique may seem strange and arbitrary. Why would this help reduce overfitting?
    Hinton says he was inspired by, among other things, a fraud-prevention mechanism
    used by banks. In his own words, “I went to my bank. The tellers kept changing,
    and I asked one of them why. He said he didn’t know, but they got moved around
    a lot. I figured it must be because it would require cooperation between employees
    to successfully defraud the bank. This made me realize that randomly removing
    a different subset of neurons on each example would prevent conspiracies and thus
    reduce overfitting.” The core idea is that introducing noise in the output values
    of a layer can break up happenstance patterns that aren’t significant (what Hinton
    refers to as *conspiracies*), which the model will start memorizing if no noise
    is present.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可能看起来奇怪而武断。为什么这有助于减少过拟合？欣顿说他受到了银行使用的一种防欺诈机制的启发。用他自己的话说，“我去了我的银行。出纳员不停地换，我问其中一个为什么。他说他不知道，但他们经常变动。我想这一定是因为成功欺诈银行需要员工之间的合作。这让我意识到，在每个例子中随机删除不同子集的神经元将防止阴谋，从而减少过拟合。”其核心思想是，向一层的输出值引入噪音可以打破无意义的模式（欣顿称之为*阴谋*），如果没有噪音存在，模型将开始记忆。
- en: In Keras, you can introduce dropout in a model via layer_dropout, which applies
    dropout to the output of the layer right before it. Let’s add two layer_dropouts
    to the IMDB model to see how well they do at reducing overfitting.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，您可以通过 layer_dropout 在模型中引入 Dropout，它将 Dropout 应用于该层之前的输出。让我们在 IMDB
    模型中添加两个 layer_dropout，看看它们在减少过拟合方面的效果如何。
- en: '**Listing 5.16 Adding dropout to the IMDB model**'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.16 向 IMDB 模型添加 Dropout**'
- en: model <- keras_model_sequential() %>%
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model_sequential() %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16,
- en: layer_dropout(0.5) %>%
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(16, activation = "relu") %>%
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(16, activation = "relu") %>%
- en: layer_dropout(0.5) %>%
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "accuracy")
- en: history_dropout <- model %>% fit(
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: history_dropout <- model %>% fit(
- en: train_data, train_labels,
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据, 训练标签,
- en: epochs = 20, batch_size = 512,
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20, batch_size = 512,
- en: validation_split = 0.4
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: validation_split = 0.4
- en: )
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: plot(history_dropout)
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: plot(history_dropout)
- en: '![Image](../images/f0163-01.jpg)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0163-01.jpg)'
- en: '[Figure 5.21](#fig5-21) shows a plot of the results. This is a clear improvement
    over the reference model—it also seems to be working much better than L2 regularization,
    because the lowest validation loss reached has improved.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.21](#fig5-21) 显示了结果的图表。这是对参考模型的明显改进，它似乎也比L2正则化工作得更好，因为达到的最低验证损失也有所改善。'
- en: '**Listing 5.17 Generating a plot to demonstrate the effect of dropout on validation
    loss**'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.17 生成一个演示 Dropout 对验证损失的效果的图表**'
- en: plot(NULL,
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: plot(NULL,
- en: main = "Effect of Dropout on Validation Loss",
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 主标题 = "Dropout 对验证损失的效果",
- en: xlab = "Epochs", xlim = c(1, history_original$params$epochs),
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: x轴标签 = "Epochs", x轴范围 = c(1, history_original$params$epochs),
- en: ylab = "Validation Loss",
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: y轴标签 = "验证损失",
- en: ylim = range(c(history_original$metrics$val_loss,
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: y轴范围 = range(c(history_original$metrics$val_loss,
- en: history_dropout $metrics$val_loss)),
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: history_dropout $metrics$val_loss)),
- en: panel.first = abline(v = 1:history_original$params$epochs,
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 面板首先 = abline(v = 1:history_original$params$epochs,
- en: lty = "dotted", col = "lightgrey"))
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: lty = "dotted", col = "lightgrey"))
- en: lines(history_original$metrics$val_loss, lty = 2)
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_original$metrics$val_loss, lty = 2)
- en: lines(history_dropout $metrics$val_loss, lty = 1)
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: lines(history_dropout $metrics$val_loss, lty = 1)
- en: legend("topleft", lty = 1:2,
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 说明框("左上角", lty = 1:2,
- en: legend = c("Validation loss of dropout-regularized model",
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图例 = c("Dropout 正则化模型的验证损失",
- en: '"Validation loss of original model"))'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '"原始模型的验证损失"))'
- en: '![Image](../images/f0164-01.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0164-01.jpg)'
- en: '**Figure 5.21 Effect of dropout on validation loss**'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.21 Dropout 对验证损失的效果**'
- en: 'To recap, these are the most common ways to maximize generalization and prevent
    overfitting in neural networks:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，以下是在神经网络中最常见的最大化泛化和防止过拟合的方法：
- en: Get more training data, or better training data.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得更多的训练数据，或更好的训练数据。
- en: Develop better features.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发展更好的特征。
- en: Reduce the capacity of the model.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型的容量。
- en: Add weight regularization (for smaller models).
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加权重正则化（适用于较小的模型）。
- en: Add dropout
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加 Dropout
- en: Summary
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'The purpose of a machine learning model is to *generalize*: to perform accurately
    on never-before-seen inputs. It’s harder than it seems'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的目的是*泛化*：在以前未见过的输入上准确执行。这比看起来更难
- en: A deep neural network achieves generalization by learning a parametric model
    that can successfully *interpolate* between training samples—such a model can
    be said to have learned the “latent manifold” of the training data. This is why
    deep learning models can make sense of only inputs that are very close to what
    they’ve seen during training
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络通过学习一个能够成功*插值*训练样本之间的参数化模型来实现泛化——这样的模型可以说已经学习了训练数据的“潜在流形”。这就是为什么深度学习模型只能理解与它们在训练中看到的非常接近的输入的原因。
- en: 'The fundamental problem in machine learning is *the tension between optimization
    and generalization*: to attain generalization, you must first achieve a good fit
    to the training data, but improving your model’s fit to the training data will
    inevitably start hurting generalization after a while. Every single deep learning
    best practice deals with managing this tension.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的基本问题是*优化和泛化之间的紧张关系*：为了实现泛化，必须首先获得对训练数据的良好拟合，但是改善模型对训练数据的拟合会在一段时间后开始损害泛化能力。每一个深度学习最佳实践都处理了这种紧张的方式。
- en: The ability of deep learning models to generalize comes from the fact that they
    manage to learn to approximate the *latent manifold* of their data and can thus
    make sense of new inputs via interpolation
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型的泛化能力来自于它们成功学习近似其数据的*潜在流形*的事实，因此可以通过插值来理解新的输入。
- en: It’s essential to be able to accurately evaluate the generalization power of
    your model while you’re developing it. You have at your disposal an array of evaluation
    methods, from simple holdout validation to *K*-fold cross-validation and iterated
    *K*-fold cross-validation with shuffling. Remember to always keep a completely
    separate test set for final model evaluation, because information leaks from your
    validation data to your model may have occurred.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发模型时，能够准确评估模型的泛化能力至关重要。你可以使用一系列评估方法，从简单的留出验证到*K*-折交叉验证和迭代*K*-折交叉验证与洗牌。请记住始终保留一个完全独立的测试集用于最终模型评估，因为验证数据泄露到模型中可能已经发生。
- en: When you start working on a model, your goal is first to achieve a model that
    has some generalization power and that can overfit. Best practices for doing this
    include tuning your learning rate and batch size, leveraging better architecture
    priors, increasing model capacity, or simply training longer.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您开始研究一个模型时，您的目标首先是获得具有一定泛化能力并且可以过拟合的模型。做到这一点的最佳实践包括调整学习率和批量大小，利用更好的架构先验，增加模型容量，或者简单地延长训练时间。
- en: As your model starts overfitting, your goal switches to improving generalization
    through *model regularization*. You can reduce your model’s capacity, add dropout
    or weight regularization, and use early stopping. And naturally, a larger or better
    dataset is always the number one way to help a model generalize.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您的模型开始过拟合时，您的目标转向通过*模型正则化*来提高泛化能力。您可以降低模型的容量，添加 dropout 或权重正则化，并使用早停技术。自然地，一个更大或更好的数据集始终是帮助模型泛化的首选方法。
- en: ^([1](#endnote1)) Mark Twain even called it “the most delicious fruit known
    to men.”
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([1](#endnote1)) 马克·吐温甚至称其为“人类所知最美味的水果。”
