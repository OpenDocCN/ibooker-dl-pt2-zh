- en: 'Chapter 5\. Transfer learning: Reusing pretrained neural networks'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章。转移学习：重用预训练的神经网络
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: What transfer learning is and why it is better than training models from scratch
    for many types of problems
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是转移学习，为什么它对许多类型的问题而言比从头开始训练模型更好
- en: How to leverage the feature-extraction power of state-of-the-art pretrained
    convnets by converting them from Keras to TensorFlow.js
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过将其从Keras转换为TensorFlow.js来利用最先进的预训练卷积神经网络的特征提取能力
- en: The detailed mechanisms of transfer-learning techniques including layer freezing,
    creating new transfer heads, and fine-tuning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移学习技术的详细机制，包括层冻结、创建新的转移头和微调
- en: How to use transfer learning to train a simple object-detection model in TensorFlow.js
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用转移学习在TensorFlow.js中训练简单的目标检测模型
- en: 'In [chapter 4](kindle_split_015.html#ch04), we saw how to train convnets to
    classify images. Now consider the following scenario. Our convnet for classifying
    handwritten digits performs poorly for a user because their handwriting is very
    different from the original training data. Can we improve the model to serve the
    user better by using a small amount of data (say, 50 examples) we can collect
    from them? Consider another scenario: an e-commerce website wishes to automatically
    classify pictures of commodity items uploaded by users. But none of the publicly
    available convnets (such as MobileNet^([[1](#ch05fn1)])) are trained on such domain-specific
    images. Is it possible to use a publicly available image model to address the
    custom classification problem, given a modest number (say, a few hundred) of labeled
    pictures?'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](kindle_split_015.html#ch04)中，我们看到了如何训练卷积神经网络来对图像进行分类。现在考虑以下情景。我们用于分类手写数字的卷积神经网络对某位用户表现不佳，因为他们的手写与原始训练数据非常不同。我们能否通过使用我们可以从他们那里收集到的少量数据（比如，50个样本）来改进模型，从而更好地为用户提供服务？再考虑另一种情况：一个电子商务网站希望自动分类用户上传的商品图片。但是公开可用的卷积神经网络（例如MobileNet^([[1](#ch05fn1)]))都没有针对这种特定领域的图像进行训练。在给定少量标记图片（比如，几百张）的情况下，是否可以使用公开可用的图像模型来解决定制分类问题？
- en: ¹
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Andrew G. Howard et al., “MobileNets: Efficient Convolutional Neural Networks
    for Mobile Vision Applications,” submitted 17 Apr. 2017, [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861).'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Andrew G. Howard等人，“MobileNets: 面向移动视觉应用的高效卷积神经网络”，2017年4月17日提交，[https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)。'
- en: Fortunately, a technique called *transfer learning*, the main focus of this
    chapter, can help solve tasks like these.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，本章的主要焦点——一种称为*转移学习*的技术，可以帮助解决这类任务。
- en: '5.1\. Introduction to transfer learning: Reusing pretrained models'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 转移学习简介：重用预训练模型
- en: In essence, transfer learning is about speeding up a new learning task by reusing
    the results of previous learning. It involves using a model already trained on
    a dataset to perform a *different but related* machine-learning task. The already-trained
    model is referred to as the *base model*. Transfer learning sometimes involves
    retraining the base model and sometimes involves creating a new model on top of
    the base model. We refer to the new model as the *transfer model*. As [figure
    5.1](#ch05fig01) shows, the amount of data used for this retraining process is
    usually much smaller compared to the data that went into training the base model
    (as with the two examples given at the beginning of this chapter). As such, transfer
    learning is often much less time-and resource-consuming compared to the base model’s
    training process. This makes it feasible to perform transfer learning in a resource-restricted
    environment like the browser using TensorFlow.js. And thus transfer learning is
    an important topic for TensorFlow.js learners.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，转移学习是通过重用先前学习结果来加速新的学习任务。它涉及使用已经在数据集上训练过的模型来执行*不同但相关*的机器学习任务。已经训练好的模型被称为*基础模型*。转移学习有时涉及重新训练基础模型，有时涉及在基础模型的顶部创建一个新模型。我们将新模型称为*转移模型*。正如[图5.1](#ch05fig01)所示，用于这个重新训练过程的数据量通常比用于训练基础模型的数据量要小得多（就像本章开头给出的两个例子一样）。因此，与基础模型的训练过程相比，转移学习通常需要的时间和资源要少得多。这使得在像浏览器这样的资源受限环境中使用TensorFlow.js进行转移学习成为可能。因此，对于TensorFlow.js学习者来说，转移学习是一个重要的主题。
- en: Figure 5.1\. The general workflow of transfer learning. A large dataset goes
    into the training of the base model. This initial training process is often long
    and computationally heavy. The base model is then retrained, possibly by becoming
    part of a new model. The retraining process usually involves a dataset much smaller
    than the original one. The computation involved in the retraining is significantly
    less than the initial training and can happen on an edge device, such as a laptop
    or a phone running TensorFlow.js.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1\. 迁移学习的一般工作流程。大型数据集用于基础模型的训练。这个初始训练过程通常很长且计算量大。然后重新训练基础模型，可能成为新模型的一部分。重新训练过程通常涉及比原始数据集小得多的数据集。重新训练所涉及的计算量远远小于初始训练，并且可以在边缘设备上进行，例如运行
    TensorFlow.js 的笔记本电脑或手机。
- en: '![](05fig01a_alt.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig01a_alt.jpg)'
- en: 'The key phrase “different but related” in the description of transfer learning
    can mean different things in different cases:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 描述迁移学习中的关键短语“不同但相关”在不同情况下可能意味着不同的事情：
- en: The first scenario mentioned at the beginning of this chapter involves adapting
    a model to the data from a specific user. Although the data is different from
    the original training set, the task is exactly the same—classifying an image into
    the 10 digits. This type of transfer learning is referred to as *model adaptation*.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章开头提到的第一个场景涉及将模型调整为特定用户的数据。尽管数据与原始训练集不同，但任务完全相同——将图像分类为 10 个数字。这种类型的迁移学习被称为*模型适应*。
- en: Other transfer-learning problems involve targets (labels) that are different
    from the original ones. The commodity image-classification scenario mentioned
    at the beginning of this chapter belongs to this category.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他迁移学习问题涉及与原始标签不同的目标。本章开头提到的商品图像分类场景属于这一类别。
- en: 'What is the advantage of transfer learning over training a new model from scratch?
    The answer is two-fold:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与从头开始训练新模型相比，迁移学习的优势是什么？答案有两个方面：
- en: Transfer learning is more efficient in terms of both the amount of data it requires
    and the amount of computation it takes.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据量和计算量两个方面来看，迁移学习更加高效。
- en: It builds on the gains of previous training by reusing the feature-extracting
    power of the base model.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它借助基础模型的特征提取能力，建立在先前训练成果的基础上。
- en: 'These points are valid regardless of the type of problem (for instance, classification
    and regression). On the first point, transfer learning uses the trained weights
    from the base model (or a subset of them). As a result, it requires less training
    data and training time to converge to a given level of accuracy compared to training
    a new model from scratch. In this regard, transfer learning is analogous to how
    humans learn new tasks: once you have mastered a task (playing a card game, for
    example), learning similar tasks (such as playing similar card games) becomes
    significantly easier and faster in the future. The saved cost of training time
    may seem relatively small for a neural network like the convnet we built for MNIST.
    However, for larger models trained on larger datasets (such as industrial-scale
    convnets trained on terabytes of image data), the savings can be substantial.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观点适用于各种类型的问题（例如分类和回归）。在第一个观点上，迁移学习使用来自基础模型（或其子集）的训练权重。因此，与从头开始训练新模型相比，它需要更少的训练数据和训练时间才能收敛到给定精度水平。在这方面，迁移学习类似于人类学习新任务的方式：一旦你掌握了一个任务（例如玩纸牌游戏），学习类似的任务（例如玩类似的纸牌游戏）在将来会变得更容易和更快。对于我们为
    MNIST 构建的 convnet 这样的神经网络来说，节省的训练时间成本可能相对较小。然而，对于在大型数据集上训练的较大模型（例如在 TB 级图像数据上训练的工业规模
    convnet），节省可以是相当可观的。
- en: On the second point, the core idea of transfer learning is reusing previous
    training results. Through learning from a very large dataset, the original neural
    network has become very good at extracting useful features from the original input
    data. These features will be useful for the new task as long as the new data in
    the transfer-learning task is not too different from the original data. Researchers
    have assembled very large datasets for common machine-learning domains. In computer
    vision, there is ImageNet,^([[2](#ch05fn2)]) which contains millions of labeled
    images from about a thousand categories. Deep-learning researchers have trained
    deep convnets using the ImageNet dataset, including ResNet, Inception, and MobileNet
    (the last of which we will soon lay our hands on). Due to the large number and
    diversity of the images in ImageNet, convnets trained on it are good feature extractors
    for general types of images. These feature extractors will be useful for working
    with small datasets like those in the aforementioned scenarios, but training such
    effective feature extractors is impossible with small datasets like those. Opportunities
    for transfer learning exist in other domains as well. For example, in natural
    language processing, people have trained word embeddings (that is, vector representation
    of all common words in a language) on large text corpora consisting of billions
    of words. These embeddings are useful for language-understanding tasks where much
    smaller text datasets are available. Without further ado, let’s see how transfer
    learning works in practice through an example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第二点，迁移学习的核心思想是重用之前的训练结果。通过从一个非常大的数据集中学习，原始神经网络已经非常擅长从原始输入数据中提取有用的特征。只要迁移学习任务中的新数据与原始数据不相差太大，这些特征对于新任务将是有用的。研究人员已经为常见的机器学习领域组装了非常大的数据集。在计算机视觉领域，有ImageNet^([[2](#ch05fn2)])，其中包含大约一千个类别的数百万张带标签的图像。深度学习研究人员已经使用ImageNet数据集训练了深度卷积神经网络，包括ResNet、Inception和MobileNet（我们将很快接触到的最后一个）。由于ImageNet中图像的数量和多样性，训练在其上的卷积神经网络是一般类型图像的良好特征提取器。这些特征提取器对于处理像前述情景中的小数据集这样的小数据集将是有用的，但是使用小数据集训练这样有效的特征提取器是不可能的。迁移学习的机会也存在于其他领域。例如，在自然语言处理领域，人们已经在包含数十亿个单词的大型文本语料库上训练了词嵌入（即语言中所有常见单词的向量表示）。这些嵌入对于可用的远小于大文本数据集的语言理解任务是有用的。话不多说，让我们通过一个例子来看看迁移学习是如何在实践中工作的。
- en: ²
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Don’t be confused by the name. “ImageNet” refers to a dataset, not a neural
    network.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不要被名字所迷惑。“ImageNet”指的是一个数据集，而不是一个神经网络。
- en: '5.1.1\. Transfer learning based on compatible output shapes: Freezing layers'
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 基于兼容输出形状的迁移学习：冻结层
- en: 'Let’s start by looking at a relatively simple example. We will train a convnet
    on only the first five digits of the MNIST dataset (0 through 4). We will then
    use the resulting model to recognize the remaining five digits (5 through 9),
    which the model never saw during the original training. Although this example
    is somewhat contrived, it illustrates the basic workflow of transfer learning.
    The example can be checked out and run with the following commands:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个相对简单的例子开始。我们将在MNIST数据集的前五个数字（0到4）上训练一个卷积神经网络。然后，我们将使用得到的模型来识别剩余的五个数字（5到9），这些数字在原始训练中模型从未见过。虽然这个例子有些人为，但它展示了迁移学习的基本工作流程。你可以通过以下命令查看并运行这个例子：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the demo web page that opens, start the transfer learning process by clicking
    the Retrain button. You can see the process reach an accuracy of about 96% on
    the new set of five digits (5 through 9), which takes about 30 seconds on a reasonably
    powerful laptop. As we will show, this is significantly faster than the non-transfer-learning
    alternative (namely, training a new model from scratch). Let’s see how this is
    done, step-by-step.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开的演示网页中，通过点击“重新训练”按钮来开始迁移学习过程。你可以看到这个过程在新的五个数字（5到9）上达到了约96%的准确率，这需要在一台性能较强的笔记本电脑上大约30秒。正如我们将要展示的，这比不使用迁移学习的替代方案（即从头开始训练一个新模型）要快得多。让我们逐步看看这是如何实现的。
- en: 'Our example loads the pretrained base model from an HTTP server instead of
    training it from scratch so as not to obscure the workflow’s key parts. Recall
    from section 4.3.3 that TensorFlow.js provides the `tf.loadLayersModel()` method
    for loading pretrained models. This is called in the loader.js file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的例子从HTTP服务器加载预训练的基础模型，而不是从头开始训练，以免混淆工作流程的关键部分。回想一下第4.3.3节，TensorFlow.js提供了`tf.loadLayersModel()`方法来加载预训练模型。这在loader.js文件中调用：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The printed summary of the model looks like [figure 5.2](#ch05fig02). As you
    can see, this model consists of 12 layers.^([[3](#ch05fn3)]) All its 600,000 or
    so weight parameters are trainable, just like all the TensorFlow.js models we
    have seen so far. Note that `loadLayersModel()` loads not only the model’s topology
    but also all its weight values. As a result, the loaded model is ready to predict
    the class of digits 0 through 4\. However, this is not how we will use the model.
    Instead, we will train the model to recognize new digits (5 through 9).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的打印摘要看起来像 [图5.2](#ch05fig02)。正如你所见，该模型由12层组成。^([[3](#ch05fn3)]) 其中的大约600,000个权重参数都是可训练的，就像迄今为止我们见过的所有
    TensorFlow.js 模型一样。请注意，`loadLayersModel()` 不仅加载模型的拓扑结构，还加载所有权重值。因此，加载的模型已经准备好预测数字
    0 到 4 的类别。但是，这不是我们将使用模型的方式。相反，我们将训练模型来识别新的数字（5 到 9）。
- en: ³
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You may not have seen the *activation* layer type in this model. Activation
    layers are simple layers that perform only an activation function (such as relu
    and softmax) on the input. Suppose you have a dense layer with the default (linear)
    activation; stacking an activation layer on top of it is equivalent to using a
    dense layer with the nondefault activation included. The latter is what we did
    for the examples in [chapter 4](kindle_split_015.html#ch04). But the former style
    is also sometimes seen. In TensorFlow.js, you can get such a model topology by
    using code like the following: `const` `model` `=` `tf.sequential();` `model.add(tf.layers.dense({untis:`
    `5,` `inputShape})); model.add(tf.layers.activation({activation: ''relu''})`.'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '在这个模型中，你可能没有看到*激活*层类型。激活层是仅对输入执行激活函数（如 relu 和 softmax）的简单层。假设你有一个具有默认（线性）激活的稠密层；在其上叠加一个激活层等同于使用具有非默认激活的稠密层。这就是我们在
    [第4章](kindle_split_015.html#ch04) 中的例子所做的。但是有时也会看到前一种风格。在 TensorFlow.js 中，你可以通过以下代码获取这样的模型拓扑：`const`
    `model` `=` `tf.sequential();` `model.add(tf.layers.dense({untis:` `5,` `inputShape}));
    model.add(tf.layers.activation({activation: ''relu''})）。'
- en: Figure 5.2\. A printed summary of the convnet for recognition of MNIST images
    and transfer learning
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2\. MNIST 图像识别和迁移学习卷积神经网络的打印摘要
- en: '![](05fig01_alt.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig01_alt.jpg)'
- en: Looking at the callback function for the Retrain button (in the `retrainModel()`
    function of index.js), you will notice a few lines of code that set the `trainable`
    property of the first seven layers of the model to `false` if the option Freeze
    Feature Layers is selected (it is selected by default).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 查看回调函数以重新训练按钮（在 index.js 的 `retrainModel()` 函数中）时，如果选择了冻结特征层选项（默认情况下选择了），你会注意到一些代码行将模型的前七层的
    `trainable` 属性设置为 `false`。
- en: What does that do? By default, the `trainable` property of each of the model’s
    layers is `true` after the model is loaded via the `loadLayersModel()` method
    or created from scratch. The `trainable` property is used during training (that
    is, calls to the `fit()` or `fitDataset()` method). It tells the optimizer whether
    the layer’s weights should be updated. By default, the weights of all layers of
    a model are updated during training. But if you set the property to `false` for
    some of the model’s layers, the weights of those layers will *not* be updated
    during training. In TensorFlow.js terminology, those layers become *untrainable*,
    or *frozen*. The code in [listing 5.1](#ch05ex01) freezes the first seven layers
    of the model, from the input conv2d layer to the flatten layer, while leaving
    the last several layers (the dense layers) trainable.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那是做什么用的？默认情况下，模型加载后通过 `loadLayersModel()` 方法或从头创建后，模型的每个层的 `trainable` 属性都是
    `true`。 `trainable` 属性在训练期间（即调用 `fit()` 或 `fitDataset()` 方法）中使用。它告诉优化器是否应该更新层的权重。默认情况下，模型的所有层的权重都在训练期间更新。但是，如果你将某些模型层的属性设置为
    `false`，那么这些层的权重在训练期间将不会更新。在 TensorFlow.js 的术语中，这些层变为*不可训练*或*冻结*。列表 5.1 中的代码冻结了模型的前七层，从输入
    conv2d 层到 flatten 层，同时保持最后几层（稠密层）可训练。
- en: Listing 5.1\. “Freezing” the first several layers of the convnet for transfer
    learning
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.1\. 冻结卷积网络的前几层以进行迁移学习
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** Freezes the layer'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 冻结层'
- en: '***2*** Makes a new model with the same topology as the old one, but with reinitialized
    weight values'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建一个与旧模型具有相同拓扑结构但重新初始化权重值的新模型'
- en: '***3*** The freezing will not take effect during fit() calls unless you compile
    the model first.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 冻结将不会在调用 fit() 时生效，除非你首先编译模型。'
- en: '***4*** Prints the model summary again after compile(). You should see that
    a number of the model’s weights have become nontrainable.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 在 compile() 后再次打印模型摘要。您应该看到模型的一些权重已变为不可训练。'
- en: 'However, setting the layers’ `trainable` property alone is not enough: if you
    just modify the `trainable` property and call the model’s `fit()` method right
    away, you will see the weights of those layers still get updated during the `fit()`
    call. You need to call `Model.compile()` before calling `Model.fit()` in order
    for the `trainable` property changes to take effect, as is done in [listing 5.1](#ch05ex01).
    We mentioned previously that the `compile()` call configures the optimizer, loss
    function, and metrics. However, the method also lets the model refresh the list
    of weight variables to be updated during those calls. After the `compile()` call,
    we call `summary()` again to print a new summary of the model. As you can see
    by comparing the new summary with the old one in [figure 5.2](#ch05fig02), some
    of the model’s weights become nontrainable:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅设置层的 `trainable` 属性是不够的：如果您只修改 `trainable` 属性并立即调用模型的 `fit()` 方法，您将看到这些层的权重在
    `fit()` 调用期间仍然会被更新。在调用 `Model.fit()` 之前，您需要调用 `Model.compile()` 以使 `trainable`
    属性更改生效，就像在 [列表 5.1](#ch05ex01) 中所做的那样。我们之前提到 `compile()` 调用配置了优化器、损失函数和指标。但是，该方法还允许模型在这些调用期间刷新要更新的权重变量列表。在
    `compile()` 调用之后，我们再次调用 `summary()` 来打印模型的新摘要。通过将新摘要与 [图 5.2](#ch05fig02) 中的旧摘要进行比较，您会发现一些模型的权重已变为不可训练：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can verify that the number of nontrainable parameters, 9,568, is the sum
    of weight parameters in the two frozen layers with weights (the two conv2d layers).
    Note that some of the layers we’ve frozen contain no weights (such as the maxPooling2d
    layer and the flatten layer) and therefore don’t contribute to the count of nontrainable
    parameters when they are frozen.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以验证非可训练参数的数量，即 9,568，是两个冻结层中的权重参数之和（两个 conv2d 层的权重）。请注意，我们已冻结的一些层不包含权重（例如
    maxPooling2d 层和 flatten 层），因此当它们被冻结时不会对非可训练参数的计数产生贡献。
- en: 'The actual transfer-learning code is shown in [listing 5.2](#ch05ex02). Here,
    we use the same `fit()` method that we’ve used to train models from scratch. In
    this call, we use the `validationData` field to get a measure of how accurate
    the model is doing on data it hasn’t seen during training. In addition, we connect
    two callbacks to the `fit()` call, one for updating the progress bar in the UI
    and the other for plotting the loss and accuracy curves using the tfjs-vis module
    (more details coming in [chapter 7](kindle_split_019.html#ch07)). This shows an
    aspect of the `fit()` API that we haven’t mentioned before: you can give a callback
    or an array of multiple callbacks to a `fit()` call. In the latter case, all the
    callbacks will be invoked (in the order they are specified in the array) during
    training.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的迁移学习代码显示在 [列表 5.2](#ch05ex02) 中。在这里，我们使用了与从头开始训练模型相同的 `fit()` 方法。在此调用中，我们使用
    `validationData` 字段来衡量模型在训练期间未见过的数据上的准确性。此外，我们将两个回调连接到 `fit()` 调用，一个用于在用户界面中更新进度条，另一个用于使用
    tfjs-vis 模块绘制损失和准确率曲线（更多细节请参见 [第 7 章](kindle_split_019.html#ch07)）。这显示了 `fit()`
    API 的一个方面，我们之前没有提到过：您可以给 `fit()` 调用一个回调或一个包含多个回调的数组。在后一种情况下，所有回调将在训练期间被调用（按照数组中指定的顺序）。
- en: Listing 5.2\. Using `Model.fit()` to perform transfer learning
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**列表 5.2** 使用 `Model.fit()` 进行迁移学习'
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1*** Giving multiple callbacks to a fit() call is allowed.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 给 `fit()` 调用添加多个回调是允许的。'
- en: '***2*** Uses tfjs-vis to plot the validation loss and accuracy during transfer
    learning'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 使用 tfjs-vis 绘制迁移学习过程中的验证损失和准确率'
- en: How does the result of the transfer learning turn out? As you can see in panel
    A of [figure 5.3](#ch05fig03), it reaches an accuracy of around 0.968 after 10
    epochs of training, which takes roughly 15 seconds on a relatively up-to-date
    laptop—not bad. But how does this compare to training a model from scratch? One
    way in which we can demonstrate the value of starting from a pretrained model
    over starting from scratch is to do an experiment in which we randomly reinitialize
    the weights of the pretrained model right before the `fit()` call. This is what
    happens if you select the Reinitialize Weights option from the Training Mode drop-down
    menu before clicking the Retrain button. The result is shown in panel B of the
    same figure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的结果如何？正如您在[图 5.3](#ch05fig03)的 A 面板中所看到的，经过 10 个 epoch 的训练后，准确率达到约 0.968，大约需要在一台相对更新的笔记本电脑上花费约
    15 秒，还算不错。但与从头开始训练模型相比如何呢？我们可以通过一个实验演示从预训练模型开始相对于从头开始的价值，即在调用 `fit()` 前随机重新初始化预训练模型的权重。在点击重新训练按钮之前，在训练模式下拉菜单中选择重新初始化权重选项即可。结果显示在同一图表的
    B 面板中。
- en: 'Figure 5.3\. The loss and validation curves for transfer learning on the MNIST
    convnet. Panel A: the curves obtained with the first seven layers of the pretrained
    model frozen. Panel B: the curves obtained with all the weights of the model reinitialized
    randomly. Panel C: the curves obtained without freezing any layers of the pretrained
    model. Note that the y-axes differ among the three panels. Panel D: a multiseries
    plot that shows the loss and accuracy curves from panels A–C on the same axes
    to facilitate comparison.'
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.3\. 在 MNIST 卷积网络上的迁移学习的损失和验证曲线。面板 A：使用预训练模型的前七层冻结得到的曲线。面板 B：使用模型的所有权重随机重新初始化得到的曲线。面板
    C：不冻结任何预训练模型层获得的曲线。请注意三个面板之间的 y 轴有所不同。面板 D：一个多系列图，显示了面板 A–C 中的损失和准确度曲线在相同轴上以便进行比较。
- en: '![](05fig02_alt.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig02_alt.jpg)'
- en: 'As you can see by comparing panel B with panel A, the random reinitialization
    of the model weights causes the loss to start at a significantly higher value
    (0.36 versus 0.30) and the accuracy to start from a significantly lower value
    (0.88 versus 0.91). The reinitialized model also ends up with a lower final validation
    accuracy (~0.954) than the model that reuses weights from the base model (~0.968).
    These differences reflect the advantage of transfer learning: by reusing weights
    in the early layers (the feature-extracting layers) of the model, the model gets
    a nice head start relative to learning everything from scratch. This is because
    the data encountered in the transfer-learning task is similar to the data used
    to train the original model. The images of digits 5 through 9 have a lot in common
    with those of digits 0 through 4: they are all grayscale images with a black background;
    they have similar visual patterns (strokes of comparable widths and curvatures).
    So, the features the model learned how to extract from digits 0 through 4 turn
    out to be useful for learning to classifying the new digits (5 through 9), too.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较 B 面板和 A 面板，可以看出模型权重的随机重新初始化导致损失从一个显著更高的值开始（0.36 对比 0.30），准确度则从一个显著更低的值开始（0.88
    对比 0.91）。重新初始化的模型最终的验证准确率也比重复使用从基本模型中的权重的模型低（约 0.954 对比 ~0.968）。这些差异反映了迁移学习的优势：通过重复使用模型的初始层（特征提取层）中的权重，相对于从头开始学习，模型获得了一个良好的起步。这是因为迁移学习任务中遇到的数据与用于训练原始模型的数据相似。数字
    5 到 9 的图像与数字 0 到 4 的图像有很多共同点：它们都是带有黑色背景的灰度图像；它们有类似的视觉模式（相似宽度和曲率的笔画）。因此，模型从数字 0
    到 4 中学习提取的特征对学习分类新数字（5 到 9）也很有用。
- en: 'What if we don’t freeze the weights of the feature layers? The Don’t Freeze
    Feature Layers option of the Training Mode drop-down menu allows you to perform
    this experiment. The result is shown in panel C of [figure 5.3](#ch05fig03). There
    are a few noteworthy differences from the results in panel A:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不冻结特征层的权重会怎样？在训练模式下拉菜单中选择**不冻结特征层**选项可以进行此实验。结果显示在[图 5.3](#ch05fig03)的 C
    面板中。与 A 面板的结果相比，有几个值得注意的差异：
- en: 'With no feature-layer freezing, the loss value starts off higher (for instance,
    after the first epoch: 0.37 versus 0.27); the accuracy starts off lower (0.87
    versus 0.91). Why is this the case? When the pretrained model is first starting
    to be trained on the new dataset, the predictions will contain a large number
    of errors because the pretrained weights generate essentially random predictions
    for the five new digits. As a result, the loss function will have very high values
    and steep slopes. This causes the gradients calculated in the early phases of
    the training to be very large, which in turn leads to large fluctuations in all
    the model’s weights. As a result, all layers’ weights will undergo a period of
    large fluctuations, which leads to the higher initial loss seen in panel C. In
    the normal transfer-learning approach (panel A), the model’s first few layers
    are frozen and are therefore “shielded” from these large initial weight perturbations.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有特征层冻结时，损失值开始较高（例如，在第一个时期之后：0.37对比0.27）；准确率开始较低（0.87对比0.91）。为什么会这样？当预训练模型首次开始在新数据集上进行训练时，预测结果将包含大量错误，因为预训练权重为五个新数字生成基本上是随机的预测。因此，损失函数将具有非常高的值和陡峭的斜率。这导致在训练的早期阶段计算的梯度非常大，进而导致所有模型的权重出现大幅波动。因此，所有层的权重都将经历一个大幅波动的时期，这导致面板C中看到的初始损失较高。在正常的迁移学习方法（面板A）中，模型的前几层被冻结，因此免受这些大的初始权重扰动的影响。
- en: Partly due to these large initial perturbations, the final accuracy achieved
    by the no-freezing approach (~0.945, panel C) is *not* appreciably higher compared
    to that from the normal transfer-learning approach with layer freezing (~0.968,
    panel A).
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这些大的初始扰动，采用无冻结方法达到的最终准确率（约为0.945，面板C）与采用正常的迁移学习方法相比（约为0.968，面板A）*并没有*明显提高。
- en: The training takes much longer when none of the model’s layers are frozen. For
    example, on one of the laptops that we use, training the model with frozen feature
    layers takes about 30 seconds, whereas training the model without any layer freezing
    takes approximately twice as long (60 seconds). [Figure 5.4](#ch05fig04) illustrates
    the reason behind this in a schematic way. The frozen layers are taken out from
    the equation during backpropagation, which causes each batch of the `fit()` call
    to go much faster.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型的任何一层都没有被冻结时，训练时间会更长。例如，在我们使用的其中一台笔记本电脑上，使用冻结特征层训练模型大约需要30秒，而没有任何层冻结的模型训练大约需要两倍长（60秒）。[图5.4](#ch05fig04)以示意的方式说明了其中的原因。在反向传播期间，冻结的层被从方程中排除，这导致每个`fit()`调用的批次速度大大加快。
- en: 'Figure 5.4\. A schematic explanation for why freezing some layers of a model
    speeds up training. In this figure, the backpropagation path is shown by the black
    arrows pointing to the left. Panel A: when no layer is frozen, all the model’s
    weights (v[1]–v[5]) need to be updated during each training step (each batch)
    and hence will be involved in backpropagation, represented by the black arrows.
    Note that the features (x) and targets (y) are never included in backpropagation
    because their values don’t need to be updated. Panel B: by freezing the first
    few layers of the model, a subset of the weights (v[1]–v[3]) are no longer a part
    of backpropagation. Instead, they become analogous to x and y, which are just
    treated as constants that factor into the computation of the loss. As a result,
    the amount of computation it takes to perform the backpropagation decreases, and
    the training speed increases.'
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4\. 模型冻结某些层加快训练速度的示意图。在该图中，通过指向左边的黑色箭头显示了反向传播路径。面板A：当没有任何层被冻结时，所有模型的权重（v[1]–v[5]）在每个训练步骤（每个批次）中都需要更新，因此将参与反向传播，由黑色箭头表示。请注意，特征（x）和目标（y）永远不会包括在反向传播中，因为它们的值不需要更新。面板B：通过冻结模型的前几层，一部分权重（v[1]–v[3]）不再是反向传播的一部分。相反，它们变成了类似于x和y的常数，只是作为影响损失计算的因素。因此，执行反向传播所需的计算量减少，训练速度提高。
- en: '![](05fig03_alt.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig03_alt.jpg)'
- en: 'These points provide justification for the layer-freezing approach of transfer
    learning: it leverages the feature-extracting layers from the base model and protects
    them from large weight perturbations during the early phases of the new training,
    thereby achieving a higher accuracy in a shorter training period.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观点为迁移学习的层冻结方法提供了理由：它利用了基础模型的特征提取层，并在新训练的早期阶段保护它们免受大的权重扰动，从而在较短的训练周期内实现更高的准确性。
- en: Two final remarks before we move on to the next section. First, model adaptation—the
    process of retraining a model to make it work better on the input data from a
    particular user—uses techniques very similar to the ones shown here, that is,
    freezing the base layers while letting the weights of the top few layers be altered
    through training on the user-specific data. This is despite the fact that the
    problem we solved in this section didn’t involve data from a different user, but
    rather involved data with different labels. Second, you might wonder how to verify
    that a weight of a frozen layer (the conv2d layers, in this case) is indeed the
    same before and after a `fit()` call. It is not very hard to do this verification.
    We leave it as an exercise for you (see exercise 2 at the end of this chapter).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续下一节之前，有两点需要注意。首先，模型适应——重新训练模型以使其在特定用户的输入数据上更有效的过程——使用的技术与此处展示的技术非常相似，即冻结基础层，同时让顶层的权重通过对用户特定数据的训练而发生变化。尽管本节解决的问题并不涉及来自不同用户的数据，而是涉及具有不同标签的数据。其次，你可能想知道如何验证冻结层（在这种情况下是
    conv2d 层）的权重在`fit()`调用之前和之后是否确实相同。这个验证并不是很难做到的。我们把它留给你作为一个练习（参见本章末尾的练习 2）。
- en: '5.1.2\. Transfer learning on incompatible output shapes: Creating a new m-
    model using outputs from the base model'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 不兼容输出形状上的迁移学习：使用基础模型的输出创建一个新的 m 模型
- en: 'In the example of transfer learning seen in the previous section, the base
    model had the same output shape as the new output shape. This property doesn’t
    hold in many other transfer-learning cases (see [figure 5.5](#ch05fig05)). For
    example, if you want to use the base model trained initially on the five digits
    to classify *four* new digits, the approach previously described will not work.
    A more common scenario is the following: given a deep convnet that has been trained
    on the ImageNet classification dataset consisting of 1,000 output classes, you
    have an image-classification task at hand that involves a much smaller number
    of output classes (case B in [figure 5.5](#ch05fig05)). Perhaps it is a binary-classification
    problem—whether the image contains a human face or not—or perhaps it is a multiclass-classification
    problem with only a handful of classes—what kind of commodity item a picture contains
    (recall the example at the beginning of this chapter). In such cases, the base
    model’s output shape doesn’t work for the new problem.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中看到的迁移学习示例中，基础模型的输出形状与新输出形状相同。这种属性在许多其他迁移学习案例中并不成立（参见[图 5.5](#ch05fig05)）。例如，如果你想要使用最初在五个数字上进行训练的基础模型来对*四个*新数字进行分类，先前描述的方法将不起作用。更常见的情况是：给定一个已经在包含
    1,000 个输出类别的 ImageNet 分类数据集上训练过的深度卷积网络，你手头有一个涉及更少输出类别的图像分类任务（[图 5.5](#ch05fig05)
    中的 B 案例）。也许这是一个二元分类问题——图像是否包含人脸——或者这是一个具有少数类别的多类分类问题——图片中包含什么类型的商品（回想一下本章开头的例子）。在这种情况下，基础模型的输出形状对于新问题不起作用。
- en: 'Figure 5.5\. Transfer learning can be divided into three types according to
    whether the output shape and activation of the new model are the same as or different
    from those of the original model. Case A: the output shape and the activation
    function of the new model match those of the base model. The transfer of the MNIST
    model onto new digits in [section 5.1.1](#ch05lev2sec1) is an example of this
    type of transfer learning. Case B: the new model has the same activation type
    as the base model because the original task and the new task are of the same type
    (for example, both are multiclass classification). However, the output shapes
    are different (for instance, the new task involves a different number of classes).
    Examples of this type of transfer learning can be found in [section 5.1.2](#ch05lev2sec2)
    (controlling a video game in the style of Pac-Man^(TM 4) through a webcam) and
    5.1.3 (recognizing a new set of spoken words). Case C: the new task is of a different
    type from the original one (such as regression versus classification). The object-detection
    model based on MobileNet is an example of this type.'
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.5\. 根据新模型的输出形状和激活方式是否与原模型相同，迁移学习可分为三种类型。情况 A：新模型的输出形状和激活函数与基础模型相匹配。将 MNIST
    模型迁移到[5.1.1节](#ch05lev2sec1)中的新数字就是这种类型的迁移学习示例。情况 B：新模型具有与基础模型相同的激活类型，因为原任务和新任务是相同类型的（例如，都是多类分类）。然而，输出形状不同（例如，新任务涉及不同数量的类）。这种类型的迁移学习示例可在[5.1.2节](#ch05lev2sec2)（通过网络摄像头控制类似于
    Pac-Man^(TM 4) 的视频游戏）和 5.1.3（识别一组新的口语单词）中找到。情况 C：新任务与原始任务的类型不同（例如，回归与分类）。基于 MobileNet
    的目标检测模型就是这种类型的示例。
- en: '![](05fig04_alt.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig04_alt.jpg)'
- en: In some cases, even the *type* of machine-learning task is different from the
    one the base model has been trained on. For instance, you can perform a regression
    task (predict a number, as in case C in [figure 5.5](#ch05fig05)) by applying
    transfer learning on the base model trained on a classification task. In [section
    5.2](#ch05lev1sec2), you will see a still more intriguing use of transfer learning—predicting
    an array of numbers, instead of a single one, for the purpose of detecting and
    localizing objects in images.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，甚至机器学习任务的*类型*也与基础模型训练的类型不同。例如，您可以通过对分类任务训练的基础模型应用迁移学习来执行回归任务（预测一个数字，如图
    5.5 中的情况 C）[5.2节](#ch05lev1sec2)中，您将看到迁移学习的更加有趣的用途——预测一系列数字，而不是单个数字，用于在图像中检测和定位对象。
- en: These cases all involve a desired output shape that differs from that of the
    base model. This makes it necessary to construct a new model. But because we are
    doing transfer learning, the new model will not be created from scratch. Instead,
    it will use the base model. We will illustrate how to do this in the webcam-transfer-learning
    example in the tfjs-examples repository.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情况都涉及期望的输出形状与基础模型不同。这使得需要构建一个新模型。但因为我们正在进行迁移学习，所以新模型不会从头开始创建。相反，它将使用基础模型。我们将在
    tfjs-examples 存储库中的 webcam-transfer-learning 示例中说明如何做到这一点。
- en: 'To see this example in action, make sure your machine has a front-facing camera—the
    example will collect the data for transfer learning from the camera. Most laptops
    and tablet computers come with a built-in front-facing camera nowadays. If you
    are using a desktop computer, however, you may need to find a webcam and attach
    it to the machine. Similar to the previous examples, you can use the following
    commands to check out and run the demo:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此示例的实际操作，请确保您的设备具有前置摄像头——示例将从摄像头收集用于迁移学习的数据。现在大多数笔记本电脑和平板电脑都配备了内置的前置摄像头。但是，如果您使用的是台式电脑，可能需要找到一个网络摄像头并将其连接到设备上。与之前的示例类似，您可以使用以下命令来查看和运行演示：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This fun demo turns your webcam into a game controller by applying transfer
    learning on a TensorFlow.js implementation of MobileNet, and lets you play the
    Pac-Man game with it. Let’s walk through the three steps it takes to run the demo:
    data collection, model transfer learning, and playing.^([[4](#ch05fn4)])'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有趣的演示将您的网络摄像头转换为游戏控制器，通过对 MobileNet 的 TensorFlow.js 实现进行迁移学习，让您可以用它玩 Pac-Man
    游戏。让我们走过运行演示所需的三个步骤：数据收集、模型迁移学习和游戏进行^([[4](#ch05fn4)])。
- en: ⁴
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pac-Man is a trademark of Bandai Namco Entertainment Inc.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pac-Man 是万代南梦宫娱乐公司的商标。
- en: The data for transfer learning is collected from your webcam. Once the demo
    is running in your browser, you will see four black squares in the bottom-right
    part of the page. They are arranged in a way similar to the four direction buttons
    on a Nintendo Family Computer controller. They correspond to the four classes
    that the model will be trained to recognize in real time. These four classes correspond
    to the four directions in which Pac-Man will go. When you click and hold one of
    them, images will be collected via the webcam at a rate of 20–30 frames per second.
    A number beneath the square tells you how many images have been collected for
    this controller direction so far.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的数据来自于您的网络摄像头。一旦演示在您的浏览器中运行，您将在页面右下角看到四个黑色方块。它们的排列方式类似于任天堂家庭电脑控制器上的四个方向按钮。它们对应着模型将实时识别的四个类别。这四个类别对应着
    Pac-Man 将要移动的四个方向。当您点击并按住其中一个时，图像将以每秒 20–30 帧的速度通过网络摄像头收集。方块下面的数字告诉您目前已经为此控制器方向收集了多少图像。
- en: For the best transfer-learning quality, make sure you 1) collect at least 50
    images per class, and 2) move and wiggle your head and face around a little bit
    during the data collection so that the training images contain more diversity,
    which benefits the robustness of the model you’ll get from the transfer learning.
    In this demo, most people turn their heads in the four directions (up, down, left,
    and right; see [figure 5.6](#ch05fig06)) to indicate which way Pac-Man should
    go. But you can use any head positions, facial expressions, or even hand gestures
    that you desire as the input images, as long as the inputs are sufficiently visually
    distinct from one class to another.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳的迁移学习质量，请确保您：1）每个类别至少收集 50 张图像；2）在数据收集过程中稍微移动和摆动您的头部和面部，以使训练图像包含更多的多样性，这有利于您从迁移学习中获得的模型的稳健性。在这个演示中，大多数人会在四个方向（上、下、左、右；参见[图5.6](#ch05fig06)）转动头部，以指示
    Pac-Man 应该朝哪个方向移动。但您可以使用任何您想要的头部位置、面部表情甚至手势作为输入图像，只要输入图像在各个类别之间足够视觉上有区别即可。
- en: Figure 5.6\. The UI of the webcam-transfer-learning example^([[5](#ch05fn5)])
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6\. 网络摄像头迁移学习示例的用户界面^([[5](#ch05fn5)])
- en: ⁵
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The UI of this webcam-transfer-learning example is the work of Jimbo Wilson
    and Shan Carter. A video recording of this fun example in action is available
    at [https://youtu.be/YB-kfeNIPCE?t=941](https://youtu.be/YB-kfeNIPCE?t=941).
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个网络摄像头迁移学习示例的用户界面是由吉姆博·威尔逊（Jimbo Wilson）和山姆·卡特（Shan Carter）完成的。您可以在 [https://youtu.be/YB-kfeNIPCE?t=941](https://youtu.be/YB-kfeNIPCE?t=941)
    查看这个有趣示例的视频录制。
- en: '![](05fig05_alt.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig05_alt.jpg)'
- en: After collecting the training images, click the Train Model button, which will
    start the transfer-learning process. Transfer learning should take only a few
    seconds. As it progresses, you should see the loss value displayed on the screen
    get smaller and smaller until it reaches a very small positive value (such as
    0.00010) and stops changing. At this point, the transfer-learning model has been
    trained, and you can use it to play the game. To start the game, just click the
    Play button and wait for game state to settle. The model will then start performing
    real-time inference on the stream of images from the webcam. At each video frame,
    the winning class (the class with the highest probability score assigned by the
    transfer-learning model) will be indicated in the bottom-right part of the UI
    with bright yellow highlighting. In addition, it will cause Pac-Man to move in
    the corresponding direction (unless blocked by a wall).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 收集完训练图像后，点击“训练模型”按钮，这将开始迁移学习过程。迁移学习应该只需要几秒钟。随着进展，您应该看到屏幕上显示的损失值变得越来越小，直到达到一个非常小的正值（例如
    0.00010），然后停止变化。此时，迁移学习模型已经被训练好了，您可以用它来玩游戏了。要开始游戏，只需点击“播放”按钮，等待游戏状态稳定下来。然后，模型将开始对来自网络摄像头的图像流进行实时推理。在每个视频帧中，赢得的类别（由迁移学习模型分配的概率分数最高的类别）将在用户界面的右下角用明亮的黄色突出显示。此外，它会导致
    Pac-Man 沿着相应的方向移动（除非被墙壁挡住）。
- en: This demo might look like magic to those unfamiliar with machine learning, but
    it is based on nothing more than a transfer-learning algorithm that uses MobileNet
    to perform a four-class classification task. The algorithm uses the small amount
    of image data collected through the webcam. Those images are conveniently labeled
    through the click-and-hold action you performed while collecting the images. Thanks
    to the power of transfer learning, this process doesn’t need much data or much
    training time (it even works on a smartphone). So, that is how this demo works
    in a nutshell. If you wish to understand the technical details, dive deep with
    us into the underlying TensorFlow.js code in the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对机器学习不熟悉的人来说，这个演示可能看起来像魔术一样，但它基于的只是一个使用 MobileNet 执行四类分类任务的迁移学习算法。该算法使用通过网络摄像头收集的少量图像数据。这些图像通过您收集图像时执行的点击和按住操作方便地标记。由于迁移学习的力量，这个过程不需要太多数据或太多的训练时间（它甚至可以在智能手机上运行）。这就是这个演示的工作原理的简要概述。如果您希望了解技术细节，请在下一节中与我们一起深入研究底层的
    TensorFlow.js 代码。
- en: Deep dive into webcam transfer learning
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深入研究网络摄像头迁移学习
- en: The code in [listing 5.3](#ch05ex03) (from webcam-transfer-learning/index.js)
    is responsible for loading the base model. In particular, we load a version of
    MobileNet that can run efficiently in TensorFlow.js. [Info box 5.1](#ch05sb01)
    describes how this model is converted from the Keras deep-learning library in
    Python. As soon as the model is loaded, we use the `getLayer()` method to get
    hold of one of its layers. `getLayer()` allows you to specify a layer by its name
    (`'conv_pw_13_relu'` in this case). You may recall another way to access a model’s
    layers from [section 2.4.2](kindle_split_013.html#ch02lev2sec17)—that is, by indexing
    into the model’s `layers` attribute, which holds all the model’s layers as a JavaScript
    array. This approach is easy to use only when the model consists of a small number
    of layers. The MobileNet model we are dealing with here has 93 layers, which makes
    that approach fragile (for example, what if more layers get added to the model
    in the future?). Therefore, the name-based `getLayer()` approach is more reliable,
    if we assume the authors of MobileNet will keep the names of the key layers unchanged
    when they release new versions of the model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.3](#ch05ex03) 中的代码（来自 webcam-transfer-learning/index.js）负责加载基础模型。特别地，我们加载了一个可以在
    TensorFlow.js 中高效运行的 MobileNet 版本。[信息框 5.1](#ch05sb01) 描述了这个模型是如何从 Python 的 Keras
    深度学习库转换而来的。一旦模型加载完成，我们使用 `getLayer()` 方法来获取其中一个层。`getLayer()` 允许您通过名称（在本例中为 `''conv_pw_13_relu''`）指定一个层。您可能还记得另一种从
    [第 2.4.2 节](kindle_split_013.html#ch02lev2sec17) 访问模型层的方法——即通过索引到模型的 `layers`
    属性，该属性将所有模型的层作为 JavaScript 数组保存。当模型由少量层组成时，这种方法很容易使用。我们正在处理的 MobileNet 模型有 93
    层，这使得这种方法变得脆弱（例如，如果将来向模型添加更多层会发生什么？）。因此，基于名称的 `getLayer()` 方法更可靠，如果我们假设 MobileNet
    的作者在发布新版本模型时会保持关键层的名称不变的话。'
- en: Listing 5.3\. Loading MobileNet and creating a “truncated” model from it
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3\. 加载 MobileNet 并从中创建一个“截断”模型
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1*** URLs under [storage.google.com/tfjs-models](http://storage.google.com/tfjs-models)
    are designed to be permanent and stable.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** [storage.google.com/tfjs-models](http://storage.google.com/tfjs-models)
    下的 URL 设计为永久和稳定的。'
- en: '***2*** Gets an intermediate layer of the MobileNet. This layer contains features
    useful for the custom image-classification task.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取 MobileNet 的一个中间层。这个层包含对于自定义图像分类任务有用的特征。'
- en: '***3*** Creates a new model that is the same as MobileNet except that it ends
    at the ''conv_pw_13_relu'' layer, that is, with the last few layers (referred
    to as the “head”) truncated'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 创建一个新模型，它与 MobileNet 相同，只是它在 ''conv_pw_13_relu'' 层结束，也就是说，最后几层（称为“头部”）被截断'
- en: '|  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Converting models from Python Keras into the TensorFlow .js format**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**将 Python Keras 模型转换为 TensorFlow .js 格式**'
- en: 'TensorFlow.js features a high degree of compatibility and interoperability
    with Keras, one of the most popular Python deep-learning libraries. One of the
    benefits that stems from this compatibility is that you can utilize many of the
    so-called “applications” from Keras. These applications are a set of pretrained
    deep convnets (see [https://keras.io/applications/](https://keras.io/applications/)).
    The authors of Keras have painstakingly trained these convnets on large datasets
    such as ImageNet and made them available via the library so that they are ready
    for reuse, including inference and transfer learning, as we are doing here. For
    those who use Keras in Python, importing an application takes just one line of
    code. Due to the interoperability previously mentioned, it is also easy for a
    TensorFlow.js user to use these applications. Here are the steps it takes:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 具有与 Keras 高度兼容和互操作的特性，Keras 是最受欢迎的 Python 深度学习库之一。从这种兼容性中获益的其中一个好处是，你可以利用
    Keras 中的许多所谓的“应用程序”。这些应用程序是一组在大型数据集（如 ImageNet）上预训练的深度卷积神经网络（详见 [https://keras.io/applications/](https://keras.io/applications/)）。Keras
    的作者们已经在库中辛苦地对这些卷积神经网络进行了训练，并使它们可通过库随时重用，包括推理和迁移学习，就像我们在这里所做的那样。对于在 Python 中使用
    Keras 的人来说，导入一个应用程序只需一行代码。由于前面提到的互操作性，一个 TensorFlow.js 用户也很容易使用这些应用程序。以下是所需步骤：
- en: 'Make sure that the Python package called `tensorflowjs` is installed. The easiest
    way to install it is via the `pip` command:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保已安装名为`tensorflowjs`的 Python 包。最简单的安装方法是通过`pip`命令：
- en: '[PRE7]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the following code through a Python source file or in an interactive Python
    REPL such as ipython:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 Python 源文件或者像 ipython 这样的交互式 Python REPL 运行以下代码：
- en: '[PRE8]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first two lines import the required `keras` and `tensorflowjs` modules.
    The third line loads MobileNet into a Python object (`model`). You can, in fact,
    print a summary of the model in pretty much the same way as you print the summary
    of a TensorFlow.js model: that is, `model.summary()`. You can see that the last
    layer of the model (the model’s output) indeed has a shape of `(None, 1000)` (equivalent
    to `[null, 1000]` in JavaScript), reflecting the 1,000-class ImageNet classification
    task that the MobileNet model was trained on. The keyword argument `alpha=0.25`
    that we specified for this constructor call chooses a version of MobileNet that
    is smaller in size. You may choose larger values of `alpha` (such as `0.75, 1`),
    and the same conversion code will continue to work.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前两行导入了所需的`keras`和`tensorflowjs`模块。第三行将 MobileNet 加载到一个 Python 对象（`model`）中。实际上，你可以以几乎与打印
    TensorFlow.js 模型摘要相同的方式打印模型的摘要：即`model.summary()`。你可以看到模型的最后一层（模型的输出）确实具有形状`(None,
    1000)`（在 JavaScript 中相当于`[null, 1000]`），反映了 MobileNet 模型在 ImageNet 分类任务上训练的 1000
    类。我们为这个构造函数调用指定的`alpha=0.25`关键字参数选择了一个更小的 MobileNet 版本。你可以选择更大的`alpha`值（如`0.75,
    1`），同样的转换代码仍将继续工作。
- en: The last line in the previous code snippet saves the model to the specified
    directory on the disk using a method from the tensorflowjs module. After the line
    finishes running, there will be a new directory at /tmp/mobilenet_0.25, with content
    that looks like
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前一代码片段中的最后一行使用了`tensorflowjs`模块中的一个方法，将模型保存到指定目录中。在该行运行结束后，将在磁盘上的/tmp/mobilenet_0.25路径下创建一个新目录，其内容如下所示：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is exactly the same format as the one we saw in section 4.3.3, when we
    showed how to save a trained TensorFlow.js model to disk using its `save()` method
    in the Node.js version of TensorFlow.js. Therefore, to the TensorFlow.js-based
    programs that load this converted model from disk, the saved format is identical
    to a model created and trained in TensorFlow.js: it can simply call the `tf.loadLayersModel()`
    method and point at the path to the model.json file (either in the browser or
    in Node.js), which is exactly what happens in [listing 5.3](#ch05ex03).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在第 4.3.3 节中看到的格式完全相同，当时我们展示了如何在 Node.js 版本的 TensorFlow.js 中使用其`save()`方法将训练好的
    TensorFlow.js 模型保存到磁盘上。因此，对于从磁盘加载此转换模型的 TensorFlow.js 程序而言，保存的格式与在 TensorFlow.js
    中创建和训练模型的格式是相同的：它可以简单地调用`tf.loadLayersModel()`方法并指向模型.json文件的路径（无论是在浏览器中还是在 Node.js
    中），这正是 [listing 5.3](#ch05ex03) 中发生的事情。
- en: The loaded MobileNet model is ready to perform the machine-learning task that
    the model was originally trained on—classify input images into the 1,000 classes
    of the ImageNet dataset. Note that this particular dataset has a heavy emphasis
    on animals, especially various breeds of cats and dogs (which is probably related
    to the abundance of such images on the internet!). For those interested in this
    particular usage, the MobileNet example in the tfjs-example repository illustrates
    how to do that ([https://github.com/tensorflow/tfjs-examples/tree/master/mobilenet](https://github.com/tensorflow/tfjs-examples/tree/master/mobilenet)).
    However, this direct usage of MobileNet is not what we focus on in this chapter;
    instead, we explore how to use the loaded MobileNet to perform transfer learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 载入的 MobileNet 模型已经准备好执行模型最初训练的机器学习任务——将输入图像分类为 ImageNet 数据集的 1,000 个类别。请注意，该特定数据集非常强调动物，特别是各种品种的猫和狗（这可能与互联网上此类图像的丰富性有关！）。对于对此特定用法感兴趣的人，tfjs-example
    仓库中的 MobileNet 示例展示了如何做到这一点（[https://github.com/tensorflow/tfjs-examples/tree/master/mobilenet](https://github.com/tensorflow/tfjs-examples/tree/master/mobilenet)）。然而，在本章中，我们不专注于直接使用
    MobileNet；相反，我们探讨如何使用载入的 MobileNet 进行迁移学习。
- en: The `tfjs.converters.save_keras_model()` method shown previously is capable
    of converting and saving not only MobileNet but also other Keras applications,
    such as DenseNet and NasNet. In exercise 3 at the end of this chapter, you will
    practice converting another Keras application (MobileNetV2) into the TensorFlow.js
    format and loading it in the browser. Furthermore, it should be pointed out that
    `tfjs.converters .save_keras_model()` is generally applicable to any model objects
    you have created or trained in Keras, not just models from `keras.applications`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 先前展示的 `tfjs.converters.save_keras_model()` 方法能够转换和保存不仅是 MobileNet 还有其他 Keras
    应用，例如 DenseNet 和 NasNet。在本章末尾的练习 3 中，您将练习将另一个 Keras 应用（MobileNetV2）转换为 TensorFlow.js
    格式并在浏览器中加载它。此外，应指出 `tfjs.converters.save_keras_model()` 通常适用于您在 Keras 中创建或训练的任何模型对象，而不仅仅是来自
    `keras.applications` 的模型。
- en: '|  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: What do we do with the `conv_pw_13_relu` layer once we get hold of it? We create
    a new model that contains the layers of the original MobiletNet model from its
    first (input) layer to the `conv_pw_13_relu` layer. This is the first time you
    see this kind of model construction in this book, so it requires some careful
    explanation. For that, we need to introduce the concept of a *symbolic tensor*
    first.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得 `conv_pw_13_relu` 层，我们该怎么做？我们创建一个包含原始 MobiletNet 模型层的新模型，从其第一（输入）层到 `conv_pw_13_relu`
    层。这是本书中首次看到这种模型构建方式，因此需要一些仔细的解释。为此，我们首先需要介绍*符号张量*的概念。
- en: Creating models from symbolic tensors
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建符号张量模型
- en: You have seen tensors so far. `Tensor` is the basic data type (also abbreviated
    as *dtype*) in TensorFlow.js. A tensor object carries concrete numeric values
    of a given shape and dtype, backed by storage on WebGL textures (if in a WebGL-enabled
    browser) or CPU/GPU memory (if in Node.js). However, `SymbolicTensor` is another
    important class in TensorFlow.js. Instead of holding concrete values, a symbolic
    tensor specifies only a shape and a dtype. A symbolic tensor can be thought of
    as a “slot” or a “placeholder,” into which an actual tensor value may be inserted
    later, given that the tensor value has a compatible shape and dtype. In TensorFlow.js,
    a layer or model object takes one or more inputs (so far, you’ve only seen cases
    of one input), and those are represented as one or more symbolic tensors.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了张量。`Tensor` 是 TensorFlow.js 中的基本数据类型（也缩写为 *dtype*）。一个张量对象携带着给定形状和
    dtype 的具体数值，支持由 WebGL 纹理（如果在启用 WebGL 的浏览器中）或 CPU/GPU 内存（如果在 Node.js 中）支持的存储。然而，`SymbolicTensor`
    是 TensorFlow.js 中另一个重要的类。与携带具体值不同，符号张量仅指定形状和 dtype。可以将符号张量视为“槽”或“占位符”，可以稍后插入一个实际张量值，前提是张量值具有兼容的形状和
    dtype。在 TensorFlow.js 中，层或模型对象接受一个或多个输入（到目前为止，您只看到了一个输入的情况），这些输入被表示为一个或多个符号张量。
- en: 'Let’s use an analogy that might help you understand a symbolic tensor. Consider
    a function in a programming language like Java or TypeScript (or any other statically
    typed language you are familiar with). The function takes one or more input arguments.
    Each argument of a function has a type, which stipulates what kind of variables
    may be passed in as the argument. However, the argument *itself* doesn’t hold
    any concrete values. By itself, the argument is just a placeholder. A symbolic
    tensor is analogous to a function argument: it specifies what kind (combination
    of shape^([[6](#ch05fn6)]) and dtype) of tensors may be used in that slot. By
    parallel, a function in a statically typed language has a return type. This is
    comparable to the output symbolic tensor of a model or layer object. It is a “blueprint”
    for the shape and dtype of the actual tensor values that the model or layer object
    will output.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个类比来帮助你理解符号张量。想象一下编程语言（比如 Java 或 TypeScript，或者其他你熟悉的静态类型语言）中的函数。函数接受一个或多个输入参数。函数的每个参数都有一个类型，规定了可以作为参数传递的变量类型。然而，参数本身并不包含任何具体的值。参数本身只是一个占位符。符号张量类似于函数的参数：它指定了可以在该位置使用的张量的种类（形状和
    dtype 的组合）。类似地，静态类型语言中的函数有一个返回类型。这与模型或层对象的输出符号张量相似。它是模型或层对象输出的实际张量值形状和 dtype 的“蓝图”。
- en: ⁶
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A difference between a tensor’s shape and a symbolic tensor’s shape is that
    the former always has fully specified dimensions (such as `[8, 32, 20]`), while
    the latter may have undetermined dimensions (such as `[null, null, 20]`). You
    have already seen this in the “Output shape” column of the model summaries.
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 张量形状和符号张量形状之间的区别在于前者始终具有完全指定的维度（比如 `[8, 32, 20]`），而后者可能具有未确定的维度（比如 `[null, null,
    20]`）。你已经在模型摘要的“输出形状”列中见过这一点。
- en: 'In TensorFlow.js, two important attributes of a model object are its inputs
    and outputs. Each of these is an array of symbolic tensors. For a model with exactly
    one input and exactly one output, both arrays have a length of 1\. Similarly,
    a layer object has two attributes: input and output, each of which is a symbolic
    tensor. Symbolic tensors can be used to create a new model. This is a new way
    of creating models in TensorFlow.js, which is different from the approach you’ve
    seen before: namely, creating sequential models with `tf.sequential()` and subsequent
    calls to the `add()` method. In the new approach, we use the `tf.model()` function,
    which takes a configuration object with two mandatory fields: `inputs` and `outputs`.
    The `inputs` field is required to be a symbolic tensor (or, alternatively, an
    array of symbolic tensors), and likewise for the `outputs` field. Therefore, we
    can obtain the symbolic tensors from the original MobileNet model and feed them
    to a `tf.model()` call. The result is a new model that consists of a part of the
    original MobileNet.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow.js 中，模型对象的两个重要属性是其输入和输出。这两者都是符号张量的数组。对于具有一个输入和一个输出的模型，这两个数组的长度都为1。类似地，层对象具有两个属性：输入和输出，每个都是一个符号张量。符号张量可以用于创建新模型。这是
    TensorFlow.js 中创建模型的新方法，与你之前见过的方法有所不同：即使用 `tf.sequential()` 创建顺序模型，然后调用 `add()`
    方法。在新方法中，我们使用 `tf.model()` 函数，它接受一个包含两个必填字段 `inputs` 和 `outputs` 的配置对象。`inputs`
    字段需要是一个符号张量（或者是一个符号张量数组），`outputs` 亦然。因此，我们可以从原始 MobileNet 模型中获取符号张量，并将它们提供给 `tf.model()`
    调用。结果是一个由原始 MobileNet 的一部分组成的新模型。
- en: This process is illustrated schematically in [figure 5.7](#ch05fig07). (Note
    that the figure reduces the number of layers from the actual MobileNet model for
    the sake of a simple-looking diagram.) The important thing to realize is that
    the symbolic tensors taken from the original model and handed to the `tf.model()`
    call are *not* isolated objects. Instead, they carry information about what layers
    they belong to and how the layers are connected to each other. For readers familiar
    with graphs in data structure, the original model is a graph of symbolic tensors,
    with the connecting edges being the layers. By specifying the inputs and outputs
    of the new model as symbolic tensors in the original model, we are extracting
    a subgraph of the original MobileNet graph. The subgraph, which becomes the new
    model, contains the first few (in particular, the first 87) layers of MobileNet,
    while the last 6 layers are left out. The last few layers of a deep convnet are
    sometimes referred to as the *head*. What we are doing with the `tf.model()` call
    can be referred to as *truncating* the model. The truncated MobileNet preserves
    the feature-extracting layers while discarding the head. Why does the head contain
    *six* layers? This is because those layers are specific to the 1,000-class classification
    task that the MobileNet was originally trained on. The layers are not useful for
    the four-class classification task we are facing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在 [图 5.7](#ch05fig07) 中以示意图形式说明。（请注意，为了简单的图示，该图将实际 MobileNet 模型的层数减少了。）重要的是要意识到，从原始模型中提取的符号张量并不是孤立的对象。相反，它们携带关于它们属于哪些层以及层如何相互连接的信息。对于熟悉数据结构中图的读者来说，原始模型是一个符号张量的图，连接边是层。通过在原始模型中指定新模型的输入和输出为符号张量，我们正在提取原始
    MobileNet 图的一个子图。这个子图成为新模型，包含 MobileNet 的前几层（特别是前 87 层），而最后 6 层则被略过。深度卷积网络的最后几层有时被称为*头部*。我们在
    `tf.model()` 调用中所做的可以称为*截断*模型。截断的 MobileNet 保留了提取特征的层，同时丢弃了头部。为什么头部包含*六*层？这是因为这些层是
    MobileNet 最初训练的 1,000 类分类任务所特有的。这些层对我们面对的四类分类任务没有用处。
- en: Figure 5.7\. A schematic drawing that explains how the new (“truncated”) model
    is created from MobileNet. See the `tf.model()` call in [listing 5.3](#ch05ex03)
    for the corresponding code. Each layer has an input and an output, both of which
    are `SymbolicTensor` instances. In the original model, SymbolicTensor0 is the
    input of the first layer and the input of the entire model. It is used as the
    input symbolic tensor of the new model. In addition, we take the output symbolic
    tensor of an intermediate layer (equivalent to `conv_pw_13_relu`) as the output
    tensor of the new model. Hence, we get a model that consists of the first two
    layers of the original model, shown in the bottom part of the diagram. The last
    layer of the original model, which is the output layer and sometimes referred
    to as the model’s head, is discarded. This is why approaches like this are sometimes
    referred to as *truncating* a model. Note that this diagram depicts models with
    small numbers of layers for the sake of clarity. What actually happens with the
    code in [listing 5.3](#ch05ex03) involves a model with many more (93) layers compared
    to the one shown in this diagram.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.7\. 示意图解释了如何从 MobileNet 创建新的（“截断的”）模型。在 [代码清单 5.3](#ch05ex03) 中的 `tf.model()`
    调用中查看相应的代码。每一层都有一个输入和一个输出，都是 `SymbolicTensor` 实例。在原始模型中，SymbolicTensor0 是第一层的输入，也是整个模型的输入。它被用作新模型的输入符号张量。此外，我们将中间层的输出符号张量（相当于
    `conv_pw_13_relu`）作为新模型的输出张量。因此，我们得到一个由原始模型的前两层组成的模型，如图的底部所示。原始模型的最后一层，即输出层，有时被称为模型的头部，被丢弃。这就是为什么有时会将这样的方法称为*截断*模型的原因。请注意，这个图示了具有少量层的模型，以便清楚地表达。实际上，在
    [代码清单 5.3](#ch05ex03) 中的代码涉及一个比这个图示的层多得多（93 层）的模型。
- en: '![](05fig06_alt.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig06_alt.jpg)'
- en: Transfer learning based on embeddings
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于嵌入的迁移学习
- en: The output of the truncated MobileNet is the activation of an intermediate layer
    of the original MobileNet.^([[7](#ch05fn7)]) But how is intermediate-layer activation
    from MobileNet useful to us? The answer can be seen in the function that handles
    the events of clicking and holding each of the four black squares ([listing 5.4](#ch05ex04).)
    Every time an input image is available from the webcam (via the `capture()` method),
    we call the `predict()` method of the truncated MobileNet and save the output
    in an object called `controllerDataset`, which will be used for transfer learning
    later.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 截断的 MobileNet 的输出是原始 MobileNet 的中间层激活。但是 MobileNet 的中间层激活对我们有何用呢？答案可以在处理每个四个黑色方块的点击和保持事件的函数中看到（[列表
    5.4](#ch05ex04)）每当摄像头可用的时候（通过 `capture()` 方法），我们调用截断的 MobileNet 的 `predict()`
    方法，并将输出保存在一个名为 `controllerDataset` 的对象中，稍后将用于迁移学习。
- en: ⁷
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A frequently asked question about TensorFlow.js models is how to obtain the
    activations of intermediate layers. The approach we showed here is the answer.
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有关 TensorFlow.js 模型的常见问题是如何获取中间层的激活。我们展示的方法就是答案。
- en: But how to interpret the output of the truncated MobileNet? For every image
    input, it is a tensor of shape `[1, 7, 7, 256]`. It is not the probabilities for
    any classification problem, nor is it the values predicted for any regression
    problem. It is a representation of the input image in a certain high-dimensional
    space. This space has 7 * 7 * 256, or approximately 12.5k, dimensions. Although
    the space has a lot of dimensions, it is lower-dimensional compared to the original
    image, which, due to the 224 × 224 image dimensions and three color channels,
    has 224 * 224 * 3 ≈ 150k dimensions. So, the output from the truncated MobileNet
    can be viewed as an efficient representation of the image. This kind of lower-dimension
    representation of inputs is often referred to as an *embedding*. Our transfer
    learning will be based on the embeddings of the four sets of images collected
    from the webcam.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如何解释截断的 MobileNet 的输出？对于每个图像输入，它都是一个形状为 `[1, 7, 7, 256]` 的张量。它不是任何分类问题的概率，也不是任何回归问题的预测值。它是输入图像在某个高维空间中的表示。该空间具有
    7 * 7 * 256，约为 12.5k，维度。尽管空间具有很多维度，但与原始图像相比，它是低维的，原始图像由于具有 224 × 224 的图像尺寸和三个颜色通道，有
    224 * 224 * 3 ≈ 150k 个维度。因此，截断的 MobileNet 的输出可以被视为图像的有效表示。这种输入的低维表示通常称为*嵌入*。我们的迁移学习将基于从网络摄像头收集到的四组图像的嵌入。
- en: Listing 5.4\. Obtaining image embeddings using a truncated MobileNet
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4\. 使用截断的 MobileNet 获取图像嵌入
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1*** Uses tf.tidy() to clean up intermediate tensors such as img. See [appendix
    B](kindle_split_030.html#app02), section B.3 for a tutorial on TensorFlow.js memory
    management in the browser.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 使用 tf.tidy() 来清理中间张量，比如 img。有关在浏览器中使用 TensorFlow.js 内存管理的教程，请参见[附录
    B](kindle_split_030.html#app02)，第 B.3 节。'
- en: '***2*** Gets MobileNet’s internal activation for the input image'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取 MobileNet 的输入图像的内部激活'
- en: Now that we have a way to get the embeddings of the webcam images, how do we
    use them to predict what direction a given image corresponds to? For this, we
    need a new model, one that takes the embedding as its input and outputs the probability
    values for the four direction classes. The code in the following listing (from
    index.js) creates such a model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了获取网络摄像头图像嵌入的方法，我们如何使用它们来预测给定图像对应的方向呢？为此，我们需要一个新模型，该模型以嵌入作为其输入，并输出四个方向类的概率值。以下代码（来自
    index.js）创建了这样一个模型。
- en: Listing 5.5\. Predicting controller direction using image embeddings
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.5\. 使用图像嵌入预测控制器方向
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***1*** Flattens the [7, 7, 256] embedding from the truncated MobileNet. The
    slice(1) operation discards the first (batch) dimension, which is present in the
    output shape but unwanted by the inputShape attribute of the layer’s factory method,
    so it can be used with a dense layer.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 将截断的 MobileNet 的[7, 7, 256]嵌入层展平。slice(1) 操作丢弃了第一个（批次）维度，该维度存在于输出形状中，但是不需要在层的工厂方法的
    inputShape 属性中，因此它可以与密集层一起使用。'
- en: '***2*** A first (hidden) dense layer with nonlinear (relu) activation'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 一个具有非线性（relu）激活的第一个（隐藏的）密集层'
- en: '***3*** The number of units of the last layer should correspond to the number
    of classes we want to predict.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 最后一层的单元数应该与我们想要预测的类的数量相对应。'
- en: 'Compared to the truncated MobileNet, the new model created in [listing 5.5](#ch05ex05)
    has a much smaller size. It consists of only three layers:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MobileNet 截断版相比，[清单 5.5](#ch05ex05) 中创建的新模型具有更小的尺寸。它仅由三层组成：
- en: The input layer is a flatten layer. It transforms the 3D embedding from the
    truncated model into a 1D tensor that subsequent dense layers can take. We have
    seen similar uses of flatten layers in the MNIST convnets in [chapter 4](kindle_split_015.html#ch04).
    We let its `inputShape` match the output shape of the truncated MobileNet (without
    the batch dimension) because the new model will be fed embeddings that come out
    of the truncated MobileNet.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层是一个展平层。它将来自截断模型的 3D 嵌入转换为 1D 张量，以便后续的密集层可以采用。我们在 `inputShape` 中设置其与截断的 MobileNet
    的输出形状匹配（不包括批处理维度），因为新模型将接收来自截断的 MobileNet 的嵌入。
- en: The second layer is a hidden layer. It is hidden because it is neither the input
    layer nor the output layer of the model. Instead, it is sandwiched between two
    other layers in order to enhance the model’s capacity. This is very similar to
    the MLPs you encountered in [chapter 3](kindle_split_014.html#ch03). It is a hidden
    dense layer with a relu activation. Recall that in the [chapter 3](kindle_split_014.html#ch03)
    section “[Avoiding the fallacy of stacking layers without nonlinearity](kindle_split_014.html#ch03lev3sec2),”
    we discussed the importance of using a nonlinear activation for hidden layers
    like this.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是隐藏层。它是隐藏的，因为它既不是模型的输入层也不是输出层。相反，它被夹在其他两层之间，以增强模型的能力。这与[第三章](kindle_split_014.html#ch03)中遇到的
    MLP 非常相似。它是一个带有 relu 激活的密集的隐藏层。回想一下，在[第三章](kindle_split_014.html#ch03)的“避免堆叠没有非线性的层的谬论”一节中，我们讨论了使用类似这样的隐藏层的非线性激活的重要性。
- en: 'The third layer is the final (output) layer of the new model. It has a softmax
    activation that suits the multiclass classification problem we are facing (that
    is, four classes: one for each Pac-Man direction).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层是新模型的最终（输出）层。它具有适合我们面临的多类分类问题的 softmax 激活（即，四个类别：每个 Pac-Man 方向一个）。
- en: Therefore, we have essentially built an MLP on top of MobileNet’s feature-extraction
    layers. The MLP can be thought of as a new head for MobileNet, even though the
    feature extractor (the truncated MobileNet) and the head are two separate models
    in this case (see [figure 5.8](#ch05fig08)). As a result of the two-model setup,
    it is not possible to train the new head directly using the image tensors (of
    the shape `[numExamples, 224, 224, 3]`). Instead, the new head must be trained
    on the embeddings of the images—the output of the truncated MobileNet. Luckily,
    we have already collected those embedding tensors ([listing 5.4](#ch05ex04)).
    All we need to do to train the new head is call its `fit()` method on the embedding
    tensors. The code that does that inside the `train()` function in index.js is
    straightforward, and we won’t elaborate on that further.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将 MLP 建立在 MobileNet 的特征提取层的顶部。即使在这种情况下，特征提取器（截断的 MobileNet）和 MLP 都是两个分离的模型（见[图
    5.8](#ch05fig08)）。由于这种两个模型的设置，不可能直接使用图像张量（形状为`[numExamples,224,224,3]`）来训练新的 MLP。相反，新的
    MLP 必须在图像的嵌入上进行训练——即截断的 MobileNet 的输出。幸运的是，我们已经收集了那些嵌入张量（[清单 5.4](#ch05ex04)）。我们只需要在嵌入张量上调用其
    `fit()` 方法即可训练新的 MLP。在 index.js 的 `train()` 函数中执行此操作的代码十分简单，我们不再详细介绍。
- en: Figure 5.8\. A schematic of the transfer-learning algorithm that underlies the
    webcam-transfer-learning example
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.8. Webcam-transfer-learning 示例背后的迁移学习算法的概要
- en: '![](05fig07_alt.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig07_alt.jpg)'
- en: Once the transfer learning has finished, the truncated model and the new head
    will be used together to obtain probability scores from input images from the
    webcam. You can find the code in the `predict()` function in index.js, shown in
    [listing 5.6](#ch05ex06). In particular, two `predict()` calls are involved. The
    first call converts the image tensor into its embedding using the truncated MobileNet;
    the second one converts the embedding into the probability scores for the four
    directions using the new head trained with transfer learning. Subsequent code
    in [listing 5.6](#ch05ex06) obtains the winning index (the index that corresponds
    to the maximum probability score among the four directions) and uses it to steer
    the Pac-Man and update UI states. As in the previous examples, we don’t cover
    the UI part of the example because it is not central to the machine-learning algorithms.
    You may study and play with the UI code at your own pleasure using the code in
    the next listing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦迁移学习完成，截断模型和新头将一起用于从网络摄像头的输入图像获取概率分数。您可以在 index.js 的 `predict()` 函数中找到代码，显示在
    [列表 5.6](#ch05ex06) 中。特别是，涉及两个 `predict()` 调用。第一个调用将图像张量转换为其嵌入，使用截断的 MobileNet；第二个使用与迁移学习训练的新头将嵌入转换为四个方向的概率分数。[列表
    5.6](#ch05ex06) 中的随后代码获取获胜索引（在四个方向的最大概率分数中对应的索引）并使用它来控制 Pac-Man 并更新 UI 状态。与之前的示例一样，我们不涵盖示例的
    UI 部分，因为它不是机器学习算法的核心。您可以使用下一个列表中的代码自行研究和玩耍 UI 代码。
- en: Listing 5.6\. Getting the prediction from a webcam input image after transfer
    learning
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.6\. 在迁移学习后从网络摄像头输入图像获取预测
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1*** Captures a frame from the webcam'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从网络摄像头捕获一帧'
- en: '***2*** Gets the embedding from the truncatedMobileNet'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 从截断的 MobileNet 获取嵌入'
- en: '***3*** Converts the embedding into the probability scores of the four directions
    using the new head model'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 使用新头模型将嵌入转换为四个方向的概率分数'
- en: '***4*** Gets the index of the maximum probability score'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 获取最大概率分数的索引'
- en: '***5*** Downloads the index from GPU to CPU'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 将索引从 GPU 下载到 CPU'
- en: '***6*** Updates the UI according to the winning direction: steers the Pac-Man
    and updates other UI states, such as the highlighting of the corresponding “button”
    on the controller'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 根据获胜方向更新 UI：控制 Pac-Man 并更新其他 UI 状态，如控制器上相应“按钮”的突出显示'
- en: 'This concludes our discussion of the part of the webcam-transfer-learning example
    relevant to the transfer-learning algorithm. One interesting aspect of the method
    we used in this example is that the training and inference process involves two
    separate model objects. This is good for our educational purpose of illustrating
    how to get embeddings from the intermediate layers of a pretrained model. Another
    advantage of this approach is that it exposes the embeddings and makes it easier
    to apply machine-learning techniques that make direct use of these embeddings.
    An example of such techniques is *k-nearest neighbors* (kNN, discussed in [info
    box 5.2](#ch05sb02)). However, exposing the embeddings directly may also be viewed
    as a shortcoming for the following reasons:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们讨论与迁移学习算法相关的 webcam-transfer-learning 示例的部分。这个示例中我们使用的方法的一个有趣之处是训练和推断过程涉及两个独立的模型对象。这对我们的教育目的来说是有好处的，因为它说明了如何从预训练模型的中间层获取嵌入。这种方法的另一个优点是它暴露了嵌入，并且使得应用直接使用这些嵌入的机器学习技术更容易。这种技术的一个例子是*k
    最近邻*（kNN，在[信息框 5.2](#ch05sb02)中讨论）。然而，直接暴露嵌入也可能被视为以下原因的缺点：
- en: It leads to slightly more complex code. For example, the inference requires
    two `predict()` calls in order to perform inference on a single image.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这导致稍微复杂一些的代码。例如，推断需要两个 `predict()` 调用才能对单个图像执行推断。
- en: Suppose we want to save the models for use in later sessions or for conversion
    to a non-TensorFlow.js library. Then the truncated model and the new head model
    need to be saved separately, as two separate artifacts.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们希望保存模型以供以后会话使用或转换为非 TensorFlow.js 库。那么截断模型和新的头模型需要分别保存，作为两个单独的构件。
- en: In some special cases, transfer learning will involve backpropagation over certain
    parts of the base model (such as the first few layers of the truncated MobileNet).
    This is not possible when the base and the head are two separate objects.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些特殊情况下，迁移学习将涉及基础模型的某些部分的反向传播（例如截断的 MobileNet 的前几层）。当基础和头部是两个分开的对象时，这是不可能的。
- en: In the next section, we will show a way to overcome these limitations by forming
    a single model object for transfer learning. It will be an end-to-end model in
    the sense that it can transform input data in the original format into the final
    desired output.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将展示一种通过形成单个模型对象来克服这些限制的方法进行迁移学习。这将是一个端到端模型，因为它可以将原始格式的输入数据转换为最终的期望输出。
- en: '|  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**k-nearest neighbors classification based on embeddings**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于嵌入的k最近邻分类**'
- en: There are non-neural network approaches to solving classification problems in
    machine learning. One of the most famous is the k-nearest neighbors (kNN) algorithm.
    Unlike neural networks, the kNN algorithm doesn’t involve a training step and
    is easier to understand.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，解决分类问题的非神经网络方法有很多。其中最著名的之一就是k最近邻（kNN）算法。与神经网络不同，kNN算法不涉及训练步骤，更容易理解。
- en: 'We can describe how the kNN classification works in a few sentences:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用几句话来描述kNN分类的工作原理：
- en: You pick a positive integer *k* (for instance, 3).
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你选择一个正整数*k*（例如，3）。
- en: You collect a number of reference examples, each labeled with the true class.
    Usually the number of reference examples collected is at least several times larger
    than *k*. Each example is represented as a series of real-valued numbers, or a
    *vector*. This step is similar to the collection of training examples in the neural
    network approach.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你收集一些带有真实类别标签的参考示例。通常收集的参考示例数量至少是*k*的几倍。每个示例都被表示为一系列实值数字，或者一个*向量*。这一步类似于神经网络方法中的训练示例的收集。
- en: In order to predict the class of a new input, you compute the distances between
    the vector representation of the new input and those of all the reference examples.
    You then sort the distances. By doing so, you can find the *k* reference examples
    that are the closest to the input in the vector space. These are called the “*k*
    nearest neighbors” of the input (the namesake of the algorithm).
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了预测新输入的类别，你计算新输入的向量表示与所有参考示例的距离。然后对距离进行排序。通过这样做，你可以找到在向量空间中距离输入最近的*k*个参考示例。这些被称为输入的“*k*个最近邻居”（算法的名字来源）。
- en: You look at the classes of the *k* nearest neighbors and use the most common
    class among them as the prediction for the input. In other words, you let the
    *k* nearest neighbors “vote” on the predicted class.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你观察*k*个最近邻居的类别，并使用它们中最常见的类别作为输入的预测。换句话说，你让*k*个最近的邻居“投票”来预测类别。
- en: An example of this algorithm is shown in the following figure.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的一个示例如下图所示。
- en: '![](05fig08_alt.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig08_alt.jpg)'
- en: An example of kNN classification in a 2D embedding space. In this case, *k*
    = 3, and there are two classes (triangles and circles). There are five reference
    examples for the triangle class and seven for the circle class. The input example
    is represented as a square. The three nearest neighbors to the input are indicated
    by the line segments that connect them with the input. Because two of the three
    nearest neighbors are circles, the predicted class for the input example will
    be a circle.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维嵌入空间中的kNN分类示例。在这种情况下，*k*=3，有两个类别（三角形和圆形）。三角形类别有五个参考示例，圆形类别有七个。输入示例表示为一个正方形。与输入相连的三个最近邻居由连线表示。因为三个最近邻居中有两个是圆形，所以输入示例的预测类别将是圆形。
- en: As you can see from the previous description, one of the key requirements of
    the kNN algorithm is that every input example is represented as a vector. Embeddings
    like the one we obtained from the truncated MobileNet are good candidates for
    such vector representations for two reasons. First, they often have a lower dimensionality
    compared to the original inputs and hence reduce the amount of storage and computation
    required by the distance calculation. Second, the embeddings usually capture more
    important features in the input (such as important geometric features in images;
    see [figure 4.5](kindle_split_015.html#ch04fig05)) and ignore less important ones
    (for example, brightness and size) owing to the fact that they have been trained
    on a large classification dataset. In some cases, embeddings give us vector representations
    for things that are not even originally represented as numbers (such as the word
    embeddings in [chapter 9](kindle_split_021.html#ch09)).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从前面的描述中可以看到的，kNN 算法的一个关键要求是，每个输入示例都表示为一个向量。像我们从截断的 MobileNet 获取的那样的嵌入是这样的向量表示的良好候选者，原因有两个。首先，与原始输入相比，它们通常具有较低的维度，因此减少了距离计算所需的存储和计算量。其次，由于它们已经在大型分类数据集上进行了训练，所以这些嵌入通常捕捉到输入中的更重要的特征（例如图像中的重要几何特征；参见[图
    4.5](kindle_split_015.html#ch04fig05)），并忽略了不太重要的特征（例如亮度和大小）。在某些情况下，嵌入给我们提供了原本不以数字形式表示的事物的向量表示（例如[第
    9 章](kindle_split_021.html#ch09)中的单词嵌入）。
- en: Compared to the neural network approach, kNN doesn’t require any training. In
    cases where the number of reference examples is not too large, and the dimensionality
    of the input is not too high, using kNN can be computationally more efficient
    than training a neural network and running it for inference.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经网络方法相比，kNN 不需要任何训练。在参考样本数量不太多且输入维度不太高的情况下，使用 kNN 可以比训练神经网络并对其进行推断的计算效率更高。
- en: However, kNN inference doesn’t scale well with the amount of data. In particular,
    given *N* reference examples, a kNN classifier must compute *N* distances in order
    to make a prediction for every input.^([[a](#ch05fn1a)]) When *N* gets large,
    the amount of computation can get intractable. By contrast, the inference with
    a neural network doesn’t change with the amount of training data. Once the network
    has been trained, it doesn’t matter how many examples went into the training.
    The amount of computation that the forward pass on the network takes is only a
    function of the network’s topology.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，kNN 推断不随数据量的增加而扩展。特别是，给定 *N* 个参考示例，kNN 分类器必须计算 *N* 个距离，以便为每个输入进行预测。当 *N*
    变大时，计算量可能变得难以处理。相比之下，神经网络的推断不随训练数据的量而变化。一旦网络被训练，训练数据的数量就不重要了。网络正向传播所需的计算量仅取决于网络的拓扑结构。
- en: ^a
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But see research efforts to design algorithms that approximate the kNN algorithm
    but run faster and scale better than kNN: Gal Yona, “Fast Near-Duplicate Image
    Search Using Locality Sensitive Hashing,” Towards Data Science, 5 May 2018, [http://mng.bz/1wm1](http://mng.bz/1wm1).'
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但是，请查看研究努力设计近似 kNN 算法但运行速度更快且规模比 kNN 更好的算法：Gal Yona，“利用局部敏感哈希进行快速近似重复图像搜索”，Towards
    Data Science，2018 年 5 月 5 日，[http://mng.bz/1wm1](http://mng.bz/1wm1)。
- en: 'If you are interested in using kNN for your applications, check out the WebGL-accelerated
    kNN library built on top of TensorFlow.js: [http://mng.bz/2Jp8](http://mng.bz/2Jp8).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣在您的应用程序中使用 kNN，请查看基于 TensorFlow.js 构建的 WebGL 加速 kNN 库：[http://mng.bz/2Jp8](http://mng.bz/2Jp8)。
- en: '|  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '5.1.3\. Getting the most out of transfer learning through fine-tuning: An audio
    example'
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 通过微调充分利用迁移学习：音频示例
- en: 'In the previous sections, the examples of transfer learning dealt with visual
    inputs. In this example, we will show that transfer learning works on audio data
    represented as spectrogram images as well. Recall that we introduced the convnet
    for recognizing speech commands (isolated, short spoken words) in [section 4.4](kindle_split_015.html#ch04lev1sec4).
    The speech-command recognizer we built was capable of recognizing only 18 different
    words (such as “one,” “two,” “up,” and “down”). What if you want to train a recognizer
    for other words? Perhaps your particular application requires the user to say
    specific words such as “red” or “blue,” or even words that are picked by the users
    themselves; or perhaps your application is intended for users who speak languages
    other than English. This is a classic example of transfer learning: with the small
    amount of data at hand, you *could* try to train a model entirely from scratch,
    but using a pretrained model as the base allows you to spend a smaller amount
    of time and computation resources while getting a higher degree of accuracy.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，迁移学习的示例处理了视觉输入。在这个例子中，我们将展示迁移学习也适用于表示为频谱图像的音频数据。回想一下，我们在[第 4.4 节](kindle_split_015.html#ch04lev1sec4)中介绍了用于识别语音命令（孤立的、短的口头单词）的卷积网络。我们构建的语音命令识别器只能识别
    18 个不同的单词（如“one”、“two”、“up” 和 “down”）。如果你想为其他单词训练一个识别器呢？也许你的特定应用程序需要用户说特定的单词，比如“red”
    或 “blue”，甚至是用户自己选的单词；或者你的应用程序面向的是讲英语以外语言的用户。这是迁移学习的一个经典例子：在手头数据量很少的情况下，你*可以*尝试从头开始训练一个模型，但使用预训练模型作为基础可以在更短的时间内和更少的计算资源下获得更高的准确度。
- en: How to do transfer learning in the speech-command example app
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如何在语音命令示例应用中进行迁移学习
- en: 'Before we describe how transfer learning works in this example, it will be
    good for you to get familiar with how to use the transfer-learning feature through
    the UI. To use the UI, make sure your machine has an audio-input device (a microphone)
    attached and that the audio-input volume is set to a nonzero value in your system
    settings. To download the code of the demo and run it, do the following (the same
    procedure as in [section 4.4.1](kindle_split_015.html#ch04lev2sec10)):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们描述如何在这个示例中进行迁移学习之前，最好让你熟悉如何通过 UI 使用迁移学习功能。要使用 UI，请确保您的计算机连接了音频输入设备（麦克风），并且在系统设置中将音频输入音量设置为非零值。要下载演示代码并运行它，请执行以下操作（与[第
    4.4.1 节](kindle_split_015.html#ch04lev2sec10)相同的过程）：
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When the UI starts up, answer Yes to the browser’s request for your permission
    to access the microphone. [Figure 5.9](#ch05fig09) shows an example screenshot
    of the demo. When it starts up, the demo page will automatically load a pretrained
    speech-commands model from the internet, using the `tf.loadLayersModel()` method
    pointing to an HTTPS URL. After the model is loaded, the Start and Enter Transfer
    Words buttons will be enabled. If you click the Start button, the demo will enter
    an inference mode in which it detects the 18 basic words (as displayed on the
    screen) in a continuous fashion. Each time a word is detected, the corresponding
    word box will light up on the screen. However, if you click the Enter Transfer
    Words button, a number of additional buttons will appear on the screen. These
    buttons are created from the comma-separated words in the text-input box to the
    right. The default words are “noise,” “red,” and “green.” These are the words
    that the transfer-learning model will be trained to recognize. But you are free
    to modify the content of the input box if you want to train a transfer model for
    other words, as long as you preserve the “noise” item. The “noise” item is a special
    one, for which you should collect background noise samples—that is, samples without
    any speech sound in them. This allows the transfer model to tell moments in which
    a word is spoken from moments of silence (background noise). When you click these
    buttons, the demo will record a 1-second audio snippet from the microphone and
    display its spectrogram next to the button. The number in the word button keeps
    track of how many examples you have collected for the particular word so far.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当 UI 启动时，请允许浏览器访问麦克风的请求。[图 5.9](#ch05fig09) 显示了演示的示例截图。当演示页面启动时，将自动从互联网上加载预训练的语音命令模型，使用指向
    HTTPS URL 的 `tf.loadLayersModel()` 方法。模型加载完成后，"开始" 和 "输入转移词" 按钮将被启用。如果点击 "开始"
    按钮，演示将进入推理模式，连续检测屏幕上显示的 18 个基本单词。每次检测到一个单词时，屏幕上相应的单词框将点亮。但是，如果点击 "输入转移词" 按钮，屏幕上将会出现一些额外的按钮。这些按钮是从右侧的文本输入框中的逗号分隔的单词创建的。默认单词是
    "noise"、"red" 和 "green"。这些是转移学习模型将被训练识别的单词。但是，如果你想为其他单词训练转移模型，可以自由修改输入框的内容，只要保留
    "noise" 项即可。"noise" 项是特殊的一个，你应该收集背景噪声样本，即没有任何语音声音的样本。这允许转移模型区分语音和静音（背景噪声）的时刻。当你点击这些按钮时，演示将从麦克风记录
    1 秒的音频片段，并在按钮旁边显示其频谱图。单词按钮中的数字跟踪到目前为止已经收集到的特定单词的示例数量。
- en: 'Figure 5.9\. An example screenshot of the transfer-learning feature of the
    speech-command example. Here, the user has entered a custom set of words for transfer
    learning: “feel,” “seal,” “veal,” and “zeal,” in addition to the always-required
    “noise” item. Furthermore, the user has collected 20 examples for each of the
    word and noise categories.'
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.9\. 语音命令示例的转移学习功能的示例截图。在这里，用户已经为转移学习输入了一组自定义单词："feel"、"seal"、"veal" 和 "zeal"，以及始终需要的
    "noise" 项。此外，用户已经收集了每个单词和噪声类别的 20 个示例。
- en: '![](f0173_01_alt.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](f0173_01_alt.jpg)'
- en: 'As is the general case in machine-learning problems, the more data you can
    collect (as permitted by the time and resources available), the better the trained
    model will turn out. The example app requires at least eight examples for every
    word. If you don’t want to or cannot collect sound samples yourself, you can download
    a precollected dataset from [http://mng.bz/POGY](http://mng.bz/POGY) (file size:
    9 MB) and upload it by using the Upload button in the Dataset IO section of the
    UI.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如同机器学习问题中的一般情况，你能够收集的数据越多（在可用时间和资源允许的范围内），训练出的模型就会越好。示例应用程序至少需要每个单词的八个示例。如果你不想或无法自己收集声音样本，可以从[http://mng.bz/POGY](http://mng.bz/POGY)（文件大小：9
    MB）下载预先收集好的数据集，并在 UI 的数据集 IO 部分使用上传按钮上传。
- en: Once the dataset is ready, through either file uploading or your own sample
    collection, the Start Transfer Learning button will become enabled. You can click
    the button to kick off the training of the transfer model. The app performs a
    3:1 split on the audio spectrograms you have collected so that a randomly selected
    75% of them will be used for training, while the remaining 25% will be used for
    validation.^([[8](#ch05fn8)]) The app displays the training-set loss and accuracy
    values along with the validation-set values as the transfer learning happens.
    Once the training is complete, you can click the Start button to let the demo
    start a continuous recognition of the transfer words, during which time you can
    assess the accuracy of the transfer model empirically.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备好后，通过文件上传或你自己的样本收集，"开始迁移学习" 按钮将变为可用状态。你可以点击该按钮启动迁移模型的训练。该应用在你收集的音频频谱图上执行3:1的分割，随机选择其中75%用于训练，剩余的25%用于验证。应用程序在迁移学习过程中显示训练集损失和准确度值以及验证集值。一旦训练完成，可以点击
    "开始" 按钮，让演示程序连续识别迁移词，此时你可以经验性地评估迁移模型的准确度。
- en: ⁸
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the reason why the demo requires you to collect at least eight samples
    per word. With fewer words, the number of samples for each word will be small
    in the validation set, leading to potentially unreliable loss and accuracy estimates.
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这也是为什么演示要求你每个单词至少收集八个样本的原因。如果单词更少，在验证集中每个单词的样本数量将很少，可能会导致不可靠的损失和准确度估计。
- en: You should experiment with different sets of words and see how they affect the
    accuracy you can get after doing transfer learning on them. In the default set,
    “red” and “green,” the words are fairly distinct from each other in terms of their
    phonemic content. For example, their onset consonants are two very distinct sounds,
    “r” and “g.” Their vowels also sound fairly distinct (“e” versus “ee”); so do
    their ending consonants (“d” versus “n”). Therefore, you should be able to get
    near-perfect validation accuracy at the end of the transfer training, as long
    as the number of examples you collect for each word is not too small (say >= 8),
    and you don’t use an epoch number that’s too small (which leads to underfitting)
    or too large (which leads to overfitting; see [chapter 8](kindle_split_020.html#ch08)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该尝试不同的词汇组合，观察它们在经过迁移学习后对精确度的影响。默认集合中，“red”和“green”这两个词在音位内容方面非常不同。例如，它们的起始辅音是两个非常不同的声音，“r”和“g”。它们的元音也听起来非常不同（“e”和“ee”）；结尾辅音也很不同（“d”和“n”）。因此，只要每个单词收集的样本数量不太小（例如>=8），使用的时代数不太小（这会导致欠拟合）或太大（这会导致过拟合；请参阅[第8章](kindle_split_020.html#ch08)），你就能够在迁移训练结束时获得几乎完美的验证精度。
- en: 'To make the transfer-learning task more challenging for the model, use a set
    consisting of 1) more confusable words and 2) a larger vocabulary. This is what
    we did for the screenshot in [figure 5.9](#ch05fig09). There, a set of four words
    that sound similar to each other are used: “feel,” “seal,” “veal,” and “zeal.”
    These words have identical vowel and ending consonants, as well as four similar-sounding
    onset consonants. They might even confuse a human listener not paying attention
    or someone listening over a bad phone line. From the accuracy curve at the bottom-right
    part of the figure, you can see that it is not an easy task for the model to reach
    an accuracy higher than 90%, for which an initial phase of transfer learning has
    to be supplemented by an additional phase of *fine-tuning*—that is, a transfer-learning
    trick.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为使模型的迁移学习任务更具挑战性，使用由1）更具混淆性的单词和2）更大的词汇组成的集合。这就是我们在[图5.9](#ch05fig09) 的屏幕截图中所做的。在该截图中，使用了四个听起来相似的单词：“feel”、“seal”、“veal”和“zeal”。这些单词的元音和结尾辅音相同，开头的辅音也相似。它们甚至可能会让一个不注意或在坏电话线路上听的人听起来混淆。从图的右下角的准确度曲线可以看出，模型要达到90%以上的准确度并不是一件容易的事，必须通过额外的“微调”阶段来补充初始的迁移学习
    - 这是一种迁移学习技巧。
- en: Deep dive into fine-tuning in transfer learning
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深入了解迁移学习中的微调
- en: Fine-tuning is a technique that helps you reach levels of accuracy not achievable
    just by training the new head of the transfer model. If you wish to understand
    how fine-tuning works, this section explains it in greater detail. There will
    be a few technical points to digest. But the deepened understanding of transfer
    learning and the related TensorFlow.js implementation that you’ll get out of it
    will be worth the effort.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一种技术，它可以帮助您达到仅通过训练迁移模型的新头部无法达到的准确度水平。如果您希望了解微调的工作原理，本节将更详细地解释。您需要消化一些技术细节。但通过它，您将深入理解迁移学习及其相关的
    TensorFlow.js 实现，这将是值得的努力。
- en: Constructing a single model for transfer learning
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 构建单个迁移学习模型
- en: First, we need to understand how the speech transfer-learning app creates the
    model for transfer learning. The code in [listing 5.7](#ch05ex07) (from speech-commands/src/browser_
    fft_recognizer.ts) creates a model from the base speech-command model (the one
    you learned in [section 4.4.1](kindle_split_015.html#ch04lev2sec10)). It first
    finds the penultimate (the second-last) dense layer of the model and gets its
    output symbolic tensor `(`truncatedBaseOutput` in the code). It then creates a
    new head model consisting of only one dense layer. The input shape of this new
    head matches the shape of the `truncatedBaseOutput` symbolic tensor, and its output
    shape matches the number of words in the transfer dataset (five, in the case of
    [figure 5.9](#ch05fig09)). The dense layer is configured to use the softmax activation,
    which suits the multiclass-classification task. (Note that unlike most of the
    other code listings in the book, the following code is written in TypeScript.
    If you’re unfamiliar with TypeScript, you can simply ignore the type notations
    such as `void` and `tf.SymbolicTensor`.)`
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要了解语音迁移学习应用程序如何为迁移学习创建模型。[列表 5.7](#ch05ex07)（来自 speech-commands/src/browser_
    fft_recognizer.ts 的代码）中的代码从基础语音命令模型（您在 [第 4.4.1 节](kindle_split_015.html#ch04lev2sec10)中学到的模型）创建一个模型。它首先找到模型的倒数第二个（倒数第二个）密集层，并获取其输出符号张量`（代码中的`truncatedBaseOutput`）。然后，它创建一个仅包含一个密集层的新头模型。这个新头的输入形状与`truncatedBaseOutput`符号张量的形状匹配，其输出形状与迁移数据集中的单词数匹配（在[图
    5.9](#ch05fig09)的情况下为五个）。密集层配置为使用 softmax 激活，适用于多类别分类任务。（请注意，与书中大多数其他代码清单不同，以下代码是用
    TypeScript 编写的。如果您不熟悉 TypeScript，可以简单地忽略类型标记，例如`void`和`tf.SymbolicTensor`。）
- en: Listing 5.7\. Creating the transfer-learning model as a single `tf.Model` object^([[9](#ch05fn9)])
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**列表 5.7**\. 将迁移学习模型创建为单个`tf.Model`对象^([[9](#ch05fn9)])'
- en: ⁹
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Two notes about this code listing: 1) The code is written in TypeScript because
    it is a part of the reusable @tensorflow-models/speech-commands library. 2) Some
    error-checking code has been removed from this code for the sake of simplicity.'
  id: totrans-197
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于此代码列表的两点说明：1）代码是用 TypeScript 编写的，因为它是可重用的 @tensorflow-models/speech-commands
    库的一部分。2）出于简化的目的，此代码中删除了一些错误检查代码。
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1*** Finds the second-last dense layer of the base model'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 找到基础模型的倒数第二个密集层'
- en: '***2*** Gets the layer that will be unfrozen during fine-tuning later (see
    [listing 5.8](#ch05ex08))'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取稍后在微调过程中将解冻的层（请参阅[列表 5.8](#ch05ex08)）'
- en: '***3*** Finds the symbolic tensor'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 找到符号张量'
- en: '***4*** Creates the new head of the model'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 创建模型的新头'
- en: '***5*** “Applies” the new head on the output of the truncated base model’s
    output to get the final output of the new model as a symbolic tensor'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 在截断的基础模型输出上“应用”新的头部，以获取新模型的最终输出作为符号张量。'
- en: '***6*** Uses the tf.model() API to create a new model for transfer learning,
    specifying the original model’s inputs as its input and the new symbolic tensor
    as the output'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 使用`tf.model()` API 创建一个新的用于迁移学习的模型，指定原始模型的输入作为其输入，新的符号张量作为输出。'
- en: 'The new head is used in a novel way: its `apply()` method is called using the
    `truncatedBaseOutput` symbolic tensor as the input argument. `apply()` is a method
    that’s available on every layer and model object in TensorFlow.js. What does the
    `apply()` method do? As its name suggests, it “applies” the new head model on
    an input and gives you an output. The important things to realize are as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 新的头部以一种新颖的方式使用：其`apply()`方法使用截断的基础输出符号张量作为输入参数进行调用。`apply()`是 TensorFlow.js
    中每个层和模型对象上都可用的方法。`apply()`方法的作用是什么？顾名思义，它“应用”新的头模型于输入，并给出输出。要认识到的重要事项如下：
- en: Both the input and output involved here are symbolic—they are placeholders for
    concrete tensor values.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出都是符号化的——它们是具体张量值的占位符。
- en: '[Figure 5.10](#ch05fig10) shows a graphical illustration of this: the symbolic
    input (`truncatedBaseOutput`) is not an isolated entity; instead, it is the output
    of the second-last dense layer of the base model. The dense layer receives inputs
    from another layer, which in turn receives inputs from its upstream layer, and
    so forth. Therefore, `truncatedBaseOutput` carries with it a subgraph of the base
    model: namely, the subgraph between the base model’s input and the second-last
    dense layer’s output. In other words, it is the entire graph of the base model,
    minus the part after the second-last dense layer. As a result, the output of the
    `apply()` call carries a graph consisting of that subgraph plus the new dense
    layer. The output and the original input are used together in a call to the `tf.model()`
    function, which yields a new model. This new model is the same as the base model
    except that its head has been replaced with the new dense layer (see the bottom
    part of [figure 5.10](#ch05fig10)).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图 5.10](#ch05fig10)给出了一个图形示例：符号输入（`truncatedBaseOutput`）不是一个孤立的实体；而是基模型倒数第二个密集层的输出。该密集层从另一层接收输入，该层又从其上游层接收输入，依此类推。因此，`truncatedBaseOutput`携带着基模型的一个子图，即基模型的输入到倒数第二个密集层的输出之间的子图。换句话说，它是基模型的整个图，减去倒数第二个密集层之后的部分。因此，`apply()`调用的输出包含该子图以及新的密集层。输出和原始输入在调用`tf.model()`函数时共同使用，得到一个新模型。这个新模型与基模型相同，只是其头部被新的密集层替换了（参见[图
    5.10](#ch05fig10)的底部部分）。'
- en: 'Figure 5.10\. A schematic illustration of the way in which the new, end-to-end
    model is created for transfer learning. This figure should be read in conjunction
    with [listing 5.7](#ch05ex07). Some parts of the figure that correspond to variables
    in [listing 5.7](#ch05ex07) are labeled in fixed-width font. Step 1: the output
    symbolic tensor of the second-to-last dense layer of the original model is obtained
    (indicated by the thick arrow). It will later be used in step 3\. Step 2: the
    new head model, consisting of a single output dense layer (labeled “dense 3”)
    is created. Step 3: the `apply()` method of the new head model is invoked with
    the symbolic tensor from step 1 as the input argument. The call connects the input
    to the new head model with the truncated base model from step 1\. Step 4: the
    return value of the `apply()` call is used in conjunction with the input symbolic
    tensor of the original model during a call to the `tf.model()` function. This
    call returns a new model that contains all the layers of the original model from
    the first layer to the second-last dense layer, in addition to the dense layer
    in the new head. In effect, this swaps the old head of the original model with
    the new head, setting the stage for subsequent training on the transfer data.
    Note that some (seven) layers of the actual speech-command model are omitted in
    this diagram for the sake of visual simplicity. In this figure, the tinted layers
    are trainable, while the white-colored layers are untrainable.'
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10\. 示出了创建迁移学习的新端到端模型的方式的示意图。在阅读此图时，请参考[list 5.7](#ch05ex07)。与[list 5.7](#ch05ex07)中的变量对应的图的某些部分用固定宽度字体标记。步骤
    1：获取原始模型倒数第二个密集层的输出符号张量（由粗箭头指示）。它将在步骤 3 中被使用。步骤 2：创建新的头模型，包含一个单输出的密集层（标记为“dense
    3”）。步骤 3：使用步骤 1 中的符号张量作为输入参数调用新头模型的`apply()`方法。此调用将该输入与步骤 1 中的截断的基模型连接起来。步骤 4：将`apply()`调用的返回值与原始模型的输入符号张量一起在调用`tf.model()`函数时使用。此调用返回一个新模型，其中包含了原始模型的所有层，从第一层到倒数第二个密集层，以及新头的密集层。实际上，这将原始模型的旧头和新头交换，为后续在迁移数据上训练做准备。请注意，为了简化可视化效果，图中省略了实际语音命令模型的一些（七个）层。在此图中，有颜色的层是可训练的，而白色的层是不可训练的。
- en: '![](05fig09_alt.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig09_alt.jpg)'
- en: 'Note that the approach here is different from how we fused models in [section
    5.1.2](#ch05lev2sec2). There, we created a truncated base and a new head model
    as two separate model instances. As a result, running inference on each input
    example involves two `predict()` calls. Here, the inputs expected by the new model
    are identical to the audio-spectrogram tensors expected by the base model. At
    the same time, the new model directly outputs the probability scores for the new
    words. Every inference takes only one `predict()` call and is therefore a more
    streamlined process. By encapsulating all the layers in a single model, our new
    approach has an additional advantage important for our application: it allows
    us to perform backpropagation through any of the layers involved in recognizing
    the new words. This enables us to perform the fine-tuning trick. This is what
    we will explore in the next section.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的方法与我们在 [5.1.2 节](#ch05lev2sec2) 中如何融合模型的方法不同。在那里，我们创建了一个被截断的基础模型和一个新的头模型作为两个独立的模型实例。因此，对每个输入示例进行推断涉及两个
    `predict()` 调用。在这里，新模型期望的输入与基础模型期望的音频频谱张量相同。同时，新模型直接输出新单词的概率分数。每次推断仅需一个 `predict()`
    调用，因此是一个更加流畅的过程。通过将所有层封装在单个模型中，我们的新方法在我们的应用中具有一个额外的重要优势：它允许我们通过参与识别新单词的任何层执行反向传播。这使我们能够执行微调技巧。这是我们将在下一节中探讨的内容。
- en: Fine-tuning through layer unfreezing
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过解冻层进行微调
- en: Fine-tuning is an optional step of transfer learning that follows an initial
    phase of model training. In the initial phase, all the layers from the base model
    were frozen (their `trainable` attribute was set to `false`), and weight updating
    happened only to the head layers. We have seen this type of initial training in
    the mnist-transfer-cnn and webcam-transfer-learning examples earlier in this chapter.
    During fine-tuning, some of the layers of the base model are unfrozen (their `trainable`
    attribute is set to `true`), and then the model is trained on the transfer data
    again. This layer unfreezing is shown schematically in [figure 5.11](#ch05fig11).
    The code in [listing 5.8](#ch05ex08) (from speech-commands/src/browser_fft_recognizer.ts)
    shows how that’s done in TensorFlow.js for the speech-command example.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是转移学习的可选步骤，紧随模型训练的初始阶段。在初始阶段，来自基础模型的所有层都被冻结（它们的`trainable`属性设置为`false`），权重更新仅发生在头部层。我们在本章前面的
    mnist-transfer-cnn 和 webcam-transfer-learning 示例中已经看到了这种初始训练类型。在微调期间，基础模型的一些层被解冻（它们的`trainable`属性设置为`true`），然后模型再次在转移数据上进行训练。这种层解冻在
    [图 5.11](#ch05fig11) 中以示意图显示。代码在 TensorFlow.js 中显示了如何为语音命令示例执行此操作，详见 [清单 5.8](#ch05ex08)（来自
    speech-commands/src/browser_fft_recognizer.ts）。
- en: Figure 5.11\. Illustrating frozen and unfrozen (that is, trainable) layers during
    the initial (panel A) and fine-tuning (panel B) phases of the transfer learning
    as done by the code in [listing 5.8](#ch05ex08). Note that the reason dense1 is
    followed immediately by dense3 is that dense2 (the original output of the base
    model) has been truncated as the first step of the transfer learning (see [figure
    5.10](#ch05fig10)).
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[图 5.11](#ch05fig11)。展示了转移学习初始阶段（面板 A）和微调阶段（面板 B）期间冻结和未冻结（即可训练）层的示意图，代码见 [清单
    5.8](#ch05ex08)。注意 dense1 紧随 dense3 之后的原因是，dense2（基础模型的原始输出）已被截断为转移学习的第一步（参见 [图
    5.10](#ch05fig10)）。'
- en: '![](05fig10_alt.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig10_alt.jpg)'
- en: Listing 5.8\. Initial transfer learning, followed by fine-tuning^([[10](#ch05fn10)])
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.8。初始转移学习，然后进行微调^([[10](#ch05fn10)])
- en: ^(10)
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Some error-checking code has been removed so as to focus on the key parts of
    the algorithm.
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一些错误检查代码已被删除，以便集中关注算法的关键部分。
- en: '[PRE15]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1*** Makes sure all layers of the truncated base model, including the one
    that will be fine-tuned later, are frozen for the initial phase of transfer training'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 确保所有截断基础模型的层，包括稍后将进行微调的层，在转移训练的初始阶段都被冻结'
- en: '***2*** Compiles the model for the initial transfer training'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 为初始转移训练编译模型'
- en: '***3*** If validationSplit is required, splits the transfer data into a training
    set and a validation set in a balanced way'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 如果需要 validationSplit，则以平衡的方式将转移数据分割为训练集和验证集'
- en: '***4*** Calls Model.fit() for initial transfer training'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 调用 Model.fit() 进行初始转移训练'
- en: '***5*** For fine-tuning, unfreezes the second-last dense layer of the base
    model (the last layer of the truncated base model)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 对于微调，解冻基础模型的倒数第二个密集层（截断基础模型的最后一层）'
- en: '***6*** Recompiles the model after unfreezing the layer (or the unfreezing
    won’t take effect)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 在解冻层之后重新编译模型（否则解冻不会生效）'
- en: '***7*** Calls Model.fit() for fine-tuning'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 调用 Model.fit() 进行微调'
- en: 'There are several important things to point out about the code in [listing
    5.8](#ch05ex08):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个关于 [列表 5.8](#ch05ex08) 中代码需要指出的重要事项：
- en: Each time you freeze or unfreeze any layers by changing their `trainable` attribute,
    you need to call the `compile()` method of the model again in order for the change
    to take effect. We’ve already covered that when talking about the MNIST transfer-learning
    example in [section 5.1.1](#ch05lev2sec1).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次您通过更改它们的 `trainable` 属性来冻结或解冻任何层时，都需要再次调用模型的 `compile()` 方法，以使更改生效。我们已经在 [第
    5.1.1 节](#ch05lev2sec1) 中讨论了这一点，当我们谈到 MNIST 迁移学习示例时。
- en: We reserve a fraction of the training data for validation. This ensures that
    the loss and accuracy we look at reflect how well the model works on inputs it
    hasn’t seen during backpropagation. However, the way in which we split a fraction
    of the collected data out for validation is different from before and deserves
    some attention. In the MNIST convnet example ([listing 4.2](kindle_split_015.html#ch04ex02)
    in [chapter 4](kindle_split_015.html#ch04)), we used the `validationSplit` parameter
    to let the `Model.fit()` reserve the last 15–20% of the data for validation. The
    same approach won’t work very well here. Why? Because we have a much smaller training
    set here compared to the size of the data in the earlier examples. As a result,
    blindly splitting the last several examples for validation may very well result
    in scenarios in which some words are underrepresented in the validation subset.
    For example, suppose you have collected eight examples for each of the four words
    “feel,” “seal,” “veal,” and “zeal” and choose the last 25% of the 32 samples (8
    examples) for validation. Then, on average, there will be only two examples for
    each word in the validation subset. Due to randomness, some of the words may end
    up having only one example in the validation subset, and others may have no example
    there at all! Obviously, if the validation set lacks certain words, it won’t be
    a very good set to measure the model’s accuracy on. This is why we use a custom
    function (`balancedTrainValSplit` in [listing 5.8](#ch05ex08)). This function
    takes into account the true word label of the examples and ensures that all the
    different words get fair representation in both the training and validation subsets.
    If you have a transfer-learning application involving a similarly small dataset,
    it is a good idea to do the same.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们保留了训练数据的一部分用于验证。这样做可以确保我们观察的损失和准确率反映了模型在反向传播期间没有见过的输入上的表现。然而，我们为验证而从收集的数据中拆分出一部分的方式与以前不同，并且值得注意。在
    MNIST 卷积网络示例（[列表 4.2](kindle_split_015.html#ch04ex02) 在 [第 4 章](kindle_split_015.html#ch04)）中，我们使用了
    `validationSplit` 参数，让 `Model.fit()` 保留最后的 15–20% 的数据用于验证。但是，在这里使用相同的方法效果不佳。为什么？因为与早期示例中的数据量相比，我们这里的训练集要小得多。因此，盲目地将最后几个示例拆分为验证可能会导致一些词在验证子集中表示不足。例如，假设您为“feel”、“seal”、“veal”
    和 “zeal” 中的每个词收集了八个示例，并选择最后的 32 个样本（8 个示例）的 25% 作为验证。那么，平均而言，验证子集中每个单词只有两个示例。由于随机性，一些单词可能最终只在验证子集中有一个示例，而其他单词可能根本没有示例！显然，如果验证集缺少某些单词，它将不是用于测量模型准确性的很好的集合。这就是为什么我们使用一个自定义函数（`balancedTrainValSplit`
    在 [列表 5.8](#ch05ex08)）。此函数考虑了示例的真实单词标签，并确保所有不同的单词在训练和验证子集中都得到公平的表示。如果您有一个涉及类似小数据集的迁移学习应用程序，那么做同样的事情是个好主意。
- en: So, what does fine-tuning do for us? What added value does it provide on top
    of the initial phase of transfer learning? To illustrate that, we plot the loss
    and accuracy curves from the initial and fine-tuning phases concatenated as continuous
    curves in panel A of [figure 5.12](#ch05fig12). The transfer dataset involved
    here consists of the same four words we saw in [figure 5.9](#ch05fig09). The first
    100 epochs of each curve correspond to the initial phase, while the last 300 epochs
    correspond to fine-tuning. You can see that toward the end of the 100 epochs of
    initial training, the loss and accuracy curves begin to flatten out and start
    to enter regimes of diminishing returns. The accuracy on the validation subset
    levels off around 84%. (Notice how misleading it would be to look at only the
    accuracy curve from the *training subset*, which easily approaches 100%.) However,
    unfreezing the dense layer in the base model, recompiling the model, and starting
    the fine-tuning phase of training, the validation accuracy gets unstuck and could
    go up to 90–92%, which is a very decent 6–8 percentage point gain in accuracy.
    A similar effect can be seen in the validation loss curve.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，微调为我们做了什么呢？在迁移学习的初始阶段之上，微调提供了什么附加价值？为了说明这一点，我们将面板 A 中初始阶段和微调阶段的损失和准确率曲线连续绘制在一起，如[图
    5.12](#ch05fig12)所示。这里涉及的迁移数据集包含了我们在[图 5.9](#ch05fig09)中看到的相同的四个单词。每条曲线的前 100
    个纪元对应于初始阶段，而最后的 300 个纪元对应于微调。你可以看到，在初始训练的前 100 个纪元结束时，损失和准确率曲线开始变平并开始进入递减回报的区域。在验证子集的准确率在约
    84% 时达到平稳状态。（请注意，仅查看 *训练子集* 的准确率曲线是多么具有误导性，因为它很容易接近 100%。）然而，解冻基础模型中的密集层，重新编译模型，并开始微调训练阶段，验证准确率就不再停滞，可以提高到
    90–92%，这是一个非常可观的准确率增加 6–8 个百分点。验证损失曲线也可以看到类似的效果。
- en: 'Figure 5.12\. Panel A: example loss and accuracy curves from transfer learning
    and the subsequent fine-tuning (FT in the plot legends). Notice the inflection
    point at the junction between the initial and fine-tuning parts of the curves.
    Fine-tuning accelerates the reduction in the loss and gain in the accuracy, which
    is due to the unfreezing of the top few layers of the base model and the resulting
    increase in the model’s capacity, and its adaptation toward the unique features
    in the transfer-learning data. Panel B: the loss and accuracy curves from training
    the transfer model an equal number of epochs (400 epochs) without fine-tuning.
    Notice that without the fine-tuning, the validation loss converges to a higher
    value and the validation accuracy to a lower value compared to panel A. Note that
    while the final accuracy reaches about *0.9* with fine-tuning (panel A), it gets
    stuck at about *0.85* without the fine-tuning but with the same number of total
    epochs (panel B).'
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.12\. 面板 A：迁移学习和随后微调（图例中标为 FT）的示例损失和准确率曲线。注意曲线初始部分和微调部分之间的拐点。微调加速了损失的减少和准确率的提高，这是由于基础模型的顶部几层解冻以及模型容量的增加，以及向迁移学习数据中的独特特征的调整所致。面板
    B：在不进行微调的情况下训练迁移模型相同数量的纪元（400 纪元）的损失和准确率曲线。注意，没有微调时，验证损失收敛到较高值，验证准确率收敛到比面板 A 低的值。请注意，虽然进行了微调（面板
    A）的最终准确率达到约 *0.9*，但在没有进行微调但总纪元数相同的情况下（面板 B），准确率停留在约 *0.85*。
- en: '![](05fig11_alt.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig11_alt.jpg)'
- en: To illustrate the value of fine-tuning over transfer learning without fine-tuning,
    we show in panel B of [figure 5.12](#ch05fig12) what happens if the transfer model
    is trained for an equal number of (400) epochs without fine-tuning the top few
    layers of the base model. There is no “inflection point” in the loss or accuracy
    curves that happened in panel A at epoch 100 when the fine-tuning kicks in. Instead,
    the loss and accuracy curves level off and converge to worse values.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明微调相对于不进行微调的迁移学习的价值，我们在[图 5.12](#ch05fig12)的面板 B 中展示了如果不微调基础模型的顶部几层，而将迁移模型训练相同数量（400）的纪元时会发生什么。在面板
    A 中发生的在第 100 纪元时进行微调的损失或准确率曲线上没有“拐点”。相反，损失和准确率曲线趋于平稳，并收敛到较差的值。
- en: 'So why does fine-tuning help? It can be understood as an increase in the model
    capacity. By unfreezing some of the topmost layers of the base model, we allow
    the transfer model to minimize the loss function in a higher-dimensional parameter
    space than the initial phase. This is similar to adding hidden layers to a neural
    network. The weight parameters of the unfrozen dense layer have been optimized
    for the original dataset (the one consisting of words like “one,” “two,” “yes,”
    and “no”), which may not be optimal for the transfer words. This is because the
    internal representations that help the model distinguish between those original
    words may not be the representations that make the transfer words easiest to distinguish
    from one another. By allowing those parameters to be optimized further (that is,
    fine-tuned) for the transfer words, we allow the representation to be optimized
    for the transfer words. Therefore, we get a boost in validation accuracy on the
    transfer words. Note that this boost is easier to see when the transfer-learning
    task is hard (as with the four confusable words: “feel,” “seal,” “veal,” and “zeal”).
    With easier tasks (more distinct words like “red” and “green”), the validation
    accuracy may well reach 100% with only the initial transfer learning.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么微调有帮助呢？可以理解为增加了模型的容量。通过解冻基本模型的一些最顶层，我们允许转移模型在比初始阶段更高维的参数空间中最小化损失函数。这类似于向神经网络添加隐藏层。解冻的密集层的权重参数已经针对原始数据集进行了优化（由诸如“one”、“two”、“yes”和“no”之类的单词组成的数据集），这可能对转移单词不是最优的。这是因为帮助模型区分这些原始单词的内部表示可能不是使转移单词最容易区分的表示。通过允许进一步优化（即微调）这些参数以用于转移单词，我们允许表示被优化用于转移单词。因此，我们在转移单词上获得验证准确性的提升。请注意，当转移学习任务很难时（如四个易混淆的单词：“feel”、“seal”、“veal”和“zeal”），更容易看到这种提升。对于更容易的任务（更不同的单词，如“red”和“green”），验证准确性可能仅仅通过初始的转移学习就可以达到100%。
- en: One question you might want to ask is, here we unfreeze only one layer in the
    base model, but will unfreezing more layers help? The short answer is, it depends,
    because unfreezing even more layers gives the model even higher capacity. But
    as we mentioned in [chapter 4](kindle_split_015.html#ch04) and will discuss in
    greater detail in [chapter 8](kindle_split_020.html#ch08), higher capacity leads
    to a higher risk of overfitting, especially when we are faced with a small dataset
    like the audio examples collected in the browser here. This is not to mention
    the additional computation load required to train more layers. You are encouraged
    to experiment with it yourself as a part of exercise 4 at the end of this chapter.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想问的一个问题是，在这里我们只解冻了基本模型的一层，但是解冻更多的层会有帮助吗？简短的答案是，这取决于情况，因为解冻更多的层会使模型的容量更高。但正如我们在[第四章](kindle_split_015.html#ch04)中提到的，并且将在[第八章](kindle_split_020.html#ch08)中更详细地讨论，更高的容量会导致过拟合的风险增加，特别是当我们面对像这里收集到的音频示例这样的小数据集时。更不用说训练更多层所需的额外计算负载了。鼓励你作为本章末尾的一部分来进行自己的实验。
- en: Let’s wrap up this section on transfer learning in TensorFlow.js. We introduced
    three different ways to reuse a pretrained model on new tasks. In order to help
    you decide which approach to use in your future transfer-learning projects, we
    summarize the three approaches and their relative pros and cons in [table 5.1](#ch05table01).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们结束TensorFlow.js中关于迁移学习的这一部分。我们介绍了三种在新任务上重用预训练模型的不同方法。为了帮助你决定在将来的迁移学习项目中使用哪种方法，我们在[table
    5.1](#ch05table01)中总结了这三种方法及其相对优缺点。
- en: Table 5.1\. A summary of three approaches to transfer learning in TensorFlow.js
    and their relative advantages and shortcomings
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1\. TensorFlow.js中三种迁移学习方法及其相对优势和缺点的总结
- en: '| Approach | Pros | Cons |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 优势 | 缺点 |'
- en: '| --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Use the original model and freeze its first several (feature-extracting)
    layers ([section 5.1.1](#ch05lev2sec1)). |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 使用原始模型并冻结其前几层（特征提取层）（[section 5.1.1](#ch05lev2sec1)）。 |'
- en: Simple and convenient
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单而方便
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Works only if the output shape and activation required by the transfer learning
    match those of the base model
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅当迁移学习所需的输出形状和激活与基本模型的形状和激活匹配时才起作用
- en: '|'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Obtain internal activations from the original model as embeddings for the
    input example, and create a new model that takes the embedding as the input ([section
    5.1.2](#ch05lev2sec2)). |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 从原始模型中获取内部激活作为输入示例的嵌入，并创建一个以该嵌入作为输入的新模型（[section 5.1.2](#ch05lev2sec2)）。
    |'
- en: Applicable to transfer-learning cases that require an output shape different
    from the original one
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于需要与原始输出形状不同的迁移学习情况
- en: Embedding tensors are directly accessible, making methods such as k-nearest
    neighbors (kNN, see [info box 5.2](#ch05sb02)) classifiers possible
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入张量是直接可访问的，使得k最近邻（kNN，见[信息框5.2](#ch05sb02)）分类器等方法成为可能
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Need to manage two separate model instances
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要管理两个独立的模型实例
- en: Difficult to fine-tune layers of the original model
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很难微调原始模型的层
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Create a new model that contains the feature-extracting layers of the original
    model and the layers of the new head ([section 5.1.3](#ch05lev2sec3)). |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 创建一个包含原始模型的特征提取层和新头部层的新模型（请参见[第5.1.3节](#ch05lev2sec3)）。 |'
- en: Applicable to transfer-learning cases that require an output shape different
    from the original one
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于需要与原始输出形状不同的迁移学习情况
- en: Only a single model instance to manage
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需要管理一个模型实例
- en: Enables fine-tuning of feature-extracting layers
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许对特征提取层进行微调
- en: '|'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Internal activations (embeddings) that are not directly accessible
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法直接访问内部激活（嵌入）张量
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 5.2\. Object detection through transfer learning on a convnet
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. 通过卷积神经网络进行目标检测的迁移学习
- en: 'The examples of transfer learning you have seen in this chapter so far have
    a commonality: the nature of the machine-learning task stays the same after the
    transfer. In particular, they take a computer-vision model trained on a multiclass-classification
    task and apply it on another multiclass-classification task. In this section,
    we will show that this doesn’t have to be the case. The base model can be used
    on a task very different from the original one—for example, when you want to use
    a base model trained on a classification task to perform regression (fitting a
    number). This type of cross-domain transfer is a good example of the versatility
    and reusability of deep learning, which is one of the main reasons behind the
    success of the field.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你在本章中所看到的迁移学习示例都有一个共同点：在迁移后机器学习任务的性质保持不变。特别是，它们采用了一个在多类分类任务上训练的计算机视觉模型，并将其应用于另一个多类分类任务。在本节中，我们将展示这并不一定是这样的。基础模型可以用于非常不同于原始任务的任务，例如当你想使用在分类任务上训练的基础模型来执行回归（拟合数字）时。这种跨领域迁移是深度学习的多功能性和可重复使用性的良好例证，是该领域成功的主要原因之一。
- en: The new task we will use to illustrate this point is *object detection*, the
    first nonclassification computer-vision problem type you encounter in this book.
    Object detection involves detecting certain classes of objects in an image. How
    is it different from classification? In object detection, the detected object
    is reported in terms of not only its class (what type of object it is) but also
    some additional information regarding the location of the object inside the image
    (where the object is). The latter is a piece of information that a mere classifier
    doesn’t provide. For example, in a typical object-detection system used by self-driving
    cars, a frame of input image is analyzed so that the system outputs not only the
    types of interesting objects that are present in the image (such as vehicles and
    pedestrians) but also the location, apparent size, and pose of such objects within
    the image’s coordinate system.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们将使用新的任务——*目标检测*，这是本书中第一个非分类计算机视觉问题类型。目标检测涉及在图像中检测特定类别的物体。它与分类有何不同？在目标检测中，检测到的物体不仅会以类别（它是什么类型的物体）的形式报告，还会包括有关物体在图像中内部位置的一些附加信息（物体在哪里）。后者是一个普通分类器无法提供的信息。例如，自动驾驶汽车使用的典型目标检测系统会分析输入图像的一个框架，以便该系统不仅输出图像中存在的有趣对象的类型（如车辆和行人），还输出这些对象在图像坐标系内的位置、表面积和姿态等信息。
- en: The example code is in the simple-object-detection directory of the tfjs-examples
    repository. Note that this example is different from the ones you have seen so
    far in that it combines model training in Node.js with inference in the browser.
    Specifically, the model training happens with tfjs-node (or tfjs-node-gpu), and
    the trained model is saved to disk. A parcel server is then used to serve the
    saved model files, along with the static index.html and index.js, in order to
    showcase the inference on the model in the browser.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码位于 tfjs-examples 仓库的 simple-object-detection 目录中。请注意，此示例与您迄今为止看到的示例不同，因为它将在
    Node.js 中的模型训练与浏览器中的推理结合起来。具体来说，模型训练使用 tfjs-node（或 tfjs-node-gpu）进行，训练好的模型将保存到磁盘上。然后使用
    parcel 服务器来提供保存的模型文件，以及静态的 index.html 和 index.js，以展示浏览器中模型的推理。
- en: 'The sequence of commands you can use for running the example is as follows
    (with some comment strings that you don’t need to include when entering the commands):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用的运行示例的命令序列如下（其中包含一些您在输入命令时不需要包含的注释字符串）：
- en: '[PRE16]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `yarn train` command performs model training on your machine and saves
    the model inside the ./dist folder when it’s finished. Note that this is a long-running
    training job and is best handled if you have a CUDA-enabled GPU, which boosts
    the training speed by a factor of 3 to 4\. To do this, you just need to add the
    `--gpu` flag to the `yarn train` command:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`yarn train`命令会在您的机器上进行模型训练，并在完成后将模型保存到`./dist`文件夹中。请注意，这是一个长时间运行的训练任务，如果您有
    CUDA 启用的 GPU，则最好处理，因为这可以将训练速度提高 3 到 4 倍。为此，您只需要向`yarn train`命令添加`--gpu`标志：'
- en: '[PRE17]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'However, if you don’t have the time or resources to train the model on your
    own machine, don’t worry: you can just skip the `yarn train` command and proceed
    directly to `yarn watch`. The inference page that runs in the browser will allow
    you to load a model we’ve already trained for you from a centralized location
    via HTTP.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有时间或资源在自己的机器上对模型进行训练，不用担心：您可以直接跳过`yarn train`命令，直接执行`yarn watch`。在浏览器中运行的推理页面将允许您通过
    HTTP 从集中位置加载我们已经为您训练好的模型。
- en: 5.2.1\. A simple object-detection problem based on synthesized scenes
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 基于合成场景的简单目标检测问题
- en: State-of-the-art object-detection techniques involve many tricks that are not
    suitable for a beginning tutorial on the topic. Our goal here is to show the essence
    of how object detection works without being bogged down by too many technical
    details. To this end, we designed a simple object-detection problem that involves
    synthesized image scenes (see [figure 5.13](#ch05fig13)). These synthesized images
    have a dimension of 224 × 224 and color depth of 3 (RGB channels) and hence match
    the input specification of the MobileNet model that will form the base of our
    model. As the example in [figure 5.13](#ch05fig13) shows, each scene has a white
    background. The object to detect is either an equilateral triangle or a rectangle.
    If the object is a triangle, its size and orientation are random; if the object
    is a rectangle, its height and width vary randomly. If the scene consisted of
    only the white background and the object of interest, the task would be too easy
    to show the power of our technique. To add to the difficulty of the task, a number
    of “noise objects” are randomly sprinkled in the scene. These include 10 circles
    and 10 line segments in every image. The locations and sizes of the circles are
    generated randomly, and so are the locations and lengths of the line segments.
    Some of the noise objects may lie on top of the target object, partially obscuring
    it. All the target and noise objects have randomly generated colors.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的目标检测技术涉及许多技巧，这些技巧不适合用于初学者教程。我们在这里的目标是展示目标检测的本质，而不受太多技术细节的困扰。为此，我们设计了一个涉及合成图像场景的简单目标检测问题（见[图
    5.13](#ch05fig13)）。这些合成图像的尺寸为 224 × 224，色深为 3（RGB 通道），因此与将成为我们模型基础的 MobileNet
    模型的输入规范匹配。正如[图 5.13](#ch05fig13)中的示例所示，每个场景都有一个白色背景。要检测的对象可以是等边三角形或矩形。如果对象是三角形，则其大小和方向是随机的；如果对象是矩形，则其高度和宽度是随机变化的。如果场景仅由白色背景和感兴趣的对象组成，则任务将太容易，无法展示我们技术的强大之处。为了增加任务的难度，在场景中随机分布了一些“噪声对象”。这些对象包括每个图像中的
    10 个圆和 10 条线段。圆的位置和大小以随机方式生成，线段的位置和长度也是如此。一些噪声对象可能位于目标对象的顶部，部分遮挡它。所有目标和噪声对象都具有随机生成的颜色。
- en: 'Figure 5.13\. An example of the synthesized scenes used by simple object detection.
    Panel A: a rotated equilateral triangle as the target object. Panel B: a rectangle
    as the target object. The boxes labeled “true” are the true bounding box for the
    object of interest. Note that the object of interest can sometimes be partially
    obscured by some of the noise objects (line segments and circles).'
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13\. 简单物体检测使用的合成场景示例。面板A：一个旋转的等边三角形作为目标对象。面板B：一个矩形作为目标对象。标记为“true”的框是感兴趣对象的真实边界框。请注意，感兴趣对象有时可能会被一些噪声对象（线段和圆）部分遮挡。
- en: '![](05fig12_alt.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig12_alt.jpg)'
- en: 'With the input data fully characterized, we can now define the task for the
    model we are about to create and train. The model will output five numbers, which
    are organized into two groups:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 随着输入数据被完全描述，我们现在可以为我们即将创建和训练的模型定义任务。该模型将输出五个数字，这些数字被组织成两组：
- en: The first group contains a single number, indicating whether the detected object
    is a triangle or a rectangle (regardless of its location, size, orientation, and
    color).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一组包含一个数字，指示检测到的对象是三角形还是矩形（不考虑其位置、大小、方向和颜色）。
- en: The remaining four numbers form the second group. They are the coordinates of
    the bounding box around the detected object. Specifically, they are the left x-coordinate,
    right x-coordinate, top y-coordinate, and bottom y-coordinate of the bounding
    box, respectively. See [figure 5.13](#ch05fig13) for an example.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩下的四个数字组成了第二组。它们是检测到的物体周围边界框的坐标。具体来说，它们分别是边界框的左 x 坐标、右 x 坐标、顶部 y 坐标和底部 y 坐标。参见[图5.13](#ch05fig13)作为示例。
- en: The nice things about using synthesized data are 1) the true label values are
    automatically known, and 2) we can generate as much data as we want. Every time
    we generate a scene image, the type of the object and its bounding box are automatically
    available to us from the generation process. So, there is no need for any labor-intensive
    labeling of the training images. This very efficient process in which the input
    features and labels are synthesized together is used in many testing and prototyping
    environments for deep-learning models and is a technique you should be familiar
    with. However, training object-detection models meant for real-life image inputs
    requires manually labeled real scenes. Luckily, there are such labeled datasets
    available. The Common Object in Context (COCO) dataset is one of them (see [http://cocodataset.org](http://cocodataset.org)).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合成数据的好处是1）真实标签值会自动知道，2）我们可以生成任意数量的数据。每次生成场景图像时，对象的类型和其边界框都会自动从生成过程中对我们可用。因此，不需要对训练图像进行任何劳动密集型的标记。这种输入特征和标签一起合成的非常高效的过程在许多深度学习模型的测试和原型环境中使用，并且这是一种你应该熟悉的技术。然而，用于真实图像输入的训练物体检测模型需要手动标记的真实场景。幸运的是，有这样的标记数据集可用。通用物体和背景（COCO）数据集就是其中之一（参见[http://cocodataset.org](http://cocodataset.org)）。
- en: After the training completes, the model should be able to localize and classify
    the target objects with reasonably good accuracy (as shown by the examples in
    [figure 5.13](#ch05fig13)). To understand how the model learns this object-detection
    task, dive with us into the code in the next section.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，模型应能够以相当高的准确性定位和分类目标对象（如[图5.13](#ch05fig13)中所示的示例所示）。要了解模型如何学习这个物体检测任务，请跟随我们进入下一节中的代码。
- en: 5.2.2\. Deep dive into simple object detection
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 深入了解简单物体检测
- en: 'Now let’s build the neural network to solve the synthesized object-detection
    problem. As before, we build our model on the pretrained MobileNet model in order
    to use the powerful general visual feature extractor in the model’s convolutional
    layers. This is what the `loadTruncatedBase()` method in [listing 5.9](#ch05ex09)
    does. However, a new challenge our new model faces is how to predict two things
    at the same time: determining what shape the target object is and finding its
    coordinates in the image. We haven’t seen this type of “dual-task prediction”
    before. The trick we use here is to let the model output a tensor that encapsulates
    both predictions, and we will design a new loss function that measures how well
    the model is doing in both tasks at once. We *could* train two separate models,
    one for classifying the shape and one for predicting the bounding box. But compared
    with using a single model to perform both tasks, running two models will involve
    more computation and more memory usage and doesn’t leverage the fact that feature-extracting
    layers can be shared between the two tasks. (The following code is from simple-object-detection/train.js.)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建神经网络来解决合成对象检测问题。与以前一样，我们在预训练的 MobileNet 模型上构建我们的模型，以使用模型的卷积层中的强大的通用视觉特征提取器。这是
    [列表 5.9](#ch05ex09) 中的 `loadTruncatedBase()` 方法所做的。然而，我们的新模型面临的一个新挑战是如何同时预测两个东西：确定目标对象的形状以及在图像中找到其坐标。我们以前没有见过这种“双任务预测”类型。我们在这里使用的技巧是让模型输出一个张量，该张量封装了两个预测，并且我们将设计一个新的损失函数，该函数同时衡量模型在两个任务中的表现如何。*我们可以*训练两个单独的模型，一个用于分类形状，另一个用于预测边界框。但是与使用单个模型执行两个任务相比，运行两个模型将涉及更多的计算和更多的内存使用，并且不利用特征提取层可以在两个任务之间共享的事实。（以下代码来自
    simple-object-detection/train.js。）
- en: Listing 5.9\. Defining a model for simple object learning based on truncating
    MobileNet^([[11](#ch05fn11)])
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.9\. 基于截断 MobileNet 定义简单对象学习模型^([[11](#ch05fn11)])
- en: ^(11)
  id: totrans-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-281
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Some code for checking error conditions is removed for clarity.
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了清晰起见，一些用于检查错误条件的代码已被删除。
- en: '[PRE18]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1*** Sets what layers to unfreeze for fine-tuning'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 设置要解冻以进行微调的层'
- en: '***2*** Gets an intermediate layer: the last feature-extraction layer'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 获取中间层：最后一个特征提取层'
- en: '***3*** Forms the truncated MobileNet'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 形成截断的 MobileNet'
- en: '***4*** Freezes all feature-extraction layers for the initial phase of transfer
    learning'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 冻结所有特征提取层以进行迁移学习的初始阶段'
- en: '***5*** Keeps track of layers that will be unfrozen during fine-tuning'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 跟踪在微调期间将解冻的层'
- en: '***6*** Builds the new head model for the simple object-detection task'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 为简单对象检测任务构建新头模型'
- en: '***7*** The length-5 output consists of a length-1 shape indicator and a length-4
    bounding box (see [figure 5.14](#ch05fig14)).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 长度为 5 的输出包括长度为 1 的形状指示器和长度为 4 的边界框（请参见 [图 5.14](#ch05fig14)）。'
- en: '***8*** Puts the new head model on top of the truncated MobileNet to form the
    entire model for object detection'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8*** 将新的头模型放在截断的 MobileNet 顶部，形成整个对象检测模型'
- en: The key part of the “dual-task” model is built by the `buildNewHead()` method
    in [listing 5.9](#ch05ex09). A schematic drawing of the model is shown in the
    left part of [figure 5.14](#ch05fig14). The new head consists of three layers.
    A flatten layer shapes the output of the last convolutional layer of the truncated
    MobileNet base so that dense layers can be added later on. The first dense layer
    is a hidden one with a relu nonlinearity. The second dense layer is the final
    output of the head and hence the final output of the entire object-detection model.
    This layer has the default linear activation. It is the key to understanding how
    this model works and therefore needs to be looked at carefully.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: “双任务”模型的关键部分由 [列表 5.9](#ch05ex09) 中的 `buildNewHead()` 方法构建。模型的示意图显示在 [图 5.14](#ch05fig14)
    的左侧。新头部由三层组成。一个展平层将截断的 MobileNet 基础的最后一个卷积层的输出形状为后续可以添加的密集层。第一个密集层是具有 relu 非线性的隐藏层。第二个密集层是头部的最终输出，因此也是整个对象检测模型的最终输出。该层具有默认的线性激活。这是理解该模型如何工作的关键，因此需要仔细查看。
- en: Figure 5.14\. The object-detection model and the custom loss function that it
    is based on. See [listing 5.9](#ch05ex09) for how the model (the left part) is
    constructed. See [listing 5.10](#ch05ex10) for how the custom loss function is
    written.
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.14\. 对象检测模型及其基于的自定义损失函数。请参见 [列表 5.9](#ch05ex09)，了解模型（左侧部分）的构建方式。请参见 [列表
    5.10](#ch05ex10)，了解自定义损失函数的编写方式。
- en: '![](05fig13_alt.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig13_alt.jpg)'
- en: 'As you can see from the code, the final dense layer has an output unit count
    of 5\. What do the five numbers represent? They combine the shape prediction and
    the bounding-box prediction. Interestingly, what determines their meaning is not
    the model itself, but rather the loss function that will be used on it. Previously,
    you saw various types of loss functions that can be straightforward string names
    such as `"meanSquaredError"` and are suitable to their respective machine-learning
    tasks (for example, see [table 3.6](kindle_split_014.html#ch03table06) in [chapter
    3](kindle_split_014.html#ch03)). However, this is only one of two ways to specify
    loss functions in TensorFlow.js. The other way, which is what we are using here,
    involves defining a custom JavaScript function that satisfies a certain signature.
    The signature is as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从代码中看到的那样，最终的稠密层输出单元数为5。这五个数字代表什么？它们结合了形状预测和边界框预测。有趣的是，决定它们含义的不是模型本身，而是将用于模型的损失函数。之前，你看到过各种类型的损失函数，可以直接使用字符串名称，如`"meanSquaredError"`，并适用于各自的机器学习任务（例如，请参阅[第3章表3.6](kindle_split_014.html#ch03table06)）。然而，这只是在TensorFlow.js中指定损失函数的两种方法之一。另一种方法，也是我们在这里使用的方法，涉及定义一个满足某个特定签名的自定义JavaScript函数。该签名如下：
- en: 'Two input arguments: 1) the true labels of the input examples and 2) the corresponding
    predictions of the model. Each of them is represented as a 2D tensor. The shape
    of the two tensors ought to be identical, with the first dimension of each tensor
    being the batch size.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有两个输入参数：1）输入示例的真实标签和2）模型的对应预测。它们都表示为2D张量。这两个张量的形状应该是相同的，每个张量的第一个维度都是批处理大小。
- en: The return value is a scalar tensor (a tensor of shape `[]`) whose value is
    the mean loss of the examples in the batch.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回值是一个标量张量（形状为`[]`的张量），其值是批处理中示例的平均损失。
- en: Our custom loss function, written according to this signature, is shown in [listing
    5.10](#ch05ex10) and graphically illustrated in the right part of [figure 5.14](#ch05fig14).
    The first input to `customLossFunction` (`yTrue`) is the true label tensor, which
    has a shape of `[batchSize, 5]`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据这个签名编写的自定义损失函数在[列表5.10](#ch05ex10)中显示，并在[图5.14的右侧](#ch05fig14)进行了图形化说明。`customLossFunction`的第一个输入（`yTrue`）是真实标签张量，其形状为`[batchSize,
    5]`。
- en: The second input (`yPred`) is the model’s output prediction, with exactly the
    same shape as `yTrue`. Of the five dimensions along the second axis of `yTrue`
    (the five columns, if we view it as a matrix), the first one is a 0–1 indicator
    for the shape of the target object (0 for triangle and 1 for rectangle). This
    is determined by how the data is synthesized (see simple-object-detection/synthetic_images.js).
    The remaining four columns are the target object’s bounding box—that is, its left,
    right, top, and bottom values—each of which ranges from 0 to `CANVAS_SIZE` (224).
    The number 224 is the height and width of the input images and comes from the
    input image size to MobileNet, which our model is based on.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输入（`yPred`）是模型的输出预测，其形状与`yTrue`完全相同。在`yTrue`的第二轴上的五个维度（如果我们将其视为矩阵，则为五列）中，第一个维度是目标对象形状的0-1指示器（三角形为0，矩形为1）。这是由数据合成方式确定的（请参阅simple-object-detection/synthetic_images.js）。其余四列是目标对象的边界框，即其左、右、上和下值，每个值范围从0到CANVAS_SIZE（224）。数字224是输入图像的高度和宽度，来自于MobileNet的输入图像大小，我们的模型基于此。
- en: Listing 5.10\. Defining the custom loss function for the object-detection task
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.10。为对象检测任务定义自定义损失函数
- en: '[PRE19]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1*** The shape-indicator column of yTrue is scaled by CANVAS_SIZE (224)
    to ensure approximately equal contribution to the loss by shape prediction and
    bounding-box prediction.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** `yTrue`的形状指示器列通过CANVAS_SIZE（224）缩放，以确保形状预测和边界框预测对损失的贡献大致相等。'
- en: The custom loss function takes `yTrue` and scales its first column (the 0–1
    shape indicator) by `CANVAS_SIZE`, while leaving the other columns unchanged.
    It then calculates the MSE between `yPred` and the scaled `yTrue`. Why do we scale
    the 0–1 shape label in `yTrue`? We want the model to output a number that represents
    whether it predicts the shape to be a triangle or a rectangle. Specifically, it
    outputs a number close to 0 for triangle and a number close to `CANVAS_SIZE` (224)
    for rectangle. So, during inference time, we can just compare the first value
    in the model’s output with `CANVAS_SIZE/2` (112) to get the model’s prediction
    of whether the shape is more like a triangle or a rectangle. The question is then
    how to measure the accuracy of this shape prediction in order to come up with
    a loss function. Our answer is to compute the difference between this number and
    the 0–1 indicator, multiplied by `CANVAS_SIZE`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义损失函数接收`yTrue`并将其第一列（0-1形状指示器）按`CANVAS_SIZE`进行缩放，同时保持其他列不变。然后它计算`yPred`和缩放后的`yTrue`之间的均方误差。为什么我们要缩放`yTrue`中的0-1形状标签？我们希望模型输出一个代表它预测形状是三角形还是矩形的数字。具体地说，它输出一个接近于0的数字表示三角形，一个接近于`CANVAS_SIZE`（224）的数字表示矩形。因此，在推理时，我们只需将模型输出的第一个值与`CANVAS_SIZE/2`（112）进行比较，以获取模型对形状更像是三角形还是矩形的预测。那么如何衡量这种形状预测的准确性以得出损失函数呢？我们的答案是计算这个数字与0-1指示器之间的差值，乘以`CANVAS_SIZE`。
- en: 'Why do we do this instead of using binary cross entropy as we did for the phishing-detection
    example in [chapter 3](kindle_split_014.html#ch03)? We need to combine two metrics
    of accuracy here: one for the shape prediction and one for the bounding-box prediction.
    The latter task involves predicting continuous values and can be viewed as a regression
    task. Therefore, MSE is a natural metric for bounding boxes. In order to combine
    the metrics, we just “pretend” that the shape prediction is also a regression
    task. This trick allows us to use a single metric function (the `tf.metric.meanSquaredError()`
    call in [listing 5.10](#ch05ex10)) to encapsulate the loss for both predictions.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们这样做而不像在[第三章](kindle_split_014.html#ch03)中钓鱼检测示例中使用二进制交叉熵呢？我们需要在这里结合两个准确度指标：一个是形状预测，另一个是边界框预测。后者任务涉及预测连续值，可以视为回归任务。因此，均方误差是边界框的自然度量标准。为了结合这些度量标准，我们只需“假装”形状预测也是一个回归任务。这个技巧使我们能够使用单个度量函数（[列表5.10](#ch05ex10)中的`tf.metric.meanSquaredError()`调用）来封装两种预测的损失。
- en: But why do we scale the 0–1 indicator by `CANVAS_SIZE`? Well, if we didn’t do
    this scaling, our model would end up generating a number in the neighborhood of
    0–1 as an indicator for whether it predicts the shape to be a triangle (close
    to 0) or a rectangle (close to 1). The difference between numbers around the `[0,
    1]` interval would clearly be much smaller compared to the differences we get
    from comparing the true bounding box and the predicted ones, which are in the
    range of 0–224\. As a result, the error signal from the shape prediction would
    be totally overshadowed by the error signal from the bounding-box prediction,
    which would not help us get accurate shape predictions. By scaling the 0–1 shape
    indicator, we make sure the shape prediction and bounding-box prediction contribute
    about equally to the final loss value (the return value of `customLossFunction()`),
    so that when the model is trained, it will optimize both types of predictions
    at once. In exercise 4 at the end of this chapter, you are encouraged to experiment
    with this scaling yourself.^([[12](#ch05fn12)])
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么我们要将0-1指示器按`CANVAS_SIZE`进行缩放呢？如果我们不进行这种缩放，我们的模型最终会生成一个接近于0-1的数字，作为它预测形状是三角形（接近于0）还是矩形（接近于1）的指示器。在`[0,
    1]`区间内的数字之间的差异显然要比我们从比较真实边界框和预测边界框得到的差异要小得多，后者在0-224的范围内。因此，来自形状预测的误差信号将完全被来自边界框预测的误差信号所掩盖，这对于我们得到准确的形状预测没有帮助。通过缩放0-1形状指示器，我们确保形状预测和边界框预测对最终损失值（`customLossFunction()`的返回值）的贡献大致相等，因此当模型被训练时，它将同时优化两种类型的预测。在本章末尾的练习4中，我们鼓励你自己尝试使用这种缩放。
- en: ^(12)
  id: totrans-306
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-307
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An alternative to the scaling and `meanSquaredError`-based approach here is
    to take the first column of `yPred` as the shape probability score and compute
    the binary cross entropy with the first column of `yTrue`. Then the binary cross
    entropy value can be summed together with the MSE calculated on the remaining
    columns of `yTrue` and `yPred`. But in this alternative approach, the cross entropy
    needs to be scaled properly to ensure the balance with the bounding-box loss,
    just like in our current approach. The scaling involves a free parameter whose
    value needs to be carefully selected. In practice, it becomes an additional hyperparameter
    of the model and requires time and compute resources to tune, which is a downside
    of the approach. We opted against the approach in favor of our current approach
    for simplicity.
  id: totrans-308
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里的一个替代方法是采用基于缩放和 `meanSquaredError` 的方法，将 `yPred` 的第一列作为形状概率分数，并与 `yTrue` 的第一列计算二元交叉熵。然后，将二元交叉熵值与在
    `yTrue` 和 `yPred` 的其余列上计算的 MSE 相加。但在这种替代方法中，需要适当缩放交叉熵，以确保与边界框损失的平衡，就像我们当前的方法一样。缩放涉及一个自由参数，其值需要仔细选择。在实践中，它成为模型的一个额外超参数，并且需要时间和计算资源来调整，这是该方法的一个缺点。为了简单起见，我们选择了当前方法，而不采用该方法。
- en: 'With the data prepared and the model and loss function defined, we are ready
    to train our model! The key parts of the model training code are shown in [listing
    5.11](#ch05ex11) (from simple-object-detection/train.js). Like the fine-tuning
    we’ve seen before ([section 5.1.3](#ch05lev2sec3)), the training proceeds in two
    phases: an initial phase, during which only the new head layers are trained, and
    a fine-tuning phase, during which the new head layers are trained together with
    the top few layers of the truncated MobileNet base. It should be noted that the
    `compile()` method must be invoked (again) right before the fine-tuning `fit()`
    call in order for the changes to the `trainable` property of the layers to take
    effect. If you run the training on your own machine, it’ll be easy to observe
    a significant downward jump in the loss values as soon as the fine-tuning phase
    starts, reflecting an increase in the capacity of the model and the adaptation
    of the unfrozen feature-extraction layers to the unique features in the object-detection
    data as a result of their unfreezing. The list of layers unfrozen during the fine-tuning
    is determined by the `fineTuningLayers` array, which is populated when we truncate
    the MobileNet (see the `loadTruncatedBase()` function in [listing 5.9](#ch05ex09)).
    These are the top nine layers of the truncated MobileNet. In exercise 3 at the
    end of the chapter, you can experiment with unfreezing fewer or more top layers
    of the base and observe how they change the accuracy of the model produced by
    the training process.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪，模型和损失函数已定义，我们准备好训练我们的模型！模型训练代码的关键部分显示在 [清单 5.11](#ch05ex11)（来自 simple-object-detection/train.js）。与我们之前见过的微调（[第
    5.1.3 节](#ch05lev2sec3)）类似，训练分为两个阶段：初始阶段，在此阶段仅训练新的头部层；微调阶段，在此阶段将新的头部层与截断的 MobileNet
    基础的顶部几层一起训练。需要注意的是，在微调的 `fit()` 调用之前，必须（再次）调用 `compile()` 方法，以便更改层的 `trainable`
    属性生效。如果在您自己的机器上运行训练，很容易观察到损失值在微调阶段开始时出现显着下降，这反映了模型容量的增加以及解冻特征提取层对目标检测数据中的唯一特征的适应能力。在微调期间解冻的层的列表由
    `fineTuningLayers` 数组确定，在截断 MobileNet 时填充（参见 [清单 5.9](#ch05ex09) 中的 `loadTruncatedBase()`
    函数）。这些是截断的 MobileNet 的顶部九层。在本章末尾的练习 3 中，您可以尝试解冻更少或更多的基础顶部层，并观察它们如何改变训练过程产生的模型的准确性。
- en: Listing 5.11\. Phase two of training the object-detection model
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 5.11\. 训练目标检测模型的第二阶段
- en: '[PRE20]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1*** Uses a relatively high learning rate for the initial phase'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 在初始阶段使用相对较高的学习率'
- en: '***2*** Performs the initial phase of transfer learning'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 执行迁移学习的初始阶段'
- en: '***3*** Fine-tuning phase starts'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 开始微调阶段'
- en: '***4*** Unfreezes some layers for fine-tuning'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4*** 解冻一些层进行微调'
- en: '***5*** Uses a slightly lower learning rate for the fine-tuning phase'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5*** 在微调阶段使用稍低的学习率'
- en: '***6*** During the fine-tuning phase, we reduce batchSize to avoid out-of-memory
    issues caused by the fact that backpropagation involves more weights and consumes
    more memory than the initial phase.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6*** 在微调阶段，我们将 batchSize 减少以避免由于反向传播涉及更多权重并消耗更多内存而引起的内存不足问题。'
- en: '***7*** Performs the fine-tuning phase'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7*** 执行微调阶段'
- en: After the fine-tuning ends, the model is saved to the disk and is then loaded
    during the in-browser inference step (started by the `yarn watch` command). If
    you load a hosted model, or if you have spent the time and compute resources to
    train a reasonably good model on your own machine, the shape and bounding-box
    prediction you’ll see in the inference page should be fairly good (validation
    loss at <100 after 100 epochs of initial training and 200 epochs of fine-tuning).
    The inference results are good but not perfect (see the examples in [figure 5.13](#ch05fig13)).
    When you examine the results, keep in mind that the in-browser evaluation is a
    fair one and reflects the model’s true generalization power because the examples
    the trained model is tasked to solve in the browser are different from the training
    and validation examples that it has seen during the transfer-learning process.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 微调结束后，模型将保存到磁盘，并在浏览器内推断步骤期间加载（通过`yarn watch`命令启动）。如果加载托管模型，或者您已经花费了时间和计算资源在自己的计算机上训练了一个相当不错的模型，那么您在推断页面上看到的形状和边界框预测应该是相当不错的（在初始训练的
    100 个周期和微调的 200 个周期后，验证损失在<100）。推断结果是好的但并非完美（请参阅 [图 5.13](#ch05fig13) 中的示例）。当您检查结果时，请记住，在浏览器内评估是公平的，并且反映了模型的真实泛化能力，因为训练模型在浏览器中要解决的示例与迁移学习过程中它所见到的训练和验证示例不同。
- en: 'To wrap up this section, we showed how a model trained previously on image
    classification can be applied successfully to a different task: object detection.
    In doing this, we demonstrated how to define a custom loss function to fit the
    “dual-task” (shape classification + bounding-box regression) nature of the object-detection
    problem and how to use the custom loss during model training. This example not
    only illustrates the basic principles behind object detection but also highlights
    the flexibility of transfer learning and the range of problems it may be used
    on. Object-detection models used in production applications are, of course, more
    complex and involve more tricks than the toy example we built using a synthesized
    dataset here. [Info box 5.3](#ch05sb03) briefly presents some interesting facts
    about advanced object-detection models, and describes how they are different from
    the simple example you just saw and how you can use one of them through TensorFlow.js.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 结束本节时，我们展示了如何将之前在图像分类上训练的模型成功地应用于不同的任务：对象检测。在此过程中，我们演示了如何定义一个自定义损失函数来适应对象检测问题的“双重任务”（形状分类
    + 边界框回归）的特性，以及如何在模型训练过程中使用自定义损失。该示例不仅说明了对象检测背后的基本原理，还突显了迁移学习的灵活性以及它可能用于的问题范围。在生产应用中使用的对象检测模型当然比我们在这里使用合成数据集构建的玩具示例更复杂，并且涉及更多技巧。[信息框
    5.3](#ch05sb03) 简要介绍了一些有关高级对象检测模型的有趣事实，并描述了它们与您刚刚看到的简单示例的区别以及您如何通过 TensorFlow.js
    使用其中之一。
- en: '|  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Production object-detection models**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**生产对象检测模型**'
- en: '![](05fig14.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig14.jpg)'
- en: An example object-detection result from the TensorFlow.js version of the Single-Shot
    Detection (SSD) model. Notice the multiple bounding boxes and their associated
    object class and confidence scores.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow.js 版本的 Single-Shot Detection (SSD) 模型的一个示例对象检测结果。注意多个边界框及其关联的对象类和置信度分数。
- en: 'Object detection is an important task of interest to many types of applications,
    such as image understanding, industrial automation, and self-driving cars. The
    most well-known state-of-the-art object-detection models include the Single-Shot
    Detection^([[a](#ch05fn2a)]) (SSD, for which an example inference result is shown
    in the figure) and You Only Look Once (YOLO).^([[b](#ch05fn2b)]) These models
    are similar to the model we saw in our simple-object-detection example in the
    following regards:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测是许多类型应用的重要任务，例如图像理解、工业自动化和自动驾驶汽车。最著名的最新对象检测模型包括 Single-Shot Detection^([[a](#ch05fn2a)])（SSD，示例推断结果如图所示）和
    You Only Look Once (YOLO)^([[b](#ch05fn2b)])。这些模型在以下方面与我们在简单对象检测示例中看到的模型类似：
- en: ^a
  id: totrans-326
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-327
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Wei Liu et al., “SSD: Single Shot MultiBox Detector,” *Lecture Notes in Computer
    Science* 9905, 2016, [http://mng.bz/G4qD](http://mng.bz/G4qD).'
  id: totrans-328
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Wei Liu 等人，“SSD: Single Shot MultiBox Detector,” *计算机科学讲义* 9905，2016，[http://mng.bz/G4qD](http://mng.bz/G4qD)。'
- en: ^b
  id: totrans-329
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^b
- en: ''
  id: totrans-330
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Joseph Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection,”
    *Proceedings IEEE Conference on Computer Vision and Pattern Recognition* (CVPR),
    2016, pp. 779–788, [http://mng.bz/zlp1](http://mng.bz/zlp1).'
  id: totrans-331
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Joseph Redmon等人，“You Only Look Once: Unified, Real-Time Object Detection,”
    *IEEE计算机视觉与模式识别会议论文集* (CVPR), 2016, pp. 779–788, [http://mng.bz/zlp1](http://mng.bz/zlp1).'
- en: They predict both the class and location of objects.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们预测对象的类别和位置。
- en: They are built on pretrained image-classification models such as MobileNet and
    VGG16^([[c](#ch05fn2c)]) and are trained through transfer learning.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是建立在MobileNet和VGG16等预训练图像分类模型上的，并通过迁移学习进行训练。
- en: ^c
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^c
- en: ''
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale
    Image Recognition,” submitted 4 Sept. 2014, [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Karen Simonyan和Andrew Zisserman，“Very Deep Convolutional Networks for Large-Scale
    Image Recognition,” 2014年9月4日提交, [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).
- en: 'However, they are also different from our toy model in many regards:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们在很多方面也与我们的玩具模型不同：
- en: Real object-detection models predict many more classes of objects than our simple
    model (for example, the COCO dataset has 80 object categories; see [http://cocodataset.org/#home](http://cocodataset.org/#home)).
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实的目标检测模型预测的对象类别比我们的简单模型多得多（例如，COCO数据集有80个对象类别；参见[http://cocodataset.org/#home](http://cocodataset.org/#home)）。
- en: They are capable of detecting multiple objects in the same image (see the example
    figure).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够在同一图像中检测多个对象（请参见示例图）。
- en: Their model architectures are more complex than the one in our simple model.
    For example, the SSD model adds multiple new heads on top of a truncated pretrained
    image model in order to predict the class confidence score and bounding boxes
    for multiple objects in the input image.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的模型架构比我们简单模型中的更复杂。例如，SSD模型在截断的预训练图像模型顶部添加了多个新头部，以便预测输入图像中多个对象的类别置信度分数和边界框。
- en: 'Instead of using a single `meanSquaredError` metric as the loss function, the
    loss function of a real object-detection model is a weighted sum of two types
    of losses: 1) a softmax cross-entropy-like loss for the probability scores predicted
    for object classes and 2) a `meanSquaredError` or `meanAbsoluteError`-like loss
    for bounding boxes. The relative weight between the two types of loss values is
    carefully tuned to ensure balanced contributions from both sources of error.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与使用单个`meanSquaredError`度量作为损失函数不同，真实目标检测模型的损失函数是两种类型损失的加权和：1）对于对象类别预测的类似softmax交叉熵的损失和
    2）对于边界框的`meanSquaredError`或`meanAbsoluteError`-like损失。两种类型损失值之间的相对权重被精心调整以确保来自两种错误来源的平衡贡献。
- en: Real object-detection models produce a large number of candidate bounding boxes
    per input image. These bounding boxes are “pruned” so that the ones with the highest
    object-class probability scores are retained in the final output.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实的目标检测模型会为每个输入图像产生大量的候选边界框。这些边界框被“修剪”以便保留具有最高对象类别概率分数的边界框作为最终输出。
- en: Some real object-detection models incorporate “prior knowledge” about the location
    of object bounding boxes. These are educated guesses for where the bounding boxes
    are in the image, based on analysis of a larger number of labeled real images.
    The priors help speed up the training of the models by starting from a reasonable
    initial state instead of from complete random guesses (as is in our simple-object-detection
    example).
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些真实的目标检测模型融合了关于对象边界框位置的“先验知识”。这些是关于边界框在图像中位置的教育猜测，基于对更多标记的真实图像的分析。这些先验知识通过从一个合理的初始状态开始而不是完全随机的猜测（就像我们简单的目标检测示例中一样）来加快模型的训练速度。
- en: 'A few real object-detection models have been ported to TensorFlow.js. For example,
    one of the best ones you can play with is in the coco-ssd directory of the tfjs-models
    repository. To see it in action, do the following:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 一些真实的目标检测模型已被移植到TensorFlow.js中。例如，你可以玩的最好的模型之一位于tfjs-models存储库的coco-ssd目录中。要看它的运行情况，执行以下操作：
- en: '[PRE21]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you are interested in learning more about real object-detection models,
    you can read the following blog posts. They are for the SSD model and YOLO model,
    respectively, which use different model architecture and postprocessing techniques:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对了解更多真实目标检测模型感兴趣，你可以阅读以下博文。它们分别是关于SSD模型和YOLO模型的，它们使用不同的模型架构和后处理技术：
- en: '“Understanding SSD MultiBox—Real-Time Object Detection In Deep Learning” by
    Eddie Forson: [http://mng.bz/07dJ](http://mng.bz/07dJ).'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“理解 SSD MultiBox——深度学习中的实时目标检测” by Eddie Forson: [http://mng.bz/07dJ](http://mng.bz/07dJ)。'
- en: '“Real-time Object Detection with YOLO, YOLOv2, and now YOLOv3” by Jona-than
    Hui: [http://mng.bz/KEqX](http://mng.bz/KEqX).'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '“使用 YOLO、YOLOv2 和现在的 YOLOv3 进行实时目标检测” by Jonathan Hui: [http://mng.bz/KEqX](http://mng.bz/KEqX)。'
- en: '|  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: So far in this book, we’ve tackled machine-learning datasets that are handed
    to us and ready to be explored. They are well-formatted, having been cleaned up
    through the painstaking work by data scientists and machine-learning researchers
    before us, to the degree that we can focus on modeling without worrying too much
    about how to ingest the data and whether the data is correct. This is true for
    the MNIST and audio datasets used in this chapter; it’s also certainly true for
    the phishing-website and iris-flower datasets we used in [chapter 3](kindle_split_014.html#ch03).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们处理的是交给我们并准备好探索的机器学习数据集。它们格式良好，已经通过我们之前的数据科学家和机器学习研究人员的辛勤工作进行了清理，以至于我们可以专注于建模，而不用太担心如何摄取数据以及数据是否正确。这对本章中使用的
    MNIST 和音频数据集是真实的；对于我们在[第三章](kindle_split_014.html#ch03)中使用的网络钓和鸢尾花数据集也是如此。
- en: We can safely say that this is *never* the case for real-world machine-learning
    problems you will encounter. Most of a machine-learning practitioner’s time is
    in fact spent on acquiring, preprocessing, cleaning, verifying, and formatting
    the data.^([[13](#ch05fn13)]) In the next chapter, we’ll teach you the tools available
    in TensorFlow.js to make these data-wrangling and ingestion workflows easier.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以肯定地说，这永远不是您将遇到的真实世界机器学习问题的情况。事实上，大多数机器学习从业者的时间都花在获取、预处理、清理、验证和格式化数据上。在下一章中，我们将教您在
    TensorFlow.js 中可用的工具，使这些数据整理和摄取工作流更容易。
- en: ^(13)
  id: totrans-352
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(13)
- en: ''
  id: totrans-353
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Gil Press, “Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science
    Task, Survey Says,” *Forbes*, 23 Mar. 2016, [http://mng.bz/9wqj](http://mng.bz/9wqj).'
  id: totrans-354
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Gil Press，“清理大数据：调查显示，最耗时、最不受欢迎的数据科学任务”，《福布斯》，2016年3月23日，[http://mng.bz/9wqj](http://mng.bz/9wqj)。
- en: Exercises
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: When we visited the mnist-transfer-cnn example in [section 5.1.1](#ch05lev2sec1),
    we pointed out that setting the `trainable` property of a model’s layers won’t
    take effect during training, unless the model’s `compile()` method is called before
    the training. Verify that by making some changes to the `retrainModel()` method
    in the index.js file of the example. Specifically,
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们在[第5.1.1节](#ch05lev2sec1)中访问 mnist-transfer-cnn 示例时，我们指出设置模型层的 `trainable`
    属性在训练期间不会生效，除非在训练之前调用模型的 `compile()` 方法。通过对示例的 index.js 文件中的 `retrainModel()`
    方法进行一些更改来验证这一点。具体来说，
- en: Add a `this.model.summary()` call right before the line with `this.model .compile()`,
    and observe the numbers of trainable and nontrainable parameters. What do they
    show? How are they different from the numbers you get after the `compile()` call?
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在带有 `this.model .compile()` 行之前添加一个 `this.model.summary()` 调用，并观察可训练和不可训练参数的数量。它们显示了什么？它们与在
    `compile()` 调用之后获得的数字有何不同？
- en: Independent from the previous item, move the `this.model.compile()` call to
    the part right before the setting of the `trainable` property of the feature layers.
    In other words, set the property of those layers after the `compile()` call. How
    does that change the training speed? Is the speed consistent with only the last
    several layers of the model being updated? Can you find other ways to confirm
    that, in this case, the weights of the first several layers of the models are
    updated during training?
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 独立于前一项，将 `this.model.compile()` 调用移到特征层的 `trainable` 属性设置之前。换句话说，在 `compile()`
    调用之后设置这些层的属性。这样做会如何改变训练速度？速度是否与仅更新模型的最后几层一致？你能找到其他确认，在这种情况下，模型的前几层的权重在训练期间是否被更新的方法吗？
- en: 'During the transfer learning in [section 5.1.1](#ch05lev2sec1) ([listing 5.1](#ch05ex01)),
    we froze the first two conv2d layers by setting their `trainable` properties to
    `false` before starting the `fit()` call. Can you add some code to the index.js
    in the mnist-transfer-cnn example to verify that the weights of the conv2d layers
    are indeed unaltered by the `fit()` call? Another approach we experimented with
    in the same section was calling `fit()` without freezing the layers. Can you verify
    that the weight values of the layers are indeed altered by the `fit()` call in
    that case? (Hint: recall that in [section 2.4.2](kindle_split_013.html#ch02lev2sec17)
    of [chapter 2](kindle_split_013.html#ch02), we used the `layers` attribute of
    a model object and its `getWeights()` method to access the value of weights.)'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[第5.1.1节](#ch05lev2sec1)的迁移学习中（[列表5.1](#ch05ex01)），我们通过将其`trainable`属性设置为`false`来冻结了前两个conv2d层，然后开始了`fit()`调用。你能在mnist-transfer-cnn示例的index.js中添加一些代码来验证conv2d层的权重确实没有被`fit()`调用改变吗？我们在同一节中尝试的另一种方法是在不冻结层的情况下调用`fit()`。你能验证在这种情况下层的权重值确实被`fit()`调用改变了吗？（提示：回想一下在[第2.4.2节](kindle_split_013.html#ch02lev2sec17)的[第2章](kindle_split_013.html#ch02)中，我们使用了模型对象的`layers`属性和其`getWeights()`方法来访问权重的值。）
- en: Convert the Keras MobileNetV2^([[14](#ch05fn14)]) (not MobileNetV1!—we already
    did that) application into the TensorFlow.js format, and load it into TensorFlow.js
    in the browser. Refer to [info box 5.1](#ch05sb01) for detailed steps. Can you
    use the `summary()` method to examine the topology of MobileNetV2 and identify
    its main differences from MobileNetV1?
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Keras MobileNetV2^([[14](#ch05fn14)])（不是MobileNetV1！—我们已经完成了）应用转换为TensorFlow.js格式，并将其加载到浏览器的TensorFlow.js中。详细步骤请参阅[信息框5.1](#ch05sb01)。你能使用`summary()`方法来检查MobileNetV2的拓扑结构，并确定其与MobileNetV1的主要区别吗？
- en: ^(14)
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(14)
- en: ''
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mark Sandler et al., “MobileNetV2: Inverted Residuals and Linear Bottlenecks,”
    revised 21 Mar. 2019, [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381).'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Mark Sandler等人，“MobileNetV2: Inverted Residuals and Linear Bottlenecks”，2019年3月21日修订，[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)。'
- en: One of the important things about the fine-tuning code in [listing 5.8](#ch05ex08)
    is that the `compile()` method of the model is called again after unfreezing the
    dense layer in the base model. Can you do the following?
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[列表5.8](#ch05ex08)中微调代码的一个重要方面是在基础模型中解冻密集层后再次调用`compile()`方法。你能做到以下几点吗？'
- en: Use the same method from exercise 2 to verify that the weights (kernel and bias)
    of the dense layer are indeed not altered by the first `fit()` call (the one for
    the initial phase of transfer learning) and that they indeed are by the second
    `fit()` call (the one for the fine-tuning phase).
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与练习2相同的方法来验证密集层的权重（核心和偏置）确实没有被第一次`fit()`调用（用于迁移学习的初始阶段）所改变，并且确实被第二次`fit()`调用（用于微调阶段）所改变。
- en: Try commenting out the `compile()` call after the unfreezing line (the line
    that changes the value of the trainable attribute) and see how that affects the
    weight value changes you just observed. Convince yourself that the `compile()`
    call is indeed necessary for letting changes in the frozen/unfrozen states of
    the model take effect.
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在解冻行之后（更改trainable属性值的行）注释掉`compile()`调用，看看这如何影响你刚刚观察到的权重值变化。确信`compile()`调用确实是让模型的冻结/解冻状态变化生效的必要步骤。
- en: Change the code and try unfreezing more weight-carrying layers of the base speech-command
    model (for instance, the conv2d layer before the second-last dense layer) and
    see how that affects the outcome of the fine-tuning.
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改代码，尝试解冻基础speech-command模型的更多承载权重的层（例如，倒数第二个密集层之前的conv2d层），看看这对微调的结果有何影响。
- en: 'In the custom loss function we defined for the simple object-detection task,
    we scaled the 0–1 shape label so the error signal from the shape prediction could
    match the error signal from the bounding-box prediction (see [listing 5.10](#ch05ex10)).
    Experiment with what happens if this scaling is not done by removing the `mul()`
    call in the code in [listing 5.10](#ch05ex10). Convince yourself that this scaling
    is necessary for ensuring reasonably accurate shape predictions. This can also
    be done by simply replacing the instances of `customLossFunction` with `meanSquaredError`
    during the `compile()` call (see [listing 5.11](#ch05ex11)). Also note that removal
    of the scaling during training needs to be accompanied by a change in the thresholding
    during inference time: change the threshold from `CANVAS_SIZE/2` to `1/2` in the
    inference logic (in simple-object-detection/index.js).'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们为简单对象检测任务定义的自定义损失函数中，我们对 0-1 形状标签进行了缩放，以便来自形状预测的误差信号与来自边界框预测的误差信号匹配（参见 [listing
    5.10](#ch05ex10) 中的代码）。试验一下，如果在代码中删除 `mul()` 调用会发生什么。说服自己这种缩放对于确保相当准确的形状预测是必要的。这也可以通过在
    `compile()` 调用中简单地将 `customLossFunction` 的实例替换为 `meanSquaredError` 来完成（参见 [listing
    5.11](#ch05ex11)）。还要注意，训练期间移除缩放需要伴随着对推断时的阈值调整：将推断逻辑中的阈值从 `CANVAS_SIZE/2` 更改为 `1/2`（在
    simple-object-detection/index.js 中）。
- en: 'The fine-tuning phase in the simple object-detection example involved unfreezing
    the nine top layers of the truncated MobileNet base (see how `fineTuningLayers`
    is populated in [listing 5.9](#ch05ex09)). A natural question to ask is, why nine?
    In this exercise, change the number of unfrozen layers by including fewer or more
    layers in the `fineTuningLayers` array. What do you expect to see in the following
    quantities when you unfreeze fewer layers during fine-tuning: 1) the final loss
    value and 2) the time each epoch takes in the fine-tuning phase? Does the experiment
    result match your expectations? How about unfreezing more layers during fine-tuning?'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在简单对象检测示例中的微调阶段，涉及到解冻截断的 MobileNet 基础的前九个顶层（参见 [listing 5.9](#ch05ex09) 中的 `fineTuningLayers`
    如何填充）。一个自然的问题是，为什么是九个？在这个练习中，通过在 `fineTuningLayers` 数组中包含更少或更多的层来改变解冻层的数量。当你在微调期间解冻更少的层时，你期望在以下情况下会看到什么：1)
    最终损失值和 2) 每个纪元在微调阶段花费的时间？实验结果是否符合你的期望？解冻更多的层在微调期间会怎样？
- en: Summary
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Transfer learning is the process of reusing a pretrained model or a part of
    it on a learning task related to, but different from, the one that the model was
    originally trained for. This reusing speeds up the new learning task.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习是在与模型最初训练的任务相关但不同的学习任务上重新使用预训练模型或其一部分的过程。这种重用加快了新的学习任务的速度。
- en: In practical applications of transfer learning, people often reuse convnets
    that have been trained on very large classification datasets, such as MobileNet
    trained on the ImageNet dataset. Due to the sheer size of the original dataset
    and the diversity of the examples it contains, such pretrained models bring with
    them convolutional layers that are powerful, general-purpose feature extractors
    for a wide variety of compute-vision problems. Such layers are difficult, if not
    impossible, to train with the small amount of data that are available in typical
    transfer-learning problems.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在迁移学习的实际应用中，人们经常会重用已经在非常大的分类数据集上训练过的卷积网络，比如在 ImageNet 数据集上训练过的 MobileNet。由于原始数据集的规模庞大以及包含的示例的多样性，这些预训练模型带来了强大的、通用的特征提取器卷积层，适用于各种计算机视觉问题。这样的层对于在典型的迁移学习问题中可用的少量数据来说很难，如果不是不可能的话，进行训练。
- en: We discussed several general approaches of transfer learning in TensorFlow.js,
    which differ from each other in terms of 1) whether new layers are created as
    the “new head” for transfer learning and 2) whether the transfer learning is done
    with one model instance or two. Each approach has its pros and cons and is suited
    for different use cases (see [table 5.1](#ch05table01)).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在 TensorFlow.js 中讨论了几种通用的迁移学习方法，它们在以下方面有所不同：1) 新层是否被创建为迁移学习的“新头部”，以及 2) 是否使用一个模型实例或两个模型实例进行迁移学习。每种方法都有其优缺点，并适用于不同的用例（参见
    [table 5.1](#ch05table01)）。
- en: By setting the `trainable` attribute of a model’s layer, we can prevent its
    weights from being updated during training (`Model.fit()` calls). This is referred
    to as freezing and is used to “protect” the base model’s feature-extraction layers
    during transfer learning.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置模型层的`trainable`属性，我们可以防止在训练（`Model.fit()`函数调用）期间更新其权重。这被称为冻结，并用于在迁移学习中“保护”基础模型的特征提取层。
- en: In some transfer-learning problems, we can boost the new model’s performance
    by unfreezing a few top layers of the base model after an initial phase of training.
    This reflects the adaptation of the unfrozen layers to the unique features in
    the new dataset.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些迁移学习问题中，我们可以在初始训练阶段之后解冻一些基础模型的顶层，从而提升新模型的性能。这反映了解冻层对新数据集中独特特征的适应性。
- en: Transfer learning is a versatile and flexible technique. The base model can
    help us solve problems that are different from the one that it is originally trained
    on. We illustrated this point by showing how to train an object-detection model
    based on MobileNet.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习是一种多用途、灵活的技术。基础模型可以帮助我们解决与其初始训练内容不同的问题。我们通过展示如何基于MobileNet训练目标检测模型来阐明这一点。
- en: Loss functions in TensorFlow.js can be defined as custom JavaScript functions
    that operate on tensor inputs and outputs. As we showed in the simple object-detection
    example, custom loss functions are often needed to solve practical machine-learning
    problems.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow.js中的损失函数可以定义为在张量输入和输出上操作的自定义JavaScript函数。正如我们在简单目标检测示例中展示的，为了解决实际的机器学习问题，通常需要自定义损失函数。
