- en: '2 Getting started with baselines: Data preprocessing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 开始使用基线：数据预处理
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括
- en: Introducing a pair of natural language processing (NLP) problems
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍一对自然语言处理（NLP）问题
- en: Obtaining and preprocessing NLP data for such problems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取和预处理用于此类问题的自然语言处理数据
- en: Establishing baselines for these problems using key *generalized linear methods*
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用关键的*广义线性方法*为这些问题建立基线
- en: In this chapter, we dive directly into solving NLP problems. This will be a
    two-part exercise, spanning this chapter and the next. Our goal will be to establish
    a set of baselines for a pair of concrete NLP problems, which we will later be
    able to use to measure progressive improvements gained from leveraging increasingly
    sophisticated transfer learning approaches. In the process of doing this, we aim
    to advance your general NLP instincts and refresh your understanding of typical
    procedures involved in setting up problem-solving pipelines for such problems.
    You will review techniques ranging from tokenization to data structure and model
    selection. We first train some traditional machine learning models from scratch
    to establish some preliminary baselines for these problems. We complete the exercise
    in chapter 3, where we apply the simplest form of transfer learning to a pair
    of recently popularized deep pretrained language models. This involves fine-tuning
    only a handful of the final layers of each network on a target dataset. This activity
    will serve as a form of an applied hands-on introduction to the main theme of
    the book—transfer learning for NLP.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们直接着手解决自然语言处理问题。这将是一个分为两部分的练习，横跨本章和下一章。我们的目标是为一对具体的自然语言处理问题建立一组基线，以便稍后用于衡量利用越来越复杂的迁移学习方法获得的渐进改进。在此过程中，我们旨在提升您的一般自然语言处理直觉，并更新您对为此类问题设置问题解决流程所涉及的典型程序的理解。您将复习从分词到数据结构和模型选择等技术。我们首先从头开始训练一些传统的机器学习模型，为这些问题建立一些初步的基线。我们在第3章中完成练习，在那里我们将最简单形式的迁移学习应用于一对最近流行的深度预训练语言模型。这涉及在目标数据集上仅微调每个网络的最终几层。这项活动将作为本书主题——自然语言处理的迁移学习的实际动手介绍的一种形式。
- en: 'We will focus on a pair of important representative example NLP problems: spam
    classification of email and sentiment classification of movie reviews. This exercise
    will arm you with a number of important skills, including some tips for obtaining,
    visualizing, and preprocessing data. We will cover three major model classes:
    generalized linear models such as logistic regression, decision tree-based models
    such as random forests, and neural network-based models such as ELMo. These classes
    are additionally represented by support vector machines (SVMs) with linear kernels,
    gradient-boosting machines (GBMs), and BERT, respectively. The different types
    of models to be explored are shown in figure 2.1\. Note that we do not explicitly
    address rule-based methods. A widely used example of these is a simple keyword-matching
    approach that would label all emails containing certain preselected phrases; for
    example, “free lottery tickets” as spam, and “amazing movie” as a positive review.
    Such methods are often implemented as the first attempt at solving NLP problems
    in many industrial applications but are quickly found to be brittle and difficult
    to scale. As such, we do not discuss rule-based approaches much further. We discuss
    data for the problems and its preprocessing, and introduce and apply generalized
    linear methods to the data in this chapter. In the next chapter, which serves
    as part two of the overall exercise, we apply decision-tree-based methods and
    neural-network-based methods to the data.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于一对重要的代表性示例自然语言处理问题：电子邮件的垃圾分类和电影评论的情感分类。这个练习将装备您一些重要的技能，包括一些获取、可视化和预处理数据的技巧。我们将涵盖三种主要的模型类别：广义线性模型，如逻辑回归，基于决策树的模型，如随机森林，以及基于神经网络的模型，如
    ELMo。这些类别另外由具有线性核的支持向量机（SVM），梯度提升机（GBM）和 BERT 所代表。要探索的不同类型的模型如图 2.1 所示。请注意，我们不明确讨论基于规则的方法。这些方法的一个广泛使用的示例是简单的关键词匹配方法，该方法会将包含某些预先选择的短语的所有电子邮件标记为垃圾邮件，例如，“免费彩票”作为垃圾邮件，“了不起的电影”作为正面评价。这些方法通常作为许多工业应用中解决自然语言处理问题的首次尝试，但很快被发现脆弱且难以扩展。因此，我们不再深入讨论基于规则的方法。我们在本章讨论这些问题的数据及其预处理，并引入和应用广义线性方法。在下一章，作为整体练习的第二部分，我们将决策树方法和神经网络方法应用于数据。
- en: '![02_01](../Images/02_01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![02_01](../Images/02_01.png)'
- en: Figure 2.1 The different types of supervised models to be explored in the content
    classification examples in this and the next chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1本章和下一章将探讨文本分类示例中不同类型的监督模型。
- en: We present code samples for every example and model class represented to enable
    you to quickly pick up the essence of these technologies, while also allowing
    you to develop coding skills that will directly transfer to your own problems.
    All code snippets are provided as rendered Jupyter notebooks in the companion
    GitHub repository to this book,[¹](#pgfId-1082177) as well as Kaggle notebooks/kernels
    that you can begin running within a few minutes without dealing with any installation
    or dependency issues. Rendered Jupyter notebooks provide representative output
    that can be expected when they are executed correctly, and Kaggle provides a browser-based
    Jupyter execution environment, which also offers a limited amount of free GPU
    computing. A mostly equivalent alternative to this is Google Colab, but this is
    not the system we elected to employ here. Jupyter can also be easily installed
    locally with Anaconda, and you are welcome to convert the notebooks into .py scripts
    for local execution, if that is your preference. However, the Kaggle notebooks
    are the recommended way of executing these methods, because they will allow you
    to get moving right away without any setup delays. Moreover, the free GPU resources
    provided by this service at the time of writing expand the accessibility of all
    these methods to people who may not have access to powerful GPUs locally, which
    is consistent with the “democratization of AI” agenda that excites so many people
    about NLP transfer learning. Appendix A provides a Kaggle quick start guide and
    a number of the author’s personal tips on how to maximize the platform’s usefulness.
    However, we anticipate that most readers should find it pretty self-explanatory
    to get started. Please also note the important technical caveats in the note that
    follows.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '我们为每个示例和模型类别提供了代码示例，让你能快速掌握这些技术的要点，同时也可以培养编码技巧，以便能够直接应用到自己的问题中。所有代码都以渲染后的 Jupyter
    笔记本形式提供在本书的[伴随 GitHub 代码库](#pgfId-1082177)，以及 Kaggle 笔记本/内核中。你可以在几分钟内开始运行 Kaggle
    笔记本/内核，而无需处理任何安装或依赖问题。渲染后的 Jupyter 笔记本提供了在正确执行时可以预期的输出示例，而 Kaggle 提供了基于浏览器的 Jupyter
    执行环境，同时还提供了有限的免费 GPU 计算资源。虽然 Google Colab 是 Jupyter 的主要替代方案之一，但我们选择在这里使用 Kaggle。你也可以使用
    Anaconda 在本地轻松安装 Jupyter，并欢迎将笔记本转换为 .py 脚本以供本地执行，如果你更喜欢的话。然而，我们推荐使用 Kaggle 笔记本来执行这些方法，因为它们可以让你立即开始，无需任何设置延迟。此外，在撰写本文时，该服务提供的免费
    GPU 资源扩大了所有这些方法的可访问性，使那些可能没有本地强大 GPU 资源的人们也能够使用，与关于 NLP 迁移学习的“人工智能民主化”议程保持一致，这激发了很多人的兴趣。附录A提供了一个
    Kaggle 快速入门指南，以及作者对如何最大化平台价值的个人建议。然而，我们预计大多数读者应该可以很轻松地开始使用它。请注意在下面的注释中附带的重要技术注意事项。 '
- en: Note Kaggle frequently updates the dependencies, that is, the versions of the
    installed libraries on its Docker images. To ensure that you are using the same
    dependencies as we did when we wrote the code—to guarantee the code works with
    minimal changes out of the box—please make sure to select “Copy and Edit Kernel”
    for each notebook of interest, links to which are listed in the companion repository
    to the book. If you copy and paste the code into a new notebook and don’t follow
    this recommended process, you may need to adapt the code slightly for the specific
    library versions installed for that notebook at the time you created it. This
    recommendation also applies if you elect to install a local environment. For local
    installation, pay attention to the frozen dependency requirement list we have
    shared in the companion repository, which will guide you on which versions of
    libraries you will need. Please note that this requirements file is for the purpose
    of documenting and exactly replicating the environment on Kaggle on which the
    results reported in the book were achieved; on a different infrastructure, it
    can be used only as a guide, and you shouldn’t expect it to work straight out
    of the box due to many potential architecture-specific dependency conflicts. Moreover,
    most of the requirements will not be necessary for local installation. Finally,
    please note that because ELMo has not yet been ported to TensorFlow 2.x at the
    time of this writing, we are forced to use TensorFlow 1.x to compare it fairlyto
    BERT. In the companion repository, we do, however, provide an illustration of
    how to use BERT with TensorFlow 2.x for the spam classification example.2 We transition
    in later chapters from TensorFlow and Keras to the Hugging Face transformers library,
    which uses TensorFlow 2.x. You could view the exercise in chapters 2 and 3 as
    a historical record of and experience with early packages that were developed
    for NLP transfer learning. This exercise simultaneously helps you juxtapose TensorFlow
    1.x with 2.x.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Kaggle 经常更新依赖项，即其 Docker 镜像上安装的库的版本。为了确保您使用的是我们编写代码时使用的相同依赖项——以保证代码可以直接使用而进行最小更改，请确保对感兴趣的每个笔记本选择“复制并编辑内核”，这些笔记本的链接列在本书的伴随存储库中。如果您将代码复制粘贴到一个新的笔记本中，并且不遵循此推荐过程，您可能需要针对创建它时为该笔记本安装的特定库版本稍微调整代码。如果选择在本地环境中安装，请注意我们在伴随存储库中共享的冻结依赖要求列表，该列表将指导您需要哪些库的版本。请注意，此要求文件旨在记录并完全复制在
    Kaggle 上实现书中报告结果的环境；在不同的基础设施上，它只能用作指南，并且您不应期望它直接使用，因为可能存在许多潜在的与架构相关的依赖冲突。此外，对于本地安装，大多数要求都不是必需的。最后，请注意，由于在撰写本文时
    ELMo 尚未移植到 TensorFlow 2.x，我们被迫使用 TensorFlow 1.x 来公平比较它和 BERT。在伴随存储库中，我们确实提供了如何在
    TensorFlow 2.x 中使用 BERT 进行垃圾邮件分类示例的示例。我们在后续章节中从 TensorFlow 和 Keras 过渡到使用 TensorFlow
    2.x 的 Hugging Face transformers 库。您可以将第 2 章和第 3 章中的练习视为早期为 NLP 迁移学习开发的早期软件包的历史记录和体验。这个练习同时帮助您将
    TensorFlow 1.x 与 2.x 进行对比。
- en: 2.1 Preprocessing email spam classification example data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 预处理电子邮件垃圾分类示例数据
- en: 'In this section, we introduce the first example dataset we will look at in
    this chapter. Here, we are interested in developing an algorithm that can detect
    whether or not any given email is spam at scale. To do this, we will build a dataset
    from two separate sources: the popular Enron email corpus as a proxy for email
    that is not spam, and a collection of “419” fraudulent emails as a proxy for email
    that is spam.[²](#pgfId-1086740)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了本章将要讨论的第一个示例数据集。在这里，我们有兴趣开发一个算法，它可以在规模上检测任何给定的电子邮件是否为垃圾邮件。为此，我们将从两个独立的来源构建数据集：流行的恩隆电子邮件语料库作为非垃圾邮件的代理，以及一系列“419”欺诈邮件作为垃圾邮件的代理。
- en: We will view this as a supervised classification task, where we will first train
    a classifier on a collection of emails labeled as either spam or not spam. Although
    some labeled datasets exist online for training and testing that match this problem
    closely, we will instead take the route of creating our own dataset from some
    other well-known email data sources. The reason for doing this is to more closely
    represent how data collection and preprocessing often happen in practice, where
    datasets first have to be built and curated, versus the simplified manner in which
    these processes are often represented in the literature.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这看作是一个监督分类任务，在这个任务中，我们将首先在一组被标记为垃圾邮件或非垃圾邮件的电子邮件上训练一个分类器。虽然在线上存在一些标记数据集用于训练和测试，与这个问题密切相关，但我们将采取另一种方式，从一些其他知名的电子邮件数据源创建我们自己的数据集。这样做的原因是更贴近实践中数据收集和预处理通常发生的方式，其中数据集首先必须被构建和筛选，而不是文献中这些过程通常被简化的方式。
- en: In particular, we will sample the Enron corpus—the largest public email collection,
    related to the notorious Enron financial scandal—as a proxy for email that are
    not spam, and sample “419” fraudulent emails, representing the best known type
    of spam, as a proxy for email that are spam. Both of these types of email are
    openly available on Kaggle,[³](#pgfId-1082193),[⁴](#pgfId-1082197) the popular
    data science competition platform, which makes running the examples there particularly
    easy without too many local resources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是，我们将采样安然公司语料库——最大的公开电子邮件收集，与臭名昭著的安然金融丑闻有关——作为非垃圾邮件的代理，以及采样“419”欺诈邮件，代表最为知名的垃圾邮件类型，作为垃圾邮件的代理。这两种类型的电子邮件都可以在Kaggle上公开获取，[³](#pgfId-1082193),[⁴](#pgfId-1082197)，这是一个流行的数据科学竞赛平台，这使得在那里运行示例特别容易，而不需要太多的本地资源。
- en: The Enron corpus contains about half a million emails written by employees of
    the Enron Corporation, as collected by the Federal Energy Commission for the purposes
    of investigating the collapse of the company. This corpus has been used extensively
    in the literature to study machine learning methods for email applications and
    is often the first data source researchers working with emails look to for initial
    experimentation with algorithm prototypes. On Kaggle, it is available as a single-column
    .csv file with one email per row. Note that this data is still cleaner than one
    can expect to typically find in many practical applications in the wild.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 安然语料库包含大约五十万封由安然公司员工撰写的电子邮件，由联邦能源委员会收集，用于调查该公司的倒闭。这个语料库在文献中被广泛用于研究用于电子邮件应用的机器学习方法，并且通常是研究人员与电子邮件一起进行初步算法原型实验的首选数据源。在Kaggle上，它作为一个单列.csv文件提供，每行一个电子邮件。请注意，与许多实际应用中可能找到的情况相比，此数据仍然更干净。
- en: Figure 2.2 shows the sequence of steps that will be performed on each email
    in this example. The body of the email will first be separated from the headers
    of the email, some statistics about the dataset will be teased out to get a sense
    of the data, stopwords will be removed from the email, and it will then be classified
    as either spam or not spam.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2显示了在这个示例中将在每封电子邮件上执行的步骤序列。电子邮件的正文将首先与电子邮件的标头分开，将提取一些关于数据集的统计信息以了解数据的情况，将从电子邮件中删除停用词，然后将其分类为垃圾邮件或非垃圾邮件。
- en: '![02_02](../Images/02_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![02_02](../Images/02_02.png)'
- en: Figure 2.2 Sequence of preprocessing tasks to be performed on input email data
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 对输入电子邮件数据执行的预处理任务序列
- en: 2.1.1 Loading and visualizing the Enron corpus
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 加载和可视化安然公司语料库
- en: 'The first thing we need to do is load the data with the popular Pandas library
    and take a peek at a slice of the data to make sure we have a good sense of what
    it looks like. Listing 2.1 shows the code to do that once the Enron corpus dataset
    has been obtained and placed in the location specified by the variable `filepath`
    (in this case, it is pointing to its location in our Kaggle notebook). Ensure
    all libraries are PIP-installed before importing via the following command:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是使用流行的Pandas库加载数据，并查看数据的一个切片，以确保我们对数据的外观有一个良好的了解。清单2.1展示了一旦获取了安然公司语料库数据集并放置在变量`filepath`指定的位置（在这种情况下，它指向我们Kaggle笔记本中的位置）后，要执行的代码。在导入之前，请确保所有库都已通过以下命令进行PIP安装：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Listing 2.1 Loading the Enron corpus
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 清单2.1 加载安然公司语料库
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Linear algebra
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 线性代数
- en: ❷ Data processing, CSV file I/O (e.g., pd.read_csv)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数据处理，CSV文件输入输出（例如，pd.read_csv）
- en: ❸ Reads the data into a Pandas DataFrame called emails
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据读入名为emails的Pandas DataFrame中
- en: ❹ Displays status and some loaded emails
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 显示状态和一些加载的电子邮件
- en: 'Successful execution of the code will confirm the number of columns and rows
    loaded, and display the first five rows of the loaded Pandas *DataFrame*, through
    an output that looks like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '执行代码成功后将确认加载的列数和行数，并显示加载的 Pandas *DataFrame* 的前五行，输出如下所示:'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Although this exercise has allowed us to get a sense of the resulting DataFrame
    and get a good feel for its shape, it is not too clear what each individual email
    looks like. To achieve this, we take a closer look at the very first email via
    the next line of code
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个练习让我们对结果 DataFrame 有了一个了解，并形成了一个很好的形状感觉，但还不太清楚每封单独的电子邮件是什么样子。为了达到这个目的，我们通过下一行代码仔细检查第一封电子邮件
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'to produce the following output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '产生以下输出:'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We see that the messages are contained within the *message* column of the resulting
    DataFrame, with the extra fields at the beginning of each message—including *Message
    ID*, *To*, *From*, and so on—being referred to as the message’s *header information*
    or simply *header*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现消息都包含在结果 DataFrame 的 *message* 列中，每条消息开头的额外字段——包括 *Message ID*、*To*、*From*
    等——被称为消息的 *头信息* 或简称*头部*。
- en: Traditional spam classification methods derive features from the header information
    for classifying the message as spam or not. Here, we would like to perform the
    same task based on the content of the message only. One possible motivation for
    this approach is the fact that email training data may often be de-identified
    in practice due to privacy concerns and regulations, thereby making header info
    unavailable. Thus, we need to separate the headers from the messages in our dataset.
    We do this via the function shown in the next listing. It employs the email package
    for processing email messages, which comes prepacked with Python (that is, it
    does not need to be PIP-installed).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的垃圾邮件分类方法是从头信息中提取特征来对消息进行分类。在这里，我们希望仅基于消息内容执行相同的任务。采用该方法的一个可能动机是，由于隐私问题和法规的原因，电子邮件训练数据在实践中经常会被去标识化，因此头部信息是不可用的。因此，我们需要在数据集中将头部信息与消息分离。我们通过下面的函数来实现这一点。它使用了
    Python 预装的电子邮件包来处理电子邮件消息（因此无需通过PIP进行安装）。
- en: Listing 2.2 Separating and extracting email bodies from header information
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 头信息分离和提取电子邮件正文
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Returns a message object structure from a string
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从字符串返回消息对象结构
- en: ❷ Gets the message body
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取消息正文
- en: 'We now execute the email-body-extracting code as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们执行提取电子邮件正文的代码如下:'
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'which confirms success by printing the following text to screen:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '通过以下文本确认成功打印到屏幕:'
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We then can display some processed emails via
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过以下方式显示一些已处理的电子邮件：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'with the display confirming successful execution by resembling the following
    output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '显示确认成功执行，输出如下:'
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 2.1.2 Loading and visualizing the fraudulent email corpus
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 加载和可视化欺诈邮件语料库
- en: Having loaded the Enron emails, let’s do the same for the “419” fraudulent email
    corpus, so that we can have some example data in our training set representing
    the spam class. Obtain the dataset from the Kaggle link that was presented earlier,
    making sure to adjust the `filepath` variable accordingly (or just use our Kaggle
    notebooks that already have the data attached to them), and repeat the steps as
    shown in listing 2.3.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 加载了 Enron 电子邮件之后，让我们对“419”欺诈邮件语料库做同样的操作，这样我们就可以在训练集中有一些代表垃圾邮件类别的示例数据。从前面呈现的
    Kaggle 链接获取数据集，确保相应调整`filepath`变量（或者直接使用我们的 Kaggle 笔记本，已经包含了数据），然后按照列表 2.3 中所示重复执行步骤。
- en: Note Because this dataset comes as a .txt file, versus a .csv file, the preprocessing
    steps are slightly different. First of all, we have to specify the encoding when
    reading the file as Latin-1; otherwise the default encoding option of UTF-8 will
    fail. It is often the case in practice that one needs to experiment with a number
    of different encodings, with the aforementioned two being the most popular ones,
    to get some datasets to read correctly. Additionally, note that because this .txt
    file is one big column of emails (with headers) separated by line breaks and white
    space, and is not separated nicely into rows with one email per row—as was the
    case for the Enron corpus—we can’t use Pandas to neatly load it as we did before.
    We will read all the emails into a single string and split the string on a code
    word that appears close to the beginning of each email’s header, for example,
    “From r.” Please see our rendered notebooks that visualize this data on either
    GitHub or Kaggle to verify that this unique code word appears close to the beginning
    of each fraudulent email in this dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 因为这个数据集是以 .txt 文件格式提供的，而不是 .csv 文件，因此预处理步骤略有不同。首先，我们必须在读取文件时指定编码为 Latin-1；否则，默认编码选项UTF-8会失败。实际上经常出现这样的情况，需要尝试多种不同的编码方式，其中前面提到的两种是最受欢迎的，来使一些数据集能够正确读取。此外，需要注意的是，由于这个
    .txt 文件是一大列带有标题的电子邮件（用换行符和空白分隔），并且没有很好地分隔成每行一个电子邮件，而并不像 Enron 语料库那样整齐地分隔成各行各个邮件，我们无法像之前那样使用Pandas
    将其整齐地加载。我们将所有的邮件读入一个字符串，然后根据出现在每封邮件标题开头附近的代码词进行分割，例如，“From r.” 请查看我们在 GitHub 或
    Kaggle 上呈现的笔记本来验证此数据上是否存在这个独特代码词出现在每封此数据集中的诈骗邮件的开头附近。
- en: Listing 2.3 Loading the “419” fraudulent email corpus
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 加载“419”诈骗邮件语料库
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Split on a code word appearing close to the beginning of each email
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在每封电子邮件开头附近的代码词上进行分割
- en: 'The following output confirms the success of the loading process:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出证实了加载过程的成功：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that the fraudulent data is loaded as a list, we can convert it into a
    Pandas DataFrame in order to process it with the functions we have already defined,
    as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，伪造的数据已经以列表的形式加载，我们可以将其转换为Pandas DataFrame，以便用我们已经定义的函数来处理，具体如下：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Successful execution of this code segment will lead to output that gives us
    a sense of the first five emails that were loaded, as shown next:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 成功执行此代码段将导致输出，让我们对加载的前五封邮件有所了解，如下所示：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Having loaded both datasets, we are now ready to sample emails from each one
    into a single DataFrame that will represent the overall dataset covering both
    classes of emails. Before doing this, we must decide how many samples to draw
    from each class. Ideally, the number of samples in each class will represent the
    natural distribution of emails in the wild—if we expect our classifier to encounter
    60% spam emails and 40% nonspam emails when deployed, then a ratio such as 600
    to 400, respectively, might make sense. Note that a severe imbalance in the data,
    such as 99% for nonspam and 1% for spam, may overfit to predict nonspam most of
    the time, an issue than needs to be considered when building datasets. Because
    this is an idealized experiment, and we do not have any information on the natural
    distributions of classes, we will assume a 50/50 split. We also need to give some
    thought to how we are going to tokenize the emails, that is, split emails into
    subunits of text—words, sentences, and so forth. To start off, we will tokenize
    into words, because this is the most common approach. We must also decide the
    maximum number of tokens per email and the maximum length of each token to ensure
    that the occasional extremely long email does not bog down the performance of
    our classifier. We do all this by specifying the following general hyperparameters,
    which will later be tuned experimentally to enhance performance as needed:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了两个数据集之后，我们现在准备从每个数据集中抽样电子邮件到一个单独的 DataFrame 中，该 DataFrame 将代表覆盖两类电子邮件的整体数据集。在这样做之前，我们必须决定从每个类别中抽取多少样本。理想情况下，每个类别中的样本数量将代表野外电子邮件的自然分布——如果我们希望我们的分类器在部署时遇到
    60% 的垃圾邮件和 40% 的非垃圾邮件，那么 600 和 400 的比率可能是有意义的。请注意，数据的严重不平衡，例如 99% 的非垃圾邮件和 1% 的垃圾邮件，可能会过度拟合以大多数时间预测非垃圾邮件，这是在构建数据集时需要考虑的问题。由于这是一个理想化的实验，我们没有任何关于类别自然分布的信息，我们将假设是
    50/50 的分布。我们还需要考虑如何对电子邮件进行标记化，即将电子邮件分割成文本的子单元——单词、句子等等。首先，我们将标记化为单词，因为这是最常见的方法。我们还必须决定每封电子邮件的最大标记数和每个标记的最大长度，以确保偶尔出现的极长电子邮件不会拖慢分类器的性能。我们通过指定以下通用超参数来完成所有这些工作，稍后将通过实验调整以根据需要提高性能：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Number of samples to generate in each class—spam and not spam
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个类别生成的样本数——垃圾邮件和非垃圾邮件
- en: ❷ The maximum number of tokens per document
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每个文档的最大标记数
- en: ❸ The maximum length of each token
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个标记的最大长度
- en: With these hyperparameters specified, we can now create a single DataFrame for
    the overarching training dataset. Let’s take the opportunity to also perform the
    remaining preprocessing tasks, namely, removing stop words, punctuations, and
    tokenizing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些指定的超参数，我们现在可以为全局训练数据集创建一个单独的 DataFrame。让我们利用这个机会执行剩余的预处理任务，即删除停用词、标点符号和标记化。
- en: Let’s proceed by defining a function to tokenize emails by splitting them into
    words as shown in the following listing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过定义一个函数来对邮件进行标记化，将它们分割成单词，如下列表所示。
- en: Listing 2.4 Tokenizing each email into words
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 将每封电子邮件标记化为单词
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Split every email string on spaces to create a list of word tokens.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 按空格分割每个电子邮件字符串，以创建单词标记列表。
- en: Taking another look at the emails on the previous two pages, we see that they
    contain a lot of punctuation characters, and the spam emails tend to be capitalized.
    In order to ensure that classification is done based on language content only,
    we define a function to remove punctuation marks and other non-word characters
    from the emails. We do this by employing *regular expressions* with the Python
    *regex* library. We also normalize words by turning them into lower case with
    the Python string function `.lower()`. The preprocessing function is shown in
    the next listing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看前两页的电子邮件，我们发现它们包含大量的标点符号，并且垃圾邮件往往是大写的。为了确保分类仅基于语言内容进行，我们定义了一个函数，用于从电子邮件中删除标点符号和其他非单词字符。我们通过使用
    Python 的 *regex* 库来使用正则表达式实现这一点。我们还通过使用 Python 字符串函数 `.lower()` 将单词转换为小写来规范化单词。预处理函数如下列表所示。
- en: Listing 2.5 Removing punctuation and other nonword characters from emails
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 从电子邮件中删除标点符号和其他非单词字符
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Match and remove any nonword characters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 匹配并移除任何非单词字符。
- en: ❷ Truncate token
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 截断标记
- en: Finally, let’s define a function to remove *stop words*—words that occur so
    frequently in language that they offer no useful information for classification.
    This includes words such as “the” and “are,” and the popular library NLTK provides
    a heavily used list that we will employ. The stop word removal function is shown
    in the next listing. Note that NLTK also has some methods for punctuation removal,
    as an alternative to what was done in listing 2.5.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们定义一个函数来移除*停用词*—在语言中频繁出现但对分类没有用的词。这包括诸如“the”和“are”等词，在流行的库 NLTK 中提供了一个被广泛使用的列表，我们将使用它。停用词移除函数在下一个清单中展示。请注意，NLTK
    还有一些用于去除标点的方法，作为清单 2.5 所做的替代方法。
- en: Listing 2.6 Remove stop words
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 移除停用词
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ This is where stop words are actually removed from token list.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这就是从标记列表中实际移除停用词的地方。
- en: ❷ Removes empty strings—'', None, and so on—as well
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 同样移除空字符串—''，None 等等
- en: We are now going to put all these functions together to build the single dataset
    representing both classes. The process is illustrated by the script in the next
    listing. In that script, we convert the combined result into a NumPy array, because
    this is the input data format expected by many of the libraries we will use.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把所有这些功能整合在一起，构建代表两个类别的单一数据集。该过程在下一个代码清单中演示。在那段代码中，我们将合并的结果转换为 NumPy 数组，因为这是许多我们将使用的库所期望的输入数据格式。
- en: Listing 2.7 Putting preprocessing steps together to build email dataset
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.7 将预处理步骤组合在一起构建电子邮件数据集
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Applies predefined processing functions
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用预定义的处理函数
- en: ❷ Samples the right number of emails from each class
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从每个类别中抽取正确数量的电子邮件样本
- en: ❸ Converts to NumPy array
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 转换为 NumPy 数组
- en: 'Now let’s take a peek at the result to make sure things are proceeding as expected:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一眼结果，确保事情正在按预期进行：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This yields the following output:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了如下输出：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We see that the resulting array has divided the text into word units, as we
    intended.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到生成的数组已经将文本分割成了单词单位，正如我们想要的。
- en: 'Let’s create the headers corresponding to these emails, consisting of `Nsamp`=1000
    of spam emails followed by `Nsamp`=1000 of nonspam emails, as shown next:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建相应的与这些电子邮件对应的头，包括`Nsamp`=1000封垃圾邮件，然后是`Nsamp`=1000封非垃圾邮件，如下所示：
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We are now ready to convert this NumPy array into numerical features that can
    actually be fed to the algorithms for classification.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备将这个 NumPy 数组转换为可以实际输入到分类算法中的数值特征。
- en: 2.1.3 Converting the email text into numbers
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 将电子邮件文本转换为数字
- en: In this chapter, we start by employing what is often considered the simplest
    method for *vectorizing* words, that is, converting them into numerical vectors—the
    *bag-of-words* model. This model simply counts the frequency of word tokens contained
    in each email and thereby represents it as a vector of such frequency counts.
    We present the function for assembling the bag-of-words model for emails in listing
    2.8\. Please note that in doing this, we retain only tokens that appear more than
    once, as captured by the variable `used_tokens`. This enables us to keep the vector
    dimensions significantly lower than they would be otherwise. Please also note
    that one can achieve this using various built-in vectorizers in the popular library
    scikit-learn (our Jupyter notebook shows how to do this). However, we focus on
    the approach shown in listing 2.8, because we find it to be more illustrative
    than a black box function achieving the same. We also note the scikit-learn vectorization
    methods include counting occurrences of sequences of any *n* words, or *n-grams*,
    as well as the *tf-idf* approach—important fundamental concepts you should brush
    up on if rusty. For the problems shown here, we did not notice an improvement
    when using these vectorization methods over the bag-of-words approach.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先采用了通常被认为是最简单的将单词*向量化*的方法，即将它们转换为数字向量——*袋词模型*。该模型简单地计算每封电子邮件中包含的单词标记的频率，从而将其表示为这种频率计数的向量。我们在清单
    2.8 中提供了组装电子邮件的词袋模型的函数。请注意，通过这样做，我们只保留出现超过一次的标记，如变量`used_tokens`所捕获的那样。这使我们能够将向量维度保持得比其他情况下低得多。还请注意，可以使用流行的库
    scikit-learn 中的各种内置矢量化器来实现这一点（我们的 Jupyter 笔记本展示了如何做到这一点）。但是，我们专注于清单 2.8 中所展示的方法，因为我们发现这比使用实现相同功能的黑匣子函数更具说明性。我们还注意到，scikit-learn
    的向量化方法包括计算任意*n*个单词序列或*n-gram*的出现次数，以及*tf-idf*方法—如果有生疏的话，这些是您应该复习的重要基本概念。在这里展示的问题中，当使用这些向量化方法时，我们并未注意到与使用词袋模型方法相比的改进。
- en: Listing 2.8 Assembling a bag-of-words representation
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.8 组装词袋表示
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ If token has been seen before, appends it to the output list used_tokens
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果标记之前已经见过，将其附加到用过的标记输出列表
- en: ❷ Creates a Pandas DataFrame counting frequencies of vocabulary words—corresponding
    to columns, in each email—corresponding to rows
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建 Pandas DataFrame 计数词汇单词的频率——对应于每封电子邮件的列——对应于行
- en: 'Having defined the `assemble_bag` function, let’s use it to actually carry
    out the vectorization and visualize it as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了`assemble_bag`函数之后，让我们使用它来实际执行向量化并将其可视化如下：
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'A slice of the output DataFrame looks as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 DataFrame 的一个片段如下所示：
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The column labels indicate words in the vocabulary of the bag-of-words model,
    and the numerical entries in each row correspond to the frequency counts of each
    such word for each of the 2,000 emails in our dataset. Notice that it is an extremely
    sparse DataFrame—it consists mostly of values of `0`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列标签指示词袋模型的词汇中的单词，每行中的数字条目对应于我们数据集中 2000 封电子邮件中每个此类单词的频率计数。请注意，这是一个极为稀疏的 DataFrame——它主要由值`0`组成。
- en: Having fully vectorized the dataset, we must remember that it is not shuffled
    with respect to classes; that is, it contains `Nsamp` = 1000 spam emails followed
    by an equal number of nonspam emails. Depending on how this dataset is split—in
    our case, by picking the first 70% for training and the remainder for testing—this
    could lead to a training set composed of spam only, which would obviously lead
    to failure. To create a randomized ordering of class samples in the dataset, we
    will need to shuffle the data in unison with the header/list of labels. The function
    for achieving this is shown in the next listing. Again, the same thing can be
    achieved using built-in scikit-learn functions, but we find the method shown next
    to be more illustrative.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集完全向量化后，我们必须记住它与类别无关的洗牌；也就是说，它包含`Nsamp` = 1000封垃圾邮件，然后是相同数量的非垃圾邮件。根据如何拆分此数据集——在我们的情况下，通过选择前70％用于训练，剩余部分用于测试——这可能导致训练集仅由垃圾邮件组成，这显然会导致失败。为了在数据集中创建类样本的随机排序，我们需要与标头/标签列表一起洗牌数据。下一个清单中显示了实现此目的的函数。同样，可以使用内置的
    scikit-learn 函数实现相同的效果，但我们发现下一个清单中显示的方法更具说明性。
- en: Listing 2.9 Shuffling data in unison with a header/list of labels
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2.9 与标头/标签列表一起洗牌数据
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As the very last step of preparing the email dataset for training by our baseline
    classifiers, we split it into independent training and testing, or validation,
    sets. This will allow us to evaluate the performance of the classifier on a set
    of data that was not used for training—an important thing to ensure in machine
    learning practice. We elect to use 70% of the data for training and 30% for testing/validation
    afterward. The following code calls the unison shuffling function and then performs
    the train/test split. The resulting NumPy array variables `train_x` and `train_y`
    will be fed directly to the classifiers in the following sections of this chapter:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为准备电子邮件数据集以供基线分类器训练的最后一步，我们将其分割为独立的训练和测试，或验证集。这将允许我们评估分类器在未用于训练的一组数据上的性能——这是机器学习实践中必须确保的重要事情。我们选择使用
    70％ 的数据进行训练，然后进行 30％ 的测试/验证。下面的代码调用了同步洗牌函数，然后执行了训练/测试分割。生成的 NumPy 数组变量`train_x`和`train_y`将直接传递给本章后续部分中的分类器：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Uses 70% of data for training
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 70％ 的数据进行训练
- en: ❷ Uses remaining 30% for testing
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用剩余 30% 进行测试
- en: Hopefully, this exercise of building and preprocessing an NLP dataset for machine
    learning tasks, now complete, has equipped you with useful skills that will carry
    over to your own projects. We will now proceed to address the preprocessing of
    the second illustrative example we will use in this and the next chapter, the
    classification of Internet Movie Database (IMDB) movie reviews. That exercise
    will be decidedly shorter, given that the IMDB dataset is in a more prepared state
    than the email dataset we assembled. However, it is an opportunity to highlight
    a different type of preprocessing required, given that the data is available in
    separate folders, organized by class.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这个为机器学习任务构建和预处理 NLP 数据集的练习现在已经完成，使您具备了可应用于自己项目的有用技能。现在我们将继续处理第二个说明性示例的预处理，该示例将在本章和下一章中使用，即互联网电影数据库（IMDB）电影评论的分类。鉴于
    IMDB 数据集比我们组装的电子邮件数据集更为准备充分，因此该练习将更为简短。然而，鉴于数据按类别分开放置在不同文件夹中，这是一个突出显示不同类型预处理要求的机会。
- en: 2.2 Preprocessing movie sentiment classification example data
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 预处理电影情感分类示例数据
- en: In this section, we preprocess and explore the second example dataset that will
    be analyzed in this chapter. This second example is concerned with classifying
    movie reviews from IMDB into positive or negative sentiments expressed. This is
    a prototypical sentiment analysis example that has been used widely in the literature
    to study many algorithms. We present the code snippets necessary to preprocess
    the data, and you are encouraged to run the code as you read for best educational
    value.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对将在本章中分析的第二个示例数据集进行预处理和探索。这第二个示例涉及将IMDB中的电影评论分类为正面或负面情绪表达。这是一个典型的情感分析示例，在文献中被广泛使用来研究许多算法。我们提供了预处理数据所需的代码片段，并鼓励您在阅读时运行代码以获得最佳的教育价值。
- en: We will use a popular labeled dataset of 25,000 reviews for this,[⁵](#pgfId-1082561)
    which was assembled by scraping the popular movie review website IMDB and mapping
    the number of stars corresponding to each review to either 0 or 1, depending on
    whether it was less than or greater than 5 out of 10 stars, respectively.[⁶](#pgfId-1082565)
    This dataset has been used widely in prior NLP literature, and this familiarity
    is part of the reason we chose it as an illustrative example for baselining.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此，我们将使用一个包含25,000条评论的流行标记数据集，[⁵](#pgfId-1082561)该数据集是通过从流行的电影评论网站IMDB抓取数据并将每条评论对应的星级数量映射到0或1（如果它小于或大于10颗星），而组装而成。[⁶](#pgfId-1082565)这个数据集在先前的NLP文献中被广泛使用，我们选择它作为基线的说明性示例的原因之一就是因为人们对它熟悉。
- en: The sequence of steps used to preprocess each IMDB movie review before analysis
    is very similar to the one presented in figure 2.2 for the email spam classification
    example. The first major difference is that no email headers are attached to these
    reviews, so the header extraction step is not applicable. Additionally, because
    some stop words, including “no” and “not,” may change the sentiment of the message,
    the stop-word removal step may need to be carried out with extra care, first making
    sure to drop such stop words from the target list. We did experiment with dropping
    such words from the list and saw little to no effect on the result. This is likely
    because other non-stop words in the reviews are very predictive features, rendering
    this step irrelevant. Thus, although we do show you how to do this in our Jupyter
    notebook, we do not discuss it any further here.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析每个IMDB电影评论之前使用的预处理步骤序列与图2.2中呈现的用于电子邮件垃圾邮件分类示例非常相似。第一个主要的区别是这些评论没有附加电子邮件标题，因此无需进行标题提取步骤。此外，由于包括“no”和“not”等一些停用词可能会改变消息的情感，因此从目标列表中删除停用词的步骤可能需要特别小心。我们确实尝试了从列表中删除这些单词，并且发现对结果几乎没有影响。这可能是因为评论中的其他非停用词非常具有预测特征，使得这一步骤变得无关紧要。因此，尽管我们在Jupyter笔记本中向您展示了如何做到这一点，但我们在这里不再讨论这个问题。
- en: 'Let’s dive right into preparing the IMDB dataset for our purposes, similarly
    to what was done for the email dataset that we assembled in the previous section.
    The IMDB dataset can be downloaded and extracted via the following shell commands
    in our Jupyter notebook:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接着手准备IMDB数据集，就像我们在上一节中组装电子邮件数据集那样。您可以通过以下shell命令在我们的Jupyter笔记本中下载并提取IMDB数据集：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that the exclamation sign, `!`, at the beginning of the command tells the
    interpreter that these are shell, not Python, commands. Also note that this is
    a Linux command. If you’re running this code locally on Windows, you may need
    to download and extract the file manually from the provided link. This yields
    two subfolders—aclImdb/pos/ and aclImdb/neg/—which we load, after tokenizing,
    removing stop words and punctuations, and shuffling, into a NumPy array using
    the function and its calling script in the following listing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，命令开头的感叹号标志`!`告诉解释器这些是shell命令，而不是Python命令。还要注意，这是一个Linux命令。如果您在Windows上本地运行此代码，则可能需要手动从提供的链接下载和解压文件。这将生成两个子文件夹
    - aclImdb/pos/ 和 aclImdb/neg/ - 我们使用以下列表中的函数和其调用脚本对其进行标记化，删除停用词和标点，并进行随机处理，并将其加载到NumPy数组中。
- en: Listing 2.10 Loading IMDB data into a NumPy array
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.10 将IMDB数据加载到NumPy数组中
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Goes through every file in current folder
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历当前文件夹中的每个文件
- en: ❷ Applies tokenization and stop-word analysis routines
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 应用分词和停用词分析程序
- en: ❸ Tracks corresponding sentiment labels
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 跟踪相应的情感标签
- en: ❹ Converts to a NumPy array
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 转换为NumPy数组
- en: ❺ Calls the function above on the data
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对数据调用上面的函数
- en: 'Note that on Windows, you may have to specify the parameter `encoding=utf-8`
    to the `open` function call in listing 2.10\. Check dimensions of loaded data
    to make sure things worked as expected, as shown here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 Windows 上，您可能需要在清单 2.10 的 `open` 函数调用中指定参数 `encoding=utf-8`。检查加载数据的维度，以确保一切按预期运行，如下所示：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This yields the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Take `Nsamp*2` random entries of the loaded data for training, as shown next:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们取加载数据的`Nsamp*2`个随机条目用于训练，如下所示：
- en: '[PRE31]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Before proceeding, we need to check the balance of the resulting data with
    regard to class. In general, we don’t want one of the labels to represent most
    of the dataset, unless that is the distribution expected in practice. Check the
    label distribution using the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要检查所得数据在类方面的平衡情况。通常情况下，我们不希望其中一个标签代表大多数数据集，除非这是实践中预期的分布。使用以下代码检查标签分布：
- en: '[PRE32]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This yields the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Having satisfied ourselves that the data is roughly balanced between the two
    classes, with each class representing roughly half of the dataset, assemble and
    visualize the bag-of-words representation with the next lines of code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保数据在两个类之间大致平衡，并且每个类大致代表数据集的一半后，使用下面的代码组装和可视化词袋表示：
- en: '[PRE34]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'A slice through the resulting DataFrame produced by this snippet looks like
    this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这段代码片段生成的结果 DataFrame 的一个切片如下所示：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that after this, you still need to split this data structure into training
    and validation sets, similar to what we did for the spam-detection example. We
    do not repeat that here in the interest of brevity, but this code is included
    in the companion Kaggle notebook.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此之后，您仍然需要将这个数据结构分割成训练集和验证集，类似于我们为垃圾邮件检测示例所做的操作。出于简洁起见，我们不在这里重复，但这段代码包含在配套的
    Kaggle 笔记本中。
- en: With this numerical representation ready, we now proceed to building out our
    baseline classifiers in the subsequent sections for the two presented example
    datasets. We start with generalized linear models in the next section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个数值表示准备好后，我们现在继续在后续部分为这两个示例数据集构建基线分类器。我们从下一节开始使用广义线性模型。
- en: 2.3 Generalized linear models
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 广义线性模型
- en: Traditionally, the development of models in any area of applied mathematics
    has started with linear models. These models are mappings that preserve addition
    and multiplication in the input and output spaces. In other words, the net response
    from a pair of inputs will be the sum of the responses to each individual input.
    This property enables a significant reduction in associated statistical and mathematical
    theory.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，在任何应用数学领域模型的发展都是从线性模型开始的。这些模型是保留输入和输出空间中的加法和乘法的映射。换句话说，对一对输入的净响应将是对每个单独输入的响应的总和。这个属性使得相关的统计和数学理论显著减少。
- en: Here, we use a relaxed definition of linearity from statistics, that of *generalized
    linear models*. Let Y be a vector of output variables or responses, X be a vector
    of independent variables and β be a vector of unknown parameters to be estimated
    by training our classifier. A generalized linear model is defined by the equation
    in figure 2.3.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了来自统计学的线性的宽松定义，即*广义线性模型*。设 Y 是输出变量或响应的向量，X 是独立变量的向量，β 是要由我们的分类器进行训练的未知参数的向量。广义线性模型由图
    2.3 中的方程定义。
- en: '![02_03](../Images/02_03.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![02_03](../Images/02_03.png)'
- en: Figure 2.3 Generalized linear model equation
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 广义线性模型方程
- en: Here, *E*[] stands for the *expected value* of the enclosed quantity, the right-hand
    side is linear in X, and g is a function that links this linear quantity to the
    expected value of Y.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*E*[] 代表所包含数量的*期望值*，右侧在 X 中是线性的，并且 g 是将这个线性数量链接到 Y 的期望值的函数。
- en: In this section, we will apply a pair of the most widely used generalized linear
    machine learning algorithms to the pair of example problems that were introduced
    in the previous section—logistic regression and support vector machines (SVMs)
    with linear kernel. Other popular generalized linear machine learning models that
    will not be applied include the simple perceptron neural architecture with a linear
    activation function, latent Dirichlet allocation (LDA), and naive Bayes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用一对最广泛使用的广义线性机器学习算法到前一节介绍的一对示例问题上——逻辑回归和带有线性核的支持向量机（SVM）。其他流行的广义线性机器学习模型不包括简单的带有线性激活函数的感知器神经架构、潜在狄利克雷分配（LDA）和朴素贝叶斯。
- en: 2.3.1 Logistic regression
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 逻辑回归
- en: Logistic regression models the relationship between a categorical output variable
    and a set of input variables by estimating probabilities with the *logistic function*.
    Assuming the existence of a single input variable x and a single output binary
    variable y with associated probability P(y=1)=*p*, the logistic equation can be
    expressed as the equation in figure 2.4.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归通过使用*逻辑函数*估计概率，对分类输出变量和一组输入变量之间的关系进行建模。假设存在单个输入变量x和单个输出二进制变量y，其相关概率为P(y=1)=*p*，则逻辑方程可以表达为图2.4中的方程。
- en: '![02_04](../Images/02_04.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![02_04](../Images/02_04.png)'
- en: Figure 2.4 The logistic regression equation
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 逻辑回归方程
- en: This can be reorganized to yield the prototypical logistic curve equation shown
    in figure 2.5.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重新组织，以得到图2.5中显示的典型逻辑曲线方程。
- en: '![02_05](../Images/02_05.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![02_05](../Images/02_05.png)'
- en: Figure 2.5 Reorganized prototypical logistic regression equation
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 重组后的典型逻辑回归方程
- en: This equation is plotted in figure 2.6\. Historically, this curve emerged from
    the study of bacterial population growth, with initial slow growth, explosion
    in growth toward the middle, and diminishing growth toward the end, as resources
    to sustain the population run out.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程在图2.6中绘制。从历史上看，这条曲线起源于对细菌种群增长的研究，初始生长缓慢，中间爆炸性增长，随着资源耗尽，生长逐渐减弱。
- en: '![02_06](../Images/02_06.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![02_06](../Images/02_06.png)'
- en: Figure 2.6 Prototypical logistic curve plot
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 典型的逻辑曲线绘图
- en: Now let’s go ahead and build our classifier using the popular library scikit-learn
    using the function shown in the next listing.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续使用流行的库scikit-learn构建我们的分类器，使用下一节中显示的函数。
- en: Listing 2.11 Building a logistic regression classifier
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.11 构建逻辑回归分类器
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Instantiates the model
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化模型
- en: ❷ Fits the model to prepared, labeled data
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将模型拟合到准备好的、标记的数据上
- en: 'To fit this model to our data for either the email or IMDB classification example,
    we need to execute only the following line of code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这个模型拟合到我们的数据中，无论是电子邮件还是IMDB分类示例，我们只需要执行以下一行代码：
- en: '[PRE37]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This should only take a few seconds on any modern PC. To evaluate performance,
    we must test on the “hold out” test/validation sets that were put together for
    each example. This can be performed using the following code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该在任何现代PC上只需几秒钟。要评估性能，我们必须在为每个示例准备的“保留”测试/验证集上进行测试。这可以使用以下代码执行：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For the email classification example, this yields:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电子邮件分类示例，这将产生：
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For the IMDB semantic analysis example, this yields:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IMDB语义分析示例，这将产生：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This appears to suggest that the spam classification problem we set up is easier
    than the IMDB movie review problem. We will address potential ways of improving
    the performance on the IMDB classifier by the conclusion of the next chapter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎表明我们设置的垃圾邮件分类问题比IMDB电影评论问题更容易解决。在下一章的结尾，我们将讨论改进IMDB分类器性能的潜在方法。
- en: Before proceeding, it is important to address the use of accuracy as the metric
    for evaluating performance. Accuracy is defined as the ratio of correctly identified
    samples—the ratio of the number of true positives and negatives to the total number
    of samples. Other potential metrics that could be used here include precision—the
    ratio of the number of true positives to all predicted positives—and recall—the
    ratio of the number of true positives to all actual positives. These two measures
    could be useful if the costs of false positives and false negatives, respectively,
    are particularly important. Crucially, the F1-score—the harmonic mean of precision
    and recall—strikes a balance between the two and is particularly useful for imbalanced
    datasets. This is the most common situation in practice, making this metric very
    important. However, remember that the datasets we have constructed so far are
    roughly balanced. Thus, accuracy is a reasonable enough metric in our case.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，解决使用准确度作为评估性能的指标是很重要的。准确度被定义为正确识别的样本的比率——真正例和真负例的比率与总样本数的比率。这里可以使用的其他潜在指标包括精确度——真正例与所有预测正例的比率——以及召回率——真正例与所有实际正例的比率。如果假阳性和假阴性的成本特别重要，这两个度量可能很有用。至关重要的是，F1分数——精确度和召回率的调和平均值——在两者之间取得平衡，对于不平衡的数据集特别有用。这是实际中最常见的情况，因此这个指标非常重要。然而，记住我们迄今为止构建的数据集大致是平衡的。因此，在我们的情况下，准确度是一个合理的度量。
- en: 2.3.2 Support vector machines (SVMs)
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 支持向量机（SVM）
- en: SVMs, as was alluded to in chapter 1, have traditionally been the most popular
    kind of kernel method. These methods attempt to find good decision boundaries
    by mapping data to a high-dimensional space, using hyperplanes as decision boundaries
    and the kernel trick to reduce computing cost. When the kernel function is a linear
    function, SVMs are not only generalized linear models but are indeed linear models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: SVM，在第1章中已经提到，一直是最受欢迎的核方法。这些方法尝试通过将数据映射到高维空间来找到好的决策边界，使用超平面作为决策边界，并使用核技巧来降低计算成本。当核函数是线性函数时，SVM不仅是广义线性模型，而且确实是线性模型。
- en: Let’s proceed with building and evaluating an SVM classifier on our two running
    illustrative example problems using the code shown in the next listing. Note that
    because this classifier takes a bit longer to train than the logistic regression
    one, we employ the built-in Python library time to determine the training time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用下一版示例中的代码构建和评估SVM分类器在我们的两个运行示例问题上。请注意，由于该分类器的训练时间比逻辑回归分类器稍长，我们使用内置的Python库time来确定训练时间。
- en: Listing 2.12 Training and testing an SVM classifier
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.12 训练和测试SVM分类器
- en: '[PRE41]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Creates a support vector classifier with linear kernel
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建具有线性内核的支持向量分类器
- en: ❷ Fits the classifier using the training data
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用训练数据拟合分类器
- en: ❸ Tests and evaluates
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 测试和评估
- en: Training the SVM classifier on the email data took 64 seconds and yielded an
    accuracy score of 0.670\. Training the classifier on the IMDB data took 36 seconds
    and yielded an accuracy score of 0.697\. We see that SVM significantly underperforms
    logistic regression for the email spam classification problem, while achieving
    lower but nearly comparable performance for the IMDB problem.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子邮件数据上训练SVM分类器共花费了64秒，并获得0.670的准确率得分。在IMDB数据上训练分类器花费36秒，并获得0.697的准确率得分。我们看到，对于电子垃圾邮件分类问题，SVM的表现明显不如逻辑回归，而对于IMDB问题，它的表现虽然较低，但几乎可以相提并论。
- en: In the next chapter, we will apply some more increasingly sophisticated methods
    to both classification problems to further baseline them and compare the performance
    of the various methods. In particular, we will explore decision-tree-based methods,
    as well as the popular neural-network-based methods ELMo and BERT.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将应用更加复杂的方法来对这两个分类问题进行基线处理，并比较各种方法的性能。特别是，我们将探索基于决策树的方法，以及流行的神经网络方法ELMo和BERT。
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: It is typical to try a variety of algorithms on any given problem of interest
    to find the best combination of model complexity and performance for your particular
    circumstances.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何感兴趣的问题上尝试各种算法以找到模型复杂度和性能的最佳组合以符合您特定的情况是很常见的。
- en: Baselines usually start with the simplest algorithms, such as logistic regression,
    and become increasingly complex until the right performance/complexity trade-off
    is attained.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线通常从最简单的算法开始，例如逻辑回归，并逐渐变得越来越复杂，直到得到正确的性能/复杂性权衡。
- en: A big part of machine learning practice is concerned with assembling and preprocessing
    data for your problem, and today this is arguably the most important part of the
    process.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习实践的一大部分涉及为您的问题组装和预处理数据，目前这可能是该过程中最重要的部分。
- en: Important model design choices include metrics for evaluating performance, loss
    functions to guide the training algorithm, and best validation practices, among
    many others, and these can vary by model and problem type.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的模型设计选择包括评估性能的指标、指导训练算法的损失函数以及最佳验证实践等，这些因模型和问题类型而异。
- en: 1. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 1. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
- en: 2. [https://www.kaggle.com/azunre/tlfornlp-chapters2-3-spam-bert-tf2](https://www.kaggle.com/azunre/tlfornlp-chapters2-3-spam-bert-tf2)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 2. [https://www.kaggle.com/azunre/tlfornlp-chapters2-3-spam-bert-tf2](https://www.kaggle.com/azunre/tlfornlp-chapters2-3-spam-bert-tf2)
- en: 3. [https://www.kaggle.com/wcukierski/enron-email-dataset](https://www.kaggle.com/wcukierski/enron-email-dataset)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 3. [https://www.kaggle.com/wcukierski/enron-email-dataset](https://www.kaggle.com/wcukierski/enron-email-dataset)
- en: 4. [https://www.kaggle.com/rtatman/fraudulent-email-corpus](https://www.kaggle.com/rtatman/fraudulent-email-corpus)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 4. [https://www.kaggle.com/rtatman/fraudulent-email-corpus](https://www.kaggle.com/rtatman/fraudulent-email-corpus)
- en: 5. [ai.stanford.edu/~amaas/data/sentiment](http://ai.stanford.edu/~amaas/data/sentiment)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 5. [ai.stanford.edu/~amaas/data/sentiment](http://ai.stanford.edu/~amaas/data/sentiment)
- en: 6. A.L. Maas et al., “Learning Word Vectors for Sentiment Analysis,” Proc. of
    NAACL-HLT (2018).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 6. A.L. Maas 等人，“学习词向量进行情感分析”，NAACL-HLT 会议论文集 (2018)。
