- en: 9 Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**9 自编码器**'
- en: “Out of intense complexities, intense simplicities emerge – Winston Churchill”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “在强烈的复杂性中，简单的事物显现出来 - 温斯顿·丘吉尔”
- en: 'This chapter covers:'
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括：
- en: Autoencoders
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器
- en: Training of autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的训练
- en: Types of autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的类型
- en: Python code using tensorflow and keras
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用tensorflow和keras的Python代码
- en: In the last chapter of the book, we explored the concepts of deep learning.
    Those are the foundational concepts enabling you to grasp unsupervised deep learning.
    So, let’s start the first topic on unsupervised deep learning. We are starting
    with Autoencoders as the very first topic. We will be first covering the basics
    of autoencoders, what are they and how do we train the autoencoders. We will then
    get into the different types of autoencoders followed by a Python code on the
    implementation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在书的最后一章，我们探讨了深度学习的概念。这些是使您能够掌握无监督深度学习的基础概念。因此，让我们开始无监督深度学习的第一个主题。我们从自编码器开始作为第一个主题。我们将首先介绍自编码器的基础知识，它们是什么以及我们如何训练自编码器。然后我们将进入不同类型的自编码器，然后是有关实现的Python代码。
- en: Welcome to the ninth chapter and all the very best!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第九章，祝你好运！
- en: 9.1 Technical toolkit
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9.1 技术工具包**'
- en: We will continue to use the same version of Python and Jupyter notebook as we
    have used so far. The codes and datasets used in this chapter have been checked-in
    at this location.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用迄今为止使用的相同版本的Python和Jupyter笔记本。本章中使用的代码和数据集已经在此位置签入。
- en: You would need to install a few Python libraries in this chapter which are –
    tensorflow and keras.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要安装几个Python库 - tensorflow和keras。
- en: Let’s get started with Chapter 9 of the book!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始阅读书籍的第9章吧！
- en: 9.2 Feature Learning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9.2 特征学习**'
- en: Predictive modelling is quite an interesting topic. Across various domains and
    business functions, predictive modelling is used for various purposes like predicting
    the sales for a business next year, predicting the amount of rainfall expected,
    predicting whether the incoming credit card transaction is fraud or not, predicting
    whether the customer will make a purchase or not and so on. The use cases are
    many and all of the above-mentioned use cases fall under supervised learning algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预测建模是一个非常有趣的话题。在各种领域和业务功能中，预测建模被用于各种目的，如预测明年业务销售额、预测预期降雨量、预测即将到来的信用卡交易是否为欺诈、预测客户是否会购买等。用例很多，上述所有用例都属于监督学习算法。
- en: The data sets that we use have variable or attributes. They are also called
    characteristics or *features*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的数据集具有变量或属性。它们也被称为特征。
- en: Whilst we wish to create these predictive models, we are also interested to
    understand the variables which are useful for making the prediction. Let’s consider
    a case where a bank wants to predict if an incoming transaction is fraudulent
    or not. In such a scenario, the bank will wish know which factors are significant
    in order to identify an incoming transaction as fraud. Factors that might be considered
    include the amount of the transaction, the time of the transaction, origin/source
    of the transaction etc. The variables which are important for making a prediction
    are called as *significant variables*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们希望创建这些预测模型，但我们也有兴趣了解对于进行预测有用的变量。让我们考虑一个银行想要预测即将到来的交易是否是欺诈的情况。在这种情况下，银行希望知道哪些因素是重要的，以便将即将到来的交易标识为欺诈。可能考虑的因素包括交易金额、交易时间、交易的来源/目的地等。用于进行预测的重要变量称为*显著变量*。
- en: For us to create a machine learning based predictive model, *feature engineering*
    is used. Feature engineering, otherwise known as feature extraction, is the process
    of extracting features from the raw data to improve the overall quality of the
    model and enhance the accuracy as compared to a model where only raw data was
    fed to the machine learning model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建基于机器学习的预测模型，我们使用*特征工程*。特征工程，又称特征提取，是从原始数据中提取特征以提高模型整体质量并增强准确性的过程，与仅向机器学习模型提供原始数据的模型相比。
- en: Feature engineering can be done using the domain understanding, using various
    manual methods and a few automated methods too. One of such methods is known as
    Feature Learning. Feature learning is the set of techniques which help a solution
    to automatically discover the representations required for feature detection.
    With the help of feature learning, manual feature engineering is not required.
    The impact of feature learning is much more relevant for datasets where images,
    text, audio and videos are being used.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程可以使用领域理解、各种手动方法以及一些自动化方法来完成。其中一种方法称为特征学习。特征学习是一组技术，它们帮助解决方案自动发现所需的特征检测表示。利用特征学习，不需要手动特征工程。特征学习对使用图像、文本、音频和视频的数据集的影响更加重要。
- en: Feature learning can be both supervised and unsupervised. For supervised feature
    learning, we have neural networks as the best example. For unsupervised feature
    learning we have examples like matrix factorization, clustering algorithms and
    autoencoders. We have already covered clustering and matrix factorization in the
    last chapters of the book. We are discussing autoencoders in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 特征学习可以是有监督和无监督的。对于有监督的特征学习，神经网络是最好的例子。对于无监督的特征学习，我们有矩阵分解、聚类算法和自编码器等例子。在本书的上一章中已经详细介绍了聚类和矩阵分解。在本章中，我们将讨论自编码器。
- en: We will start with introductions to autoencoders in the next section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在下一节中介绍自编码器。
- en: 9.3 Introducing Autoencoders
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 自编码器介绍
- en: When we start with any data science problem, data plays the most significant
    role. A data set which has a lot of noise is one of the biggest challenges in
    data science and machine learning. There are quite a few solutions available now
    and autoencoders are one of them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始处理任何数据科学问题时，数据起到最重要的作用。具有大量噪声的数据集是数据科学和机器学习中最大的挑战之一。现在有很多解决方案，自编码器就是其中之一。
- en: Simply put, an autoencoder is a type of artificial neural network and they are
    used to learn the data encodings. They are typically used for dimensionality reduction
    methods. They can also be used as generative models which can create synthetic
    data for us which is like the old data. For example, if we do not have good amount
    of data to train a machine learning, we can use generated synthetic data to train
    the models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，自编码器是一种人工神经网络，用于学习数据编码。它们通常用于降维方法。它们还可以用作生成模型，可以为我们创建类似于旧数据的合成数据。例如，如果我们没有足够的数据来训练机器学习模型，我们可以使用生成的合成数据来训练模型。
- en: Autoencoders are feed forward neural networks and they compress the input into
    a lower-dimensional code and then try to reconstruct the output from this representation.
    The objective for an autoencoder is to learn the lower-dimensional representation
    (also sometimes known as encoding) for a high-dimensional dataset. Recall from
    the previous chapters about Principal Component Analysis (PCA). Autoencoders can
    be thought as a generalization for PCA. PCA is a linear method whereas autoencoders
    can learn non-linear relationships as well. Hence, autoencoders are required for
    dimensionality reduction solutions wherein they capture the most significant attributes
    from the input data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是前馈神经网络，它们将输入压缩成低维代码，然后尝试从这个表示中重建输出。自编码器的目标是学习高维数据集的低维表示（有时也称编码）。回顾一下前面章节中的主成分分析（PCA）。自编码器可以看作是PCA的一般化。PCA是一种线性方法，而自编码器也可以学习非线性关系。因此，自编码器非常适合用于捕捉输入数据的最重要属性的降维解决方案。
- en: We will now study the various components of autoencoders in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在下一节中研究自编码器的各个组成部分。
- en: 9.4 Components of autoencoders
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 自编码器的组成部分
- en: 'The architecture for an autoencoder is quite simple to understand. An autoencoder
    consists of three parts: an encoder, a bottleneck or a code, and a decoder as
    shown in (Figure 9.1). In simple terms, an encoder compresses the input data,
    a bottleneck or code contains this compressed information and the decoder decompresses
    the knowledge and hence reconstructs this data back to its original form. Once
    the decompression has been done and the data has been reconstructed to its encoded
    form, the input and output can be compared.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的结构相当简单。自动编码器由三部分组成：编码器、瓶颈或代码以及解码器，如（图9.1）所示。简单来说，编码器压缩输入数据，瓶颈或代码包含这些压缩信息，解码器解压知识，从而将数据重构回其原始形式。一旦解压完成并且数据被重构成其已编码形式，输入和输出可以进行比较。
- en: Let’s study these components in more detail.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地学习这些组件。
- en: '**Encoder**: the input data passes through the encoder. Encoder is nothing
    but a fully connected artificial neural network. It compresses the input data
    into an encoded representation and hence, in the process the output generated
    is much reduced in size. Encoder compresses the input data into a compressed module
    known as the bottleneck.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器**：输入数据通过编码器。编码器实际上是一个全连接的人工神经网络。它将输入数据压缩成编码表示，并且，在过程中生成的输出大大缩小。编码器将输入数据压缩成一个称为瓶颈的压缩模块。'
- en: '**Bottleneck**: The bottleneck can be called as the brain of the encoder. It
    contains the compressed information representations and it is the job of the bottleneck
    to allow only the most important information to pass through it.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**瓶颈**：瓶颈可以称为编码器的大脑。它包含压缩的信息表示，瓶颈的工作是只允许最重要的信息通过。'
- en: '**Decoder**: The information received from the bottleneck is decompressed by
    decoder. It recreates the data back to its original or encoded form. Once the
    job of decoder is done, the actual values are compared with the decompressed values
    created by the decoder.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解码器**：从瓶颈接收到的信息由解码器解压。它将数据重新创建为其原始或编码形式。一旦解码器完成其工作，实际值将与解码器创建的解压值进行比较。'
- en: Figure 9.1 Structure of an autoencoder with input layer, hidden layer and output
    layer.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1 自动编码器的结构，包括输入层、隐藏层和输出层。
- en: '![09_01](images/09_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![09_01](images/09_01.png)'
- en: 'For autoencoders, a few important points should be noticed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动编码器，应该注意几个重要的点：
- en: There is a loss of information in autoencoders when the decompression is done
    as compared to the original inputs. So, when the compressed data is decompressed,
    then there is a loss as compared to the original data.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在自动编码器中，当解压缩与原始输入相比时会出现信息丢失。因此，当压缩数据解压时，与原始数据相比会存在损失。
- en: Autoencoders are specific to datasets. This means that an algorithm which is
    trained on images of flowers will not work on the images of traffic signals and
    vice versa. This is due to the fact that the features the autoencoder would have
    learned will be specific to flowers only. So, we can say that auto-encoders are
    only able to compress the data similar to the one used for training.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动编码器是特定于数据集的。这意味着在花朵图片上训练的算法不会在交通信号灯图片上起作用，反之亦然。这是因为自动编码器学到的特征只适用于花朵。因此，我们可以说自动编码器只能压缩与训练数据相似的数据。
- en: It is relatively easier to train specialized instances of algorithms to perform
    good on specific type of inputs. We just need representative training datasets
    to tarin the autoencoder.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练专门的算法实例以在特定类型的输入上良好执行相对较容易。我们只需要代表性的训练数据集来训练自动编码器。
- en: We’ve now covered the major components of autoencoders. Next let’s go into the
    process of training an autoencoder.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了自动编码器的主要组件。接下来让我们进入训练自动编码器的过程。
- en: 9.5 Training of autoencoders
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 自动编码器的训练
- en: It is important to note that if there is no correlation between the variables
    in the data, then it is really difficult to compress and subsequently decompress
    the input data. For us to create a meaningful solution, there ought to be some
    level of relationships or correlations between the variables in the input data.
    To create an autoencoder we require an encoding method, a decoding method and
    a loss function to compare the actual vs decompressed values.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果在数据中变量之间没有相关性，那么很难对输入数据进行压缩和随后进行解压。为了创建一个有意义的解决方案，输入数据中的变量之间应该存在某种程度的关系或相关性。为了创建一个自动编码器，我们需要一个编码方法，一个解码方法和一个损失函数来比较实际值与解压值。
- en: 'The process is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 过程如下：
- en: First the input data passes through the encoder module.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，输入数据通过编码器模块。
- en: The encoder compresses the input to a model into a compact bottleneck.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器将模型的输入压缩成紧凑的瓶颈。
- en: It is the job of the bottleneck to restrict the flow of information and allows
    only important information to pass through and hence bottleneck is sometimes referred
    to as *knowledge-representation*.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓶颈的任务是限制信息的流动，并只允许重要信息通过，因此有时将瓶颈称为*知识表示*。
- en: Following the bottleneck is the decoder which decompresses the information and
    recreates the data back to its original or encoded form.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧随瓶颈之后的是解码器，它对信息进行解压缩，并将数据重新创建为其原始或编码形式。
- en: This encoder-decoder architecture is quite efficient in getting the most significant
    attributes from the input data.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个编码器-解码器架构非常有效地从输入数据中获取最重要的属性。
- en: The objective of the solution is generating an output as identical to the input.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决方案的目标是生成与输入相同的输出。
- en: Generally, it is observed that decoder architecture is mirror image of the coder
    architecture. It is not mandatory but is generally followed. We ensure that the
    dimensionality of the input and outputs are same.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常观察到解码器架构是编码器架构的镜像。这不是强制性的，但通常会遵循。我们确保输入和输出的维度相同。
- en: 'We need to define four hyperparameters for training an autoencoder:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为训练自动编码器定义四个超参数：
- en: 'Code size: It is perhaps the most significant hyperparameter. It represents
    the number of nodes in the middle layer. This decides the compression of the data
    and can also act as a regularization term. The lesser the value of code size,
    the compression of the data is more.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Code size: 这可能是最重要的超参数。它表示中间层中节点的数量。这决定了数据的压缩程度，也可以作为正则化项。代码大小的值越小，数据的压缩程度就越大。'
- en: Number of layers is a parameter which denotes the depth of the autoencoder.
    A model which has more depth is obviously more complex and will take higher processing
    time.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层数是指自动编码器的深度的参数。具有更深度的模型显然更复杂，处理时间也更长。
- en: Number of nodes per layer is weight used per layer. It generally decreases with
    every subsequent layer as the input becomes smaller across the layers. And it
    increases back in the decoder.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每层的节点数是每层使用的权重。随着每个后续层的输入在层之间变小，每层的节点数通常会减少。然后在解码器中增加。
- en: The final hyperparameter is the loss function used. If the input values are
    in [0,1] range binary cross-entropy is preferred, else mean squared error is used.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一个超参数是所使用的损失函数。如果输入值在[0,1]范围内，首选二进制交叉熵，否则使用均方误差。
- en: We have covered the hyperparameters used in training autoencoders. The training
    process is similar to the backpropagation which we have already covered in the
    last chapter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了用于训练自动编码器的超参数。训练过程类似于我们在上一章中已经介绍过的反向传播。
- en: We will now move to a few important applications of autoencoders in the next
    section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在下一节中介绍自动编码器的一些重要应用。
- en: 9.6 Application of autoencoders
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 自动编码器的应用
- en: 'Autoencoders are capable of solving a number of problems inherent to unsupervised
    learning. Major applications for autoencoders include:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器能够解决许多固有于无监督学习的问题。自动编码器的主要应用包括：
- en: Dimensionality reduction is one of the major applications of autoencoders. It
    has been observed that sometimes autoencoders can learn more complex data projections
    than Principal Component analysis and other techniques.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降维是自动编码器的主要应用之一。有时观察到自动编码器可以学习比主成分分析和其他技术更复杂的数据投影。
- en: Anomaly detection is also an application of autoencoders. The error or the reconstruction
    error (error between the actual data and the reconstructed data) can be used to
    detect the anomalies.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 异常检测也是自动编码器的一种应用。错误或重建数据之间的误差（实际数据和重建数据之间的误差）可以用来检测异常。
- en: Data compression is also thought as one of the major applications of autoencoders.
    But it has been observed that it is so difficult to beat the basic solutions like
    JPEG by training the algorithm. Moreover, since autoencoders are data specific,
    they can be used only the types of datasets they have been trained upon. If we
    wish to enhance the capacity to include more data types and make it more general,
    they the amount of the training data required will be too high, and obviously
    the time required will be high too.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据压缩也被认为是自动编码器的主要应用之一。但已经观察到，通过训练算法很难打败像JPEG这样的基本解决方案。此外，由于自动编码器是数据特定的，它们只能用于它们已经训练过的数据类型。如果我们希望增加容量以包含更多数据类型并使其更加通用，那么所需的训练数据量将会非常高，显然所需的时间也会很长。
- en: There are other applications too like drug discovery, machine translation, image
    denoising etc. But there are still not a lot of practical implementations of autoencoders
    in the real world. This is because of multitude of reasons like the non-availability
    of data sets, infrastructure, readiness of various systems etc.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有其他应用，比如药物发现，机器翻译，图像去噪等。但是在现实世界中，自动编码器的实际实现还不是很多。这是因为诸多原因，如数据集的不可用性，基础设施，各种系统的准备情况等。
- en: We will now proceed to the types of autoencoders in the next section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在下一节继续介绍自动编码器的类型。
- en: 9.7 Types of autoencoders
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 自动编码器的类型
- en: There are five main types of autoencoders. A brief description of the different
    types of encoders is given below. We have kept the section mathematically light
    and skipped the mathematics behind the scenes as it is quite complex to understand.
    For the curious readers, the link to understand the mathematics behind the scene
    is shared in the Further Reading section of the chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有五种主要类型的自动编码器。下面简要描述了不同类型编码器的情况。我们将本节的数学内容保持简单，并跳过幕后的数学原理，因为这些原理相当复杂难以理解。对于好奇的读者，本章的进一步阅读部分中分享了理解幕后数学原理的链接。
- en: '**Undercomplete autoencoders**: Undercomplete autoencoder is the simplest form
    of an autoencoder. It simply takes an input dataset and then reconstructs the
    same dataset again from the compressed bottleneck region. By penalizing the neural
    network as per the reconstruction error, the model will learn the most significant
    attributes of the data. By learning the most important attributes, the model will
    be able to reconstruct the original data from the compressed state. As we know
    that there is a loss when the compressed data is reconstructed, and this loss
    is called as *reconstruction* loss.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**欠完备自动编码器**：欠完备自动编码器是自动编码器的最简单形式。它只是接受一个输入数据集，然后从压缩的瓶颈区域重新构建相同的数据集。通过根据重构误差惩罚神经网络，模型将学习数据的最重要属性。通过学习最重要的属性，模型将能够从压缩状态重构原始数据。由于在重构压缩数据时存在损失，这种损失被称为*重构*损失。'
- en: Undercomplete autoencoders are really unsupervised in nature as they do not
    have any target label to train. Such types of autoencoders are used for dimensionality
    reduction. Recall in chapter 2, we discussed the dimensionality reduction (PCA)
    and in chapter 6 we discussed the advanced dimensionality reduction algorithms
    (t-SNE and MDS).
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欠完备自动编码器在本质上是无监督的，因为它们没有任何目标标签来训练。这种类型的自动编码器用于降维。回想一下第2章中我们讨论的降维（PCA），以及第6章中我们讨论的高级降维算法（t-SNE和MDS）。
- en: Figure 9.2 The performance starts to improve with more dimensions but decreases
    after sometime. Curse of dimensionality is a real problem
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2 随着维度的增加，性能开始改善，但在一段时间后开始下降。维度灾难是一个真正的问题
- en: '![09_02](images/09_02.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![09_02](images/09_02.png)'
- en: 'Dimensionality reduction is possible using undercomplete autoencoders as the
    bottleneck is created which is the compressed form of the input data. This compressed
    data can be decompressed back with the aid of the network. Recall in chapter 3,
    we discussed that PCA provides a linear combination of the input variables. For
    more details and to refresh PCA, please refer to Chapter 3\. We know that PCA
    tries to get a low dimensional hyperplane to describe the original dataset, undercomplete
    autoencoders can also learn non-linear relationships. We have shown the difference
    in the Figure 9.3 below:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用欠完备自编码器可以进行降维，因为会创建一个压缩形式的输入数据瓶颈。可以利用网络将这种压缩数据解压缩。回想一下在第3章中我们讨论过 PCA 提供了输入变量的线性组合。要获取更多详细信息并刷新
    PCA，请参阅第3章。我们知道 PCA 尝试获得一个低维超平面来描述原始数据集，而欠完备自编码器也可以学习非线性关系。我们在下图9.3中展示了它们之间的差异：
- en: Figure 9.3 PCA is a linear in nature while autoencoders are non-linear in nature.
    It is the core difference between the two algorithms
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3 PCA 是线性的，而自编码器是非线性的。这是两种算法之间的核心区别。
- en: '![09_03](images/09_03.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![09_03](images/09_03.png)'
- en: Interestingly, if all the non-linear activation functions are removed from the
    undercomplete autoencoder and only linear layers are used, it is equivalent to
    a PCA only. To make the autoencoder generalize and not memorize the training data,
    an undercomplete autoencoder is regulated and is fine-tuned by the size of the
    bottleneck. It allows the solution to not memorize the training data and generalize
    very well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，如果从欠完备自编码器中去除所有非线性激活函数，只使用线性层，那么它等价于只有 PCA。为了使自编码器能够泛化而不是记住训练数据，欠完备自编码器是受到限制的，并通过瓶颈的大小进行微调。它允许解决方案不记住训练数据并且泛化能力非常好。
- en: If a machine learning model works very well on the training data but does not
    work on the unseen test data, it is called as overfitting.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个机器学习模型在训练数据上表现非常好，但在未见过的测试数据上却不起作用，则称为过拟合。
- en: '**Sparse autoencoders**: Sparse autoencoders are similar to undercomplete autoencoders
    except they use a different methodology to tackle overfitting. Conceptually, a
    sparse autoencoder changes the number of nodes at each of the hidden layer and
    keep it flexible. Now since it is not possible to have a neural network with such
    a capability of flexible number of neurons, the loss function is customized for
    it. In the loss function, a term is introduced which captures the number of activated
    neurons. There is one more term as the penalty term which is proportional to the
    number of activated neurons. The higher the number of activated neurons, the higher
    is the penalty. This penalty is called *sparsity function*. Using the penalty,
    it is possible to reduce the number of activated neurons, hence the penalty is
    lower and the network is able to tackle the issue of overfitting.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**稀疏自编码器**：稀疏自编码器与欠完备自编码器类似，只是它们使用了不同的方法来解决过拟合问题。在概念上，稀疏自编码器改变了每个隐藏层的节点数量并使其灵活。现在，由于不可能有一个具有这种灵活神经元数量能力的神经网络，因此为其定制了损失函数。在损失函数中，引入了一个项来捕捉激活的神经元数量。还有一个与激活的神经元数量成比例的惩罚项。激活的神经元数量越多，惩罚就越高。这个惩罚被称为*稀疏函数*。通过使用惩罚，可以减少激活的神经元数量，因此惩罚较低，网络能够解决过拟合问题。'
- en: '**Contractive autoencoders**: Contractive autoencoders work on the similar
    concept as other autoencoders. They consider that the inputs which are quite same,
    should be encoded same. And hence, they should have same latent space representation.
    It means that there should not be much difference between the input data and the
    latent space.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收缩自编码器**：收缩自编码器与其他自编码器的工作原理类似。它们认为输入数据相似的部分应该被编码成相同的。因此，它们应该具有相同的潜在空间表示。这意味着输入数据和潜在空间之间不应该有太大的差异。'
- en: '**Denoising autoencoders**: Denoising means removing the noise and that is
    the precise task of denoising autoencoders. They do not take an image as an input,
    instead they take a noisy version of an image as an input as shown in the (Figure
    9.4) below.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**去噪自编码器**：去噪意味着去除噪音，这正是去噪自编码器的精确任务。它们不接受图像作为输入，而是接受图像的嘈杂版本作为输入，如下图9.4所示。'
- en: Figure 9.4 Original image, noisy output and the outputs from the autoencoder
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4 原始图像，嘈杂输出和自编码器的输出
- en: '![09_04](images/09_04.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![09_04](images/09_04.png)'
- en: The process in denoising autoencoder is depicted below. The original image is
    changed by adding noise to it. This noisy image is fed to the encoder-decoder
    architecture and the output received is compared to the original image. The autoencoder
    learns the representation of the image which is used to remove the noise, and
    it is achieved by mapping the input image into a lower-dimensional manifold.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器中的过程如下所示。 通过向原始图像添加噪声来改变原始图像。 这个嘈杂图像被输入到编码器-解码器架构中，接收的输出与原始图像进行比较。 自编码器学习了图像的表示，该表示用于去除噪音，并且通过将输入图像映射到低维流形来实现。
- en: Figure 9.5 The process of denoising in an autoencoder. It starts with original
    image, noise is added which results in a noisy image and then it is fed to the
    autoencoder.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5：自编码器中的去噪过程。 它从原始图像开始，添加噪音产生一个嘈杂图像，然后将其输入到自编码器中。
- en: '![09_05](images/09_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![09_05](images/09_05.png)'
- en: We can use denoising autoencoders for non-linear dimensionality reduction.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用去噪自编码器进行非线性降维。
- en: '**Variational autoencoders**: In a standard autoencoder model, represent the
    input in a compressed form using bottleneck. They are probabilistic generative
    models and only need neural networks as a part of their overall structure. They
    are trained using expectation-maximization meta algorithms. The technical details
    are beyond the scope of this book.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**变分自编码器**：在标准自编码器模型中，使用瓶颈以压缩形式表示输入。 它们是概率生成模型，只需要神经网络作为整体结构的一部分。 它们是使用期望最大化元算法进行训练的。
    技术细节超出了本书的范围。'
- en: We will now move to creating an autoencoder using Python in the next section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在下一节中学习如何使用Python创建一个自编码器。
- en: 9.8 Python implementation of Autoencoders
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.8 自编码器的Python实现
- en: We are creating two versions of autoencoder here. The code has been taken from
    the official source at Keras website ([https://blog.keras.io/building-autoencoders-in-keras.html](blog.keras.io.html))
    and has been modified for our usage.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里创建了两个版本的自编码器。 代码来源于Keras网站的官方来源（[https://blog.keras.io/building-autoencoders-in-keras.html](blog.keras.io.html)），并已经修改以供我们使用。
- en: 'Step 1: First we will import the necessary libraries:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1：首先，我们将导入必要的库：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: We are creating our network architecture here'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：我们在这里创建我们的网络架构
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 3: Add more details to the model'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3：向模型添加更多细节
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 4: load the datasets'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4：加载数据集
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 5: Create the train and test datasets'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 5：创建训练和测试数据集
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 6: fit the model now'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 6：现在适配模型
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![09_05a](images/09_05a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![09_05a](images/09_05a.png)'
- en: 'Step 7: test it on the test dataset'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤7：在测试数据集上进行测试
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 8: and plot the results. You can see the original image and the final
    output.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 8：并绘制结果。 您可以看到原始图像和最终输出。
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![09_05b](images/09_05b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![09_05b](images/09_05b.png)'
- en: 9.9 Closing Thoughts
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.9 思考收尾
- en: Deep learning is a powerful tool. Having a sound business problem and a quality
    dataset, we can create a lot of innovative solutions. Autoencoders are one of
    the types of such solutions only.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个强大的工具。 有了合理的业务问题和高质量的数据集，我们可以创建许多创新解决方案。 自编码器只是这类解决方案中的一种。
- en: In this chapter, we started with feature engineering which allows us to extract
    the most significant features from a dataset. Then we moved to autoencoders. Autoencoders
    are type of neural networks only used to learning efficient codings of unlabeled
    datasets. Autoencoders can be applied to many business problems like facial recognition,
    anamoly detection, image recognition, drug discovery, machine translation and
    so on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从特征工程开始，它允许我们从数据集中提取最重要的特征。 然后我们转向自编码器。 自编码器是一种仅用于学习未标记数据集的高效编码的神经网络类型。
    自编码器可以应用于许多业务问题，如面部识别，异常检测，图像识别，药物发现，机器翻译等。
- en: In this chapter we covered autoencoders. In the next chapter, we are going to
    talk about Generative AI or GenAI which is a hot topic. That will be the tenth
    and penultimate chapter of the book.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们涵盖了自编码器。 在下一章中，我们将讨论生成人工智能或GenAI，这是一个热门话题。 这将是本书的第十章，也是倒数第二章。 '
- en: 9.10 Summary
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.10 总结
- en: We studied feature engineering in this chapter. Feature engineering or feature
    extraction is the process of extracting features from the raw data to improve
    the overall quality of the model and enhance the accuracy as compared to a model
    where only raw data was fed to the machine learning model.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在本章中学习了特征工程。 特征工程或特征提取是从原始数据中提取特征以改善模型的总体质量和增强与仅将原始数据馈送到机器学习模型相比的精度的过程。
- en: We then covered autoencoders in the next sections. We studied autoencoders in
    this chapter. Autoencoders are feed forward neural networks and they compress
    the input into a lower-dimensional code and then try to reconstruct the output
    from this representation.
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们在接下来的章节中介绍了自编码器。我们在本章中学习了自编码器。自编码器是前馈神经网络，它们将输入压缩成低维代码，然后尝试从该表示重构输出。
- en: A typical architecture of autoencoder is an encoder, bottleneck and a decoder.
    Anamoly detection, data compression and dimensionality reductions are major usages
    of autoencoders.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的典型架构包括编码器、瓶颈和解码器。异常检测、数据压缩和降维是自编码器的主要用途。
- en: We also covered different types of autoencoders like undercomplete, sparse,
    denoisining, contractive and variational autoencoders.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还涵盖了不同类型的自编码器，如欠完备、稀疏、去噪、收缩和变分自编码器。
- en: Practical next steps and suggested readings
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际的下一步和建议阅读
- en: There is a great blog here [https://blog.keras.io/building-autoencoders-in-keras.html](blog.keras.io.html)
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有一个很棒的博客 [https://blog.keras.io/building-autoencoders-in-keras.html](blog.keras.io.html)
- en: Study the paper Transforming Auto-encoders by G.E.Hinton, A. Krizhevsky, S.D.Wang
    at [https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/capsules/transforming-autoencoders,-hinton,-icann-2011.pdf](capsules.html)
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读 G.E.Hinton、A. Krizhevsky、S.D.Wang 撰写的关于自编码器的论文《转换自编码器》[https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/capsules/transforming-autoencoders,-hinton,-icann-2011.pdf](capsules.html)
- en: Study the paper Autoencoders by Dor Bank, Naom Koenigstein, Raja Giryes at [https://arxiv.org/abs/2003.05991](abs.html)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读 Dor Bank、Naom Koenigstein、Raja Giryes 撰写的关于自编码器的论文《自编码器》[https://arxiv.org/abs/2003.05991](abs.html)
- en: Study the paper An introduction to Autoencoders by Umberto Michelucci at [https://arxiv.org/abs/2201.03898](abs.html)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读 Umberto Michelucci 撰写的关于自编码器的论文《自编码器简介》[https://arxiv.org/abs/2201.03898](abs.html)
- en: There is a good code and data set available at tensorflow official page at [https://www.tensorflow.org/tutorials/generative/autoencoder](generative.html)
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 tensorflow 官方页面有很好的代码和数据集可用 [https://www.tensorflow.org/tutorials/generative/autoencoder](generative.html)
