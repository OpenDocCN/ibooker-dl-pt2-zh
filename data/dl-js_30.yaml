- en: B.4\. Calculating gradients
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4\. 计算梯度
- en: 'This section is for readers who are interested in performing derivative and
    gradient calculation in TensorFlow.js. For most deep-learning models in this book,
    the calculation of derivatives and gradients is taken care of under the hood by
    `model.fit()` and `model.fitDataset()`. However, for certain problem types, such
    as finding maximally activating images for convolution filters in [chapter 7](kindle_split_019.html#ch07)
    and RL in [chapter 11](kindle_split_023.html#ch11), it is necessary to calculate
    derivatives and gradients explicitly. TensorFlow.js provides APIs to support such
    use cases. Let’s start from the simplest scenario—namely, a function that takes
    a single input tensor and returns a single output tensor:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分适用于对在 TensorFlow.js 中执行导数和梯度计算感兴趣的读者。对于本书中的大多数深度学习模型，导数和梯度的计算都是在`model.fit()`和`model.fitDataset()`中自动完成的。然而，对于某些问题类型，比如在[第7章](kindle_split_019.html#ch07)中找到卷积滤波器的最大激活图像和在[第11章](kindle_split_023.html#ch11)中的
    RL，需要显式地计算导数和梯度。TensorFlow.js 提供了支持此类用例的 API。让我们从最简单的情景开始，即一个接受单个输入张量并返回单个输出张量的函数：
- en: '[PRE0]'
  id: totrans-2
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to calculate the derivative of the function (`f`) with respect to
    the input (`x`), we use the `tf.grad()` function:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算函数(`f`)相对于输入(`x`)的导数，我们使用`tf.grad()`函数：
- en: '[PRE1]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that `tf.grad()` doesn’t give you the derivative’s value right away. Instead,
    it gives you a *function* that is the derivative of the original function (`f`).
    You can invoke that function (`df`) with a concrete value of `x`, and that’s when
    you get the value of `df/dx`. For example,
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`tf.grad()`不会立即给出导数的值。相反，它会给出一个*函数*，即原始函数(`f`)的导数。您可以使用具体的`x`值调用该函数(`df`)，这时您就会得到`df/dx`的值。例如，
- en: '[PRE2]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'which gives you an output that correctly reflects the derivative of the `atan()`
    function at x-values of –4, –2, 0, 2, and 4 (see [figure B.5](#app02fig05)):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它给出了一个输出，正确反映了在x值为-4、-2、0、2和4时`atan()`函数的导数（参见[图 B.5](#app02fig05)）：
- en: '[PRE3]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Figure B.5\. A plot of the function `atan(x)`
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 B.5\. 函数`atan(x)`的图表
- en: '![](btab05_alt.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](btab05_alt.jpg)'
- en: '`tf.grad()` is limited to a function with only one input tensor. What if you
    have a function with multiple inputs? Let’s consider an example of `h(x, y)`,
    which is simply the product of two tensors:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.grad()`仅适用于具有单个输入张量的函数。如果您有一个具有多个输入的函数怎么办？让我们考虑一个简单的例子`h(x, y)`，它只是两个张量的乘积：'
- en: '[PRE4]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`tf.grads()` (with the “s” in the name) generates a function that returns the
    partial derivative of the input function with respect to all the arguments:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.grads()`（带有名称中的“s”）生成一个函数，该函数返回输入函数相对于所有参数的偏导数：'
- en: '[PRE5]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: which gives the results
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了结果
- en: '[PRE6]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These results are correct because the partial derivative of *x* `*` *y* with
    respect to *x* is *y* and that with respect to *y* is *x*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果是正确的，因为相对于*x*的偏导数`*`*y*是*y*，相对于*y*的偏导数是*x*。
- en: 'The functions generated by `tf.grad()` and `tf.grads()` give you only the derivatives,
    not the return value of the original function. In the example of *h*(*x*, *y*),
    what if we want to get not only the derivatives but also the value of *h*? For
    that, you can use the `tf.valueAndGrads()` function:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由`tf.grad()`和`tf.grads()`生成的函数只给出导数，而不是原始函数的返回值。在*h*(*x*, *y*)的示例中，如果我们不仅想要得到导数，还想要*h*的值，那么可以使用`tf.valueAndGrads()`函数：
- en: '[PRE7]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output (`out`) is an object with two fields: `value`, which is the value
    of *h* given the input values, and `grads`, which has the same format as the return
    value of the function generated by `tf.grads()`—namely, an array of partial-derivative
    tensors:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输出(`out`)是一个具有两个字段的对象：`value`，即给定输入值时*h*的值，以及`grads`，其格式与由`tf.grads()`生成的函数的返回值相同，即偏导数张量的数组：
- en: '[PRE8]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The APIs discussed are all about calculating the derivatives of functions with
    respect to their explicit arguments. However, a common scenario in deep learning
    involves functions that use weights in their calculation. Those weights are represented
    as `tf.Variable` objects and are *not* explicitly passed to the functions as arguments.
    For such functions, we often need to calculate their derivatives with respect
    to the weights during training. This workflow is served by the `tf.variableGrads()`
    function, which keeps track of what trainable variables are accessed by the function
    being differentiated and automatically calculates the derivatives with respect
    to them. Consider the following example:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的 API 都涉及到计算函数相对于其显式参数的导数。然而，在深度学习中的一个常见情景是函数在计算中使用了权重。这些权重表示为 `tf.Variable`
    对象，并且*不*作为参数明确传递给函数。对于这样的函数，我们经常需要在训练过程中计算相对于权重的导数。`tf.variableGrads()` 函数就是为此工作流程提供支持的，它跟踪被求导函数所访问的可训练变量，并自动计算相对于它们的导数。考虑以下示例：
- en: '[PRE9]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1*** *f*(*a*, *b*) = *a* * *x* ^ 2 + *b* * *x*. The sum() method is called
    because tf.variableGrads() requires the function being differentiated to return
    a scalar.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** *f*(*a*, *b*) = *a* * *x* ^ 2 + *b* * *x*。调用 `sum()` 方法是因为 `tf.variableGrads()`
    要求被求导函数返回一个标量。'
- en: The `value` field of `tf.variableGrads()`’s output is the return value of `f`
    given the current values of `a`, `b`, and `x`. The `grads` field is a JavaScript
    object that carries the derivatives with respect to the two variables (`a` and
    `b`) under the corresponding key names. For example, the derivative of `f(a, b)`
    with respect to `a` is `x ^ 2`, and the derivative of `f(a, b)` with respect to
    `b` is `x`,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.variableGrads()` 的输出中的 `value` 字段是在给定 `a`、`b` 和 `x` 的当前值时 `f` 的返回值。 `grads`
    字段是一个 JavaScript 对象，它在相应的键名下携带了对两个变量（`a` 和 `b`）的导数。例如，`f(a, b)` 关于 `a` 的导数是 `x
    ^ 2`，`f(a, b)` 关于 `b` 的导数是 `x`，'
- en: '[PRE10]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: which correctly gives
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这正确给出了
- en: '[PRE11]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
