- en: appendix C Natural language processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录C 自然语言处理
- en: 'C.1 Touring around the zoo: Meeting other Transformer models'
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.1 环游动物园：遇见其他Transformer模型
- en: In chapter 13, we discussed a powerful Transformer-based model known as BERT
    (bidirectional encoder representations from Transformers). But BERT was just the
    beginning of a wave of Transformer models. These models grew stronger and better,
    either by solving theoretical issues with BERT or re-engineering various aspects
    of the model to perform faster and better. Let’s understand some of these popular
    models to learn what sets them apart from BERT.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第13章，我们讨论了一种强大的基于Transformer的模型，称为BERT（双向编码器表示来自Transformer）。但BERT只是一波Transformer模型的开始。这些模型变得越来越强大，更好，要么通过解决BERT的理论问题，要么重新设计模型的各个方面以实现更快更好的性能。让我们了解一些流行的模型，看看它们与BERT的不同之处。
- en: C.1.1 Generative pre-training (GPT) model (2018)
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1.1 生成式预训练（GPT）模型（2018）
- en: The story actually starts even before BERT. OpenAI introduced a model called
    GPT in the paper “Improving Language Understanding by Generative Pre-Training”
    by Radford et al. ([http://mng.bz/1oXV](http://mng.bz/1oXV)). It is trained in
    a similar fashion to BERT, pretraining on a large corpus of text followed by fine-tuning
    on a discriminative task. The GPT model is a *Transformer decoder* compared to
    BERT, which is a *Transformer encoder*. The difference is that the GPT model has
    left-to-right (or causal) attention, whereas BERT has bidirectional (i.e., left-to-right
    and right-to-left) attention used when computing self-attention outputs. In other
    words, the GPU model pays attention only to the words to the left of it as it
    computes the self-attention output of a given word. This is the same as the masked
    attention component we discussed in chapter 5\. Due to this, GPT is also called
    an *autoregressive model*, whereas BERT is called an *autoencoder*. In addition,
    unlike BERT, adapting GPT to different tasks like sequence classification, token
    classification, or question answering requires slight architectural changes, which
    is cumbersome. GPT has three versions (GPT-1, GPT-2, and GPT-3); each model became
    bigger while introducing slight changes to improve performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 故事实际上甚至早于BERT。OpenAI在Radford等人的论文“通过生成式预训练改善语言理解”中引入了一个模型称为GPT（[http://mng.bz/1oXV](http://mng.bz/1oXV)）。它的训练方式类似于BERT，首先在大型文本语料库上进行预训练，然后进行有区分性的任务微调。与BERT相比，GPT模型是一个*Transformer解码器*。它们的区别在于，GPT模型具有从左到右（或因果）的注意力，而BERT在计算自注意力输出时使用双向（即从左到右和从右到左）注意力。换句话说，GPU模型在计算给定单词的自注意力输出时只关注其左侧的单词。这与我们在第5章讨论的掩码注意力组件相同。因此，GPT也被称为*自回归模型*，而BERT被称为*自编码器*。此外，与BERT不同，将GPT适应不同的任务（如序列分类、标记分类或问题回答）需要进行轻微的架构更改，这很麻烦。GPT有三个版本（GPT-1、GPT-2和GPT-3）；每个模型都变得更大，同时引入轻微的改进以提高性能。
- en: 'NOTE OpenAI, TensorFlow: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 OpenAI，TensorFlow：[https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)。
- en: C.1.2 DistilBERT (2019)
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1.2 DistilBERT（2019）
- en: 'Following BERT, DistilBERT is a model introduced by Hugging Face in the paper
    “DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter”
    by Sanh et al. ([https://arxiv.org/pdf/1910.01108v4.pdf](https://arxiv.org/pdf/1910.01108v4.pdf))
    in 2019\. The primary focus of DistilBERT is to compress BERT while keeping the
    performance similar. It is trained using a transfer learning technique known as
    *knowledge distillation* ([http://mng.bz/qYV2](http://mng.bz/qYV2)). The idea
    is to have a teacher model (i.e., BERT), and a smaller model (i.e., DistilBERT)
    that tries to mimic the teacher’s output. The DistilBERT model is smaller compared
    to BERT and only has 6 layers, as opposed to BERT, which has 12 layers. The DistilBERT
    model is initialized with the initialization of every other layer of BERT (because
    DistilBERT has exactly half the layers of BERT). Another key difference of DistilBERT
    is that it is only trained on the masked language modeling task and not on the
    next-sentence prediction task.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '跟随 BERT，DistilBERT 是由 Hugging Face 在 Sanh 等人的论文“DistilBERT, a distilled version
    of BERT: Smaller, faster, cheaper and lighter”（[https://arxiv.org/pdf/1910.01108v4.pdf](https://arxiv.org/pdf/1910.01108v4.pdf)）中介绍的模型。DistilBERT
    的主要焦点是在保持性能相似的情况下压缩 BERT。它是使用一种称为*知识蒸馏*（[http://mng.bz/qYV2](http://mng.bz/qYV2)）的迁移学习技术进行训练的。其思想是有一个教师模型（即
    BERT），和一个更小的模型（即 DistilBERT），试图模仿教师的输出。DistilBERT 模型相比于 BERT 更小，只有 6 层，而不是 BERT
    的 12 层。DistilBERT 模型是以 BERT 的每一层的初始化为基础进行初始化的（因为 DistilBERT 恰好有 BERT 层的一半）。DistilBERT
    的另一个关键区别是它只在掩码语言建模任务上进行训练，而不是在下一个句子预测任务上进行训练。'
- en: 'NOTE Hugging Face’s Transformers: [https://huggingface.co/transformers/model_doc/distilbert.xhtml](https://huggingface.co/transformers/model_doc/distilbert.xhtml).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Hugging Face 的 Transformers：[https://huggingface.co/transformers/model_doc/distilbert.xhtml](https://huggingface.co/transformers/model_doc/distilbert.xhtml)。
- en: C.1.3 RoBERT/ToBERT (2019)
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1.3 RoBERT/ToBERT（2019）
- en: RoBERT (recurrence over BERT) and ToBERT (Transformer over BERT) are two models
    introduced in the paper “Hierarchical Transformer Models for Long Document Classification”
    by Pappagari et al. ([https://arxiv.org/pdf/1910.10781.pdf](https://arxiv.org/pdf/1910.10781.pdf)).
    The main problem addressed by this paper is the inability or the performance degradation
    experienced for long text sequences (e.g., call transcripts) in BERT. This is
    because the self-attention layer has a computational complexity of O(n²) for a
    sequence of length n. The solution proposed in these models is to factorize long
    sequences to smaller segments of length k (with overlap) and feed each segment
    to BERT to generate the pooled output (i.e., the output of the [CLS] token) or
    the posterior probabilities (from a task-specific classification layer). Next,
    stack the outputs returned by BERT for each segment and pass them on to a recurrent
    model like LSTM (RoBERT) or a smaller Transformer (ToBERT).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERT（递归 BERT）和 ToBERT（基于 BERT 的 Transformer）是由 Pappagari 等人在论文“Hierarchical
    Transformer Models for Long Document Classification”（[https://arxiv.org/pdf/1910.10781.pdf](https://arxiv.org/pdf/1910.10781.pdf)）中介绍的两个模型。这篇论文解决的主要问题是
    BERT 在长文本序列（例如，电话转录）中的性能下降或无法处理。这是因为自注意力层对长度为 n 的序列具有 O(n²) 的计算复杂度。这些模型提出的解决方案是将长序列分解为长度为
    k 的较小段（有重叠），并将每个段馈送到 BERT 以生成汇集输出（即 [CLS] 标记的输出）或来自任务特定分类层的后验概率。然后，堆叠 BERT 为每个段返回的输出，并将其传递给像
    LSTM（RoBERT）或较小 Transformer（ToBERT）这样的递归模型。
- en: 'NOTE Hugging Face’s Transformers: [https://huggingface.co/transformers/model_doc/roberta.xhtml](https://huggingface.co/transformers/model_doc/roberta.xhtml).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Hugging Face 的 Transformers：[https://huggingface.co/transformers/model_doc/roberta.xhtml](https://huggingface.co/transformers/model_doc/roberta.xhtml)。
- en: C.1.4 BART (2019)
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1.4 BART（2019）
- en: 'BART (bidirectional and auto-regressive Transformers), proposed in “BART: Denoising
    Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,
    and Comprehension” by Lewis et al. ([https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf))
    is a sequence-to-sequence model. We discussed sequence-to-sequence models in chapters
    11 and 12, and BART draws on the same concepts. BART has an encoder and a decoder.
    If you remember from chapter 5, the Transformer model also has an encoder and
    a decoder and can be thought of as a sequence-to-sequence model. The encoder of
    a Transformer has bidirectional attention, whereas the decoder of a Transformer
    has left-to-right attention (i.e., is autoregressive).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: BART（双向和自回归Transformer）由Lewis等人在“BART：去噪序列到序列预训练用于自然语言生成、翻译和理解”（[https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)）中提出，是一个序列到序列模型。我们在第11和12章中已经讨论了序列到序列模型，BART也借鉴了这些概念。BART有一个编码器和一个解码器。如果你还记得第5章，Transformer模型也有一个编码器和一个解码器，可以视为序列到序列模型。Transformer的编码器具有双向注意力，而Transformer的解码器具有从左到右的注意力（即是自回归的）。
- en: Unlike the vanilla Transformer model, BART uses several innovative pre-training
    techniques (document reconstruction) to pretrain the model. Particularly, BART
    is trained as a denoising autoencoder, where a noisy input is provided and the
    model needs to reconstruct the true input. In this case, the input is a document
    (a collection of sentences). The documents are corrupted using the methods listed
    in table C.1.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始Transformer模型不同，BART使用了几种创新的预训练技术（文档重建）来预训练模型。特别地，BART被训练成为一个去噪自编码器，其中提供了一个有噪声的输入，并且模型需要重建真实的输入。在这种情况下，输入是一个文档（一系列句子）。这些文件使用表C.1中列出的方法进行损坏。
- en: Table C.1 Various methods employed in corrupting documents. The true document
    is “I was hungry. I went to the café.” The “_” character denotes the mask token.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表C.1 文档损坏所采用的各种方法。真实文档为“I was hungry. I went to the café.”下划线字符（_）代表遮蔽标记。
- en: '| **Method** | **Description** | **Example** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **方法** | **描述** | **例子** |'
- en: '| Token masking | Tokens in the sentences are randomly masked. | I was _ .
    I _ to the cafe |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 记号遮蔽 | 句子中的记号随机遮蔽。 | 我饿了。我去了_咖啡馆。 |'
- en: '| Token deletion | Tokens are randomly deleted. | I hungry . I went to café
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 记号删除 | 随机删除记号。 | 我饿了。我去了咖啡馆。 |'
- en: '| Sentence permutation | Change the order of the sentences. | I went to the
    café . I was hungry |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 句子排列 | 更改句子顺序。 | 我去了咖啡馆。我饿了。 |'
- en: '| Document rotation | Rotate the document so that the starting and ending of
    the document changes. | Café . I was hungry . I went to the |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 文档旋转 | 旋转文档，以便文档的开头和结尾发生变化。 | 咖啡馆。我饿了。我去了 |'
- en: '| Text infilling | Instead of a single token, mask spans tokens with a single
    mask token. A 0 length span would insert the mask token. | I was _ hungry . I
    _ the café |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 文本补齐 | 使用单一遮蔽标记遮蔽跨度标记。一个长度为0的跨度将插入遮蔽标记。| 我饿了。我_去了咖啡馆。 |'
- en: With the corruption logic, we generate inputs to BART, and the target will be
    the true document without corruptions. Initially, the corrupted document is input
    to the encoder, and then the decoder is asked to recursively predict the true
    sequence while using the previously predicted output(s) as the next input. This
    is similar to how we predicted translations using a machine translation model
    in chapter 11.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用损坏逻辑，我们生成输入到BART的数据，目标就是没有损坏的真实文档。初始时，已经损坏的文档被输入到编码器，然后解码器被要求递归地预测出真实的序列，同时使用先前预测出的输出作为下一个输入。这类似于第11章使用机器翻译模型预测翻译的方法。
- en: 'Once the model is pretrained, you can use BART for any of the NLP tasks that
    Transformer models are typically used for. For example, BART can be used for sequence
    classification tasks (e.g., sentiment analysis) in the following way:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预训练后，你可以将BART用于Transformer模型通常用于的任何NLP任务。例如，可以按以下方式将BART用于序列分类任务（例如，情感分析）：
- en: Input the token sequence (e.g., movie review) to both the encoder and the decoder.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 把记号序列（例如，电影评论）输入到编码器和解码器中。
- en: Add a special token (e.g., [CLS]) to the end of the sequence when feeding the
    decoder input. We added the special token to the beginning of the sequence when
    working with BERT.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在给解码器输入时，在序列末尾添加一个特殊记号（例如，[CLS]）。我们在使用BERT时将特殊记号添加到序列开头。
- en: Get the hidden representation output for the special token by the decoder and
    feed that to a downstream classifier that will predict the final output (e.g.,
    positive/negative prediction).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过解码器得到特殊标记的隐藏表示结果，并将其提供给下游分类器以预测最终输出（例如，积极/消极的预测结果）。
- en: 'To use BART for sequence-to-sequence problems (e.g., machine translation),
    follow these steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要将 BART 用于序列到序列的问题（例如机器翻译），请按照以下步骤进行：
- en: Input the source sequence to the encoder.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将源序列输入编码器。
- en: Add a starting (e.g., [SOS]) and ending (e.g., [EOS]) special token to the start
    and end of the target sequence, respectively.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向目标序列的开头和结尾分别添加起始标记（例如，[SOS]）和结束标记（例如，[EOS]）。
- en: During training, train the decoder with all tokens in the target sequence except
    the last as the input and all tokens but the first as the target (i.e., teacher
    forcing).
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练时，使用除了最后一个标记以外的所有目标序列标记作为输入，使用除了第一个标记以外的所有标记作为目标（即，teacher forcing）来训练解码器。
- en: During inference, provide the starting token as the first input to the decoder
    and recursively predict the next output while using the previous output(s) as
    the input (i.e., autoregressive),
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在推理时，将起始标记提供给解码器作为第一个输入，并递归地预测下一个输出，同时将前一个输出（件）作为输入（即自回归）。
- en: 'NOTE Hugging Face’s Transformers: [http://mng.bz/7ygy](http://mng.bz/7ygy).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '注意 Hugging Face’s Transformers: [http://mng.bz/7ygy](http://mng.bz/7ygy)。'
- en: C.1.5 XLNet (2020)
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1.5 XLNet (2020)
- en: 'XLNet was introduced in the paper “XLNet: Generalized Autoregressive Pretraining
    for Language Understanding” by Yang et al. ([https://arxiv.org/pdf/1906.08237.pdf](https://arxiv.org/pdf/1906.08237.pdf))
    in early 2020\. Its primary focus was to capture the best of both worlds in autoencoder-based
    models (e.g., BERT) and autoregressive models (e.g., GPT). For this discourse,
    it is important to understand the advantages and drawbacks of the two approaches.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'XLNet 是由杨飞等人在 2020 年初发布的论文 "XLNet: Generalized Autoregressive Pretraining for
    Language Understanding" 中推出的。它的主要重点是捕捉基于自编码器模型（例如 BERT）和自回归模型（例如 GPT）两种方法的优点，这很重要。'
- en: A key advantage that BERT has as an autoencoder model is that the task-specific
    classification head has hidden representations of tokens that are enriched by
    bidirectional context, as it can pay attention to both sides of a given token.
    And as you can imagine, knowing what comes before as well as after for a given
    token yield results in better downstream tasks. Conversely, GPT pays attention
    to only the left side of a given word to generate the representation. Therefore,
    GPT’s token representations are suboptimal in the sense that they only pay unidirectional
    attention (left side) to the token.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为自编码器模型，BERT 具有的一个关键优势是，任务特定的分类头包含了由双向上下文丰富化的标记的隐藏表示。正如你所想像的那样，了解给定标记之前和之后的内容能够得到更好的下游任务效果。相反地，GPT
    只关注给定单词的左侧来生成表示。因此，GPT 的标记表示是次优的，因为它们只关注（左侧）上下文的单向 注意力。
- en: On the other hand, the pretraining methodology of BERT involves introducing
    the special token [MASK]. Though this token appears in the pretraining context,
    it never appears in the fine-tuning context, creating a pretraining fine-tuning
    discrepancy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，BERT 的预训练方法涉及引入特殊标记 [MASK]。虽然这个标记出现在预训练的上下文中，但它在微调的上下文中从未出现过，造成了预训练和微调之间的差异。
- en: There’s a more critical issue that is lurking in BERT. BERT formulates the language
    modeling under the assumption that masked tokens are separately constructed (i.e.,
    independence assumption). In other words, if you have the sentence “I love [MASK][1]
    [MASK][2] city,” the second mask token is generated with no attention to what
    was chosen for the [MASK][1] token. This is wrong because to generate a valid
    city name, you must know the value of [MASK][1] before generating [MASK][2]. However,
    the autoregressive nature of GPT allows the model to first predict the value for
    [MASK][1] and then use that along with other words to its left to generate the
    value for [MASK][2] about the first word in the city before generating the second
    word (i.e., context aware).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 中存在一个更为关键的问题。BERT 假设掩盖标记是单独构建的（即独立假设），这在语言建模中是错误的。换句话说，如果你有句子 "I love [MASK][1]
    [MASK][2] city"，第二个掩盖标记是独立于 [MASK][1] 的选择而生成的。这是错误的，因为要生成一个有效的城市名称，必须在生成 [MASK][2]
    之前先了解 [MASK][1] 的值。然而，GPT 的自回归性质允许模型先预测 [MASK][1] 的值，然后使用其与其左侧的其他单词一起生成城市的第一个单词的值，然后生成第二个单词的值（即上下文感知）。
- en: 'XLNet blends these two language modeling approaches into one so that you have
    the bidirectional context coming from the approach used in BERT and the context
    awareness from GPT’s approach. The new approach is called *permutation language
    modeling*. The idea is as follows. Consider a T elements-long sequence of words.
    There are T! permutations for that sequence. For example, the sentence “Bob loves
    cats” will have 3! = 3 × 2 × 1 = 6 permutations:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet将这两种语言建模方法融合为一体，因此你可以从BERT使用的方法中得到双向上下文，以及从GPT的方法中得到上下文感知。这种新方法被称为*置换语言建模*。其思想如下。考虑一个长度为T的单词序列。对于该序列，有T!种排列方式。例如，句子“Bob爱猫”将有3!
    = 3 × 2 × 1 = 6种排列方式：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If the parameters of the language model used to learn this are shared across
    all the permutations, not only can we use an autoregressive approach to learn
    it, but we can also capture information from both sides of the text for a given
    word. This is the main idea explored in the paper.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用于学习的语言模型的参数在所有排列中共享，我们不仅可以使用自回归方法来学习它，还可以捕获给定单词的文本两侧的信息。这是论文中探讨的主要思想。
- en: 'NOTE Hugging Face’s Transformers: [http://mng.bz/mOl2](http://mng.bz/mOl2).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Hugging Face的Transformers：[http://mng.bz/mOl2](http://mng.bz/mOl2)。
- en: C.1.6 Albert (2020)
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1.6 Albert（2020）
- en: 'Albert is a variant BERT model that delivers competitive performance to BERT
    with fewer parameters. Albert makes two important contributions: reducing the
    model size and introducing a new self-supervised loss that helps the model capture
    language better.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Albert是BERT模型的一种变体，其性能与BERT相媲美，但参数更少。Albert做出了两项重要贡献：减少模型大小和引入一种新的自监督损失，有助于模型更好地捕捉语言。
- en: Factorization of the embedding layer
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的因式分解
- en: First, Albert factorizes the embedding matrix used in BERT. In BERT the embeddings
    are metrics of size V × H, where V is the vocabulary size and H is the hidden
    state size. In other words, there is a tight coupling between the embedding size
    (i.e., length of an embedding vector) and the final hidden representation size.
    However, the embeddings (e.g., WordPiece embeddings in BERT) are not designed
    to capture context, whereas hidden states are computed taking both the token and
    its context into account. Therefore, it makes sense to have a large hidden state
    size H, as the hidden state captures a more informative representation of a token
    than embeddings. But doing so increases the size of the embedding matrix due to
    the tight coupling present. Therefore, Albert suggests factorizing the embedding
    matrix to two matrices, V × E and E × H, decoupling the embedding size and the
    hidden state size. With this design, one can increase the hidden state size while
    keeping the embedding size small.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Albert对BERT中使用的嵌入矩阵进行了因式分解。在BERT中，嵌入是大小为V × H的度量，其中V是词汇表大小，H是隐藏状态大小。换句话说，嵌入大小（即嵌入向量的长度）与最终隐藏表示大小之间存在紧密耦合。然而，嵌入（例如BERT中的WordPiece嵌入）并不是设计用来捕捉上下文的，而隐藏状态是在考虑了标记及其上下文的情况下计算的。因此，增大隐藏状态大小H是有意义的，因为隐藏状态比嵌入更能捕获标记的信息性表示。但是，由于存在紧密耦合，这样做会增加嵌入矩阵的大小。因此，Albert建议对嵌入矩阵进行因式分解为两个矩阵，V
    × E和E × H，从而解耦嵌入大小和隐藏状态大小。通过这种设计，可以增加隐藏状态大小，同时保持嵌入大小较小。
- en: Cross-layer parameter sharing
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 跨层参数共享
- en: 'Cross-layer parameter sharing is another technique introduced in Albert to
    reduce the parameter space. Since all layers in BERT (as well the Transformer
    model in general) have uniform layers from top to bottom, parameter sharing is
    trivial. Parameter sharing can happen in one of the following three ways:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 跨层参数共享是Albert中引入的另一种减少参数空间的技术。由于BERT中的所有层（以及Transformer模型一般）从顶部到底部都具有统一的层，参数共享是微不足道的。参数共享可以通过以下三种方式之一实现：
- en: Across all self-attention sublayers
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有自注意子层之间
- en: Across all the fully connected sublayers
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有完全连接的子层之间
- en: Across self-attention and fully connected sublayers (separately)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自注意和完全连接的子层之间（分开）
- en: Albert uses sharing all parameters across layers as the default strategy. By
    using this strategy, Albert achieves a 71%-86% parameter reduction without compromising
    the performance of the model significantly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Albert将跨层共享所有参数作为默认策略。通过使用这种策略，Albert实现了71%到86%的参数减少，而不会显著影响模型的性能。
- en: Sentence-order prediction instead of next sentence prediction
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 句序预测而非下一句预测
- en: 'Finally, the authors of the paper argue that the value added by the next-sentence
    prediction pretraining task in BERT is doubtful, which is supported by several
    previous studies. Therefore, they introduce a new, more challenging model that
    focuses primarily on language coherence: sentence-order prediction. In this, the
    model is trained with a binary classification head to predict whether a given
    pair of sentences are in the correct order. The data can be easily generated,
    where positive samples are taken as sentences next to each other in that order,
    and negative samples are generated by swapping two adjacent sentences. The authors
    argue that this is more challenging than next-sentence prediction, leading to
    a more informed model than BERT.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者最终认为，BERT 中基于下一句预测的预训练任务所增加的价值是值得怀疑的，这一点得到了多项先前研究的支持。因此，他们提出了一种新的、更具挑战性的模型，主要关注语言的一致性：句子顺序预测。在这个模型中，模型使用一个二元分类头来预测给定的一对句子是否是正确的顺序。数据可以很容易地生成，正样本是按照顺序排列的相邻句子，而负样本则是通过交换两个相邻句子生成的。作者认为这比基于下一句预测的任务更具挑战性，导致比
    BERT 更具见解的模型。
- en: 'NOTE TFHub: ([https://tfhub.dev/google/albert_base/3](https://tfhub.dev/google/albert_base/3)).
    Hugging Face’s Transformers: ([http://mng.bz/5QM1](http://mng.bz/5QM1)).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '备注 TFHub: ([https://tfhub.dev/google/albert_base/3](https://tfhub.dev/google/albert_base/3))。Hugging
    Face 的 Transformers: ([http://mng.bz/5QM1](http://mng.bz/5QM1))。'
- en: C.1.7 Reformer (2020)
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1.7 Reformer (2020)
- en: 'The Reformer was one of the latest to join the family of Transformers. The
    main idea behind the Reformer is its ability to scale to sequences that are several
    tens of thousands of tokens long. The Reformer was introduced in the paper “Reformer:
    The Efficient Transformer” by Kitaev et al. ([https://arxiv.org/pdf/2001.04451.pdf](https://arxiv.org/pdf/2001.04451.pdf))
    in early 2020.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reformer 是最近加入 Transformer 家族的模型之一。Reformer 的主要思想是能够扩展到包含数万个标记的序列。Reformer
    在 2020 年初由 Kitaev 等人在论文“Reformer: The Efficient Transformer”中提出（[https://arxiv.org/pdf/2001.04451.pdf](https://arxiv.org/pdf/2001.04451.pdf)）。'
- en: The main limitation of the vanilla Transformers that prevents them from being
    used for long sequences is the computational complexity of the self-attention
    layer. It needs to look at every other word for every word to generate the final
    representation, which has a O(L²) complexity for a sequence that has L tokens.
    The reformer uses locality-sensitive hashing (LSH) to reduce this complexity to
    O(L logL). The idea of LSH is to assign every input a hash; the inputs having
    the same hash are considered similar and assigned to the same bucket. With that,
    similar inputs are placed in one bucket. To do that, we have to introduce several
    modifications to the self-attention sublayer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 防止普通 Transformer 被用于长序列的主要限制是自注意力层的计算复杂性。它需要对每个词查看所有其他词，以生成最终的表示，这对含有 L 个标记的序列具有
    O(L²) 的复杂性。Reformer 使用局部敏感哈希（LSH）将这种复杂性降低到 O(L logL)。LSH 的思想是为每个输入分配一个哈希；具有相同哈希的输入被认为是相似的，并被分配到同一个桶。这样，相似的输入就会被放在同一个桶中。为此，我们需要对自注意力子层进行若干修改。
- en: Locality-sensitive hashing in the self-attention layer
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层中的局部敏感哈希
- en: First, we need to make sure the Q and K matrices are identical. This is a necessity
    as the idea is to compute similarity between queries and keys. This is easily
    done by sharing the weights between Q and K weight matrices. Next, a hashing function
    needs to be developed, which can generate a hash for a given query/key so that
    similar queries/keys (shared-qk) get similar hashes. Also remember that this must
    be done in a differentiable way to ensure end-to-end training of the model. The
    following hashing function is used
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确保 Q 和 K 矩阵是相同的。这是必要的，因为其思想是计算查询和键之间的相似性。这可以通过共享 Q 和 K 的权重矩阵的权重来轻松实现。接下来，需要开发一个哈希函数，可以为给定的查询/键生成哈希，使得相似的查询/键（共享-qk）获得相似的哈希值。同时需要记住，这必须以可微的方式完成，以确保模型的端到端训练。使用了以下哈希函数：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where R is a random matrix of size [d_model, b/2] for a user-defined b (i.e.,
    number of buckets) and x is the input of shape [b, L, d_model]. By using this
    hashing function, you get a bucket ID for each input token in a batch in a given
    position. To learn more about this technique, refer to the original paper “Practical
    and Optimal LSH for Angular Distance” by Andoni et al. ([https://arxiv.org/pdf/1509.02897.pdf](https://arxiv.org/pdf/1509.02897.pdf)).
    According to the bucket ID, the shared-qk items are sorted.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，R是大小为[d_model, b/2]的随机矩阵，用于用户定义的b（即，桶的数量），x是形状为[b, L, d_model]的输入。通过使用这个散列函数，您可以得到批处理中每个输入标记在给定位置的桶ID。要了解更多关于这个技术的信息，请参考原始论文“Practical
    and Optimal LSH for Angular Distance” by Andoni et al.（[https://arxiv.org/pdf/1509.02897.pdf](https://arxiv.org/pdf/1509.02897.pdf)）。根据桶ID，共享的qk项被排序。
- en: Then the sorted shared-qk items are chunked using a fixed chunk size. A larger
    chunk size means more computations (i.e., more words are considered for a given
    token), whereas a smaller chunk size can mean underperformance (i.e., not enough
    tokens to look at).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，排序后的共享的qk项使用固定的块大小进行分块。更大的块大小意味着更多的计算（即，会考虑更多单词来给定标记），而较小的块大小可能意味着性能不佳（即，没有足够的标记可供查看）。
- en: Finally, the self-attention is computed as follows. For a given token, look
    in the same chunk that it’s in as well as the previous chunk and attend to the
    words with the same bucket ID in both of those chunks. This will produce the self-attention
    output for all the tokens provided in the input. This way, the model does not
    have to look at every other word for every token and can focus on a subset of
    words or tokens for a given token. This makes the model scalable to sequences
    several tens of thousands of tokens long.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，自注意力的计算如下。对于给定的标记，查看它所在的相同块以及前一个块，并关注这两个块中具有相同桶ID的单词。这将为输入中提供的所有标记产生自注意力输出。这样，模型不必为每个标记查看每个其他单词，可以专注于给定标记的子集单词或标记。这使得模型可伸缩到长达几万个标记的序列。
- en: 'NOTE Hugging Face’s Transformers: [http://mng.bz/6XaD](http://mng.bz/6XaD).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '注意Hugging Face''s Transformers: [http://mng.bz/6XaD](http://mng.bz/6XaD)。'
