- en: 9 Stackable deep learning (Transformers)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9种可堆叠的深度学习（transformers）。
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Understanding what makes transformers so powerful
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解transformers如此强大的原因。
- en: Seeing how transformers enable limitless "stacking" options for NLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看transformers如何为自然语言处理提供无限的“堆叠”选项。
- en: Encoding text to create meaningful vector representations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码文本以创建有意义的向量表示。
- en: Decoding semantic vectors to generate text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码语义向量以生成文本。
- en: Finetuning transformers (BERT, GPT) for your application
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的应用程序对transformers（BERT、GPT）进行微调。
- en: Applying transformers to extractive and abstraction summarization of long documents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将transformers应用于长文档的抽取和摘要概括。
- en: Generating grammatically correct and interesting text with transformers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用transformers生成语法正确且有趣的文本。
- en: Estimating the information capacity of a transformer network required for a
    particular problem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算transformer网络为特定问题所需的信息容量。
- en: '*Transformers* are changing the world. The increased intelligence that transformers
    bring to AI is transforming culture, society, and the economy. For the first time,
    transformers are making us question the long-term economic value of human intelligence
    and creativity. And the ripple effects of transformers go deeper than just the
    economy. Transformers are changing not only how we work and play, but even how
    we think, communicate, and create. Within less than a year, transformer-enabled
    AI known as Large Language Models (LLMs) created whole new job categories such
    as *prompt engineering* and real-time content curation and fact-checking (grounding).
    Tech companies are racing to recruit engineers that can design effective LLM prompts
    and incorporate LLMs into their workflows. Transformers are automating and accelerating
    productivity for information economy jobs that previously required a level of
    creativity and abstraction out of reach for machines.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*Transformers*正在改变世界。transformers为人工智能带来的增强智能正在改变文化、社会和经济。transformers首次让我们质疑人类智能和创造力的长期经济价值。而transformers的涟漪效应不仅仅限于经济。transformers不仅正在改变我们的工作和娱乐方式，甚至还在改变我们的思维、沟通和创造方式。在不到一年的时间里，被称为大型语言模型（LLMs）的transformer启用的人工智能创造了全新的工作类别，如*prompt
    engineering*和实时内容策划与事实核查（grounding）。科技公司正在争相招募能够设计有效的LLM提示并将LLMs纳入其工作流程的工程师。transformers正在自动化和加速信息经济工作的生产力，而这些工作以前需要机器无法达到的创造力和抽象水平。'
- en: As transformers automate more and more information economy tasks, workers are
    reconsidering whether their jobs are as essential to their employers as they thought.
    For example, influential CyberSecurity experts are bragging about augmenting their
    thinking, planning, and creativity with the help of dozens of ChatGPT suggestions
    every day.^([[1](#_footnotedef_1 "View footnote.")]) Microsoft News and the MSN.com
    website laid off its journalists in 2020, replacing them with transformer models
    capable of curating and summarizing news articles automatically. This race to
    the bottom (of the content quality ladder) probably won’t end well for media companies
    or their advertisers and employees.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着transformers自动化越来越多的信息经济任务，工作者们开始重新考虑他们的工作是否像他们想象的那样对雇主至关重要。例如，有影响力的网络安全专家每天都在吹嘘，他们正在借助ChatGPT的数十个建议来增强他们的思维、规划和创造力。2020年，微软新闻和MSN.com网站裁员了其新闻记者，用能够自动策划和摘要新闻文章的transformer模型取代了他们。这场（内容质量阶梯的）竞赛可能不会对媒体公司或他们的广告客户和员工带来好的结果。
- en: In this chapter, you will learn how to use transformers to *improve* the accuracy
    and thoughtfulness of natural language text. Even if your employer tries to program
    away your job, you will know how to program transformers to create new opportunities
    for yourself. Program or be programmed. Automate or be automated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用**transformers**来*提高*自然语言文本的准确性和思考性。即使你的雇主试图用编程来替代你的工作，你也会知道如何使用transformers来为自己创造新的机会。编程还是被编程。自动化还是被自动化。
- en: And transformers are your best choice not only for natural language generation,
    but also for natural language understanding. Any system that relies on a vector
    representation of meaning can benefit from transformers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 并且transformers不仅是自然语言生成的最佳选择，也是自然语言理解的最佳选择。任何依赖于意义向量表示的系统都可以从transformers中受益。
- en: At one point Replika used GPT-3 to generate more than 20% of its replies
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一段时间，Replika使用**GPT-3**来生成超过20%的回复。
- en: Qary uses BERT to generate open domain question answers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qary使用BERT生成开放域问题答案。
- en: Google uses models based on BERT to improve search results and query a knowledge
    graph
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌使用基于**BERT**的模型来改进搜索结果并查询知识图谱。
- en: '`nboost` uses transformers to create a semantic search proxy for ElasticSearch'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nboost` 使用transformers为ElasticSearch创建语义搜索代理'
- en: aidungeon.io uses GPT-3 to generate an endless variety of rooms
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: aidungeon.io 使用GPT-3生成无尽种类的房间
- en: Most vector databases for semantic search rely on transformers.^([[2](#_footnotedef_2
    "View footnote.")])
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数语义搜索的向量数据库都依赖于transformers。^([[2](#_footnotedef_2 "查看脚注。")])
- en: Even if you only want to get good at *prompt engineering*, your understanding
    of transformers will help you design prompts for LLMs that avoid the holes in
    LLM capabilities. And LLMs are so full of holes that engineers and statisticians
    often use the *swiss cheese model* when thinking about how LLMs fail. ^([[3](#_footnotedef_3
    "View footnote.")]) The conversational interface of LLMs makes it easy to learn
    how to cajole the snarky conversational AI systems into doing valuable work. People
    that understand how LLMs work and can fine tune them for their own applications,
    those people will have their hands at the helm of a powerful machine. Imagine
    how sought-after you’d be if you could build a "TutorGPT" that can help students
    solve arithmetic and math word problems. Shabnam Aggarwal at Rising Academies
    in Kigali is doing just that with her Rori.AI WhatsApp math tutor bot for middle
    school students.^([[4](#_footnotedef_4 "View footnote.")]) ^([[5](#_footnotedef_5
    "View footnote.")]) And Vishvesh Bhat did this for college math students as a
    passion project.^([[6](#_footnotedef_6 "View footnote.")]) ^([[7](#_footnotedef_7
    "View footnote.")])
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你只想精通*提示工程*，你对transformers的理解也将帮助你设计避开LLM（大型语言模型）能力缺陷的LLM提示。而LLM充满漏洞，以至于工程师和统计学家在思考LLM失败时常常使用*瑞士奶酪模型*。^([[3](#_footnotedef_3
    "查看脚注。")]) LLM的会话界面使学习如何诱导尖刻的对话人工智能系统做出有价值的工作变得容易。了解LLM如何工作并能够为自己的应用程序微调它们的人，将掌握一台强大的机器。想象一下，如果你能够构建一个能够帮助学生解决算术和数学问题的“TutorGPT”，那么你将会受到多么追捧。基加利的Rising
    Academies的Shabnam Aggarwal正用她的Rori.AI WhatsApp数学辅导机器人帮助中学生做到这一点。^([[4](#_footnotedef_4
    "查看脚注。")]) ^([[5](#_footnotedef_5 "查看脚注。")]) 而Vishvesh Bhat则将这一点做为自己的激情项目帮助大学数学学生做到了。^([[6](#_footnotedef_6
    "查看脚注。")]) ^([[7](#_footnotedef_7 "查看脚注。")])
- en: 9.1 Recursion vs recurrence
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 递归 vs 循环
- en: Transformers are the latest big leap forward in auto-regressive NLP models.
    Auto-regressive models predict one discrete output value at a time, usually a
    token or word in natural language text. An autoregressor recycles the output to
    reuse it as an input for predicting the next output so auto-regressive neural
    networks are *recursive*. The word "recursive" is a general term for any recycling
    of outputs back into the input, a process that can continue indefinitely until
    an algorithm or computation "terminates." A recursive function in computer science
    will keep calling itself until it achieves the desired result.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 是自回归式自然语言处理模型中的最新重大进展。自回归模型一次预测一个离散输出值，通常是自然语言文本中的一个标记或词语。自回归模型将输出循环利用作为输入来预测下一个输出，因此自回归神经网络是*递归*的。词语“递归”是一个通用术语，用于描述将输出再次引入输入的任何循环过程，这个过程可以无限地继续，直到算法或计算“终止”。在计算机科学中，递归函数会一直调用自身，直到达到期望的结果。
- en: But transformers are recursive in a bigger and more general way than *Recurrent*
    Neural Networks. Transformers are called *recursive* NNs rather than *recurrent*
    NNs because *recursive* is a more general term for any system that recycles the
    input.^([[8](#_footnotedef_8 "View footnote.")]) The term *recurrent* is used
    exclusively to describe RNNs such as LSTMs and GRUs where the individual neurons
    recycle their outputs into the same neuron’s input for each step through the sequence
    tokens.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，transformers的递归方式更大、更一般，而不像*循环*神经网络那样。transformers被称为*递归*NN，而不是*循环*NN，因为*递归*是一个更一般的术语，用于描述任何将输入循环利用的系统。^([[8](#_footnotedef_8
    "查看脚注。")]) 术语*循环*专门用于描述像LSTM和GRU这样的RNN，其中各个神经元将其输出循环到同一神经元的输入，以便在序列标记的每个步骤中进行。
- en: Transformers are a *recursive* algorithm but do not contain *recurrent* neurons.
    As you learned in Chapter 8, recurrent neural networks recycle their output within
    each individual neuron or RNN *unit*. But transformers wait until the very last
    layer to output a token embedding that can be recycled back into the input. The
    entire transformer network, both the encoder and the decoder, must be run to predict
    each token so that token can be used to help it predict the next one. In the computer
    science world, you can see that a transformer is one big recursive function calling
    a series of nonrecursive functions inside. The whole transformer is run *recursively*
    to generate one token at a time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 是一种*递归*算法，但不包含*循环*神经元。正如你在第 8 章学到的那样，循环神经网络在每个单个神经元或 RNN *单元*内循环利用其输出。但是
    Transformers 等待到最后一层才输出一个可以回收到输入中的令牌嵌入。整个 Transformer 网络，包括编码器和解码器，必须运行以预测每个令牌，以便该令牌可以用来帮助预测下一个。在计算机科学世界中，你可以看到
    Transformer 是一个大的递归函数调用一系列非递归函数内部。整个 Transformer 递归运行以生成一个令牌。
- en: '![transformer recursion drawio](images/transformer_recursion_drawio.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![transformer recursion drawio](images/transformer_recursion_drawio.png)'
- en: Because there is no recurrence within the inner guts of the transformer it doesn’t
    need to be "unrolled." This gives transformers a huge advantage over RNNs. The
    individual neurons and layers in a transformer can be run in parallel all at once.
    For an RNN, you had to run the functions for the neurons and layers one at a time
    in sequence. *Unrolling* all these recurrent function calls takes a lot of computing
    power and it must be performed in order. You can’t skip around or run them in
    parallel. They must be run sequentially all the way through the entire text. A
    transformer breaks the problem into a much smaller problem, predicting a single
    token at a time. This way all the neurons of a transformer can be run in parallel
    on a GPU or multi-core CPU to dramatically speed up the time it takes to make
    a prediction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Transformer 内部没有循环，所以不需要“展开”。这使得 Transformers 比 RNN 有巨大优势。Transformer 中的单个神经元和层可以同时并行运行。对于
    RNN，你必须按顺序依次运行神经元和层的函数。*展开*所有这些循环函数调用需要大量计算资源，并且必须按顺序执行。你不能跳过或并行运行它们。它们必须按顺序一直运行到整个文本的结尾。Transformer
    将问题分解为一个更小的问题，一次预测一个令牌。这样，Transformer 的所有神经元都可以在 GPU 或多核 CPU 上并行运行，从而大大加快预测所需的时间。
- en: They use the last predicted output as the input to predict the next output.
    But transformers are *recursive* not *recurrent*. Recurrent neural networks (RNNs)
    include variational autoencoders, RNNs, LSTMs, and GRUs. When researchers combine
    five NLP ideas to create the transformer architecture, they discovered a total
    capability that was much greater than the sum of its parts. Let’s looks at these
    ideas in detail.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用最后预测的输出作为输入来预测下一个输出。但是 Transformers 是*递归*而不是*循环*的。循环神经网络（RNNs）包括变分自动编码器、RNNs、LSTMs
    和 GRUs。当研究人员将五种自然语言处理思想结合起来创建 Transformer 架构时，他们发现总体能力远远超过其各部分之和。让我们详细看看这些思想。
- en: 9.1.1 Attention is NOT all you need
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 注意力不是你所需要的全部
- en: '*Byte pair encoding (BPE)*:: Tokenizing words based on character sequence statistics
    rather than spaces and punctuation'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字节对编码（BPE）*：基于字符序列统计而不是空格和标点符号对单词进行标记化'
- en: '*Attention*:: Connecting important word patterns together across long stretches
    of text using a connection matrix (attention)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意力*：使用连接矩阵（注意力）在长段文本中连接重要的单词模式'
- en: '*Positional encoding*:: Keeping track of where each token or pattern is located
    within the token sequence'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置编码*：跟踪令牌序列中每个令牌或模式的位置'
- en: Byte pair encoding (BPE) is an often overlooked enhancement of transformers.
    BPE was originally invented to encode text in a compressed binary (byte sequence)
    format. But BPE really came into its own when it was used as a tokenizer in NLP
    pipelines such as search engines. Internet search engines often contain millions
    of unique words in their vocabulary. Imagine all the important names a search
    engine is expected to understand and index. BPE can efficiently reduce your vocabulary
    by several orders of magnitude. The typical transformer BPE vocabulary size is
    only 5000 tokens. And when you’re storing a long embedding vector for each of
    your tokens, this is a big deal. A BPE vocabulary trained on the entire Internet
    can easily fit in the RAM of a typical laptop or GPU.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 字节对编码（Byte pair encoding，BPE）经常被忽视，是transformer的一种常见增强。BPE最初是为了将文本编码成压缩的二进制（字节序列）格式而发明的。但是，当BPE被用作NLP流水线（如搜索引擎）中的分词器时，它真正展现了其作用。互联网搜索引擎通常包含数百万个词汇。想象一下搜索引擎预期要理解和索引的所有重要名称。BPE可以有效地将您的词汇量减少几个数量级。典型的transformer
    BPE词汇量仅为5000个标记。当您为每个标记存储一个长的嵌入向量时，这是一件大事。一个在整个互联网上训练的BPE词汇表可以轻松适应典型笔记本电脑或GPU的RAM中。
- en: Attention gets most of the credit for the success of transformers because it
    made the other parts possible. The attention mechanism is a much simpler approach
    than the complicated math (and computational complexity) of CNNs and RNNs. The
    attention mechanism removes the recurrence of the encoder and decoder networks.
    So a transformer has neither the *vanishing gradients* nor the *exploding gradients*
    problem of an RNN. Transformers are limited in the length of text they can process
    because the attention mechanism relies on a fixed-length sequence of embeddings
    for both the inputs and outputs of each layer. The attention mechanism is essentially
    a single CNN kernel that spans the entire sequence of tokens. Instead of rolling
    across the text with convolution or recurrence, the attention matrix is simply
    multiplied once by the entire sequence of token embeddings.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制获得了transformer成功的大部分赞誉，因为它使其他部分成为可能。注意力机制比CNN和RNN的复杂数学（和计算复杂度）更简单。注意力机制消除了编码器和解码器网络的循环。因此，transformer既没有RNN的*梯度消失*问题，也没有*梯度爆炸*问题。transformer在处理的文本长度上受到限制，因为注意力机制依赖于每层的输入和输出的固定长度的嵌入序列。注意力机制本质上是一个跨越整个令牌序列的单个CNN核。注意力矩阵不是通过卷积或循环沿着文本滚动，而是简单地将其一次性乘以整个令牌嵌入序列。
- en: The loss of recurrence in a transformer creates a new challenge because the
    transformer operates on the entire sequence all at once. A transformer is *reading*
    the entire token sequence all at once. And it outputs the tokens all at once as
    well, making bi-directional transformers an obvious approach. Transformers do
    not care about the normal causal order of tokens while it is reading or writing
    text. To give transformers information about the causal sequence of tokens, positional
    encoding was added. And it doesn’t even require additional dimensions within the
    vector embedding, positional encoding is spread out over the entire embedding
    sequence by multiplying them by the sine and cosine functions. Positional encoding
    enables nuanced adjustment to a transformer’s understanding of tokens depending
    on their location in a text. With positional encoding, the word "sincerely" at
    the beginning of an email has a different meaning than it does at the end of an
    email.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: transformer中的循环丢失造成了一个新的挑战，因为transformer一次性操作整个序列。transformer一次性*读取*整个令牌序列。而且它也一次性输出令牌，使得双向transformer成为一个明显的方法。transformer在读取或写入文本时不关心令牌的正常因果顺序。为了给transformer提供关于令牌因果序列的信息，添加了位置编码。而且甚至不需要向量嵌入中的额外维度，位置编码通过将它们乘以正弦和余弦函数分散在整个嵌入序列中。位置编码使transformer对令牌的理解能够根据它们在文本中的位置进行微妙调整。有了位置编码，邮件开头的词“真诚”与邮件末尾的词“真诚”具有不同的含义。
- en: 'Limiting the token sequence length had a cascading effect of efficiency improvements
    that give transformers an unexpectedly powerful advantage over other architectures:
    *scalability*. BPE plus *attention* plus positional encoding combine together
    to create unprecedented scalability. These three innovations and simplifications
    of neural networks combined to create a network that is both much more stackable
    and much more parallelizable.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 限制令牌序列长度对效率改进产生了连锁反应，为transformers赋予了意外的强大优势：*可扩展性*。BPE加上*注意力*和位置编码结合在一起，创造了前所未有的可扩展性。这三项创新和神经网络的简化结合在一起，创建了一个更易堆叠和更易并行化的网络。
- en: '*Stackability*:: The inputs and outputs of a transformer layer have the exact
    same structure so they can be stacked to increase capacity'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*堆叠性*：transformers层的输入和输出具有完全相同的结构，因此它们可以堆叠以增加容量。'
- en: '*Parallelizability*:: The cookie cutter transformer layers all rely heavily
    on large matrix multiplications rather than complex recurrence and logical switching
    gates'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*并行性*：模板化的transformers层主要依赖于大型矩阵乘法，而不是复杂的递归和逻辑切换门。'
- en: This stackability of transformer layers combined with the parallelizablity of
    the matrix multiplication required for the attention mechanism creates unprecedented
    scalability. And when researchers tried out their large-capacity transformers
    on the largest datasets they could find (essentially the entire Internet), they
    were taken aback. The extremely large transformers trained on extremely large
    datasets were able to solve NLP problems previously thought to be out of reach.
    Smart people are beginning to think that world-transforming conversational machine
    intelligence (AGI) may only be years away, if it isn’t already upon us.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: transformers层的堆叠性与用于注意机制的矩阵乘法的可并行性相结合，创造了前所未有的可扩展性。当研究人员将其大容量transformers应用于他们能找到的最大数据集（基本上是整个互联网）时，他们感到惊讶。在极大的数据集上训练的极大transformers能够解决以前认为无法解决的NLP问题。聪明的人们开始认为，世界改变性的对话式机器智能（AGI）可能只有几年的时间，如果它已经存在的话。
- en: 9.1.2 Much attention about everything
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 关于一切的关注
- en: You might think that all this talk about the power of attention is much ado
    about nothing. Surely transformers are more than just a simple matrix multiplication
    across every token in the input text. Transformers combine many other less well-known
    innovations such as BPE, self-supervised training, and positional encoding. The
    attention matrix was the connector between all these ideas that helped them work
    together effectively. And the attention matrix enables a transformer to accurately
    model the connections between *all* the words in a long body of text, all at once.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为所有关于注意力强大之说都是无中生有。毕竟，transformers不仅仅是在输入文本的每个令牌上进行简单的矩阵乘法。transformers结合了许多其他不那么知名的创新，如BPE、自监督训练和位置编码。注意力矩阵是所有这些想法之间的连接器，帮助它们有效地协同工作。注意力矩阵使transformers能够准确地建模长篇文本中*所有*单词之间的联系，一次完成。
- en: As with CNNs and RNNs (LSTMs & GRUs), each layer of a transformer gives you
    a deeper and deeper representation of the *meaning* or *thought* of the input
    text. But unlike CNNs and RNNs, the transformer layer outputs an encoding that
    is the exact same size and shape as the previous layers. Likewise for the decoder,
    a transformer layer outputs a fixed-size sequence of embeddings representing the
    semantics (meaning) of the output token sequence. The outputs of one transformer
    layer can be directly input into the next transformer layer making the layers
    even more *stackable* than CNN’s. And the attention matrix within each layer spans
    the entire length of the input text, so each transformer layer has the same internal
    structure and math. You can stack as many transformer encoder and decoder layers
    as you like creating as deep a neural network as you need for the information
    content of your data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与CNN和RNN（LSTM和GRU）一样，transformers的每一层都为您提供了输入文本*含义*或*思想*的越来越深入的表示。但与CNN和RNN不同，transformers层的输出编码与之前的层大小和形状完全相同。同样，对于解码器，transformers层输出一个固定大小的嵌入序列，表示输出令牌序列的语义（含义）。一个transformers层的输出可以直接输入到下一个transformers层中，使层更加*堆叠*，而不是CNN的情况。每个层内的注意力矩阵跨越整个输入文本的长度，因此每个transformers层具有相同的内部结构和数学。您可以堆叠尽可能多的transformers编码器和解码器层，为数据的信息内容创建您所需的深度神经网络。
- en: Every transformer layer outputs a consistent *encoding* with the same size and
    shape. Encodings are just embeddings but for token sequences instead of individual
    tokens. In fact, many NLP beginners use the terms "encoding" and embedding" interchangeably,
    but after this chapter, you will understand the difference. The word "embedding",
    used as a noun, is 3 times more popular than "encoding", but as more people catch
    up with you in learning about transformers that will change.^([[9](#_footnotedef_9
    "View footnote.")]) If you don’t need to make it clear which ones you are talking
    about you can use "semantic vector", a term you learned in Chapter 6.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个transformers层都输出一个一致的*编码*，大小和形状相同。编码只是嵌入，但是针对标记序列而不是单个标记。事实上，许多自然语言处理初学者将术语“编码”和“嵌入”视为同义词，但在本章之后，您将了解到它们之间的区别。作为名词使用的“嵌入”一词比“编码”更受欢迎3倍，但随着更多的人在学习transformers方面跟上你的步伐，情况将会改变。[[9]](#_footnotedef_9)
- en: Like all vectors, encodings maintain a consistent structure so that they represent
    the meaning of your token sequence (text) in the same way. And transformers are
    designed to accept these encoding vectors as part of their input to maintain a
    "memory" of the previous layers' understanding of the text. This allows you to
    stack transformer layers with as many layers as you like if you have enough training
    data to utilize all that capacity. This "scalability" allows transformers to break
    through the diminishing returns ceiling of RNNs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有向量一样，编码保持一致的结构，以便它们以相同的方式表示您的标记序列（文本）的含义。transformers被设计为接受这些编码向量作为其输入的一部分，以保持对文本前几层理解的“记忆”。这使您可以堆叠任意多层的transformers层，只要您有足够的训练数据来利用所有这些容量。这种“可伸缩性”使得transformers能够突破循环神经网络的收益递减上限。
- en: And because the attention mechanism is just a connection matrix, it can be implemented
    as a matrix multiplication with a PyTorch `Linear` layer. Matrix multiplications
    are parallelized when you run your PyTorch network on a GPU or multicore CPU.
    This means that much larger transformers can be parallelized and these much larger
    models can be trained much faster. *Stackability* plus *Parallelizablity* equals
    *Scalability*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注意力机制只是一个连接矩阵，因此可以将其实现为与PyTorch `Linear`层的矩阵乘法。当您在GPU或多核CPU上运行PyTorch网络时，矩阵乘法是并行化的。这意味着可以并行化更大的transformers，并且这些更大的模型可以训练得更快。*堆叠性*加上*可并行化*等于*可扩展性*。
- en: Transformer layers are designed to have inputs and outputs with the same size
    and shape so that the transformer layers can be stacked like Lego bricks that
    all have the same shape. The transformer innovation that catches most researchers'
    attention is the *attention mechanism*. Start there if you want to understand
    what makes transformers so exciting to NLP and AI researchers. Unlike other deep
    learning NLP architectures that use recurrence or convolution, the transformer
    architecture uses stacked blocks of attention layers which are essentially fully-connected
    feedforward layers with the same.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: transformers层被设计为具有相同大小和形状的输入和输出，以便transformers层可以像形状相同的乐高积木一样堆叠。吸引大多数研究人员注意力的transformers创新是*注意力机制*。如果您想要了解使transformers对自然语言处理和人工智能研究人员如此兴奋的原因，请从那里开始。与使用循环或卷积的其他深度学习自然语言处理架构不同，transformers架构使用堆叠的注意力层块，它们本质上是具有相同形状的全连接前馈层。
- en: In Chapter 8, you used RNNs to build encoders and decoders to transform text
    sequences. In encoder-decoder (*transcoder* or *transduction*) networks,^([[10](#_footnotedef_10
    "View footnote.")]) the encoder processes each element in the input sequence to
    distill the sentence into a fixed-length thought vector (or *context vector*).
    That thought vector can then be passed on to the decoder where it is used to generate
    a new sequence of tokens.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章，您使用了循环神经网络来构建编码器和解码器以转换文本序列。在编码器-解码器（*转码器*或*传导*）网络中，[[10]](#_footnotedef_10)编码器处理输入序列中的每个元素，将句子提炼成一个固定长度的思想向量（或*上下文向量*）。然后，该思想向量可以传递给解码器，解码器将其用于生成一个新的标记序列。
- en: The encoder-decoder architecture has a big limitation — it can’t handle longer
    texts. If a concept or thought is expressed in multiple sentences or a long complex
    sentence, then the encoded thought vector fails to accurately encapsulate *all*
    of that thought. The *attention mechanism* presented by Bahdanau et al ^([[11](#_footnotedef_11
    "View footnote.")]) to solve this issue is shown to improve sequence-to-sequence
    performance, particularly on long sentences, however it does not alleviate the
    time sequencing complexity of recurrent models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构有一个很大的限制 —— 它无法处理更长的文本。如果一个概念或思想用多个句子或一个复杂的长句表达，那么编码的思想向量就无法准确地概括*所有*这些思想。
    Bahdanau等人提出的*注意机制* ^([[11](#_footnotedef_11 "View footnote.")]) 解决了这个问题，并显示出改善序列到序列性能，特别是对于长句子，但它并不能缓解循环模型的时间序列复杂性。
- en: 'The introduction of the *transformer* architecture in "Attention Is All You
    Need" ^([[12](#_footnotedef_12 "View footnote.")]) propelled language models forward
    and into the public eye. The transformer architecture introduced several synergistic
    features that worked together to achieve as yet impossible performance:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在“Attention Is All You Need”中引入的*transformers*架构推动了语言模型向前发展并进入了公众视野。transformers架构引入了几个协同特性，共同实现了迄今为止不可能的性能：
- en: The most widely recognized innovation in the transformer architecture is *self-attention*.
    Similar to the memory and forgetting gates in a GRU or LSTM, the attention mechanism
    creates connections between concepts and word patterns within a lengthy input
    string.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: transformers架构中最广为人知的创新是*自注意力*。类似于GRU或LSTM中的记忆和遗忘门，注意机制在长输入字符串中创建概念和词模式之间的连接。
- en: In the next few sections, you’ll walk through the fundamental concepts behind
    the transformer and take a look at the architecture of the model. Then you will
    use the base PyTorch implementation of the Transformer module to implement a language
    translation model, as this was the reference task in "Attention Is All You Need",
    to see how it is both powerful and elegant in design.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将学习transformers背后的基本概念，并查看模型的架构。然后，你将使用transformers模块的基本PyTorch实现来实现一个语言翻译模型，因为这是“Attention
    Is All You Need”中的参考任务，看看它在设计上是如何强大而优雅的。
- en: Self-attention
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自注意力
- en: 'When we were writing the first edition of this book, Hannes and Cole (the first
    edition coauthors) were already focused on the attention mechanism. It’s now been
    6 years and attention is still the most researched topic in deep learning. The
    attention mechanism enabled a leap forward in capability for problems where LSTMs
    struggled:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们写第一版这本书时，汉斯和科尔（第一版合著者）已经专注于注意机制。现在已经过去6年了，注意力仍然是深度学习中最研究的话题。注意机制为那些LSTM难以处理的问题的能力提升了一大步：
- en: '*Conversation* — Generate plausible responses to conversational prompts, queries,
    or utterances.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对话* —— 生成对话提示、查询或话语的合理响应。'
- en: '*Abstractive summarization or paraphrasing*:: Generate a new shorter wording
    of a long text summarization of sentences, paragraphs, and even several pages
    of text.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*抽象摘要或释义*:: 生成长文本的新的较短措辞，总结句子、段落，甚至是数页的文本。'
- en: '*Open domain question answering*:: Answering a general question about anything
    the transformer has ever read.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开放域问题回答*:: 回答transformers曾经阅读过的关于任何事物的一般问题。'
- en: '*Reading comprehension question answering*:: Answering questions about a short
    body of text (usually less than a page).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*阅读理解问题回答*:: 回答关于一小段文本（通常少于一页）的问题。'
- en: '*Encoding*:: A single vector or sequence of embedding vectors that represent
    the meaning of body of text in a vector space — sometimes called *task-independent
    sentence embedding*.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码*:: 单个向量或一系列嵌入向量，表示文本内容在向量空间中的含义 —— 有时被称为*任务无关的句子嵌入*。'
- en: '*Translation and code generation* — Generating plausible software expressions
    and programs based on plain English descriptions of the program’s purpose.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*翻译和代码生成* —— 基于纯英文程序描述生成合理的软件表达和程序。'
- en: Self-attention is the most straightforward and common way to implement attention.
    It takes the input sequence of embedding vectors and puts them through linear
    projections. A linear projection is merely a dot product or matrix multiplication.
    This dot product creates key, value and query vectors. The query vector is used
    along with the key vector to create a context vector for the words' embedding
    vectors and their relation to the query. This context vector is then used to get
    a weighted sum of values. In practice, all these operations are done on sets of
    queries, keys, and values packed together in matrices, *Q*, *K*, and *V*, respectively.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是实现注意力的最直接和常见的方法。它接受嵌入向量的输入序列，并将它们通过线性投影处理。线性投影仅仅是点积或矩阵乘法。这个点积创建了键、值和查询向量。查询向量与键向量一起被用来为单词的嵌入向量和它们与查询的关系创建一个上下文向量。然后这个上下文向量被用来得到值的加权和。在实践中，所有这些操作都是在包含在矩阵中的查询、键和值的集合上进行的，分别是*Q*、*K*和*V*。
- en: 'There are two ways to implement the linear algebra of an attention algorithm:
    *additive attention* or *dot-product attention*. The one that was most effective
    in transformers is a scaled version of dot-production attention. For dot-product
    attention, the scalar products between the query vectors *Q* and the key vectors
    *K*, are scaled down based on how many dimensions there are in the model. This
    makes the dot product more numerically stable for large dimensional embeddings
    and longer text sequences. Here’s how you compute the self-attention outputs for
    the query, key, and value matrices *Q*, *K*, and *V*.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实现注意力算法的线性代数有两种方式：*加性注意力*或*点积注意力*。在transformers中效果最好的是点积注意力的缩放版本。对于点积注意力，查询向量*Q*和键向量*K*之间的数量积会根据模型中有多少维度而被缩小。这使得点积对于大尺寸嵌入和长文本序列更加稳定。以下是如何计算查询、键和值矩阵*Q*、*K*和*V*的自注意力输出。
- en: Equation 9.1 Self-attention outputs
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 9.1 自注意力输出
- en: \[Attention(Q, K, V ) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V\]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[Attention(Q, K, V ) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V\]
- en: The high dimensional dot products create small gradients in the softmax due
    to the law of large numbers. To counteract this effect, the product of the query
    and key matrices is scaled by \(\frac{1}{\sqrt{d_{k}}}\). The softmax normalizes
    the resulting vectors so that they are all positive and sum to 1\. This "scoring"
    matrix is then multiplied with the values matrix to get the weighted values matrix
    in figure [9.1](#figure-scaled-dot-product-attention).^([[13](#_footnotedef_13
    "View footnote.")]) ^([[14](#_footnotedef_14 "View footnote.")])
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 高维度点积会导致softmax中的梯度变小，这是由大数定律决定的。为了抵消这种效应，查询和键矩阵的乘积要被\(\frac{1}{\sqrt{d_{k}}}\)缩放。softmax对结果向量进行归一化，使它们都是正数且和为1。这个“打分”矩阵然后与值矩阵相乘，得到图[9.1](#figure-scaled-dot-product-attention)中的加权值矩阵。^([[13](#_footnotedef_13
    "查看脚注。")]) ^([[14](#_footnotedef_14 "查看脚注。")])
- en: Figure 9.1 Scaled dot product attention
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1 缩放点积注意力
- en: '![transformer attention](images/transformer_attention.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![transformer注意力](images/transformer_attention.png)'
- en: Unlike RNNs, where there is recurrence and shared weights, in self-attention
    all of the vectors used in the query, key, and value matrices come from the input
    sequences' embedding vectors. The entire mechanism can be implemented with highly
    optimized matrix multiplication operations. And the *Q* *K* product forms a square
    matrix that can be understood as the connection between words in the input sequence.
    A toy example is shown in figure [9.2](#figure-attention-matrix-illustration).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与RNN不同，在自注意力中，查询、键和值矩阵中使用的所有向量都来自输入序列的嵌入向量。整个机制可以通过高度优化的矩阵乘法操作来实现。而*Q* *K*产品形成一个可以被理解为输入序列中单词之间连接的方阵。图[9.2](#figure-attention-matrix-illustration)中展示了一个玩具例子。
- en: Figure 9.2 Encoder attention matrix as connections between words
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2 作为单词连接的编码器注意力矩阵
- en: '![attention heatmap](images/attention_heatmap.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![注意力热图](images/attention_heatmap.png)'
- en: Multi-Head Self-Attention
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多头自注意力
- en: Multi-head self-attention is an expansion of the self-attention approach to
    creating multiple attention heads that each attend to different aspects of the
    words in a text. So if a token has multiple meanings that are all relevant to
    the interpretation of the input text, they can each be accounted for in the separate
    attention heads. You can think of each attention head as another dimension of
    the encoding vector for a body of text, similar to the additional dimensions of
    an embedding vector for an individual token (see Chapter 6). The query, key, and
    value matrices are multiplied *n* (*n_heads*, the number of attention heads) times
    by each different \(d_q\) , \(d_k\), and \(d_v\) dimension, to compute the total
    attention function output. The *n_heads* value is a hyperparameter of the transformer
    architecture that is typically small, comparable to the number of transformer
    layers in a transformer model. The \(d_v\)-dimensional outputs are concatenated
    and again projected with a \(W^o\) matrix as shown in the next equation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力是将自注意力方法扩展为创建多个注意力头，每个头都关注文本中不同的词方面。因此，如果一个标记具有多个与输入文本解释相关的意义，那么它们可以分别在不同的注意力头中考虑到。你可以将每个注意力头视为文本编码向量的另一个维度，类似于单个标记的嵌入向量的附加维度（见第6章）。查询、键和值矩阵分别由不同的\(d_q\)、\(d_k\)和\(d_v\)维度乘以*n*（*n_heads*，注意力头的数量）次，以计算总的注意力函数输出。*n_heads*值是transformers架构的超参数，通常较小，可与transformers模型中的transformers层数相媲美。\(d_v\)维输出被连接，然后再次使用\(W^o\)矩阵进行投影，如下一个方程所示。
- en: Equation 9.2 Multi-Head self-attention
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式9.2 多头自注意力
- en: \[MultiHeadAttention(Q, K, V ) = Concat(head_1, ..., head_n) W^o\\ where\ head_i
    = Attention(QW_i^Q, KW_i^K, VW_i^V)\]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \[MultiHeadAttention(Q, K, V ) = Concat(head_1, ..., head_n) W^o\\ 其中\ head_i
    = Attention(QW_i^Q, KW_i^K, VW_i^V)\]
- en: The multiple heads allow the model to focus on different positions, not just
    ones centered on a single word. This effectively creates several different vector
    subspaces where the transformer can encode a particular generalization for a subset
    of the word patterns in your text. In the original transformers paper, the model
    uses *n*=8 attention heads such that \(d_k = d_v = \frac{d_{model}}{n} = 64\).
    The reduced dimensionality in the multi-head setup is to ensure the computation
    and concatenation cost is nearly equivalent to the size of a full-dimensional
    single-attention head.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 多个头使得模型能够关注不同位置，而不仅仅是以单个词为中心的位置。这有效地创建了几个不同的向量子空间，其中transformers可以为文本中的词模式子集编码特定的泛化。在原始transformers论文中，模型使用*n*=8个注意力头，使得\(d_k
    = d_v = \frac{d_{model}}{n} = 64\)。多头设置中的降维是为了确保计算和连接成本几乎等同于完整维度的单个注意力头的大小。
- en: If you look closely you’ll see that the attention matrices (attention heads)
    created by the product of *Q* and *K* all have the same shape, and they are all
    square (same number of rows as columns). This means that the attention matrix
    merely rotates the input sequence of embeddings into a new sequence of embeddings,
    without affecting the shape or magnitude of the embeddings. And this makes it
    possible to explain a bit about what the attention matrix is doing for a particular
    example input text.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，你会发现由*Q*和*K*的乘积创建的注意力矩阵（注意力头）都具有相同的形状，它们都是方阵（行数与列数相同）。这意味着注意力矩阵仅将嵌入的输入序列旋转为新的嵌入序列，而不影响嵌入的形状或大小。这使得能够解释注意力矩阵对特定示例输入文本的作用。
- en: Figure 9.3 Multi-Head Self-Attention
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3 多头自注意力
- en: '![multi head attention drawio](images/multi-head-attention_drawio.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![多头注意力绘图](images/multi-head-attention_drawio.png)'
- en: It turns out, the multi-head attention mechanism is just a fully connected linear
    layer under the hood. After all is said and done, the deepest of the deep learning
    models turned to be nothing more than a clever stacking of what is essentially
    linear and logistic regressions. This is why it was so surprising that transformers
    were so successful. And this is why it was so important for you to understand
    the basics of linear and logistic regression described in earlier chapters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，多头注意力机制实际上只是一个全连接的线性层。毕竟，最深层的深度学习模型实质上只是线性和逻辑回归的巧妙堆叠。这就是为什么transformers如此成功令人惊讶的原因。这也是为什么理解前几章描述的线性和逻辑回归的基础知识是如此重要的原因。
- en: 9.2 Filling the attention gaps
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 填充注意力空白
- en: The attention mechanism compensates for some problems with RNNs and CNNs of
    previous chapters but creates some additional challenges. Encoder-decoders based
    on RNNs don’t work very well for longer passages of text where related word patterns
    are far apart. Even long sentences are a challenge for RNNs doing translation.^([[15](#_footnotedef_15
    "View footnote.")]) And the attention mechanism compensates for this by allowing
    a language model to pick up important concepts at the beginning of a text and
    connect them to text that is towards the end. The attention mechanism gives the
    transformer a way to reach back to any word it has ever seen. Unfortunately, adding
    the attention mechanism forces you to remove all recurrence from the transformer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制弥补了前几章的 RNN 和 CNN 存在的一些问题，但也带来了一些额外的挑战。基于 RNN 的编码器-解码器在处理较长的文本段落时效果不佳，其中相关的单词模式相距甚远。即使是长句对于进行翻译的
    RNN 来说也是一个挑战。[^15] 注意机制通过允许语言模型在文本开头捕捉重要概念并将其连接到文本末尾的文本来弥补了这一点。注意机制使得transformers能够回溯到它曾经见过的任何单词。不幸的是，添加注意机制会迫使你从transformers中删除所有的循环。
- en: CNNs are another way to connect concepts that are far apart in the input text.
    A CNN can do this by creating a hierarchy of convolution layers that progressively
    "necks down" the encoding of the information within the text it is processing.
    And this hierarchical structure means that a CNN has information about the large-scale
    position of patterns within a long text document. Unfortunately, the outputs and
    the inputs of a convolution layer usually have different shapes. So CNNs are not
    stackable, making them tricky to scale up for greater capacity and larger training
    datasets. So to give a transformer the uniform data structure it needs for stackability,
    transformers use byte pair encoding and positional encoding to spread the semantic
    and position information uniformly across the encoding tensor.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是连接输入文本中相距甚远概念的另一种方法。CNN 可以通过创建一系列逐渐“缩颈”文本信息编码的卷积层来实现这一点。这种分层结构意味着 CNN 具有有关长文本文档中模式的大规模位置的信息。不幸的是，卷积层的输出和输入通常具有不同的形状。因此，CNN
    不可叠加，这使得它们难以扩展以处理更大容量和更大训练数据集。因此，为了给transformers提供其需要的用于可堆叠的统一数据结构，transformers使用字节对编码和位置编码来在编码张量中均匀传播语义和位置信息。
- en: 9.2.1 Positional encoding
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 位置编码
- en: 'Word order in the input text matters, so you need a way to bake in some positional
    information into the sequence of embeddings that is passed along between layers
    in a transformer. A positional encoding is simply a function that adds information
    about the relative or absolute position of a word in a sequence to the input embeddings.
    The encodings have the same dimension, \(d_{model}\), as the input embeddings
    so they can be summed with the embedding vectors. The paper discusses learned
    and fixed encodings and proposes a sinusoidal function of sin and cosine with
    different frequencies, defined as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本中的单词顺序很重要，因此你需要一种方法将一些位置信息嵌入到在transformers的各层之间传递的嵌入序列中。位置编码简单地是一个函数，它将一个单词在序列中的相对或绝对位置的信息添加到输入嵌入中。编码具有与输入嵌入相同的维度\(d_{model}\)，因此它们可以与嵌入向量相加。论文讨论了学习的和固定的编码，并提出了一个以正弦和余弦为基础的正弦函数，具有不同的频率，定义为：
- en: Equation 9.3 Positional encoding function
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 9.3 位置编码函数
- en: \[PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\ PE_{(pos,
    2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: \[PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\ PE_{(pos,
    2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]
- en: This mapping function was chosen because for any offset *k*, \(PE_{(pos+k)}\)
    can be represented as a linear function of \(PE_{pos}\). In short, the model should
    be able to learn to attend to relative positions easily.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这个映射函数是因为对于任何偏移 *k*，\(PE_{(pos+k)}\) 可以表示为 \(PE_{pos}\) 的线性函数。简而言之，模型应该能够轻松地学会关注相对位置。
- en: 'Let’s look at how this can be coded in Pytorch. The official Pytorch Sequence-to-Sequence
    Modeling with `nn.Transformer` tutorial ^([[16](#_footnotedef_16 "View footnote.")])
    provides an implementation of a PositionEncoding nn.Module based on the previous
    function:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这如何在 Pytorch 中编码。官方 Pytorch 序列到序列建模教程提供了基于前述函数的 PositionEncoding nn.Module
    的实现：
- en: Listing 9.1 Pytorch PositionalEncoding
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.1 Pytorch 位置编码
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will use this module in the translation transformer you build. However,
    first, we need to fill in the remaining details of the model to complete your
    understanding of the architecture.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在构建的翻译transformers中使用此模块。但是，首先，我们需要填充模型的其余细节，以完善您对架构的理解。
- en: 9.2.2 Connecting all the pieces
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 连接所有部分
- en: Now that you’ve seen the hows and whys of BPE, embeddings, positional encoding,
    and multi-head self-attention, you understand all the elements of a transformer
    layer. You just need a lower dimensional linear layer at the output to collect
    all those attention weights together to create the output sequence of embeddings.
    And the linear layer output needs to be scaled (normalized) so that the layers
    all have the same scale. These linear and normalization layers are stacked on
    top of the attention layers to create reusable stackable transformer blocks as
    shown in figure [9.4](#figure-transformer-architecture).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了BPE、嵌入、位置编码和多头自注意力的原因和方法，你理解了transformers层的所有要素。你只需要在输出端添加一个较低维度的线性层，将所有这些注意力权重收集在一起，以创建嵌入的输出序列。线性层的输出需要进行缩放（归一化），以使所有层具有相同的尺度。这些线性和归一化层堆叠在注意力层之上，以创建可重复使用的可堆叠transformers块，如图
    [9.4](#figure-transformer-architecture) 所示。
- en: Figure 9.4 Transformer architecture
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.4 transformers架构
- en: '![transformer original](images/transformer_original.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![transformers原理图](images/transformer_original.png)'
- en: In the original transformer, both the encoder and decoder are comprised of *N*
    = 6 stacked identical encoder and decoder layers, respectively.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始transformers中，编码器和解码器均由*N* = 6个堆叠的相同编码器和解码器层组成。
- en: Encoder
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The encoder is composed of multiple encoder layers. Each encoder layer has
    two sub-layers: a multi-head attention layer and a position-wise fully connected
    feedforward network. A residual connection is made around each sub-layer. And
    each encoder layer has its output normalized so that all the values of the encodings
    passed between layers range between zero and one. The outputs of all sub-layers
    in a transformer layer (PyTorch module) that are passed between layers all have
    dimension \(d_{model}\). And the input embedding sequences to the encoder are
    summed with the positional encodings before being input into the encoder.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由多个编码器层组成。每个编码器层有两个子层：一个多头注意力层和一个位置感知的全连接前馈网络。在每个子层周围都有一个残差连接。每个编码器层的输出都被归一化，以使所有层之间传递的编码值的范围在零和一之间。传递给编码器的输入嵌入序列在输入编码器之前与位置编码相加。
- en: Decoder
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder is nearly identical to the encoder in the model but has three sublayers
    instead of one. The new sublayer is a fully connected layer similar to the multi-head
    self-attention matrix but contains only zeros and ones. This creates a *masking*
    of the output sequences that are to the right of the current target token (in
    a left-to-right language like English). This ensures that predictions for position
    *i* can depend only on previous outputs, for positions less than *i*. In other
    words, during training, the attention matrix is not allowed to "peek ahead" at
    the subsequent tokens that it is supposed to be generating in order to minimize
    the loss function. This prevents *leakage* or "cheating" during training, forcing
    the transformer to attend only to the tokens it has already seen or generated.
    Masks are not required within the decoders for an RNN, because each token is only
    revealed to the network one at a time. But transformer attention matrices have
    access to the entire sequence all at once during training.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与模型中的编码器几乎相同，但子层数量为三而不是一个。新的子层是一个完全连接的层，类似于多头自注意力矩阵，但只包含零和一。这会创建一个*掩码*，用于右边的当前目标令牌的输出序列（在英语等从左到右的语言中）。这确保了对于位置*i*的预测只能依赖于先前的输出，对于小于*i*的位置。换句话说，在训练期间，注意力矩阵不允许“偷看”它应该生成的后续令牌，以最小化损失函数。这样可以防止在训练过程中出现*泄漏*或“作弊”，强制transformers只关注它已经看到或生成的令牌。在RNN中，解码器内部不需要掩码，因为每个令牌在训练过程中只向网络逐个显示。但是，在训练期间，transformers注意力矩阵可以一次性访问整个序列。
- en: Figure 9.5 Connections between encoder and decoder layers
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.5 编码器和解码器层之间的连接
- en: '![encoder decoder drawio](images/encoder_decoder_drawio.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![编码器解码器示意图](images/encoder_decoder_drawio.png)'
- en: 9.2.3 Transformer Translation Example
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 transformers翻译示例
- en: Transformers are suited for many tasks. The "Attention Is All You Need" paper
    showed off a transformer that achieved better translation accuracy than any preceding
    approach. Using `torchtext`, you will prepare the Multi30k dataset for training
    a Transformer for German-English translation using the `torch.nn.Transformer`
    module. In this section, you will customize the decoder half of the `Transformer`
    class to output the self-attention weights for each sublayer. You use the matrix
    of self-attention weights to explain how the words in the input German text were
    combined together to create the embeddings used to produce the English text in
    the output. After training the model you will use it for inference on a test set
    to see for yourself how well it translates German text into English.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: transformers适用于许多任务。《注意力就是你的一切》论文展示了一个transformers，其翻译精度优于任何之前的方法。使用 `torchtext`，你将准备 Multi30k
    数据集，用于训练一个德语-英语翻译的 Transformer，使用 `torch.nn.Transformer` 模块。在本节中，你将自定义 `Transformer`
    类的解码器部分，以输出每个子层的自注意力权重。你使用自注意力权重矩阵来解释输入德语文本中的单词是如何组合在一起的，以生成输出的英语文本中使用的嵌入。训练模型后，你将在测试集上使用它进行推理，以查看它将德语文本翻译成英语的效果如何。
- en: Preparing the Data
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'You can use the Hugging Face datasets package to simplify bookkeeping and ensure
    your text is fed into the Transformer in a predictable format compatible with
    PyTorch. This is one of the trickiest parts of any deep learning project, ensuring
    that the structure and API for your dataset matches what your PyTorch training
    loop expects. Translation datasets are particularly tricky unless you use Hugging
    Face:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Hugging Face 数据集包来简化记录工作，并确保你的文本以与 PyTorch 兼容的可预测格式输入到 Transformer 中。这是任何深度学习项目中最棘手的部分之一，确保你的数据集的结构和
    API 与你的 PyTorch 训练循环所期望的相匹配。翻译数据集特别棘手，除非你使用 Hugging Face：
- en: Listing 9.2 Load a translation dataset in Hugging Face format
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2 在 Hugging Face 格式中加载翻译数据集
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Not all Hugging Face datasets have predefined test and validation splits of
    the data. But you can always create your own splits using the `train_test_split`
    method as in listing [9.3](#listing-translation-dataset-split).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是所有的 Hugging Face 数据集都有预定义的测试和验证数据集拆分。但你可以像列表 [9.3](#listing-translation-dataset-split)
    中所示的那样，使用 `train_test_split` 方法创建自己的拆分。
- en: Listing 9.3 Load a translation dataset in Hugging Face format
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.3 在 Hugging Face 格式中加载翻译数据集
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It’s always a good idea to examine some examples in your dataset before you
    start a long training run. This can help you make sure the data is what you expect.
    The `opus_books` doesn’t contain many books. So it’s not a very diverse (representative)
    sample of German. It has been segmented into only 50,000 aligned sentence pairs.
    Imagine having to learn German by having only a few translated books to read.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始长时间的训练之前，检查数据集中的一些示例总是一个好主意。这可以帮助你确保数据符合你的预期。`opus_books` 并不包含很多书籍。所以它不是很多样化（代表性）的德语样本。它只被分成了
    50,000 对齐的句子对。想象一下只有几本翻译过的书籍可供阅读时学习德语是什么感觉。
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you would like to use a custom dataset of your own creation, it’s always
    a good idea to comply with an open standard like the Hugging Face datasets package
    shown in listing [9.2](#listing-hugging-face-translation-datasets) gives you a
    "best practice" approach to structuring your datasets. Notice that a translation
    dataset in Hugging Face contains an array of paired sentences with the language
    code in a dictionary. The `dict` keys of a translation example are the two-letter
    language code (from ISO 639-2)^([[17](#_footnotedef_17 "View footnote.")]). The
    `dict` values of an example text are the sentences in each of the two languages
    in the dataset.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用自己创建的自定义数据集，遵循像 Hugging Face 数据集包中所示的开放标准总是一个好主意，它给出了一个“最佳实践”的数据集结构方法。注意，在
    Hugging Face 中的翻译数据集包含一个带有语言代码的句子对数组和一个字典。翻译示例的 `dict` 键是两字母语言代码（来自 ISO 639-2）^([[17](#_footnotedef_17
    "View footnote.")])。示例文本的 `dict` 值是数据集中每种语言中的句子。
- en: Tip
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: You’ll avoid insidious, sometimes undetectable bugs if you resist the urge to
    invent your own data structure and instead use widely recognized open standards.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你抵制了发明自己的数据结构的冲动，而是使用广泛认可的开放标准，你就能避免一些隐蔽的、有时无法检测到的错误。
- en: If you have access to a GPU, you probably want to use it for training transformers.
    Transformers are made for GPUs with their matrix multiplication operations for
    all the most computationally intensive parts of the algorithm. CPUs are adequate
    for most pre-trained Transformer models (except LLMs), but GPUs can save you a
    lot of time for training or fine-tuning a transformer. For example, GPT2 required
    3 days to train with a relatively small (40 MB) training dataset on a 16-core
    CPU. It trained in 2 hours for the same dataset on a 2560-core GPU (40x speedup,
    160x more cores). Listing [9.4](#listing-torch-gpu) will enable your GPU if one
    is available.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有GPU，你可能想要用它来训练Transformer。Transformer由于其矩阵乘法运算而适用于GPU，GPU可用于算法中所有最计算密集的部分。对于大多数预训练的Transformer模型（除了LLM），CPU是足够的，但GPU可以节省大量用于训练或微调Transformer的时间。例如，GPT2在16核CPU上使用相对较小（40
    MB）的训练数据集训练了3天。在2560核GPU上，相同数据集训练时间为2小时（速度提升40倍，核心数增加160倍）。[9.4节](#listing-torch-gpu)将启用你的GPU（如果有）。
- en: Listing 9.4 Enable any available GPU
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第9.4节 启用任何可用的GPU
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To keep things simple you can tokenize your source and target language texts
    separately with specialized tokenizers for each. If you use the Hugging Face tokenizers
    they will keep track of all of the special tokens that you’ll need for a transformer
    to work on almost any machine learning task:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化操作，你可以分别使用专门的分词器对源语言文本和目标语言文本进行标记化处理。如果你使用Hugging Face的分词器，它们将跟踪你几乎任何机器学习任务中需要的所有特殊标记：
- en: '**start-of-sequence token**::typically `"<SOS>"` or `"<s>"`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列开始标记**::通常为`"<SOS>"`或者`"<s>"`'
- en: '**end-of-sequence token**::typically `"<EOS>"` or `"</s>"`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列结束标记**::通常为`"<EOS>"`或`"</s>"`'
- en: '**out-of-vocabulary (unknown) token**::typically `"<OOV>"`, `"<unk>"`'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未知词（out-of-vocabulary）标记**::通常为`"<OOV>"`，`"<unk>"`'
- en: '**mask token**::typically `"<mask>"`'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**屏蔽标记**::通常为`"<mask>"`'
- en: '**padding token**::typically `"<pad>"`'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充标记**::通常为`"<pad>"`'
- en: The *start-of-sequence token* is used to trigger the decoder to generate a token
    that is suitable for the first token in a sequence. And many generative problems
    will require you to have an *end-of-sequence token*, so that the decoder knows
    when it can stop recursively generating more tokens. Some datasets use the same
    token for both the *start-of-sequence* and the *end-of-sequence* marker. They
    do not need to be unique because your decoder will always "know" when it is starting
    a new generation loop. The padding token is used to fill in the sequence at the
    end for examples shorter than the maximum sequence length. The mask token is used
    to intentionally hide a known token for training task-independent encoders such
    as BERT. This is similar to what you did in Chapter 6 for training word embeddings
    using skip grams.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*序列开始标记*用于触发解码器生成适合序列中第一个标记的标记。许多生成性问题将需要你有一个*序列结束标记*，这样解码器就知道何时可以停止递归生成更多标记。一些数据集使用相同的标记表示*序列开始*和*序列结束*。它们不需要是唯一的，因为你的解码器始终会“知道”何时开始新的生成循环。填充标记用于在示例短于最大序列长度时填充序列末尾。屏蔽标记用于故意隐藏已知标记，以用于训练诸如BERT之类的任务无关编码器。这类似于第6章使用跳字训练词嵌入时所做的操作。'
- en: You can choose any tokens for these marker (special) tokens, but you want to
    make sure that they are not words used within the vocabulary of your dataset.
    So if you are writing a book about natural language processing and you don’t want
    your tokenizer to trip up on the example SOS and EOS tokens, you may need to get
    a little more creative to generate tokens not found in your text.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择任何标记作为这些标记（特殊标记），但你需要确保它们不是数据集词汇表中使用的词汇。因此，如果你正在撰写一本关于自然语言处理的书，并且你不希望你的分词器在示例SOS和EOS标记上出现问题，你可能需要更有创意地生成文本中找不到的标记。
- en: Create a separate Hugging Face tokenizer for each language to speed up your
    tokenization and training and avoid having tokens leak from your source language
    text examples into your generated target language texts. You can use any language
    pair you like, but the original AIAYN paper demo examples usually translate from
    English (source) to German (target).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快标记化和训练速度，并避免标记从源语言文本示例泄漏到生成的目标语言文本中，你可以为每种语言创建一个单独的Hugging Face分词器。你可以选择任何语言对，但原始的AIAYN论文演示例通常是从英语（源语言）到德语（目标语言）的翻译。
- en: '[PRE5]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `ByteLevel` part of your BPE tokenizer ensures that your tokenizer will
    never miss a beat (or byte) as it is tokenizing your text. A byte-level BPE tokenizer
    can always construct any character by combining one of the 256 possible single-byte
    tokens available in its vocabulary. This means it can process any language that
    uses the Unicode character set. A byte-level tokenizer will just fall back to
    representing the individual bytes of a Unicode character if it hasn’t seen it
    before or hasn’t included it in its token vocabulary. A byte-level tokenizer will
    need an average of 70% more tokens (almost double the vocabulary size) to represent
    a new text containing characters or tokens that it hasn’t been trained on.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 BPE 分词器中的 `ByteLevel` 部分确保您的分词器在对文本进行分词时永远不会漏掉任何一个字节。字节级别 BPE 分词器始终可以通过组合其词汇表中提供的
    256 个可能的单字节令牌之一来构建任何字符。这意味着它可以处理任何使用 Unicode 字符集的语言。字节级别分词器将会简单地回退到表示 Unicode
    字符的各个字节，如果它以前没有见过或者没有将其包含在其令牌词汇表中。字节级别分词器将需要平均增加 70% 的令牌数量（词汇表大小几乎翻倍）来表示包含它未曾训练过的字符或令牌的新文本。
- en: Character-level BPE tokenizers have their disadvantages too. A character-level
    tokenizer must hold each one of the multibyte Unicode characters in its vocabulary
    to avoid having any meaningless OOV (out-of-vocabulary) tokens. This can create
    a huge vocabulary for a multilingual transformer expected to handle most of the
    161 languages covered by Unicode characters. There are 149,186 characters with
    Unicode code points for both historical (Egyptian hieroglyphs for example) and
    modern written languages. That’s about 10 times the memory to store all the embeddings
    and tokens in your transformer’s tokenizer. In the real world, it is usually practical
    to ignore historical languages and some rare modern languages when optimizing
    your transformer BPE tokenizer for memory and balancing that with your transformer’s
    accuracy for your problem.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 字符级别 BPE 分词器也有其缺点。字符级别分词器必须将每个多字节 Unicode 字符都包含在其词汇表中，以避免出现任何无意义的 OOV（词汇外）标记。对于预期处理大部分
    Unicode 字符涵盖的 161 种语言的多语言transformers，这可能会创建一个巨大的词汇表。Unicode 代码点有 149,186 个字符，用于历史（例如埃及象形文字）和现代书面语言。这大约是存储transformers分词器中所有嵌入和令牌所需的内存的
    10 倍。在实际应用中，通常会忽略历史语言和一些罕见的现代语言，以优化transformers BPE 分词器的内存使用，并将其与您的问题的transformers准确性平衡。
- en: Important
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: The BPE tokenizer is one of the five key "superpowers" of transformers that
    makes them so effective. And a `ByteLevel` BPE tokenizer isn’t quite as effective
    at representing the meaning of words even though it will never have OOV tokens.
    So in a production application, you may want to train your pipeline on both a
    character-level BPE tokenizer as well as a byte-level tokenizer. That way you
    can compare the results and choose the approach that gives you the best performance
    (accuracy and speed) for *your* application.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 分词器是transformers的五个关键“超级能力”之一，使它们如此有效。而 `ByteLevel` BPE 分词器虽然永远不会有 OOV（Out Of Vocabulary）标记，但在表示单词含义方面并不像预期的那样有效。因此，在生产应用中，您可能希望同时训练管道使用字符级别
    BPE 分词器和字节级分词器。这样，您就可以比较结果，并选择为*您的*应用提供最佳性能（准确性和速度）的方法。
- en: You can use your English tokenizer to build a preprocessing function that *flattens*
    the `Dataset` structure and returns a list of lists of token IDs (without padding).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用英文分词器构建一个预处理函数，用于 *展平* `Dataset` 结构并返回不带填充的标记 ID 列表的列表。
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: TranslationTransformer Model
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 翻译transformers模型
- en: At this point, you have tokenized the sentences in the Multi30k data and converted
    them to tensors consisting of indexes into the vocabularies for the source and
    target languages, German and English, respectively. The dataset has been split
    into separate training, validation and test sets, which you have wrapped with
    iterators for batch training. Now that the data is prepared you turn your focus
    to setting up the model. Pytorch provides an implementation of the model presented
    in "Attention Is All You Need", `torch.nn.Transformer`. You will notice the constructor
    takes several parameters, familiar amongst them are `d_model=512`, `nhead=8`,
    `num_encoder_layers=6`, and `num_decoder_layers=6`. The default values are set
    to the parameters employed in the paper. Along with several other parameters for
    the feedforward dimension, dropout, and activation, the model also provides support
    for a `custom_encoder` and `custom_decoder`. To make things interesting, create
    a custom decoder that additionally outputs a list of attention weights from the
    multi-head self-attention layer in each sublayer of the decoder. It might sound
    complicated, but it’s actually fairly straightforward if you simply subclass `torch.nn.TransformerDecoderLayer`
    and `torch.nn.TransformerDecoder` and augment the *forward()* methods to return
    the auxiliary outputs - the attention weights.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经对 Multi30k 数据中的句子进行了标记化，并将其转换为了分别对应源语言和目标语言（德语和英语）词汇表的索引张量。数据集已经被拆分为独立的训练、验证和测试集，并且已经用批量训练的迭代器进行了包装。现在数据已经准备好了，你需要将注意力转移到设置模型上。Pytorch
    提供了 "Attention Is All You Need" 中提出的模型实现，`torch.nn.Transformer`。你会注意到构造函数接受几个参数，其中一些是很熟悉的，比如
    `d_model=512`、`nhead=8`、`num_encoder_layers=6` 和 `num_decoder_layers=6`。默认值设置为论文中使用的参数。除了用于前馈维度、丢弃和激活的几个参数之外，该模型还支持
    `custom_encoder` 和 `custom_decoder`。为了让事情变得有趣起来，创建一个自定义解码器，除了输出每个子层中的多头自注意力层的注意力权重外，还可以创建一个具有辅助输出的
    *forward()* 方法 - 注意力权重的列表。
- en: Listing 9.5 Extend torch.nn.TransformerDecoderLayer to additionally return multi-head
    self-attention weights
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.5 将 torch.nn.TransformerDecoderLayer 扩展为额外返回多头自注意力权重
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Listing 9.6 Extend torch.nn.TransformerDecoder to additionally return list of
    multi-head self-attention weights
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.6 将 torch.nn.TransformerDecoder 扩展为额外返回多头自注意力权重列表
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The only change to `.forward()` from the parent’s version is to cache weights
    in the list member variable, `attention_weights`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与父类版本的 `.forward()` 唯一的改变就是将权重缓存在列表成员变量 `attention_weights` 中。
- en: To recap, you have extended the `torch.nn.TransformerDecoder` and its sublayer
    component, `torch.nn.TransformerDecoderLayer`, mainly for exploratory purposes.
    That is, you save the multi-head self-attention weights from the different decoder
    layers in the Transformer model you are about to configure and train. The *forward()*
    methods in each of these classes copy the one in the parent nearly verbatim, with
    the exception of the changes called out to save the attention weights.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回顾一下，你已经对 `torch.nn.TransformerDecoder` 及其子层组件 `torch.nn.TransformerDecoderLayer`
    进行了扩展，主要是出于探索性的目的。也就是说，你保存了将要配置和训练的 Transformer 模型中不同解码器层的多头自注意力权重。这些类中的 *forward()*
    方法几乎与父类一模一样，只是在保存注意力权重时做了一些不同的改动。
- en: The `torch.nn.Transformer` is a somewhat bare-bones version of the sequence-to-sequence
    model containing the main secret sauce, the multi-head self-attention in both
    the encoder and decoder. If one looks at the source code for the module ^([[18](#_footnotedef_18
    "View footnote.")]), the model does not assume the use of embedding layers or
    positional encodings. Now you will create your *TranslationTransformer* model
    that uses the custom decoder components, by extending `torch.nn.Transformer` module.
    Begin with defining the constructor, which takes parameters `src_vocab_size` for
    a source embedding size, and `tgt_vocab_size` for the target, and uses them to
    initialize a basic `torch.nn.Embedding` for each. Notice a `PositionalEncoding`
    member, `pos_enc`, is created in the constructor for adding the word location
    information.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Transformer`是一个相对简单的序列到序列模型，其中包含主要的秘密武器，即编码器和解码器中的多头自注意力。如果查看该模块的源代码
    ^([[18](#_footnotedef_18 "查看脚注。")])，则该模型不假设使用嵌入层或位置编码。现在，您将创建使用自定义解码器组件的*TranslationTransformer*模型，通过扩展`torch.nn.Transformer`模块。首先定义构造函数，它接受`src_vocab_size`用于源嵌入大小的参数，以及`tgt_vocab_size`用于目标的参数，并使用它们初始化基本的`torch.nn.Embedding`。注意，在构造函数中创建了一个`PositionalEncoding`成员变量，`pos_enc`，用于添加单词位置信息。'
- en: Listing 9.7 Extend nn.Transformer for translation with a CustomDecoder
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.7 扩展 nn.Transformer 以使用 CustomDecoder 进行翻译
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note the import of `rearrange` from the `einops` ^([[19](#_footnotedef_19 "View
    footnote.")]) package. Mathematicians like it for tensor reshaping and shuffling
    because it uses a syntax common in graduate level applied math courses. To see
    why you need to `rearrange()` your tensors refer to the `torch.nn.Transformer`
    documentation ^([[20](#_footnotedef_20 "View footnote.")]). If you get any one
    of the dimensions of any of the tensors wrong it will mess up the entire pipeline,
    sometimes invisibly.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意从`einops` ^([[19](#_footnotedef_19 "查看脚注。")])包导入`rearrange`的重要性。数学家喜欢它用于张量重塑和洗牌，因为它使用了研究生级别应用数学课程中常见的语法。要了解为什么需要`rearrange()`你的张量，请参阅`torch.nn.Transformer`文档
    ^([[20](#_footnotedef_20 "查看脚注。")])。如果您把任何张量的任何维度都弄错了，它将破坏整个管道，有时会无形中出现问题。
- en: Listing 9.8 torch.nn.Transformer "shape" and dimension descriptions
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.8 torch.nn.Transformer的“形状”和维度描述
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The datasets you created using `torchtext` are batch-first. So, borrowing the
    nomenclature in the Transformer documentation, your source and target tensors
    have shape *(N, S)* and *(N, T)*, respectively. To feed them to the `torch.nn.Transformer`
    (i.e. call its `forward()` method), the source and target must be reshaped. Also,
    you want to apply the embeddings plus the positional encoding to the source and
    target sequences. Additionally, a *padding key mask* is needed for each and a
    *memory key mask* is required for the target. Note, you can manage the embeddings
    and positional encodings outside the class, in the training and inference sections
    of the pipeline. However, since the model is specifically set up for translation,
    you make a stylistic/design choice to encapsulate the source and target sequence
    preparation within the class. To this end, you define `prepare_src()` and `prepare_tgt()`
    methods for preparing the sequences and generating the required masks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`torchtext`创建的数据集是批量优先的。因此，借用Transformer文档中的术语，您的源和目标张量分别具有形状*(N, S)*和*(N,
    T)*。要将它们馈送到`torch.nn.Transformer`（即调用其`forward()`方法），需要对源和目标进行重塑。此外，您希望对源和目标序列应用嵌入加上位置编码。此外，每个都需要一个*填充键掩码*，目标需要一个*内存键掩码*。请注意，您可以在类的外部管理嵌入和位置编码，在管道的培训和推理部分。但是，由于模型专门用于翻译，您选择在类内封装源和目标序列准备。为此，您定义了用于准备序列和生成所需掩码的`prepare_src()`和`prepare_tgt()`方法。
- en: Listing 9.9 TranslationTransformer prepare_src()
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.9 TranslationTransformer prepare_src()
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `make_key_padding_mask()` method returns a tensor set to 1’s in the position
    of the padding token in the given tensor, and zero otherwise. The `prepare_src()`
    method generates the padding mask and then rearranges the `src` to the shape that
    the model expects. It then applies the positional encoding to the source embedding
    multiplied by the square root of the model’s dimension. This is taken directly
    from "Attention Is All You Need". The method returns the `src` with positional
    encoding applied, and the key padding mask for it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_key_padding_mask()`方法返回一个张量，在给定张量中填充标记的位置设置为1，否则为零。`prepare_src()`方法生成填充蒙版，然后将`src`重新排列为模型期望的形状。然后，它将位置编码应用于源嵌入，乘以模型维度的平方根。这直接来自于“注意力机制都是你需要的”。该方法返回应用了位置编码的`src`，以及适用于它的键填充蒙版。'
- en: The `prepare_tgt()` method used for the target sequence is nearly identical
    to `prepare_src()`. It returns the `tgt` adjusted for positional encodings, and
    a target key padding mask. However, it also returns a "subsequent" mask, `tgt_mask`,
    which is a triangular matrix for which columns (ones) in a row that are permitted
    to be observed. To generate the subsequent mask you use `Transformer.generate_square_subsequent_mask()`
    method defined in the base class as shown in the following listing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 用于目标序列的`prepare_tgt()`方法几乎与`prepare_src()`相同。它返回已调整位置编码的`tgt`，以及目标键填充蒙版。但是，它还返回一个“后续”蒙版，`tgt_mask`，它是一个三角形矩阵，用于允许观察的一行中的列（1）。要生成后续蒙版，你可以使用基类中定义的`Transformer.generate_square_subsequent_mask()`方法，如下清单所示。
- en: Listing 9.10 TranslationTransformer prepare_tgt()
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.10 TranslationTransformer prepare_tgt()
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You put `prepare_src()` and `prepare_tgt()` to use in the model’s `forward()`
    method. After preparing the inputs, it simply invokes the parent’s `forward()`
    and feeds the outputs through a Linear reduction layer after transforming from
    (T, N, E) back to batch first (N, T, E). We do this for consistency in our training
    and inference.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你在模型的`forward()`方法中使用`prepare_src()`和`prepare_tgt()`。在准备好输入后，它只是调用父类的`forward()`，并在从（T，N，E）转换回批量优先（N，T，E）后，将输出馈送到线性缩减层。我们这样做是为了保持训练和推断的一致性。
- en: Listing 9.11 TranslationTransformer forward()
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.11 TranslationTransformer forward()
- en: '[PRE13]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Also, define an `init_weights()` method that can be called to initialize the
    weights of all submodules of the Transformer. Xavier initialization is commonly
    used for Transformers, so use it here. The Pytorch `nn.Module` documentation ^([[21](#_footnotedef_21
    "View footnote.")]) describes the `apply(fn)` method that recursively applies
    `fn` to every submodule of the caller.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，定义一个`init_weights()`方法，可调用来初始化 Transformer 的所有子模块的权重。在 Transformer 中常用 Xavier
    初始化，因此在这里使用它。Pytorch 的`nn.Module`文档^([[21](#_footnotedef_21 "View footnote.")])描述了`apply(fn)`方法，该方法递归地将`fn`应用到调用者的每个子模块上。
- en: Listing 9.12 TranslationTransformer init_weights()
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.12 TranslationTransformer init_weights()
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The individual components of the model have been defined and the complete model
    is shown in the next listing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的各个组件已经定义好了，完整的模型在下一个清单中展示。
- en: Listing 9.13 TranslationTransformer complete model definition
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.13 TranslationTransformer 完整模型定义
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Finally, you have a complete transformer all your own! And you should be able
    to use it for translating between virtually any pair of languages, even character-rich
    languages such as traditional Chinese and Japanese. And you have explicit access
    to all the hyperparameters that you might need to tune your model for your problem.
    For example, you can increase the vocabulary size for the target or source languages
    to efficiently handle *character-rich* languages such as traditional Chinese and
    Japanese.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你拥有了一个完整的 Transformer！你应该能够用它来在几乎任何一对语言之间进行翻译，甚至包括像传统中文和日语这样字符丰富的语言。你可以明确地访问所有你可能需要调整模型以解决问题的超参数。例如，你可以增加目标语言或源语言的词汇量，以有效处理*字符丰富*的语言，比如传统中文和日语。
- en: Note
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注
- en: Traditional Chinese and Japanese (kanji) are called *character-rich* because
    they have a much larger number of unique characters that European languages. Chinese
    and Japanese languages use logograph characters. Logograph characters look a bit
    like small pictographs or abstract hieroglyphic drawings. For example, the kanji
    character "日" can mean day and it looks a little like the day block you might
    see on a calendar. Japanese logographic characters are roughly equivalent to word
    pieces somewhere between morphemes and words in the English language. This means
    that you will have many more unique characters in logographic languages than in
    European languages. For instance, traditional Japanese uses about 3500 unique
    kanji characters.^([[22](#_footnotedef_22 "View footnote.")]) English has roughly
    7000 unique syllables within the most common 20,000 words.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于中文和日语（汉字）拥有比欧洲语言更多的独特字符，所以它们被称为*字符丰富*。中文和日语使用形码字符。形码字符看起来有点像小的象形文字或抽象的象形图。例如，汉字字符"日"可以表示"天"，它看起来有点像日历上可能看到的日期方块。日语形码字符在英语中大致相当于形态素和词之间的词素。这意味着在形码语言中，您将有比欧洲语言更多的独特字符。例如，传统日语使用大约3500个独特汉字字符。英语在最常用的20000个单词中有大约7000个独特音节。
- en: You can even change the number of layers in the encoder and decoder sides of
    the transformer, depending on the source (encoder) or target (decoder) language.
    You can even create a translation transformer that simplifies text for explaining
    complex concepts to five-year-olds, or adults on Mastodon server focused on ELI5
    ("explain it like I’m 5") conversations. If you reduce the number of layers in
    the decoder this will create a "capacity" bottleneck that can force your decoder
    to simplify or compress the concepts coming out of the encoder. Similarly, the
    number of attention heads in the encoder or decoder layers can be adjusted to
    increase or decrease the capacity (complexity) of your transformer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是transformers的编码器和解码器端的层数也可以根据源（编码器）或目标（解码器）语言进行更改。您甚至可以创建一个翻译transformers，将复杂概念简化为5岁的孩子或专注于ELI5（"像我5岁时解释"）对话的Mastodon服务器上的成年人。如果减少解码器的层数，这将创建一个"容量"瓶颈，迫使解码器简化或压缩来自编码器的概念。同样，编码器或解码器层中的注意力头的数量可以调整以增加或减少transformers的容量（复杂性）。
- en: Training the TranslationTransformer
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练TranslationTransformer
- en: Now let’s create an instance of the model for our translation task and initialize
    the weights in preparation for training. For the model’s dimensions you use the
    defaults, which correlate to the sizes of the original "Attention Is All You Need"
    transformer. Know that since the encoder and decoder building blocks comprise
    duplicate, stackable layers, you can configure the model with any number of these
    layers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为我们的翻译任务创建一个模型实例，并初始化权重以准备训练。对于模型的维度，您使用默认值，这些默认值与原始的"Attention Is All
    You Need"transformers的大小相对应。请注意，由于编码器和解码器构建块包括可堆叠的重复层，因此您可以配置模型以使用任意数量的这些层。
- en: Listing 9.14 Instantiate a TranslationTransformer
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.14 实例化TranslationTransformer
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: PyTorch creates a nice `_\_str\_\_` representation of your model. It displays
    all the layers and their inner structure including the shapes of the inputs and
    outputs. You may even be able to see the parallels between the layers of your
    models and the diagrams of tranformers that you see in this chapter or online.
    From the first half of the text representation for your transformer, you can see
    that all of the encoder layers have exactly the same structure. The inputs and
    outputs of each `TransformerEncoderLayer` have the same shape, so this ensures
    that you can stack them without reshaping linear layers between them. Transformer
    layers are like the floors of a skyscraper or a child’s stack of wooden blocks.
    Each level has exactly the same 3D shape.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch创建了一个漂亮的`_\_str\_\_`模型表示。它显示了所有层及其内部结构，包括输入和输出的形状。您甚至可以看到您的模型的层与本章或在线看到的transformers图的类比。从transformers的文本表示的前半部分，您可以看到所有的编码器层具有完全相同的结构。每个`TransformerEncoderLayer`的输入和输出具有相同的形状，因此这可以确保您可以将它们堆叠在一起而不需要在它们之间重塑线性层。transformers层就像摩天大楼或儿童木块的楼层一样。每个层级具有完全相同的3D形状。
- en: '[PRE17]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that you set the sizes of your source and target vocabularies in the
    constructor. Also, you pass the indices for the source and target padding tokens
    for the model to use in preparing the source, targets, and associated masking
    sequences. Now that you have the model defined, take a moment to do a quick sanity
    check to make sure there are no obvious coding errors before you set up the training
    and inference pipeline. You can create "batches" of random integer tensors for
    the sources and targets and pass them to the model, as demonstrated in the following
    listing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在构造函数中设置源词汇表和目标词汇表的大小。你还将传递源填充符和目标填充符的索引，以便模型在准备源、目标和相关掩码序列时使用。现在，你已经定义好了模型，请花点时间做一个快速的健全检查，确保没有明显的编码错误，然后再设置训练和预测流水线。你可以为源和目标创建随机整数张量的“批次”，并将它们传递给模型，如下面的示例所示。
- en: Listing 9.15 Quick model sanity check with random tensors
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.15 使用随机张量进行快速模型验证
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We created two tensors, `src` and `tgt`, each with random integers between
    1 and 100 distributed uniformly. Your model accepts tensors having batch-first
    shape, so we made sure that the batch sizes (10 in this case) were identical -
    otherwise we would have received a runtime error on the forward pass, that looks
    like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个张量 `src` 和 `tgt`，每个张量中的随机整数均匀分布在 1 到 100 之间。你的模型接受批次优先形状的张量，因此我们确保批次大小（本例中为
    10）相同，否则在前向传递中将会出现运行时错误，错误如下所示：
- en: '[PRE19]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It may be obvious, the source and target sequence lengths do not have to match,
    which is confirmed by the successful call to *model(src, tgt)*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 源序列和目标序列的长度不必相等，这一点很明显，*model(src, tgt)* 的成功调用证实了这一点。
- en: Tip
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: When setting up a new sequence-to-sequence model for training, you may want
    to initially use smaller tunables in your setup. This includes limiting max sequence
    lengths, reducing batch sizes, and specifying a smaller number of training loops
    or epochs. This will make it easier to debug issues in your model and/or pipeline
    to get your program executing end-to-end more quickly. Be careful not to draw
    any conclusions on the capabilities/accuracy of your model at this "bootstrapping"
    stage; the goal is simply to get the pipeline to run.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在为训练设置新的序列到序列模型时，你可能希望在设置中初始使用较小的参数。这包括限制最大序列长度、减小批次大小以及指定较小数量的训练循环或 epochs。这将使得在模型和/或流水线中调试问题并使程序能够端到端执行更容易。在这个“引导”阶段，不要对模型的能力/准确性做出任何结论；目标只是让流水线运行起来。
- en: Now that you feel confident the model is ready for action, the next step is
    to define the optimizer and criterion for training. "Attention Is All You Need"
    used Adam optimizer with a warmup period in which the learning rate is increased
    followed by a decreasing rate for the duration of training. You will use a static
    rate, 1e-4, which is smaller than the default rate 1e-2 for Adam. This should
    provide for stable training as long as you are patient to run enough epochs. You
    can play with learning rate scheduling as an exercise if you are interested. Other
    Transformer based models you will look at later in this chapter use a static learning
    rate. As is common for this type of task, you use `torch.nn.CrossEntropyLoss`
    for the criterion.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于你对模型的准备工作感到自信，下一步是为训练定义优化器和损失函数。《Attention is All You Need》使用了 Adam 优化器，其中学习率在训练的开始阶段逐渐增加，然后在训练的过程中逐渐减小。你将使用一个静态的学习率
    1e-4，该学习率小于 Adam 的默认学习率 1e-2。只要你愿意运行足够的 epochs，这应该能够提供稳定的训练。如果你有兴趣，可以尝试学习率调度。本章稍后介绍的其他基于
    Transformer 的模型会使用静态学习率。对于这类任务来说，你将使用 `torch.nn.CrossEntropyLoss` 作为损失函数。
- en: Listing 9.16 Optimizer and Criterion
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.16 优化器和损失函数
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Ben Trevett contributed much of the code for the Pytorch Transformer Beginner
    tutorial. He, along with colleagues, has written an outstanding and informative
    Jupyter notebook series for their Pytorch Seq2Seq tutorial ^([[23](#_footnotedef_23
    "View footnote.")]) covering sequence-to-sequence models. Their Attention Is All
    You Need ^([[24](#_footnotedef_24 "View footnote.")]) notebook provides a from-scratch
    implementation of a basic transformer model. To avoid re-inventing the wheel,
    the training and evaluation driver code in the next sections is borrowed from
    Ben’s notebook, with minor changes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Ben Trevett为Pytorch Transformer初学者教程贡献了大量代码。他和同事们为他们的Pytorch Seq2Seq教程编写了一系列出色且信息丰富的Jupyter笔记本系列^([[23](#_footnotedef_23
    "查看脚注。")])，涵盖了序列到序列模型。他们的 Attention Is All You Need^([[24](#_footnotedef_24 "查看脚注。")])
    笔记本提供了一个基本的Transformer模型的从零开始的实现。为了避免重复造轮子，在接下来的部分中，训练和评估驱动代码是从Ben的笔记本中借用的，只做了少量修改。
- en: The `train()` function implements a training loop similar to others you have
    seen. Remember to put the model into `train` mode before the batch iteration.
    Also, note that the last token in the target, which is the EOS token, is stripped
    from `trg` before passing it as input to the model. We want the model to predict
    the end of a string. The function returns the average loss per iteration.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`train()`函数实现了类似于你见过的其他训练循环。在批次迭代之前记得将模型设置为`train`模式。此外，请注意，在传递给模型之前，目标中的最后一个标记，即EOS标记，已从`trg`中删除。我们希望模型能够预测字符串的结束。该函数返回每次迭代的平均损失。'
- en: Listing 9.17 Model training function
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第9.17节 模型训练函数
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `evaluate()` function is similar to `train()`. You set the model to `eval`
    mode and use the `with torch.no_grad()` paradigm as usual for straight inference.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate()`函数类似于`train()`。你将模型设置为`eval`模式，并像通常一样使用`with torch.no_grad()`范式进行直接推理。'
- en: Listing 9.18 Model evaluation function
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第9.18节 模型评估函数
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next a straightforward utility function `epoch_time()`, used for calculating
    the time elapsed during training, is defined as follows.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，定义一个直接的实用函数`epoch_time()`，用于计算训练过程中经过的时间，如下所示。
- en: Listing 9.19 Utility function for elapsed time
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第9.19节 用于计算经过时间的实用函数
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, let’s proceed to setup the training. You set the number of epochs to 15,
    to give the model enough opportunities to train with the previously selected learning
    rate of 1e-4\. You can experiment with different combinations of learning rates
    and epoch numbers. In a future example, you will use an early stopping mechanism
    to avoid over-fitting and unnecessary training time. Here you declare a filename
    for `BEST_MODEL_FILE` and after each epoch, if the validation loss is an improvement
    over the previous best loss, the model is saved and the best loss is updated as
    shown.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续设置训练。你将训练的轮次数设置为15，以便模型有足够的机会以之前选择的学习率`1e-4`进行训练。你可以尝试不同的学习率和轮次数的组合。在未来的例子中，你将使用早停机制来避免过拟合和不必要的训练时间。在这里，你声明了一个文件名`BEST_MODEL_FILE`，并且在每个轮次之后，如果验证损失优于之前的最佳损失，那么模型将会被保存，最佳损失将会被更新，如下所示。
- en: Listing 9.20 Run the TranslationTransformer model training and save the **best**
    model to file
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第9.20节 运行 TranslationTransformer 模型训练并将**最佳**模型保存到文件中
- en: '[PRE24]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Notice that we could have probably run a few more epochs given that validation
    loss was still decreasing prior to exiting the loop. Let’s see how the model performs
    on a test set by loading the *best* model and running the `evaluate()` function
    on the test set.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在退出循环之前，验证损失仍在减小，我们可能可以再运行几个轮次。让我们加载*最佳*模型，并在测试集上运行`evaluate()`函数，看看模型的表现如何。
- en: Listing 9.21 Load *best* model from file and perform evaluation on test data
    set
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第9.21节 从文件加载*最佳*模型并在测试数据集上执行评估
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Your translation transformer achieves a log loss of about 1.6 on the test set.
    For a translation model trained on such a small dataset, this is not too bad.
    Log loss of 1.59 corresponds to a 20% probability (`exp(-1.59)`) of generating
    the correct token and the exact position it was provided in the test set. Because
    there are many different correct English translations for a given German text,
    this is a reasonable accuracy for a model that can be trained on a commodity laptop.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你的翻译转换器在测试集上实现了约1.6的对数损失。对于在如此小的数据集上训练的翻译模型来说，这还算不错。1.59的对数损失对应于生成正确标记的概率约为20%（`exp(-1.59)`），并且在测试集中提供的确切位置。由于对于给定的德语文本，有许多不同的正确英语翻译，所以这对于可以在普通笔记本电脑上进行训练的模型来说是合理的准确率。
- en: TranslationTransformer Inference
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TranslationTransformer 推理
- en: You are now convinced your model is ready to become your personal German-to-English
    interpreter. Performing translation requires only slightly more work to set up,
    which you do in the `translate_sentence()` function in the next listing. In brief,
    start by tokenizing the source *sentence* if it has not been tokenized already
    and end-capping it with the *<sos>* and *<eos>* tokens. Next, you call the `prepare_src()`
    method of the model to transform the *src* sequence and generate the source key
    padding mask as was done in training and evaluation. Then run the prepared `src`
    and `src_key_padding_mask` through the model’s encoder and save its output (in
    `enc_src`). Now, here is the fun part, where the target sentence (the translation)
    is generated. Start by initializing a list, `trg_indexes`, to the SOS token. In
    a loop - while the generated sequence has not reached a maximum length - convert
    the current prediction, *trg_indexes*, to a tensor. Use the model’s *prepare_tgt()*
    method to prepare the target sequence, creating the target key padding mask, and
    the target sentence mask. Run the current decoder output, the encoder output,
    and the two masks through the decoder. Get the latest predicted token from the
    decoder output and append it to *trg_indexes*. Break out of the loop if the prediction
    was an *<eos>* token (or if maximum sentence length is reached). The function
    returns the target indexes converted to tokens (words) and the attention weights
    from the decoder in the model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你确信你的模型已经准备好成为你个人的德语到英语的翻译器了。执行翻译只需要稍微多一点的设置工作，你会在接下来的代码清单的`translate_sentence()`函数中完成。简而言之，如果源句子还没有被分词，就先对其进行分词，然后在句子的开头和结尾加上
    *<sos>* 和 *<eos>* 标记。接下来，调用模型的`prepare_src()`方法来转换 *src* 序列，并生成与训练和评估中相同的源键填充蒙版。然后，运行准备好的`src`和`src_key_padding_mask`通过模型的编码器，并保存其输出（在`enc_src`中）。现在，这是有趣的部分，目标句子（即翻译）的生成。首先，初始化一个列表`trg_indexes`，包含
    SOS 标记。在循环中 - 只要生成的序列还没有达到最大长度 - 将当前预测的 *trg_indexes* 转换为张量。使用模型的`prepare_tgt()`方法准备目标序列，创建目标键填充蒙版和目标句子蒙版。将当前解码器输出、编码器输出和两个蒙版通过解码器。从解码器输出中获取最新预测的标记，并将其附加到
    *trg_indexes*。如果预测是一个 *<eos>* 标记（或达到最大句子长度），则退出循环。该函数返回目标索引转换为标记（单词）和模型中解码器的注意权重。
- en: Listing 9.22 Define *translate_sentence()* for performing inference
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 9.22 定义 *translate_sentence()* 函数以执行推断
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Your `translate_sentence()` wraps up your big transformer into a handy package
    you can use to translate whatever German sentence you run across.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你的`translate_sentence()`将你的大型transformers封装成一个方便的包，你可以用来翻译任何德语句子。
- en: TranslationTransformer Inference Example 1
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TranslationTransformer 推断示例 1
- en: 'Now you can use your `translate_sentence()` function on an example text. Since
    you probably do not know German, you can use a random example from the test data.
    Try it for the sentence "Eine Mutter und ihr kleiner Sohn genießen einen schönen
    Tag im Freien." In the OPUS dataset the character case was folded so that the
    text you feed into your transformer should be "eine mutter und ihr kleiner sohn
    genießen einen schönen tag im freien." And the correct translation that you’re
    looking for is: "A mother and her little [or young] son are enjoying a beautiful
    day outdoors."'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以在一个示例文本上使用你的`translate_sentence()`函数了。由于你可能不懂德语，你可以从测试数据中随机选择一个例子。试试这个句子："Eine
    Mutter und ihr kleiner Sohn genießen einen schönen Tag im Freien." 在 OPUS 数据集中，字符大小写已折叠，所以你输入到你的transformers的文本应该是："eine
    mutter und ihr kleiner sohn genießen einen schönen tag im freien." 你期望的正确翻译是："A
    mother and her little [or young] son are enjoying a beautiful day outdoors."
- en: Listing 9.23 Load sample at *test_data* index 10
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 9.23 在 *test_data* 索引 10 处加载样本
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It looks like the OPUS dataset is not perfect - the target (translated) token
    sequence is missing the verb "are" between "song" and "enjoying". And, the German
    word "kleiner" can be translated as little or young, but OPUS dataset example
    only provides one possible "correct" translation. And what about that "young song,"
    that seems odd. Perhaps that’s a typo in the OPUS test dataset.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 OPUS 数据集并不完美 - 目标（翻译）的标记序列在 "song" 和 "enjoying" 之间缺少动词 "are"。而且，德语单词 "kleiner"
    可以翻译为 "little" 或 "young"，但 OPUS 数据集示例只提供了一种可能的 "正确" 翻译。那个 "young song" 是什么意思，似乎有点奇怪。也许这是
    OPUS 测试数据集中的一个打字错误。
- en: Now you can run the `src` token sequence through your translator to see how
    it deals with that ambiguity.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，你可以将`src`标记序列通过你的翻译器，看看它是如何处理这种歧义的。 '
- en: Listing 9.24 Translate the test data sample
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码清单 9.24 翻译测试数据样本
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Interestingly, it appears there is a typo in the translation of the German word
    for "son" ("sohn") in the OPUS dataset. The dataset incorrectly translates "sohn"
    in German to "song" in English. Based on context, it appears the model did well
    to infer that a mother is (probably) with her young (little) "son". The model
    gives us the adjective "little" instead of "young", which is acceptable, given
    that the direct translation of the German word "kleiner" is "smaller".
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在 OPUS 数据集中，德语单词“son”（“sohn”）的翻译出现了拼写错误。该数据集错误地将德语中的“sohn”翻译为英语中的“song”。根据上下文，模型表现出了良好的推断能力，推断出母亲（可能）和她的小（年幼的）“son”在一起。该模型给出了形容词“little”，而不是“young”，这是可以接受的，因为德语单词“kleiner”的直译是“smaller”。
- en: Let’s focus our attention on, um, *attention*. In your model, you defined a
    *CustomDecoder* that saves the average attention weights for each decoder layer
    on each forward pass. You have the *attention* weights from the translation. Now
    write a function to visualize self-attention for each decoder layer using `matplotlib`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把注意力集中在，嗯，*注意力* 上。在你的模型中，你定义了一个 *CustomDecoder*，它保存了每次前向传递时每个解码器层的平均注意力权重。你有来自翻译的
    *attention* 权重。现在编写一个函数，使用 `matplotlib` 来可视化每个解码器层的自注意力。
- en: Listing 9.25 Function to visualize self-attention weights for decoder layers
    of the TranslationTransformer
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.25：用于可视化翻译transformers解码器层自注意力权重的函数
- en: '[PRE30]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The function plots the attention values at each index in the sequence with the
    original sentence on the x-axis and the translation along the y-axis. We use the
    *gist_yarg* color map since it’s a gray-scale scheme that is printer-friendly.
    Now you display the attention for the "mother and son enjoying the beautiful day"
    sentence.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数在序列中的每个索引处绘制注意力值，原始句子在 x 轴上，翻译句子在 y 轴上。我们使用 *gist_yarg* 颜色图，因为它是一种打印友好的灰度方案。现在展示“母亲和儿子享受美好的一天”句子的注意力。
- en: Listing 9.26 Visualize the self-attention weights for the test example translation
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.26：可视化测试示例翻译的自注意力权重
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Looking at the plots for the initial two decoder layers we can see that an area
    of concentration is starting to develop along the diagonal.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 查看最初的两个解码器层的图表，我们可以看到一个区域的注意力开始沿着对角线发展。
- en: 'Figure 9.6 Test Translation Example: Decoder Self-Attention Layers 1 and 2'
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.6：测试翻译示例：解码器自注意力层 1 和 2
- en: '![translation attention 1 2](images/translation_attention_1_2.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![翻译注意力 1 2](images/translation_attention_1_2.png)'
- en: In the subsequent layers, three and four, the focus is appearing to become more
    refined.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续的三层和四层中，注意力似乎变得更加精细。
- en: 'Figure 9.7 Test Translation Example: Decoder Self-Attention Layers 3 and 4'
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.7：测试翻译示例：解码器自注意力层 3 和 4
- en: '![translation attention 3 4](images/translation_attention_3_4.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![翻译注意力 3 4](images/translation_attention_3_4.png)'
- en: In the final two layers, we see the attention is strongly weighted where direct
    word-to-word translation is done, along the diagonal, which is what you likely
    would expect. Notice the shaded clusters of article-noun and adjective-noun pairings.
    For example, "son" is clearly weighted on the word "sohn", yet there is also attention
    given to "kleiner".
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两层中，我们看到注意力在直接词对词翻译的地方有很强的权重，沿着对角线，这是你可能期望的。请注意有阴影的文章名词和形容词名词对的聚类。例如，“son”明显在单词“sohn”上有权重，但也注意到了“kleiner”。
- en: 'Figure 9.8 Test Translation Example: Decoder Self-Attention Layers 5 and 6'
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.8：测试翻译示例：解码器自注意力层 5 和 6
- en: '![translation attention 5 6](images/translation_attention_5_6.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![翻译注意力 5 6](images/translation_attention_5_6.png)'
- en: You selected this example arbitrarily from the test set to get a sense of the
    translation capability of the model. The attention plots appear to show that the
    model is picking up on relations in the sentence, but the word importance is still
    strongly positional in nature. By that, we mean the German word at the current
    position in the original sentence is generally translated to the English version
    of the word at the same or similar position in the target output.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你随机选择了这个例子来自测试集，以了解模型的翻译能力。注意力图似乎显示出模型正在捕捉句子中的关系，但单词重要性仍然强烈地与位置有关。换句话说，原始句子中当前位置的德语单词通常被翻译为目标输出中相同或类似位置的英语单词。
- en: TranslationTransformer Inference Example 2
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 翻译transformers推理示例 2
- en: Have a look at another example, this time from the validation set, where the
    ordering of clauses in the input sequence and the output sequence are different,
    and see how the attention plays out. Load and print the data for the validation
    sample at index 25 in the next listing.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 再看另一个例子，这次是来自验证集的例子，在输入序列和输出序列的从句顺序不同的情况下，看看注意力是如何起作用的。在接下来的列表中加载并打印索引 25 处的验证样本数据。
- en: Listing 9.27 Load sample at *valid_data* index 25
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.27 在 *valid_data* 索引 25 处加载样本
- en: '[PRE32]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Even if your German comprehension is not great, it seems fairly obvious that
    the *orange toy* ("orangen spielzeug") is at the end of the source sentence, and
    the *in the tall grass* is in the middle. In the English sentence, however, "in
    tall grass" completes the sentence, while "with an orange toy" is the direct recipient
    of the "play" action, in the middle part of the sentence. Translate the sentence
    with your model.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的德语理解能力不强，很明显 *orange toy*（“orangen spielzeug”）在源句的末尾，而 *in the tall grass*
    在中间。然而，在英语句子中，“in tall grass” 完成了句子，而“with an orange toy” 是“play”行为的直接接受者，在句子的中间部分。用你的模型翻译这个句子。
- en: Listing 9.28 Translate the validation data sample
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.28 翻译验证数据样本
- en: '[PRE33]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This is a pretty exciting result for a model that took about 15 minutes to train
    (depending on your computing power). Again, plot the attention weights by calling
    the *display_attention()* function with the *src*, *translation* and *attention*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个大约花费 15 分钟训练的模型来说，这是一个相当令人兴奋的结果（取决于你的计算能力）。再次通过调用 *display_attention()*
    函数，绘制注意力权重，传入 *src*、*translation* 和 *attention*。
- en: Listing 9.29 Visualize the self-attention weights for the validation example
    translation
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.29 可视化验证示例翻译的自注意力权重
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here we show the plots for the last two layers (5 and 6).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了最后两层（第 5 和第 6 层）的图表。
- en: 'Figure 9.9 Validation Translation Example: Decoder Self-Attention Layers 5
    and 6'
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.9 验证翻译示例：解码器自注意力层 5 和 6
- en: '![translation attention validation 5 6](images/translation_attention_validation_5_6.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![翻译注意力验证 5 6](images/translation_attention_validation_5_6.png)'
- en: This sample excellently depicts how the attention weights can break from the
    position-in-sequence mold and actually attend to words later or earlier in the
    sentence. It truly shows the uniqueness and power of the multi-head self-attention
    mechanism.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这个样本出色地展示了注意力权重如何打破了位置-序列模式，实际上关注了句子中更早或更晚的单词。它真正展示了多头自注意机制的独特性和力量。
- en: To wrap up the section, you will calculate the BLEU (bilingual evaluation understudy)
    score for the model. The `torchtext` package supplies a function, *bleu_score*,
    for doing the calculation. You use the following function, again from Mr. Trevett’s
    notebook, to do inference on a dataset and return the score.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一节，你将计算模型的 BLEU（双语评估助手）得分。`torchtext` 包提供了一个函数，*bleu_score*，用于执行计算。你使用下面的函数，同样来自特雷维特先生的笔记本，对数据集进行推理并返回得分。
- en: '[PRE35]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Calculate the score for your test data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 计算你的测试数据得分。
- en: '[PRE36]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: To compare to Ben Trevett’s tutorial code, a convolutional sequence-to-sequence
    model ^([[25](#_footnotedef_25 "View footnote.")]) achieves a 33.3 BLEU and the
    smaller-scale Transformer scores about 35\. Your model uses the same dimensions
    of the original "Attention Is All You Need" Transformer, hence it is no surprise
    that it performs well.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与本·特雷维特（Ben Trevett）的教程代码进行比较，一个卷积序列到序列模型^([[25](#_footnotedef_25 "查看脚注。")])获得了
    33.3 的 BLEU，而较小规模的 Transformer 得分约为 35。你的模型使用了与原始的“注意力机制就是一切”Transformer 相同的维度，因此它表现良好并不奇怪。
- en: 9.3 Bidirectional backpropagation and "BERT"
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 双向反向传播和“BERT”
- en: Sometimes you want to predict something in the middle of a sequence — perhaps
    a masked-out word. Transformers can handle that as well. And the model doesn’t
    need to be limited to reading your text from left to right in a "causal" way.
    It can read the text from right to left on the other side of the mask as well.
    When generating text, the unknown word your model is trained to predict is at
    the end of the text. But transformers can also predict an interior word, for example,
    if you are trying to unredact the secret blacked-out parts of the Mueller Report.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你想要预测序列中的某些东西 —— 也许是一个被屏蔽的单词。Transformer 也可以处理这个。而且模型不需要局限于以“因果”的方式从左到右读取文本。它也可以从右边的掩码另一侧从右到左读取文本。在生成文本时，你的模型被训练来预测的未知单词位于文本的末尾。但是
    Transformer 也可以预测一个内部单词，例如，如果你试图揭开米勒报告中被涂黑的秘密部分。
- en: When you want to predict an unknown word *within* your example text you can
    take advantage of the words before and *after* the masked word. A human reader
    or an NLP pipeline can start wherever they like. And for NLP you always have a
    particular piece of text, with finite length, that you want to process. So you
    could start at the end of the text or the beginning…​ or *both*! This was the
    insight that BERT used to create task-independent embeddings of any body of text.
    It was trained on the general task of predicting masked-out words, similar to
    how you learned to train word embeddings using skip-grams in Chapter 6\. And,
    just as in word embedding training, BERT created a lot of useful training data
    from unlabeled text simply by masking out individual words and training a bidirectional
    transformer model to restore the masked word.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想预测一个未知的词 *在* 你的示例文本中时，你可以利用被遮罩词之前和 *之后* 的单词。人类读者或 NLP 流水线可以从任何位置开始。对于 NLP，你总是有一个特定长度的特定文本需要处理。因此，你可以从文本的末尾或开头开始…​或
    *两者都*！这就是 BERT 利用的洞察力，用于创建任何文本的任务无关嵌入。它是在通常任务上训练的，即预测遮罩词，类似于你在第 6 章中使用 skip-grams
    训练单词嵌入的方式。而且，就像在单词嵌入训练中一样，BERT 通过屏蔽单词并训练双向 transformer 模型来恢复被遮罩的单词，从未标记的文本中创建了大量有用的训练数据。
- en: In 2018, researchers at Google AI unveiled a new language model they call BERT,
    for "Bi-directional Encoder Representations from Transformers" ^([[26](#_footnotedef_26
    "View footnote.")]). The "B" in "BERT" is for "bidirectional." It isn’t named
    for a Sesame Street character it means "Bidirectional Encoder Representations
    from Transformers" - basically just a bidirectional transformer. Bidirectional
    transformers were a huge leap forward for machine-kind. In the next chapter, chapter
    10, you will learn about the three tricks that helped Transformers (souped-up
    RNNs) reach the top of the leaderboard for many of the hardest NLP problems. Giving
    RNNs the ability to read in both directions simultaneously was one of these innovative
    tricks that helped machines surpass humans at reading comprehension tasks.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年，Google AI 的研究人员推出了一种称为 BERT 的新语言模型，即“双向编码器从 transformer 中获取的表示”^([[26](#_footnotedef_26
    "查看脚注。")])。"BERT" 中的 "B" 是指 "双向"。它不是以芝麻街的角色命名的，而是指 "双向编码器从 transformer 中获取的表示"
    - 基本上就是一个双向 transformer。双向 transformer 对机器来说是一个巨大的进步。在下一章，第 10 章中，你将了解到帮助 transformer（升级版
    RNN）在许多最困难的 NLP 问题中登顶的三个技巧之一是什么。赋予 RNN 读取双向文本的能力就是其中之一，它帮助机器在阅读理解任务中超越人类。
- en: The BERT model, which comes in two flavors (configurations) — BERT *BASE* and
    BERT *LARGE* — is comprised of a stack of encoder transformers with feedforward
    and attention layers. Different from transformer models that preceded it, like
    OpenAI GPT, BERT uses masked language modeling (MLM) objective to train a deep
    bi-directional transformer. MLM involves randomly masking tokens in the input
    sequence and then attempting to predict the actual tokens from context. More powerful
    than typical left-to-right language model training, the MLM objective allows BERT
    to better generalize language representations by joining the left and right context
    of a token in all layers. The BERT models were pre-trained in a semi-unsupervised
    fashion on the English Wikipedia sans tables and charts (2500M words), and the
    BooksCorpus (800M words and upon which GPT was also trained). With simply some
    tweaks to inputs and the output layer, the models can be fine tuned to achieve
    state-of-the-art results on specific sentence-level and token-level tasks.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型有两种版本（配置） — BERT *BASE* 和 BERT *LARGE* — 由一堆带有前馈和注意层的编码器 transformer
    组成。与之前的 transformer 模型（如 OpenAI GPT）不同，BERT 使用了遮罩语言建模（MLM）目标来训练一个深度双向 transformer。MLM
    包括在输入序列中随机遮罩标记，然后尝试从上下文预测实际标记。MLM 目标比典型的从左到右的语言模型训练更强大，它允许 BERT 在所有层中通过连接标记的左右上下文来更好地概括语言表示。BERT
    模型是在英文维基百科中（不包括表格和图表，共 2500M 个单词）和 BooksCorpus（也是 GPT 的训练基础，共 800M 个单词）上以半监督的方式预训练的。通过简单调整输入和输出层，模型可以被微调以在特定的句子级和标记级任务上达到最先进的结果。
- en: 9.3.1 Tokenization and Pre-training
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 分词和预训练
- en: 'The input sequences to BERT can ambiguously represent a single sentence or
    a pair of sentences. BERT uses WordPiece embeddings with the first token of each
    sequence always set as a special *[CLS]* token. Sentences are distinguished by
    a trailing separator token, *[SEP]*. Tokens in a sequence are further distinguished
    by a separate segment embedding with either sentence A or B assigned to each token.
    Additionally, a positional embedding is added to the sequence, such that each
    position the input representation of a token is formed by summation of the corresponding
    token, segment, and positional embeddings as shown in the figure below (from the
    published paper):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到BERT的序列可以模糊地表示一个单独的句子或一对句子。 BERT使用WordPiece嵌入，每个序列的第一个令牌始终设置为特殊的*[CLS]*令牌。
    句子通过尾随的分隔符令牌*[SEP]*进行区分。 序列中的令牌进一步通过单独的段嵌入进行区分，每个令牌分配给句子A或B。 此外，还向序列添加了位置嵌入，以便于序列的每个位置的输入表示由相应的令牌、段和位置嵌入的总和形成，如下图所示（来自已发表的论文）：
- en: '![bert inputs](images/bert_inputs.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![bert inputs](images/bert_inputs.png)'
- en: During pre-training a percentage of input tokens are masked randomly (with a
    *[MASK]* token) and the model the model predicts the actual token IDs for those
    masked tokens. In practice, 15% of the WordPiece tokens were selected to be masked
    for training, however, a downside of this is that during fine-tuning there is
    no *[MASK]* token. To work around this, the authors came up with a formula to
    replace the selected tokens for masking (the 15%) with the *[MASK]* token 80%
    of the time. For the other 20%, they replace the token with a random token 10%
    of the time and keep the original token 10% of the time. In addition to this MLM
    objective pre-training, secondary training is done for Next Sentence Prediction
    (NSP). Many downstream tasks, such as Question Answering (QA), depend upon understanding
    the relationship between two sentences, and cannot be solved with language modeling
    alone. For the NSP wave of training, the authors generated a simple binarized
    NSP task by selecting pairs of sentences A and B for each sample and labeling
    them as *IsNext* and *NotNext*. Fifty percent of the samples for the pre-training
    had selections where sentence B followed sentence A in the corpus, and for the
    other half sentence B was chosen at random. This plain solution shows that sometimes
    one need not overthink a problem.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，输入令牌的一部分被随机屏蔽（使用*[MASK]*令牌），模型预测这些屏蔽令牌的实际令牌ID。 在实践中，选择了15%的WordPiece令牌进行训练，然而，这样做的一个缺点是在微调过程中没有*[MASK]*令牌。
    为了解决这个问题，作者提出了一个公式，以使被选中的令牌（15%）在80%的时间内替换为*[MASK]*令牌。 对于其他20%，他们将令牌替换为随机令牌的10%的时间，并保留原始令牌的10%的时间。
    除了这个MLM目标预训练，还进行了次要训练以进行下一句子预测（NSP）。 许多下游任务，如问答（QA），依赖于理解两个句子之间的关系，不能仅通过语言建模来解决。
    对于NSP训练的波段，作者通过为每个样本选择句子A和B并将它们标记为*IsNext*和*NotNext*，生成了一个简单的二元NSP任务。 预训练的50%的样本中的选择是语料库中句子B跟随句子A，另一半的句子B是随机选择的。
    这个简单的解决方案表明有时候人们无需过度思考问题。
- en: 9.3.2 Fine-tuning
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 微调
- en: For most BERT tasks, you will want to load the BERT[BASE] or BERT[LARGE] model
    with all its parameters initialized from the pre-training and fine tune the model
    for your specific task. The fine-tuning should typically be straightforward; one
    simply plugs in the task-specific inputs and outputs and then commences training
    all parameters end-to-end. Compared to the initial pre-training, the fine-tuning
    of the model is much less expensive. BERT is shown to be more than capable on
    a multitude of tasks. For example, at the time of its publication, BERT outperformed
    the current state-of-the-art OpenAI GPT model on the General Language Understanding
    Evaluation (GLUE) benchmark. And BERT bested the top-performing systems (ensembles)
    on the Stanford Question Answering Dataset (SQuAD v1.1), where the task is to
    select the text span from a given Wikipedia passage that provides the answer to
    a given question. Unsurprisingly, BERT was also best at a variation of this task,
    SQuAD v2.0, where it is allowed that a short answer for the problem question in
    the text might not exist.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数BERT任务，你会想要加载BERT[BASE]或BERT[LARGE]模型，并使用所有参数从预训练进行初始化，然后针对你的特定任务对模型进行微调。微调通常是直接的；只需将任务特定的输入和输出插入，并开始对所有参数进行训练。与初始预训练相比，模型的微调要便宜得多。BERT在许多任务上表现出色。例如，在发布时，BERT在通用语言理解评估（GLUE）基准测试中超过了当前最先进的OpenAI
    GPT模型。并且在斯坦福问答数据集（SQuAD v1.1）上，BERT超过了最佳系统（集成模型），这个任务是从给定的维基百科段落中选择包含给定问题答案的文本片段。不出所料，对于这个任务的一个变种，SQuAD
    v2.0，BERT也是最好的，它允许问题的简短答案在文本中可能不存在。
- en: 9.3.3 Implementation
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3实现
- en: Borrowing from the discussion on the original transformer earlier in the chapter,
    for the BERT configurations, *L* denotes the number of transformer layers. The
    hidden size is *H* and the number of self-attention heads is *A*. BERT[BASE] has
    dimensions *L*=12, *H*=768, and *A*=12, for a total of 110M parameters. BERT[LARGE]
    has *L*=24, *H*=1024, and *A*=16 for 340M total parameters! The large model outperforms
    the base model on all tasks, however depending on hardware resources available
    to you, you may find working with the base model more than adequate. There are
    are *cased* and *uncased* versions of the pre-trained models for both, the base
    and large configurations. The *uncased* version had the text converted to all
    lowercase before pre-training WordPiece tokenization, while there were no changes
    made to the input text for the *cased* model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，从原始Transformer的讨论中借鉴来的，对于BERT的配置，*L* 表示Transformer层的数量，隐藏层大小为 *H*，自注意力头的数量为
    *A*。BERT[BASE]的尺寸为 *L*=12，*H*=768，*A*=12，总共有1.1亿个参数。BERT[LARGE]有 *L*=24，*H*=1024，*A*=16，总共有3.4亿个参数！大型模型在所有任务上都超过了基准模型的性能，但是取决于你可用的硬件资源，你可能会发现使用基准模型已经足够了。对于基准和大型配置，预训练模型都有*大小写（cased）*
    和 *不区分大小写（uncased）* 的版本。*不区分大小写（uncased）* 版本在预训练WordPiece分词之前将文本转换为全小写，而 *大小写（cased）*
    模型的输入文本没有作任何更改。
- en: The original BERT implementation was open-sourced as part of the TensorFlow
    *tensor2tensor* library ^([[27](#_footnotedef_27 "View footnote.")]) A *Google
    Colab* notebook ^([[28](#_footnotedef_28 "View footnote.")]) demonstrating how
    to fine tune BERT for sentence-pair classification tasks was published by the
    TensorFlow Hub authors circa the time the BERT academic paper was released. Running
    the notebook requires registering for access to Google Cloud Platform Compute
    Engine and acquiring a Google Cloud Storage bucket. At the time of this writing,
    it appears Google continues to offer monetary credits for first-time users, but
    generally, you will have to pay for access to computing power once you have exhausted
    the initial trial offer credits.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的原始实现是作为 TensorFlow *tensor2tensor* 库的一部分进行开源的 ^([[27](#_footnotedef_27
    "查看脚注。")])。 TensorFlow Hub 的作者们在BERT学术论文发布时，编写了一个 *Google Colab* 笔记本 ^([[28](#_footnotedef_28
    "查看脚注。")])，展示了如何对BERT进行句对分类任务的微调。运行这个笔记本需要注册以访问Google Cloud Platform Compute Engine，并获取一个Google
    Cloud Storage存储桶。在撰写本文时，谷歌似乎仍然给首次用户提供货币积分，但通常情况下，一旦你耗光了最初试用积分，就需要支付以获得计算能力的费用。
- en: Note
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: As you go deeper into NLP models, especially with the use of models having deep
    stacks of transformers, you may find that your current computer hardware is insufficient
    for computationally expensive tasks of training and/or fine-tuning large models.
    You will want to evaluate the costs of building out a personal computer to meet
    your workloads and weigh that against pay-per-use cloud and virtual computing
    offerings for AI. We reference basic hardware requirements and compute options
    in this text, however, discussion of the "right" PC setup or providing an exhaustive
    list of competitive computing options are outside the scope of this book. In addition
    to the Google Compute Engine, just mentioned, the appendix has instructions for
    setting up Amazon Web Services (AWS) GPU.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您深入研究 NLP 模型，特别是使用具有深度transformers堆栈的模型，您可能会发现您当前的计算机硬件不足以满足训练和/或微调大型模型的计算密集型任务。您将需要评估建立个人计算机以满足工作负载的成本，并将其与用于人工智能的按使用量付费的云和虚拟计算方案进行权衡。我们在本文中提及了基本的硬件要求和计算选项，然而，讨论
    "正确" 的个人电脑设置或提供竞争性计算选项的详尽列表超出了本书的范围。除了刚刚提到的 Google 计算引擎之外，附录中还有设置亚马逊网络服务（AWS）GPU
    的说明。
- en: Pytorch versions of BERT models were implemented in the `pytorch-pretrained-bert`
    library ^([[29](#_footnotedef_29 "View footnote.")]) and then later incorporated
    in the indispensable HuggingFace *transformers* library ^([[30](#_footnotedef_30
    "View footnote.")]). You would do well to spend some time reading the "Getting
    Started" documentation and the summaries of the transformer models and associated
    tasks on the site. To install the transformers library, simply use `pip install
    transformers`. Once installed, import the BertModel from transformers using the
    `BertModel.from_pre-trained()` API to load one by name. You can print a summary
    for the loaded "bert-base-uncased" model in the listing that follows, to get an
    idea of the architecture.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型的 Pytorch 版本是在 `pytorch-pretrained-bert` 库中实现的 ^([[29](#_footnotedef_29
    "查看脚注。")])，后来又被纳入了不可或缺的 HuggingFace *transformers* 库中 ^([[30](#_footnotedef_30
    "查看脚注。")])。您最好花一些时间阅读网站上的 "入门" 文档以及transformers模型和相关任务的摘要。要安装 transformers 库，只需使用 `pip
    install transformers`。安装完成后，使用 `BertModel.from_pre-trained()` API 按名称加载一个。您可以在下面的列表中打印加载的
    "bert-base-uncased" 模型的摘要，以了解其架构。
- en: Listing 9.30 Pytorch summary of BERT architecture
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.30 BERT 架构的 Pytorch 摘要
- en: '[PRE37]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: After import a BERT model you can display its string representation to get a
    summary of its structure. This is a good place to start if you are considering
    designing your own custom bidirectional transformer. But in most cases, you can
    use BERT directly to create encodings of English text that accurately represent
    the meaning of most text. A pretrained BERT model is all you may need for applications
    such as chatbot intent labeling (classification or tagging), sentiment analysis,
    social media moderation, semantic search, and FAQ question answering. And if you’re
    considering storing embeddings in a vector database for semantic search, vanilla
    BERT encodings are your best bet.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 BERT 模型后，您可以显示其字符串表示以获取其结构的摘要。如果您考虑设计自己的自定义双向transformers，这是一个很好的起点。但在大多数情况下，您可以直接使用
    BERT 来创建准确表示大多数文本含义的英文文本编码。预训练的 BERT 模型可能是您应用程序所需的全部，例如聊天机器人意图标记（分类或标记）、情感分析、社交媒体管理、语义搜索和常见问题解答。如果您考虑在向量数据库中存储嵌入以进行语义搜索，那么普通的
    BERT 编码是您的最佳选择。
- en: In the next section, you’ll see an example of how to use a pretrained BERT model
    to identify toxic social media messages. And then you will see how to fine tune
    a BERT model for your application by training it for additional epochs on your
    dataset. You will see that fine tuning BERT can significantly improve your toxic
    comment classification accuracy without overfitting.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将看到如何使用预训练的 BERT 模型识别有毒社交媒体消息的示例。然后，您将看到如何通过在数据集上进行额外的训练周期来微调 BERT 模型以适用于您的应用程序。您将看到，微调
    BERT 可以显著提高您的有毒评论分类准确性，而不会过度拟合。
- en: 9.3.4 Fine-tuning a pre-trained BERT model for text classification
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 对预训练的 BERT 模型进行微调以进行文本分类
- en: In 2018, the Conversation AI ^([[31](#_footnotedef_31 "View footnote.")]) team
    (a joint venture between Jigsaw and Google) hosted a Kaggle competition to develop
    a model to detect various types of toxicity in a online social media posts. At
    the time, LSTM’s and Convolutional Neural Networks were the state of the art.
    Bi-directional LSTMs with attention achieved the best scores in this competition.
    The promise of BERT is that it can simultaneously learn word context from words
    both left and right of the current word being processed by the transformer. This
    makes it especially useful for creating multipurpose encoding or embedding vectors
    for use in classification problems like detecting toxic social media comments.
    And because BERT is pre-trained on a large corpus, you don’t need a huge dataset
    or supercomputer to be able to fine tune a model that achieves good performance
    using the power of *transfer learning*.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Conversation AI ^([[31](#_footnotedef_31 "查看脚注。")]) 团队（由 Jigsaw 和 Google
    联合创办）举办了一场 Kaggle 竞赛，旨在开发一个模型来检测在线社交媒体帖子中的各种毒性。当时，LSTM 和卷积神经网络是当时的最新技术。具有注意力的双向
    LSTM 在这场比赛中取得了最佳成绩。BERT 的承诺是它可以同时从当前正在处理的转换器的当前单词的左右单词中学习单词上下文。这使得它特别适用于创建多用途的编码或嵌入向量，用于解决诸如检测有毒社交媒体评论之类的分类问题。而且由于
    BERT 是在一个大型语料库上预训练的，所以你不需要庞大的数据集或超级计算机，就能够利用 *迁移学习* 的力量来微调一个性能良好的模型。
- en: In this section, you will use the library to quickly fine tune a pre-trained
    BERT model for classifying toxic social media posts. After that, you will make
    some adjustments to improve the model in your quest to combat bad behavior and
    rid the world of online trolls.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，你将使用该库快速微调一个预先训练好的 BERT 模型，用于分类有毒的社交媒体帖子。之后，你将进行一些调整，以改进模型，以期在消除恶意行为，清除网络喷子的道路上取得成功。
- en: A toxic dataset
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个有毒的数据集
- en: You can download the "Toxic Comment Classification Challenge" dataset (`archive.zip`)
    from kaggle.com. ^([[32](#_footnotedef_32 "View footnote.")]) You can put the
    data in your `$HOME/.nlpia2-data/` directory with all the other large datasets
    from the book, if you like. When you unzip the `archive.zip` file you’ll see it
    contains the training set (`train.csv`) and test set (`test.csv`) as separate
    CSV files. In the real world you would probably combine the training and test
    sets to create your own sample of validation and test examples. But to make your
    results comparable to what you see on the competition website you will first only
    work with the training set.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 kaggle.com 下载"有毒评论分类挑战"数据集（`archive.zip`）。^([[32](#_footnotedef_32 "查看脚注。")])
    如果愿意，你可以将数据放在你的 `$HOME/.nlpia2-data/` 目录中，和本书的其他大型数据集一起。当你解压 `archive.zip` 文件时，你会看到它包含了训练集（`train.csv`）和测试集（`test.csv`）作为单独的
    CSV 文件。在现实世界中，你可能会将训练集和测试集合并起来，创建自己的验证和测试示例样本。但为了使你的结果与竞赛网站上看到的结果可比，你首先只会使用训练集。
- en: Begin by loading the training data using pandas and take a look at the first
    few entries as shown in the next listing. Normally you would wan to take a look
    at examples from the dataset to get a feel for the data and see how it is formatted.
    It’s usually helpful to try to do the same task that you are asking the model
    to do, to see if it’s a reasonable problem for NLP. Here are the first five examples
    in the training set. Fortunately the dataset is sorted to contain the nontoxic
    posts first, so you won’t have to read any toxic comments until the very end of
    this section. If you have a grandmother named "Terri" you can close your eyes
    at the last line of code in the last code block of in this section `;-)`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用 pandas 加载训练数据，并查看接下来列表中显示的前几个条目。通常，你会想查看数据集中的示例，以了解数据的格式。尝试做与你要求模型执行的相同任务通常是有帮助的，以查看它是否是一个合理的
    NLP 问题。这里是训练集中的前五个示例。幸运的是，数据集被排序为首先包含非有毒的帖子，所以你不必在这一节的最后读取任何有毒评论。如果你有一个名叫"Terri"的祖母，你可以在这一节的最后一个代码块的最后一行闭上你的眼睛
    `;-)`。
- en: Listing 9.31 Load the toxic comments dataset
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.31 载入有毒评论数据集
- en: '[PRE39]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Whew, luckily none of the first five comments are obscene, so they’re fit to
    print in this book.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，幸运的是，前五条评论都不是淫秽的，所以它们可以打印在这本书中。
- en: Spend time with the data.
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 花时间与数据相处。
- en: Typically at this point you would explore and analyze the data, focusing on
    the qualities of the text samples and the accuracy of the labels and perhaps ask
    yourself questions about the data. How long are the comments in general? Does
    sentence length or comment length have any relation to toxicity? Consider focusing
    on some of the *severe_toxic* comments. What sets them apart from the merely *toxic*
    ones? What is the class distribution? Do you need to potentially account for a
    class imbalance in your training techniques?
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在这一点上，你会探索和分析数据，关注文本样本的特性和标签的准确性，也许会问自己关于数据的问题。评论通常有多长？句子长度或评论长度是否与毒性有关？考虑专注于一些*severe_toxic*评论。它们与仅仅有毒的评论有什么不同？类分布是什么样的？你是否需要在训练技术中考虑到类别不平衡？
- en: You want to get to the training, so let’s split the data set into training and
    validation (evaluation) sets. With almost 160,000 samples available for model
    tuning, we elect to use an 80-20 train-test split.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要进行训练，所以让我们将数据集分割为训练集和验证（评估）集。由于可供模型调整的样本数量几乎为160,000个，我们选择使用80-20的训练-测试分割。
- en: Listing 9.32 Split data into training and validation sets
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.32 将数据分割为训练集和验证集
- en: '[PRE40]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now you have your data in a Pandas DataFrame with descriptive column names you
    can use to interpret the test results for your model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经将数据存储在一个具有描述性列名的Pandas DataFrame中，你可以使用这些列名来解释模型的测试结果。
- en: There’s one last ETL task for you to deal with, you need a wrapper function
    to ensure the batches of examples passed to your transformer have the right shape
    and content. You are going to use the `simpletransformers` library which provides
    wrappers for various Hugging Face models designed for classification tasks including
    multilabel classification, not to be confused with multiclass or multioutput classification
    models. ^([[33](#_footnotedef_33 "View footnote.")]) The Scikit-Learn package
    also contains a `MultiOutputClassifier` wrapper that you can use to create multiple
    estimators (models), one for each possible target label you want to assign to
    your texts.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个最后的ETL任务需要你处理，你需要一个包装函数来确保传递给转换器的示例批次具有正确的形状和内容。你将使用`simpletransformers`库，该库为各种Hugging
    Face模型设计了包装器，用于分类任务，包括多标签分类，不要与多类别或多输出分类模型混淆。Scikit-Learn包还包含一个`MultiOutputClassifier`包装器，你可以使用它来创建多个评估器（模型），每个目标标签对应一个。
- en: Important
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: A multilabel classifier is a model that outputs multiple different predicted
    discrete classification labels ('toxic', 'severe', and 'obscene') for each input.
    This allows your text to be given multiple different labels. Like a fictional
    family in Tolstoy’s *Anna_Karenina*, a toxic comment can be toxic in many different
    ways, all at the same time. You can think of a multilabel classifier as applying
    hashtags or emojis to a text. To prevent confusion you can call your models "taggers"
    or "tagging models" so others don’t misunderstand you.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类器是一个模型，它为每个输入输出多个不同的预测离散分类标签（'toxic'，'severe'和'obscene'），这允许你的文本被赋予多个不同的标签。就像托尔斯泰的*《安娜·卡列尼娜》*中的虚构家庭一样，一条有毒的评论可以以多种不同的方式毒害人，而且同时进行。你可以将多标签分类器视为向文本应用标签或表情符号。为了避免混淆，你可以将你的模型称为“标签器”或“标签模型”，以免别人误解你。
- en: Since each comment can be assigned multiple labels (zero or more) the `MultiLabelClassificationModel`
    is your best bet for this kind of problem. According to the documentation,^([[34](#_footnotedef_34
    "View footnote.")]) the `MultiLabelClassificationModel` model expects training
    samples in the format of `["text", [label1, label2, label3, …​]]`. This keeps
    the outer shape of the dataset the same no matter how many different kinds of
    toxicity you want to keep track of. The Hugging Face `transformers` models can
    handle any number of possible labels (tags) with this data structure, but you
    need to be consistent within your pipeline, using the same number of possible
    labels for each example. You need a *multihot* vector of zeros and ones with a
    constant number of dimensions so your model knows where to put the predictions
    for each kind of toxicity. The next listing shows how you can arrange the batches
    of data within a wrapper function that you run during training and evaluation
    of you model.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个评论可以被分配多个标签（零个或多个），因此 `MultiLabelClassificationModel` 是解决这类问题的最佳选择。 根据文档^([[34](#_footnotedef_34
    "View footnote.")])，`MultiLabelClassificationModel` 模型期望以 `["text", [label1, label2,
    label3, …​]]` 的格式提供训练样本。 这样做可以保持数据集的外部形状不变，无论您想要跟踪多少不同类型的毒性。 Hugging Face `transformers`
    模型可以处理具有任意数量可能标签（标签）的数据结构，但您需要在管道内保持一致，在每个示例中使用相同数量的可能标签。 您需要一个具有恒定维度数量的*multihot*零和一向量，以便您的模型知道在哪里放置对每种毒性的预测。
    下一个列表显示了如何安排包含在训练和评估模型期间运行的包装器函数的数据批次。
- en: Listing 9.33 Create datasets for model
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.33 为模型创建数据集
- en: '[PRE41]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You can now see that this dataset has a pretty low bar for toxicity if mothers
    and grandmothers are the target of bullies' insults. This means this dataset may
    be helpful even if you have extremely sensitive or young users that you are trying
    to protect. If you are trying to protect modern adults or digital natives that
    are used to experiencing cruelty online, you can augment this dataset with more
    extreme examples from other sources.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到，如果母亲和祖母是欺负者侮辱的目标，那么该数据集对毒性的门槛相当低。 这意味着即使您要保护极端敏感或年轻的用户，该数据集也可能会有所帮助。
    如果您试图保护现代成年人或习惯于在线体验残酷的数字原住民，您可以从其他来源增加这个数据集的更极端的例子。
- en: Detect toxic comments with `simpletransformers`
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 `simpletransformers` 检测有毒评论
- en: You now have a function for passing batches of labeled texts to the model and
    printing some messages to monitor your progress. So it’s time to choose a BERT
    model to download. You need to set up just a few basic parameters and then you
    will be ready to load a pre-trained BERT for multi-label classification and kick
    off the fine-tuning (training).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有一个函数，用于将带标签的文本批次传递给模型并打印一些消息以监视您的进度。 所以现在是选择要下载的 BERT 模型的时候了。 您只需要设置几个基本参数，然后就可以准备加载预训练的
    BERT 进行多标签分类并开始微调（训练）。
- en: Listing 9.34 Setup training parameters
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.34 设置训练参数
- en: '[PRE42]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the listing below you load the pre-trained `bert-base-cased` model configured
    to output the number of labels in our toxic comment data (6 total) and initialized
    for training with your `model_args` dictionary.^([[35](#_footnotedef_35 "View
    footnote.")])
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，您加载了预训练的 `bert-base-cased` 模型，配置为输出我们有毒评论数据中的标签数（总共 6 个），并使用您的 `model_args`
    字典初始化进行训练^([[35](#_footnotedef_35 "View footnote.")])。
- en: Listing 9.35 Load pre-trained model and fine tune
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.35 载入预训练模型并微调
- en: '[PRE43]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `train_model()` is doing the heavy lifting for you. It loads the pre-trained
    `BertTokenizer` for the pre-trained *bert-base-cased* model you selected and uses
    it to tokenize the `train_df['text']` to inputs for training the model. The function
    combines these inputs with the `train_df[labels]` to generate a `TensorDataset`
    which it wraps with a PyTorch `DataLoader`, that is then iterated over in batches
    to comprise the training loop.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_model()` 为您完成了繁重的工作。 它加载了您选择的预训练*bert-base-cased*模型的预训练 `BertTokenizer`
    并将其用于对 `train_df[''text'']` 进行标记以用于训练模型的输入。 该函数将这些输入与 `train_df[labels]` 结合生成一个
    `TensorDataset`，然后将其与 PyTorch `DataLoader` 包装，然后以批次进行迭代，以构成训练循环。'
- en: 'In other words, with just a few lines of code and one pass through your data
    (one epoch) you’ve fine tuned a 12-layer transformer with 110 million parameters!
    The next question is: did it help or hurt the model’s translation ability? Let’s
    run inference on your evaluation set and check the results.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，只需几行代码和一次通过您的数据（一个时期），您就可以微调具有 1100 万参数的 12 层变换器！ 下一个问题是：它有助于还是损害了模型的翻译能力？
    让我们对评估集运行推理并检查结果。
- en: Listing 9.36 Evaluation
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.36 评估
- en: '[PRE44]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The ROC (Receiver Operating Characteristic curve) AUC (Area Under the Curve)
    metric balances all the different ways a classifier can be wrong by computing
    the integral (area) under the precision vs recall plot (curve) for a classifier.
    This ensures that models that are confidently wrong are penalized more than models
    that are closer to the truth with their predicted probability values. And the
    `roc_auc_score` within this `simpletransformers` package will give you the micro
    average of all the examples and all the different labels it could have chosen
    for each text.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ROC（Receiver Operating Characteristic curve）AUC（曲线下面积）指标通过计算分类器在精度与召回率曲线（曲线）下的积分（面积）来平衡分类器可能出现的所有错误方式。这确保了那些自信错误的模型比那些在预测概率值上更接近真相的模型受到更严厉的惩罚。而`simpletransformers`包中的`roc_auc_score`会给出所有示例和每个文本可能选择的所有不同标签的微平均值。
- en: The ROC AUC micro average score is essentially the sum of all the `predict_proba`
    error values, or how far the predicted probability values are from the 0 or 1
    values that each example was given by a human labeler. It’s always a good idea
    to have that mental model in mind when your are measuring model accuracy. Accuracy
    is just how close to what your human labelers thought the correct answer was,
    not some absolute truth about the meaning or intent or effects of the words that
    are being labeled. Toxicity is a very subjective quality.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ROC AUC微平均分数实质上是所有`predict_proba`错误值的总和，或者说是预测概率值与人类标注者给定的0或1值之间的距离。在测量模型准确性时，记住这个心理模型是一个好主意。准确性只是离人类标注者认为的正确答案有多接近，而不是关于被标注的词语的意义、意图或影响的绝对真相。毒性是一个非常主观的质量。
- en: A `roc_auc_score` of 0.981 is not too bad out of the gate. While it’s not going
    to win you any accolades ^([[36](#_footnotedef_36 "View footnote.")]), it does
    provide encouraging feedback that your training simulation and inference is setup
    correctly.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: AUC分数为0.981并不算太差。虽然它不会为你赢得任何称赞^([[36](#_footnotedef_36 "View footnote.")])，但它确实提供了对你的训练模拟和推理是否设置正确的鼓励性反馈。
- en: The implementations for `eval_model()` and `train_model()` are found in the
    base class for both `MultiLabelClassificationModel` and `ClassificationModel`.
    The evaluation code will look familiar to you, as it uses the `with torch.no_grad()`
    context manager for doing inference, as one would expect. Taking the time to look
    at the method implementations is suggested. Particularly, `train_model()` is helpful
    for viewing exactly how the configuration options you select in the next section
    are employed during training and evaluation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`eval_model()`和`train_model()`的实现方法都可以在`MultiLabelClassificationModel`和`ClassificationModel`的基类中找到。评估代码会让你感到熟悉，因为它使用了`with
    torch.no_grad()`上下文管理器来进行推理，这是人们所期望的。建议你花些时间看看这些方法的实现情况。特别是`train_model()`对于查看在训练和评估过程中如何使用下一部分中选择的配置选项是有帮助的。'
- en: A better BERT
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更好的BERT
- en: Now that you have a first cut at a model you can do some more fine tuning to
    help your BERT-based model do better. And "better" in this case simply means having
    a higher AUC score. Just like in the real world, you’ll need to decide what better
    is in your particular case. So don’t forget to pay attention to how the model’s
    predictions are affecting the user experience for the people or businesses using
    your model. If you can find a better metric that more directly measures what "better"
    means for your users you should use that in place of the AUC score for your application
    you should substitute it in this code.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了一个模型的初步版本，可以进行更细致的调整，以帮助你的基于BERT的模型做得更好。在这种情况下，“更好”简单地意味着有更高的AUC分数。就像在现实世界中一样，你需要决定在你的特定情况中什么是更好的。因此，不要忘记关注模型的预测如何影响使用你的模型的人或企业的用户体验。如果你能找到一个更好的度量标准，更直接地衡量对于你的用户来说什么是“更好”，那么你应该将它替换掉这段代码中的AUC分数。
- en: Building upon the training code you executed in the previous example, you’ll
    work on improving your model’s accuracy. Cleaning the text a bit with some preprocessing
    is fairly straightforward. The book’s example source code comes with a utility
    `TextPreprocessor` class we authored to replace common misspellings, expand contractions
    and perform other miscellaneous cleaning such as removing extra white-space characters.
    Go ahead and rename the `comment_text` column to `original_text` in the loaded
    *train.csv* dataframe. Apply the preprocessor to the original text and store the
    refined text back in a `comment_text` column.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中执行的训练代码基础上，您将致力于提高模型的准确性。通过一些预处理稍微清理文本是相当直接的。该书的示例源代码附带了一个我们编写的实用程序`TextPreprocessor`类，用于替换常见的拼写错误，扩展缩略语并执行其他杂项清理，如删除额外的空白字符。继续并将加载的*train.csv*数据框中的`comment_text`列重命名为`original_text`。将预处理器应用于原始文本，并将精炼后的文本存储回`comment_text`列。
- en: Listing 9.37 Preprocessing the comment text
  id: totrans-314
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.37 预处理评论文本
- en: '[PRE45]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: With the text cleaned, turn your focus to tuning the model initialization and
    training parameters. In your first training run, you accepted the default input
    sequence length (128) as an explicit value for `max_sequence_length` was not provided
    to the model. The BERT-base model can handle sequences of a maximum length of
    512\. As you increase `max_sequence_length` you may need to decrease `train_batch_size`
    and `eval_batch_size` to fit tensors into GPU memory, depending on the hardware
    available to you. You can do some exploration on the lengths of the comment text
    to find an optimal max length. Be mindful that at some point you’ll get diminishing
    returns, where longer training and evaluation times incurred by using larger sequences
    do not yield a significant improvement in model accuracy. For this example pick
    a `max_sequence_length` of 300, which is between the default of 128 and the model’s
    capacity. Also explicitly select `train_batch_size` and `eval_batch_size` to fit
    into GPU memory.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 文本清理完成后，将重点转向调整模型初始化和训练参数。在第一次训练运行中，由于未向模型提供`max_sequence_length`的显式值，您接受了默认的输入序列长度（128）。BERT-base
    模型可以处理最大长度为 512 的序列。随着`max_sequence_length`的增加，您可能需要减小`train_batch_size`和`eval_batch_size`以将张量适应
    GPU 内存，具体取决于您可用的硬件。您可以对评论文本的长度进行一些探索，以找到最佳的最大长度。请注意，某些时候您将获得收益递减，使用较大的序列导致更长的训练和评估时间，并不会显著提高模型的准确性。对于此示例，选择一个最大长度为
    300 的`max_sequence_length`，介于默认值 128 和模型容量之间。还要显式选择`train_batch_size`和`eval_batch_size`以适应
    GPU 内存。
- en: Warning
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: You’ll quickly realize your batch sizes are set too large if a GPU memory exception
    is displayed shortly after training or evaluation commences. And you don’t necessarily
    want to maximize the batch size based on this warning. The warning may only appear
    late in your training runs and ruin a long running training session. And larger
    isn’t always better for the `batch_size` parameter. Sometimes smaller batch sizes
    will help your training be a bit more stochastic (random) in its gradient descent.
    Being more random can sometimes help your model jump over ridges and saddle points
    in your the high dimensional nonconvex error surface it is trying to navigate.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在训练或评估开始后不久显示 GPU 内存异常，则会很快意识到批处理大小设置过大。您不一定想要基于此警告来最大化批处理大小。警告可能仅在训练运行后期出现，并破坏长时间的训练会话。对于`batch_size`参数，更大并不总是更好。有时，较小的批处理大小将有助于让您的训练在梯度下降中更具随机性。更随机有时可以帮助您的模型跳过高维度非凸误差表面上的峰值和鞍点。
- en: Recall that in your first fine-tuning run, the model trained for exactly one
    epoch. Your hunch that the model could have trained longer to achieve better results
    is likely correct. You want to find the sweet spot for the amount of training
    to do before the model overfits on the training samples. Configure options to
    enable evaluation during training so you can also set up the parameters for early
    stopping. The evaluation scores during training are used to inform early stopping.
    So set `evaluation_during_training=True` to enable it, and set `use_early_stopping=True`
    also. As the model learns to generalize, we expect oscillations in performance
    between evaluation steps, so you don’t want to stop training just because the
    accuracy declined from the previous value in the latest evaluation step. Configure
    the *patience* for early stopping, which is the number of consecutive evaluations
    without improvement (defined to be greater than some delta) at which to terminate
    the training. You’re going to set `early_stopping_patience=4` because you’re somewhat
    patient but you have your limits. Use `early_stopping_delta=0` because no amount
    of improvement is too small.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在第一次微调运行中，模型正好训练了一个时期。您认为模型可能需要更长时间的训练才能取得更好的结果的直觉很可能是正确的。您希望找到在模型对训练样本过拟合之前需要进行训练的数量的平衡点。配置选项以在训练期间启用评估，以便您还可以设置早停的参数。在训练期间的评估分数用于通知早停。因此，设置`evaluation_during_training=True`以启用它，还要设置`use_early_stopping=True`。随着模型学会泛化，我们预期在评估步骤之间的性能会出现波动，因此您不希望仅仅因为准确性从最新的评估步骤的上一个值下降而停止训练。配置早停的*耐心*，即连续几次评估没有改善（定义为大于某个增量）时终止训练的次数。您将设置`early_stopping_patience=4`，因为您有点耐心，但也有自己的极限。使用`early_stopping_delta=0`，因为没有任何改善量都不会太小。
- en: Saving these transformers models to disk repeatedly during training (e.g. after
    each evaluation phase or after each epoch) takes time and disk space. For this
    example, you’re looking to keep the *best* model generated during training, so
    specify `best_model_dir` to save your best-performing model. It’s convenient to
    save it to a location under the `output_dir` so all your training results are
    organized as you run more experiments on your own.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间重复将这些转换器模型保存到磁盘（例如，在每个评估阶段或每个时期之后）需要时间和磁盘空间。对于本示例，您希望保留在训练过程中生成的*最佳*模型，因此请指定`best_model_dir`来保存表现最佳的模型。将其保存到`output_dir`下的位置很方便，这样您运行更多实验时，所有的训练结果都可以组织起来。
- en: Listing 9.38 Setup parameters for evaluation during training and early stopping
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[在训练和早停期间设置评估参数](https://wiki.example.org/setup_parameters_for_evaluation_during_training_and_early_stopping)'
- en: '[PRE46]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Train the model by calling `model.train_model()`, as you did previously. One
    change is that you are now going to `evaluate_during_training` so you need to
    include an `eval_df` (your validation data set). This allows your training routine
    to estimate how well your model will perform in the real world while it is still
    training the model. If the validation accuracy starts to degrade for several (`early_stoping_patience`)
    epochs in a row, your model will stop the training so it doesn’t continue to get
    worse.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`model.train_model()`来训练模型，就像之前做的那样。一个变化是你现在要`evaluate_during_training`，所以你需要包括一个`eval_df`（你的验证数据集）。这允许您的训练例程在训练模型时估计您的模型在现实世界中的表现如何。如果验证准确性连续几个（`early_stoping_patience`）时期下降，您的模型将停止训练，以免继续恶化。
- en: Listing 9.39 Load pre-trained model and fine tune with early stopping
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[加载预训练模型并进行早停微调](https://wiki.example.org/load_pretrained_model_and_fine_tune_with_early_stopping)'
- en: '[PRE47]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Your *best* model was saved during training in the `best_model_dir`. It should
    go without saying that this is the model you want to use for inference. The evaluation
    code segment is updated to load the model by passing `best_model_dir` for the
    `model_name` parameter in the model class' constructor.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 您的*最佳*模型在训练期间保存在`best_model_dir`中。不用说，这是您想要用于推断的模型。评估代码段已更新，通过在模型类的构造函数中将`best_model_dir`传递给`model_name`参数来加载模型。
- en: Listing 9.40 Evaluation with the best model
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[评估最佳模型](https://wiki.example.org/evaluation_with_the_best_model)'
- en: '[PRE48]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now that’s looking better. A 0.989 accuracy puts us in contention with the top
    challenge solutions of early 2018\. And perhaps you think that 98.9% accuracy
    may be a little too good to be true. You’d be right. Someone fluent in German
    would need to dig into several of the translations to find all the translation
    errors your model is making. And the false negatives — test examples incorrectly
    marked as correct — would be even harder to find.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来更好了。0.989的准确率使我们有机会与2018年初的顶级挑战解决方案竞争。也许你认为98.9%的准确率可能有点太好以至于难以置信。你是对的。精通德语的人需要挖掘一些翻译以找到模型的所有翻译错误。而误判的负样本-被错误标记为正确的测试样本-会更难找到。
- en: If you’re like me, you probably don’t have a fluent German translator lying
    around. So here’s a quick example of a more English-focused translation application
    that you may be able to appreciate more, grammar checking and correcting. And
    even if you are still an English learner, you can appreciate the benefit of having
    a personalized tool to help you write. A personalized grammar checker may be your
    personal killer app that helps you develop strong communication skills and advance
    your NLP career.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我一样，可能没有一个流利的德语翻译者。所以这里有一个更注重英语的翻译应用的快速示例，你可能更能欣赏，即语法检查和修正。即使你还是一个英语学习者，你也会欣赏到拥有一个个性化工具来帮助你写作的好处。个性化语法检查器可能是帮助你发展强大沟通技巧和推进你的NLP职业的个人杀手级应用。
- en: 9.4 Test Yourself
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 自测题
- en: How is the input and output dimensionality of a transformer layer different
    from any other deep learning layer like CNN, RNN, or LSTM layers?
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其他深度学习层（如CNN，RNN或LSTM层）相比，转换器层的输入和输出维度有什么不同？
- en: How could you expand the information capacity of a transformer network like
    BERT or GPT-2?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何扩展像BERT或GPT-2这样的转换器网络的信息容量？
- en: What is a rule of thumb for estimating the information capacity required to
    get high accuracy on a particular labeled dataset?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计在特定标记数据集上获得高准确度所需的信息容量的经验法则是什么？
- en: What is a good measure of the relative information capacity of 2 deep learning
    networks?
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是衡量2个深度学习网络相对信息容量的好方法？
- en: What are some techniques for reducing the amount of labeled data required to
    train a transformer for a problem like summarization?
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有什么方法可以减少训练转换器所需的标记数据量，以解决摘要等问题？
- en: How do you measure the accuracy or loss of a summarizer or translator where
    there can be more than one right answer?
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何衡量摘要器或翻译器的准确性或损失，当可能存在不止一个正确答案时？
- en: 9.5 Summary
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 总结
- en: By keeping the inputs and outputs of each layer consistent, transformers gained
    their key superpower — infinite stackabilty
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过保持每层的输入和输出一致，转换器获得了它们的关键超能力-无限可堆叠性。
- en: 'Transformers combine three key innovations to achieve world-changing NLP power:
    BPE tokenization, multi-head attention, and positional encoding.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器结合了三个关键创新，以实现改变世界的NLP能力：BPE分词，多头注意力和位置编码。
- en: The GPT transformer architecture is the best choice for most text generation
    tasks such as translation and conversational chatbots
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT转换器架构是大多数文本生成任务（如翻译和会话式聊天机器人）的最佳选择。
- en: Despite being more than 5 years old (when this book was released) the BERT transformer
    model is still the right choice for most NLU problems.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管已经发布了5年以上（当本书发布时）BERT转换器模型仍然是大多数NLU问题的正确选择。
- en: If you chose a pretrained model that is efficient, you can fine-tune it to achieve
    competitive results for many difficult Kaggle problems, using only affordable
    hardware such as a laptop or free online GPU resources
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你选择了一个高效的预训练模型，你可以通过精调它来在许多困难的Kaggle问题上取得竞争力，只需使用像笔记本电脑或免费的在线GPU资源这样的经济设备。
- en: '[[1]](#_footnoteref_1) For months following ChatGPT’s public release, Dan Miessler
    spent almost half of his "Unsupervised Learning" podcasts discussing transformer-based
    tools such as InstructGPT, ChatGPT, Bard and Bing ( [https://danielmiessler.com/](danielmiessler.com.html))'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#_footnoteref_1) 在ChatGPT公开发布后的几个月里，丹·米斯勒几乎花了一半的“Unsupervised Learning”播客时间来讨论基于transformer的工具，如InstructGPT，ChatGPT，Bard和Bing（[https://danielmiessler.com/](danielmiessler.com.html)）'
- en: '[[2]](#_footnoteref_2) PineCone.io, Milvus.io, Vespa.ai, Vald.vdaas.org use
    transformers'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#_footnoteref_2)PineCone.io、Milvus.io、Vespa.ai、Vald.vdaas.org使用transformers'
- en: '[[3]](#_footnoteref_3) "Swiss cheese model" on Wikipedia ( [https://en.wikipedia.org/wiki/Swiss_cheese_model](wiki.html))'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#_footnoteref_3)维基百科上的“瑞士奶酪模型”（[https://en.wikipedia.org/wiki/Swiss_cheese_model](wiki.html)）'
- en: '[[4]](#_footnoteref_4) Sebastian Larson, an actual middle schooler, won our
    competition to develop Rori’s `mathtext` NLP algorithm ( [https://gitlab.com/tangibleai/community/team/-/tree/main/exercises/2-mathtext](exercises.html))'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4]](#_footnoteref_4) 一位名叫 Sebastian Larson 的中学生赢得了我们的比赛，开发了 Rori 的 `mathtext`
    自然语言处理算法（[https://gitlab.com/tangibleai/community/team/-/tree/main/exercises/2-mathtext]('
- en: '[[5]](#_footnoteref_5) All of Rori.AI’s NLP code is open source and available
    on Huggingface ( [https://huggingface.co/spaces/TangibleAI/mathtext-fastapi](TangibleAI.html)).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5]](#_footnoteref_5) Rori.AI的所有自然语言处理代码都是开源的，可以在 Huggingface 上获得（[https://huggingface.co/spaces/TangibleAI/mathtext-fastapi](TangibleAI.html)）'
- en: '[[6]](#_footnoteref_6) Vish built an transformer-based teaching assistant called
    Clevrly (clevrly.io)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#_footnoteref_6) Vish 构建了一个基于 transformer 的教学助理，名叫 Clevrly（clevrly.io）'
- en: '[[7]](#_footnoteref_7) Some of Vish’s fine tuned transformers are available
    on Huggingface ( [https://huggingface.co/clevrly](huggingface.co.html))'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#_footnoteref_7) Vish的一些精细调整的 transformers 可以在 Huggingface 上获得（[https://huggingface.co/clevrly](huggingface.co.html)）'
- en: '[[8]](#_footnoteref_8) Stats Stack Exchange answer ( [https://stats.stackexchange.com/a/422898/15974](422898.html))'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#_footnoteref_8) Stats Stack Exchange 的答案（[https://stats.stackexchange.com/a/422898/15974](422898.html)）'
- en: '[[9]](#_footnoteref_9) N-Gram Viewer query "embedding_NOUN" / "encoding_NOUN"
    ( [https://books.google.com/ngrams/graph?content=embedding_NOUN+%2F+encoding_NOUN&year_start=2010&year_end=2019&corpus=en-2019&smoothing=3](ngrams.html))'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#_footnoteref_9) N-Gram Viewer 查询 "embedding_NOUN" / "encoding_NOUN"（[https://books.google.com/ngrams/graph?content=embedding_NOUN+%2F+encoding_NOUN&year_start=2010&year_end=2019&corpus=en-2019&smoothing=3](ngrams.html)）'
- en: '[[10]](#_footnoteref_10) "Gentle Introduction to Transduction in Machine Learning"
    blog post on *Machine Learning Mastery* by Jason Brownlee 2017 ( [https://machinelearningmastery.com/transduction-in-machine-learning/](transduction-in-machine-learning.html))'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#_footnoteref_10) "机器学习中关于传导的简明介绍"，Jason Brownlee于2017年在 *Machine Learning
    Mastery* 博客上发布的文章（[https://machinelearningmastery.com/transduction-in-machine-learning/](transduction-in-machine-learning.html)）'
- en: '[[11]](#_footnoteref_11) Neural Machine Translation by Jointly Learning to
    Align and Translate: [https://arxiv.org/abs/1409.0473](abs.html)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#_footnoteref_11) 通过联合学习对齐和翻译实现的神经机器翻译：[https://arxiv.org/abs/1409.0473](abs.html)'
- en: '[[12]](#_footnoteref_12) "Attention Is All You Need" by Vaswani, Ashish et
    al. 2017 at Google Brain and Google Research ( [https://arxiv.org/abs/1706.03762](abs.html))'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#_footnoteref_12) "Attention Is All You Need"，Vaswani, Ashish等人于2017年于Google
    Brain和Google Research发表的论文（[https://arxiv.org/abs/1706.03762](abs.html)）'
- en: '[[13]](#_footnoteref_13) "Scaled dot product attention from scratch" by Jason
    Brownlee ( [https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/](how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras.html))'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#_footnoteref_13) "如何从头开始实现缩放点积注意力"，Jason Brownlee 的文章（[https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/](how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras.html)）'
- en: '[[14]](#_footnoteref_14) "Attention is all you Need" by Ashish Vaswani et al
    2017 ( [https://arxiv.org/abs/1706.03762](abs.html))'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#_footnoteref_14) "Attention is all you Need"，Ashish Vaswani等人于2017年发表的论文（[https://arxiv.org/abs/1706.03762](abs.html)）'
- en: '[[15]](#_footnoteref_15) [http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-using-attention-with-pytorch/](machine-translation-using-attention-with-pytorch.html)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15]](#_footnoteref_15) [http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-using-attention-with-pytorch/](machine-translation-using-attention-with-pytorch.html)'
- en: '[[16]](#_footnoteref_16) Pytorch Sequence-to-Sequence Modeling With nn.Transformer
    Tutorial: [https://simpletransformers.ai/docs/multi-label-classification/](multi-label-classification.html)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[[16]](#_footnoteref_16) Pytorch 序列到序列建模教程：[https://simpletransformers.ai/docs/multi-label-classification/](multi-label-classification.html)'
- en: '[[17]](#_footnoteref_17) List of ISO 639 language codes on Wikipedia ( [https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes](wiki.html))'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[[17]](#_footnoteref_17) 维基百科上的 ISO 639 语言代码列表（[https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes](wiki.html)）'
- en: '[[18]](#_footnoteref_18) Pytorch nn.Transformer source: [https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](modules.html)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18]](#_footnoteref_18) Pytorch nn.Transformer 源代码：[https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py](modules.html)'
- en: '[[19]](#_footnoteref_19) einops: [https://github.com/arogozhnikov/einops](arogozhnikov.html)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[[19]](#_footnoteref_19) einops：[https://github.com/arogozhnikov/einops](arogozhnikov.html)'
- en: '[[20]](#_footnoteref_20) Pytorch torch.nn.Transformer documentation: [https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html](generated.html)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[[20]](#_footnoteref_20) Pytorch torch.nn.Transformer 文档：[https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html](generated.html)'
- en: '[[21]](#_footnoteref_21) Pytorch nn.Module documentation: [https://pytorch.org/docs/stable/generated/torch.nn.Module.html](generated.html)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[[21]](#_footnoteref_21) PyTorch nn.Module 文档: [https://pytorch.org/docs/stable/generated/torch.nn.Module.html](generated.html)'
- en: '[[22]](#_footnoteref_22) Japanese StackExchange answer with counts of Japanese
    characters ( [https://japanese.stackexchange.com/a/65653/56506](65653.html))'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[[22]](#_footnoteref_22) Japanese StackExchange 上关于日语字符计数的回答 ( [https://japanese.stackexchange.com/a/65653/56506](65653.html))'
- en: '[[23]](#_footnoteref_23) Trevett,Ben - PyTorch Seq2Seq: [https://github.com/bentrevett/pytorch-seq2seq/](pytorch-seq2seq.html)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '[[23]](#_footnoteref_23) Trevett,Ben - PyTorch Seq2Seq: [https://github.com/bentrevett/pytorch-seq2seq/](pytorch-seq2seq.html)'
- en: '[[24]](#_footnoteref_24) Trevett,Ben - Attention Is All You Need Jupyter notebook:
    [https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb/](6%20-%20Attention%20is%20All%20You%20Need.ipynb.html)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24]](#_footnoteref_24) Trevett,Ben - 注意力就是一切 Jupyter 笔记本: [https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb/](6%20-%20Attention%20is%20All%20You%20Need.ipynb.html)'
- en: '[[25]](#_footnoteref_25) Trevett,Ben - Convolutional Sequence to Sequence Learning:
    [https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb/](5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb.html)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[[25]](#_footnoteref_25) Trevett,Ben - 卷积序列到序列学习: [https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb/](5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb.html)'
- en: '[[26]](#_footnoteref_26) BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding: [https://arxiv.org/abs/1810.04805/](1810.04805.html)
    (Devlin, Jacob et al. 2018)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[[26]](#_footnoteref_26) BERT：用于语言理解的深度双向变换器的预训练: [https://arxiv.org/abs/1810.04805/](1810.04805.html)
    (Devlin, Jacob 等人 2018)'
- en: '[[27]](#_footnoteref_27) "tensor2tensor" library on GitHub ( [https://github.com/tensorflow/tensor2tensor/](tensor2tensor.html)).'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[[27]](#_footnoteref_27) GitHub 上的 "tensor2tensor" 库 ( [https://github.com/tensorflow/tensor2tensor/](tensor2tensor.html)).'
- en: '[[28]](#_footnoteref_28) "BERT Fine-tuning With Cloud TPUS" Jupyter Notebook
    ( [https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb](colab.html)).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[[28]](#_footnoteref_28) "使用 Cloud TPUS 对 BERT 进行微调" 的 Jupyter 笔记本 ( [https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb](colab.html)).'
- en: '[[29]](#_footnoteref_29) pytorch-pretrained-bert on PyPi ( [https://pypi.org/project/pytorch-pretrained-bert/](pytorch-pretrained-bert.html)).'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[[29]](#_footnoteref_29) PyPi 上的 pytorch-pretrained-bert ( [https://pypi.org/project/pytorch-pretrained-bert/](pytorch-pretrained-bert.html)).'
- en: '[[30]](#_footnoteref_30) Hugging Face transformer models on Hugging Face Hub
    - ( [https://huggingface.co/transformers/](transformers.html)).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[[30]](#_footnoteref_30) Hugging Face Hub 上的 Hugging Face 变换器模型 - ( [https://huggingface.co/transformers/](transformers.html)).'
- en: '[[31]](#_footnoteref_31) Conversation AI: ( [https://conversationai.github.io/](conversationai.github.io.html))'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[[31]](#_footnoteref_31) Conversation AI: ( [https://conversationai.github.io/](conversationai.github.io.html))'
- en: '[[32]](#_footnoteref_32) Jigsaw toxic comment classification challenge on Kaggle
    ( [https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge](julian3833.html))'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[[32]](#_footnoteref_32) Kaggle 上的有毒评论分类挑战 ( [https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge](julian3833.html))'
- en: '[[33]](#_footnoteref_33) SciKit Learn documentation on multiclass and multioutput
    models( [https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification](modules.html))'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[[33]](#_footnoteref_33) SciKit Learn 多类别和多输出模型文档( [https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification](modules.html))'
- en: '[[34]](#_footnoteref_34) Simpletransformers Multi-Label Classification documentation
    ( [https://simpletransformers.ai/docs/multi-label-classification/](multi-label-classification.html))'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[[34]](#_footnoteref_34) Simpletransformers 多标签分类文档 ( [https://simpletransformers.ai/docs/multi-label-classification/](multi-label-classification.html))'
- en: '[[35]](#_footnoteref_35) See "Configuring a Simple Transformers Model" section
    of the following webpage for full list of options and their defaults: [https://simpletransformers.ai/docs/usage/](usage.html)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[[35]](#_footnoteref_35) 在以下网页的 "配置简单变换器模型" 部分查看完整的选项列表及其默认值：[https://simpletransformers.ai/docs/usage/](usage.html)'
- en: '[[36]](#_footnoteref_36) Final leader board from the Kaggle Toxic Comment Classification
    Challenge: [https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/leaderboard](jigsaw-toxic-comment-classification-challenge.html)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[[36]](#_footnoteref_36) Kaggle 有毒评论分类挑战的最终排行榜：[https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/leaderboard](jigsaw-toxic-comment-classification-challenge.html)'
