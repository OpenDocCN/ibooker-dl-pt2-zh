- en: 11 Information extraction and knowledge graphs (grounding)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 信息提取和知识图谱（基础）
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Extracting named entities from text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本中提取命名实体
- en: Understanding the structure of the sentence using dependency parsing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用依存解析理解句子的结构
- en: Converting a dependency tree into a knowledge (fact)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将依存树转换为知识（事实）
- en: Building a knowledge graph from text
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本构建知识图谱
- en: In the previous chapter (Chapter 10) you learned how to use large transformers
    to generate smart *sounding* words. But language models on their own are just
    faking it by predicting the next word that will *sound* reasonable to you. Your
    AI can’t reason about the real world until you give them access to facts and knowledge
    about the world. In Chapter 2 you learned how to do exactly this, but you didn’t
    know it then. You were able to tag tokens with their part of speech and their
    logical role in the meaning of a sentence (dependency tree). This old-fashioned
    token tagging algorithm is all you need to give your generative language models
    (AI) knowledge about the real world. The goal of this chapter is to teach your
    bot to understand what it reads. And you’ll put that understanding into a flexible
    data structure designed to store knowledge, known as *knowledge graph*. Then your
    bot can use that knowledge to make decisions and say smart stuff about the world.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章（第10章）中，你学会了如何使用大型transformers生成聪明的*听起来像*单词。但单独的语言模型只是通过预测下一个对你来说*听起来*合理的单词来进行欺骗。直到你为它们提供有关世界的事实和知识之前，你的AI才能推理真实世界。在第2章中，你学会了如何做到这一点，但当时你并不知道。你能够标记代币及其在句子意义中的逻辑角色（依存树）。这种老式的代币标记算法是为了给你的生成式语言模型（AI）提供关于真实世界的知识。本章的目标是教你的机器人理解它所读的内容。然后，你将理解放入一个旨在存储知识的灵活数据结构中，称为*知识图谱*。然后，你的机器人可以利用这些知识做出决策，并就世界发表聪明的言论。
- en: Correctly parsing your text into *entities* and discovering the *relations*
    between them is how you’ll go about extracting facts from the text. A *knowledge
    graph*, also called *knowledge database* (knowledge base) or a *semantic net*,
    is a database that stores knowledge as relationships between concepts. Though
    you can use a relational database to store the relations and concepts, sometimes
    it is more appropriate to use a *graph* data structure. The nodes in the graph
    would be *entities*, while the edges would be relations between these entities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正确地将文本解析为*实体*，并发现它们之间的*关系*，这是你从文本中提取事实的方法。 *知识图谱*，也称为*知识数据库*（知识库）或*语义网络*，是一个将知识存储为概念之间关系的数据库。虽然你可以使用关系型数据库存储关系和概念，但有时使用*图*数据结构更为合适。图中的节点将是*实体*，而边将是这些实体之间的关系。
- en: You can see an example of a knowledge graph in Figure [11.1](#figure-knowledge-graph).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图 [11.1](#figure-knowledge-graph) 中看到一个知识图谱示例。
- en: Figure 11.1 An example of a knowledge graph
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.1 知识图谱示例
- en: '![kg 150 biotech company graphviz](images/kg_150_biotech_company_graphviz.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![kg 150 生物技术公司 Graphviz](images/kg_150_biotech_company_graphviz.png)'
- en: Each fact you extract will create a new connection between the nodes of the
    graph - or possibly, create new nodes. This allows you to ask questions about
    the relationships between things using a query language such as GraphQL, Cypher
    or even SQL.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每个你提取的事实都会在图的节点之间创建一个新的连接 - 或者，可能会创建新的节点。这使得你可以使用诸如GraphQL、Cypher甚至SQL的查询语言来询问关系。
- en: And your algorithms can then do fact-checking, not only on the text written
    by humans but also text generated by your NLP pipeline or AI. Finally, your AI
    algorithms will be able to do introspection to let you know if what they are telling
    you might actually have some semblance of truth to it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你的算法可以对文本进行事实检查，不仅限于人类编写的文本，还包括你的NLP管道或AI生成的文本。最后，你的AI算法将能够自省，让你知道它们告诉你的内容是否实际上有些真实的相似之处。
- en: Your AI can use knowledge graphs to fill the *common sense knowledge* gap in
    large language models and perhaps live up to a little bit of the hype around LLMs
    and AI. This is the missing link in the NLP chain that you need to create true
    AI. And you can use a knowledge graph to programmatically generate text that makes
    sense because it is grounded in facts in your database. You can even infer new
    facts or *logical inferences* about the world that aren’t yet included in your
    knowledge base.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 AI 可以使用知识图谱来填补大型语言模型中的 *常识知识* 差距，也许有点符合关于 LLMs 和 AI 的炒作。这是你需要创建真正 AI 所需的
    NLP 链中缺失的环节。你可以使用知识图谱来以编程方式生成合理的文本，因为它基于你的数据库中的事实。你甚至可以推断出关于世界的新事实或 *逻辑推理*，这些事实尚未包含在你的知识库中。
- en: You may remember hearing about "inference" when people talk about forward propagation
    or prediction using deep learning models. A deep learning language model uses
    statistics to estimate or guess the next word in the text that you prompt it with.
    And deep learning researchers hope that one day, neural networks will be able
    to match the natural human ability to logically infer things and reason about
    the world. But this isn’t possible, because words don’t contain all the knowledge
    about the world that a machine would need to process to make factually correct
    inferences. So you’re going to use a tried and true logical inference approach
    called "symbolic reasoning."
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们谈论前向传播或使用深度学习模型进行预测时，你可能会听说过“推理”的概念。深度学习语言模型利用统计学来估计或猜测你输入的文本中的下一个词。深度学习研究人员希望有一天，神经网络能够达到与自然人类推理和思考世界的能力相匹配。但这是不可能的，因为单词并不包含机器需要处理的关于世界的全部知识，以做出事实正确的推断。因此，你将使用一种经过验证和可靠的逻辑推理方法，称为“符号推理”。
- en: If you’re familiar with the concept of a compiler then you may want to think
    of the dependency tree as a parse tree or abstract syntax tree (AST). An AST defines
    the logic of a machine language expression or program. You’re going to use the
    natural language dependency tree to extract the logical relations within natural
    language text. And this logic will help you *ground* the statistical deep learning
    models so they can do more than merely make statistical "guesses" about the world
    as they did in previous chapters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉编译器的概念，那么你可能会想到依赖树作为解析树或抽象语法树（AST）。AST 定义了机器语言表达式或程序的逻辑。你将使用自然语言依赖树来提取自然语言文本中的逻辑关系。这种逻辑将帮助你
    *基于事实* 地对统计深度学习模型进行推理，使它们能够做更多不仅仅是像之前章节中那样做统计学上的“猜测”世界的工作。
- en: 11.1 Grounding
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 推理
- en: Once you have a knowledge graph, your chatbots and AI agents will have a way
    to correctly reason about the world in an explainable way. And if you can extract
    facts from the text your deep learning model generates, you can check to see if
    that text agrees with the knowledge you’ve collected in your knowledge graph.
    This is called *grounding* when you maintain a knowledge graph and then use it
    to double-check the facts and reasoning in the generated text. When you ground
    your language model you attach it to some ground truth facts about the world.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了一个知识图谱，你的聊天机器人和 AI 代理就会有一种以可解释的方式正确推理世界的方法。如果你能从你的深度学习模型生成的文本中提取事实，你可以检查该文本是否与你在知识图谱中收集的知识一致。当你维护一个知识图谱并使用它来对生成的文本中的事实和推理进行双重检查时，这被称为
    *推理*。当你将语言模型与关于世界的一些基本事实联系起来时，你就在为其打下基础。
- en: Grounding can also benefit your NLP pipeline in other ways. Using a knowledge
    graph for the reasoning part of your algorithm can free up your language model
    to do what it does best — generate plausible, grammatical text. So you can fine
    tune your language model to have the tone that you want, without trying to build
    a chameleon that pretends to understand and reason about the world. And your knowledge
    graph can be designed to contain just the facts about a world that you want your
    AI to understand — whether it is facts about the real world that you have in mind
    or some fictional world that you are creating. By separating the reasoning from
    the language you can create an NLP pipeline that both sounds correct and *is*
    correct.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 推理也可以以其他方式使你的 NLP 流程受益。在算法的推理部分使用知识图谱可以释放你的语言模型，让它做它最擅长的事情——生成合理的、符合语法的文本。因此，你可以微调你的语言模型，让它具有你想要的语气，而不是试图构建一个假装理解和推理世界的变色龙。你的知识图谱可以被设计成只包含你希望你的
    AI 理解的世界的事实——无论是你心目中的真实世界的事实还是你正在创建的某个虚构世界的事实。通过将推理与语言分离，你可以创建一个既听起来正确又 *是* 正确的
    NLP 流程。
- en: There are a few other terms that are often used when referring to this grounding
    process. Sometimes it’s referred to as *symbolic reasoning* as opposed to the
    probabilistic reasoning of machine learning models. *First order logic* is one
    system for symbolic reasoning.^([[1](#_footnotedef_1 "View footnote.")]) This
    was the preferred approach to building expert systems and theorem provers before
    the data and processing power was available for machine learning and deep learning.
    It’s also called Good Old Fashioned AI or GOFAI, pronounced "Go Fie". GOFAI is
    back in fashion as researchers attempt to build generally intelligent systems
    that we can rely on to make important decisions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在提及这个接地过程时，还有一些其他术语经常被使用。有时它被称为*符号推理*，与机器学习模型的概率推理相对。*一阶逻辑*是符号推理的一种系统。这是在数据和处理能力不足以支持机器学习和深度学习之前构建专家系统和定理证明器的首选方法。它也被称为Good
    Old Fashioned AI或GOFAI，发音为“高菲”。随着研究人员试图构建我们可以依赖于做出重要决策的普遍智能系统，GOFAI重新流行起来。
- en: Another advantage of grounding your NLP pipeline is that you can use the facts
    in your knowledge base to *explain* its reasoning. If you ask an ungrounded LLM
    to explain why it said something unreasonable, it will just keep digging a hole
    for itself (and you) by making up more and more nonsense reasons. You saw this
    in the previous chapters when LLMs confidently hallucinated (fabricated) nonexistent
    but plausible references and fictional people to explain where they got their
    nonsense from. The key to creating AI you can trust is to put a floor of reason
    underneath it using a knowledge graph. The first, and perhaps most important algorithm
    in this grounding process is *knowledge extraction*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的自然语言处理（NLP）流水线接地的另一个优势是，您可以使用知识库中的事实来*解释*其推理过程。如果您要求一个未接地的语言模型解释为什么会说出不合理的话，它只会继续为自己（和您）挖一个越来越深的坑，通过编造越来越多的无稽理由。在前几章中，您已经看到了这一点，当语言模型自信地产生（虚构）不存在但合理的引用和虚构人物来解释他们的胡言乱语的来源时。创造一个您可以信任的人工智能的关键是在其下放置一个理性的地板，使用知识图谱。这个接地过程中的第一个，也许是最重要的算法是*知识提取*。
- en: '11.1.1 Going old-fashioned: information extraction with patterns'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 传统方法：模式匹配的信息提取
- en: In this chapter, we’ll also get back to methods you see in the very early chapters,
    like regular expressions. Why return to hard-coded (manually composed) regular
    expressions and patterns? Because your statistical or data-driven approach to
    NLP has limits. You want your machine learning pipeline to be able to do some
    basic things, such as answer logical questions or perform actions such as scheduling
    meetings based on NLP instructions. And machine learning falls flat here.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们还将回顾您在前几章中看到的方法，比如正则表达式。为什么要回到硬编码（手动组成的）正则表达式和模式？因为您对自然语言处理的统计或数据驱动方法有限制。您希望您的机器学习流水线能够做一些基本的事情，比如回答逻辑问题或执行根据自然语言指令安排会议。而机器学习在这方面效果不佳。
- en: Plus, as you’ll see here, you can define a compact set of condition checks (a
    regular expression) to extract key bits of information from a natural language
    string. And it can work for a broad range of problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，正如您在这里所看到的，您可以定义一组紧凑的条件检查（正则表达式），以从自然语言字符串中提取关键信息。它可以适用于广泛的问题。
- en: Pattern matching (and regular expressions) continues to be the state-of-the-art
    approach for information extraction and related tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模式匹配（以及正则表达式）仍然是信息提取及相关任务的最先进方法。
- en: Enough of a preamble. Let’s start the journey of knowledge extraction and grounding!
    But we have to cover an important step in processing your documents, to generate
    a proper input to your knowledge extraction pipeline. We need to break our text
    into smaller units.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 先说正事吧。让我们开始知识提取和接地之旅吧！但在处理您的文档之前，我们必须覆盖一个重要步骤，以生成适当的输入到您的知识提取流水线。我们需要将文本分解成较小的单元。
- en: '11.2 First things first: segmenting your text into sentences'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 先解决重要问题：将您的文本分割成句子
- en: Before you can dive into extracting your knowledge from raw text, you need to
    break it down into chunks that your pipeline can work on. Document "chunking"
    is useful for creating semi-structured data about documents that can make it easier
    to search, filter, and sort documents for information retrieval. And for information
    extraction, if you’re extracting relations to build a knowledge base such as NELL
    or Freebase (more about them in a bit), you need to break it into parts that are
    likely to contain a fact or two. When you divide natural language text into meaningful
    pieces, it’s called *segmentation*. The resulting segments can be phrases, sentences,
    quotes, paragraphs, or even entire sections of a long document.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始从原始文本中提取知识之前，您需要将其分解为管道可以处理的块。文档“分块”对于创建关于文档的半结构化数据非常有用，这样可以更容易地搜索、过滤和对信息检索的文档进行排序。而对于信息提取，如果您正在提取关系以构建知识库，如
    NELL 或 Freebase（稍后将更详细介绍），则需要将其分成可能包含一个或两个事实的部分。当您将自然语言文本分解为有意义的片段时，这称为*分割*。生成的片段可以是短语、句子、引用、段落，甚至是长文档的整个部分。
- en: Sentences are the most common chunk for most information extraction problems.
    Sentences are usually punctuated with one of a few symbols (".", "?", "!", or
    a new line). And grammatically correct English language sentences must contain
    a subject (noun) and a verb, which means they’ll usually have at least one fact
    worth extracting. Sentences are often self-contained packets of meaning that don’t
    rely too much on preceding text to convey most of their information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数信息提取问题来说，句子是最常见的块。句子通常用几个符号（“。”、“？”、“！”或换行符）标点。而语法上正确的英语句子必须包含一个主语（名词）和一个动词，这意味着它们通常至少有一个值得提取的事实。句子通常是自包含的意义包，大部分信息不太依赖于前文来传达。
- en: In addition to facilitating information extraction, you can flag some of those
    statements and sentences as being part of a dialog or being suitable for replies
    in a dialog. Using a sentence segmenter (sentencizer) allows you to train your
    chatbot on longer texts, such as books. Choosing those books appropriately gives
    your chatbot a more literary, intelligent style than if you trained it purely
    on Twitter streams or IRC chats. And these books give your chatbot access to a
    much broader set of training documents to build its common sense knowledge about
    the world.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了促进信息提取外，您还可以将其中一些陈述和句子标记为对话的一部分或适合对话中的回复。使用句子分段器（sentencizer）可以让您在更长的文本（如书籍）上训练您的聊天机器人。选择这些书籍可以使您的聊天机器人比纯粹在
    Twitter 流或 IRC 聊天上训练它更具文学性和智能风格。而且这些书籍为您的聊天机器人提供了一个更广泛的训练文档集，以建立关于世界的常识知识。
- en: Sentence segmentation is the first step in your information extraction pipeline.
    It helps isolate facts from each other. Most sentences express a single coherent
    thought. And many of those thoughts are about real things in the real world. And,
    most importantly, all the natural languages have sentences or logically cohesive
    sections of text of some sort. And all languages have a widely shared process
    for generating them (a set of grammar "rules" or habits).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分割是信息提取管道的第一步。它有助于将事实与其他事实隔离开来。大多数句子表达一个单一的连贯思想。而且许多这些思想都是关于现实世界中的真实事物。最重要的是，所有的自然语言都有句子或逻辑上连贯的文本部分。并且所有语言都有一个广泛共享的生成它们的过程（一组语法“规则”或习惯）。
- en: But segmenting text and identifying sentence boundaries is a bit trickier than
    you might think. In English, for example, no single punctuation mark or sequence
    of characters always marks the end of a sentence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，分割文本并识别句子边界比你想象的要棘手。例如，在英语中，没有单个标点符号或字符序列总是标记句子的结束。
- en: 11.2.1 Why won’t split('.!?') work?
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 为什么 split('.!?') 不能起作用？
- en: 'Even a human reader might have trouble finding an appropriate sentence boundary
    within each of the following quotes. Here are some example sentences that most
    humans would be tempted to split into multiple sentences:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至即使是人类读者在以下每个引号内找到适当的句子边界也可能有困难。以下是大多数人类可能会尝试拆分为多个句子的一些示例句子：
- en: '*She yelled "It’s right here!" but I kept looking for a sentence boundary anyway.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*她喊道：“就在这里！”但我仍然在寻找一个句子的边界。*'
- en: '*I stared dumbfounded on, as things like "How did I get here?", "Where am I?",
    "Am I alive?" flittered across the screen.*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*我目瞪口呆地盯着，像“我是怎么到这里的？”、“我在哪里？”、“我还活着吗？”在屏幕上飞来飞去。*'
- en: '*The author wrote "''I don’t think it’s conscious.'' Turing said."*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者写道：“''我不认为它是有意识的。'' 图灵说。”*'
- en: Even a human reader would have trouble finding an appropriate sentence boundary
    within each of these quotes and nested quotes and stories within stories.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是人类读者也会很难在每个引号和嵌套引号以及故事中找到合适的句子边界。
- en: More sentence segmentation "edge cases" such as this are available at tm-town.com.
    ^([[2](#_footnotedef_2 "View footnote.")])
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于句子分割的“边缘情况”可以在 tm-town.com 上找到。^([[2](#_footnotedef_2 "查看脚注。")])
- en: Technical text is particularly difficult to segment into sentences because engineers,
    scientists, and mathematicians tend to use periods and exclamation points to signify
    a lot of things besides the end of a sentence. When we tried to find the sentence
    boundaries in this book, we had to manually correct several of the extracted sentences.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 技术文本特别难以分割成句子，因为工程师、科学家和数学家倾向于使用句号和感叹号来表示除了句子结束以外的很多事情。当我们试图找出本书中的句子边界时，我们不得不手动纠正了几个提取出的句子。
- en: 'If only we wrote English like telegrams, with a "STOP" or unique punctuation
    mark at the end of each sentence. But since we don’t, you’ll need some more sophisticated
    NLP than just `split(''.!?'')`. Hopefully, you’re already imagining a solution
    in your head. If so, it’s probably based on one of the two approaches to NLP you’ve
    used throughout this book:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们写英语像发电报一样，每个句子的结尾都有一个“STOP”或独特的标点符号。但由于我们不是这样做的，你将需要比 `split('.!?')` 更复杂的自然语言处理。希望你已经在脑海中想象出了一个解决方案。如果是这样，那么它可能基于你在本书中使用过的两种
    NLP 方法之一：
- en: Manually programmed algorithms (regular expressions and pattern-matching)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动编程算法（正则表达式和模式匹配）
- en: Statistical models (data-based models or machine learning)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型（基于数据的模型或机器学习）
- en: We use the sentence segmentation problem to revisit these two approaches by
    showing you how to use regular expressions as well as more advanced methods to
    find sentence boundaries. And you’ll use the text of this book as a training and
    test set to show you some of the challenges. Fortunately, you haven’t inserted
    any newlines within sentences, to manually "wrap" text like in newspaper column
    layouts. Otherwise, the problem would be even more difficult. In fact, much of
    the source text for this book, in ASCIIdoc format, has been written with "old-school"
    sentence separators (two spaces after the end of every sentence), or with each
    sentence on a separate line. This was so we could use this book as a training
    and test set for your segmenters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用句子分割问题来重新审视这两种方法，向你展示如何使用正则表达式以及更先进的方法来找到句子边界。你将使用本书的文本作为训练和测试集来展示一些挑战。幸运的是，你没有在句子内部插入换行符，像报纸栏目布局中那样手动“换行”文本。否则，问题会更加困难。事实上，这本书的大部分源文本，都是以
    ASCIIdoc 格式编写的，带有“老式”的句子分隔符（每个句子结束后有两个空格），或者每个句子都在单独的一行上。这样做是为了让我们能够将这本书用作你的分段器的训练和测试集。
- en: 11.2.2 Sentence segmentation with regular expressions
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 使用正则表达式进行句子分割
- en: Regular expressions are just a shorthand way of expressing the tree of “if…​then”
    rules (regular grammar rules) for finding character patterns in strings of characters.
    As we mentioned in Chapters 1 and 2, regular expressions (regular grammars) are
    a particularly succinct way to specify the structure of a finite state machine.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式只是一种简写的方式，用来表达字符串中寻找字符模式的“如果...则...”规则（正则语法规则）。正如我们在第1章和第2章中提到的，正则表达式（正则语法）是一种特别简洁的方式，用来指定有限状态机的结构。
- en: 'Any formal grammar can be used by a machine in two ways:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 任何形式的语法都可以被机器用两种方式使用：
- en: To recognize "matches" to that grammar
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于识别与该语法匹配的内容
- en: To generate a new sequence of symbols
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个新的符号序列
- en: Not only can you use patterns (regular expressions) for extracting information
    from natural language, but you can also use them to generate strings that match
    that pattern! Check out the `rstr` (short for "random string") package if you
    ever need to generate example strings that match a regular expresssion.^([[3](#_footnotedef_3
    "View footnote.")]) for some of your information extraction patterns here.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你不仅可以用模式（正则表达式）从自然语言中提取信息，还可以用它们生成与该模式匹配的字符串！如果你需要生成符合正则表达式的示例字符串，可以查看 `rstr`（缩写为“随机字符串”）包。^([[3](#_footnotedef_3
    "查看脚注。")])这里是你的一些信息提取模式。
- en: This formal grammar and finite state machine approach to pattern matching has
    some other awesome features. A true finite state machine is guaranteed to eventually
    stop (halt) in a finite number of steps. So if you use a regular expression as
    your pattern matcher you know that you will always receive an answer to your question
    about whether you’ve found a match in your string or not. It will never get caught
    in a perpetual loop…​ as long as you don’t "cheat" and use look-aheads or look-backs
    in your regular expressions. And because a regular expression is deterministic
    it always returns a match or non-match. It will never give you less than 100%
    confidence or probability of there being a match.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式化的语法和有限状态机方法对模式匹配还有其他一些令人惊叹的特点。真正的有限状态机保证在有限步骤内最终停止（停机）。所以如果你使用正则表达式作为你的模式匹配器，你知道你总是会得到关于你是否在你的字符串中找到了一个匹配的答案。它永远不会陷入永久循环……只要你不
    "作弊" 并在你的正则表达式中使用向前看或向后看。而且因为正则表达式是确定性的，它总是返回匹配或非匹配。它永远不会给你不到 100% 的置信度或匹配的概率。
- en: So you’ll stick to regular expressions that don’t require these "look-back"
    or "look-ahead" cheats. You’ll make sure your regular expression matcher processes
    each character and moves ahead to the next character only if it matches — sort
    of like a strict train conductor walking through the seats checking tickets. If
    you don’t have one, the conductor stops and declares that there’s a problem, a
    mismatch, and he refuses to go on, or look ahead or behind you until he resolves
    the problem. There are no "go-backs" or "do-overs" for train passengers, or for
    strict regular expressions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你会坚持使用不需要这些 "向后看" 或 "向前看" 的正则表达式。你会确保你的正则表达式匹配器处理每个字符，并且仅在它匹配时向前移动到下一个字符 -
    就像一个严格的列车售票员走过座位检查票一样。如果没有，售票员会停下来宣布有问题，不匹配，并且他拒绝继续前进，或者向你前后看直到他解决问题。对于列车乘客或严格的正则表达式来说，没有
    "回头" 或 "重做"。
- en: 'Our regex or FSM has only one purpose in this case: identifying sentence boundaries.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的正则表达式或有限状态机在这种情况下只有一个目的：识别句子边界。
- en: If you do a web search for sentence segmenters,^([[4](#_footnotedef_4 "View
    footnote.")]) you’re likely to be pointed to various regular expressions intended
    to capture the most common sentence boundaries. Here are some of them, combined
    and enhanced to give you a fast, general-purpose sentence segmenter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你搜索句子分割器，^([[4](#_footnotedef_4 "View footnote.")]) 你可能会被指向旨在捕捉最常见句子边界的各种正则表达式。以下是一些结合和增强以给你一个快速、通用的句子分割器的正则表达式。
- en: The following regex would work with a few "normal" sentences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下正则表达式将适用于一些 "正常" 句子。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unfortunately, this `re.split` approach gobbles up (consumes) the sentence-terminating
    token. Notice how the ellipsis and period at the end of "Hello World" are missing
    in the returned list? The splitter only returns the sentence terminator if it
    is the last character in a document or string. A regular expression that assumes
    your sentences will end in white space does do a good job of ignoring the trickery
    of periods within doubly-nested quotes, though:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种 `re.split` 方法会吞噬掉句子终止符。注意一下 "Hello World" 结尾的省略号和句号在返回列表中消失了吗？分割器只在它是文档或字符串中的最后一个字符时才返回句子终止符。一个假设你的句子将以空白结束的正则表达式确实可以很好地忽略双重嵌套引号中的句号的伎俩，但：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: See how the returned list contains only one sentence without messing up the
    quote within a quote? Unfortunately, this regex pattern also ignores periods in
    quotes that terminate an actual sentence, so any sentences that end in a quote
    will be joined with the subsequent sentence. This may reduce the accuracy of the
    information extraction steps that follow your sentence segmenter if they rely
    on accurate sentence splits.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 看到返回的列表只包含一个句子而不会弄乱引号内的引用吗？不幸的是，这个正则表达式模式也会忽略引号中终止实际句子的句号，因此任何以引号结尾的句子都将与随后的句子连接起来。如果后续的信息提取步骤依赖于准确的句子分割，这可能会降低其准确性。
- en: 'What about text messages and tweets with abbreviated text, informal punctuation,
    and emojis? Hurried humans squish sentences together, leaving no space surrounding
    periods. The following regex could deal with periods in SMS messages that have
    letters on either side and it would safely skip over numerical values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那么文本消息和带有缩写文本、非正式标点和表情符号的 tweets 呢？匆忙的人类会将句子挤在一起，句号周围没有空格。以下正则表达式可以处理具有字母的短信消息中的句号，并且它会安全地跳过数值：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Even combining these two regexes into a monstrosity such as `r'?<!\d)\.|\.(?!\d|([!.?]+)[\s$]+'`
    is not enough to get all the sentences right. If you parsed the AciiDoc text for
    the manuscript of this chapter, it would make several mistakes.^([[5](#_footnotedef_5
    "View footnote.")]) You’d have to add a lot more "look-ahead" and "look-back"
    to the regex pattern to improve its accuracy as a sentence segmenter. You were
    warned!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 即使将这两个正则表达式组合成一个像`r'?<!\d)\.|\.(?!\d|([!.?]+)[\s$]+'`这样的怪物也不足以让所有的句子都正确。如果你解析了本章手稿的AciiDoc文本，它会犯几个错误。你需要在正则表达式模式中添加更多的“向前查找”和“向后查找”来提高其作为句子分割器的准确性。你已经被警告了！
- en: If looking for all the edge cases and designing rules around them feel cumbersome,
    that’s because it is. A better approach for sentence segmentation is to use a
    machine learning algorithm trained on a labeled set of sentences. Often a logistic
    regression or a single-layer neural network (perceptron) is enough.^([[6](#_footnotedef_6
    "View footnote.")]) Several packages contain such a statistical model you can
    use to improve your sentence segmenter. SpaCy ^([[7](#_footnotedef_7 "View footnote.")])
    and Punkt (in NLTK) ^([[8](#_footnotedef_8 "View footnote.")]) both have good
    sentence segmenters. You can guess which one we use.^([[9](#_footnotedef_9 "View
    footnote.")])
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查找所有边缘情况并为其设计规则感觉繁琐，那是因为确实如此。句子分割的更好方法是使用在标记句子集上训练过的机器学习算法。通常情况下，逻辑回归或单层神经网络（感知器）就足够了。有几个包含这样的统计模型，你可以用来改进你的句子分割器。SpaCy和Punkt（在NLTK中）都有很好的句子分割器。你可以猜猜我们使用哪一个。
- en: 'SpaCy has a sentence segmenter built into the default parser pipeline that
    is your best bet for mission-critical applications. It is almost always the most
    accurate, robust, performant option. Here is how you segment text into sentences
    with spaCy:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy在默认解析器管道中内置了一个句子分割器，这是你在关键任务应用中的最佳选择。它几乎总是最准确、最健壮、性能最好的选择。以下是如何使用spaCy将文本分割成句子：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: SpaCy’s accuracy relies on dependency parsing. A dependency parser identifies
    how each word depends on the other words in a sentence diagram, like the one you
    learned about in grammar school (elementary school). Having this dependency structure
    along with the token embeddings helps the spacy sentence segmenter deal with ambiguous
    punctuation and capitalization accurately. But all that sophistication takes processing
    power and time. Speed is not important when you are only processing a few sentences,
    but what if you wanted to parse the AsciiDoc manuscript for Chapter 9 of this
    book?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy的准确性依赖于依赖解析。依赖解析器识别每个词在句子图中如何依赖于其他词，就像你在文法学校（小学）学到的那样。拥有这种依赖结构以及令牌嵌入帮助SpaCy句子分割器准确处理模糊的标点和大写字母。但所有这些复杂性都需要处理能力和时间。当你只处理几个句子时，速度并不重要，但如果你想要解析本书第9章的AsciiDoc手稿呢？
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Wow, that *is* slow! SpaCy is about 700 times slower than a regular expression.
    If you have millions of documents instead of just this one chapter of text, then
    you will probably need to do something different. For example, on a medical records
    parsing project we needed to switch to a regular expression tokenizer and sentence
    segmenter. The regex parser reduced our processing time from weeks to days, but
    it also reduced the accuracy of the rest of our NLP pipeline.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这真是太慢了！SpaCy比正则表达式慢了大约700倍。如果你有数百万个文档而不只是这一章的文本，那么你可能需要做一些不同的事情。例如，在一个医疗记录解析项目中，我们需要切换到正则表达式标记器和句子分割器。正则表达式解析器将我们的处理时间从几周缩短到几天，但也降低了我们NLP管道的准确性。
- en: SpaCy has now (as of 2023) caught up with our need for customization. SpaCy
    now allows you to enable or disable any piece of the pipeline you like. And it
    has a statistical sentence segmenter that doesn’t rely on the other elements of
    the spaCy pipeline such as the word embeddings and named entity recognizer. When
    you want to speed up your spaCy NLP pipeline you can remove all the elements you
    do not need and add back just the pipeline elements you want.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy现在（截至2023年）已经满足了我们对定制化的需求。SpaCy现在允许你启用或禁用任何你喜欢的管道部分。它还有一个统计句子分段器，不依赖于SpaCy管道的其他元素，比如词嵌入和命名实体识别器。当你想加速你的SpaCy
    NLP管道时，你可以移除所有你不需要的元素，然后只添加你想要的管道元素回来。
- en: First, check out the pipeline attribute of a spacy NLP pipeline to see what
    is there by default. Then use the `exclude` keyword argument to `load` clean out
    the pipeline.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查spacy NLP管道的`pipeline`属性，查看默认值中包含什么。然后使用`exclude`关键字参数来`load`清理管道。
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that you’ve cleaned your pipes, you can add back the important pieces that
    you need. For this speed run through Chapter 9, your NLP pipeline will only need
    the `senter` pipeline element. The `senter` pipeline is the statistical sentence
    segmenter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经清理了管道，现在可以添加回所需的重要部分。在本章的快速运行中，您的NLP管道只需要`senter`管道元素。`senter`管道是统计句子分割器。
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That is a significant time saver — 2.3 vs 11.5 seconds on an 8-core i7 laptop.
    The statistical sentence segmenter is about 5x faster than the full spaCy pipeline.
    The regular expression approach will still be much faster, but the statistical
    sentence segmenter will be more accurate. You can estimate the accuracy of these
    two algorithms by comparing the lists of sentences to see if they produced the
    same splits. This will not tell you which of the two approaches is correctly segmenting
    a particular text line, but at least you will see when the two spaCy pipelines
    agree.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的时间节省器-在8核i7笔记本电脑上为2.3秒与11.5秒。统计句子分割器比完整的spaCy管道快约5倍。正则表达式方法仍将快得多，但统计句子分割器将更准确。您可以通过比较句子列表估算这两种算法的准确性，以查看它们是否产生了相同的分割。这不会告诉你哪种方法正在正确地分段特定的文本行，但至少你会看到两个spaCy管道什么时候达成一致。
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So it appears that about 93% of the sentences of this book were segmented the
    same way with the slow and fast pipelines. Look at some example segmentations
    to see which one might be better for your use cases.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该书约93%的句子通过慢速和快速管道进行分段。请查看一些示例分段，以确定哪种方法适合您的用例。
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It looks like that opening sentence with the leading underscore character (\_)
    is bit more difficult for the faster statistical segmenter. So you probably want
    to use the full spacy model whenever you are parsing Markdown or AsciiDoc text
    files. The formatting characters will confuse a statistic segmenter if it has
    not been trained on similar text.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来以前导下划线字符（_）开头的句子对于更快的统计分割器要困难一些。因此，您在解析Markdown或AsciiDoc文本文件时可能需要使用完整的spacy模型。如果统计分割器没有接受过类似文本的训练，那么格式字符会使它混淆。
- en: 11.2.3 Sentence semantics
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3句子语义
- en: Now that you have your text segmented into sentences containing discrete facts,
    you are ready to start extracting those facts and giving them structure in a knowledge
    graph. To get started, create a heatmap of the BERT embeddings of all the sentences
    of chapter 9.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的文本已被分割成包含离散事实的句子，您已经准备好开始在知识图谱中将这些事实提取出来并给它们构建结构。要开始，创建第九章所有句子的BERT嵌入热力图。
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Take a look at this DataFrame. It has columns that contain tags for each line
    of text. You can use the tags to filter out the lines that you don’t want to process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个DataFrame，它具有包含每行文本标签的列。您可以使用这些标签来过滤掉不想处理的行。
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now you can use the 'is_body' tag to process all the sentences within the body
    of the manuscript. These lines should contain mostly complete sentences so that
    you can compare them semantically to each other to see a heatmap of how often
    we say similar things. Now that you understand transformers such as BERT, you
    can use it to give you even more meaningful representations of this text than
    what SpaCy creates.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用'is_body'标记来处理手稿正文内的所有句子。这些行应该包含大部分完整的句子，以便您可以语义地将它们与其他语句进行比较，以查看我们有多经常说类似的话的热力图。现在您已经了解了像BERT这样的转换器，可以使用它来为您提供比SpaCy创建的更有意义的文本表示。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The MiniLM model is a multipurpose BERT transformer that has been optimized
    and "distilled." It provides high accuracy and speed and should not take long
    to download from Hugging Face. Now you have 689 passages of text (mostly individual
    sentences). The MiniLM language model has embedded them into a 384-dimensional
    vector space. As you learned in Chapter 6, embedding vector semantic similarity
    is computed with the normalized dot product.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MiniLM模型是一个经过优化和“蒸馏”的通用BERT转换器。它提供高精度和速度，并且从Hugging Face下载不需要很长时间。现在，您有689个文本段落（主要是单个句子）。MiniLM语言模型已将它们嵌入到384维向量空间中。正如您在第6章中了解的那样，嵌入向量语义相似度计算使用归一化点积。
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you have a square matrix, one row and one column for each passage of text
    and its BERT embedding vector. And the value in each cell of the matrix contains
    the cosine similarity between that pair of embedding vectors. If you label the
    columns and rows with the first few characters of the text passages, that will
    make it easier to interpret all this data with a heatmap.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有一个方阵，每个文本段和它的BERT嵌入向量有一行和一列。矩阵的每个单元格中的值包含该嵌入向量对之间的余弦相似度。如果用文本段的前几个字符标记列和行，那将使得用热图解释所有这些数据更容易。
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As usual, the cosine similarity ranges between zero and one and most values
    are less than .85 (85%) unless they are for sentences that say essentially the
    same thing. So 85% would be a good threshold for identifying redundant statements
    that might be consolidated or reworded to improve the quality of the writing in
    a book such as this. Here’s what the heatmap of these cosine similarity values
    looks like.^([[10](#_footnotedef_10 "View footnote.")])
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，余弦相似度在零到一之间变化，大多数值都小于0.85（85%），除非它们是说基本相同事情的句子。因此，85%将是识别可能被合并或重新措辞以提高本书写作质量的冗余语句的良好阈值。这是这些余弦相似度值的热图。^([[10](#_footnotedef_10
    "查看脚注。")])
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![ch9 heatmap](images/ch9-heatmap.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![第9章热图](images/ch9-heatmap.png)'
- en: 'There seems to be only a small square of white-hot similarity about 60% of
    the way through Chapter 9, perhaps near the line that begins "Epoch: 13…​". This
    line corresponds to the output text from a transformer training run, so it is
    not surprising that a natural language model would see these machine-generated
    lines as semantically similar. After all, the BERT language model is just saying
    to you "It’s all just Greek to me." The regular expressions in the scripts for
    tagging lines of the manuscript as natural language or software blocks are not
    working very well.^([[11](#_footnotedef_11 "View footnote.")]) If you improved
    the regular expressions in `nlpia2.text_processing.extractors` you could have
    your heatmap skip over these irrelevant code lines. And AsciiDoc files are structured
    data, so they should be machine-readable without any regular expression guesswork…​
    if only there were an up-to-date Python library for parsing AsciiDoc text.^([[12](#_footnotedef_12
    "View footnote.")])'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '在第9章大约60%的位置似乎只有一个小小的白热相似度方块，也许在以“Epoch: 13…”开头的那行附近。这行对应于transformers训练运行的输出文本，因此自然语言模型会将这些机器生成的行视为语义上相似是不奇怪的。毕竟，BERT语言模型只是对你说“对我来说这都是希腊语。”手稿标记行为自然语言或软件块的正则表达式工作得不是很好。如果你改进了`nlpia2.text_processing.extractors`中的正则表达式，你可以让你的热图跳过这些不相关的代码行。而且AsciiDoc文件是结构化数据，所以它们应该是机器可读的，不需要任何正则表达式的猜测…如果只有一个用于解析AsciiDoc文本的最新的Python库。^([[12](#_footnotedef_12
    "查看脚注。")])'
- en: Here’s another heatmap of the Chapter 3 text. Do you see anything interesting
    here?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第三章文本的另一个热图。你在这里看到了什么有趣的东西吗？
- en: '![ch3 heatmap](images/ch3-heatmap.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![第3章热图](images/ch3-heatmap.png)'
- en: Notice the giant dark red cross (*gray* cross in print) spanning the entire
    chapter? This means the text in the middle of that cross is very different from
    all the other text in the chapter. Can you guess why? That section contains a
    sentence that starts with "Ernqnov…​", an encrypted line from the "Zen of Python"
    (`import this`). And the tiny white rectangle at that location shows that each
    line of that encrypted poem is very similar to the lines near it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意跨越整个章节的巨大深红色十字（*打印时为灰色*）？这意味着该十字中间的文本与该章节中的所有其他文本非常不同。你能猜到为什么吗？那个部分包含以“Ernqnov…”开头的一个句子，这是“Python禅”的加密行（`import
    this`）。而那个位置的小小白色矩形表明该加密诗的每一行与其附近的行非常相似。
- en: A semantic heatmap is one way to find structure in your text data, but if you
    want to create knowledge from text you will need to go further. Your next step
    is to use the vector representations of sentences to create a "graph" of connections
    between entities. Entities in the real world are related by facts. Our mental
    model of the world is a belief network or a *knowledge graph* — a newtwork of
    connections between all the things (entities) you know something about.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 语义热图是在文本数据中找到结构的一种方式，但如果你想从文本中创造知识，你需要更进一步。你的下一步是利用句子的向量表示来创建实体之间的连接“图”。现实世界中的实体是通过事实相关的。我们对世界的心理模型是一个信念网络或*知识图*
    - 所有你知道一些事情的东西（实体）之间的连接的网络。
- en: 11.3 A knowledge extraction pipeline
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 知识提取管道
- en: Once you have your sentences organized you can start extracting concepts and
    relations from natural language text. For example, imagine a chatbot user says
    "Remind me to read AI Index on Monday."^([[13](#_footnotedef_13 "View footnote.")])
    You’d like that statement to trigger a calendar entry or alarm for the next Monday
    after the current date. Easier said than done.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您组织好句子，就可以开始从自然语言文本中提取概念和关系。例如，想象一下，聊天机器人用户说"提醒我在星期一阅读 AI Index。"^([[13](#_footnotedef_13
    "View footnote.")]) 您希望这个声明能触发一个日历条目或在当前日期之后的下一个星期一设置一个闹钟。说起来容易做起来难。
- en: 'To trigger correct actions with natural language you need something like an
    NLU pipeline or parser that is a little less fuzzy than a transformer or large
    language model. You need to know that "me" represents a particular kind of named
    entity: a person. Named entities are natural language terms or n-grams that refer
    to a particular thing in the real world, such as a person, place or thing. Sound
    familiar? In English grammar, the part of speech (POS) for a person, place or
    thing is "noun". So you’ll see that the POS tag that spaCy associates with the
    tokens for a named entity is "NOUN".'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过自然语言触发正确的动作，您需要类似 NLU 流水线或解析器的东西，它比transformers或大型语言模型稍微清晰一些。您需要知道"我"表示一种特定类型的命名实体：一个人。命名实体是自然语言术语或
    n-gram，指的是现实世界中的特定事物，如人、地方或物品。听起来熟悉吗？在英语语法中，一个人、地方或物体的词性 (POS) 是"名词"。因此，您会发现 spaCy
    与命名实体的标记所关联的标记是"NOUN"。
- en: And the chatbot should know that it can expand or *resolve* that word by replacing
    it with that person’s username or other identifying information. You’d also need
    your chatbot to recognize that "aiindex.org" is an abbreviated URL, which is a
    named entity - a name of a specific instance of something, like a website or company.
    And you need to know that a normalized spelling of this particular kind of named
    entity might be " [http://aiindex.org](.html) ", " [https://aiindex.org](.html)
    ", or maybe even " [https://www.aiindex.org](.html) ". Likewise, you need your
    chatbot to recognize that Monday is one of the days of the week (another kind
    of named entity called an "event") and be able to find it on the calendar.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 并且聊天机器人应该知道它可以通过替换该单词与该人的用户名或其他识别信息来扩展或*解析*它。您还需要让聊天机器人识别"aiindex.org"是一个缩写的
    URL，它是一个命名实体-一个特定实例的名称，比如一个网站或公司的名称。您还需要知道这种特定类型的命名实体的规范化拼写可能是"[http://aiindex.org](.html)"，"[https://aiindex.org](.html)"，甚至是"[https://www.aiindex.org](.html)"。同样，您需要让您的聊天机器人识别星期一是一周的某一天（称为"事件"的另一种命名实体）并能够在日历上找到它。
- en: For the chatbot to respond properly to that simple request, you also need it
    to extract the relation between the named entity "me" and the command "remind."
    You’d even need to recognize the implied subject of the sentence, "you", referring
    to the chatbot, another person named entity. And you need to teach the chatbot
    that reminders happen in the future, so it should find the soonest upcoming Monday
    to create the reminder.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使聊天机器人能够正确地回应这个简单的请求，您还需要它来提取命名实体"我"和命令"提醒"之间的关系。您甚至需要识别出句子的暗示主语"你"，指的是聊天机器人，另一个命名实体。而且您还需要教聊天机器人提醒事件发生在未来，因此它应该找到最接近的下一个星期一来创建提醒。
- en: And that’s just a simple use case. You can construct a graph from scratch using
    your own common sense knowledge or the domain knowledge that you want your AI
    to know about. But if you can extract knowledge from text you can build much larger
    knowledge graphs much quicker. Plus, you will need this algorithm to double-check
    any text generated by your language models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 而这仅仅是一个简单的使用案例。您可以使用自己的常识知识或者您希望您的 AI 知道的领域知识从零开始构建一个图。但是，如果您能从文本中提取知识，您可以更快地构建更大的知识图谱。此外，您还需要这个算法来验证由您的语言模型生成的任何文本。
- en: 'Knowledge extraction requires four main steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 知识提取需要四个主要步骤：
- en: Figure 11.2 Four stages of knowledge extraction
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2 知识提取的四个阶段
- en: '![knowledge graph extraction drawio](images/knowledge-graph-extraction_drawio.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![知识图谱提取示意图](images/knowledge-graph-extraction_drawio.png)'
- en: 'Fortunately, the spaCy language models include the building blocks for knowledge
    extraction: named entity recognition, coreference resolution, and relation extraction.
    You only need to know how to combine the results of each of these steps to connect
    the pieces together. Let’s look at each stage separately by looking at an article
    about Timnit Gebru, a thought leader in AI ethics. We’ll continue using the spaCy
    nlp model we initialized in the previous section.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，spaCy 语言模型包含了知识提取的构建块：命名实体识别、指代消解和关系提取。你只需要知道如何将每个步骤的结果组合起来，将这些碎片连接在一起。让我们分别看看每个阶段，通过查看关于
    AI 伦理思想领袖 Timnit Gebru 的一篇文章来继续使用我们在前一节中初始化的 spaCy nlp 模型。
- en: Let’s start by downloading the Wikipedia article about Timnit Gebru.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载维基百科关于 Timnit Gebru 的文章。
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Have you heard of Timnit Gebru before? She’s famous among people in your area
    of interest and she’s written several influential papers:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你之前听说过 Timnit Gebru 吗？她在你感兴趣的领域中很有名，并且撰写了几篇有影响力的论文：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: That’s a pretty interesting research paper title. It certainly seems like something
    her bosses would be interested in publishing. But you aren’t interested in reading
    all of Wikipedia to find interesting tidbits about Stochastic Parrots and AI ethics
    experts such as Timnit Gebru. An information extraction pipeline can automatically
    recognize interesting named entities (people, places, things, and even dates and
    times). And if you want to support her, your NLP pipeline will be able to recognize
    mentions of her hidden behind pronouns in X messages (tweets).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个相当有趣的研究论文标题。这肯定是她的老板们有兴趣发表的东西。但你并不想阅读整个维基百科来找到关于随机鹦鹉和 AI 伦理专家 Timnit Gebru
    的有趣片段。一个信息提取管道可以自动识别有趣的命名实体（人、地点、事物，甚至是日期和时间）。如果你想支持她，你的 NLP 管道将能够识别在 X 条信息（推文）中隐藏在代词后面的提及。
- en: 11.4 Entity Recognition
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 实体识别
- en: The first step in extracting knowledge about some *thing* is to find the *things*
    that you want to know about. The most important things in natural language text
    are the names of people, places, and things. In linguistics named things are called
    "named entities." These are not just names - they might be things like dates,
    locations, and any piece of information that can be placed into your knowledge
    graph.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 提取有关某 *事物* 的知识的第一步是找到你想了解的 *事物*。自然语言文本中最重要的东西是人名、地点和事物的名称。在语言学中，这些被称为“命名实体”。这些不仅仅是名称
    - 它们可能是诸如日期、地点和任何可以放入你的知识图中的信息。
- en: As with sentences, you can go two ways about the task of Named Entity Recognition
    (NER) - using pattern-matching, and using the neural approach.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与句子一样，你可以有两种方式来处理命名实体识别（NER）的任务 - 使用模式匹配和使用神经方法。
- en: 'You’ll discover that there are cases in which regular expressions are as precise,
    or even more precise, than neural networks. Here are some keystone bits of quantitative
    information that are worth the effort of "hand-crafted" regular expressions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，有些情况下，正则表达式的精度甚至比神经网络更高。以下是一些值得投入“手工制作”正则表达式的关键性定量信息：
- en: GPS locations
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPS 位置
- en: Dates
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期
- en: Prices
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格
- en: Numbers
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字
- en: Let’s make a quick detour to learn how to extract such numerical data in the
    next section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节学习如何提取这样的数字数据时，快速地绕个弯。
- en: '11.4.1 Pattern-based entity recognition: extracting GPS locations'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.1 基于模式的实体识别：提取 GPS 位置
- en: GPS locations are typical of the kinds of numerical data you’ll want to extract
    from text using regular expressions. GPS locations come in pairs of numerical
    values for latitude and longitude. They sometimes also include a third number
    for altitude or height above sea level, but you’ll ignore that for now. Let’s
    just extract decimal latitude/longitude pairs, expressed in degrees. This will
    work for many Google Maps URLs. Though URLs are not technically natural language,
    they are often part of unstructured text data, and you’d like to extract this
    bit of information, so your chatbot can know about places as well as things.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: GPS 位置是你希望使用正则表达式从文本中提取的数值数据的典型示例。GPS 位置由纬度和经度的数值对组成。它们有时还包括第三个数字表示海拔或高度，但你暂时会忽略这个。让我们只提取用度数表示的十进制纬度/经度对。这对许多
    Google 地图 URL 都适用。尽管 URL 不属于技术上的自然语言，但它们经常是非结构化文本数据的一部分，你希望提取这一部分信息，以便你的聊天机器人也能了解地点和事物。
- en: Let’s use your decimal number pattern from previous examples, but let’s be more
    restrictive and make sure the value is within the valid range for latitude (+/-
    90 deg) and longitude (+/- 180 deg). You can’t go any farther north than the North
    Pole (+90 deg) or farther south than the South Pole (-90 deg). And if you sail
    from Greenwich England 180 deg east (+180 deg longitude), you’ll reach the date
    line, where you’re also 180 deg west (-180 deg) from Greenwich.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前示例中的十进制数模式，但是让我们更为严格，以确保值在有效的纬度（±90度）和经度（±180度）范围内。您不能到达比北极更北的任何地方（+90度），也不能到达南极比更南的任何地方（-90度）。如果你从英国的格林威治出发东行180度（+180度经度），你会到达日期变更线，那儿也是距离格林威治180度西经度（-180度）。
- en: Listing 11.1 Regular expression for GPS coordinates
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列出了GPS坐标的正则表达式11.1
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Numerical data is pretty easy to extract, especially if the numbers are part
    of a machine-readable string. URLs and other machine-readable strings put numbers
    such as latitude and longitude in a predictable order, format, and units to make
    things easy for us.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数字数据很容易提取，特别是如果数字是机器可读的格式的一部分。URL和其他机器可读的字符串将纬度和经度等数字以可预测的顺序、格式和单位排列，使我们的工作更易于进行。
- en: However, if we want to extract people’s names, nationalities, places and other
    things that don’t have a standard format, things become much more complicated.
    We can of course account for all the names, locations, and organizations possible.
    But keeping such a collection up to date would be a tremendously laborious task.
    For this, we’ll need the neural approach.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想提取人名、国籍、地名和其他没有标准格式的内容，情况会变得更加复杂。当然，我们可以考虑所有可能的名称、位置和组织，但是维护这样的集合将是一项巨大的工作。因此，我们需要神经网络的方法。
- en: 11.4.2 Named entity recognition with spaCy
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.2 使用spaCy进行命名实体识别。
- en: Because NER is just a foundational task, you can imagine researchers have started
    trying to do it efficiently way before Neural Nets.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因为命名实体识别（NER）只是一个基础任务，所以在神经网络出现之前，研究人员已经开始尝试高效地完成它。
- en: However, the neural networks gave a huge boost to how fast and accurate NER
    can be performed on a text. Note that recognizing and categorizing named entities
    is not as straightforward as you might think. One of the common challenges of
    NER is *segmentation*, or defining boundaries of the named entity (is "New York"
    one named entity or two separate ones?) Another, even trickier one, is categorizing
    the type of the entity. For example, the name Washington can be used to signify
    a person (such as the writer Washington Irving), a location (Washington DC), an
    organization (Washington Post) and even a sports team (as in "Washington won two
    games in the last season").
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，神经网络大大提高了在文本上执行NER的速度和准确性。请注意，识别和分类命名实体并不像您想象的那样简单。命名实体识别的一个常见挑战是 *分段* ，即定义命名实体的边界（例如“纽约”是一个命名实体还是两个不同的实体？）另一个更加棘手的挑战是分类实体的类型。例如，姓名华盛顿可以用于表示人（如作家华盛顿·欧文）、地点（华盛顿特区）、组织机构（《华盛顿邮报》）甚至运动队（如在“华盛顿在上赛季赢了两场比赛”中）。
- en: So you can see how the *context* of the entity - both the words that came before
    it and after it, potentially much later in the sentence - matters. That’s why
    the popular approaches to NER with neural networks include multi-level CNNs, and
    bi-directional transformers such as BERT, or bi-directional LSTMs. The last one,
    combined with a technique called Conditional Random Weights (CRF) is what spaCy
    uses in its named entity recognition module.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到实体的 *上下文* - 包括它前面和后面的单词，可能远在句子之后 - 很重要。这就是为什么使用多级CNN和双向转换器（如BERT或双向LSTM）进行NER的流行方法，以及与称为条件随机场（CRF）的技术相结合，是spaCy在命名实体识别模块中使用的方法。
- en: Of course, you don’t have to know how to build neural networks in order to extract
    the named entities from a text. The 'ents' attribute of a `doc` object that gets
    created once you run spaCy on a text contains a list of all those named entities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您不必知道如何构建神经网络才能从文本中提取命名实体。在运行spaCy处理文本后创建的 `doc` 对象的 'ents' 属性包含了所有这些命名实体的列表。
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The challenge of named entity recognition is closely related to a more basic
    problem - part-of-speech (POS) tagging. To recognize named entities in the sentence,
    you need to know which part of speech each word belongs to. In English grammar,
    the *part of speech* (POS) for a person, place or thing is "noun". And your named
    entity will often be a proper noun - a noun that refers to a *particular* person,
    place or thing in the real world. And the part of speech tag for relations is
    a *verb*. The verb tokens will be used to connect the named entities to each other
    as the edges in your knowledge graph.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别的挑战与一个更基本的问题——词性（POS）标记密切相关。为了识别句子中的命名实体，你需要知道每个单词属于哪个词性。在英语语法中，人、地点或物体的词性（POS）是"noun"。而你的命名实体通常会是一个专有名词——一个指代现实世界中的*特定*人、地点或物体的名词。而与关系相关的词性标记是一个*动词*。动词标记将用于将命名实体连接到知识图谱中的边缘。
- en: Part-of-speech tagging is also crucial to the next stage in our pipeline - dependency
    parsing. To determine the relationships between different entities inside the
    sentence, we will need to recognize the verbs in our sentence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标记在我们流水线的下一个阶段——依存解析中也非常重要。为了确定句子中不同实体之间的关系，我们需要识别出动词。
- en: Luckily, spaCy already did that for you the moment you fed the text to it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，spaCy在你提供文本的那一刻已经为你完成了这一切。
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Can you make sense of this? PUNCT, NOUN and VERB are pretty self-explanatory;
    and you can guess that PROPN stands for Proper Noun. But what about CCONJ? Luckily,
    you can let spaCy explain it to you.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你能理解这个吗？PUNCT、NOUN和VERB都很容易理解；你可以猜到PROPN代表Proper Noun（专有名词）。但CCONJ是什么意思呢？幸运的是，你可以让spaCy为你解释它。
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Another tool spaCy gives you is the `tag_` property of each token. While the
    `pos_` tag gives you the part of speech or a particular token, the `tag_` gives
    you more information and details about the token. Let’s see an example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy给你的另一个工具是每个标记的`tag_`属性。尽管`pos_`标记为你提供了特定标记的词性，但`tag_`标记为你提供了更多关于标记的信息和细节。让我们看一个例子：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Wow, this looks much more cryptic. You can vaguely intuit the connection between
    PROPN and NNP, but what is VBZ?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，看起来更加神秘。你可以隐约地感觉到PROPN和NNP之间的关联，但VBZ是什么意思呢？
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That’s for sure much more information, albeit served in a more cryptical form.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这无疑是更多信息，尽管以更加神秘的形式呈现。
- en: Let’s bring all the information about your tokens together in one table.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把关于你的标记的所有信息都汇总在一张表中。
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now you have a function you can use to extract the tags you are interested in
    for any sentence or text (document). If you coerce a list of dictionaries into
    a DataFrame you will be able to see the sequence of tokens and tags side by side.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有一个函数，可以用它来从任何句子或文本（文档）中提取你感兴趣的标签。如果你将一个字典列表强制转换成一个DataFrame，你就能够看到标记的序列和标签并排在一起。
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You are already familiar with the 'POS' and 'TAG' labels for tokens. The fourth
    column 'ENT_TYPE', gives you information about the type of the named entity that
    token is a part of. Many named entities span several tokens, such as "Timnit Gebru"
    with spans two tokens. You can see that the small spaCy model didn’t do that well;
    it missed Timnit Gebru as a named entity at the beginning of the text. And when
    spaCy did finally recognize it towards the end of the Wikipedia article, it labeled
    its entity type as "organization."
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你对标记的'POS'和'TAG'标签已经很熟悉了。第四列'ENT_TYPE'为你提供了关于该标记所属的命名实体类型的信息。许多命名实体跨越多个标记，比如"Timnit
    Gebru"跨越两个标记。你可以看到小的spaCy模型做得不太好；它在文章开头没有正确识别出Timnit Gebru作为一个命名实体。而当spaCy最终在维基百科文章的末尾识别出它时，它将其实体类型标记为"组织"。
- en: A larger spaCy model should be able to improve your accuracy a little bit, especially
    for words that aren’t very common in the datasets used to train spaCy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更大的spaCy模型应该能够在一定程度上提高你的准确性，特别是对于在训练spaCy时不太常见的词汇。
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This looks better! "Timnit Gebru" is now correctly classified as a `PERSON`,
    and "Wikimedia" is properly tagged as `ORG` (organization). So this will usually
    be the first algorithm in your knowledge extraction pipeline, the spaCy language
    model that tokenizes your text and tags each token with the linguistic features
    you need for knowledge extraction.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来更好！"Timnit Gebru"现在被正确分类为`PERSON`，"Wikimedia"被正确标记为`ORG`（组织）。所以通常这将是你知识提取流水线中的第一个算法，即spaCy语言模型，它对你的文本进行标记并给每个标记的语言特征。
- en: Once you understand how a named entity recognizer works, you can expand the
    kinds of nouns and noun phrases you want to recognize and include them in your
    knowledge graph. This can help generalize your knowledge graph and create a more
    generally intelligent NLP pipeline.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解了命名实体识别器的工作原理，你就可以扩展你想要识别的名词和名词短语的种类，并将它们包含在你的知识图中。这可以帮助泛化你的知识图，并创建一个更普遍的智能NLP管道。
- en: But you have yet to use the last column in your DataFrame of token tags, `DEP`
    (dependency). The `DEP` tag indicates the token’s role in the dependency tree.
    Before you move on to dependency parsing and relation extraction, you need to
    learn how to deal with step 2 of the knowledge extraction pipeline, coreference
    resolution.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但你还没有使用DataFrame中标记令牌的最后一列，即`DEP`（依赖关系）。`DEP`标记指示令牌在依赖关系树中的角色。在进行依赖关系分析和关系提取之前，你需要学习如何处理知识抽取管道的第2步，即指代消解。
- en: 11.5 Coreference Resolution
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 指代消解
- en: Imagine you’re running NER on a text, and you obtain the list of entities that
    the model has recognized. On closer inspection, you realize over half of them
    are duplicates because they’re referring to the same terms! This is where *Coreference
    resolution* comes in handy because it identifies all the mentions of a noun in
    a sentence. This will consolidate mentions of the same *things* in your knowledge
    graph instead of creating redundant nodes and edges and potentially creating incorrect
    relations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在对一段文本进行NER，然后你获得了模型识别出的实体列表。仔细检查后，你意识到超过一半的实体是重复的，因为它们指代了相同的术语！这就是*指代消解*派上用场的地方，因为它识别了句子中名词的所有提及。这将在你的知识图中合并相同*事物*的提及，而不是创建冗余的节点和边缘，并可能创建错误的关系。
- en: 'Can you see the coreferences to "Timnit Gebru" in this sentence about that
    paper and her bosses:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇关于那篇论文和她的老板的句子中，你能看到对"Timnit Gebru"的指代吗：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As a human, you can understand that "Gebru", "she" and "her" all relate. But
    it’s trickier for a machine to recognize that, especially if "she" is mentioned
    before "Gebru" (a phenomenon called *cataphora*).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，你可以理解"Gebru"，"she"和"her"之间的关系。但对于机器来说，识别这一点就更加困难，特别是如果"she"在"Gebru"之前提及（这种现象称为*前行指代*）。
- en: 'And that’s a relatively simple case! Consider this sentence: "The city councilmen
    refused the demonstrators a permit because they feared violence". Who does "they"
    in the sentence refer to? Our common sense tells us that it refers to the "city
    councilmen" and the answer seems to be easy for us, but this task of identifying
    mentions using common sense is surprisingly difficult for deep learning models.
    This task is called the Winograd schema challenge, or a "common-sense reasoning"
    or "common-sense inference" problem.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这还是一个相对简单的情况！考虑这个句子："市议员拒绝了示威者的许可证，因为他们担心暴力"。在这句话中，"they"指的是谁？我们的常识告诉我们，它指的是"市议员"，对我们来说答案似乎很容易，但对于深度学习模型来说，使用常识识别提及是非常困难的。这个任务被称为温诺格拉德模式挑战，或者称为"常识推理"或"常识推断"问题。
- en: Let’s see how NLP deals with this difficult NLP task. Deep problems call for
    deep learning!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看NLP如何处理这个困难的NLP任务。深刻的问题需要深度学习！
- en: 11.5.1 Coreference resolution with spaCy
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 使用spaCy进行指代消解
- en: As of this writing NeuralCoref 4.0 was the fastest and most accurate entity
    resolver available in the open-source community. As the name suggests NeuralCoref
    uses a deep learning neural network (transformer) to resolve coreferences to named
    entities. SpaCy incorporated transformers and NeuralCoref into its "Universe"
    collection of pipelines and models. NeuralCoref uses the original spaCy pipelines
    for `POS` tagging, named entity recognition, and extracting *coreferences* (secondary
    mentions of entities) in the text. It then takes the words surrounding each mention
    of an entity and feeds them into a feed-forward neural network or transformer
    to compute a score estimating whether each pair of mentions refer to the same
    object (entity). Comparing these scores is how the network resolves what each
    mention refers to.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本文时，NeuralCoref 4.0是开源社区中最快、最准确的实体解析器。正如其名称所示，NeuralCoref使用深度学习神经网络（transformers）来解析指称实体的指代关系。SpaCy将transformers和NeuralCoref整合到其"Universe"管道和模型集合中。NeuralCoref使用原始的SpaCy管道进行`POS`标记、命名实体识别，并提取文本中的*指代*（实体的次要提及）。然后，它将每个实体提及周围的单词输入到前馈神经网络或transformers中，计算一个估计值，判断每对提及是否指向同一个对象（实体）。通过比较这些得分，网络可以确定每个提及指向的内容。
- en: The `spacy-experimental` package includes coreference resolution algorithms
    within the `CoreferenceResolver` class, but to use NeuralCoref directly you will
    need to install and import the `coreferee` package. The original NeuralCoref is
    no longer actively maintained but spaCy has ported the algorithms to the `coreferee`
    package which works as a custom pipeline within spaCy. You will also need to download
    a transformer-based spaCy language model to use the `coreferee` pipeline.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`spacy-experimental`包包括`CoreferenceResolver`类内的指代消解算法，但要直接使用NeuralCoref，您需要安装并导入`coreferee`包。原始的NeuralCoref已不再积极维护，但spaCy已将算法移植到`coreferee`包中，该包作为spaCy内的自定义管道运行。您还需要下载基于transformer的spaCy语言模型来使用`coreferee`管道。'
- en: Like other spacy language models, you must first download "en_core_web_trf"
    before you can `load` and run it. The `trf` suffix indicates that this language
    model is a recent addition to the spaCy toolbox that incorporates a *transformer*
    neural network into the pipeline. This is a very large language model, so you
    probably don’t want to run the `cli.download()` function any more than you need
    to.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 像其他spacy语言模型一样，您必须先下载"en_core_web_trf"才能`load`并运行它。`trf`后缀表示这个语言模型是spaCy工具箱的最新添加，它将一个*transformer*神经网络整合到管道中。这是一个非常庞大的语言模型，因此您可能不想运行`cli.download()`函数的次数超过必要的次数。
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So this pipeline was able to find 2 *coreference chains* that link mentions
    of entities together. These two chains represent two distinct real-world objects,
    "Gebru" and "advice". The "Gebru" token at position 13 is linked to the three
    "she" pronouns at positions 16, 26 and 34\. The "advice" token is linked to the
    word "it" at position 56.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该管道能够找到2个*指代链*，将实体的提及链接在一起。这两个链表示两个不同的现实世界对象，"Gebru"和"advice"。位置13的"Gebru"标记与位置16、26和34的三个"she"代词相连。"advice"标记与位置56的单词"it"相连。
- en: So now you have consolidated all the mentions of Gebru in this single sentence
    from Wikipedia, and you can use those coreferences to extract important relations
    and facts about her.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经将维基百科中对Gebru的所有提及汇总到了这一句中，您可以利用这些指代消解来提取有关她的重要关系和事实。
- en: 11.5.2 Entity name normalization
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 实体名称规范化
- en: Closely related to coreference resolution is the issue of *normalization* of
    entities. The normalized representation of an entity is usually a string, even
    for numerical information such as dates. For example, the normalized ISO format
    for Timnit Gebru’s date of birth would be "1983-05-13". A normalized representation
    for entities enables your knowledge base to connect all the different things that
    happened in the world on that same date to that same node (entity) in your graph.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与指代消解密切相关的是实体的*规范化*问题。实体的规范化表示通常是一个字符串，即使是数字信息如日期也是如此。例如，Timnit Gebru的出生日期的规范化ISO格式将是"1983-05-13"。对实体进行规范化表示使您的知识库能够将世界上在同一日期发生的所有不同事件连接到图中的同一节点（实体）。
- en: You’d do the same for other named entities. You’d correct the spelling of words
    and attempt to resolve ambiguities for names of objects, animals, people, places,
    and so on. For example, San Francisco may be referred to, in different places
    as "San Fran", "SF", "'Frisco" or "Fog City". Normalization of named entities
    ensures that spelling and naming variations don’t pollute your vocabulary of entity
    names with confounding, redundant names.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您对其他命名实体也会执行相同的操作。您会更正单词的拼写，并尝试解决对象、动物、人物、地点等名称的歧义。例如，旧金山可能在不同的地方被称为"San Fran"、"SF"、"'Frisco"或"Fog
    City"。命名实体的规范化确保拼写和命名变体不会使您的实体名称词汇受到混淆、冗余名称的污染。
- en: A knowledge graph should normalize each kind of entity the same way, to prevent
    multiple distinct entities of the same type from sharing the same "name." You
    don’t want multiple person name entries in your database referring to the same
    physical person. Even more importantly, the normalization should be applied consistently — both
    when you write new facts to the knowledge base or when you read or query the knowledge
    base.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图应以相同方式规范化每种类型的实体，以防止同一类型的多个不同实体共享相同的"名称"。您不希望数据库中有多个人名条目指向同一物理人。更重要的是，规范化应该一致应用——无论是在向知识库写入新事实时还是在读取或查询知识库时。
- en: If you decide to change the normalization approach after the database has been
    populated, the data for existing entities in the knowledge should be "migrated",
    or altered, to adhere to the new normalization scheme. Schemaless databases (key-value
    stores), like the ones used to store knowledge graphs or knowledge bases, are
    not free from the migration responsibilities of relational databases. After all,
    schemaless databases are interface wrappers for relational databases under the
    hood.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定在数据库填充后更改归一化方法，则应“迁移”或修改知识库中现有实体的数据以符合新的归一化方案。无模式数据库（键值存储）例如用于存储知识图或知识库的数据库也不免于关系型数据库的迁移责任。毕竟，无模式数据库是关系型数据库的接口包装器。
- en: 11.6 Dependency Parsing
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 依存句法分析
- en: In the previous section, you learned how to recognize and tag named entities
    in text. Now you’ll learn how to find relationships between these entities. A
    typical sentence may contain several named entities of various types, such as
    geographic entities, organizations, people, political entities, times (including
    dates), artifacts, events, and natural phenomena. And a sentence can contain several
    *relations*, too — facts about the relationship between the named entities in
    the sentence
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学会了如何识别和标记文本中的命名实体。现在你将学习如何找到这些实体之间的关系。一个典型的句子可能包含多个不同类型的命名实体，如地理实体、组织机构、人物、政治实体、时间（包括日期）、物件、事件和自然现象。同时句子中可能也包含多个*关系*，这些关系是在句中命名实体之间关系的事实。
- en: 'NLP researchers have identified two separate problems or models that can be
    used to identify how the words in a sentence work together to create meaning:
    *dependency parsing* and *constituency parsing*. *Dependency parsing* will give
    your NLP pipelines the ability to diagram sentences like you learned to do in
    grammar school (elementary school). And these tree data structures give your model
    a representation of the logic and grammar of a sentence. This will help your applications
    and bots become a bit smarter about how they interpret sentences and act on them.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: NLP研究人员确定了可以用于识别句子中单词如何共同创造含义的两个单独的问题或模型：*依存句法分析*和*成分句法分析*。*依存句法分析*将为你的NLP流水线提供像你在语法学校（小学）学习的语句图表一样的图表，这些树形数据结构为你的模型提供了一个句子的逻辑和语法的表示。这将帮助你的应用程序和机器人变得更加聪明，以更好解释句子并对其进行操作。
- en: '*Constituency parsing* is another technique, and it’s concerned with identifying
    the *constituent subphrases* in a sentence. While dependency parsing deals with
    relationships between words, constituency parsing aims to parse a sentence into
    a series of constituents. These constituents can be, for example, a noun phrase
    ("My new computer") or a verb phrase ("has memory issues"). Its approach is more
    top-down, trying to iteratively break constituents into smaller units and relationships
    between them. Though constituency parsing can capture more syntactic information
    about the sentence, its results are slower to compute and more difficult to interpret.
    So we will focus on dependency parsing for now.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*成分句法分析*是另一种技术，它专注于识别句子中的*成分子短语*。虽然依存句法分析涉及单词之间的关系，但成分句法分析的目标是将句子解析为一系列子短语。这些子短语可以是名词短语（"我的新电脑"）或动词短语（"有内存问题"）等。成分句法分析的方法更为自上而下，尝试将成分迭代地分解为更小的单元和它们之间的关系。虽然成分句法分析可以捕捉更多关于句子的句法信息，但其结果的计算速度更慢，更难解释。因此我们现在将专注于依存句法分析。'
- en: But wait, you’re probably wondering why understanding relationships between
    entities and sentence diagrams are so important. After all, you’ve probably already
    forgotten how to create them yourself and have probably never used them in real
    life. But that’s only because you’ve internalized this model of the world. We
    need to create that understanding in bots so they can be used to do the same things
    you do without thinking, from simple tasks like grammar checking to complex virtual
    assistants.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等，你可能会想知道为什么理解实体之间的关系和句子图表如此重要。毕竟，你可能已经忘记了如何创建这些图表，并且可能从未在现实生活中使用过它们。但这只是因为你已经内化了这个世界的模型。我们需要在机器人中创建这种理解，以便它们可以像你一样无意识地完成同样的任务，从简单的语法检查到复杂的虚拟助手。
- en: Basically, dependency parsing will help your NLP pipelines for all those applications
    mentioned in Chapter 1…​ better. Have you noticed how chatbots like GPT-3 often
    fall on their face when it comes to understanding simple sentences or having a
    substantive conversation? As soon as you start to ask them about the logic or
    reasoning of the words they are "saying" they stumble. Chatbot developers and
    conversation designers get around this limitation by using rule-based chatbots
    for substantive conversations like therapy and teaching. The open-ended neural
    network models like PalM and GPT-3 are only used when the user tries to talk about
    something that hasn’t yet been programmed into it. And the language models are
    trained with the objective of steering the conversation back to something that
    the bot knows about and has rules for.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，依存句法分析将为第1章中提到的所有应用提供帮助。你是否注意到像GPT-3这样的聊天机器人在理解简单句子或进行实质性对话时常常失败？一旦你开始询问它们所“说”单词的逻辑或推理，它们就会失误。聊天机器人开发人员和对话设计师通过使用基于规则的聊天机器人来解决这个限制，用于进行治疗和教学等实质性对话。只有当用户试图谈论尚未编程到其中的内容时，才会使用开放式神经网络模型，如PalM和GPT-3。而语言模型的训练目标是将对话引导回机器人知道和有规则的东西。
- en: Dependency parsing, as the name suggests, relies on "dependencies" between the
    words in a sentence to extract information. "Dependencies" between two words could
    refer to their grammatical, phrasal, or any custom relations. But in the context
    of dependency parse trees, we refer to the grammatical relationships between word
    pairs of the sentence, one of them acting as the "head" and the other one acting
    as the "dependent". There exists only one word in a sentence that is not dependent
    on any other word in the parse tree, and this word is called the "root" ("ROOT").
    The root is the starting point for the dependency tree just as the main root of
    a tree in the forest starts the growth of its trunk and branches (relations).
    There are 37 kinds of dependency relations that a word could have, and these relations
    are adapted from the *Universal Stanford Dependencies* system.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 依存句法分析（Dependency parsing）正如其名称所示，依赖于句子中单词之间的“依存关系”以提取信息。两个单词之间的“依存关系”可以是它们的语法、短语或任何自定义的关系。但在依存句法树的上下文中，我们指的是句子中一对单词之间的语法关系，其中一个充当“头”（head），另一个充当“从属”（dependent）。在句法树中，有一个词在结构树中不依赖于任何其他单词，这个词被称为“根”（root）。根是依存树的起点，就像森林中一棵树的主根一样，它开始生长树干和树枝（关系）。一个词可以有37种依存关系，这些关系来自于*通用斯坦福依存关系系统*。
- en: 'The spaCy package knows how to recognize these relations between words and
    phrases, and even plot the dependency diagrams for you. Let’s try to do dependency
    parsing of a single sentence:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy软件包知道如何识别单词和短语之间的这些关系，并为您绘制依存图。让我们尝试对一个句子进行依存句法分析：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can see that the ROOT of the sentence is the verb "fired". This is because
    in our sentence, the word "fired" happens to be the main verb when you organize
    it into a Subject-Verb-Object triple. And the dependency (`DEP`) role that the
    word "Gebru" serves is as the "passive nominal subject" (`nsubjpass`). Is there
    a dependency between them "fired" and "Gebru" that you can use to create a relationship
    or fact in a knowledge graph? The `children` attribute gives you a list of all
    the words that depend on a particular token. These dependencies are the key to
    connecting tokens in a relationship to construct a fact.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这个句子的根是动词"fired"。这是因为在我们的句子中，单词"fired"恰好是主谓宾结构中的主动动词。而单词"Gebru"扮演的依存角色是“被动名词主语”(`nsubjpass`)。你能否找到它们之间的依存关系，以创建一个关系或知识图中的事实呢？`children`
    属性给出了一个列表，其中包含所有依赖于特定标记的词。这些依存关系是连接标记、构建事实的关键。
- en: So you will need to include the `children` attribute in your `token_dict` function
    if you want it to show you children of each token in a sentence.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你想要展示句子中每个标记的子标记，你需要在 `token_dict` 函数中包含 `children` 属性。
- en: '[PRE29]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It may seem weird to you that the token "Gebru" doesn’t have any children (dependents)
    in this sentence. It’s the subject of the sentence, after all. The child-parent
    relationship of natural language grammar rules will be a little confusing at first,
    but you can use `displacy` and your `doc2df` function to help you develop a mental
    model for how words depend on each other.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能觉得奇怪的是，在这个句子中，“Gebru”这个标记没有任何子节点（依赖项）。毕竟，它是句子的主语。自然语言语法规则的子父关系一开始可能会有点混乱，但是你可以使用`displacy`和你的`doc2df`函数来帮助你建立单词相互依赖的心理模型。
- en: Redefine the doc2df function to add the `children` attribute as a column so
    you can see if any other words in this sentence have dependents (children).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 重新定义doc2df函数以将`children`属性作为列添加，这样你就可以看到这个句子中是否有其他单词有依赖项（子节点）。
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Looks like the sentence root (labeled `ROOT`) has the most children. "Fired"
    is the most important word in the sentence and all the other words depend on it.
    Every word in a dependency tree is connected to another word elsewhere in the
    sentence. To see this, you need to examine that long list of children in the sentence
    root, 'fired'.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来句子的根节点（标记为`ROOT`）有最多的子节点。"Fired"是句子中最重要的词，所有其他词都依赖于它。依赖树中的每个词都与句子中其他地方的另一个词相连接。要看到这一点，你需要检查句子根节点“fired”中那长长的子节点列表。
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The sentence root branches out to the word "Gebru" and several other words including
    "from." And the word "from" leads to "team", then to "her" and "AI". And "AI"
    leads to "Ethical." You can see that children modify their parents.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 句子根节点延伸到“Gebru”一词，以及包括“from”在内的几个其他单词。而“from”一词导向“team”，然后是“her”和“AI”。而“AI”导向“Ethical”。你可以看到子节点修改其父节点。
- en: The `ROOT` of the dependency tree is the main verb of a sentence. This is where
    you will usually find tokens with the most children. Verbs become relationships
    in a knowledge graph and children become the objects of that relationship in the
    relationship tripple. The token "Gebru" is a child of the passive verb fired,
    so you know that she was the one being fired, but this sentence does not say who
    is responsible for firing her. Since you do not know the subject of the verb "fired"
    you cannot determine who deserves the "unethically" adverb that describes their
    actions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖树的`ROOT`是句子的主要动词。这通常是您会找到最多子节点的标记位置。动词在知识图谱中变成关系，子节点成为关系三元组的对象。标记“Gebru”是被动动词"fired"的子节点，所以你知道她是被解雇的人，但是这个句子没有说是谁负责解雇她。由于你不知道动词“fired”的主语，你无法确定谁应该得到描述他们行为的“不道德”副词。
- en: Time for dependency diagrams to shine! We’ll use one of spaCy’s sub-libraries
    called `displacy`. It can generate a *scalable vector graphics* SVG string (or
    a complete HTML page), which can be viewed as an image in a browser. This visualization
    can help you find ways to use the tree to create tag patterns for relation extraction.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候让依赖图发光了！我们将使用spaCy的一个子库称为`displacy`。它可以生成*可缩放矢量图形*SVG字符串（或完整的HTML页面），可以在浏览器中作为图像查看。这种可视化可以帮助你找到使用树来创建关系抽取标签模式的方法。
- en: Listing 11.2 Visualize a dependency tree
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.2 可视化一个依赖树
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: When you open the file, you should see something like Figure [11.3](#figure-dependency-diagram).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打开文件时，你应该看到类似于图[11.3](#figure-dependency-diagram)的东西。
- en: Figure 11.3 Dependency diagram for a sentence
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3 句子的依赖图
- en: '![dependency diagram](images/dependency_diagram.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![依赖图](images/dependency_diagram.png)'
- en: Before we explain the connection between dependency parsing and relation extraction,
    let’s briefly dive into another tool at our disposal - constituency parsing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解释依赖解析与关系抽取之间的连接之前，让我们简要介绍一下我们可以使用的另一个工具 - 组成解析。
- en: 11.6.1 Constituency parsing with benepar
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.6.1 使用benepar进行组成解析
- en: Berkeley Neural Parser and Stanza have been the go-to options for the extraction
    of constituency relations in text. Let’s explore one of them, Berkeley Neural
    Parser.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 伯克利神经解析器和Stanza一直是文本中提取组成关系的首选选项。让我们探索其中一个，伯克利神经解析器。
- en: This parser cannot be used on its own and requires either spaCy or NLTK to load
    it along with their existing models. You want to use spaCy as your tokenizer and
    dependency tree parse because it is continually improving.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解析器不能单独使用，需要spaCy或NLTK之一来加载它以及它们现有的模型。你想要使用spaCy作为你的分词器和依赖树解析器，因为它不断在改进。
- en: Listing 11.3 Download the necessary packages
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.3 下载必要的包
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: After downloading the packages, we can test it out with a sample sentence. But
    we will be adding `benepar` to spaCy’s pipeline first.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 下载包后，我们可以先将`benepar`添加到spaCy的流水线中，然后用一个示例句子测试它。
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Looks quite cryptic, right? In the example above, we generated a parsed string
    for the test sentence. The parse string includes various phrases and the POS tags
    of the tokens in the sentence. Some common tags you may notice in our parse string
    are NP ("Noun Phrase"), VP ("Verb Phrase"), S ("Sentence"), and PP ("Prepositional
    Phrase"). Now you can see how it’s a bit more difficult to extract information
    from the constituency parser’s output. However, it can be useful to identify all
    the phrases in the sentence and use them in sentence simplification and/or summarization.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来相当神秘，对吗？在上面的示例中，我们为测试句子生成了一个解析字符串。解析字符串包括句子中各种短语和标记的词性标记。你可能会在我们的解析字符串中注意到一些常见的标记，如NP（“名词短语”）、VP（“动词短语”）、S（“句子”）和PP（“介词短语”）。现在你可以看出，从成分解析器的输出中提取信息有点难。然而，它可以用来识别句子中的所有短语，并在句子简化和/或摘要中使用它们。
- en: You now know how to extract the syntactic structure of sentences. How will it
    help you in your quest for an intelligent chatbot?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何提取句子的句法结构。这将如何帮助你追求一个智能聊天机器人？
- en: 11.7 From dependency parsing to relation extraction
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.7 从依存解析到关系提取
- en: 'We’ve come to the crucial stage of helping our bot learn from what it reads.
    Take this sentence from Wikipedia:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到了帮助我们的机器人从所阅读的内容中学习的关键阶段。从维基百科上拿这样一句话：
- en: '*In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense
    Forces, saved the world from nuclear war.*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*1983年，苏联防空军的一名中校斯坦尼斯拉夫·彼得罗夫拯救了世界免受核战争的威胁。*'
- en: 'If you were to take notes in a history class after reading or hearing something
    like that, you’d probably paraphrase things and create connections in your brain
    between concepts or words. You might reduce it to a piece of knowledge, that thing
    that you "got out of it." You’d like your bot to do the same thing. You’d like
    it to "take note" of whatever it learns, such as the fact or knowledge that Stanislav
    Petrov was a lieutenant colonel. This could be stored in a data structure something
    like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在阅读或听到类似的历史课内容后做笔记，你可能会改述这些内容并在大脑中建立概念或词汇之间的联系。你可能将其简化为一个知识点，那就是你从中“得到的东西”。你希望你的机器人也能做同样的事情。你希望它能“记录”它所学到的任何东西，比如斯坦尼斯拉夫·彼得罗夫是一名中校这样的事实或知识。这可以存储在类似以下的数据结构中：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is an example of two named entity nodes ('Stanislav Petrov' and 'lieutenant
    colonel') and a relation or connection ('is a') between them in a knowledge graph
    or knowledge base. When a relationship like this is stored in a form that complies
    with the RDF standard (resource description format) for knowledge graphs, it’s
    referred to as an RDF triplet. Historically these RDF triplets were stored in
    XML files, but they can be stored in any file format or database that can hold
    a graph of triplets in the form of `(subject, relation, object)`. A collection
    of these triplets will be your knowledge graph!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是知识图谱或知识库中两个命名实体节点（'Stanislav Petrov' 和 'lieutenant colonel'）以及它们之间的关系或连接（'is
    a'）的示例。当像这样的关系以符合知识图谱的RDF标准（资源描述格式）的形式存储时，它被称为RDF三元组。历史上这些RDF三元组被存储在XML文件中，但它们可以存储在任何能够以`(subject,
    relation, object)`形式持有三元组图的文件格式或数据库中。这些三元组的集合将是你的知识图谱！
- en: Let’s go ahead and create some fodder for your knowledge graph using the two
    approaches we know - patterns and machine learning.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用我们所知道的两种方法——模式和机器学习——为你的知识图谱创建一些素材。
- en: 11.7.1 Pattern-based relation extraction
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.7.1 基于模式的关系提取
- en: Remember how you used regular expressions to extract character patterns? Word
    patterns are just like regular expressions but for words instead of characters.
    Instead of character classes, you have word classes. For example, instead of matching
    a lowercase character, you might have a word pattern decision to match all the
    singular nouns ("NN" POS tag).^([[14](#_footnotedef_14 "View footnote.")]) Some
    seed sentences are tagged with some correct relationships (facts) extracted from
    those sentences. A POS pattern can be used to find similar sentences where the
    subject and object words might change or even the relationship words.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得你如何使用正则表达式来提取字符模式吗？单词模式就像是针对单词而不是字符的正则表达式一样。与字符类不同，你有单词类。例如，不是匹配小写字符，而是可能有一个单词模式决定匹配所有单数名词（“NN”
    POS标签）。[[14](#_footnotedef_14 "查看脚注。")] 一些种子句子被标记了一些正确的关系（事实），这些关系是从这些句子中提取出来的。POS模式可以用来找到类似的句子，其中主语和宾语词可能会改变，甚至关系词也会改变。
- en: 'The simplest way to extract relations out of the text is to look for all "Subject-Verb-Object"
    triplets using the "nsubj" and "dobj" tags of the ROOT word. But let’s do something
    a bit more complex. What if we want to extract information about meetings between
    historical figures from Wikipedia? You can use the spaCy package in two different
    ways to match these patterns in \(O(1)\) (constant time) no matter how many patterns
    you want to match:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取关系的最简单方法是查找所有"主语-动词-宾语"三元组，使用ROOT词的"nsubj"和"dobj"标签。但是让我们做一些更复杂的事情。如果我们想从维基百科中提取关于历史人物之间会面的信息怎么办？你可以使用spaCy包的两种不同方式来匹配这些模式，在\(O(1)\)（常数时间）内无论你要匹配多少模式：
- en: PhraseMatcher for any word/tag sequence patterns ^([[15](#_footnotedef_15 "View
    footnote.")])
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于任何单词/标记序列模式的PhraseMatcher[[15](#_footnotedef_15 "查看脚注。")]
- en: Matcher for POS tag sequence patterns ^([[16](#_footnotedef_16 "View footnote.")])
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于POS标签序列模式的匹配器[[16](#_footnotedef_16 "查看脚注。")]
- en: Let’s start with the latter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从后者开始。
- en: 'First, let’s look at an example sentence and see the POS for every word:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一个例句并查看每个词的POS：
- en: Listing 11.4 Helper functions for spaCy tagged strings
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: spaCy标记字符串的辅助函数
- en: '[PRE36]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now you can see the sequence of POS or TAG features that will make a good pattern.
    If you’re looking for "has-met" relationships between people and organizations,
    you’d probably like to allow patterns such as "PROPN met PROPN", "PROPN met the
    PROPN", "PROPN met with the PROPN", and "PROPN often meets with PROPN". You could
    specify each of those patterns individually, or try to capture them all with some
    * or ? operators on "any word" patterns between your proper nouns:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以看到形成良好模式的POS或TAG特征的序列。如果你正在寻找人与组织之间的“会面”关系，你可能希望允许诸如“PROPN met PROPN”、“PROPN
    met the PROPN”、“PROPN met with the PROPN”和“PROPN often meets with PROPN”等模式。你可以单独指定每个模式，也可以尝试通过一些*或？操作符捕捉所有这些模式之间的“任意单词”模式：
- en: '[PRE37]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Patterns in spaCy are a lot like this pseudocode, but much more powerful and
    flexible. SpaCy patterns are very similar to regular expressions for tokens. Like
    regular expressions, you have to be very verbose to explain exactly the word features
    you’d like to match at each position in the token sequence. In a spaCy pattern,
    you use a dictionary of lists to capture all the parts-of-speech and other features
    that you want to match for each token or word.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，模式与此伪代码非常相似，但更加强大和灵活。SpaCy模式非常类似于标记的正则表达式。像正则表达式一样，你必须非常冗长地解释你想在标记序列的每个位置上精确匹配的单词特征。在spaCy模式中，你使用列表的字典来捕捉你想要为每个标记或单词匹配的所有词性和其他特征。
- en: Listing 11.5 Example spaCy POS pattern
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例spaCy POS模式
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can then extract the tagged tokens you need from your parsed sentence.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以从解析后的句子中提取你需要的标记化标记。
- en: Listing 11.6 Creating a POS pattern matcher with spaCy
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建一个使用spaCy的POS模式匹配器
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: A spacy matcher will list the pattern matches as 3-tuples containing match ID
    integers, plus the start and stop token indices (positions) for each match. So
    you extracted a match from the original sentence from which you created the pattern,
    but what about similar sentences from Wikipedia?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一个spaCy匹配器将列出模式匹配为包含匹配ID整数的3元组，以及每个匹配的起始和停止标记索引（位置）。因此，你从创建模式的原始句子中提取了一个匹配项，但是关于维基百科的类似句子呢？
- en: Listing 11.7 Using a POS pattern matcher
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用POS模式匹配器
- en: '[PRE40]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You need to add a second pattern to allow for the verb to occur after the subject
    and object nouns.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要添加第二个模式以允许动词在主语和宾语名词之后出现。
- en: Listing 11.8 Combine patterns together to handle more variations
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.8 将模式组合在一起以处理更多变化
- en: '[PRE41]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: So now you have your entities and a relationship. You can even build a pattern
    that is less restrictive about the verb in the middle ("met") and more restrictive
    about the names of the people and groups on either side. Doing so might allow
    you to identify additional verbs that imply that one person or group has met another,
    such as the verb "knows" or even passive phrases such as "had a conversation"
    or "became acquainted with". Then you could use these new verbs to add relationships
    for new proper nouns on either side.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了你的实体和一个关系。你甚至可以构建一个在中间动词（“遇见”）上不那么限制性而在两边的人和团体的名字上更加限制性的模式。这样做可能会使你能够识别出其他暗示一个人或团体遇见另一个人或团体的动词，例如动词“知道”甚至被动短语，如“交谈”或“结识”。然后你可以使用这些新动词为两边的新专有名词添加关系。
- en: But you can see how you’re drifting away from the original meaning of your seed
    relationship patterns. This is called semantic drift. To ensure that the new relations
    found in new sentences are truly analogous to the original seed (example) relationships,
    you often need to constrain the subject, relation, and object word meanings to
    be similar to those in the seed sentences. The best way to do this is with some
    vector representation of the meaning of words. Fortunately for you, spaCy tags
    words in a parsed document with not only their POS and dependency tree information
    but also provides the Word2Vec word vector. You can use this vector to prevent
    the connector verb and the proper nouns on either side from drifting too far away
    from the original meaning of your seed pattern.^([[17](#_footnotedef_17 "View
    footnote.")])
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你可以看到你的种子关系模式与原始含义渐行渐远。这被称为语义漂移。为了确保新句子中找到的新关系真正类似于原始种子（例子）关系，你通常需要限制主语、关系和宾语的词义与种子句子中的词义相似。做到这一点的最佳方式是利用单词含义的某种向量表示。幸运的是，spaCy不仅使用其POS和依赖树信息为解析文档中的单词打标签，还提供了Word2Vec单词向量。你可以利用这个向量防止连接动词和两侧的专有名词与你的种子模式的原始含义相去甚远。
- en: Using semantic vector representations for words and phrases has made automatic
    information extraction accurate enough to build large knowledge bases automatically.
    But human supervision and curation are required to resolve much of the ambiguity
    in natural language text.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单词和短语的语义向量表示已经使得自动信息抽取的准确度足以自动构建大型知识库。但是需要人类监督和策划来解决自然语言文本中的大部分歧义。
- en: 11.7.2 Neural relation extraction
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.7.2 神经关系抽取
- en: 'Now that you’ve seen the pattern-based method for relation extraction, you
    can imagine that researchers have already tried to do the same with a neural network.
    The neural relation extraction task is traditionally classified into two categories:
    closed and open.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了基于模式的关系抽取方法，你可以想象研究人员已经尝试使用神经网络做同样的事情了。神经关系抽取任务传统上被分类为两类：封闭和开放。
- en: In *closed* relation extraction, the model extracts relations only from a given
    list of relation types. The advantages of this are that we can minimize the risk
    of getting untrue and bizarre relation labels between entities which makes us
    more confident about using them in real life. But the limitation is that it needs
    human labelers to come up with a list of relevant labels for every category of
    text, which as you can imagine, can get tedious and expensive.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在*封闭*关系抽取中，模型仅从给定的关系类型列表中提取关系。这样做的优点是我们可以尽量减少在实体之间得到不真实和奇怪的关系标签的风险，这使我们更有信心在现实生活中使用它们。但是限制是需要人类标记者为每个文本类别的相关标签制定一个列表，你可以想象，这可能会变得繁琐和昂贵。
- en: In *open* relation extraction, the model tries to come up with its own set of
    probable labels for the named entities in the text. This is suitable for processing
    large and generally unknown texts like Wikipedia articles and news entries.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在*开放*关系抽取中，模型试图为文本中的命名实体提出其自己的一组可能标签。这适用于处理大型且通常不为人所知的文本，如维基百科文章和新闻条目。
- en: Over the past few years, experiments with Deep Neural Networks have given strong
    results on triplet extraction and subsequently, most of the research on the topic
    now follow neural methods.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，深度神经网络的实验在三元组抽取方面取得了很强的结果，随后，关于这个主题的大部分研究都采用了神经方法。
- en: Unfortunately, there aren’t as many out-of-the-box solutions for relation extraction
    as there are for the previous stages of the pipeline. What’s more, your relation-extraction
    is usually going to be pretty targeted. In most cases, you wouldn’t want to extract
    ALL possible relations between entities, but only those that are relevant to the
    task you’re trying to perform. For example, you might want to extract interactions
    between drugs from a set of pharmaceutical documents.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，与管道的前几个阶段相比，关系提取的现成解决方案并不多。而且，您的关系提取通常会非常有针对性。在大多数情况下，您不会想提取实体之间的所有可能关系，而只会提取与您要执行的任务相关的关系。例如，您可能希望从一组制药文件中提取药物之间的相互作用。
- en: One of the state-of-the-art models that is used nowadays to extract relations
    is LUKE (Language Understanding with Knowledge-based Embeddings). LUKE uses *entity-aware
    attention* - meaning that its training data included information on whether each
    token is an entity or not. It was also trained to be able to "guess" a masked
    entity in a Wikipedia-based dataset (rather than just guessing all masked words,
    like the BERT model was trained).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 目前用于提取关系的先进模型之一是LUKE（基于知识的语言理解）。LUKE使用*实体感知注意力* - 这意味着其训练数据包含了每个标记是否是实体的信息。它还经过训练，可以“猜测”基于维基百科数据集中的屏蔽实体（而不仅仅是猜测所有屏蔽的单词，就像BERT模型经过训练的那样）。
- en: SpaCy also includes some infrastructure to create your own relation extraction
    component, but that requires quite a bit of work. We won’t cover it as part of
    this book. Fortunately, authors like Sofie Van Landeghem have created great resources
    ^([[18](#_footnotedef_18 "View footnote.")]) for you to learn from if you want
    to custom-train a relation extractor for your particular needs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: SpaCy还包括一些基础设施来创建您自己的关系提取组件，但这需要相当多的工作。我们不会在本书的内容中涵盖这一部分。幸运的是，像Sofie Van Landeghem这样的作者已经创建了很好的资源^（[18](#_footnotedef_18
    "查看脚注。")^），供您根据自己的特定需求进行定制培训关系提取器时参考。
- en: Training your relation extraction model
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练您的关系提取模型
- en: When training your relation extractor, you will need labeled data where the
    relations relative to your task are tagged properly in order for the model to
    learn to recognize them. But big datasets are hard to create and label, so it’s
    worth checking if some of the existing datasets used for benchmarking and finetuning
    state-of-the-art models already have the data that you need.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练您的关系提取器时，您将需要标记正确的标签数据，以便模型学会识别与您任务相关的关系。但是创建和标记大型数据集很困难，因此值得检查一下一些用于基准测试和微调最先进模型的现有数据集是否已经包含了您需要的数据。
- en: DocRED and Stanford TACRED together are the de-facto benchmark datasets and
    models for relation extraction methods because of their size and the generality
    of the knowledge graphs
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: DocRED和斯坦福TACRED一起是关系提取方法的事实标准基准数据集和模型，因为它们的大小和知识图的一般性。
- en: Stanford’s Text Analysis Conference Relation Extraction Dataset (TACRED) contains
    more than 100,000 example natural language passages paired with their corresponding
    relations and entities. It covers 41 relation types. Over the past few years,
    researchers have improved TACRED’s data quality and reduced ambiguity in the relation
    classes with datasets such as Re-TACRED and DocRED.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福文本分析会议关系提取数据集（TACRED）包含超过100,000个示例自然语言段落，配对其相应的关系和实体。它涵盖了41种关系类型。在过去几年里，研究人员通过诸如Re-TACRED和DocRED等数据集改进了TACRED的数据质量，并减少了关系类别中的歧义。
- en: The Document Relation Extraction Dataset (DocRED) expands the breadth of natural
    language text that can be used for relation extraction because it includes relations
    that require parsing of multiple sentences of natural language text. The training
    and validation dataset used to train DocRED is currently (in 2023) the largest
    human-annotated dataset for document-level relation extraction. Most of the human-annotated
    knowledge graph data in DocRED is included in the Wikidata knowledge base. And
    the corresponding natural language text examples can be found in the archived
    version of Wikipedia.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 文档关系提取数据集（DocRED）扩展了可以用于关系提取的自然语言文本的广度，因为它包括需要解析多个句子的自然语言文本的关系。用于训练DocRED的训练和验证数据集目前（2023年）是最大的人工注释数据集，用于文档级关系提取。DocRED中的大部分人工注释知识图数据包含在Wikidata知识库中。相应的自然语言文本示例可以在维基百科的存档版本中找到。
- en: Now you have a better idea of how to take an unstructured text and turn it into
    a collection of facts. Time for the last stage of our pipeline - building a knowledge
    database.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对如何将非结构化文本转化为一系列事实有了更好的了解。现在是我们流程的最后阶段了——建立知识数据库。
- en: 11.8 Building your knowledge base
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.8 建立你的知识库
- en: So, you have your relations extracted from your text. You could put them all
    into a big table; and yet, we keep talking about knowledge *graphs*. What really
    makes this particular way of structuring data so powerful?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你从文本中提取出了关系。你可以把它们都放到一个大表格中；但是，我们仍然在谈论知识*图谱*。是什么让这种特定的数据结构方式如此强大呢？
- en: Let’s go back to Stanislav Petrov, whom we’ve met in the last chapter. What
    if we wanted to answer a question like "What is Stanislav Petrov’s military rank?"
    This is a question that a single relation triple 'Stanislav Petrov', 'is-a', 'lieutenant
    colonel' isn’t enough to answer - because your question-answering machine also
    needs to know that "lieutenant colonel" is a military rank. However, if you organize
    your knowledge as a graph, answering the question becomes possible. Take a look
    at Figure [11.4](#figure-stanislav-knowledge-graph) to understand how it happens.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们在上一章中遇到的斯坦尼斯拉夫·彼得罗夫。如果我们想回答类似“斯坦尼斯拉夫·彼得罗夫的军衔是什么？”这样的问题，一个单一的关系三元组‘斯坦尼斯拉夫·彼得罗夫’、‘是-一个’、‘中校’是不够的——因为你的问答机器还需要知道“中校”是一个军衔。然而，如果你将你的知识组织成图形，回答这个问题就成为可能。看一下图[11.4](#figure-stanislav-knowledge-graph)了解它是如何发生的。
- en: Figure 11.4 Stanislav knowledge graph
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4 斯坦尼斯拉夫知识图谱
- en: '![Stanislav Knowledge Graph](images/Stanislav-Knowledge-Graph.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![斯坦尼斯拉夫知识图谱](images/Stanislav-Knowledge-Graph.png)'
- en: The red edge and node in this knowledge graph represent a fact that could not
    be directly extracted from the statement about Stanislav. But this fact that "lieutenant
    colonel" is a military rank could be inferred from the fact that the title of
    a person who is a member of a military organization is a military rank. This logical
    operation of deriving facts from a knowledge graph is called knowledge graph *inference*.
    It can also be called querying a knowledge base, analogous to querying a relational
    database. A whole field called Knowledge Base Question Answering is focused on
    finding ways to answer questions like this (they are called "multi-hop questions")
    more efficiently.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此知识图谱中的红色边和节点代表了一个不能直接从关于斯坦尼斯拉夫的陈述中提取出的事实。但是，“中校”是一个军衔的事实可以从一个成员是军事组织成员的人的头衔是一个军衔的事实推断出来。从知识图谱中推导事实的这种逻辑操作称为知识图谱*推理*。它也可以称为查询知识库，类似于查询关系数据库。一个名为知识库问答的领域专注于找到更有效地回答这类问题（它们被称为“多跳问题”）的方法。
- en: For this particular inference or query about Stanislov’s military ranks, your
    knowledge graph would have to already contain facts about militaries and military
    ranks. It might even help if the knowledge base had facts about the titles of
    people and how people relate to occupations (jobs). Perhaps you can see now how
    a base of knowledge helps a machine understand more about a statement than it
    could without that knowledge. Without this base of knowledge, many of the facts
    in a simple statement like this will be "over the head" of your chatbot. You might
    even say that questions about occupational rank would be "above the pay grade"
    of a bot that only knew how to classify documents according to randomly allocated
    topics. (See Chapter 4 if you’ve forgotten about how random topic allocation can
    be.)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关于斯坦尼斯拉夫军衔的特定推理或查询，你的知识图谱必须已经包含有关军队和军衔的事实。如果知识库中还有关于人的头衔以及人与职业（工作）的关系的事实，甚至可能会有所帮助。也许现在你能明白，一组知识如何帮助机器比没有这些知识时更多地理解一个陈述。没有这组知识，一个简单陈述中的许多事实将会“超出”你的聊天机器人的理解范围。你甚至可以说，对职业等级的问题对于一个只知道如何根据随机分配的主题对文档进行分类的机器人来说是“超出了薪酬水平”的。（如果你忘记了随机主题分配的工作原理，请参阅第4章。）
- en: It may not be obvious how big a deal this is, but it is a *BIG* deal. If you’ve
    ever interacted with a chatbot that doesn’t understand "which way is up", literally,
    you’d understand. One of the most daunting challenges in AI research is the challenge
    of compiling and efficiently querying a knowledge graph of common sense knowledge.
    We take common-sense knowledge for granted in our everyday conversations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能不太明显，但这是一个*大*问题。如果你曾经与一个不懂得"哪个方向是上"的聊天机器人交互过，字面上而言，你会明白。在人工智能研究中最具挑战性的挑战之一是编译和高效查询常识知识的知识图谱。我们在日常对话中理所当然地运用常识知识。
- en: Humans start acquiring much of their common sense knowledge even before they
    acquire language skills. We don’t spend our childhood writing about how a day
    begins with light and sleep usually follows sunset. And we don’t edit Wikipedia
    articles about how an empty belly should only be filled with food rather than
    dirt or rocks. This makes it hard for machines to find a corpus of common sense
    knowledge to read and learn from. No common-sense knowledge Wikipedia articles
    exist for your bot to do information extraction on. And some of that knowledge
    is instinct, hard-coded into our DNA.^([[19](#_footnotedef_19 "View footnote.")])
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在获得语言技能之前就开始获取大部分常识知识。我们的童年并不是在写关于白天从光开始，夜晚通常在日落后开始睡觉的文章。我们也不会编辑维基百科文章，说明空腹应该只填满食物而不是泥土或石头。这使得机器很难找到一个常识知识语料库来阅读和学习。不存在供你的机器人进行信息提取的常识知识维基百科文章。而且其中一些知识是本能，已经硬编码到我们的
    DNA 中。^([[19](#_footnotedef_19 "查看脚注。")])
- en: All kinds of factual relationships exist between things and people, such as
    "kind-of", "is-used-for", "has-a", "is-famous-for", "was-born", and "has-profession."
    NELL, the Carnegie Mellon Never Ending Language Learning bot is focused almost
    entirely on the task of extracting information about the `'kind-of'` relationship.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 事物和人之间存在各种各样的事实关系，如 "kind-of"、"is-used-for"、"has-a"、"is-famous-for"、"was-born"
    和 "has-profession"。卡内基梅隆大学的永不停止的语言学习机器人 NELL 几乎完全专注于提取关于 `'kind-of'` 关系的信息。
- en: Most knowledge bases normalize the strings that define these relationships,
    so that "kind of" and "type of" would be assigned a normalized string or ID to
    represent that particular relation. And some knowledge bases also normalize the
    nouns representing the objects in a knowledge base, using coreference resolution
    that we described before. So the bigram "Stanislav Petrov" might be assigned a
    particular ID. Synonyms for "Stanislav Petrov", like "S. Petrov" and "Lt Col Petrov",
    would also be assigned to that same ID, if the NLP pipeline suspected they referred
    to the same person.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数知识库会规范化定义这些关系的字符串，这样 "kind of" 和 "type of" 就会被分配一个规范化的字符串或 ID 来表示该特定关系。而一些知识库还会对表示知识库中对象的名词进行规范化，使用我们之前描述的指代消解。因此，二元组
    "Stanislav Petrov" 可能会被分配一个特定的 ID。 "Stanislav Petrov" 的同义词，如 "S. Petrov" 和 "Lt
    Col Petrov"，如果 NLP 流水线怀疑它们指的是同一个人，则也会被分配给相同的 ID。
- en: 11.8.1 A large knowledge graph
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.8.1 一个庞大的知识图谱
- en: 'If you’ve ever heard of a "mind map" they can give a pretty good mental model
    of what knowledge graphs are: connections between concepts in your mind. To give
    you a more concrete mental model of the concept of knowledge graphs you probably
    want to explore the oldest public knowledge graph on the web: NELL (Never Ending
    Language Learning) graph, created by the bot we met in the last section.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经听说过"思维导图"，它们可以很好地展示知识图谱是什么：你头脑中概念之间的连接。为了给你一个更具体的概念模型，你可能想探索网络上最古老的公共知识图谱：NELL（永不停止的语言学习）图，由我们在上一节中遇到的机器人创建。
- en: The NLPiA2 Python package has several utilities for making the NELL knowledge
    graph a bit easier to wrap your head around. Later in the chapter, you’ll see
    the details about how these work so you can prettify whatever knowledge graph
    you are working with.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: NLPiA2 Python 包有一些实用工具，可以让 NELL 知识图谱稍微容易理解一些。在本章后面，你将看到关于这些工具如何工作的详细信息，以便你可以美化你正在处理的任何知识图谱。
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The entity names are very precise and well-defined within a hierarchy, like
    paths for a file or name-spaced variable names in Python. All of the entity and
    value names start with "concept:" so you can strip that from your name strings
    to make the data a bit easier to work with. To simplify things further, you can
    eliminate the namespacing hierarchy and focus on just the last name in the hierarchy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 实体名称在层次结构中非常精确和明确定义，就像文件的路径或 Python 中的命名空间变量名一样。所有实体和值名称都以“概念:”开头，因此你可以从你的名称字符串中去掉这个来使数据更容易处理。为了进一步简化事情，你可以消除命名空间层次结构，只关注层次结构中的最后一个名称。
- en: '[PRE43]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The `nlpia2.nell` module simplifies the names of things even further. This makes
    it easier to navigate the knowledge graph in a network diagram. Otherwise, the
    names of entities can fill up the width of the plot and crowd each other out.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`nlpia2.nell` 模块进一步简化了事物的名称。这使得在网络图中浏览知识图谱变得更加容易。否则，实体的名称可能会填满绘图的宽度，并相互挤出。'
- en: '[PRE44]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: NELL scrapes text from Twitter, so the spelling and wording of facts can be
    quite varied. In NELL the names of entities, relations and objects have been normalized
    by lowercasing them and removing all punctuation like apostrophes and hyphens.
    Only proper names are allowed to retain their spaces, to help distinguish between
    names that contain spaces and those that are smashed together. However, in NELL,
    just as in Word2vec token identifiers, proper names are joined with underscore
    ("\_") characters.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: NELL 从 Twitter 上抓取文本，因此事实的拼写和措辞可能会变化很大。在 NELL 中，实体、关系和对象的名称已经通过将它们转换为小写并删除所有标点符号（如撇号和连字符）进行了标准化。只有专有名词允许保留它们的空格，以帮助区分包含空格的名称和被拼接在一起的名称。然而，在
    NELL 中，就像在 Word2vec 标记标识符中一样，专有名词是用下划线（"\_"）字符连接的。
- en: Entity and relation names are like variable names in Python. You want to be
    able to query them like field names in a database, so they should not have ambiguous
    spellings. The original NELL dataset contains one row per triple (fact). Triples
    can be read like a terse, well-defined sentence. Knowledge triples describe a
    single isolated fact about the world. They give you one piece of information about
    an entity (object) in the world.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 实体和关系名称类似于 Python 中的变量名称。你希望能够像数据库中的字段名称那样查询它们，因此它们不应该有歧义的拼写。原始的 NELL 数据集每行包含一个三元组（事实）。三元组可以被读作简洁、明确定义的句子。知识三元组描述了世界上的一个单独事实。它们给出了关于世界中一个实体（对象）的一条信息。
- en: As a minimum, a knowledge triple consists of an entity, relation and value.
    The first element of a knowledge triple gives you the name of the entity that
    the fact is about. The second column, "relation," contains the relationship to
    some other quality (adjective) or object (noun) in the world called its value.
    A relation is usually a verb phrase that starts with or implies words like "is"
    or "has." The third column, "value," contains an identifier for some quality of
    that relation. The "value" is the object of the relationship and is a named entity
    just as the subject ("entity") of the triple is.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，知识三元组由实体、关系和值组成。知识三元组的第一个元素给出了关于事实的实体名称。第二列“关系”包含与世界中某些其他性质（形容词）或对象（名词）的关系，称为它的值。关系通常是以“是”或“有”等词开始或暗示的动词短语。第三列“值”包含该关系的某个质量的标识符。该“值”是关系的对象，并且与三元组的主语（“实体”）一样是一个命名实体。
- en: Because NELL crowdsources the curation of the knowledge base, you also have
    a probability or confidence value that you can use to make inferences on conflicting
    pieces of information. And NELL has 9 more columns of information about the fact.
    It lists all the alternative phrases that were used to reference a particular
    entity, relation or value. NELL also identifies the iteration (loop through Twitter)
    that the fact was created during. The last column provides the source of the data
    - a list of all the texts that created the fact.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 NELL 众包了知识库的管理，所以你还有一个概率或置信度值，可以用来推断冲突信息。而且 NELL 还有关于事实的 9 个更多信息列。它列出了用于引用特定实体、关系或值的所有替代短语。NELL
    还识别了创建该事实的迭代（遍历 Twitter）。最后一列提供了数据的来源 - 创建事实的所有文本列表。
- en: NELL contains facts about more than 800 unique relations and more than 2 million
    entities. Because Twitter is mostly about people, places and businesses, it’s
    a good knowledge base to use to augment a common sense knowledge base. And it
    can be useful for doing fact-checking about famous people or businesses and places
    that are often the targets of misinformation campaigns. There’s even a "latitudelongitude"
    relation that you could use to verify any facts related to the location of things.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: NELL 包含有关 800 多种唯一关系和超过 200 万个实体的事实。因为 Twitter 主要涉及人物、地点和企业，所以它是一个很好的知识库，可用于增强常识知识库。它对于对名人或企业以及经常是错误信息宣传目标的地点进行事实核查也可能很有用。甚至有一个
    "latitudelongitude" 关系，您可以使用它来验证与事物位置相关的任何事实。
- en: '[PRE45]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now you have learned how facts can be organized into a knowledge graph. But
    what do we do when we need to use this knowledge - for example, for answering
    questions? That’s what we’ll be dealing with in the last section of this chapter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何将事实组织成知识图谱。但是当我们需要使用这些知识时 - 例如，用于回答问题时，我们该怎么办？这将是本章最后一节要处理的内容。
- en: 11.9 Finding answers in a knowledge graph
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.9 在知识图谱中查找答案
- en: Now that our facts are all organized in a graph database, how do we retrieve
    that knowledge? As with any database, graph databases have special query languages
    to pull information from them. Just as SQL and its different dialects are used
    to query relational databases, a whole family of languages such as SPARQL (SPARQL
    Protocol and RDF Query Language), Cypher, and AQL exist to query graph databases.
    In this book, we’ll focus on SPARQL, as it was adopted as a standard by the open-source
    communities. Other languages, such as Cypher or AQL, are used to query specific
    graph knowledge bases, such as Neo4j and ArangoDB.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的事实都组织在一个图形数据库中，我们如何检索那些知识呢？与任何数据库一样，图形数据库有特殊的查询语言来从中提取信息。就像 SQL 及其不同的方言用于查询关系数据库一样，一系列语言，如
    SPARQL（SPARQL 协议和 RDF 查询语言），Cypher 和 AQL 存在用于查询图数据库。在本书中，我们将专注于 SPARQL，因为它已被开源社区采用为标准。其他语言，如
    Cypher 或 AQL，用于查询特定的图知识库，如 Neo4j 和 ArangoDB。
- en: 'As our knowledge base, we’ll use an even bigger knowledge graph than NELL:
    Wikidata, the knowledge database version of Wikipedia. It contains more than 100
    million data items (entities and relations) and is maintained by volunteer editors
    and bots, just like all the other Wikimedia projects.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个比 NELL 更大的知识图谱作为我们的知识基础：Wikidata，维基百科的知识数据库版本。它包含超过 1 亿个数据项（实体和关系），由志愿编辑和机器人维护，就像所有其他维基媒体项目一样。
- en: 'In Wikidata, the relations between entities are called *properties*. There
    are more than 11,000 properties in Wikidata system, and each one has its "P-id",
    a unique identifier that is used to represent that property in queries. Similarly,
    every entity has its own unique "Q-id". You can easily retrieve the Q-id of any
    Wikipedia article by using Wikidata’s REST API:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Wikidata 中，实体之间的关系被称为 *属性*。Wikidata 系统中有超过 11,000 个属性，每个属性都有其 "P-id"，这是一个用于在查询中表示该属性的唯一标识符。同样，每个实体都有其自己独特的
    "Q-id"。您可以通过使用 Wikidata 的 REST API 轻松检索任何维基百科文章的 Q-id：
- en: '[PRE46]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You can confirm your findings by heading to ([http://www.wikidata.org/entity/Q59753117](entity.html))
    and finding there more properties of this entity, that link it to different entities.
    As you can see, this is a simple "GET" query that only works if we already have
    the entity’s name and want to find the Q-id (or vice-versa). For more complex
    queries, we will need to use SPARQL. Let’s write your first query then!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过前往 ([http://www.wikidata.org/entity/Q59753117](entity.html)) 确认您的发现，并在那里找到该实体的更多属性，将其链接到不同的实体。正如您所看到的，这是一个简单的
    "GET" 查询，只有在我们已经有了实体名称并且想要找到 Q-id（或反之）时才有效。对于更复杂的查询，我们将需要使用 SPARQL。那么我们来写你的第一个查询吧！
- en: Let’s say you want to find out who were Timnit Gebru’s co-authors on her notable
    paper about Stochastic Parrots. If you don’t remember the name of the paper exactly,
    you can actually find it with a simple query. For this, you’ll need a couple of
    property and entity IDs - for simplicity, we just list them in the code.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想知道谁是 Timnit Gebru 的关于 Stochastic Parrots 的著名论文的合著者。如果你不记得论文的名称确切，你实际上可以通过一个简单的查询找到它。为此，你需要一些属性和实体
    ID - 为简单起见，我们只在代码中列出它们。
- en: '[PRE47]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Important
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Don’t forget to double escape the curly braces in f-strings! And you cannot
    use a backslash as an escape character in f-strings. *WRONG*: f"\{" Instead you
    must double the curly braces. *RIGHT*: f"{{"'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记在 f-strings 中双重转义大括号！而且你不能在 f-strings 中使用反斜杠作为转义字符。 *错误*：f"\{"，而应该是双大括号。
    *正确*：f"{{"
- en: And if you are familiar with the `jinja2` package, be careful mixing using Python
    f-strings to populate jinja2 templates, you would need four curly braces to create
    a literal double curly brace.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉`jinja2`包，请注意混合使用 Python f-strings 来填充 jinja2 模板时，你需要四个花括号来创建一个文字双花括号。
- en: Cryptic at first sight, what this query means is "Find an entity A such that
    Timnit Gebru has A as notable work, and also A is an instance of an academic article".
    You can see how each relational condition is codified in SPARQL, with operand
    `wd:` preceding entity Q-ids and the operand `wdt:` preceding property P-ids.
    Each relation constraint has a form of "ENTITY has-property ENTITY".
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这个查询看起来有些神秘，它的意思是“找到一个实体 A，使得 Timnit Gebru 有 A 作为知名作品，并且 A 是学术文章的一个实例”。你可以看到每个关系条件在
    SPARQL 中是如何编码的，操作数 `wd:` 在实体 Q-id 前面，操作数 `wdt:` 在属性 P-id 前面。每个关系约束都有一个“实体有-属性-实体”的形式。
- en: 'Let’s now use WIKIDATA’s SPARQL API to retrieve the results of our query. For
    this, we will use a dedicated `SPARQLWrapper` package that will simplify the process
    of querying for us. First, let’s set up our wrapper:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 WIKIDATA 的 SPARQL API 来检索我们查询的结果。为此，我们将使用一个专门的`SPARQLWrapper`包，它将简化我们的查询过程。首先，让我们设置我们的包装器：
- en: '[PRE48]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once that’s set, you can execute your query and examine the response:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置好，你就可以执行你的查询并检查响应：
- en: '[PRE49]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This looks right! Now that you’ve got the Q-id of the article - you can retrieve
    its authors by using the ''author'' property of the article:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来没问题！现在你已经得到了文章的 Q-id - 你可以通过使用文章的 'author' 属性来检索它的作者：
- en: '[PRE50]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: And here you have the answer to your question!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了你问题的答案！
- en: 'Instead of doing two queries, we could have achieved the same result by nesting
    our queries, within each other, like this:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将查询嵌套在彼此内部来完成相同的结果，而不是执行两个查询，就像这样：
- en: '[PRE51]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: SPARQL is a well-developed language whose functionality includes much more than
    just simple queries. Wikidata itself has a pretty good manual on SPARQL.^([[20](#_footnotedef_20
    "View footnote.")]) The deeper you dig into Wikidata using SPARQL the more uses
    you will find for it in your NLP applications. It is one of the only ways you
    can automatically evaluate the quality and correctness of the facts that your
    NLP pipeline asserts to your users.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: SPARQL 是一种功能齐全的语言，其功能不仅限于简单的查询。Wikidata 本身对 SPARQL 有一个相当不错的手册。你挖掘 Wikidata 使用
    SPARQL 的深度越深，你将在你的 NLP 应用中找到越多的用途。这是你可以自动评估你的 NLP 流水线对用户断言的事实的质量和正确性的唯一方式之一。
- en: 11.9.1 From questions to queries
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.9.1 从问题到查询
- en: So, you managed to find the answer to a pretty complex question in a knowledge
    database. That would have been pretty much impossible to do if your database was
    relational, or if all you had was unstructured text.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你成功在知识数据库中找到了一个相当复杂的问题的答案。如果你的数据库是关系型的，或者你只有非结构化的文本，那几乎是不可能做到的。
- en: However, looking for the answer took us quite a lot of work and two SPARQL queries.
    How do you transform a natural-language question into a query in a structured
    language like SPARQL?
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，寻找答案花费了我们相当多的工作和两个 SPARQL 查询。如何将自然语言问题转化为像 SPARQL 这样的结构化语言的查询？
- en: You already did this kind of transformation before, back in Chapter 9\. Translating
    human language into machine language is a bit harder than translating between
    human languages, but it’s still the same basic problem for a machine. And now
    you know that transformers are good at transforming (pun intended) one language
    into another. LLMs, being huge transformers, are especially good at it. Sachin
    Charma created a great example of constructing a knowledge graph using another
    graph database, ArangoDB. She used OpenAI’s models to enable natural language
    question answering on the database he created.^([[21](#_footnotedef_21 "View footnote.")])
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你以前已经做过这种转换，在第 9 章的时候。将人类语言翻译成机器语言比在人类语言之间进行翻译要困难一些，但对于机器来说，这仍然是同一个基本问题。现在你知道了转换器擅长将一种语言转换成另一种语言。作为庞大的转换器，LLMs
    尤其擅长此类操作。Sachin Charma 创建了一个很好的示例，使用另一个图数据库 ArangoDB 构建知识图谱。他使用 OpenAI 的模型来使数据库上的自然语言问答成为可能。
- en: 11.10 Test yourself
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.10 自我测试
- en: Give an example of a question that’s easier to answer with a graph database
    than with a relational database.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给出一个问题的例子，这个问题比使用关系数据库更容易回答。
- en: Convert a `networkx` directed graph to an edge list in a Pandas DataFrame with
    two columns `source_node` and `target_node`. How long does it take to retrieve
    all the target_node IDs for a single source node? What about all the target_nodes
    for those new source nodes? How would you speed up the Pandas graph query with
    an index?
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `networkx` 的有向图将其转换为一个 Pandas DataFrame 的边列表，其中包含两列 `source_node` 和 `target_node`。对于单个源节点，检索所有目标节点
    ID 需要多长时间？对于这些新的源节点的所有目标节点呢？如何通过索引加速 Pandas 图查询？
- en: Create a Spacy Matcher that can more of Timnit Gebru’s places of work out of
    the Wikipedia articles about her. How many could you retrieve?
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 Spacy Matcher，可以从关于 Timnit Gebru 的维基百科文章中提取更多的工作地点。您能够检索到多少个？
- en: Is there anything a graph database can do that a relational database cannot?
    Can a relational database do anything that a graph database cannot?
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图数据库能做到的事情有关系数据库不能做到的吗？关系数据库能做到图数据库不能做到的事情吗？
- en: Use a Large Language Model to generate a SPARQL wikidata query from natural
    language. Did it work correctly without you editing the code? Will it work for
    a query that requires five relationship (edge) traversals in your knowledge graph?
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大型语言模型从自然语言生成 SPARQL wikidata 查询。在没有编辑代码的情况下，它是否正确工作？对于需要在您的知识图谱中进行五次关系（边）遍历的查询，它是否有效？
- en: 'Use `extractors.py` and `heatmaps.py` in `nlpia2.text_processing` to create
    a BERT similarity heatmap for sentences extracted from a long document of your
    own (perhaps a sequence of Mastodon microblog posts about NLP). Edit the `heatmaps.py`
    code to improve it so that you can focus on just the lines that are very similar.
    Hint: You can scale the cosine similarity values with a nonlinear function and
    reset the similarity values to zero using a threshold value.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `nlpia2.text_processing` 中的 `extractors.py` 和 `heatmaps.py` 为从您自己的长文档（可能是一系列关于自然语言处理的
    Mastodon 微博帖子）中提取的句子创建 BERT 相似度热图。编辑 `heatmaps.py` 代码以改进它，以便您可以专注于非常相似的行。提示：您可以使用非线性函数来缩放余弦相似度值，并使用阈值将相似度值重置为零。
- en: 11.11 Summary
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.11 总结
- en: A knowledge graph can be built to store relationships between entities.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识图谱可以用来存储实体之间的关系。
- en: You can isolate and extract information from unstructured text using either
    rule-based methods (like regular expressions) or neural-based methods.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用基于规则的方法（如正则表达式）或基于神经网络的方法来隔离和提取非结构化文本中的信息。
- en: Part-of-speech tagging and dependency parsing allow you to extract relationships
    between entities mentioned in a sentence.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注和依赖分析允许您提取句子中提到的实体之间的关系。
- en: Languages like SPARQL can help you find the information you need in a knowledge
    graph.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像 SPARQL 这样的语言可以帮助您在知识图谱中找到所需的信息。
- en: '[[1]](#_footnoteref_1) Wikipedia article "Symbolic AI" ( [https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence](wiki.html))'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#_footnoteref_1) Wikipedia 上的“Symbolic AI”文章（[https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence](wiki.html)）'
- en: '[[2]](#_footnoteref_2) See the web page titled "Natural Language Processing
    : TM-Town" ( [https://www.tm-town.com/natural-language-processing#golden_rules](www.tm-town.com.html)).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#_footnoteref_2) 参见名为“自然语言处理：TM-Town”的网页（[https://www.tm-town.com/natural-language-processing#golden_rules](www.tm-town.com.html)）。'
- en: '[[3]](#_footnoteref_3) "Rstr package on PyPi ( [https://pypi.org/project/rstr/](rstr.html)).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#_footnoteref_3) “PyPi 上的 Rstr 包”（[https://pypi.org/project/rstr/](rstr.html)）。'
- en: '[[4]](#_footnoteref_4) See the web page titled "Python sentence segment at
    DuckDuckGo" ( [https://duckduckgo.com/?q=Python+sentence+segment&t=canonical&ia=qa](duckduckgo.com.html)).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4]](#_footnoteref_4) 参见名为“在 DuckDuckGo 上搜索 Python 句子分段”的网页（[https://duckduckgo.com/?q=Python+sentence+segment&t=canonical&ia=qa](duckduckgo.com.html)）。'
- en: '[[5]](#_footnoteref_5) Manuscript source code on GitLab ( [https://gitlab.com/tangibleai/nlpia2/-/tree/main/src/nlpia2/data/manuscript/adoc](manuscript.html))'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5]](#_footnoteref_5) GitLab 上的手稿源代码（[https://gitlab.com/tangibleai/nlpia2/-/tree/main/src/nlpia2/data/manuscript/adoc](manuscript.html)）'
- en: '[[6]](#_footnoteref_6) Each neuron in a single-layer neural net or perceptron,
    is mathematically equivalent to a logistic regression.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#_footnoteref_6) 单层神经网络或感知器中的每个神经元，在数学上等同于逻辑回归。'
- en: '[[7]](#_footnoteref_7) See the web page titled "Facts & Figures : spaCy Usage
    Documentation" ( [https://spacy.io/usage/facts-figures](usage.html)).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#_footnoteref_7) 参见名为“Facts & Figures：spaCy 使用文档”的网页（[https://spacy.io/usage/facts-figures](usage.html)）。'
- en: '[[8]](#_footnoteref_8) See the web page titled "nltk.tokenize package — NLTK
    3.3 documentation" ( [http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt](api.html)).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#_footnoteref_8) 参见名为“nltk.tokenize 包 — NLTK 3.3 文档”的网页（[http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt](api.html)）。'
- en: '[[9]](#_footnoteref_9) SpaCy is far and away the most accurate and efficient
    NLP parser we’ve found, and it is maintained and updated regularly by a brilliant,
    supercooperating team of NLP engineers at Explosion.ai in Europe ( [https://explosion.ai/about](explosion.ai.html)).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#_footnoteref_9) SpaCy是迄今为止我们发现的最准确、最高效的NLP解析器，并由欧洲的一个出色的、超级合作的NLP工程师团队在Explosion.ai定期维护和更新（[https://explosion.ai/about](explosion.ai.html)）。'
- en: '[[10]](#_footnoteref_10) The `heatmaps.py` module in the nlpia2 package on
    GitLab ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/heatmaps.py](nlpia2.html))'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#_footnoteref_10) GitLab上nlpia2包中的`heatmaps.py`模块（[https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/heatmaps.py](nlpia2.html)）。'
- en: '[[11]](#_footnoteref_11) The `extractors.extract_lines()` function in the nlpia2
    package on GitLab ( [https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/text_processing/extractors.py#L69](text_processing.html))'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#_footnoteref_11) GitLab上nlpia2包中的`extractors.extract_lines()`函数（[https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/text_processing/extractors.py#L69](text_processing.html)）。'
- en: '[[12]](#_footnoteref_12) The official AsciiDoc parser is Ruby. No Python packages
    exist yet, according to the docs ( [https://gitlab.eclipse.org/eclipse-wg/asciidoc-wg/asciidoc.org/-/blob/main/awesome-asciidoc.adoc#convert](main.html))'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#_footnoteref_12) 官方AsciiDoc解析器是Ruby。根据文档，目前还没有Python包（[https://gitlab.eclipse.org/eclipse-wg/asciidoc-wg/asciidoc.org/-/blob/main/awesome-asciidoc.adoc#convert](main.html)）。'
- en: '[[13]](#_footnoteref_13) Statistics about AI research at the AI Index by Stanford
    University ( [https://AIIndex.org](.html))'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#_footnoteref_13) 斯坦福大学AI指数上有关AI研究的统计数据（[https://AIIndex.org](.html)）。'
- en: '[[14]](#_footnoteref_14) spaCy uses the "OntoNotes 5" POS tags: ( [https://spacy.io/api/annotation#pos-tagging](api.html))'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#_footnoteref_14) spaCy使用“OntoNotes 5”词性标签：（[https://spacy.io/api/annotation#pos-tagging](api.html)）。'
- en: '[[15]](#_footnoteref_15) See the web page titled "Code Examples : spaCy Usage
    Documentation" ( [https://spacy.io/usage/examples#phrase-matcher](usage.html)).'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15]](#_footnoteref_15) 查看名为“代码示例：spaCy使用文档”的网页（[https://spacy.io/usage/examples#phrase-matcher](usage.html)）。'
- en: '[[16]](#_footnoteref_16) See the web page titled "Matcher : spaCy API Documentation"
    ( [https://spacy.io/api/matcher](api.html)).'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '[[16]](#_footnoteref_16) 查看名为“匹配器：spaCy API文档”的网页（[https://spacy.io/api/matcher](api.html)）。'
- en: '[[17]](#_footnoteref_17) This is the subject of active research: [https://nlp.stanford.edu/pubs/structuredVS.pdf](pubs.html)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[[17]](#_footnoteref_17) 这是一项积极研究的课题：[https://nlp.stanford.edu/pubs/structuredVS.pdf](pubs.html)。'
- en: '[[18]](#_footnoteref_18) "Implementing a custom trainable component for relation
    extraction": ( [https://explosion.ai/blog/relation-extraction](blog.html))'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18]](#_footnoteref_18) “实现用于关系抽取的自定义可训练组件”：（[https://explosion.ai/blog/relation-extraction](blog.html)）。'
- en: '[[19]](#_footnoteref_19) There are hard-coded common-sense knowledge bases
    out there for you to build on. Google Scholar is your friend in this knowledge
    graph search.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[[19]](#_footnoteref_19) 有一些硬编码的常识知识库供您使用。Google Scholar是您在知识图谱搜索中的好朋友。'
- en: '[[20]](#_footnoteref_20) Wikidata SPARQL tutorial: ( [https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial](wiki.html))'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '[[20]](#_footnoteref_20) Wikidata SPARQL教程：（[https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial](wiki.html)）。'
- en: '[[21]](#_footnoteref_21) How to Build Knowledge Graph Enhanced Chatbot with
    ChatGPT and ArangoDB ( [http://archive.today/fJB7H](archive.today.html)).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[[21]](#_footnoteref_21) 如何使用ChatGPT和ArangoDB构建知识图谱增强的聊天机器人（[http://archive.today/fJB7H](archive.today.html)）。'
