- en: 'B.3\. Memory management in TensorFlow.js: tf.dispose() and tf.tidy()'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3\. TensorFlow.js中的内存管理：`tf.dispose()`和`tf.tidy()`
- en: 'In TensorFlow.js, if you deal directly with tensor objects, you need to perform
    memory management on them. In particular, a tensor needs to be disposed after
    creation and use, or it will continue to occupy the memory allocated for it. If
    undisposed tensors become too many in number or too large in their total size,
    they will eventually cause the browser tab to run out of WebGL memory or cause
    the Node.js process to run out of system or GPU memory (depending on whether the
    CPU or GPU version of tfjs-node is being used). TensorFlow.js does not perform
    automatic garbage collection of user-created tensors.^([[5](#app02fn5)]) This
    is because JavaScript does not support object finalization. TensorFlow.js provides
    two functions for memory management: `tf.dispose()` and `tf.tidy()`.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow.js中，如果你直接处理张量对象，你需要对它们执行内存管理。特别是在创建和使用张量后，张量需要被释放，否则它将继续占用分配给它的内存。如果未释放的张量数量过多或者总大小过大，它们最终将导致浏览器标签页耗尽WebGL内存或导致Node.js进程耗尽系统或GPU内存（取决于是否使用tfjs-node的CPU或GPU版本）。TensorFlow.js不会自动对用户创建的张量进行垃圾回收。[[5]](#app02fn5)
    这是因为JavaScript不支持对象终结。TensorFlow.js提供了两个内存管理函数：`tf.dispose()`和`tf.tidy()`。
- en: ⁵
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, the tensors created inside TensorFlow.js functions and object methods
    are managed by the library itself, so you don’t need to worry about wrapping calls
    to such functions or methods in `tf.tidy()`. Examples of such functions include
    `tf.confusionMatrix()`, `tf.Model.predict()`, and `tf.Model.fit()`.
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，在TensorFlow.js函数和对象方法中创建的张量由库本身管理，因此你不需要担心在调用这些函数或方法时包装它们在`tf.tidy()`中。其中的示例函数包括`tf.confusionMatrix()`、`tf.Model.predict()`和`tf.Model.fit()`。
- en: 'For example, consider an example in which you perform repeated inference on
    a TensorFlow.js model using a `for` loop:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑使用`for`循环对TensorFlow.js模型进行重复推理的示例：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1*** Loads a pretrained model from the web'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 从网络上加载预先训练好的模型'
- en: '***2*** Creates a dummy input tensor'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 创建一个虚拟输入张量'
- en: '***3*** Checks the number of currently allocated tensors'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 检查当前已分配张量的数量'
- en: The output will look like
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see in the console log, every time `model.predict()` is called, it
    generates an additional tensor, which doesn’t get disposed after the iteration
    ends. If the `for` loop is allowed to run for enough iterations, it will eventually
    cause an out-of-memory error. This is because the output tensor `y` is not disposed
    properly, leading to a tensor memory leak. There are two ways to fix this memory
    leak.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在控制台日志中看到的那样，每次调用`model.predict()`都会生成一个额外的张量，在迭代结束后不会被释放。如果允许`for`循环运行足够数量的迭代，它最终会导致内存不足错误。这是因为输出张量`y`没有被正确释放，导致张量内存泄漏。有两种方法可以修复这个内存泄漏。
- en: 'In the first approach, you can call `tf.dispose()` on the output tensor when
    it is no longer needed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法中，你可以在不再需要输出张量时调用`tf.dispose()`：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1*** Disposes the output tensor after its use'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 在使用后释放输出张量'
- en: 'In the second approach, you can wrap the body of the `for` loop with `tf.tidy()`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种方法中，你可以在`for`循环的主体部分使用`tf.tidy()`：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1*** tf.tidy() automatically disposes all tensors created within the function
    passed to it except the tensors that are returned by the function.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** `tf.tidy()`自动释放传递给它的函数中创建的所有张量，除了由该函数返回的张量。'
- en: 'With either approach, you should see the number of allocated tensors become
    constant over the iterations, indicating that there is no tensor memory leak anymore.
    Which approach should you prefer? In general, you should use `tf.tidy()` (the
    second approach), because it gets rid of the need to keep track of what tensors
    to dispose. `tf.tidy()` is a smart function that disposes all tensors created
    within the anonymous function passed to it as the argument (except those that
    are returned by the function—more on that later), even for the tensors not bound
    to any JavaScript objects. For example, suppose we modify the previous inference
    code slightly in order to obtain the index of the winning class using `argMax()`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选用哪种方法，你应该看到迭代中分配的张量数量变为常数，表明不再有张量内存泄漏。哪种方法应该优先使用呢？通常，你应该使用`tf.tidy()`（第二种方法），因为它消除了跟踪需要释放哪些张量的需要。`tf.tidy()`是一个智能函数，它释放传递给它的匿名函数中创建的所有张量（除了由该函数返回的张量-稍后再说），即使这些张量没有绑定到任何JavaScript对象。例如，假设我们稍微修改先前的推理代码，以便使用`argMax()`获得获胜类别的索引：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When this code runs, you will see that instead of leaking one tensor per iteration,
    it leaks two:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当这段代码运行时，你会发现它每次迭代泄漏两个张量：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Why are two tensors leaked per iteration? Well, the line
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么每次迭代泄漏两个张量？因为这行代码：
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'generates two new tensors. The first is the output of `model.predict()`, and
    the second is the return value of `argMax()`. Neither of the tensors is bound
    to any JavaScript object. They are used immediately after creation. The two tensors
    are “lost” in the sense that there are no JavaScript objects you can use to refer
    to them. Hence, `tf.dispose()` cannot be used to clean up the two tensors. However,
    `tf.tidy()` can still be used to fix the memory leak, as it performs bookkeeping
    on new tensors regardless of whether they are bound to JavaScript objects:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 生成两个新的张量。第一个是`model.predict()`的输出，第二个是`argMax()`的返回值。这两个张量都没有绑定到任何 JavaScript
    对象上。它们被创建后立即使用。这两个张量在某种意义上“丢失”——没有 JavaScript 对象可供您用来引用它们。因此，`tf.dispose()`不能用于清理这两个张量。但是，`tf.tidy()`仍然可以用来修复内存泄漏，因为它会对新张量执行簿记，无论它们是否绑定到
    JavaScript 对象上：
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1*** tf.tidy() automatically disposes tensors created in the body of an
    anonymous function passed to it as the argument, even when those tensors are not
    bound to JavaScript objects'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** `tf.tidy()`会自动处理传递给它作为参数的匿名函数中创建的张量，即使这些张量没有绑定到 JavaScript 对象上。'
- en: 'The example usages of `tf.tidy()` operate on functions that do not return any
    tensors. If the function returns tensors, you do not want them to be disposed
    because they need to be used afterward. This situation is encountered frequently
    when you write custom tensor operations by using the basic tensor operations provided
    by TensorFlow.js. For example, suppose we want to write a function that calculates
    the normalized value of the input tensor—that is, a tensor with the mean subtracted
    and the standard deviation scaled to 1:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.tidy()`的示例用法操作的是不返回任何张量的函数。如果该函数返回张量，则不希望将它们处理掉，因为它们需要在后续使用。这种情况在使用 TensorFlow.js
    提供的基本张量操作编写自定义张量操作时经常遇到。例如，假设我们想编写一个函数来计算输入张量的标准化值——即，减去平均值并将标准差缩放为1的张量：'
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'What is the problem with this implementation?^([[6](#app02fn6)]) In terms of
    memory management, it leaks a total of three tensors: 1) the mean, 2) the SD,
    and 3) a more subtle one: the return value of the `sub()` call. To fix the memory
    leak, we wrap the body of the function with `tf.tidy()`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现有什么问题？^([[6](#app02fn6)]) 就内存管理而言，它泄漏了三个张量：1）均值，2）SD 和 3）一个更微妙的泄漏：`sub()`调用的返回值。为了修复内存泄漏问题，我们将函数体包装在`tf.tidy()`中：
- en: ⁶
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are other problems with this implementation. For instance, it doesn’t
    perform sanity checks on the input tensor to make sure it has at least two elements
    so SD won’t be zero, which would lead to division by zero and infinite results.
    But those problems are not directly related to the discussion here.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个实现还有其他问题。例如，它没有对输入张量进行健全性检查，以确保它至少有两个元素，使 SD 不为零，否则将导致除以零和无限结果。但是这些问题与此处的讨论无直接关联。
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, `tf.tidy()` does three things for us:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`tf.tidy()`为我们完成了三个操作：
- en: It automatically disposes the tensors that are created in the anonymous function
    but not returned by it, including all three leaks mentioned. We have seen this
    in the previous examples.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会自动处理在匿名函数中创建但未被它返回的张量，包括之前提到的所有泄漏。我们在之前的例子中已经看到这一点了。
- en: It detects that the output of the `div()` call is returned by the anonymous
    function and hence will forward it to its own return value.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它检测到`div()`调用的输出由匿名函数返回，因此将其转发到自己的返回值。
- en: In the meantime, it will avoid disposing that particular tensor, so it can be
    used outside the `tf.tidy()` call.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此期间，它将避免处理该特定张量，以便它可以在`tf.tidy()`调用之外使用。
- en: 'As we can see, `tf.tidy()` is a smart and powerful function for memory management.
    It is used extensively in the TensorFlow.js code base itself. You will also see
    it many times throughout the examples in this book. However, it has the following
    important limitation: the anonymous function passed to `tf.tidy()` as the argument
    must *not* be async. If you have some async code that requires memory management,
    you should use `tf.dispose()` and keep track of the to-be-disposed tensors manually
    instead. In such cases, you can use `tf.memory().numTensor` to check the number
    of leaked tensors. A good practice is to write unit tests that assert on the absence
    of memory leaks.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`tf.tidy()` 是一种智能而强大的内存管理函数。它在 TensorFlow.js 代码库中被广泛使用。在本书的示例中，您还将经常看到它。然而，它有以下重要限制：作为参数传递给
    `tf.tidy()` 的匿名函数 *不* 能是异步的。如果您有一些需要内存管理的异步代码，您应该使用 `tf.dispose()` 并手动跟踪待处理的张量。在这种情况下，您可以使用
    `tf.memory().numTensor` 来检查泄漏张量的数量。一个好的做法是编写单元测试来断言不存在内存泄漏。
