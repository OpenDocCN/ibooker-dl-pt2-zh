- en: 10 Adopting PyTorch Lightning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用 PyTorch Lightning
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Implementing PyTorch Lightning to reduce boilerplate code
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning 减少样板代码
- en: Adding training, validation, and test support for the DC taxi model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 DC 出租车模型添加训练、验证和测试支持
- en: Analyzing DC taxi model training and validation using pandas
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 分析 DC 出租车模型的训练和验证
- en: Thus far, you have written your own implementation related to training and testing
    your machine learning model. However, much of the code you wrote was unrelated
    to your machine learning model architecture and could have applied to a broad
    range of distinct models. Building on this observation, this chapter introduces
    you to PyTorch Lightning, a framework that can help you reduce the amount of boilerplate
    engineering code in your machine learning system, and consequently help you focus
    on evolving your model design and implementation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经编写了与训练和测试您的机器学习模型相关的实现。然而，您编写的大部分代码与您的机器学习模型架构无关，可以适用于广泛范围的不同模型。基于这一观察结果，本章介绍了
    PyTorch Lightning，这是一个可以帮助您减少机器学习系统中样板工程代码量的框架，并因此帮助您专注于发展您的模型设计和实现的框架。
- en: 10.1 Understanding PyTorch Lightning
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 PyTorch Lightning
- en: This section introduces the PyTorch Lightning framework for your PyTorch DC
    taxi fare estimation model and teaches you the steps involved in enabling PyTorch
    Lightning training, validation, and test features.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 PyTorch Lightning 框架，用于您的 PyTorch DC 出租车车费估算模型，并教您如何启用 PyTorch Lightning
    训练、验证和测试特性的步骤。
- en: Thus far, you have implemented a sizable portion of Python and PyTorch boilerplate
    code for your machine learning model. This meant that only a few parts of your
    implementation were model specific, such as the code to
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止，您已经为您的机器学习模型实现了大部分的 Python 和 PyTorch 样板代码。这意味着你的实现中只有少部分是模型特定的，比如
- en: Package the feature values as tensors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征值打包为张量
- en: Configure the neural net layers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置神经网络层
- en: Calculate the tensors for the loss
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失的张量
- en: Report on the model metrics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型指标报告
- en: Much of the remaining code, such as the code to iterate over the training batches,
    validation batches, and epochs of training, is largely boilerplate, meaning that
    it can be re-used unmodified across various changes to the model-specific code.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的大部分代码，比如迭代培训批次、验证批次和培训纪元的代码，主要是样板代码，这意味着它可以在模型特定的代码发生各种变化时不加修改地重复使用。
- en: 'As your machine learning system grows more complex, so does the boilerplate
    code in your system’s implementation. For example, mature machine learning systems
    require periodic saving (checkpointing) of the model’s weight values to storage
    in order to enable reproducibility. Having model checkpoints also enables machine
    learning training pipelines to resume from a pre-trained model. Other examples
    include the code involved in integration with hyperparameter optimization services,
    metric tracking, and other experiment management tools that control repeated executions
    of a machine learning pipeline. This should not come as a surprise: recall from
    chapter 1 that over 90% of the components of a production machine learning system
    are complementary to the core machine learning code.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您的机器学习系统变得更加复杂，系统实现中的样板代码也会变得更加复杂。例如，成熟的机器学习系统需要周期性保存（检查点）模型的权重值到存储介质，以便实现可复制性。拥有模型检查点还可以使机器学习训练流程从预训练模型中恢复。其他例子包括与超参数优化服务、指标跟踪和控制机器学习流程的其他实验管理工具集成的代码。这不应该令人惊讶：回想一下第一章中提到的，一个生产级机器学习系统的组件中有超过
    90% 是辅助于核心机器学习代码的。
- en: 'The PyTorch Lightning framework ([https://www.pytorchlightning.ai](https://www.pytorchlightning.ai))
    aims to make PyTorch developers more productive by helping them focus on developing
    core machine learning code instead of getting distracted by the boilerplate. In
    case of the DC taxi model, adopting PyTorch Lightning is straightforward. Before
    starting, you need to ensure that you have the pip package for PyTorch Lightning
    installed by running the following in your shell environment:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'PyTorch Lightning 框架 ([https://www.pytorchlightning.ai](https://www.pytorchlightning.ai))
    的目标是通过帮助开发者专注于开发核心机器学习代码而不被样板代码分散注意力来提高 PyTorch 开发者的生产力。就 DC 出租车模型而言，采用 PyTorch
    Lightning 是直接的。在开始之前，您需要确保已经在您的 shell 环境中运行以下内容安装了 PyTorch Lightning 的 pip 包:'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: PyTorch Lightning is a comprehensive framework with a sizable feature set for
    machine learning model development. This book does not aim to replace existing
    PyTorch Lightning tutorials or documentation; instead, the upcoming sections focus
    on the features of the framework you can adopt for the DC taxi model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 是一个全面的框架，具有可观的特性集，用于机器学习模型开发。本书不旨在取代现有的 PyTorch Lightning
    教程或文档；相反，接下来的章节专注于您可以为 DC 出租车模型采用的框架特性。
- en: 10.1.1 Converting PyTorch model training to PyTorch Lightning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 将 PyTorch 模型训练转换为 PyTorch Lightning
- en: This section teaches you about the PyTorch Lightning __init__, training_step
    and configure_optimizers methods and then illustrates how to implement these methods
    for the DC taxi model and how to train the PyTorch Lighting—based DC taxi model
    using a small, sample training data set.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本节教你关于 PyTorch Lightning 的 __init__、training_step 和 configure_optimizers 方法，然后演示如何为
    DC 出租车模型实现这些方法以及如何使用小型的示例训练数据集训练基于 PyTorch Lightning 的 DC 出租车模型。
- en: Assuming that the PyTorch Lighting package is installed correctly in your environment,
    you can implement a minimal, trainable version of the DC taxi model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 PyTorch Lightning 包在您的环境中正确安装，您可以实现一个最小的、可训练的 DC 出租车模型。
- en: Listing 10.1 A basic PyTorch Lightining DC taxi model with support
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：带有支持的基本 PyTorch Lightning DC 出租车模型
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Import the PyTorch Lightning library and alias it as pl.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 PyTorch Lightning 库，并将其别名为 pl。
- en: ❷ Use torch.float64 for dtype of the model parameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 torch.float64 作为模型参数的 dtype。
- en: ❸ PyTorch Lightning models must extend from LightningModule.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PyTorch Lightning 模型必须扩展自 LightningModule。
- en: ❹ The **kwargs are used to pass hyperparameters to the model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 **kwargs 传递超参数给模型。
- en: ❺ The LightningModule subclass must call parent __init__ first.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ LightningModule 子类必须首先调用父类的 __init__。
- en: ❻ Save hyperparameters from **kwargs to self.hparams.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将 **kwargs 中的超参数保存到 self.hparams。
- en: ❼ Set the pseudorandom number generator per hyperparameter settings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 根据超参数设置设置伪随机数生成器。
- en: ❽ Use a simple linear regression model for this illustration.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 为此示例使用简单的线性回归模型。
- en: ❾ Re-use functions from chapter 7 for the batchToXy . . .
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 重用第 7 章的函数进行 batchToXy 的 . . .
- en: ❿ . . . and forward implementations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ . . . 以及前向实现。
- en: ⓫ Use PyTorch Lightning built-in logging for MSE and RMSE measures.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 使用 PyTorch Lightning 内置的日志记录来记录 MSE 和 RMSE 测量值。
- en: ⓬ The training_step method must return the loss tensor.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ training_step 方法必须返回损失张量。
- en: ⓭ A LightningModule subclass must have a configure_optimizers method.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ LightningModule 子类必须有一个 configure_optimizers 方法。
- en: ⓮ Return a configured optimizer instance specified by hyperparameters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 返回由超参数指定的配置好的优化器实例。
- en: ⓯ Instantiate a PyTorch Lightning version of the DcTaxiModel as model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ⓯ 实例化一个 PyTorch Lightning 版本的 DcTaxiModel，命名为 model。
- en: 'Notice that the DcTaxiModel class inherits from the base pl.LightningModule
    class, which requires a separate initialization in the __init__ method by the
    super() .__init__() method call. The rest of the __init__ method of the class
    is simplified here for illustration and to highlight the following key concepts:
    storage of a reference to the model’s hyperparameters in self.hparams as well
    as the instantiation of the model parameters in the self.layers instance.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，DcTaxiModel 类继承自基础的 pl.LightningModule 类，在 __init__ 方法中通过 super().__init__()
    方法调用需要单独初始化。类的其余 __init__ 方法在此处被简化以进行说明，并突出显示以下关键概念：将模型的超参数存储在 self.hparams 中以及在
    self.layers 实例中实例化模型参数。
- en: The training_step method is the workhorse of the PyTorch Lightning implementation,
    performing the forward step through the model layers, computing the loss, and
    returning the loss value. Notice that it relies on the batchToXy method (introduced
    in chapter 7) responsible for converting a batch of training examples into the
    format suitable for model training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: training_step 方法是 PyTorch Lightning 实现的工作核心，执行模型层的前向步骤，计算损失并返回损失值。请注意，它依赖于 batchToXy
    方法（在第 7 章介绍），该方法负责将一批训练样例转换为适合模型训练的格式。
- en: The conversion amounts to eliminating any dimensions that have a shape of 1
    using the squeeze_ method. For example, a tensor with a shape of [1, 128, 5, 1]
    is reshaped to [128, 5] after the application of squeeze_. The use of squeeze_
    (with the trailing underscore) instead of the squeeze method is a minor performance
    optimization. Recall from chapter 5 that the trailing underscore in squeeze_ indicates
    that this PyTorch method performs the operation in place, mutating the tensor,
    as opposed to returning a new tensor instance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 转换就是使用 squeeze_ 方法消除任何形状为 1 的维度。例如，形状为 [1, 128, 5, 1] 的张量在 squeeze_ 应用后被重新调整为
    [128，5]。在 squeeze_ 中使用下划线进行了小幅度的性能优化。回想一下第五章所讲的，squeeze_ 中的下划线表示 PyTorch 方法将就地执行操作，对张量进行突变，而不是返回新的张量实例。
- en: The DcTaxiModel implementation assumes that the first column in the tensor is
    the label with the remaining columns as features, so the concluding portion of
    the batchToXy code simply aliases the label as y and features as X, returning
    the result.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: DcTaxiModel 的实现假定张量中的第一列是标签，其余列是特征。因此，在 batchToXy 代码的结尾部分，只需将标签简单地别名为 y，将特征别名为
    X 并返回结果。
- en: The calls to self.log in the training_step method report the training MSE and
    RMSE values computed by the model. As explained in chapter 5, in the PyTorch tensor
    API the item method of a PyTorch scalar tensor returns a regular Python value
    instead of a tensor. Hence, the values that are logged using self.log are Python
    numerics rather than PyTorch tensors. The self.log method of PyTorch Lightning
    is a generic API for an extensible logging framework, which will be covered in
    more detail later in this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 training_step 方法中调用 self.log 报告模型计算出的训练 MSE 和 RMSE 值。正如第五章所解释的那样，在 PyTorch
    张量 API 中，标量张量的 item 方法返回常规的 Python 值而不是张量。因此，使用 self.log 记录的值是 Python 数值而不是 PyTorch
    张量。PyTorch Lightning 的 self.log 方法是一个可扩展的日志框架的通用 API，稍后在本章中有更详细的介绍。
- en: The configure_optimizers method in the example uses a dictionary of optimizers
    in order to enable the model to switch between different optimization algorithms
    (Adam versus SGD) based on the value of the optimizer hyperparameter. Although
    this implementation of model training does not yet use hyperparameter optimization,
    the dictionary-based lookup approach shown in configure_optimizers ensures that
    the model code does not need to be changed at the later stages of development
    when hyperparameter optimization is enabled.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的 configure_optimizers 方法使用优化器字典，以便使模型能够根据优化器超参数的值在不同的优化算法（Adam 和 SGD）之间切换。尽管这种模型训练的实现尚未使用超参数优化，但在
    configure_optimizers 中展示的基于字典查找的方法确保了当后续开发启用超参数优化时，模型代码无需改变。
- en: In PyTorch Lightning, training of a model is performed using an instance of
    a Trainer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch Lightning 中，使用一个 Trainer 实例来训练模型。
- en: Listing 10.2 PyTorch Lightining Trainer to train a subclass
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 10.2：使用 PyTorch Lightining Trainer 训练子类
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ CSVLogger is used to illustrate analysis with pandas.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 CSVLogger 可以用 pandas 进行分析。
- en: ❷ The seed hyperparameter is used to uniquely identify the model log.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ seed 超参数被用于唯一标识模型日志。
- en: ❸ Use multiple GPUs for training when available.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在有多个 GPU 可用时使用多个 GPU 进行训练。
- en: ❹ Use 1 since the training duration is controlled by max_batches.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在此将其设置为 1，因为训练持续时间由 max_batches 控制。
- en: ❺ Use max_batches to set the number of the training iterations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 max_batches 来设置训练迭代的次数。
- en: ❻ Ensure that every call to self.log is persisted in the log.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 确保每次调用 self.log 的记录都被保存在日志中。
- en: ❼ Persist the values sent to self.log based on csvLog setup.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 基于 csvLog 设置将发送到 self.log 的值保存下来。
- en: 'The hyperparameter values can be applied to the machine learning pipeline and
    not just to the model: for example, the max_batches hyperparameter controls the
    duration of the model training. As you will see in the remainder of this chapter,
    hyperparameter values can be used throughout the stages of the machine learning
    pipeline. The max_epochs settings in the code example is designed to ensure that
    the training pipeline can support both Iterable as well as Map PyTorch data sets.
    Recall from chapter 7 that IterableDataset instances have a variable number of
    examples per training data set; hence, training with this category is controlled
    by limiting the number of training batches. This number is specified using the
    limit_train_batches parameter of the Trainer.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数值可以应用于机器学习流水线，而不仅仅是模型：例如，max_batches 超参数控制模型训练的持续时间。正如你将在本章的其余部分中看到的，超参数值可以在机器学习流水线的各个阶段使用。代码示例中的
    max_epochs 设置旨在确保训练流水线可以支持 Iterable 和 Map PyTorch 数据集。回想一下第 7 章，IterableDataset
    实例具有可变数量的训练数据集示例；因此，对这一类别的训练受到限制，限制训练批次的数量。这个数字是使用 Trainer 的 limit_train_batches
    参数指定的。
- en: The progress_bar_refresh_rate and weight_summary settings in listing 10.2 are
    reasonable defaults to use with the Trainer to minimize the amount of the logging
    information reported during training. If you prefer to have a report on the training
    model parameters, you can change weights_summary to "full" to report all weights,
    or to "top" to report just the weights of the top (the ones connected to the trunk
    of the model) layers in the model. Similarly, the progress_bar_refresh_rate can
    be changed to an integer value representing the frequency (in terms of the number
    of training steps) for how often to redraw a progress bar showing progress toward
    training completion.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 10.2 中，progress_bar_refresh_rate 和 weight_summary 设置是 Trainer 的合理默认设置，以最小化训练过程中报告的日志信息量。如果你喜欢对训练模型参数进行报告，你可以将
    weights_summary 更改为 "full"，以报告所有权重，或者更改为 "top"，以仅报告模型中顶部（连接到模型主干的权重）的层的权重。同样，progress_bar_refresh_rate
    可以更改为表示多久重新绘制一次显示训练完成进度的进度条的整数值（以训练步骤的数量为单位）。
- en: To provide the model with the training examples, you can use the ObjectStorageDataset
    introduced in chapter 7\. Before executing the code snippet in the next example,
    ensure that you have the Kaen framework installed using
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要提供训练样本给模型，你可以使用第 7 章介绍的 ObjectStorageDataset。在执行下一个示例中的代码片段之前，请确保已安装 Kaen 框架使用
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, to execute just the training of the model, you can invoke the fit method
    on the instance of pl.Trainer, passing in a PyTorch DataLoader with the training
    examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，要仅执行模型的训练，你可以在 pl.Trainer 的实例上调用 fit 方法，传入一个包含训练样本的 PyTorch DataLoader：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the example, the sample of the DC taxi data set available from [http://mng.bz/nr9a](http://mng.bz/nr9a)
    is used to simply illustrate how to use PyTorch Lightning. In the next chapter,
    you will see how to scale to larger data sets by simply changing the URL string
    passed to osds.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在例子中，使用来自 [http://mng.bz/nr9a](http://mng.bz/nr9a) 的 DC 出租车数据集的样本简单说明了如何使用 PyTorch
    Lightning。在下一章中，你将看到如何通过简单地更改传递给 osds 的 URL 字符串来扩展到更大的数据集。
- en: Since during training the loss and metric values are logged to a CSV file, once
    the training is finished, you can load the values into a pandas DataFrame and
    plot the results using
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在训练过程中损失和指标值被记录到一个 CSV 文件中，一旦训练结束，你可以将这些值加载到一个 pandas DataFrame 中，并使用以下方式绘制结果，
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: which should output a graph resembling figure 10.1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出一个类似于图 10.1 的图形。
- en: '![10-01](Images/10-01.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![10-01](Images/10-01.png)'
- en: Figure 10.1 On a small sample, the trivial linear regression model converges
    as expected.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 在一个小样本上，微不足道的线性回归模型按预期收敛。
- en: According to figure 10.1, the naive linear regression model from listing 10.1
    converges to a consistent loss value. To examine the details of the loss value
    at the trailing 25 steps of the convergence, you can again take advantage of pandas
    DataFrame APIs,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图 10.1，来自列表 10.1 的简单线性回归模型收敛到一致的损失值。要检查收敛的最后 25 步的损失值的详细信息，你可以再次利用 pandas
    DataFrame API，
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: which plots figure 10.2.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这绘制了图 10.2。
- en: '![10-02](Images/10-02.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![10-02](Images/10-02.png)'
- en: Figure 10.2 Trailing 25 steps of training converge to a RMSE of roughly 4.0.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 训练的最后 25 步收敛到大约 4.0 的 RMSE。
- en: You can confirm that toward the last 25 steps of training, the model converges
    at an average RMSE of about 4.0\. Even with the trivial linear regression model,
    this should not come as a surprise since this illustration used a tiny training
    sample.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以确认在训练的最后 25 步左右，模型以大约 4.0 的平均 RMSE 收敛。即使对于微不足道的线性回归模型，这也不应该令人惊讶，因为本示例使用了一个小训练样本。
- en: At this point, it is useful to introduce a build function that can be invoked
    to instantiate, train, and later validate, as well as test the model. For convenience,
    here’s the entire implementation of this version of the model with the training
    steps encapsulated.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，引入一个 build 函数很有用，可以调用它来实例化、训练，以及稍后验证和测试模型。为了方便起见，以下是此版本模型的完整实现，其中包含了训练步骤的封装。
- en: Listing 10.3 Basic PyTorch Lightining DC taxi model
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.3 基本的 PyTorch Lightning DC 出租车模型
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 10.1.2 Enabling test and reporting for a trained model
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 启用已训练模型的测试和报告
- en: This section describes the test_step method for a PyTorch Lightning model and
    how the method can be used to test and report on the metrics of a trained model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了 PyTorch Lightning 模型的 test_step 方法以及如何使用该方法测试和报告已训练模型的指标。
- en: 'Once the model is trained, the Trainer instance can also be used to report
    on the model loss and metrics against a test data set. However, in order to support
    testing in PyTorch Lightning, a LightningModule subclass must be extended with
    an implementation of a test_step method. The following code snippet describes
    the corresponding implementation for DcTaxiModel:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，Trainer 实例也可以用于报告模型在测试数据集上的损失和度量。但是，为了支持 PyTorch Lightning 中的测试，必须将
    LightningModule 子类扩展为实现 test_step 方法。以下代码片段描述了 DcTaxiModel 的相应实现：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Ignore the gradient graph during testing for better performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在测试期间忽略梯度图以获得更好的性能。
- en: ❷ Use test_mse instead of train_mse . . .
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 test_mse 而不是 train_mse . . .
- en: ❸ . . . and test_rmse instead of train_rmse when logging test measurements.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ . . . 并在记录测试测量时使用 test_rmse 而不是 train_rmse。
- en: The PyTorch Lightning test_step does not require any return values; instead,
    the code is expected to report the metrics computed using a trained model. Recall
    from the discussion of autodiff in chapter 6 that maintaining the backward graph
    of the gradients carries additional performance overhead. Since the model gradients
    are not needed during model testing (or validation), the forward and mse_loss
    methods are invoked in the context of with pt.no_grad():, which disables the tracking
    needed for gradient calculations of the loss.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 的 test_step 不需要任何返回值；相反，代码应报告使用训练模型计算的指标。回顾第 6 章中的自动微分讨论，保持梯度的反向图会带来额外的性能开销。由于在模型测试（或验证）期间不需要模型梯度，因此在
    pt.no_grad() 的上下文中调用 forward 和 mse_loss 方法，该上下文禁用了用于损失梯度计算的跟踪。
- en: Outside of the minor change related to renaming the logged loss and metric measures
    (e.g., test_rmse versus train_rmse), the implementation of test_step logging is
    identical to the one from training_step function.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与重命名的记录损失和度量测量相关的轻微更改（例如，test_rmse 对比 train_rmse），test_step 记录的实现与 training_step
    函数完全相同。
- en: 'To introduce configuration changes to the Trainer instance for testing and
    to create the DataLoader for the test data, the build function needs to be modified
    ❶—❹:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向 Trainer 实例引入配置更改以进行测试并创建测试数据的 DataLoader，需要修改 build 函数 ❶—❹：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Pass URL globs to instantiate DataLoader for training and test data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过 URL 通配符实例化 DataLoader 以用于训练和测试数据。
- en: ❷ Use the test data set only once to report on the loss and metric measures.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仅使用测试数据集一次来报告损失和度量。
- en: ❸ Use test_glob to instantiate the train_ds . . .
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 test_glob 实例化 train_ds . . .
- en: ❹ . . . and train_dl instances.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ . . . 并创建 train_dl 实例。
- en: ❺ Test and report on the model performance using the Trainer.test method.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 Trainer.test 方法测试并报告模型性能。
- en: After executing the training and test using the updated model and build implementation
    using
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用更新后的模型和构建实现进行训练和测试之后
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'you should get test results resembling the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该获得类似以下的测试结果：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 10.1.3 Enabling validation during model training
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 启用模型训练期间的验证
- en: This section illustrates how to use the validation_step method in a LightningModule
    subclass to enable support for PyTorch model validation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节说明了如何在 LightningModule 子类中使用 validation_step 方法来启用对 PyTorch 模型的验证支持。
- en: 'The advantages of using PyTorch Lightning become more evident when you modify
    your implementation to support recurring validation steps during training. For
    example, to add model validation to the DcTaxiModel implementation, you simply
    introduce the validation_step method:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当您修改实现以支持训练期间的重复验证步骤时，使用 PyTorch Lightning 的优势变得更加明显。例如，要将模型验证添加到 DcTaxiModel
    实现中，只需引入 validation_step 方法：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following code describes the remaining changes needed to configure the
    trainer instance to perform validation over a fixed-sized data set (as opposed
    to k-fold cross validation):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码描述了配置训练器实例以在固定大小的数据集上执行验证（而不是 k 折交叉验证）所需的剩余更改：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Validate just 1 batch of validation DataLoader data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅验证 1 批验证 DataLoader 数据。
- en: ❷ Validate before training to ensure the validation data set is available.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在训练之前进行验证以确保验证数据集可用。
- en: ❸ Validate after every 20 training iterations (steps) of gradient descent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在每 20 次梯度下降的训练迭代（步骤）之后进行验证。
- en: The limit_val_batches acts similarly to limit_train_batches in specifying the
    number of batches from the validation Dataset to use for validation. The num_sanity_val_
    steps parameter to the Trainer controls a feature of PyTorch Lightning that uses
    the validation data set to ensure that the model, as well as the validation DataLoader,
    are instantiated correctly and are ready for training. In the example, setting
    the value of num_sanity_val_steps to 1 performs a single validation step and reports
    the corresponding metrics. The val_check_interval parameter specifies that at
    most after every 20 iterations of training, PyTorch Lightning should perform validation
    using the number of batches specified by the limit_val_batches parameters. The
    use of the min function with val_check_interval ensures that if the hyperparameter
    for max_batches is set to be less than 20, the validation is performed at the
    conclusion of training.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: limit_val_batches 的作用类似于 limit_train_batches，指定用于验证的验证数据集中的批次数量。Trainer 中的 num_sanity_val_steps
    参数控制了 PyTorch Lightning 的一个特性，该特性使用验证数据集来确保模型以及验证 DataLoader 被正确实例化并准备好进行训练。在本示例中，将
    num_sanity_val_steps 的值设置为 1 执行单个验证步骤并报告相应的指标。val_check_interval 参数指定，在每 20 次训练迭代之后，PyTorch
    Lightning 应使用 limit_val_batches 参数指定的批次数进行验证。使用 val_check_interval 的 min 函数确保如果
    max_batches 的超参数设置为小于 20，则在训练结束时执行验证。
- en: Listing 10.4 PyTorch Lightning DC taxi linear regression model
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.4 PyTorch Lightning DC 出租车线性回归模型
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can train, validate, and test the entire model by running
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行来训练、验证和测试整个模型
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you load the resulting logs from the logs/dctaxi/version_1686523060 folder
    as a pandas DataFrame and plot the result using
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将来自 logs/dctaxi/version_1686523060 文件夹的生成日志加载为 pandas DataFrame，并使用以下代码绘制结果
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: you should observe a graph resembling figure 10.3\. Since the val_check_interval
    parameter was set to 20, most of the values for the val_rmse_step column in the
    data frame are missing. The fillna(method='ffill') call fills the missing values
    forward, for example by setting missing values for steps 81, 82, and so on, based
    on the validation RMSE from step 80.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该观察到类似于图 10.3 的图形。由于 val_check_interval 参数设置为 20，因此数据帧中 val_rmse_step 列的大多数值都缺失。fillna(method='ffill')
    调用向前填充缺失值，例如，根据步骤 80 的验证 RMSE 设置步骤 81、82 等的缺失值。
- en: '![10-03](Images/10-03.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![10-03](Images/10-03.png)'
- en: Figure 10.3 Despite reasonable test performance, validation RMSE signals overfitting.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 尽管测试性能合理，但验证 RMSE 信号表明过拟合。
- en: The lackluster performance against the validation data set, as shown in figure
    10.3, points at the likelihood of the model overfitting on the training data set.
    Before putting the code in production, the model implementation should be refactored
    to become more generalizable and less reliant on memorization of the training
    data set. This means that to move forward, you need a model development approach
    with more comprehensive support for experimentation and hyperparameter optimization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 10.3 所示，对验证数据集的平淡性能表明模型可能在训练数据集上过拟合。在将代码投入生产之前，应重构模型实现以使其更具一般性，并且不那么依赖于训练数据集的记忆。这意味着为了前进，您需要具有更全面的实验和超参数优化支持的模型开发方法。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Adopting the PyTorch Lightning framework can help you refactor your machine
    learning implementation to reduce the fraction of incidental, boilerplate code
    and to focus on model-specific development.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用 PyTorch Lightning 框架可以帮助您重构机器学习实现，减少附带的样板代码比例，并专注于模型特定开发。
- en: In a PyTorch Lightning-based implementation for a machine learning model, you
    can incrementally add support for model training, validation, and test support,
    along with pluggable features such as a logging framework for analysis.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于 PyTorch Lightning 的机器学习模型实现中，您可以逐步添加对模型训练、验证和测试的支持，以及可插拔的特性，例如用于分析的日志框架。
- en: The CSVLogger from PyTorch Lightning saves the results of model training, validation,
    and test features to CSV files that you analyzed using pandas.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Lightning 中的 CSVLogger 将模型训练、验证和测试结果保存到 CSV 文件中，您可以使用 pandas 进行分析。
