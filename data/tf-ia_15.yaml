- en: '12 Sequence-to-sequence learning: Part 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 序列到序列学习：第 2 部分
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Implementing the attention mechanism for the seq2seq model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现序列到序列模型的注意力机制
- en: Generating visualizations from the attention layer to glean insights from the
    model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从注意力层生成可视化图表以获取模型洞察
- en: 'In the previous chapter, we built an English-to-German machine translator.
    The machine learning model was a sequence-to-sequence model that could learn to
    map arbitrarily long sequences to other arbitrarily long sequences. It had two
    main components: an encoder and a decoder. To arrive at that, we first downloaded
    a machine translation data set, examined the structure of that data set, and applied
    some processing (e.g., adding SOS and EOS tokens) to prepare it for the model.
    Next, we defined the machine translation model using standard Keras layers. A
    special characteristic of this model is its ability to take in raw strings and
    convert them to numerical representations internally. To achieve this, we used
    the Keras’s TextVectorization layer. When the model was defined, we trained it
    using the data set we processed and evaluated it on two metrics: per-word accuracy
    of the sequences produced and BLEU. BLEU is a more advanced metric than accuracy
    that mimics how a human would evaluate the quality of a translation. To train
    the model, we used a technique known as teacher forcing. When teacher forcing
    is used, we feed the decoder, with the target translation offset by 1\. This means
    the decoder predicts the next word in the target sequence given the previous word(s),
    instead of trying to predict the whole target sequence without any knowledge of
    the target sequence. This leads to better performance. Finally, we had to redefine
    our model to suit inference. This is because we had to modify the decoder such
    that it predicted one word at a time instead of a sequence. This way, we can create
    a recursive decoder at inference time, which predicts a word and feeds the predicted
    word as an input to predict the next word in the sequence.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了一个英语到德语的机器翻译器。机器学习模型是一个序列到序列模型，可以学习将任意长的序列映射到其他任意长的序列。它有两个主要组成部分：编码器和解码器。为了达到这个目的，我们首先下载了一个机器翻译数据集，检查了该数据集的结构，并对其进行了一些处理（例如，添加
    SOS 和 EOS 标记）以准备好用于模型。接下来，我们使用标准的 Keras 层定义了机器翻译模型。这个模型的一个特殊特点是它能够接受原始字符串并在内部将其转换为数字表示。为了实现这一点，我们使用了
    Keras 的 TextVectorization 层。当模型被定义后，我们使用我们处理的数据集进行了训练，并对两个指标进行了评估：生成的序列的每个词的准确率和
    BLEU。BLEU 是一个比准确率更高级的指标，它模拟了人类如何评估翻译质量。训练模型时，我们使用了一种称为教师强迫的技术。当使用教师强迫时，我们将目标翻译的解码器提供给目标翻译偏移了
    1。这意味着解码器根据前一个词预测目标序列中的下一个词，而不是在不了解目标序列的情况下尝试预测整个目标序列。这导致了更好的性能。最后，我们不得不重新定义我们的模型以适应推断。这是因为我们必须修改解码器，使其一次预测一个词而不是一个序列。这样，我们可以在推断时创建一个递归解码器，该解码器预测一个词并将预测的词作为输入预测序列中的下一个词。
- en: In this chapter, we will explore ways to increase the accuracy of our model.
    To do that, we will use the attention mechanism. Without attention, machine translation
    models rely on the last output produced after processing the input sequence. Through
    the attention mechanism, the model is able to obtain rich representations from
    all the time steps (while processing the input sequence) during the generation
    of the translation. Finally, we will conclude the chapter by visualizing the attention
    mechanisms that will give insights into how the model pays attention to words
    provided to it during the translation process.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将探讨提高模型准确性的方法。为此，我们将使用注意力机制。没有注意力机制，机器翻译模型依赖于处理输入序列后产生的最后输出。通过注意力机制，模型能够在生成翻译过程中从所有时间步骤（处理输入序列时）获得丰富的表示。最后，我们将通过可视化注意力机制来总结本章，以洞察模型在翻译过程中如何关注提供给它的单词。
- en: 'The data and the processing we do in this chapter are going to be identical
    to the last chapter. Therefore, we will not discuss data in detail. You have been
    provided all the code necessary to load and process data in the notebook. But
    let’s refresh the key steps we performed:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们进行的数据和处理与上一章完全相同。因此，我们不会详细讨论数据。您已经提供了在笔记本中加载和处理数据所需的所有代码。但让我们回顾一下我们执行的关键步骤：
- en: Download the data set manually from [http://www.manythings.org/anki/deu-eng.zip](http://www.manythings.org/anki/deu-eng.zip).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[http://www.manythings.org/anki/deu-eng.zip](http://www.manythings.org/anki/deu-eng.zip)手动下载数据集。
- en: The data is in tab-separated format and <German phrase><tab><English phrase><tab><Attribution>
    format. We really care about the first two tab-separated values in a record. We
    are going to predict the German phrase given the English phrase.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据以制表符分隔的格式和<德语短语><制表符><英语短语><制表符><属性>的格式呈现。我们特别关注记录中的前两个制表符分隔的值。我们将预测给定英语短语的德语短语。
- en: We randomly sample 50,000 data points from the data set and use 5,000 (i.e.,
    10%) as validation data and another 5,000 (i.e., 10%) as test data.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从数据集中随机抽样了50,000个数据点，并使用其中的5,000个（即10%）作为验证数据，另外的5,000个（即10%）作为测试数据。
- en: We add a start token (e.g., SOS) and an end token (e.g., EOS) to each German
    phrase. This is an important preprocessing step, as this helps us to recursively
    infer words from our recursive decoder at inference time (i.e., provide SOS as
    the initial seed and keep predicting until the model outputs EOS or reaches a
    maximum length).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为每个德语短语添加了一个开始标记（例如，SOS）和一个结束标记（例如，EOS）。这是一个重要的预处理步骤，因为这帮助我们在推断时从我们的递归解码器中递归地推断单词（即，提供SOS作为初始种子并继续预测直到模型输出EOS或达到最大长度）。
- en: We look at summary statistics of vocabulary size and sequence length, as these
    hyperparameters are very important for our TextVectorization layer (the layer
    can be found at tensorflow.keras.layers.experimental.preprocessing.TextVectorization*).*
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们查看词汇量和序列长度的摘要统计数据，因为这些超参数对我们的TextVectorization层非常重要（该层可以在tensorflow.keras.layers.experimental.preprocessing.TextVectorization*中找到）。
- en: The vocabulary size is set as the number of unique words that appear more than
    10 times in the corpus for both languages, and the sequence length is set as the
    99% quantile (plus a buffer of 5) for both languages.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量设置为语料库中出现次数超过10次的唯一单词数，序列长度设置为两种语言的99%分位数（再加上5的缓冲区）。
- en: '12.1 Eyeballing the past: Improving our model with attention'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 着眼于过去：通过注意力改进我们的模型
- en: You have a working prototype of the translator but still think you can push
    the accuracy up by using attention. Attention provides a richer output from the
    encoder to the decoder by allowing the decoder to look at all the outputs produced
    by the encoder over the entire input sequence. You will modify the previously
    implemented model to incorporate an attention layer that takes all the encoder
    outputs (one for each time step) and produces a sequence of outputs for each decoder
    step that will be concatenated with the standard output produced by the decoder.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个可工作的翻译器原型，但仍认为可以通过使用注意力来提高准确性。注意力通过允许解码器查看编码器在整个输入序列上产生的所有输出，为解码器提供了更丰富的输出。您将修改以前实施的模型以包含一个注意力层，该层接受编码器的所有输出（每个时间步长一个）并为每个解码器步骤产生一系列输出，这些输出将与解码器产生的标准输出串联起来。
- en: We have a working machine translator model that can translate from English to
    German. Performance of this model can be pushed further using something known
    as *Bahdanau attention*. Bahdanau attention was introduced in the paper “Neural
    Machine Translation by Jointly Learning to Align and Translate” by Bahdanau et
    al. ([https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)).
    We already discussed self-attention in chapter 5\. The underlying principle between
    the two attention mechanisms is the same. They both allow the model to get a rich
    representation of historical/future input in a sequence to facilitate the model
    in understanding the language better. Let’s see how the attention mechanism can
    be tied in with the encoder-decoder model we have.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个可以从英语翻译成德语的工作机器翻译模型。通过使用所谓的*Bahdanau注意力*可以进一步提高该模型的性能。 Bahdanau注意力是由Bahdanau等人在论文“Neural
    Machine Translation by Jointly Learning to Align and Translate”中介绍的（[https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)）。我们已经在第5章中讨论过自注意力。这两种注意机制的基本原理是相同的。它们都允许模型以更好地理解语言为目的，在序列中获取历史/未来输入的丰富表示。让我们看看如何将注意力机制与我们现有的编码器-解码器模型结合起来。
- en: The attention mechanism produces an output for each decoder time step, similar
    to how the decoder’s GRU model produces an output at each time step. The attention
    output is combined with the decoder’s GRU output and fed to the subsequent hidden
    layer in the decoder. The attention output produced at each time step of the decoder
    combines the encoder’s outputs from all the time steps, which provides valuable
    information about the English input sequence to the decoder. The attention layer
    is allowed to mix the encoder outputs differently to produce the output for each
    decoder time step, depending on which part of the translation the decoder model
    is working on at a given moment. You should be able to see how powerful the attention
    mechanism is. Previously, the context vector was the only input from the encoder
    that was accessible to the decoder. This is a massive performance bottleneck,
    as it is impractical for the encoder to encode all the information present in
    a sentence using a small-sized vector.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制为每个解码器时间步产生一个输出，类似于解码器的GRU模型在每个时间步产生一个输出。注意力输出与解码器的GRU输出相结合，并馈送到解码器中的后续隐藏层。解码器的每个时间步产生的注意力输出结合了来自所有时间步的编码器输出，这为解码器提供了有关英语输入序列的宝贵信息。注意力层被允许以不同的方式混合编码器输出，以产生每个解码器时间步的输出，具体取决于解码器模型在给定时刻正在处理的翻译部分。您应该能够看到注意力机制有多强大。以前，上下文向量是编码器向解码器可访问的唯一输入。这是一个巨大的性能瓶颈，因为使用小型向量编码句子中存在的所有信息对编码器来说是不切实际的。
- en: Let’s probe a bit more to understand the specific computations that transpire
    during the computation of the attention outputs. Let’s assume that the encoder
    output at position *j* (1 < *j* < *T*[e]) is denoted by *h*[j], and the decoder
    RNN output state at time i (1 < *i* < *T*[d]) is denoted by *s*[i] ; then the
    attention output *c*[i] for the *i*^(th) decoding step is computed by
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨一下，在计算注意力输出时发生的具体计算。假设在位置*j*（1 < *j* < *T*[e]）处的编码器输出被表示为*h*[j]，并且在时间*i*（1
    < *i* < *T*[d]）的解码器RNN输出状态被表示为*s*[i]；那么第*i*次解码步骤的注意力输出*c*[i]由以下计算得出
- en: '*e*[ij] = *v*^T *tanh*(*s*[i -1] *W* + *h*[j]*U*)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*e*[ij] = *v*^T *tanh*(*s*[i -1] *W* + *h*[j]*U*)'
- en: '![12_00a](../../OEBPS/Images/12_00a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![12_00a](../../OEBPS/Images/12_00a.png)'
- en: '![12_00b](../../OEBPS/Images/12_00b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![12_00b](../../OEBPS/Images/12_00b.png)'
- en: Here, W, U, and v are weight matrices (initialized randomly just like neural
    network weights). Their shapes are defined in accordance with the dimensionality
    of hidden representations s and h, which will be discussed in detail soon. In
    summary, this set of equations, for a given decoder position
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，W、U和v是权重矩阵（像神经网络权重一样随机初始化）。它们的形状根据隐藏表示s和h的维度进行定义，这将很快详细讨论。总之，对于给定的解码器位置，这组方程
- en: Computes energy values representing how important each encoder output is for
    that decoding step using a small fully connected network whose weights are W,
    U, and v
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小型全连接网络计算能量值，表示每个编码器输出对于该解码步骤的重要程度
- en: Normalizes energies to represent a probability distribution over the encoder
    steps
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将能量归一化为表示编码器步骤上的概率分布
- en: Computes a weighted sum of encoder outputs using the probability distribution
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用概率分布计算编码器输出的加权和
- en: 12.1.1 Implementing Bahdanau attention in TensorFlow
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 在TensorFlow中实现Bahdanau注意力
- en: 'Unfortunately, TensorFlow does not have a built-in layer to readily use in
    our models to enable the attention mechanism. Therefore, we will implement an
    Attention layer using the Keras subclassing API. We will call this the DecoderRNNAttentionWrapper
    and will have to implement the following functions:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，TensorFlow没有内置的层可供我们在模型中直接使用以启用注意力机制。因此，我们将使用Keras子类化API实现一个Attention层。我们将称之为DecoderRNNAttentionWrapper，并且必须实现以下函数：
- en: __init__—Defines various initializations that need to happen before the layer
    can operate correctly
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: __init__—定义层在能够正确操作之前需要进行的各种初始化
- en: build()—Defines the parameters (e.g., trainable weights) and their shapes associated
    with the computation
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: build()—定义与计算相关联的参数（例如，可训练权重）及其形状
- en: call()—Defines the computations and the final output that should be produced
    by the layer
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用（call()）—定义应由该层进行的计算和最终输出
- en: The __init__() function initializes the layer with any attributes it requires
    to operate correctly. In this case, our DecoderRNNAttentionWrapper takes in a
    cell_fn as the argument. cell_fn needs to be a Keras layer object that implements
    the tf.keras .layers.AbstractRNNCell interface ([http://mng.bz/pO18](http://mng.bz/pO18)).
    There are several options, such as tf.keras.layers.GRUCell, tf.keras.layers.LSTMCell,
    and tf.keras.layers.RNNCell. In this case, we will use the tf.keras.layers.GRUCell.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: __init__() 函数用于初始化层，包括需要正确运行的任何属性。在这种情况下，我们的 DecoderRNNAttentionWrapper 接受一个
    cell_fn 作为参数。cell_fn 需要是一个实现了 tf.keras.layers.AbstractRNNCell 接口的 Keras 层对象（[http://mng.bz/pO18](http://mng.bz/pO18)）。有几个选项，例如
    tf.keras.layers.GRUCell、tf.keras.layers.LSTMCell 和 tf.keras.layers.RNNCell。在这个例子中，我们将使用
    tf.keras.layers.GRUCell。
- en: Difference between tf.keras.layers.GRUCell and tf.keras.layers.GRU
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: tf.keras.layers.GRUCell 和 tf.keras.layers.GRU 之间的区别
- en: 'The GRUCell can be thought of as an abstraction of the GRU layer. The GRUCell
    encompasses the most minimalist computation you can think of in an RNN layer.
    Given an input and a previous state, it computes the next output and the next
    state. This is the most primitive computation that governs an RNN layer:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GRUCell 可以看作是 GRU 层的一个抽象，它包含了 RNN 层中最简化的计算。给定一个输入和上一个状态，它计算下一个输出和下一个状态。这是控制
    RNN 层的最原始的计算：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In other words, a GRUCell encapsulates the computations required to compute
    a single time step in an input sequence. The GRU layer is a fully fledged implementation
    of the GRUCell that can process the whole sequence. Furthermore, the GRU layer
    gives options like return_state and return_sequence to control the output produced
    by the GRU layer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，GRUCell 封装了计算输入序列中单个时间步所需的计算。GRU 层是 GRUCell 的完全实现，可以处理整个序列。此外，GRU 层还提供了
    return_state 和 return_sequence 等选项来控制 GRU 层产生的输出。
- en: In short, the GRU layer provides the convenience for processing input sequences,
    and the GRUCell exposes the more fine-grained implementation details that allow
    one to process a single time step in the sequence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，GRU 层提供了便利的处理输入序列的方式，而 GRUCell 则暴露了更细粒度的实现细节，允许处理序列中的单个时间步。
- en: 'Here, we have decided to go with GRU, as the GRU model is a lot simpler than
    an LSTM (meaning there is reduced training time) but achieves roughly similar
    results on NLP tasks:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们决定使用 GRU，因为 GRU 模型比 LSTM 简单得多（意味着减少了训练时间），但在 NLP 任务上实现了大致相似的结果：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, the build() function is defined. The build function declares the three
    weight matrices used in the attention computation: W, U and v. The argument input_shape
    contains the shapes of the inputs. Our input will be a tuple containing encoder
    outputs and the decoder RNN inputs:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义了 build() 函数。build 函数声明了用于注意力计算的三个权重矩阵：W、U 和 v。参数 input_shape 包含了输入的形状。我们的输入将是一个包含编码器输出和解码器
    RNN 输入的元组：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The most important argument to note in the weight definitions is the shape argument.
    We are defining them so that
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意权重定义中最重要的参数是 shape 参数。我们定义它们的形状为
- en: W_a (representing W) has a shape of [ <encoder hidden size>, <attention hidden
    size>]
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W_a（表示 W）的形状为[<encoder hidden size>, <attention hidden size>]
- en: U_a (representing U) has a shape of [<decoder hidden size>, <attention hidden
    size>]
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U_a（表示 U）的形状为[<decoder hidden size>, <attention hidden size>]
- en: V_a (representing v) has a shape of [<attention hidden size>, 1]
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V_a（表示 v）的形状为[<attention hidden size>, 1]
- en: Here, the <encoder hidden size> and <decoder hidden size> are the number of
    units in the final output of the RNN layer of the encoder or the decoder, respectively.
    We typically keep the encoder and decoder RNN sizes the same to simplify the computations.
    The <attention hidden size> is a hyperparameter of the layer that can be set to
    any value and represents the dimensionality of internal computations of the attention.
    Finally, we define the call() method (see listing 12.1). The call() method encapsulates
    the computations that take place when the layer is called with inputs. This is
    where the heavy lifting required to compute the attention outputs happens. At
    a high level, the attention layer needs to traverse all the encoder inputs (i.e.,
    each time step) for each decoder input.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<encoder hidden size> 和 <decoder hidden size> 分别是编码器或解码器的RNN层的最终输出中的单元数。我们通常保持编码器和解码器
    RNN 大小相同，以简化计算。<attention hidden size>是该层的一个超参数，可以设置为任何值，并表示注意力内部计算的维度。最后，我们定义了call()方法（见列表12.1）。call()方法封装了在输入时发生的计算。这是计算注意力输出所需的繁重工作的地方。在高层次上，注意力层需要遍历所有编码器输入（即每个时间步）以及每个解码器输入。
- en: Listing 12.1 Attention computation in the DecoderRNNAttentionWrapper
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.1 解码器 RNN 注意力包装中的注意力计算。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ When calling the _step function, we are passing encoder_outputs as a constant,
    as we need access to the full encoder sequence. Here we access it within the _step
    function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在调用_step函数时，我们将encoder_outputs作为常量传递，因为我们需要访问完整的编码器序列。在_step函数内部访问它。
- en: ❷ Computes S.Wa where S represents all the encoder outputs and S=[s0, s1, ...,
    si]. This produces a [batch size, en_seq_len, hidden size]-sized output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算S.Wa，其中S表示所有编码器输出，S=[s0, s1, ..., si]。这产生一个大小为[batch size, en_seq_len, hidden
    size]的输出。
- en: ❸ Computes hj.Ua, where hj represent the j^{th} decoding step. This produces
    a [ batch_size, 1, hidden size]-sized output
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算hj.Ua，其中hj表示第j个解码步骤。这产生一个大小为[batch_size, 1, hidden size]的输出。
- en: ❹ Computes tanh(S.Wa + hj.Ua). This produces a [batch_size, en_seq_len, hidden
    size]-sized output
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算tanh(S.Wa + hj.Ua)。这产生一个大小为[batch_size, en_seq_len, hidden size]的输出。
- en: ❺ Computes the energies and normalizes them. Produces a [batch_size, en_seq_len]
    sized output
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算能量并对其进行归一化。产生一个大小为[batch_size, en_seq_len]的输出。
- en: ❻ Computes the final attention output (c_i) as a weighted sum of h_j (for all
    j), where weights are denoted by a_i. Produces a [batch_size, hidden_size] output
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将最终注意力输出（c_i）计算为h_j（对所有j）的加权和，其中权重由a_i表示。产生一个大小为[batch_size, hidden_size]的输出。
- en: ❼ Concatenate sthe current input and c_i and feeds it to the decoder RNN to
    get the output
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将当前输入和c_i连接起来，并将其馈送到解码器 RNN 以获得输出。
- en: ❽ The inputs to the attention layer are encoder outputs and decoder RNN inputs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 注意力层的输入是编码器输出和解码器 RNN 输入。
- en: ❾ The K.rnn() function executes the _step() function for every input in the
    decoder inputs to produce attention outputs for all the decoding steps.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ K.rnn()函数对解码器输入中的每个输入执行_step()函数，以生成所有解码步骤的注意力输出。
- en: '❿ The final output is two-fold: attention outputs of a [batch size, de_seq_len,
    hidden size]-sized output and attention energies [batch dize, de_seq_len, en_seq_len]-sized
    outputs'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 最终输出有两部分：一个大小为[batch size, de_seq_len, hidden size]的注意力输出，以及一个大小为[batch dize,
    de_seq_len, en_seq_len]的注意力能量输出。
- en: 'Let’s demystify what’s done in this function. The input to this layer is an
    iterable of two elements: encoder output sequence (encoder_outputs) and decoder
    RNN input sequence (decoder_inputs). Next, we use a special backend function of
    Keras called K.rnn() ([http://mng.bz/OoPR](http://mng.bz/OoPR)) to iterate through
    these inputs while computing the final output required. In our example, it is
    called as'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们揭开这个函数中做了什么。这个层的输入是一个两个元素的可迭代对象：编码器输出序列（encoder_outputs）和解码器 RNN 输入序列（decoder_inputs）。接下来，我们使用Keras的一个特殊后端函数K.rnn()（[http://mng.bz/OoPR](http://mng.bz/OoPR)）来迭代这些输入，同时计算所需的最终输出。在我们的例子中，它被称为
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, it applies the step_function to each time step slice of the inputs tensor.
    For example, the decoder_inputs is a [<batch size>, <decoder time steps>, <embedding
    size>]-sized input. Then the K.rnn() function applies the step_function to every
    [<batch size>, <embedding size>] output for <decoder time steps> number of times.
    The update this function does is a recurrent update, meaning that it takes an
    initial state and produces a new state until it reaches the end of the input sequence.
    For that, initial_states provides the starting states. Finally, we are passing
    encoder_outputs as a constant to the step_function. This is quite important as
    we need the full sequence of the encoder’s hidden outputs to compute attention
    at each decoding step. Within the step_function, constants gets appended to the
    value of the states argument. So, you can access encoder_outputs as the last element
    of states.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，它将 step_function 应用于输入张量的每个时间步切片。例如，decoder_inputs 是一个 [<batch size>, <decoder
    time steps>, <embedding size>] 大小的输入。然后，K.rnn() 函数将 step_function 应用于每个 [<batch
    size>, <embedding size>] 输出，重复 <decoder time steps> 次。此函数执行的更新是递归更新，意味着它接受一个初始状态并生成一个新状态，直到达到输入序列的结尾。为此，initial_states
    提供了起始状态。最后，我们将 encoder_outputs 作为常数传递给 step_function。这非常重要，因为我们需要计算每个解码步骤的注意力时编码器隐藏输出的完整序列。在
    step_function 中，常数被附加到状态参数的值之后。因此，您可以将 encoder_outputs 访问为 states 的最后一个元素。
- en: The _step function does the computations we outlined in listing 12.1 for a single
    decoder time step. It takes inputs (a slice of the time dimension of the original
    input) and states (initialized with the initial_states value in the K.rnn() function).
    Next, using these two entities, the normalized attention energies (i.e., α[ij])
    for a single time step are computed (a_i). Following that, c_i is computed, which
    is a weighted sum of encoder_outputs weighted by a_i. Afterward, it updates the
    cell_fn (i.e., GRUCell) with the current input and the state. Note that the current
    input to the cell_fn is a concatenation of the decoder input and c_i (i.e., the
    weighted sum of encoder inputs). The cell function then outputs the output state
    along with the next state. We return this information out. In other words, the
    _step() function outputs the output for that time step (i.e., a tuple of decoder
    RNN output and normalized energies that computed the weighted sum of encoder inputs)
    and the next state of the decoder RNN.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: _step 函数执行我们在列表 12.1 中概述的计算，用于单个解码器时间步骤。它接受输入（原始输入的时间维度的一个切片）和状态（在 K.rnn() 函数中用
    initial_states 值初始化）。接下来，使用这两个实体计算单个时间步骤的归一化注意力能量（即 α[ij]）（a_i）。随后，计算 c_i，这是由
    a_i 加权的编码器输出的加权和。然后，它使用当前输入和状态更新 cell_fn（即 GRUCell）。注意，cell_fn 的当前输入是解码器输入和 c_i
    的连接（即编码器输入的加权和）。cell 函数然后输出输出状态以及下一个状态。我们返回此信息。换句话说，_step() 函数输出该时间步的输出（即解码器 RNN
    输出和计算编码器输入加权和的归一化能量的元组）以及解码器 RNN 的下一个状态。
- en: Finally, you can obtain the full output of the _step function for all the decoder
    time steps using the K.rnn() function as shown. We are only interested in the
    output itself (denoted by attn_outputs) and will ignore the other things output
    by the function.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用 K.rnn() 函数获得所有解码器时间步骤的_step函数的完整输出，如所示。我们只对输出本身感兴趣（由 attn_outputs 表示），将忽略函数输出的其他内容。
- en: 'The K.rnn() function outputs the following outputs when called:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用 K.rnn() 函数时，该函数输出以下输出：
- en: last_output—The last output produced by the _step_function after it reaches
    the end of the sequence
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: last_output——step_function 在序列结束时产生的最后输出
- en: outputs—All the outputs produced by the step_function
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: outputs——step_function 产生的所有输出
- en: new_states—The last states produced by the step_function after it reaches the
    end of the sequence
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: new_states——step_function 在序列结束时产生的最后状态
- en: 'Finally, the call() function produces two outputs:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，call() 函数产生两个输出：
- en: attn_out—Holds all the attention outputs for all the decoding steps
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: attn_out——保存所有解码步骤的注意力输出
- en: attn_energy—Provides the normalized energy values for a batch of data, where
    the energy matrix for one example contains energy values for all the encoder time
    steps for every decoder time step
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: attn_energy——为一批数据提供归一化能量值，其中一个示例的能量矩阵包含所有编码器时间步的能量值，用于每个解码器时间步
- en: We have discussed the most important functions of the DecoderRNNAttentionWrapper
    layer. If you want to see the full sub-classed implementation of the DecoderRNNAttentionWrapper,
    please refer to the code at Ch11/11.1_seq2seq_machine_translation .ipynb.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 DecoderRNNAttentionWrapper 层的最重要的功能。如果你想查看 DecoderRNNAttentionWrapper
    的完整子类实现，请参考 Ch11/11.1_seq2seq_machine_translation.ipynb 中的代码。
- en: 12.1.2 Defining the final model
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 定义最终模型
- en: When defining the final model, the get_vectorizer() and get_encoder() functions
    remain identical to what was shown in the previous section. All the modifications
    required need to happen in the decoder. Therefore, let’s define a function, get_
    final_seq2seq_model_with_attention(), that provides us the decoder with Bahdanau
    attention in place, as shown in the next listing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义最终模型时，get_vectorizer() 和 get_encoder() 函数保持与上一节中显示的相同。所有需要的修改都需要在解码器中进行。因此，让我们定义一个函数，get_final_seq2seq_model_with_attention()，它提供了具有
    Bahdanau 注意力的解码器，如下一个列表所示。
- en: Listing 12.2 Defining the final sequence-to-sequence model with attention
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.2 定义具有注意力的最终序列到序列模型
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Get the encoder outputs for all the timesteps.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取所有时间步长的编码器输出。
- en: ❷ The input is (None,1) shaped and accepts an array of strings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入的形状为（None,1），接受字符串数组。
- en: ❸ Vectorize the data (assign token IDs).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据向量化（分配标记 ID）。
- en: ❹ Define an embedding layer to convert IDs to word vectors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义一个嵌入层，将 ID 转换为单词向量。
- en: ❺ Define the initial state to the decoder as the concatenation of the last forward
    and backward encoder states.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将解码器的初始状态定义为最后一个正向和反向编码器状态的串联。
- en: ❻ Define a GRUCell, which will then be used for the Attention layer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义一个 GRUCell，然后将其用于注意力层。
- en: ❼ Get the attention outputs. The GRUCell is passed as the cell_fn, where the
    inputs are en_states (i.e., all of the encoder states) and d_emb_out (input to
    the decoder RNN).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 获取注意力输出。将 GRUCell 作为 cell_fn 传递，其中输入是 en_states（即所有编码器状态）和 d_emb_out（解码器 RNN
    的输入）。
- en: ❽ Define the intermediate and final Dense layer outputs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义中间和最终 Dense 层的输出。
- en: ❾ Define a model that takes encoder and decoder inputs as inputs and outputs
    the final predictions (d_final_out).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义一个模型，以编码器和解码器输入作为输入，并输出最终预测（d_final_out）。
- en: 'We already have done all the hard work. Therefore, changes to the decoder can
    be summarized in two lines of code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经做了所有的艰苦工作。因此，对解码器的更改可以总结为两行代码：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We first define a GRUCell object with 256 hidden units. Then we define the DecoderRNNAttentionWrapper,
    where the cell_fn is the GRUCell we defined and units is set to 512\. units in
    the DecoderRNNAttentionWrapper defines the dimensionality of the weights and the
    intermediate attention outputs. We pass en_states (i.e., encoder output sequence)
    and d_emb_out (i.e., decoder input sequence to the RNN) and set the initial state
    as the final state of the encoder (i.e., d_init_state).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个具有 256 个隐藏单元的 GRUCell 对象。然后我们定义了 DecoderRNNAttentionWrapper，其中 cell_fn
    是我们定义的 GRUCell，而单位设置为 512。DecoderRNNAttentionWrapper 中的单位定义了权重和中间注意力输出的维度。我们传递
    en_states（即编码器输出序列）和 d_emb_out（即传递给 RNN 的解码器输入序列），并将初始状态设置为编码器的最终状态（即 d_init_state）。
- en: Next, as before, we have to define a get_vectorizer() function (see the next
    listing) to get the English/German vectorizers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，和以前一样，我们必须定义一个 get_vectorizer() 函数（请参见下一个列表），以获取英语/德语矢量化器。
- en: Listing 12.3 Defining the TextVectorizers for the encoder-decoder model
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.3 定义用于编码器-解码器模型的 TextVectorizers
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Define an input layer that takes a list of strings (or an array of strings).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个输入层，接受字符串列表（或字符串数组）。
- en: ❷ When defining the vocab size, there are two special tokens, (Padding) and
    '[UNK]' (OOV tokens), added automatically.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在定义词汇大小时，有两个特殊标记，（填充）和 '[UNK]'（OOV 标记），会自动添加。
- en: ❸ Fit the vectorizer layer on the data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在数据上拟合 vectorizer 层。
- en: ❹ Get the token IDs for the data fed to the input.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取输入数据的标记 ID。
- en: ❺ Return only the model, which takes an array of a string and outputs a tensor
    of token IDs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 仅返回模型，该模型接受字符串数组并输出标记 ID 的张量。
- en: ❻ Return the vocabulary in addition to the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 除了模型外，返回词汇表。
- en: The get_encoder() function shown in the following listing builds the encoder.
    As these have been discussed in detail, they will not be repeated here.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中显示的 get_encoder() 函数构建了编码器。由于这些已经详细讨论过，因此不会在这里重复。
- en: Listing 12.4 The function that returns the encoder
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.4 返回编码器的函数
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ The input is (None,1) shaped and accepts an array of strings.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入的形状为（None,1），接受字符串数组。
- en: ❷ Vectorize the data (assign token IDs).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对数据进行向量化（分配令牌 ID）。
- en: ❸ Define an embedding layer to convert IDs to word vectors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个嵌入层来将 ID 转换为单词向量。
- en: ❹ Get the embeddings of the token IDs
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取令牌 ID 的嵌入。
- en: ❺ Define a bidirectional GRU layer. The encoder looks at the English text (i.e.,
    the input) both backward and forward; this leads to better performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义一个双向 GRU 层。编码器同时查看英文文本（即输入）的前向和后向；这将导致更好的性能。
- en: ❻ Get the output of the GRU layer (the last output state vector returned by
    the model).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取 GRU 层的输出（模型返回的最后一个输出状态向量）。
- en: ❼ Define the encoder model; this takes in a list/array of strings as the input
    and returns the last output state of the GRU model as the output.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义编码器模型；这将接受一个字符串列表/数组作为输入，并将 GRU 模型的最后输出状态作为输出返回。
- en: 'As the very last step, we define the final model and compile it using the same
    specifications we used for the earlier model:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们定义了最终模型，并使用与之前模型相同的规格进行编译：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 12.1.3 Training the model
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.3 训练模型
- en: 'Training the model is quite straightforward as it remains the same as before.
    All we need to do is call the train_model() function with the arguments model
    (a Keras model to be trained/evaluated), vectorizer (a target language vectorizer
    to convert token IDs to text), train_df (training data), valid_df (validation
    data), test_df (testing data), epochs (an int to represent how many epochs the
    model needs to be trained) and batch_size (size of a training/evaluation batch):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型非常简单，因为它与以前的情况相同。我们所需要做的就是调用 train_model() 函数，并使用参数模型（一个要训练/评估的 Keras 模型）、矢量化器（将令牌
    ID 转换为文本的目标语言矢量化器）、train_df（训练数据）、valid_df（验证数据）、test_df（测试数据）、epochs（表示模型需要训练多少个周期的整数）和
    batch_size（训练/评估批次的大小）：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will output
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Compared to the last model we had, this is quite an improvement. We have almost
    doubled the validation and testing BLEU scores. All this was possible because
    we introduced the attention mechanism to alleviate a huge performance bottleneck
    in the encoder-decoder model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前拥有的最后模型相比，这是相当大的改进。我们的验证和测试 BLEU 分数几乎翻了一番。所有这些都可能是因为我们引入了注意机制来缓解编码器-解码器模型中的巨大性能瓶颈。
- en: NOTE On an Intel Core i5 machine with an NVIDIA GeForce RTX 2070 8 GB, the training
    took approximately five minutes to run five epochs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在一台配备 NVIDIA GeForce RTX 2070 8 GB 的英特尔 Core i5 机器上，训练大约需要五分钟来运行五个周期。
- en: 'Finally, for later use, we save the trained model, along with the vocabularies:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了以后使用，我们保存了训练好的模型，以及词汇表：
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: State-of-the-art results for English-German translation
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 英德翻译的最新结果
- en: One way to know where our model stands is to compare it to the state-of-the-art
    result that has been achieved on English-German translation. In 2021, at the time
    of writing this book, a BLEU score of 0.3514 has been achieved by one of the models.
    The model is introduced in the paper “Lessons on Parameter Sharing across Layers
    in Transformers” by Takase et al. ([https://arxiv.org/pdf/2104.06022v1.pdf](https://arxiv.org/pdf/2104.06022v1.pdf)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 了解我们模型的位置的一种方法是将其与在英德翻译上取得的最新成果进行比较。在编写本书时的 2021 年，模型已经达到了 0.3514 的 BLEU 分数。该模型在文章“Lessons
    on Parameter Sharing across Layers in Transformers”中由高瀬等人介绍（[https://arxiv.org/pdf/2104.06022v1.pdf](https://arxiv.org/pdf/2104.06022v1.pdf)）。
- en: This should not be taken as an exact comparison to our model, as the benchmarked
    models are typically trained on the WMT English-German data set ([https://nlp.stanford.edu/projects/nmt/](https://nlp.stanford.edu/projects/nmt/)),
    which is a much larger and more complex data set. However, given that we have
    a relatively simple model with no special training time optimizations, 0.1978
    is a decent score.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这不应被视为与我们模型的确切比较，因为基准模型通常是在 WMT 英德数据集上训练的（[https://nlp.stanford.edu/projects/nmt/](https://nlp.stanford.edu/projects/nmt/)），这是一个更大更复杂的数据集。但是，考虑到我们有一个相对简单的模型，并且没有特殊的训练时间优化，0.1978
    是一个不错的分数。
- en: With that, we will discuss how we can visualize the attention weights to see
    the attention patterns the model uses when decoding inputs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们将讨论如何可视化注意权重以查看模型在解码输入时使用的注意模式。
- en: Exercise 1
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1
- en: You have invented a novel attention mechanism called AttentionX. Unlike Bahdanau
    attention, this attention mechanism takes encoder inputs and the decoder’s RNN
    outputs to produce the final output. The fully connected layers take this final
    output instead of the usual decoder’s RNN output. Given that, you’ve implemented
    the new attention mechanism in a layer called AttentionX. For encoder input x
    and decoder’s RNN output y, it can be called as
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您发明了一种新型的注意力机制称为 AttentionX。与 Bahdanau 注意力不同，这种注意力机制需要编码器输入和解码器的 RNN 输出才能产生最终输出。全连接层使用这个最终输出而不是常规的解码器
    RNN 输出。在名为 AttentionX 的层中实现了这种新的注意力机制。对于编码器输入 x 和解码器的 RNN 输出 y，可以调用它：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: where the final output z is a [<batch size>, <decoder time steps>, <hidden size>]-sized
    output. How would you change the following decoder to use this new attention mechanism?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出 z 是一个大小为 [<批次大小>，<解码器时间步数>，<隐藏大小>] 的输出。您将如何更改以下解码器以使用这种新的注意力机制？
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 12.2 Visualizing the attention
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 可视化注意力
- en: You have determined that the attention-based model works better than the one
    without attention. But you are skeptical and want to understand if the attention
    layer is producing meaningful outputs. For that, you are going to visualize attention
    patterns generated by the model for several input sequences.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经确定基于注意力机制的模型比没有注意力的模型效果更好。但是您还是有怀疑，并想要了解注意力层是否产生有意义的输出。为此，您将可视化模型为几个输入序列生成的注意力模式。
- en: Apart from the performance, one of the lucrative advantages of the attention
    mechanism is the interpretability it brings along to the model. The normalized
    energy values, one of the interim outputs of the attention mechanism, can provide
    powerful insights into the model. Since the normalized energy values represent
    how much each encoder output contributed to decoding/translating at each decoding
    timestep, it can be used to generate a heatmap, highlighting the most important
    words in English that correspond to a particular German word.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能之外，注意力机制带给模型的另一个优势就是它带来的可解释性。注意力机制的中间输出之一——规范化能量值，可以提供有力的洞见。由于规范化能量值表示每个编码器输出在每个解码时步骤中对解码/翻译的贡献程度，因此它可以用于生成热力图，突出显示与特定德语单词对应的最重要的英语单词。
- en: 'If we go back to the DecoderRNNAttentionWrapper, by calling it on a certain
    input, it produces two outputs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到 DecoderRNNAttentionWrapper，调用它对某个输入进行操作，将会产生两个输出：
- en: Decoder RNN output sequence
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器 RNN 输出序列
- en: Alpha (i.e., normalized energy values) for all the encoder positions for every
    decoder position
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个解码器位置对所有编码器位置的 alpha（即规范化能量值）
- en: The second output is what we are after. That tensor holds the key to unlocking
    the powerful interpretability brought by the attention mechanism.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要获取的是第二个输出。那个张量拥有解锁注意力机制所带来的强大可解释性的关键。
- en: Let’s write a function called the attention_visualizer() that will load the
    saved model and outputs not only the predictions of the model, but also the attention
    energies that will help us generate the final heatmap. In this function, we will
    load the model and create outputs by using the trained layers to retrace the interim
    and final outputs of the decoder, as shown in the next listing. This is similar
    to how we retraced the various steps in the model to create an inference model
    from the trained model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写一个名为 attention_visualizer() 的函数，该函数将加载保存的模型，并输出模型的预测结果以及有助于生成最终热力图的注意力能量。在这个函数中，我们将加载模型并使用训练后的层来重现解码器的中间和最终输出，就像下一个列表中所示的那样。这类似于我们如何重现模型中的各个步骤，从训练好的模型中创建推理模型一样。
- en: Listing 12.5 A model that visualizes attention patterns from input text
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单 12.5 可视化输入文本中的注意力模式的模型
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Load the model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载模型。
- en: ❷ Define the encoder input for the model and get the final outputs of the encoder.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为模型定义编码器输入并获取编码器的最终输出。
- en: ❸ Get the encoder vectorizer (required to interpret the final output).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取编码器向量化器（用于解释最终输出）。
- en: ❹ Define the decoder input.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义解码器输入。
- en: ❺ Get the decoder vectorizer and the output.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取解码器向量器和输出。
- en: ❻ The next few steps just reiterate the steps in the trained model. We simply
    get the corresponding layers and pass the output of the previous step to the current
    step.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 接下来的几个步骤只是重复训练模型中的步骤。我们只需获取相应的层，并将上一步的输出传递给当前步骤即可。
- en: ❼ Here we define the final model to visualize attention patterns; we are interested
    in the attn_states output (i.e., normalized energy values). We will also need
    the vectorized token IDs to annotate the visualization.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在这里，我们定义了最终用于可视化注意力模式的模型；我们对attn_states输出感兴趣（即，归一化的能量值）。我们还需要向可视化添加向量化的令牌ID。
- en: 'Note how the final model we defined returns four different outputs, as opposed
    to the trained model, which only returned the predictions. We will also need a
    get_ vocabulary() function that will load the saved vocabularies:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们定义的最终模型返回了四个不同的输出，而不是训练好的模型只返回了预测结果。我们还需要一个get_vocabulary()函数，它将加载保存的词汇表：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, call these functions so that we have the vocabularies and the model
    ready:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，调用这些函数，使得我们拥有词汇表和模型准备就绪：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we’ll move on to visualizing the outputs produced by the visualizer_model;
    we will be using the Python library matplotlib to visualize attention patterns
    for several examples. Let’s define a function called visualize_attention() that
    takes in the visualizer_model, the two vocabularies, a sample English sentence,
    and the corresponding German translation (see the next listing). Then it will
    make a prediction on the inputs, retrieve the attention weights, generate a heatmap,
    and annotate the two axes with the English/German tokens.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续可视化visualizer_model生成的输出；我们将使用Python库matplotlib来可视化几个示例的注意力模式。让我们定义一个名为visualize_attention()的函数，它接受visualizer_model、两个词汇表、一个样本英文句子和相应的德文翻译（请参见下面的代码）。然后它将对输入进行预测，检索注意力权重，生成热图，并用英文/德文标记两个轴。
- en: Listing 12.6 Visualizing attention patterns using input text
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 12.6 使用输入文本可视化注意力模式
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Get the model predictions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取模型的预测结果。
- en: ❷ Get the token IDs of the predictions of the model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取模型预测结果的令牌ID。
- en: ❸ Our y tick labels will be the input English words. We stop as soon as we see
    padding tokens.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们的y轴刻度标签将是输入的英文单词。一旦看到填充标记，我们就停止。
- en: ❹ Our x tick labels will be the predicted German words. We stop as soon as we
    see the EOS token.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们的x轴刻度标签将是预测的德文单词。一旦看到EOS标记，我们就停止。
- en: ❺ We are going to visualize only the useful input and predicted words so that
    things like padded values and anything after the EOS token are discarded.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们将只可视化有用的输入和预测的单词，这样像填充值和EOS标记之后的内容都将被丢弃。
- en: ❻ Generate the attention heatmap.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 生成注意力热图。
- en: ❼ Set the x ticks, y ticks, and tick labels.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 设置x轴刻度、y轴刻度和刻度标签。
- en: ❽ Generate the color bar to understand the value range found in the heat map.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 生成色条以了解热图中的值范围。
- en: ❾ Save the figure to the disk.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 将图保存到磁盘。
- en: First, we input the English and German input text to the model to generate a
    prediction. We need to input both the English and German inputs as we are still
    using the teacher-forced model. You might be wondering, “Does that mean I have
    to have the German translation ready and can only visualize attention patterns
    in the training mode?” Of course not! You can have an inference model defined,
    like we did in a previous section in this chapter, and still visualize the attention
    patterns. We are using the trained model itself to visualize patterns, as I want
    to focus on visualizing attention patterns rather than defining the inference
    model (which we already did for another model).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将英文和德文输入文本输入模型以生成预测。由于我们仍在使用教师强迫模型，因此我们需要输入英文和德文输入文本。你可能会想，“这是否意味着我必须准备好德文翻译，并且只能在训练模式下可视化注意力模式？”当然不是！你可以像我们在本章的前一节中所做的那样定义一个推断模型，并且仍然可以可视化注意力模式。我们正在使用训练好的模型本身来可视化模式，因为我想专注于可视化注意力模式，而不是定义推断模型（我们已经为另一个模型完成了这个任务）。
- en: 'Once the predictions and attention weights are obtained, we define two lists:
    x_ticklabels and y_ticklabels. They will be the labels (i.e., English/German words)
    you see on the two axes in the heatmap. We will have the English words on the
    row dimension and German words in the column dimension (figure 12.1). We will
    also do a simple filtering to get rid of paddings (i.e., "") and German text appearing
    after the EOS token and get the attention weights within the range that satisfy
    these two criteria. You can then simply call the matplotlib’s imshow() function
    to generate the heatmap and set the axes’ ticks and the labels for those ticks.
    Finally, the figure is saved to the disk.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预测和注意权重得到，我们定义两个列表：x_ticklabels 和 y_ticklabels。它们将是热图中两个轴上看到的标签（即英语/德语单词）。我们将在行维度上有英语单词，列维度上有德语单词（图
    12.1）。我们还将进行简单的过滤，以消除填充（即“”）和出现在 EOS 标记之后的德语文本，并获得满足这两个条件的范围内的注意力权重。然后，您只需调用 matplotlib
    的 imshow() 函数来生成热图，并设置轴的刻度和这些刻度的标签。最后，将图保存到磁盘上。
- en: 'Let’s give this a trial run! Let’s take a few examples from our test DataFrame
    and visualize attention patterns. We will create 10 visualizations and will also
    make sure that those 10 examples we choose have at least 10 English words to make
    sure we don’t visualize very short phrases:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试这个！让我们从我们的测试 DataFrame 中选取几个示例并可视化注意力模式。我们将创建 10 个可视化效果，并确保我们选择的这 10 个示例至少有
    10 个英语单词，以确保我们不可视化非常短的短语：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you run this code successfully, you should get 10 attention visualizations
    shown and stored on the disk. In figures 12.1 and 12.2, we show two such visualizations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您成功运行此代码，您应该会看到并将 10 个注意力可视化显示并存储在磁盘上。在图 12.1 和 12.2 中，我们展示了两个这样的可视化。
- en: '![12-01](../../OEBPS/Images/12-01.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![12-01](../../OEBPS/Images/12-01.png)'
- en: Figure 12.1 Attention patterns visualized for an input English text
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 为输入英文文本可视化的注意力模式
- en: In the figures, the lighter the color, the more the model has paid attention
    to that word. In figure 12.1, we can see that, when translating the words, “und”
    and “maria,” the model has mostly paid attention to “and” and “mary,” respectively.
    If you go to Google Translate and do the German translations for the word “and,”
    for example, you will see that this is, in fact, correct. In figure 12.2, we can
    see that when generating “hast keine nicht,” the model has paid attention to the
    phrase “have no idea.” The other observation we can make is that the attention
    pattern falls roughly diagonally. This makes sense as both these languages roughly
    follow the same writing style.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些图中，颜色越浅，模型就越关注该单词。在图 12.1 中，我们可以看到，当翻译单词“und”和“maria”时，模型主要关注的是“and”和“mary”分别。例如，如果您去谷歌翻译并为“and”这个词做德语翻译，您将会发现这是正确的。在图
    12.2 中，我们可以看到，在生成“hast keine nicht”时，模型关注的是短语“have no idea”。我们可以做出的另一个观察是，注意力模式大致呈对角线。这是有道理的，因为这两种语言大致遵循相同的书写风格。
- en: '![12-02](../../OEBPS/Images/12-02.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![12-02](../../OEBPS/Images/12-02.png)'
- en: Figure 12.2 Attention patterns visualized for an input English text
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 为输入英文文本可视化的注意力模式
- en: 'This concludes our discussion about sequence-to-sequence models. In the next
    chapter, we will discuss a family of models that has been writing the state-of-the-art
    of machine learning for a few years: Transformers.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的讨论到此结束关于序列到序列模型的内容。在下一章中，我们将讨论一个家族的模型，这些模型已经几年来一直处于机器学习的最前沿：transformers。
- en: Exercise 2
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2
- en: You have an attention matrix given by attention_matrix, with English words given
    by english_text_labels and German words given by german_text_labels. How would
    you create a visualization similar to figure 12.1? Here, you will need to use
    the imshow(), set_xticks(), set_yticks(), set_xticklabels(), and set_yticklabels()
    functions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个由 attention_matrix 给出的注意力矩阵，用英语单词表示为 english_text_labels，用德语单词表示为 german_text_labels。你会如何创建类似于图
    12.1 的可视化？在这里，您将需要使用 imshow()、set_xticks()、set_yticks()、set_xticklabels() 和 set_yticklabels()
    函数。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Using attention in sequence-to-sequence models can greatly help shoot their
    performance up.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在序列到序列模型中使用注意力可以极大地提高其性能。
- en: Using attention at each decoding time step, the decoder gets to see all the
    historical outputs of the encoder and select and mix these outputs to come up
    with an aggregated (e.g., summed) representation of that, which gives a holistic
    view of what was in the encoder input.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个解码时间步中使用注意力，解码器可以看到编码器的所有历史输出，并选择并混合这些输出，以产生一个综合的（例如，求和）表示，这给出了编码器输入的整体视图。
- en: One of the intermediate products in the attention computation is the normalized
    energy values, which give a probability distribution of how important each encoded
    position was for decoding a given time step for every decoding step. In other
    words, this is a matrix that has a value for every encoder time step and decoder
    time step combination. This can be visualized as a heatmap and can be used to
    interpret which words the decoder paid attention to when translating a certain
    token in the decoder.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注意力计算的中间产物之一是归一化的能量值，它给出了每个编码位置对于每个解码步骤的解码时间步骤的重要性的概率分布。换句话说，这是一个矩阵，对于每个编码器时间步和解码器时间步的组合都有一个值。这可以可视化为热图，并且可以用于解释解码器在翻译解码器中的某个令牌时关注了哪些单词。
- en: Answers to exercises
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Exercise 2**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
