- en: 3 Dimensionality reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3降维
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Curse of dimensionality and its disadvantages
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维数灾难及其缺点
- en: Various methods of reducing dimensions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的降维方法
- en: Principal Component Analysis (PCA)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: Singular Value Decomposition (SVD)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）
- en: Python solutions for both PCA and SVD
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA和SVD的Python解决方案
- en: Case study on dimension reduction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维的案例研究
- en: “Knowledge is a process of piling up facts; wisdom lies in their simplification.”
    – Martin H. Fischer
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “知识是事实的累积过程；智慧则在于它们的简化。”– 马丁·H·菲舍尔
- en: We face complex situations in life. Life throws multiple options at us and we
    choose a few viable ones from them. This decision of shortlisting is based on
    the significance, feasibility, utility, and perceived profit from each of the
    options. The ones which fit the bill are then chosen. A perfect example can be
    selecting your holiday destination. Based on the weather, travel time, safety,
    food, budget, and a number of other options, we choose a few where we would like
    to spend our next holiday. In this chapter, we are studying precisely the same
    – how to reduce the number of options, albeit in the data science and machine
    learning world.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在生活中面临复杂的情况。生活给我们提供了多种选择，我们从中选择一些可行的选项。这种筛选的决定基于每个选项的重要性、可行性、效用和预期利润。符合条件的选项随后被选中。一个完美的例子就是选择度假目的地。基于天气、旅行时间、安全、食物、预算等多种选项，我们选择了一些地方作为下一个假期的目的地。在本章中，我们正在学习同样的东西–如何在数据科学和机器学习世界中减少选项的数量。
- en: In the last chapter, we covered major clustering algorithms. We also studied
    a case study over there. The datasets we generate and use in such real-world examples
    have a lot of variables. Sometimes, there can be more than 100 variables or *dimensions*
    in the data. But not all of them are important; not all of them are significant.
    Having a lot dimensions in the dataset is referred to as the “*Curse of Dimensionality*”.
    To perform any further analysis we choose a few from the list of all of the dimensions
    or variables. In this chapter, we are going to study the need for dimension reductions,
    various dimensionality techniques, and the respective pros and cons. We will dive
    deeper into the concepts of Principal Component Analysis (PCA) and SVD (Singular
    Value Decomposition), their mathematical foundation and complement with the Python
    implementation. And continuing our structure from the last chapter, we will examine
    a real-world case study in the telecommunication sector towards the end. There
    are other advanced dimensionality reduction techniques like t-SNE, and LDA which
    we will explore in later chapters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们涵盖了主要的聚类算法。我们还在那里进行了一个案例研究。在这些真实案例的数据集中，有很多变量。有时，数据中可能有超过100个变量或*维度*。但并非所有变量都重要；并非所有变量都显著。数据集中有很多维度被称为“*维数灾难*”。为了进行进一步的分析，我们从所有维度或变量中选择了一些。在这一章中，我们将学习降维的必要性，各种降维技术，以及各自的优缺点。我们将深入研究主成分分析（PCA）和SVD（奇异值分解）的概念，它们的数学基础，并用Python实现。延续上一章的结构，我们将在最后探讨电信行业的一个真实案例研究。在后面的章节中，我们还将探索其他高级的降维技术，比如t-SNE和LDA。
- en: Clustering and dimensionality reductions are the major categories of unsupervised
    learning. We studied major clustering methods in the last chapter and we will
    cover dimensionality reduction in this chapter. With these two solutions, we would
    cover a lot of ground in the unsupervised learning domain. But there are much
    more advanced topics to be covered, which are part of the latter chapters of the
    book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维是无监督学习的主要类别。上一章我们学习了主要的聚类方法，而本章将涵盖降维。有了这两种解决方案，我们将在无监督学习领域取得很大的进展。但是还有更多的高级话题需要涵盖，这些是本书后面章节的内容。
- en: Let’s first understand what we mean by “*Curse of dimensionality*”.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解一下“*维数灾难*”是什么意思。
- en: 3.1 Technical toolkit
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1技术工具包
- en: We are using the 3.6+ version of Python as used in the last chapters. Jupyter
    Notebook will be used in this chapter too.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一章相同的Python 3.6+版本。Jupyter Notebook在本章中也将被使用。
- en: All the datasets and code files are available at the GitHub repository at ([https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter3](main.html)).
    You need to install the following Python libraries to execute the `numpy`, `pandas`,
    `matplotlib`, `scipy`, `sklearn`. Since you have used the same packages in the
    last chapter, you need not install them again. CPU is good enough for execution,
    but if you face some computing problems, switch to GPU or Google Colab. You can
    refer to Appendix to the book if you face any issues in the installation of any
    of these packages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据集和代码文件都位于GitHub存储库中（[https://github.com/vverdhan/UnsupervisedLearningWithPython/tree/main/Chapter3](main.html)）。您需要安装以下Python库才能执行`numpy`、`pandas`、`matplotlib`、`scipy`、`sklearn`等。由于您在上一章中已经使用了相同的软件包，所以无需再安装它们。CPU足以执行，但如果遇到一些计算问题，请切换到GPU或Google
    Colab。如果在安装这些包中遇到任何问题，请参考本书的附录。
- en: Now, let’s start by developing our understanding further with regard to “*Curse
    of dimensionality,*” in the following section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始在以下部分进一步了解“*维数灾难*”。
- en: 3.2 Curse of Dimensionality
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2维数灾难
- en: Let us continue with the holiday destination example we introduced earlier.
    The choice of destination is dependent on a number of parameters – safety, availability,
    food, nightlife, weather, budget, health, and so on. Having too many parameters
    to decide is a confusing thought. Let us understand by a real-life example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用我们先前介绍的度假目的地示例。目的地的选择取决于许多参数 - 安全性、可用性、餐食、夜生活、天气、预算、健康等等。有太多的参数需要决定，这是一个令人困惑的想法。我们通过一个现实生活的例子来理解。
- en: 'Consider this: A retailer wishes to launch a new range of shoes in the market.
    And for that, a target group of customers have to be chosen. This target group
    will be reached through email, newsletter, etc. The business objective is to entice
    these customers to buy the newly launched shoes. From the entire customer base,
    the target group of customers can be chosen based on the variables like customer’s
    age, gender, pocket size, preferred category, average spend, frequency of shopping,
    and so on. These many variables or *dimensions* give us a hard time shortlisting
    the customers based on a sound data analysis technique. We would be analyzing
    too many parameters simultaneously, examining the impact of each on the shopping
    probability of the customer and hence it becomes too tedious and confusing a task.
    It is the *Curse of Dimensionality* problem we face in real-world data science
    projects. We can face the curse of dimensionality in one more situation wherein
    the number of observations is lesser than the number of variables. Consider a
    dataset where the number of observations is X, while the number of variables is
    more than X – in such a case we face the Curse of Dimensionality.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：一家零售商想要在市场上推出一款新的鞋类产品。为此，需要选择一个目标客户群体，这个客户群体将通过电子邮件、通讯等方式进行接触。业务目标是引诱这些客户购买新发布的鞋类产品。从整个客户群体中，可以基于诸如客户年龄、性别、消费能力、首选类别、平均开销、购物频率等变量选择目标客户群体。这么多变量或*维度*使我们在基于有效的数据分析技术进行客户筛选时受到困扰，需要同时分析过多参数，检验每个参数对客户购物概率的影响，因此这个任务变得非常繁琐和困惑。这是真实世界的数据科学项目中我们所面临的*维数灾难*问题。我们还可以在另一种情况下遇到维度灾难现象，即当观察值的数量小于变量的数量时。考虑一个数据集，其中观察值的数量为X，而变量的数量超过了X，这种情况下我们就面临维度灾难。
- en: An easy method to understand any dataset is through visualization. Let’s visualize
    a dataset in a vector-space diagram. If we have only one attribute or feature
    in the dataset, we can represent it in one dimension. It is shown in Figure 3.1(i).
    For example, we might wish to capture only the height of an object using a single
    dimension. In case we have two attributes, we need two dimensions as shown in
    Figure 3.1(ii), wherein to get the area of an object we will require both length
    and breadth. In case we have three attributes, for example, to calculate the volume
    which requires length, breadth, and height, it requires a three-dimensional space,
    as shown in Figure 3.1(iii). And this requirement will continue to grow based
    on the number of attributes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化是理解任何数据集的简单方法。让我们在一个向量空间图中可视化一个数据集。如果数据集中只有一个属性或特征，我们可以用一个维度表示它。如图3.1(i)所示。例如，我们可能希望只用一个维度来捕捉一个物体的高度。如果我们有两个属性，我们需要两个维度，如图3.1(ii)所示，在这种情况下，为了得到一个物体的面积，我们将需要长度和宽度。如果我们有三个属性，例如，为了计算需要长度、宽度和高度的体积，它需要一个三维空间，如图3.1(iii)所示。这个需求将根据属性的数量而继续增长。
- en: Figure 3.1 (i) Only one dimension is required to represent the data points,
    for example, to represent the height of an object (ii) We need two dimensions
    to represent a data point. Each data point can correspond to the length and breadth
    of an object which can be used to calculate the area (iii) Three dimensions are
    required to show a point. Here, it can be length, breadth, and height which are
    required to get the volume of an object. This process continues based on the number
    of dimensions present in the data.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 (i) 只需要一个维度来表示数据点，例如，表示物体的高度 (ii) 我们需要两个维度来表示一个数据点。每个数据点可以对应一个物体的长度和宽度，用于计算面积
    (iii) 三个维度用于展示一个点。这里，需要长度、宽度和高度来获得物体的体积。这个过程根据数据中存在的维度数量而继续。
- en: '![03_01](images/03_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![03_01](images/03_01.png)'
- en: Now imagine if we have in total of 20 data points to be analyzed. If we have
    only one attribute, we can represent it as x[1], x[2], x[3], x[4], x[5] …. x[20]
    and hence a 20-dimensional space will be sufficient to represent these points.
    In the second example where we require two dimensions, we will require (x[1,]y[1]),
    (x[2,]y[2]), (x[3,]y[3]), (x[4,]y[4]), (x[5,]y[5])….. (x[20,]y[20]) or in other
    words 20*20 = 400 dimension space. For a three-dimensional one, we will represent
    a point as (x[1,]y[1,]z[1]), (x[2,]y[2,]z[2]), (x[3,]y[3,]z[3]), (x[4,]y[4,]z[4]),
    (x[5,]y[5,]z[5])….. (x[20,]y[20,]z[20]) and we will need 20*20*20 = 800 dimension
    space. And this process will continue.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，如果我们总共有20个要分析的数据点。如果我们只有一个属性，我们可以将它表示为x[1]、x[2]、x[3]、x[4]、x[5] …. x[20]，因此，20维空间足以表示这些点。在第二个例子中，我们需要两个维度，我们将需要(x[1,]y[1])、(x[2,]y[2])、(x[3,]y[3])、(x[4,]y[4])、(x[5,]y[5])…..
    (x[20,]y[20])，换句话说是20*20 = 400维空间。对于三维空间，我们将表示一个点为(x[1,]y[1,]z[1])、(x[2,]y[2,]z[2])、(x[3,]y[3,]z[3])、(x[4,]y[4,]z[4])、(x[5,]y[5,]z[5])…..
    (x[20,]y[20,]z[20])，我们将需要20*20*20 = 800维空间。这个过程将继续下去。
- en: Hence, it is quite easy for us to conclude that with an increase in the number
    of dimensions, the amount of space required to represent increases by leaps and
    bounds. It is referred to as the *Curse of Dimensionality*. The term was introduced
    by Richard E. Bellman and is used to refer to the problem of having too many variables
    in a dataset – some of which are significant while a lot may be of less importance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们很容易得出结论，随着维度数量的增加，所需的空间量会大幅增加。这被称为*维度灾难*。这个术语是由理查德·E·贝尔曼引入的，用来指代数据集中有太多变量的问题——其中一些是重要的，而很多可能不太重要。
- en: There is another well-known theory known as *Hughes’ Phenomenon* shown in Figure
    3.2\. Generally, in data science and machine learning, we wish to have as many
    variables as possible to train our model. It is observed that the performance
    of the supervised learning classifier algorithm will increase to a certain limit
    and will peak with the most optimal number of variables. But, using the same amount
    of training data and with an increased number of dimensions, there is a decrease
    in the performance of a supervised classification algorithm. In other words, it
    is not advisable to have the variables in a dataset if they are not contributing
    to the accuracy of the solution. And we should remove such variables from the
    dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个众所周知的理论，即*休斯现象*如图3.2所示。一般来说，在数据科学和机器学习中，我们希望有尽可能多的变量来训练我们的模型。观察到，监督学习分类器算法的性能会增加到一定的极限，随着最适数量的变量达到顶峰。但是，使用相同数量的训练数据，并增加维度数量，监督分类算法的性能会下降。换句话说，如果变量对解决方案的准确性没有贡献，最好不要将其包含在数据集中，并应将这些变量从数据集中删除。
- en: Figure 3.2 Hughes phenomenon shows that the performance of a machine learning
    model will improve initially with increase in the number of dimensions. But with
    a further increase, it leads to a decrease in the model’s performance.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 Hughes现象显示，随着维度数量的增加，机器学习模型的性能会在初期得到提升。但是随着进一步增加，模型的性能会下降。
- en: '![03_02](images/03_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![03_02](images/03_02.png)'
- en: 'Increase in number of dimensions has the following impacts on the machine learning
    model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 维度数量的增加对机器学习模型的影响如下：
- en: As the model is dealing with an increased number of variables, the mathematical
    complexity increases. For example, in the case of the k-means clustering method
    we discussed in the last chapter – when we have a greater number of variables,
    the distance calculation between respective points will become complex. And hence,
    the overall model becomes more complex.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于模型处理了更多的变量，数学复杂度也会增加。例如，在上一章讨论的k均值聚类方法中，当变量数量更多时，各个点之间的距离计算将变得更加复杂。因此，整体模型会更加复杂。
- en: The dataset generated in a larger-dimensional space can be much sparser as compared
    to a smaller number of variables. The dataset will be sparser as some of the variables
    will have missing values, NULLs, etc. The space is hence much emptier, the dataset
    is less dense, and a smaller number of variables have values associated with them.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在较大维度空间生成的数据集相对于较少变量来说可能更加稀疏。数据集将变得更稀疏，因为有些变量将会有缺失值、NULL等。因此，空间更为空，数据集更加稀疏，少数变量与之相关联的值更少。
- en: With increased complexity in the model, the processing time required increases.
    The system feels the pressure to deal with so many dimensions.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着模型复杂度的增加，所需的处理时间也会增加。系统在处理如此多维度的情况下将感到压力。
- en: And the overall solution becomes more complex to comprehend and execute. Recall
    from Chapter 1 where we have discussed supervised learning algorithms. Due to
    the high number of dimensions, we might face the problem of overfitting in supervised
    learning models.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整体解决方案变得更加复杂和难以理解和执行。回顾第1章中我们讨论过的监督学习算法。由于维度数量较高，我们可能会在监督学习模型中面临过拟合问题。
- en: When a supervised learning model has good accuracy on training data but lesser
    accuracy on unseen data, it is referred to as Overfitting. Overfitting is a nuisance
    as the very aim of machine learning models is to work well on unseen datasets
    and overfitting defeats this purpose.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当监督学习模型在训练数据上有很好的准确度，但在未知数据上准确度较低时，这就被称为过拟合。过拟合是一个麻烦，因为机器学习模型的目的是在未知数据上表现良好，而过拟合则违背了这一目的。
- en: Let us relate things to a real-world example. Consider an insurance company
    offering different types of insurance policies like life insurance, vehicle insurance,
    health insurance, home insurance, etc. The company wishes to leverage data science
    and execute clustering use cases to increase the customer base and the total number
    of policies sold. They have customer details like age, gender, profession, policy
    amount, historical transactions, number of policies held, annual income, type
    of policy, number of historical defaults, etc. At the same time, let us assume
    that variables like whether the customer is left-handed or right-handed, wears
    black or brown shoes, the shampoo brand used, the color of hair, and favorite
    restaurant are also captured. If we include all the variables in the dataset,
    the total number of variables in the resultant dataset will be quite high. The
    distance calculation will be more complex for a k-means clustering algorithm,
    the processing time will increase and the overall solution will be quite a complex
    one.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将事物与一个现实世界的例子联系起来。考虑一个保险公司提供不同类型的保险政策，如人寿保险、车辆保险、健康保险、家庭保险等。该公司希望利用数据科学并执行聚类用例来增加客户群和销售的总保单数量。他们拥有客户的详细信息，如年龄、性别、职业、保单金额、历史交易、持有的保单数量、年收入、保单类型、历史违约次数等。同时，让我们假设还捕捉到是否客户是左撇子还是右撇子、穿黑鞋还是棕鞋、使用的洗发水品牌、头发颜色和最喜欢的餐厅等变量。如果我们将所有变量包括在数据集中，那么结果数据集中的变量总数将会相当高。对于
    k-means 聚类算法，距离计算将变得更加复杂，处理时间将增加，整体解决方案将变得相当复杂。
- en: It is also imperative to note that *not* all the dimensions or variables are
    significant. Hence, it is vital to filter the important ones from all the variables
    we have. Remember, nature always prefers simpler solutions! In the case discussed
    above, it is highly likely that variables like the color of the hair and favorite
    restaurant, etc. will not impact the outputs. So, it is in our best interest to
    reduce the number of dimensions to ease the complexity and reduce the computation
    time. At the same time, it is also vital to note that dimensionality reduction
    is not always desired. It depends on the type of dataset and the business problem
    we wish to resolve. We will explore it more when we work on the case study in
    subsequent sections of the chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一点必须注意，*并非*所有的维度或变量都是重要的。因此，从我们拥有的所有变量中筛选出重要的变量至关重要。记住，自然总是倾向于简单的解决方案！在上述讨论的情况下，像头发颜色和最喜欢的餐厅等变量很可能不会影响结果。因此，我们最好减少维度以简化复杂性并减少计算时间。同时，还必须注意，降维并不总是需要的。它取决于数据集的类型和我们希望解决的业务问题。在本章后续部分的案例研究中，我们将进一步探讨这个问题。
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 小测验 - 回答这些问题来检查你的理解。答案在书的最后。'
- en: 1.   Curse of dimensionality refers to having a big size of data. TRUE or FALSE.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1.   维度灾难指的是数据规模很大。TRUE 或 FALSE。
- en: 2.   Having a high number of variables will always increase the accuracy of
    a solution. TRUE or FALSE.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2.   在数据集中拥有大量变量将始终提高解决方案的准确性。TRUE 或 FALSE。
- en: 3.   How a large number of variables in a dataset impact the model?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3.   数据集中有大量变量会如何影响模型？
- en: We have established that having a lot of dimensions is a challenge for us. We
    are now examining the various methods to reduce the number of dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，拥有许多维度对我们来说是一个挑战。我们现在正在研究各种减少维度的方法。
- en: 3.3 Dimension reduction methods
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 维度减少方法
- en: We studied the disadvantages of having really high dimensional data in the last
    section. A lesser number of dimensions might result in a simpler structure for
    our data, which will be computationally efficient. At the same time, we should
    be careful with reducing the number of variables. The output of the dimension
    reduction method should be complete enough to represent the original data and
    should not lead to any information loss. In other words, if originally, we had
    for example 500 variables and we reduced it to 120 significant ones, still these
    120 *should* be robust enough to capture *almost* all the information. Let us
    understand using a simple example.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们研究了拥有非常高维数据的缺点。维度较少可能会导致数据结构更简单，这将提高计算效率。同时，我们应该小心减少变量的数量。降维方法的输出应该足够完整以代表原始数据，不应导致任何信息损失。换句话说，如果原本我们有
    500 个变量，我们将它降低到 120 个显著变量，那么这 120 个变量*应该*足够强大，能*几乎*捕捉到所有信息。让我们通过一个简单的例子来理解。
- en: 'Consider this: We wish to predict the amount of rainfall a city will receive
    in the next month. The rainfall prediction for that city might be dependent on
    temperature over a period of time, wind speed measurements, pressure, distance
    from the sea, elevation above sea level etc. These variables make sense if we
    wish to predict rainfall. At the same time, variables like the number of cinema
    halls in the city, whether the city is the capital of the country or the number
    of red cars in the city might not impact the prediction of rainfall. In such a
    case, if we do not use the number of cinema halls in the city to predict the amount
    of rainfall, it will not reduce the capability of the system. The solution in
    all probability, will be still able to perform quite well. And hence, in such
    a case no information will be lost by dropping such a variable and surely, we
    can drop it from the dataset. On the other hand, removing variables such as temperature
    or distance from the ocean very likely will negatively affect the prediction.
    It is a very simple example to highlight the need to reduce the number of variables.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：我们希望预测下个月一个城市会接收到多少降雨量。该城市的降雨预测可能取决于一段时间内的温度、风速测量、压力、距离海洋的距离、海拔等。如果我们想要预测降雨，这些变量是有意义的。同时，例如城市中电影院的数量、城市是否是国家的首都或城市中红色汽车的数量可能不会影响降雨的预测。在这种情况下，如果我们不使用城市中电影院的数量来预测降雨量，那么它将不会降低系统的性能。解决方案很有可能仍然能够表现良好。因此，在这种情况下，通过放弃这样的变量不会丢失任何信息，当然，我们可以将其从数据集中删除。另一方面，移除温度或距离海洋的变量很可能会对预测产生负面影响。这是一个非常简单的例子，用于强调减少变量数量的必要性。
- en: The dimensions or the number of variables can be reduced by a combination of
    manual and algorithm-based methods. But before studying them in detail, there
    are a few mathematical terms and components which we should be aware of before
    proceeding ahead, which we will discuss next.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 维度或变量数量可以通过手动和基于算法的方法的组合来减少。但在详细研究它们之前，我们应该了解一些数学术语和组件，然后再继续，下面我们将对此进行讨论。
- en: 3.3.1 Mathematical foundation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 数学基础
- en: There are quite a few mathematical terms that one must grab to develop a thorough
    understanding of dimensionality reduction methods.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多数学术语是必须掌握的，以便全面了解降维方法。
- en: We are trying to reduce the number of dimensions of a dataset. A dataset is
    nothing but a matrix of values – hence a lot of the concepts are related to matrix
    manipulation methods, geometrical representation of them, and performing transformations
    on such matrices. The mathematical concepts are discussed in the Appendix Mathematical
    Foundation of the book. You would also require an understanding of eigenvalues
    and eigenvectors. These concepts will be reused throughout the book and hence
    they have been put in the Appendix for quick reference. You are advised to go
    through them before proceeding. We will now explore a few manual methods for dimensionality
    reduction methods and then proceed to algorithm-based ones.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在尝试减少数据集的维度。数据集只是一个值矩阵——因此很多概念与矩阵操作方法、它们的几何表示以及对这些矩阵进行变换有关。数学概念在本书的附录数学基础中进行了讨论。您还需要理解特征值和特征向量。这些概念将在整本书中被重复使用，因此它们已经放在附录中供快速查阅。在继续之前，建议您先阅读这些内容。现在我们将探讨一些手动的降维方法，然后再转向基于算法的方法。
- en: 3.4 Manual methods of dimensionality reduction
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 手动降维方法
- en: To tackle the curse of dimensionality, we wish to reduce the number of variables
    in a dataset. The reduction can be done by removing the variables from the dataset.
    Or a very simple solution for dimensionality reduction can be combining the variables
    which can be grouped logically or can be represented using a common mathematical
    operation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决维度灾难，我们希望减少数据集中的变量数量。可以通过从数据集中移除变量来实现减少。或者，一个非常简单的解决方案是合并那些可以逻辑分组或用共同的数学运算表示的变量。
- en: For example, as shown in Table 3.1 below, the data can be from a retail store
    where different customers have generated different transactions. We will get the
    sales, number of invoices and the number of items bought for each customer over
    a period of time. In the table below, customer 1 has generated two invoices, bought
    5 items in total, and generated a total sale of 100.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如下表3.1所示，数据可以来自零售商店，不同的客户产生了不同的交易。我们将获得每个客户一段时间内的销售额、发票数和购买的商品数量。在下表中，客户1产生了两张发票，总共购买了5件商品，并产生了100的销售额。
- en: If we wish to reduce the number of variables, we might combine three variables
    as two variables. Here, we have introduced variables ATV (average transaction
    value) and ABS (average basket size) wherein ATV = Sales/Invoices and ABS = NumberOfItems/Invoices.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望减少变量的数量，我们可以将三个变量合并为两个变量。在这里，我们引入了变量 ATV（平均交易价值）和 ABS（平均篮子大小），其中 ATV =
    销售额/发票数，ABS = 商品数量/发票数。
- en: So, in the second table for Customer 1, we have ATV as 50 and ABS as 2.5\. Hence,
    the number of variables has been reduced from three to two. The process here is
    only an example to show how we can combine various variables. It does not mean
    that we should replace sales with ATV as a variable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在第二个表中，对于客户1，我们有ATV为50，ABS为2.5。因此，变量的数量已经从三个减少到两个。该过程只是一个示例，展示了如何可以结合各种变量。这并不意味着我们应该用ATV替换销售额作为一个变量。
- en: Table 3.1 In the first table, we have the sales, invoices and number of items
    as the variables. In the second table, they have been combined to create new variables.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 在第一张表中，我们有销售额、发票和商品数量作为变量。在第二个表中，它们被合并为创建新变量。
- en: '![03_T05](images/03_T05.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![03_T05](images/03_T05.png)'
- en: And this process can continue to reduce the number of variables. Similarly,
    for a telecom subscriber, we will have the minutes of mobile calls made during
    30 days in a month. We can add them to create a single variable – *minutes used
    in a month*. The above examples are very basic ones to start with. Using the manual
    process, we can employ two other commonly used methods – manual selection and
    using correlation coefficient.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以继续减少变量的数量。同样地，对于一个电信用户，我们将拥有一个月内手机通话的分钟数。我们可以将它们相加，创建一个单一的变量 - *一个月内使用的分钟数*。上面的例子是非常基础的，用于起步。使用手动过程，我们可以采用另外两种常用的方法
    - 手动选择和使用相关系数。
- en: 3.4.1 Manual feature selection
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 手动特征选择
- en: Continuing from the rainfall prediction example we discussed in the last section
    – a data scientist might be able to drop a few variables. This will be based on
    a deep understanding of the business problem at hand and the corresponding dataset
    being used. However, it is an underlying assumption that the dataset is quite
    comprehensible for the data scientist and they understand the business domain
    well. Most of the time, the business stakeholders will be able to guide on such
    methods. It is also necessary that the variables are unique and not much of a
    dependency exists.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 继续上一节中讨论的降雨预测示例 - 数据科学家可能能够去掉一些变量。这将基于对手头的业务问题和相应数据集的深刻理解。然而，这是一个潜在的假设，即数据科学家能够充分理解数据集，并对业务领域有深刻理解。大部分时间，业务利益相关者将能够指导这样的方法。同样重要的是，变量是独特的，并且不存在太多的依赖性。
- en: As shown in Table 3.2 below, we can remove a few of the variables which might
    not be useful for predicting rainfall.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表3.2所示，我们可以删除一些对于预测降雨可能没有用的变量。
- en: Table 3.2 In the first table, we have all the variables present in the dataset.
    Using business logic, some of the variables which might be not much use have been
    discarded in the second table. But this is to be done with due caution. And the
    best way is to get guidance from the business stakeholders.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 在第一个表中，我们有数据集中所有的变量。使用业务逻辑，一些可能没有太多用处的变量在第二个表中被丢弃了。但这需要谨慎处理。最好的方法是从业务利益相关者那里得到指导。
- en: '![03_T06](images/03_T06.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![03_T06](images/03_T06.png)'
- en: Sometimes, feature selection methods are also referred to as *wrapper methods*.
    Here, a machine learning model is wrapped or fitted with a subset of variables.
    In each iteration, we will get a different set of results. The set which generates
    the best results is selected for the final model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，特征选择方法也被称为*包装器方法*。在这里，机器学习模型使用变量的子集进行包装或拟合。在每次迭代中，我们将得到不同的结果集。选择生成最佳结果的集合用于最终模型。
- en: The next methods are based on the correlation existing between various attributes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的方法是基于各种属性之间的相关性而存在的。
- en: 3.4.2 Correlation coefficient
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 相关系数
- en: Correlation between two variables simply means that have a mutual relationship
    with each other. The change in the value of one variable will impact the value
    of another, which means that data points with similar values in one variable have
    similar values for the other variable. The variables which are highly correlated
    with each other are supplying similar information and hence one of them can be
    dropped.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 两个变量之间的相关性简单地意味着它们彼此具有相互关系。一个变量值的变化将影响另一个变量的值，这意味着一个变量的数值相似的数据点也在另一个变量中具有相似的数值。高度相关的变量彼此提供类似的信息，因此其中一个可以被舍弃。
- en: Correlation is described in detail in the Appendix Mathematical Foundation of
    the book.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性在书的附录数学基础中有详细描述。
- en: For example, for a retail store, the number of invoices generated in a day will
    be highly correlated with the amount of sales generated and hence, one of them
    can be dropped. Another example can be – students who study for a higher number
    of hours will have better grades than the ones who study less (mostly!).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于零售店，一天内产生的发票数量与产生的销售额将高度相关，因此可以舍弃其中之一。另一个例子是 - 学习时间更长的学生通常比学习时间较短的学生成绩更好（大多数情况下！）。
- en: But we should be careful in dropping the variables and should not trust correlation
    alone. The business context of a variable should be thoroughly understood before
    taking any decision.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但在放弃变量时我们应该小心，不应仅仅依赖相关性。在做出任何决定之前，应该充分了解变量的业务背景。
- en: It is a good idea to discuss with the business stakeholders before dropping
    any variables from the study.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在从研究中删除任何变量之前，与业务利益相关者讨论是个好主意。
- en: Correlation-based methods are sometimes called *filter methods*. Using correlation
    coefficients, we can filter and choose the variables which are most significant.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于相关性的方法有时被称为*过滤方法*。使用相关系数，我们可以过滤并选择最重要的变量。
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 自测题 – 回答这些问题以检查你的理解。答案在书的末尾'
- en: 1.   We can drop a variable simply if we feel is not required. TRUE or FALSE.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 1.   如果我们觉得一个变量不需要，我们可以简单地将其舍弃。是或否。
- en: 2.   If two variables are correlated, ALWAYS drop one of them. TRUE or FALSE.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 2.   如果两个变量相关，总是舍弃其中一个。是或否。
- en: Manual methods are easier solutions and can be executed quite efficiently. The
    dataset size is reduced and we can proceed ahead with the analysis. But manual
    methods are sometimes subjective and depend a lot on the business problem at hand.
    Many times, it is also not possible to employ manual methods for dimension reduction.
    In such situations, we have algorithm-based methods which we are studying in the
    next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 手动方法是更简单的解决方案，可以相当高效地执行。数据集的大小被减小，我们可以继续进行分析。但是手动方法有时是主观的，并且在很大程度上取决于手头的业务问题。许多时候，不可能使用手动方法进行降维。在这种情况下，我们有基于算法的方法，我们将在下一节中学习。
- en: 3.4.3 Algorithm-based methods for reducing dimensions
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 基于算法的降维方法
- en: We examined manual methods in the last section. Continuing from there, we will
    examine algorithm-based methods in this section. The algorithm-based techniques
    are based on a more mathematical base and hence prove to be more scientific methods.
    In real-world business problems, we use a combination of both manual and algorithm-based
    techniques. Manual methods are straightforward to execute as compared to algorithm-based
    techniques. Also, we cannot comment on the comparison of both techniques, as they
    are based on different foundations. But at the same time, it is imperative that
    you put due diligence in the implementation of algorithm-based techniques.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了手动方法。从那里继续，我们将在本节中研究基于算法的方法。基于算法的技术是基于更数学的基础的，因此证明是更科学的方法。在现实世界的业务问题中，我们使用手动和基于算法的技术的组合。与基于算法的技术相比，手动方法执行起来更直接。此外，我们无法评论两种技术的比较，因为它们基于不同的基础。但与此同时，你必须在实施基于算法的技术时尽职尽责。
- en: The major techniques used in dimensionality reductions are listed below. We
    will be exploring most of them in this book.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 降维中使用的主要技术如下所示。我们将在本书中探讨其中大部分。
- en: Principal Component Analysis (PCA)
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: Singular Value Decomposition (SVD)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）
- en: Linear Discriminant Analysis (LDA)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）
- en: Generalized Discriminant Analysis (GDA)
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 广义判别分析（GDA）
- en: Non-negative matrix factorization (NMF)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）
- en: Multi-dimension scaling (MDS)
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多维缩放（MDS）
- en: Locally Linear Embeddings (LLE)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 局部线性嵌入（LLE）
- en: IsoMaps
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等距映射
- en: Autoencoders
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器
- en: t-SNE (T-distributed Stochastic Neighbor Embedding)
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: t-SNE（T分布随机邻域嵌入）
- en: These techniques are utilized for the common end goal – transform the data from
    a high-dimensional space to a low-dimensional one. Some of the data transformations
    are linear in nature, while some are non-linear.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术被用于一个共同的最终目标 - 将数据从高维空间转换为低维空间。一些数据转换是线性的，而一些是非线性的。
- en: We are going to discuss Principal Component Analysis (PCA) and Singular Value
    Decomposition (SVD) in detail in this chapter. In the later chapters of the book,
    other major techniques are being explored. Perhaps, PCA is the most quoted dimensionality
    reduction method which we are exploring in the next section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章详细讨论主成分分析（PCA）和奇异值分解（SVD）。在本书的后续章节中，将探讨其他主要技术。也许，PCA是我们在下一节中探讨的最常引用的降维方法。
- en: 3.5 Principal Component Analysis (PCA)
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 主成分分析（PCA）
- en: 'Consider this: You are working on a dataset that has 250 variables. It is almost
    impossible to visualize such a high-dimensional space. Some of the 250 variables
    might be correlated with each other, some of them might not be and there is a
    need to reduce the number of variables without losing much information. Principal
    Component Analysis or PCA allows us to mathematically select the most important
    features and leave the rest. PCA does reduce the number of dimensions but also
    preserves the most important relationships between the variables and the important
    structures in the dataset. Hence, the number of variables is reduced but the important
    information in the dataset is kept safe.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：你正在处理一个有250个变量的数据集。几乎不可能可视化这样一个高维空间。这250个变量中的一些可能彼此相关，一些可能不相关，并且有必要减少变量的数量而不丢失太多信息。主成分分析或PCA允许我们在数学上选择最重要的特征并留下其他特征。PCA确实减少了维数，但也保留了变量之间最重要的关系和数据集中的重要结构。因此，变量的数量减少了，但数据集中的重要信息得到了保留。
- en: PCA is a projection of high-dimensional data in lower dimensions. In simpler
    terms, we are reducing a n-dimensional space into an m-dimensional one where n
    > m while maintaining the nature and the essence of the original dataset. In the
    process, the old variables are reduced to newer ones, while maintaining the crux
    of the original dataset The new variables thus created are called *Principal Components*.
    The principal components are a linear combination of the raw variables. As a result
    of this transformation, the first principal component captures the maximum randomness
    or the highest variance in the dataset. The second principal component created
    is orthogonal to the first component.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是将高维数据投影到低维空间的过程。简单来说，我们将一个n维空间降维到一个m维空间（其中n > m），同时保持原始数据集的本质和基本特征。在这个过程中，旧的变量被降维为新的变量，同时保持了原始数据集的关键信息，新创建的变量称为*主成分*。主成分是原始变量的线性组合。由于这种转换，第一个主成分捕获了数据集中的最大随机性或最高方差。创建的第二个主成分与第一个主成分正交。
- en: If two straight lines are orthogonal to each other, it means they are at an
    angle of 90⁰ to each other,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两条直线彼此正交，意味着它们相互成`90⁰`的角度，
- en: And the process continues to the third component and so on. Orthogonality allows
    us to maintain that there is no correlation between subsequent principal components.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程一直持续到第三个成分等等。正交性使我们能够保持后续主成分之间没有相关性。
- en: PCA utilizes linear transformation of the dataset and such methods are sometimes
    referred to as feature projections. The resultant dataset or the projection is
    used for further analysis.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PCA利用数据集的线性变换，这样的方法有时被称为特征投影。结果数据集或投影用于进一步的分析。
- en: Let us understand better by means of an example. In (Table 3.3) shown below,
    we have represented the total perceived value of a home using some variables.
    The variables are area (sq m), number of bedrooms, number of balconies, distance
    from the airport, distance from the train station, and so on – we have 100+ variables.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子更好地理解。在下面（表3.3）所示的例子中，我们用一些变量来表示一个家庭的总感知价值。这些变量包括面积（平方米）、卧室数、阳台数、距离机场的距离、距离火车站的距离等等——我们有100多个变量。
- en: Table 3.3 The variables based on which a price of a house can be estimated
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3 房价估算的基于的变量
- en: '![03_T07](images/03_T07.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![03_T07](images/03_T07.png)'
- en: We can combine some of the variables mathematically and logically. PCA will
    create a new variable which is a linear combination of some of the variables as
    shown in the example below. It will get the best *linear* combination of original
    variables so that the new_variable is able to capture the maximum variance of
    the dataset. The Equation 3.1 is only an example shown for illustration purposes
    wherein we are showing a new_variable created by a combination of other variables.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以数学上和逻辑上结合一些变量。PCA将创建一个新变量，它是一些变量的线性组合，如下面的示例所示。它将得到原始变量的最佳*线性*组合，以便新变量能够捕获数据集的最大方差。方程式3.1仅是为了说明目的而显示的一个示例，在这个示例中我们展示了一个新变量，它是其他变量的组合。
- en: (Equation 3.1)
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程式3.1）
- en: new_variable = a*area – b*bedrooms + c*distance – d*schools
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: new_variable = a*面积 – b*卧室数 + c*距离 – d*学校数
- en: Now let’s understand the concept visually. In a vector-space diagram, we can
    represent the dataset as shown in Figure 3.3 below. The first figure represents
    the raw data where we can visualize the variables in an x-y diagram. As discussed
    above, we wish to create a linear combination of variables. Or in other words,
    we wish to create a mathematical equation that will be able to explain the relationship
    between x and y.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过视觉概念来理解这个概念。在矢量空间图中，我们可以如下图3.3所示地表示数据集。第一张图代表了原始数据，我们可以在x-y图表中可视化变量。如上所述，我们希望创建变量的线性组合。换句话说，我们希望创建一个数学方程，能够解释x和y之间的关系。
- en: The output of such a process will be a straight line as shown in the second
    figure in Figure 3.3\. This straight line is sometimes referred to as the *Line
    of Best fit.* Using this line of best fit, we can predict a value of y for a given
    value of x. These predictions are nothing but the projections of data points on
    a straight line.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种过程的输出将是一条直线，如图3.3中的第二张图所示。这条直线有时被称为*最佳拟合线*。利用这条最佳拟合线，我们可以预测给定x值的y值。这些预测实际上就是数据点在直线上的投影。
- en: The difference between the actual value and the projections is the error as
    shown in the third figure in Figure 3.3 below. The total sum of these errors is
    called the total projection error.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图 3.3 中的第三个图所示，实际值与投影之间的差异是误差。这些误差的总和被称为总投影误差。
- en: Figure 3.3 (i) The dataset can be represented in a vector-space diagram (ii)
    The straight line can be called as the line of best fit having the projections
    of all the data points on it. (iii) The difference between the actual value and
    the projections are the error terms.
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3 (i) 数据集可以在向量空间图中表示 (ii) 直线可以称为最佳拟合线，其具有所有数据点的投影 (iii) 实际值与投影之间的差异是误差项。
- en: '![03_04](images/03_04.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![03_04](images/03_04.png)'
- en: There can be multiple options for this straight line as shown in Figure 3.4
    below. These different straight lines will have different errors and different
    values of variances captured.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图 3.4 所示，这条直线可以有多种选项。这些不同的直线将具有不同的误差和捕捉到的方差值。
- en: Figure 3.4 The data set can be captured by a number of lines, but not all the
    straight lines will be able to capture the maximum variance. The equation which
    gives the minimum error will be the chosen one.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.4 数据集可以用多条直线来捕捉，但并非所有直线都能捕捉到最大的方差。给出最小误差的方程将被选定。
- en: '![03_05](images/03_05.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![03_05](images/03_05.png)'
- en: The straight line which is able to capture the maximum variance will be chosen
    one. Or in other words, it is giving the minimum error. It will be the *first
    principal component* and the direction of maximum spread will be the *principal
    axis*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 能够捕捉最大方差的直线将被选定。换句话说，它给出了最小的误差。它将是*第一个主成分*，最大扩展方向将是*主轴*。
- en: The second principal component will be derived in a similar fashion. Since we
    know the first principal axis, we can subtract the variance along this principal
    axis from the total variance to get the residual variance. Or in other words,
    using the first principal component we would capture some variance in the dataset.
    But there will be a portion of the total variance in the dataset which is still
    unexplained by the first principal component. The portion of the total variance
    unexplained is the residual variance. Using the second principal component, we
    wish to capture as much variance as we can.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第二主成分将以类似的方式导出。由于我们知道第一个主轴，我们可以从总方差中减去沿着该主轴的方差以获得残差方差。换句话说，使用第一个主成分，我们将在数据集中捕捉一些方差。但数据集中仍有部分总方差尚未由第一个主成分解释。未解释的总方差部分是残差方差。使用第二主成分，我们希望尽可能多地捕捉方差。
- en: Using the same process to capture the direction of maximum variance, we will
    get the second principal component. The second principal component can be at a
    number of angles with respect to the first one as shown in Figure 3.5\. It is
    mathematically proven that if the second principal component is orthogonal to
    the first principal component, it allows us to capture the maximum variance using
    the two principal components. In Figure 3.5 we can observe that the two principal
    components are at an angle of 90⁰ with each other.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的过程来捕捉最大方差的方向，我们将得到第二个主成分。第二主成分可以与第一个主成分呈多个角度，如图 3.5 所示。数学上已经证明，如果第二主成分与第一个主成分正交，那么我们可以使用两个主成分来捕捉最大方差。在图
    3.5 中，我们可以观察到两个主成分彼此之间呈 90⁰ 角。
- en: Figure 3.5 (i) The first figure on the left is the first principal component.
    (ii) The second principal component can be at different angles with respect to
    the first principal component. We have to find the second principal which allows
    to capture the maximum variance (iii) To capture the maximum variance, the second
    principal component should be orthogonal to the first one and hence the combined
    variance captured in maximized.
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.5 (i) 左侧的第一个图是第一个主成分。 (ii) 第二主成分可以相对于第一个主成分处于不同角度。我们必须找到第二主成分，它允许捕捉最大方差
    (iii) 为了捕捉最大方差，第二主成分应与第一个主成分正交，因此组合方差被最大化。
- en: '![03_06](images/03_06.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![03_06](images/03_06.png)'
- en: And the process continues for the third, fourth principal component, and so
    on. With more principal components, the representation in a vector-space becomes
    difficult to visualize. You can think of a vector space diagram with more than
    three axes. Once all the principal components are derived, the dataset is projected
    onto these axes. The columns in this transformed dataset are the *principal components*.
    The principal components created will be lesser than the number of original variables
    and capture maximum information present in the dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第三、第四个主成分等依此类推。随着主成分的增多，向量空间中的表示变得难以可视化。你可以将其想象成一个带有多个轴的向量空间图。一旦所有主成分都被导出，数据集就会投影到这些轴上。这个转换后的数据集中的列是*主成分*。创建的主成分数量会少于原始变量的数量，并且捕获数据集中存在的最大信息。
- en: 'Before we examine the process of PCA in-depth, let’s study its important characteristics:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究 PCA 过程之前，让我们先了解其重要特性：
- en: PCA aims to reduce the number of dimensions in the resultant dataset.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 的目标是减少结果数据集中的维数。
- en: PCA produces principal components which aim to reduce the noise in the dataset
    by maximizing the feature variance.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 生成的主成分旨在通过最大化特征方差来减少数据集中的噪声。
- en: At the same time, the principal components reduce the redundancy in the dataset.
    It is achieved by minimizing the covariance between the pairs of features.
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，主成分减少了数据集中的冗余。这是通过最小化特征对之间的协方差实现的。
- en: The original variables no longer exist in the newly created dataset. Instead,
    new variables are created using these variables.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始变量不再存在于新创建的数据集中。相反，使用这些变量创建新变量。
- en: It is not necessary that the principal components will map one-to-one with all
    the variables present in the dataset. They are a new combination of the existing
    variables. And hence, they can be a combination of several different variables
    in one principal component (as shown in Equation 3.1).
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分不一定会与数据集中的所有变量一一对应。它们是现有变量的新组合。因此，它们可以是一个主成分中多个不同变量的组合（如方程式 3.1 所示）。
- en: The new features created from the dataset do not share the same column names.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集创建的新特征不共享相同的列名。
- en: The original variables might be correlated with each other, but the newly created
    variables are unrelated with each other.
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始变量可能彼此相关，但新创建的变量彼此不相关。
- en: The number of newly created variables is lesser than the original number of
    variables. We process to select the number of principal components has been described
    in the Python implementation section. After all, that is the whole purpose of
    dimensionality reduction.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新创建的变量数量少于原始变量的数量。我们选择主成分数量的过程已在 Python 实现部分进行了描述。毕竟，降维的整个目的就在于此。
- en: If PCA has been used for reducing the number of variables in a training dataset,
    the testing/validation datasets have to be reduced by using PCA.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 PCA 用于减少训练数据集中的变量数量，则必须使用 PCA 减少测试/验证数据集。
- en: PCA is not synonymous with dimensionality reduction. It can be put into use
    for a number of other usages too. It is generally a PCA only for dimensionality
    reduction will be a misnomer for sure.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 不等同于降维。它可以用于许多其他用途。一般来说，仅仅将 PCA 用于降维是错误的。
- en: We will now examine the approach used while implementing PCA and then we will
    develop a Python solution using PCA. Though we need not apply all the steps while
    we develop the codes, as the heavy lifting has already been done by the packages
    and libraries. The steps given below are hence taken care of by the packages,
    but still, it is imperative that you understand these steps to properly appreciate
    how PCA works.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将研究在实现 PCA 时采用的方法，然后我们将使用 PCA 开发一个 Python 解决方案。虽然我们在开发代码时不需要应用所有步骤，因为这些重活已经由包和库完成。下面给出的步骤已由这些包处理，但仍然必须理解这些步骤，以正确理解
    PCA 的工作原理。
- en: 'The steps followed in PCA are:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 所遵循的步骤是：
- en: In PCA, we start with **normalizing our dataset** as a first step. It ensures
    that all our variables have a common representation and become comparable. We
    have methods to perform the normalization in Python, which we will study when
    we develop the code. To explore more on normalizing the dataset, you can refer
    to the Appendix Mathematical Foundation.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PCA中，我们首先**对数据集进行标准化**。 这确保我们所有的变量都具有共同的表示并且可比较。 我们有方法在Python中执行标准化，我们将在开发代码时学习。
    要更多了解数据集的标准化，请参考附录数学基础。
- en: '**Get the covariance in** the normalized dataset. It allows us to study the
    relationship between the variables. We generally create a covariance matrix as
    shown in the Python example in the next section.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在标准化数据集中**获取协方差**。 这使我们能够研究变量之间的关系。 我们通常创建如下所示的协方差矩阵，如下一节的Python示例所示。
- en: We can then calculate the eigenvectors and eigenvalues of the covariance matrix.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以计算协方差矩阵的特征向量和特征值。
- en: We then sort the eigenvalues in decreasing order of eigenvalues. The eigenvectors
    corresponding to the maximum value of eigenvalues are chosen. The components hence
    chosen will be able to capture the maximum variance in the dataset. There are
    other methods to shortlist the principal components which we will explore while
    we develop the Python code.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们按照特征值的降序对特征值进行排序。 选择与最大特征值对应的特征向量。 因此所选的组件将能够捕获数据集中的最大方差。 还有其他方法来列出主要组件，我们将在开发Python代码时进行探讨。
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 快速测验 - 回答这些问题以检查您的理解。 书的末尾有答案'
- en: 1.   PCA will result in the same number of variables in the dataset. TRUE or
    FALSE.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 1. PCA将导致数据集中变量的数量相同。 真还是假。
- en: 2.   PCA will be able to capture 100% information in the dataset. TRUE or FALSE.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 2. PCA将能够在数据集中捕获100％的信息。 真还是假。
- en: 3.   What is the logic of selecting principal components in PCA?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 选择PCA中主要组件的逻辑是什么？
- en: So, in essence, principal components are the linear combinations of the original
    variables. The weight in this linear combination is the eigenvector satisfying
    the error criteria of the least square method. We are studying Eigenvalue decomposition
    now and SVD is covered in the next section (3.6).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从本质上讲，主成分是原始变量的线性组合。 这种线性组合中的权重是满足最小二乘法误差标准的特征向量。 我们现在正在研究特征值分解，而奇异值分解将在下一节（3.6）中介绍。
- en: 3.5.1 Eigenvalue Decomposition
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 特征值分解
- en: We studied PCA in the last section where we said that principal components are
    the linear combination of the original variables. We will explore on the eigenvalue
    decomposition for PCA now.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中学习了PCA，我们说主要组件是原始变量的线性组合。 现在我们将探讨PCA的特征值分解。
- en: In the context of PCA, the eigenvector will represent the direction of the vector
    and the eigenvalue will be the variance that is captured along that eigenvector.
    It can be shown by means of Figure 3.6 below, where we are breaking the original
    nxn matrix into components.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCA的上下文中，特征向量将表示矢量的方向，特征值将是沿着该特征向量捕获的方差。 下面的图3.6可以说明，我们正在将原始的nxn矩阵分解为组件。
- en: Figure 3.6 Using Eigenvalue decomposition, the original matrix can be broken
    into eigenvector matrix, eigenvalue matrix and an inverse of eigenvector matrix.
    We implement PCA using this methodology.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6 使用特征值分解，原始矩阵可以分解为特征向量矩阵，特征值矩阵和特征向量矩阵的逆。 我们使用这种方法实现PCA。
- en: '![03_07](images/03_07.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![03_07](images/03_07.png)'
- en: Mathematically, we can show the relation by Equation 3.2
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以通过方程式3.2来表示关系。
- en: (Equation 3.2)
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （方程式3.2）
- en: A*v = λ*v
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: A*v = λ*v
- en: where A is a square matrix, v is the eigenvector and λ is the eigenvalue. Here,
    it is important to note that Eigenvector matrix is the orthonormal matrix and
    its columns are eigenvectors. Eigenvalue matrix is the diagonal matrix and its
    eigenvalues are the diagonal elements. The last component is the inverse of the
    eigenvector matrix. Once we have the eigenvalues and the eigenvectors, we can
    choose the significant eigenvectors for getting the principal components.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中A是方阵，v是特征向量，λ是特征值。 在这里，重要的是要注意特征向量矩阵是正交矩阵，其列是特征向量。 特征值矩阵是对角线矩阵，其特征值是对角线元素。
    最后一个组件是特征向量矩阵的逆。 一旦我们有了特征值和特征向量，我们就可以选择显著的特征向量来获取主成分。
- en: We are presenting PCA and SVD as two separate methods in this book. Both of
    the methods are used to reduce high-dimensional data into fewer ones, and in the
    process retain the maximum information in the dataset. The difference between
    the two is – SVD exists for any sort of matrix (rectangular or square), whereas
    the eigen decomposition is possible only for square matrices. You will understand
    it better once we have covered SVD later in this chapter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中将PCA和SVD作为两种单独的方法进行介绍。这两种方法都用于将高维数据降维到较少的维度，并在此过程中保留数据集中的最大信息量。两者的区别在于
    - SVD存在于任何类型的矩阵（矩形或方形），而特征值分解仅适用于方形矩阵。等我们在本章后面介绍SVD时，你会更好地理解它。
- en: We will now create a Python solution using eigenvalue decomposition.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用特征值分解创建一个Python解决方案。
- en: 3.5.2 Python solution using PCA
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 使用PCA的Python解决方案
- en: We have studied the concepts of PCA and the process using eigenvalue decomposition.
    It is time for us to dive into Python and develop a PCA solution on a dataset.
    We will show you how to create eigenvectors and eigenvalues on the dataset. To
    implement the PCA algorithms, we will use the `sklearn` library. Libraries and
    packages provide a faster solution for implementing algorithms.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了PCA的概念和使用特征值分解的过程。现在是时候进入Python并在数据集上开发一个PCA解决方案了。我们将向你展示如何在数据集上创建特征向量和特征值。为了实现PCA算法，我们将使用`sklearn`库。库和包提供了一种更快的实现算法的解决方案。
- en: We are using the Iris dataset for this problem. It is one of the most popular
    datasets used for machine learning problems. The dataset contains data of three
    iris species with 50 samples each and having properties of each flower – like
    petal length, sepal length etc. The objective of the problem is to predict the
    species using the properties of the flower. The independent variables hence are
    the flower properties whereas the variable “Species” is the target variable. The
    dataset and the code are checked-in at the GitHub repository. Here, we are using
    the inbuilt PCA functions which reduce the effort required to implement PCA.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用鸢尾花数据集来解决这个问题。这是用于机器学习问题的最受欢迎的数据集之一。该数据集包含三种鸢尾花的数据，每种鸢尾花有50个样本，并且具有每朵花的特性
    - 如花瓣长度，萼片长度等。问题的目标是使用花的特性来预测物种。因此，独立变量是花的特性，而变量“物种”是目标变量。数据集和代码已经提交到GitHub仓库。在这里，我们使用内置的PCA函数来减少实现PCA所需的工作量。
- en: '**Step 1:** Load all the necessary libraries first. We are going to use numpy,
    pandas, seaborn, matplotlib and sklearn. Note that we have imported PCA from sklearn.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：** 首先加载所有必要的库。我们将使用numpy、pandas、seaborn、matplotlib和sklearn。请注意，我们从sklearn中导入PCA。'
- en: These are the most standard libraries. You will find that almost all the machine
    learning solutions would import these libraries in the solution notebook.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是最标准的库。你会发现几乎所有的机器学习解决方案都会在解决方案笔记本中导入这些库。
- en: '[PRE0]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2:** Load the dataset now. It is a .csv file.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：** 现在加载数据集。它是一个.csv文件。'
- en: '[PRE1]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3:** We will now perform a basic check on the dataset – looking at the
    first five rows, shape of the data, spread of the variables etc. We are not performing
    an extensive EDA here as the steps are covered in Chapter 2\. The dataset has
    150 rows and 6 columns.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：** 现在我们将对数据集进行基本检查 - 查看前五行，数据的形状，变量的分布等。我们在这里不进行详尽的探索性数据分析，因为这些步骤在第2章中已经覆盖了。数据集有150行和6列。'
- en: '[PRE2]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![03_07a](images/03_07a.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![03_07a](images/03_07a.png)'
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![03_07b](images/03_07b.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![03_07b](images/03_07b.png)'
- en: '[PRE4]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Step 4:** Here we have to break the dataset into independent variables and
    target variable. X_variables here represent the independent variables which are
    in the first 4 columns of the dataset while y_variable is the target variable
    which is Species in this case and is the final column in the dataset. Recall,
    we wish to predict the Species of a flower using the other properties. Hence we
    have separated the target variable Species and other independent variables.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4：** 这里我们需要将数据集分成自变量和因变量。X_variables代表自变量，它们位于数据集的前4列，而y_variable是因变量，在这种情况下是物种，是数据集中的最后一列。回想一下，我们希望使用其他属性来预测花的物种。因此，我们将目标变量物种和其他自变量分开。'
- en: '[PRE5]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Step 5:** We are now normalizing our dataset. The inbuilt method of StandardScalar()
    does the job for us quite easily.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5：** 现在我们正在对数据集进行标准化。StandardScalar() 内置方法可以很容易地完成这项工作。'
- en: StandardScalar method normalizes the dataset for us. It subtracts the mean from
    the variable and divided by the standard deviation. For more details on normalization,
    refer to the Appendix Mathematical Foundation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: StandardScalar 方法为我们对数据集进行了归一化处理。它从变量中减去均值，然后除以标准差。有关归一化的更多细节，请参阅附录数学基础知识。
- en: We invoke the method and then use it on our dataset to get the transformed dataset.
    Since, we are working on independent variables, we are using X_variables here.
    First we invoke the StandardScalar() method. And then we use fit_transform method.
    The fit_transform method first fits the transformers to X and Y and then returns
    a transformed version of X.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用该方法，然后在我们的数据集上使用它来获得转换后的数据集。由于我们正在处理自变量，所以这里使用了X_variables。首先我们调用了StandardScalar()方法。然后使用fit_transform方法。fit_transform方法首先将转换器拟合到X和Y，然后返回X的转换版本。
- en: '[PRE6]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Step 6:** We will now calculate the covariance matrix. And print it, the
    output is shown below. Getting the covariance matrix is straightforward using
    numpy.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 6 步：** 现在我们将计算协方差矩阵，并将其打印出来，输出如下所示。使用 numpy 很容易得到协方差矩阵。'
- en: '[PRE7]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![03_07c](images/03_07c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![03_07c](images/03_07c.png)'
- en: '**Step 7:** Now in this step the eigenvalues are being calculated. Inside numpy
    library we have the inbuilt functionality to calculate the eigenvalues. We will
    then sort the eigenvalues in descending order. To shortlist the principal components,
    we can choose eigenvalues greater than 1\. This criterion is called *Kaiser criteria.*
    We are exploring other methods too.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 7 步：** 现在，在这一步中正在计算特征值。在 numpy 库中，我们有内置功能来计算特征值。然后我们按降序对特征值进行排序。为了筛选主成分，我们可以选择大于1的特征值。这个标准被称为*Kaiser准则*。我们也在探索其他方法。'
- en: Eigenvalue represents how much a component is good as a summary of the data.
    If the eigenvalue is 1, it means that the component contains the same amount of
    information as a single variable. And hence we choose the eigenvalue which is
    greater than 1.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值代表一个成分作为数据摘要的优劣程度。如果特征值为1，意味着该成分包含与单个变量相同数量的信息。因此我们选择大于1的特征值。
- en: In this code, first we are getting the `eigen_values` and `eigen_vectors`. And
    then we are arranging then in descending order.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们首先获得`eigen_values`和`eigen_vectors`。然后按降序排列它们。
- en: '[PRE8]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![03_07d](images/03_07d.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![03_07d](images/03_07d.png)'
- en: '**Step 8:** We will now invoke the PCA method from the `sklearn` library. The
    method is used to fit the data here. To be noted is, we have not yet determined
    the number of principal components we wish to use in this problem yet.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 8 步：** 现在，我们将从`sklearn`库中调用PCA方法。该方法用于在这里拟合数据。需要注意的是，我们尚未确定在这个问题中希望使用多少个主成分。'
- en: '[PRE9]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Step 9:** The principal components are now set. Let’s have a look at the
    variance explained by them. We can observe that the first component captures 72.77%
    variation, second captures 23.03% variation and so on.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 9 步：** 主成分现在已经被确定。让我们来看看它们解释的方差。我们可以观察到第一个成分解释了72.77%的变化，第二个解释了23.03%的变化，依此类推。'
- en: '[PRE10]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![03_07e](images/03_07e.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![03_07e](images/03_07e.png)'
- en: '**Step 10:** We are now plotting the components in a bar plot for better visualization.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 10 步：** 现在我们正在绘制一个条形图来展现这些成分以获得更好的可视化效果。'
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![03_07f](images/03_07f.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![03_07f](images/03_07f.png)'
- en: '**Step 11:** We are drawing a scree-plot to visualize the cumulative variance
    being explained by the principal components.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 11 步：** 我们绘制一个屏风图来可视化主成分解释的累积方差。'
- en: '[PRE12]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![03_07g](images/03_07g.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![03_07g](images/03_07g.png)'
- en: '**Step 12:** In this case study, if we choose the top two principal components
    as the final solutions as these two capture 95.08% of the total variance in the
    dataset.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 12 步：** 在这个案例研究中，如果我们选择前两个主成分作为最终解决方案，因为这两个成分捕获了数据集中95.08%的总方差。'
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Step 13:** We will now plot the dataset with respect to two principal components.
    For that, it is a requirement that Species are tied-back to the actual values
    of the Species variable which are Iris-setosa, Iris-versicolor and Iris-virginica.
    Here, 0 is mapped to Iris-setosa, 1 is Iris-versicolor and 2 is Iris-virginica.
    In the code below, first the Species variable gets its values replaced by using
    the mapping discussed above.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 13 步：** 现在，我们将根据两个主成分来绘制数据集。对于此操作，有一个要求，即物种必须被映射回物种变量的实际值，即山鸢尾、变色鸢尾和维吉尼亚鸢尾。在这里，0
    对应山鸢尾，1 对应变色鸢尾，2 对应维吉尼亚鸢尾。在下面的代码中，首先使用上述映射替换了物种变量的值。'
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 14:** We will now plot the results with respect to two principal components.
    The plot is showing the dataset reduced to two principal components we have just
    created. These principal components are able to capture 95.08% variance of the
    dataset. The first principal component represents the x-axis in the plot while
    the second principal component represents the y-axis in the plot. The color represents
    the various classes of Species.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**第14步：** 现在我们将对两个主成分绘制结果。这个图表显示了刚刚创建的数据集被减少到两个主成分。这些主成分能够捕捉95.08%的数据集方差。图中，第一主成分代表图表的x轴，而第二主成分代表图表的y轴。颜色代表了不同类别的物种。'
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![03_07h](images/03_07h.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![03_07h](images/03_07h.png)'
- en: The above solution has reduced the number of components from four to 2 and still
    is able to retain most of the information. Here, we have examined three approaches
    to select the principal components – based on Kaiser criteria, the variance captured,
    and the scree plot.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以上解决方案将成分数量从四减少到2，并且仍能保留大部分信息。在这里，我们已经检查了三种选择主成分的方法 - 基于凯撒标准、捕获的方差以及剪切图。
- en: Let us quickly analyze what we have achieved using PCA. Figure 3.7 shows two
    representations of the same dataset. The one on the left is the original dataset
    of X_variables. It has four variables and 150 rows. The right is the output of
    PCA. It has 150 rows but only two variables. Recall, we have reduced the number
    of dimensions from four to two. So the number of observations has remained as
    150 only while the number of variables has reduced from four to two.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速分析一下使用PCA我们所取得的成果。图3.7展示了同一数据集的两种表示形式。左边是X变量的原始数据集。它有四个变量和150行。右边是PCA的输出。它有150行，但只有两个变量。回顾一下，我们已经将维度从四减少到了两。因此观测数量仍然是150，而变量的数量已经从四个减少到两个。
- en: Figure 3.7 The figure on the left shows the original dataset which has 150 rows
    and 4 variables. After the implementation of PCA, the number of variables has
    been reduced to two. The number of rows remain the same as 150 which is shown
    by the length of pca_2d.
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7 左侧的图展示了原始数据集，它有150行和4个变量。在实施PCA后，变量的数量已被减少到了两个。行数仍然是150，这由pca_2d的长度表明。
- en: '![03_08](images/03_08.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![03_08](images/03_08.png)'
- en: Once we have reduced the number of components, we can continue to implement
    a supervised learning or an unsupervised learning solution. We can implement the
    above solution for any of the other real-world problems where we aim to reduce
    the number of dimensions. You will be exploring more on it in the case study section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们减少了成分的数量，我们可以继续实施监督学习或无监督学习的解决方案。我们可以将以上解决方案应用于其他真实世界问题中，这些问题中我们的目标是减少维度。您将在案例研究部分进一步探讨这个问题。
- en: With this we have covered PCA. The Github repo contains a very interesting PCA
    decomposition with variables and corresponding plot. We will now explore Singular
    Value Decomposition (SVD) in the next section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一部分，我们已经学习了PCA。Github仓库中包含了一个带有变量和相应图表的非常有趣的PCA分解。接下来我们将在下一节探讨奇异值分解（SVD）。
- en: 3.6 Singular Value Decomposition (SVD)
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 奇异值分解（SVD）
- en: In the last section we studied PCA. PCA transforms the data linearly and generate
    principal components which are not correlated with each other. But the process
    followed of eigenvalue decomposition can only be applied to *square matrices*.
    Whereas SVD can be implemented to any m x n matrix. We will study this in more
    detail now.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了主成分分析（PCA）。PCA线性地转换数据并生成彼此不相关的主成分。但是特征值分解的过程只能应用于*方阵*。而奇异值分解（SVD）可以应用于任何m
    x n矩阵。我们现在将更详细地学习这一点。
- en: Let us consider we have a matrix A. The shape of A is m x n or it contains m
    rows and n columns. The transpose of A can be represented as A^T.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑我们有一个矩阵A。A的形状是m x n，或者包含m行和n列。A的转置可以表示为A^T。
- en: We can create two other matrices using A and A^T as *A A*^T and *A*^T*A*. These
    resultant matrices *A A*^T and *A*^T*A* have some special properties which are
    listed below. The mathematical proof of the properties is beyond the scope of
    the book.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用A和A^T创建另外两个矩阵作为*A A*^T和*A*^T*A*。这些结果矩阵*A A*^T和*A*^T*A*具有一些特殊的性质，这些特性如下所列。这些性质的数学证明超出了本书的范围。
- en: 'The properties of *A A*^T and *A*^T*A* are:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*A A*^T 和 *A*^T*A* 的性质是：'
- en: They are symmetric and square matrices.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都是对称的方阵。
- en: Their eigenvalues are either positive or zero.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的特征值要么是正的，要么是零。
- en: Both A A^T and A^TA are having the same eigenvalue.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: A A^T 和 A^TA（带上▲）具有相同的特征值。
- en: Both A A^T and A^TA are having same rank as the original matrix A.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A A^T和A^TA的秩与原矩阵A相同。
- en: The eigenvectors of *A A*^T and *A*^T*A* are referred to as **singular vectors
    of A**. The square root of their eigenvalues are called **singular values**.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: A A^T和A^TA的特征向量分别被称为A的**奇异向量**。它们的特征值的平方根被称为**奇异值**。
- en: Since both of these matrices (A A^T and A^TA) are symmetrical, their eigenvectors
    are orthonormal to each other. In other words, being symmetrical - the eigenvectors
    are perpendicular to each other and can be of unit length.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个矩阵（A A^T和A^TA）都是对称的，它们的特征向量互相正交。换句话说，由于是对称的，特征向量是相互垂直的，并且可以具有单位长度。
- en: Now, with this mathematical understanding, we can define SVD. As per the Singular
    Value Decomposition method, it is possible to factorize any matrix A as
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了这种数学理解，我们可以定义SVD。根据奇异值分解方法，可以将任何矩阵A分解为
- en: (Equation 3.3)
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （公式3.3）
- en: A = U * S * V^T
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: A = U * S * V^T
- en: Here, A is the original matrix,
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，A是原始矩阵，
- en: U and V are the orthogonal matrices with orthonormal eigenvectors taken from
    A A^T and A^TA respectively and
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: U和V是正交矩阵，它们的正交特征向量来自于A A^T或A^TA，分别是
- en: S is the diagonal matrix with r elements equal to the singular values.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: S是对角矩阵，其r个元素等于奇异值。
- en: In simple terms, SVD can be said as an enhancement of the PCA methodology using
    eigenvalue decomposition.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地说，SVD可以被视为使用特征值分解对PCA方法进行增强。
- en: Singular values are better and numerically more robust than eigenvalues decomposition.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解比特征值分解更好，且在数值上更健壮。
- en: PCA was defined as the linear transformation of input variables using principal
    components. All those concepts of linear transformation, choosing the best components
    etc. remain the same. The major process steps remain similar, except in SVD we
    are using a slightly different approach wherein the eigenvalue decomposition has
    been replaced with using singular vectors and singular values. It is often advisable
    to use SVD when we have a sparse dataset, in the case of denser dataset PCA can
    be utilized.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: PCA被定义为使用主成分对输入变量进行线性转换。所有线性转换、选择最佳成分等概念都保持相同。主要的过程步骤保持相似，除了在SVD中，我们使用了稍微不同的方法，其中特征值分解被替换为使用奇异向量和奇异值。通常建议在数据集稀疏时使用SVD，在数据集较密集时使用PCA。
- en: '![](images/tgt.png) POP QUIZ – answer these questions to check your understanding..
    Answers at the end of the book'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '![](images/tgt.png) 小测验——回答这些问题以检查您的理解力。本书末尾将给出答案。'
- en: 1.   SVD works on eigenvalue decomposition technique. TRUE or FALSE.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 1.   SVD基于特征值分解技术。是真还是假？
- en: 2.   PCA is a much more robust methodology than SVD. TRUE or FALSE.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 2.   PCA比SVD更具鲁棒性。是真还是假？
- en: 3.   What are singular values and singular vectors in SVD?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 3.   SVD中奇异值和奇异向量是什么？
- en: We will now create a Python solution using SVD in the next section.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在下一节中，我们将使用SVD创建一个Python解决方案。
- en: 3.6.1 Python solution using SVD
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 使用SVD的Python解决方案。
- en: In this case study, we are using the *mushrooms* dataset. This dataset contains
    descriptions of 23 species of grilled mushrooms. There are two classes – either
    the mushroom is “e” which means it is edible else the mushroom is “p” meaning
    it is poisonous.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在此案例研究中，我们使用的是“蘑菇”数据集。该数据集包含23个烤蘑菇物种的描述。有两个类别——蘑菇是可食用的，“e”，否则蘑菇是有毒的，“p”。
- en: '**Step 1:** Import the libraries. We are importing'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1:** 导入库。我们正在导入'
- en: '[PRE16]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 2:** Import the dataset and check for shape, head etc.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：** 导入数据集并检查形状、头等。'
- en: '[PRE17]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![03_08a](images/03_08a.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![03_08a](images/03_08a.png)'
- en: '**Step 3:** As we can observe, the values are categorical in nature in the
    dataset. They have to be first encoded into numeric values. This is not the only
    approach we deal with categorical variables. There are other techniques too which
    we will explore throughout the book. We will dive deep into the techniques in
    the last chapter of the book.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：** 我们可以观察到，数据集中的值是分类的。它们必须首先被编码为数字值。这不是处理分类变量的唯一方法。本书的最后一章将探讨其他技术。我们将深入探索这些技术。'
- en: First, we are invoking the LabelEncoder and then apply it to all the columns
    in the dataset. The LabelEncoder converts the categorical variables into numeric
    ones using one-hot encoding method.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们调用LabelEncoder，然后将其应用到数据集中的所有列。LabelEncoder使用一种独热编码方法将分类变量转换为数字变量。
- en: '[PRE18]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 4:** Have a re-look at the dataset. All the categorical values have
    been converted to numeric ones.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4:** 重新查看数据集。所有分类值都已转换为数字值。'
- en: '[PRE19]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![03_08b](images/03_08b.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![03_08b](images/03_08b.png)'
- en: '**Step 5:** The next two steps are same as the last case study wherein we break
    the dataset into X_variables and y_label. And then the dataset is normalized.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**第5步：** 接下来的两步与上一个案例研究相同，在这两步中，我们将数据集分解为X变量和y标签。然后对数据集进行标准化。'
- en: '[PRE20]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Step 6:** In this step, we implement the SVD. There is a method in numpy
    that implements SVD. The output is u, s and v, where u and v are the singular
    vectors and s is the singular value. If you wish you can analyze their respective
    shapes and dimensions.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**第6步：** 在这一步中，我们实现了SVD。在numpy中有一个实现SVD的方法。输出是u、s和v，其中u和v是奇异向量，s是奇异值。如果您愿意，您可以分析它们各自的形状和维度。'
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Step 7:** We know that singular values allow us to compute variance explained
    by each of the singular vectors. We will now analyze the % variance explained
    by each singular vector and plot it. The results are shown to three places of
    decimal. And then we are plotting the results as a histogram plot. On the x-axis
    we have the singular vectors while on the y-axis we have the % of variance explained.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**第7步：** 我们知道奇异值允许我们计算每个奇异向量解释的方差。我们现在将分析每个奇异向量解释的%方差，并绘制出来。结果显示到小数点后三位。然后我们将结果绘制成直方图。在x轴上是奇异向量，而在y轴上是%解释的方差。'
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![03_08c](images/03_08c.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![03_08c](images/03_08c.png)'
- en: '**Step 8:** We will now create a data frame. This new data frame svd_df contains
    the first two singular vectors and the metadata. We are then printing the first
    5 rows using the head command.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**第8步：** 现在我们将创建一个数据框。这个新的数据框svd_df包含了前两个奇异向量和元数据。然后我们使用head命令打印前5行。'
- en: '[PRE23]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![03_08d](images/03_08d.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![03_08d](images/03_08d.png)'
- en: '**Step 9:** Similar to the last case study, we are replacing numeric values
    with actual class labels. 1 is edible while 0 is Poisonous.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**第9步：** 与上一案例类似，我们将数值替换为实际的类标签。1代表可食用，而0代表有毒。'
- en: '[PRE24]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Step 10:** We are now plotting the variance explained by the two components.
    Here we have chosen only the first two components. You are advised to take the
    optimum number of components using the methods described in the last section and
    plot the respective scatter plots. Here, on the x-axis we have shown the first
    singular vector SV1 and on the y-axis we have shown the second singular vector
    SV2.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**第10步：** 我们现在正在绘制两个组件解释的方差。这里我们只选择了前两个组件。建议您使用上一节描述的方法选择最佳的组件数量，并绘制相应的散点图。在这里，x轴上显示了第一个奇异向量SV1，而y轴上显示了第二个奇异向量SV2。'
- en: '[PRE25]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![03_08e](images/03_08e.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![03_08e](images/03_08e.png)'
- en: We can observe the distribution of the two classes with respect to the two components.
    The two classes – edible and poison are color coded as black and red respectively.
    As we have noted above, we have chosen only two components to show the impact
    using a visualization plot. You are advised to choose the optimum number of components
    using the methods described in the last case study and then visualize the results
    using different singular vectors. This solution can be used to reduce dimensions
    in a real-world dataset.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察两个类别相对于两个组件的分布情况。两个类别 - 可食用和有毒 - 被分别以黑色和红色编码。正如我们上面所指出的，我们只选择了两个组件来展示使用可视化图的影响。建议您使用上一个案例研究中描述的方法选择最佳的组件数量，然后使用不同的奇异向量来可视化结果。这个解决方案可以用来在实际数据集中减少维度。
- en: This concludes our discussion on SVD. We will now observe the positives and
    challenges with PCA and SVD in the next section.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对SVD的讨论。我们将在下一节中观察PCA和SVD的优势和挑战。
- en: 3.7 Pros and cons of dimensionality reduction
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 维度缩减的优缺点
- en: In the initial sections of the chapter, we discussed the drawbacks the of curse
    of dimensionality. In the last few sections, we discovered PCA and SVD and implemented
    using Python. In the current section, we will examine the advantages and challenges
    we have with these techniques.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头部分，我们讨论了维度诅咒的缺点。在最后几节中，我们了解了PCA和SVD，并使用Python进行了实现。在当前部分，我们将审查这些技术的优势和挑战。
- en: 'The major advantages we get with implementing PCA or SVD are as:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 实施PCA或SVD时我们获得的主要优势如下：
- en: '**Reduced number of dimensions** lead to less complexity in the dataset. The
    correlated features are removed and are transformed. Treating correlated variables
    manually is a tough task which is quite manual and frustrating. Techniques like
    PCA and SVD do that job for us quite easily. The number of correlated features
    is minimized, and overall dimensions are reduced.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降低维度**会减少数据集的复杂性。相关特征被移除并进行了转换。手动处理相关变量是一项相当手动和令人沮丧的工作。PCA和SVD等技术可以很容易地帮助我们完成这项工作。相关特征的数量被最小化，并且整体维度被降低。'
- en: '**Visualization of the dataset** is better if the number of dimensions is lesser.
    It is very difficult to visualize and depict a very-high dimensional dataset.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果维度较少，数据集的**可视化**效果会更好。极高维度的数据集很难进行可视化。
- en: '**Accuracy** of the machine learning model is improved if the correlated variables
    are removed. These variables do not add anything to the performance of the model.'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果移除相关变量，机器学习模型的**准确性**会得到改善。这些变量不会对模型的性能有所贡献。
- en: '**The training time is reduced** as the dataset is less complex. Hence, lesser
    computation power is required, and lesser time is required.'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间减少**因为数据集的复杂性较低。因此，需要较少的计算力和时间。'
- en: '**Overfitting** is a nuisance in supervised machine learning models. It is
    a condition where the model is behaving very well on the training dataset but
    not so well on the testing/validation dataset. It means that the model may not
    be able to perform well on real-world unseen datasets. And it beats the entire
    purpose of building the machine learning model. PCA/SVD helps in tackling overfitting
    by reducing the number of variables.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在监督机器学习模型中，**过拟合**是一个讨厌的问题。这是一种情况，模型在训练数据集上表现良好，但在测试/验证数据集上却表现不佳。这意味着模型可能无法在真实世界的未知数据集上表现良好。而这违背了构建机器学习模型的整个目的。PCA/SVD通过减少变量数量来帮助解决过拟合问题。
- en: 'At the same time, there are a few challenges we face with dimensionality reduction
    techniques which are as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们在降维技术中面临一些挑战，如下所述：
- en: The new components created by PCA/SVD are **less interpretable**. They are the
    combination of the independent variables in the dataset and do not actually relate
    to the real-world, hence it can be difficult to relate them to the real-world
    scenario.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA/SVD创建的新组件**不太可解释**。它们是数据集中独立变量的组合，实际上与真实世界没有关系，因此很难将它们与真实世界的情况联系起来。
- en: '**Numeric variables** are required for PCA/SVD. And hence all the categorical
    variables have to be represented in numeric form.'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA/SVD需要**数值变量**。因此，所有分类变量必须以数值形式表示。
- en: '**Normalization/standardization** of the dataset is required to be done before
    the solution can be implemented.'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实施解决方案之前，需要对数据集进行**归一化/标准化**。
- en: There might be **information loss** when we use PCA or SVD. The principal components
    *cannot* replace the original dataset and hence there might be some loss of information
    while we implement these methods.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用PCA或SVD时可能会发生**信息损失**。主成分*无法*取代原始数据集，因此在实施这些方法时可能会有一些信息损失。
- en: But despite a few challenges, PCA and SVD are used for reducing the dimensions
    in the dataset. They are one of the most popular methods and quite heavily used.
    At the same time, it is imperative to note that these are linear methods and we
    will be covering non-linear methods in the later part of the book.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 但尽管存在一些挑战，PCA和SVD用于降低数据集的维度。它们是最受欢迎的方法之一，也被广泛使用。与此同时，必须注意的是，这些都是线性方法，我们将在本书的后期部分介绍非线性方法。
- en: With this we have covered the two important techniques used in dimensionality
    reduction. We will examine more advanced techniques in the later chapters. It
    is time to move to the case study which is the next section of the chapter.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经涵盖了在降维中使用的两种重要技术。我们将在后面的章节中探讨更高级的技术。是时候转到案例研究了，这是本章的下一部分内容。
- en: 3.8 Case study for dimension reduction
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.8 降维案例研究
- en: We will now explore a real-world case to relate the usage of PCA and SVD in
    real-world business scenarios.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探索一个真实案例，以了解PCA和SVD在真实商业场景中的使用。
- en: 'Consider this: You are working for a telecommunication service provider. You
    have a subscriber base, and you wish to cluster the consumers over a number of
    parameters. But the challenge can be the huge number of dimensions available to
    be analyzed.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下：你正在为一个电信服务提供商工作。你有一个订户基础，并希望根据许多参数对消费者进行聚类。但挑战在于需要分析的庞大维度。
- en: The objective will be to reduce the number of attributes using dimension reduction
    algorithms. The consumer dataset can look like below.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是使用降维算法减少属性的数量。消费者数据集可以如下所示。
- en: Demographic details of the subscriber will consist of age, gender, occupation,
    household size, marital status etc. The list shown below is not exhaustive.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 订户的人口统计详情将包括年龄、性别、职业、家庭规模、婚姻状况等。下面显示的列表并不全面。
- en: Table 3.4 Demographic details of a subscriber like age, gender, marital status,
    household size, City etc.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3.4 订户的人口统计详情，如年龄、性别、婚姻状况、家庭规模、城市等。
- en: '![03_T08](images/03_T08.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![03_T08](images/03_T08.png)'
- en: Subscription details of the consumer might look like the table below. The list
    shown below is not exhaustive.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消费者的订阅详情可能如下表所示。下面显示的列表并不全面。
- en: Table 3.5 Subscription details of a subscriber like tenure, postpaid/prepaid
    connection etc.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3.5 订户的订阅详情，如服务期限、预付费/后付费连接等。
- en: '![03_T09](images/03_T09.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![03_T09](images/03_T09.png)'
- en: The usage of the consumer will describe the minutes, call rates, data usages,
    services etc. The list shown below is not exhaustive.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消费者的使用情况将描述分钟数、通话费率、数据使用情况、服务等。下面显示的列表并不全面。
- en: Table 3.6 Usage of a subscriber specifies the number of minutes used, SMS sent,
    data used, days spent in a network, national or international usage etc.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3.6 订户的使用情况指定了使用的分钟数、发送的短信、使用的数据、在网络中度过的天数、国内或国际使用情况等。
- en: '![03_T10](images/03_T10.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![03_T10](images/03_T10.png)'
- en: Payment and transaction details of the subscribers will talk about the various
    transactions made, the mode of payment, frequency of payments, days since last
    payment made etc.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 订户的付款和交易详情将涉及到所做的各种交易、付款方式、付款频率、自上次付款以来的天数等。
- en: Table 3.7 Transaction details of a subscriber showing all the details of amount,
    mode etc.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3.7 展示了订户的交易详情，包括金额、模式等的所有详情。
- en: '![03_T11](images/03_T11.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![03_T11](images/03_T11.png)'
- en: The dataset can have many more attributes. So far, we have established that
    the number of variables involved are indeed high. Once we join all these datapoints,
    the number of dimensions in the final data can be very huge.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可能有更多属性。到目前为止，我们已经确定所涉及的变量数量确实很高。一旦我们汇总了所有这些数据点，最终数据的维数就可能非常庞大。
- en: Table 3.8 The final dataset is a combination of all the above-mentioned datasets.
    It will be a big, really high-dimensional dataset which is to be analyzed
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3.8 最终数据集是所有上述数据集的组合。这将是一个庞大的、真正高维的数据集，需要进行分析。
- en: '![03_T12](images/03_T12.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![03_T12](images/03_T12.png)'
- en: We have to reduce the number of attributes before we can proceed to any supervised
    or unsupervised solution. In this chapter, we are focusing on dimensionality reduction
    techniques and hence the steps are covering that aspect of the process. In the
    later chapters, we will examine the exploratory analysis in more detail.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够进行任何监督或无监督解决方案之前，我们必须减少属性的数量。在本章中，我们专注于降维技术，因此步骤涵盖了该过程的这一方面。在后面的章节中，我们将更详细地研究探索性分析。
- en: As a first step, we will perform a sanity check of the dataset and do the data
    cleaning. We will examine the number of data points, number of missing values,
    duplicates, junk values present etc. This will allow us to delete any variables
    which might be very sparse or contain not much information. For example, if the
    gender is available for only 0.01% of the customer base it might be a good idea
    to drop the variable. Or if all the customers have gender as male, the variable
    is not adding any new information to us and hence it can be discarded. Sometimes,
    using business logic a variable might be dropped from the dataset. An example
    has been discussed in the earlier sections. In this step, we might combine a few
    variables. For example, we might create a new variable as average transaction
    value by dividing total amount spent by total number of transactions. In this
    way, we will be able to reduce a few dimensions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将对数据集进行健全性检查并进行数据清理。我们将检查数据点的数量，缺失值的数量，重复值，存在的垃圾值等。这将使我们能够删除可能非常稀疏或包含不多信息的任何变量。例如，如果性别仅适用于客户基数的0.01％，则去除该变量可能是个不错的主意。或者如果所有客户的性别都为男性，该变量对我们没有添加任何新信息，因此可以丢弃。有时，使用业务逻辑，可能会从数据集中删除一个变量。在前面的部分中已经讨论了一个例子。在这一步骤中，我们可能会组合一些变量。例如，我们可能通过将总支出金额除以总交易数来创建一个新变量作为平均交易值。以这种方式，我们将能够减少一些维度。
- en: A Python Jupyter notebook is available at the Github repository, wherein we
    have given a very detailed solution for data cleaning step.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Python Jupyter笔记本可在Github存储库中找到，我们在那里提供了对数据清理步骤的非常详细的解决方案。
- en: Once we are done with the basic cleaning of the data, we start with the exploratory
    data analysis. As a part of exploratory analysis, we examine the spread of the
    variable, its distribution, mean/median/mode of numeric variables. This is sometimes
    referred to as *univariate analysis.* This step allows us to measure the spread
    of the variables, understand the central tendencies, examine the distribution
    of different classes for categorical variables and look for any anomalies in the
    values. For example, using the dataset mentioned above we will be interested to
    analyze the maximum/minimum/average data usage or the % distribution of gender
    or age. We would want to know the most popular method to make a transaction and
    we would also be interested to know maximum/minimum/average amount of the transactions.
    And this list goes on.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 基本数据清洗完成后，我们开始进行探索性数据分析。作为探索性分析的一部分，我们检查变量的分布、其分布情况，数值变量的平均数/中位数/众数。这有时被称为*单变量分析*。这一步骤允许我们测量变量的分散程度，了解中心趋势，检查分类变量的不同类别的分布，并查找值中的任何异常情况。例如，使用上述提到的数据集，我们将有兴趣分析数据使用的最高/最低/平均值，或者性别或年龄的％分布。我们想知道最受欢迎的交易方式，我们也对交易的最高/最低/平均金额感兴趣。等等，这个列表还可以继续下去。
- en: Then we explore the relationships between variables which is referred to as
    *bivariate analysis*. Crosstabs, distribution of data is a part of bivariate analysis.
    A correlation matrix is created during this step. Variables that are highly correlated
    are examined thoroughly. And based on business logic, one of them might be dropped.
    This step is useful to visualize and understand the behavior of one variable in
    the presence of other variables. We can examine their mutual relationships and
    the respective strength of the relationships. In this case study, we would answer
    the questions such as– do subscribers who use more data spend more time on network
    as compared to subscribers who send more SMS. Or hypothesis like – do the subscribers
    who make a transaction using online mode generate more revenue than the ones using
    cash. Or is there a relationship between gender/age with the data usage. Many
    such questions are answered during this phase of the project.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们探索变量之间的关系，这被称为*双变量分析*。交叉表，数据分布是双变量分析的一部分。在这一步骤, 创建了一个相关矩阵。需要仔细研究高度相关的变量。根据业务逻辑,
    其中一个变量可能会被丢弃。这一步骤有助于可视化和理解一个变量在其他变量存在的情况下的行为。我们可以检查它们的相互关系和关系的强度。在本案例研究中, 我们将回答诸如
    - 对比使用更多数据的订阅者与发送更多短信的订阅者, 是否在网络上花费更多时间。或假设 - 使用在线模式进行交易的订阅者是否比使用现金进行交易的订阅者产生更多收入。或者性别/年龄与数据使用之间是否存在关系。在项目的这个阶段,
    将回答许多这样的问题。
- en: A Python Jupyter notebook is available at the Github repository, which provides
    detailed steps and code for the univariate and bivariate phase. Check it out!
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Github存储库中提供了Python Jupyter笔记本，提供了单变量和双变量阶段的详细步骤和代码。来看一下吧！
- en: At this step, we have a dataset which has a huge number of dimensions and we
    want to reduce the number of dimensions. Now it is good time to implement PCA
    or SVD. The techniques will reduce the number of dimensions and will make the
    dataset ready for the next steps in the process, as shown in Figure 3.8\. The
    figure is only representative in nature to depict the impact of dimensionality
    reduction methods. Notice how the large number of black lines in the left figure
    are getting reduced to lesser number of red lines in the right figure.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，我们拥有一个具有大量维度的数据集，并希望减少维数。现在是实施PCA或SVD的好时机。这些技术将减少维数，并使数据集准备好在过程的下一步中使用，如图3.8所示。这张图仅是一个代表性的结构，用于描述维数约简方法的影响。请注意，左图中的大量黑线正在缩减为右图中较少的红线。
- en: Figure 3.8 A very high dimensional dataset will be reduced to a low dimensional
    one by using principal components which capture the maximum variance in the dataset
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8 非常高维的数据集将通过捕获数据集中的最大方差来减少到低维度
- en: '![03_09](images/03_09.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![03_09](images/03_09.png)'
- en: The output of dimensionality reduction methods will be a dataset with a lower
    number of variables. The dataset can be then used for supervised or unsupervised
    learning. We have already looked at the examples using Python in the earlier sections
    of the chapter.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 降维方法的输出将是一个少于原始变量数量的数据集。该数据集可以用于监督学习或无监督学习。我们已经在本章前面的章节中介绍了使用Python的例子。
- en: This concludes our case study on telecom subscribers. The case can be extended
    to any other domain like retail, BFSI, aviation, healthcare, manufacturing, etc.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们关于电信用户的案例研究。这种情况可以扩展到其他领域，如零售、银行金融、航空、医疗保健、制造等。
- en: We will now proceed to the summary of the chapter.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将进入本章的总结。
- en: 3.9 Closing Thoughts
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.9 总结
- en: Data is everywhere, in various forms, levels, dimensions, and with varying levels
    of complexity. It is often mentioned that “the more data the better. It is indeed
    true to a certain extent. But with a really high number of dimensions, it becomes
    quite a herculean task to make sense out of it. The analysis can become biased
    and really complex to deal with. We explored this curse of dimensionality in this
    third chapter. PCA/SVD are helpful to reduce this complexity. They make the dataset
    ready for the next steps.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在，以各种形式、层次、维度呈现，具有不同的复杂程度。人们经常提到“数据越多越好”。在一定程度上这确实是正确的。但是，如果维度非常高，从中获取有用信息将会非常困难。数据分析可能会出现偏差，并且处理起来非常复杂。我们在本章探讨了维数灾难。PCA
    / SVD可以帮助减少这种复杂性。他们使数据集准备好下一步。
- en: But dimensionality reduction is not as straightforward as it looks. It is not
    an easy task. But it is certainly a very rewarding one. And requires a combination
    of business acumen, logic, and common sense to deal with. The resultant dataset
    might still require some additional work. But it is a very good point for building
    a machine learning model.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 但是维数约简并不像看起来那么简单。这不是一个容易完成的任务。但它确实是一个非常有回报的任务。并需要商业眼光、逻辑和常识的结合来处理。结果数据集可能仍需要一些额外的工作。但这是建立机器学习模型的一个很好的起点。
- en: This marks the end of the third chapter. It also ends the part one of the book.
    In this part, we have covered the more basic algorithms. We started with the first
    chapter of the book, where we explored the fundamentals and basics of machine
    learning. In the second chapter we examined three algorithms for clustering. In
    this third chapter, we explored PCA and SVD.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着第三章的结束。同时也结束了本书的第一部分。在本部分中，我们介绍了更基础的算法。我们从本书的第一章开始探究机器学习的基础和基本知识。在第二章中，我们探讨了三个聚类算法。在第三章中，我们探索了PCA和SVD。
- en: In the second part of the book, we are changing gears and studying more advanced
    topics. We are starting with association rules in the next chapter. Then we go
    into advanced clustering methods of time-series clustering, fuzzy clustering,
    GMM clustering, etc. It is followed by a chapter on advanced dimensionality reduction
    algorithms like t-SNE, LDA. And then to conclude the second part, we are examining
    unsupervised learning on text datasets. The third part of the book is even more
    advanced where we dive into neural network-based solutions and use image datasets.
    So still a long way to go! Stay tuned!
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在书的第二部分，我们正在转向并学习更先进的话题。我们将在下一章开始研究关联规则。然后我们进入高级聚类方法，比如时间序列聚类、模糊聚类、GMM聚类等等。接着是一章关于高级的降维算法，比如t-SNE、LDA。然后，在第二部分的结束，我们将研究无监督学习在文本数据集上的应用。书的第三部分更加先进，我们将深入基于神经网络的解决方案并使用图像数据集。所以还有很长的路要走！敬请关注！
- en: You can proceed to the question section now!
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以进入问题部分了！
- en: Practical next steps and suggested readings
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际的下一步和建议的阅读
- en: Use the vehicles dataset used in the last chapter for clustering and implement
    PCA and SVD on it. Compare the performance on clustering before and after implementing
    PCA and SVD.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一章节使用的车辆数据集进行聚类，并在其上实施PCA和SVD。比较在实施PCA和SVD之前和之后进行聚类的性能。
- en: Get the datasets from the ([https://data.world/datasets/pca](datasets.html)).
    Here, you will find a lot of datasets like Federal plan cyber security, Pizza
    dataset etc. Compare the performance of PCA and SVD on these datasets.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 ([https://data.world/datasets/pca](datasets.html)) 获取数据集。在这里，你会发现许多数据集，比如联邦计划网络安全、比萨数据集等等。比较在这些数据集上实施PCA和SVD的性能。
- en: Go through the following papers on PCA
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读下面关于主成分分析（PCA）的论文
- en: '[https://www.sciencedirect.com/science/article/pii/009830049390090R](pii.html)'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.sciencedirect.com/science/article/pii/009830049390090R](pii.html)'
- en: '[https://web.stanford.edu/~hastie/Papers/spc_jcgs.pdf](Papers.html)'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://web.stanford.edu/~hastie/Papers/spc_jcgs.pdf](Papers.html)'
- en: '[https://web.cs.ucdavis.edu/~vemuri/papers/pcaVisualization.pdf](papers.html)'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://web.cs.ucdavis.edu/~vemuri/papers/pcaVisualization.pdf](papers.html)'
- en: '[https://cseweb.ucsd.edu/~ravir/papers/pca/pamifinal.pdf](pca.html)'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://cseweb.ucsd.edu/~ravir/papers/pca/pamifinal.pdf](pca.html)'
- en: Go through the following research papers on SVD
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读一下关于奇异值分解的研究论文
- en: '[https://people.maths.ox.ac.uk/porterm/papers/s4.pdf](papers.html)'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://people.maths.ox.ac.uk/porterm/papers/s4.pdf](papers.html)'
- en: '[https://papers.nips.cc/paper/3473-quic-svd-fast-svd-using-cosine-trees.pdf](paper.html)'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://papers.nips.cc/paper/3473-quic-svd-fast-svd-using-cosine-trees.pdf](paper.html)'
- en: '[https://arxiv.org/pdf/1211.7102.pdf](pdf.html)'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1211.7102.pdf](pdf.html)'
- en: '[http://glaros.dtc.umn.edu/gkhome/fetch/papers/sarwar_SVD.pdf](papers.html)'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://glaros.dtc.umn.edu/gkhome/fetch/papers/sarwar_SVD.pdf](papers.html)'
- en: 3.10 Summary
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.10 总结
- en: We understood that having a lot of dimensions in a data set gives rise to a
    problem known as the Curse of dimensionality. Because of the curse of dimensionality,
    the dataset becomes very complex to process and hence the processing time also
    increases a lot.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们了解到，在数据集中拥有很多维度会引发一个问题，即维度灾难。由于维度灾难的存在，数据集变得非常复杂，处理起来也变得更加耗时。
- en: We also mentioned that there can be a number of techniques to tackle the problem
    of curse of dimensionality like PCA, LDA, SVD, Autoencoders, t-SNE, Isomaps etc.
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还提到了可以有多种技术来解决维度灾难的问题，比如PCA、LDA、SVD、自编码器、t-SNE、等等。
- en: We covered Principal Component Analysis (PCA) in detail where we studied that
    a principal component is a linear combination of various variables. Using this
    methodology, the total number of dimensions are reduced by having principal components
    which capture the maximum variance in the dataset.
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们详细介绍了主成分分析（PCA），在其中我们学到，一个主成分是各种变量的线性组合。使用这种方法，通过具有捕捉数据集中最大方差的主成分，可以减少总维数。
- en: We then moved to Singular Value Decomposition where most of the process is the
    same as PCA except the eigenvalue decomposition in PCA has been replaced with
    using singular vectors and singular values in SVD.
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们转向奇异值分解，在这一部分，大部分过程与PCA相同，只是PCA中的特征值分解被奇异向量和奇异值在SVD中替换了。
- en: We also studied that when we get the principal components, though we solve the
    problem of the curse of dimensionality, the original variables are lost.
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还学到，当得到主成分后，虽然解决了维度灾难的问题，但原始变量却丢失了。
- en: Finally, we had the Python implementation of the techniques by using sklearn
    library.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用sklearn库对这些技术进行了Python实现。
