- en: 13 Combining Gaussian processes with neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13将高斯过程与神经网络结合
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The difficulty of processing complex, structured data with common covariance
    functions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用常见协方差函数处理复杂结构化数据的困难
- en: Using neural networks to handle complex, structured data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络处理复杂结构化数据
- en: Combining neural networks with GPs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经网络与GP结合
- en: In chapter 2, we learned that the mean and covariance functions of a Gaussian
    process (GP) act as prior information that we’d like to incorporate into the model
    when making predictions. For this reason, the choice for these functions greatly
    affects how the trained GP behaves. Consequently, if the mean and covariance functions
    are misspecified or inappropriate for the task at hand, the resulting predictions
    won’t be useful.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，我们学到了高斯过程（GP）的均值和协方差函数作为我们希望在模型中融入的先验信息，当进行预测时。因此，这些函数的选择极大地影响了训练后的GP的行为。因此，如果均值和协方差函数被错误地指定或不适用于手头的任务，那么得到的预测就不会有用。
- en: As an example, remember that a *covariance function*, or *kernel*, expresses
    the correlation—that is, similarity—between two points. The more similar the two
    points are, the more likely they are to have similar values for the labels we’re
    trying to predict. In our housing price prediction example, similar houses are
    likely to go for similar prices.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，记住*covariance function*，或*kernel*，表示两个点之间的相关性——即相似性。两个点越相似，它们的标签值可能越相似，我们试图预测的标签值。在我们的房价预测示例中，相似的房子可能会有类似的价格。
- en: How does a kernel exactly compute the similarity between any two given houses?
    Let’s consider two cases. In the first, a kernel only considers the color of the
    front door and outputs 1 for any two houses of the same door color and 0 otherwise.
    In other words, this kernel thinks two houses are similar if and only if they
    have the same color for their front doors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 核到底如何计算两个给定房子之间的相似性？我们考虑两种情况。在第一种情况中，核函数仅考虑前门的颜色，并对于任何具有相同门颜色的两个房子输出1，否则输出0。换句话说，如果两个房子的前门颜色相同，这个核函数认为它们相似。
- en: This kernel, as illustrated by figure 13.1, is a bad choice for a housing price
    prediction model. The kernel thinks that the house on the left and the one in
    the middle should have similar prices, while the house on the right and the one
    in the middle should have different prices. This is not appropriate, as the left
    house is much bigger than the other two, which are of similar size. The misprediction
    happens because the kernel is wrong about which characteristic of a house is a
    good predictive feature for how much the house costs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图13.1所示，这个核函数对于房价预测模型来说是一个糟糕的选择。核函数认为左边的房子和中间的房子应该有相似的价格，而右边的房子和中间的房子应该有不同的价格。这是不合适的，因为左边的房子比其他两个大得多，而其他两个房子的大小相似。误差发生的原因是核函数错误地判断了房子的哪个特征是房子成本的良好预测特征。
- en: '![](../../OEBPS/Images/13-01.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-01.png)'
- en: Figure 13.1 The covariance between houses computed by an inappropriate kernel.
    Because it looks only at the color of the front door, this kernel doesn’t produce
    appropriate covariances.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 由不合适的核函数计算的房屋之间的协方差。因为它只关注前门的颜色，所以这个核函数无法产生合适的协方差。
- en: The other kernel is more sophisticated and accounts for relevant factors, such
    as location and living area. This kernel is more appropriate as it can describe
    the similarity between the prices of two houses more reasonably. Having the appropriate
    kernel—that is, the correct measure of similarity—is paramount for GPs. If the
    kernel can correctly describe how similar or different a given pair of data points
    are, the GP using the covariance will be able to produce well-calibrated predictions.
    Otherwise, the predictions will be of low quality.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个核函数更加复杂，并考虑了相关因素，比如位置和居住面积。这个核函数更加合适，因为它可以更合理地描述两个房子之间的价格相似性。拥有合适的核函数——即正确的相似度度量——对于GP来说至关重要。如果核函数能够正确地描述给定数据点之间的相似性或差异性，那么使用协方差的GP将能够产生良好校准的预测。否则，预测将具有较低的质量。
- en: 'You might think a kernel for houses that only considers door color is inappropriate,
    and no reasonable kernels in ML would behave this way. However, as we show in
    this chapter, some common kernels we have used thus far (e.g., RBF and Matérn)
    fall apart in the same way when processing structured input data, such as images.
    Specifically, they fail to adequately describe the similarity between two images,
    which poses a challenge for training GPs on these structured data types. The approach
    we take is to use neural networks. Neural networks are flexible models that can
    approximate any function well, given enough data. We learn to use a neural network
    to transform input data that a GP’s kernel isn’t able to process well. In doing
    this, we get the best of both worlds: flexible modeling with a neural network
    *and* uncertainty-calibrated predictions from a GP.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为一个只考虑门颜色的房屋核函数是不合适的，并且在ML中没有合理的核函数会表现出这种行为。然而，正如我们在本章中所展示的，到目前为止我们使用的一些常见核函数（例如，RBF和Matérn）在处理结构化输入数据（例如图像）时会出现相同的问题。具体来说，它们未能充分描述两个图像之间的相似性，这给在这些结构化数据类型上训练GPs带来了挑战。我们采取的方法是使用神经网络。神经网络是灵活的模型，可以在有足够数据的情况下很好地逼近任何函数。我们学会使用神经网络来转换GP的核函数无法很好地处理的输入数据。通过这样做，我们既得到了神经网络的灵活建模，*又*从GP中获得了不确定性校准的预测。
- en: In this chapter, we show that our usual RBF kernel doesn’t capture the structure
    of a common dataset well, resulting in bad predictions from the GP. We then combine
    a neural network model with this GP and see that the new kernel can successfully
    reason about similarity. By the end of the chapter, we obtain a framework to help
    a GP handle structured data types and improve predictive performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了我们通常的RBF核函数不能很好地捕捉常见数据集的结构，从而导致GP的预测不佳。然后我们将一个神经网络模型与此GP结合起来，看到新的核函数可以成功地推理相似性。到本章结束时，我们获得了一个框架，帮助GP处理结构化数据类型并提高预测性能。
- en: 13.1 Data that contains structures
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 包含结构的数据
- en: 'In this section, we explain what exactly we mean by *structured data*. Unlike
    the type of data we have been using to train our GPs in previous chapters, where
    each feature (column) in the dataset could take on values within a continuous
    range, in many applications, data has more complexity. Take the following, for
    instance:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们解释了*结构化数据*的确切含义。与我们在之前章节中用来训练GPs的数据类型不同，在那些数据类型中，数据集中的每个特征（列）可以取得连续范围内的值，而在许多应用中，数据具有更复杂性。比如说：
- en: The number of stories in a house can only be a positive integer.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 房子的楼层数只能是正整数。
- en: In computer vision tasks, the pixel value in an image is an integer between
    0 and 255.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算机视觉任务中，图像中的像素值是0到255之间的整数。
- en: In molecular ML, a molecule is often represented as a graph.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分子ML中，分子通常被表示为图形。
- en: 'That is, there are *structures* embedded in the data points in these applications,
    or requirements that the data points need to follow: no house can have a negative
    number of stories; a pixel cannot take a fraction as its value; a graph denoting
    a molecule will have nodes and edges representing chemicals and bindings. We call
    these kinds of data *structured data*. Throughout this chapter, we use the popular
    MNIST handwritten digit dataset (see [https://huggingface.co/datasets/mnist](https://huggingface.co/datasets/mnist))
    as a case study for our discussions.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是，这些应用中的数据点中嵌入了*结构*，或者需要数据点遵循的要求：没有房子可以有负数的楼层；像素不能以分数作为其值；表示分子的图形将具有表示化学物质和结合物的节点和边缘。我们称这些类型的数据为*结构化数据*。在本章中，我们将使用流行的MNIST手写数字数据集（见[https://huggingface.co/datasets/mnist](https://huggingface.co/datasets/mnist)）作为我们讨论的案例研究。
- en: Definition The Modified National Institute of Standards and Technology (MNIST))
    dataset contains images of handwritten digits. Each image is a 28-by-28 matrix
    of integers between 0 and 255.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 修改后的美国国家标准与技术研究所（MNIST）数据集包含手写数字的图像。每个图像是一个28×28的整数矩阵，取值范围在0到255之间。
- en: An example data point from this set is shown in figure 13.2, where a pixel is
    shown with a shade corresponding to its value; 0 corresponds to a white pixel,
    and 255 corresponds to a dark pixel. We see that this data point is an image of
    the number five.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集中的一个示例数据点如图13.2所示，其中像素的阴影对应于其值；0对应于白色像素，255对应于黑色像素。我们看到这个数据点是数字五的图像。
- en: '![](../../OEBPS/Images/13-02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-02.png)'
- en: Figure 13.2 A data point from the MNIST dataset, which is an image with 28 rows
    and 28 columns of pixels, represented as a PyTorch tensor
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 来自MNIST数据集的数据点，这是一个由28行和28列像素组成的图像，表示为一个PyTorch张量
- en: Note While this handwritten digit recognition task is technically a classification
    problem, we use it to simulate a regression problem (which is the type of problem
    we aim to solve in BayesOpt). Since each label is a number (a digit), we pretend
    these labels exist in a continuous range and directly use them as our prediction
    target.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 虽然这个手写数字识别任务在技术上是一个分类问题，但我们使用它来模拟一个回归问题（这是我们在BayesOpt中要解决的问题类型）。由于每个标签都是一个数字（一个数字），我们假装这些标签存在于一个连续的范围内，并直接将它们用作我们的预测目标。
- en: Our task is to train a GP on a dataset of image labels (each label here is the
    value of the digit written in the corresponding image) and then use it to predict
    on a test set. This distinction is illustrated in figure 13.3, which shows that
    unlike classification, where we choose one of the classes as a prediction for
    each data point, each prediction here is a number inside a continuous range in
    a regression task.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的任务是在一个图像标签数据集上训练一个GP（每个标签都是对应图像中写的数字的值），然后在一个测试集上进行预测。这个区别在图13.3中有所体现，它显示与分类不同，在分类中，我们选择一个类作为每个数据点的预测，而在回归任务中，这里的每个预测是一个连续范围内的数字。 '
- en: '![](../../OEBPS/Images/13-03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-03.png)'
- en: Figure 13.3 Classification vs. regression in the context of the MNIST data.
    Each prediction is a classification task that corresponds to one of the classes;
    each prediction in regression is a number inside a continuous range.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 在MNIST数据的上下文中的分类与回归。每个预测是一个分类任务，对应于其中的一个类别；在回归中的每个预测是一个连续范围内的数字。
- en: 'There are many real-world applications that follow this form of regression
    problem on structured data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多现实世界的应用遵循这种结构化数据的回归问题的形式：
- en: In product recommendation, we want to predict the probability that someone will
    click on a customized ad. The ads, which are images one can customize, are the
    structured data, and the click probability is the prediction target. This probability
    can be any number between 0 and 1.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在产品推荐中，我们希望预测某人点击自定义广告的概率。广告是可以自定义的图片，是结构化数据，点击概率是预测目标。这个概率可以是0到1之间的任何数字。
- en: In materials science, a scientist may want to predict the energy level of a
    molecular composition when that composition is synthesized in a laboratory. Each
    molecular composition can be represented as a graph of a specific structure with
    nodes and edges, and the energy level can be any number between the theoretical
    minimum and maximum energy levels that a composition can exhibit.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在材料科学中，科学家可能希望在实验室中合成某种分子组合时预测其能量水平。每种分子组合都可以表示为具有节点和边的特定结构的图，并且能量水平可以是理论最小和最大能量水平之间的任何数字，一个组合可能表现出的。
- en: In drug discovery, we want to predict the effectiveness of a drug that could
    be produced. As illustrated in figure 13.4, each drug corresponds to a chemical
    compound, which can also be represented as a graph. Its effectiveness can be a
    real number on a scale (say from 0 to 10).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在药物发现中，我们希望预测可能产生的药物的有效性。如图13.4所示，每种药物对应于一种化合物，它也可以表示为一个图。其有效性可以是一个实数，介于某个范围内（比如从0到10）。
- en: '![](../../OEBPS/Images/13-04.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-04.png)'
- en: Figure 13.4 Drug discovery as an example of a structured regression problem.
    Each compound is represented as a structured graph, and we aim to predict the
    effectiveness of the compound in treating some disease on a scale from 0 to 10.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 药物发现作为一个结构化回归问题的例子。每个化合物都表示为一个结构化图，并且我们的目标是预测这种化合物在治疗某种疾病方面的有效性，其范围从0到10。
- en: In all of these applications, the input data we’d like to perform a prediction
    on is structured, and our prediction target is a real-valued number. In short,
    they are regression problems on structured data. Using the MNIST dataset, we simulate
    one such problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些应用中，我们想要进行预测的输入数据是结构化的，我们的预测目标是一个实数值。简而言之，它们是结构化数据的回归问题。使用MNIST数据集，我们模拟了这样一个问题。
- en: 13.2 Capturing similarity within structured data
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 捕捉结构化数据内的相似性
- en: 'In this section, we explore how common kernels, such as the radial basis function
    (RBF) kernel (see section 2.4), fail to describe similarity within structured
    data. The output of the kernel for any two inputs *x*[1] and *x*[2], which quantifies
    the covariance of the two inputs, is defined as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨常见内核（如径向基函数内核）如何无法描述结构化数据中的相似性。量化两个输入的协方差的内核输出，*x*[1] 和 *x*[2] 的输出定义如下：
- en: '![](../../OEBPS/Images/13-04-Equations_ch-13.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-04-Equations_ch-13.png)'
- en: This output, which is the covariance between the two variables, is the exponential
    of the negated difference in values between the two inputs divided by a scale.
    The output is always between 0 and 1, and the bigger the difference in values,
    the smaller the output.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出是两个变量之间的协方差，是两个输入之间差异的负指数除以一个标度。输出始终在 0 和 1 之间，而且越大的差异，输出越小。
- en: This makes sense in many cases since if the two inputs have similar values,
    and hence a small difference, their covariance will be high, and if they have
    different values, the covariance will be low. Two houses with roughly equal living
    area are likely to have similar prices—that is, their prices have a high covariance;
    the prices of a very large house and very small one, on the other hand, are likely
    to have a low covariance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这在许多情况下是有道理的，因为如果两个输入具有类似的值，因此差异很小，则它们的协方差将很高；而如果它们具有不同的值，则协方差将很低。两栋面积大致相等的房屋可能会有类似的价格，也就是说，它们的价格具有高的协方差；另一方面，非常大和非常小的房子的价格可能会具有较低的协方差。
- en: 13.2.1 Using a kernel with GPyTorch
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 使用 GPyTorch 的内核
- en: 'Let’s verify this with code. We usually initialize an `RBFKernel` object when
    creating a GP model. Here, we work directly with this kernel object. To do this,
    we first create an RBF kernel object with GPyTorch:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用代码验证一下。当我们创建 GP 模型时，通常会初始化一个 `RBFKernel` 对象。这里，我们直接使用这个内核对象进行工作。为此，我们首先使用
    GPyTorch 创建一个 RBF 内核对象：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note As always, GPyTorch is our library of choice for implementing GP-related
    objects in Python. Refer to section 2.4 for a refresher on how a kernel object
    is used by a GP in GPyTorch.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，作为 Python 中实现 GP 相关对象的首选库，我们一如既往地使用 GPyTorch。有关如何在 GPyTorch 中使用内核对象的详细信息，请参见第2.4节。
- en: 'To compute the covariance between two inputs, we simply pass them to this kernel
    object. For example, let’s compute the covariance between 0 and 0.1:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算两个输入之间的协方差，我们只需将它们传递给该内核对象即可。例如，让我们计算 0 和 0.1 之间的协方差：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'These two numbers are close to each other on the real number line (that is,
    they are similar), so their covariance is very high—almost 1\. Let’s now compute
    the covariance between 0 and 10, two numbers that are different:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个数字在实数线上非常接近（也就是说，它们是相似的），因此它们的协方差非常高，几乎为 1。现在让我们计算 0 和 10 之间的协方差，这是两个不同的数字：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This time, as the difference between the two numbers is much larger, their covariance
    drops to 0\. This contrast, which is a reasonable behavior to have, is illustrated
    by figure 13.5.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，由于两个数字之间的差异要大得多，它们的协方差降为 0。这种对比是合理的行为，并且通过图13.5进行说明。
- en: '![](../../OEBPS/Images/13-05.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-05.png)'
- en: Figure 13.5 The covariance between various numbers. When the difference between
    two numbers is small, the covariance increases. When the difference is large,
    the covariance decreases.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 各种数字之间的协方差。两个数字之间的差异较小时，协方差增加；差异较大时，协方差降低。
- en: The problem arises when the difference in value between two inputs doesn’t capture
    the structural difference in the meaning of the data. This is often the case for
    structured data such as images, as we will see now.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个输入之间的值差异不能捕捉到数据结构差异时，问题就出现了。这通常是对结构化数据（如图像）的情况。接下来，我们将看到像径向基函数（RBF）这样的常见内核如何无法描述结构化数据中的相似性。
- en: 13.2.2 Working with images in PyTorch
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 在 PyTorch 中处理图像
- en: 'In this subsection, we see how images are imported and stored as PyTorch tensors
    as well as how value-based similarity metrics, such as the RBF kernel, break down
    when processing this kind of data. First, we redefine our RBF kernel with a large-length
    scale so that higher covariances are more likely:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将看到如何将图像导入和存储为 PyTorch 张量，以及如何处理此类数据时，基于值的相似度度量（如 RBF 内核）如何失效。首先，我们重新定义我们的
    RBF 内核，使其具有较大的长度尺度，因此更有可能出现较高的协方差：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The large-length scale leads to higher covariances.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 长度尺度越大，协方差越高。
- en: 'Now, we need to import images from the MNIST dataset into our Python code.
    We can do this using PyTorch and its popular add-on library, torchvision:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将 MNIST 数据集中的图像导入到我们的 Python 代码中。我们可以使用 PyTorch 及其流行的附加库 torchvision
    来实现：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Defines the transformation to normalize the pixel values
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义规范化像素值的转换
- en: ❷ Downloads and imports the dataset
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 下载并导入数据集
- en: ❸ Extracts the pixel values as a flattened tensor
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提取像素值作为一个扁平化的张量
- en: We won’t go deeply into this code, as it’s not the focus of our discussion.
    All we need to know is that `train_x` contains the images in the MNIST dataset,
    each of which is stored as a PyTorch tensor containing the pixel values that represent
    an image of a handwritten digit.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入研究这段代码，因为它不是我们讨论的重点。我们只需要知道 `train_x` 包含 MNIST 数据集中的图像，每个图像都存储为一个 PyTorch
    张量，其中包含表示手写数字图像的像素值。
- en: 'As the data points are images, we can visualize them as heat maps, using the
    familiar `imshow()` function from Matplotlib. For example, the following code
    visualizes the first data point in `train_x`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据点是图像，我们可以将它们可视化为热图，使用 Matplotlib 中熟悉的 `imshow()` 函数。例如，以下代码可视化了 `train_x`
    中的第一个数据点：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Each image has 28 rows and 28 columns of pixels, so we need to reshape it
    into a 28-by-28 square tensor.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个图像有 28 行和 28 列的像素，因此我们需要将其重塑为一个 28×28 的方形张量。
- en: 'This code produces figure 13.2, which we see is an image of the digit 5\. When
    we print out the actual values of this first data point, we see that it’s a 28
    × 28 = 784-element PyTorch tensor:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成了图 13.2，我们看到它是数字 5 的图像。当我们打印出这个第一个数据点的实际值时，我们看到它是一个 28 × 28 = 784 元素的
    PyTorch 张量：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Each element in this tensor, ranging between 0 and 255, represents a pixel we
    see in figure 13.2\. A value of 0 corresponds to the lowest signal, the background
    in the figure, while higher values correspond to the bright spots.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此张量中的每个元素范围在 0 到 255 之间，表示我们在图 13.2 中看到的像素。值为 0 对应于最低信号，即图中的背景，而较高的值对应于亮点。
- en: 13.2.3 Computing the covariance of two images
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.3 计算两个图像的协方差
- en: 'That’s all the background information we need to explore the problem that common
    GP kernels face when processing structured data. To highlight the problem, we
    single out three specific data points, which we call point A, point B, and point
    C, respectively, indexed by the following numbers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们探索普通 GP 核处理结构化数据时所需要的所有背景信息。为了突出问题，我们单独提出了三个特定的数据点，分别称为点 A、点 B 和点 C，它们的索引如下：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Point A
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 点 A
- en: ❷ Point B
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 点 B
- en: ❸ Point C
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 点 C
- en: 'Before checking the actual digits these images show, let’s compute their covariance
    matrix using our RBF kernel:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查这些图像实际显示的数字之前，让我们使用我们的 RBF 核来计算它们的协方差矩阵：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is a 3-by-3 covariance matrix with a familiar structure: the diagonal
    elements take on the value of 1, representing the variances of the individual
    variables, while the off-diagonal elements represent the different covariances.
    We see that the points A and C are completely uncorrelated, having a covariance
    of zero, while points A and B are slightly correlated. According to the RBF kernel,
    points A and B are similar to each other and are completely different from point
    C.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 3×3 协方差矩阵，具有熟悉的结构：对角线元素取值为 1，表示各个变量的方差，而非对角线元素表示不同的协方差。我们看到点 A 和 C 完全不相关，协方差为零，而点
    A 和 B 稍微相关。根据 RBF 核，点 A 和 B 相似，并且与点 C 完全不同。
- en: We should, then, expect points A and B to share the same label. However, this
    is not the case! Once again, we visualize these data points as heat maps, and
    we obtain figure 13.6.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该期望点 A 和 B 具有相同的标签。然而，事实并非如此！再次将这些数据点可视化为热图，我们得到图 13.6。
- en: '![](../../OEBPS/Images/13-06.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-06.png)'
- en: Figure 13.6 Three specific data points from the MNIST dataset. The first and
    second points have a nonzero covariance, despite having different labels. The
    first and third points have a zero covariance, despite having the same label.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 MNIST 数据集中的三个特定数据点。第一个和第二个点具有非零协方差，尽管标签不同。第一个和第三个点具有零协方差，尽管标签相同。
- en: 'Here, it’s points A and C that share the same label (digit 9). So why does
    the RBF kernel think points A and B are correlated? Looking at figure 13.6, we
    can make a good guess that while points A and B have different labels, the images
    themselves are similar in the sense that many of the pixels match. In fact, the
    stroke that makes up the tail of the digits almost matches exactly in the two
    images. So, in a sense, the RBF kernel is doing its job, computing the difference
    between the images and outputting a number representing their covariance based
    on that difference. However, the difference is computed by comparing the pixels
    in and of themselves, which is not a metric that is indicative of what we’re trying
    to learn: the values of the digits.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，点 A 和 C 共享相同的标签（数字 9）。那么为什么 RBF 核会认为点 A 和 B 有相关性呢？看图 13.6，我们可以猜测，虽然点 A 和
    B 有不同的标签，但图像本身在很多像素上是相似的。事实上，构成数字尾巴的笔画在这两幅图像中几乎完全相同。因此，在某种程度上，RBF 核正在做它的工作，根据这种差异计算图像之间的差异并输出代表它们协方差的数字。然而，这种差异是通过比较像素本身来计算的，这不是我们试图学习的指标：数字的值。
- en: 'By only looking at the pixel values, the RBF kernel is overestimating the covariance
    between points A and B, which have different labels, and underestimating the covariance
    between points A and C, which share the same label, as illustrated by figure 13.7\.
    An analogy can be used here to demonstrate the inappropriate house kernel we mentioned
    at the introduction of the chapter: this kernel looks only at the color of the
    front door to decide whether two houses are correlated, leading to inaccurate
    predictions of their prices. In a similar (but not as extreme) manner, the RBF
    kernel only considers the values of the pixels, rather than the higher-level patterns,
    when comparing two images, which leads to inferior predictive performance.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅仅查看像素值，RBF 核高估了点 A 和 B 之间的协方差，这两个点具有不同的标签，并低估了点 A 和 C 之间的协方差，它们具有相同的标签，正如图
    13.7 所示。这里可以使用类比来演示我们在本章开头提到的不恰当的 house 核：这个核只看前门的颜色来决定两个房屋是否相关，导致对它们价格的不准确预测。类似地（但不如此极端），RBF
    核在比较两幅图像时只考虑像素的值，而不考虑更高级别的模式，这导致了较差的预测性能。
- en: '![](../../OEBPS/Images/13-07.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-07.png)'
- en: Figure 13.7 The covariance between handwritten digits computed by the RBF kernel.
    Because it only looks at the pixel values, the RBF kernel doesn’t produce appropriate
    covariances.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 由 RBF 核计算的手写数字之间的协方差。因为它只看像素值，所以 RBF 核无法产生适当的协方差。
- en: 13.2.4 Training a GP on image data
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.4 在图像数据上训练 GP
- en: By using the wrong metric for similarity, RBF confuses which of points B and
    C is the one correlated to point A, which, as we see next, leads to poor results
    when training a GP. We once again use the MNIST dataset, this time extracting
    1,000 data points to make up the training set and another 500 points as the test
    set. Our data preparation and learning workflow is summarized in figure 13.8,
    the distinct steps of which we go through in detail.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用错误的相似度度量标准，RBF 混淆了点 B 和 C 中哪个与点 A 相关联，这导致在训练 GP 时产生了不良结果。我们再次使用 MNIST 数据集，这次提取
    1,000 个数据点作为训练集，另外 500 个数据点作为测试集。我们的数据准备和学习工作流程总结在图 13.8 中，我们将详细介绍其中的不同步骤。
- en: '![](../../OEBPS/Images/13-08.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-08.png)'
- en: Figure 13.8 Flowchart of the case study of GP learning on MNIST. We extract
    1,000 data points to make up the training set and another 500 points as the test
    set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 在 MNIST 上进行 GP 学习案例的流程图。我们提取 1,000 个数据点作为训练集，另外 500 个数据点作为测试集。
- en: 'First, we import PyTorch and torchvision—the latter of which is an extension
    of PyTorch that manages computer-vision–related functionalities and datasets,
    such as MNIST. From torchvision, we import the modules `datasets` and `transforms`,
    which help us download and manipulate the MNIST data, respectively:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入 PyTorch 和 torchvision——后者是 PyTorch 的一个扩展，管理与计算机视觉相关的功能和数据集，如 MNIST。从
    torchvision 中，我们导入模块 `datasets` 和 `transforms`，它们帮助我们下载和操作 MNIST 数据，分别是：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the second data preparation step, we again use the object that transforms
    the images to PyTorch tensors (which is the data structure that GPs implemented
    in GPyTorch can process) and normalizes the pixel values. This normalization is
    done by subtracting the pixel values by 0.1307 (the mean of the data) and dividing
    the values by 0.3081 (the standard deviation of the data). This normalization
    is considered common practice for the MNIST dataset, and more details on this
    step can be found in PyTorch’s official forum discussion ([http://mng.bz/BmBr](http://mng.bz/BmBr)):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个数据准备步骤中，我们再次使用将图像转换为 PyTorch 张量的对象（这是 GPyTorch 中实现的 GP 可以处理的数据结构）并规范化像素值。此规范化通过将像素值减去
    0.1307（数据的平均值）并将值除以 0.3081（数据的标准差）来完成。这种规范化被认为是 MNIST 数据集的常见做法，有关此步骤的更多详细信息可以在
    PyTorch 的官方论坛讨论中找到（[http://mng.bz/BmBr](http://mng.bz/BmBr)）：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Transforms the data to PyTorch tensors
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据转换为 PyTorch 张量
- en: ❷ Normalizes the tensors
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 规范化张量
- en: 'This transformation object stored in `transform` can now be passed to a call
    to any torchvision dataset initialization, and the transformations (conversion
    to PyTorch tensors and normalization) will be applied to our data. We initialize
    the MNIST dataset with this transformation object as follows. Note that we create
    the dataset twice, once setting `train` `=` `True` to create the training set
    and once setting `train` `=` `False` for the test set:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在 `transform` 中的此转换对象现在可以传递给对任何 torchvision 数据集初始化的调用，并且将应用转换（转换为 PyTorch
    张量和规范化）到我们的数据上。我们使用此转换对象初始化 MNIST 数据集如下。请注意，我们创建数据集两次，一次将 `train` 设置为 `True` 以创建训练集，另一次将
    `train` 设置为 `False` 以创建测试集：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Downloads and imports the training set
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 下载并导入训练集
- en: ❷ Downloads and imports the test set
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 下载并导入测试集
- en: 'As the last step of data preparation, we extract the first 1,000 data points
    from the training set and 500 points from the test set. We do this by accessing
    from the dataset objects `dataset1` and `dataset2`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据准备的最后一步，我们从训练集中提取前 1,000 个数据点和测试集中的 500 个点。我们通过从数据集对象 `dataset1` 和 `dataset2`
    中访问来实现这一点：
- en: The `data` attribute to obtain the features, the pixel values making up the
    image for each data point
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `data` 属性获取特征，即构成每个数据点图像的像素值
- en: 'The `targets` attribute to obtain the labels, the values of the handwritten
    digits:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `targets` 属性获取标签，即手写数字的值：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Gets the first 1,000 points in the training set
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取训练集中的前 1,000 个点
- en: ❷ Gets the first 500 points in the test set
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取测试集中的前 500 个点
- en: 'We also implement a simple GP model with a constant mean and an RBF kernel
    with an output scale:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了一个简单的 GP 模型，具有恒定均值和带有输出比例的 RBF 核：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ A constant mean function
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个恒定均值函数
- en: ❷ An RBF covariance function with an output scale
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 具有输出比例的 RBF 协方差函数
- en: ❸ Makes an MVN distribution as the prediction for input x
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 以输入 x 的预测制作 MVN 分布
- en: Note The `forward()` method of a GPyTorch GP model was first discussed in section
    2.4.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 GPyTorch GP 模型的 `forward()` 方法首次讨论于第 2.4 节。
- en: 'Then, we initialize our GP and train it on the 1,000-point training set, using
    gradient descent with the Adam optimizer. This code will optimize the values of
    the hyperparameters of the GP (e.g., the mean constant and the length and output
    scale) so that we achieve a high marginal likelihood of the data we observe:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化我们的 GP 并在 1,000 个点的训练集上进行训练，使用 Adam 优化器的梯度下降。此代码将优化 GP 的超参数值（例如，均值常量和长度和输出比例），以便我们获得观察到的数据的高边际似然度：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Declares the likelihood function and the GP model
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 声明似然函数和 GP 模型
- en: ❷ Declares the gradient descent algorithm and the loss function
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 声明梯度下降算法和损失函数
- en: ❸ Enables training mode
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 启用训练模式
- en: ❹ Runs five hundred iterations of gradient descent
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 运行五百次梯度下降迭代
- en: ❺ Enables prediction mode
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 启用预测模式
- en: Note Refer to section 2.3.2 for a discussion of how gradient descent optimizes
    the likelihood of the data we observe—that is, how gradient descent trains a GP.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 参考第 2.3.2 节，了解梯度下降如何优化我们观察到的数据的似然度，即梯度下降如何训练 GP。
- en: Finally, to see how well our model performs on the test set, we compute the
    average absolute difference between the GP’s prediction and the ground truth (the
    value of label of each data point). This metric is often called the *mean absolute
    error*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了查看我们的模型在测试集上的表现如何，我们计算 GP 预测值与地面实况（每个数据点的标签值）之间的平均绝对差异。这个指标通常被称为 *平均绝对误差*。
- en: Note The typical metric for the MNIST dataset is the percentage of the test
    set that the model predicts correctly (that is, the accuracy), which is the norm
    for a classification problem. As we are using this dataset to simulate a regression
    problem, the mean absolute error is appropriate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 MNIST数据集的典型指标是该模型正确预测测试集的百分比（即准确度），这是分类问题的规范。由于我们使用这个数据集来模拟一个回归问题，因此均方误差是合适的。
- en: 'This is done by comparing the mean predictions against the true labels stored
    in `test_y`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过将均值预测与存储在`test_y`中的真实标签进行比较来完成的：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This output means that, on average, the GP’s prediction for the value of the
    digit depicted in an image is off by almost 3\. This performance is quite low,
    given that there are only 10 values to learn in this task. This result highlights
    regular GP models' incapability of dealing with structured data, such as images.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出意味着，平均而言，高斯过程对图像中描绘的数字的值的预测误差几乎达到了3。考虑到这项任务只有10个值需要学习，这个表现相当差。这个结果强调了常规高斯过程模型处理结构化数据（如图像）的无能。
- en: 13.3 Using neural networks to process complex structured data
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3使用神经网络处理复杂的结构化数据
- en: The root cause of our GP’s inferior performance, as we have seen, is that the
    kernels aren’t equipped to process the complex structure of the input data, leading
    to poor covariance calculation. The RBF kernel specifically has a simple form
    that only accounts for the difference in numeric values between two inputs. In
    this section, we learn to address this problem by using a neural network to process
    structured data, before feeding that processed data to a GP’s mean function and
    kernel.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所看到的高斯过程表现较差的根本原因是核不具备处理输入数据的复杂结构的装备，从而导致协方差计算不良。特别是，径向基核具有一个简单的形式，只考虑两个输入之间的数字值的差异。在本节中，我们学习如何通过使用神经网络处理结构化数据，然后将处理后的数据馈给高斯过程的均值函数和核来解决这个问题。
- en: 13.3.1 Why use neural networks for modeling?
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1为什么使用神经网络进行建模？
- en: We noted at the beginning of the book that neural networks aren’t great at making
    uncertainty-calibrated predictions, especially when data is expensive to obtain.
    (This is the whole reason why GPs are used in BayesOpt.) However, what neural
    networks are good at is learning complex structures. This flexibility is a result
    of having multiple layers of computation (specifically, matrix multiplication)
    in a neural network, as illustrated in figure 13.9.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书开始时指出，神经网络在进行昂贵的数据获取时，特别是在进行不确定性校准的预测方面表现不佳。（这是为什么BayesOpt中使用高斯过程的全部原因。）然而，神经网络擅长学习复杂结构。这种灵活性是由于神经网络中有多个计算层（具体来说，是矩阵乘法），如图13.9所示。
- en: '![](../../OEBPS/Images/13-09.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-09.png)'
- en: Figure 13.9 A neural network is a collection of layered computations. By chaining
    multiple layers of computations together, a neural network can model complex functions
    well.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9神经网络是一组层计算的集合。通过将多个计算层链接在一起，神经网络可以很好地模拟复杂函数。
- en: In a neural network, each layer corresponds to a matrix multiplication whose
    output is then put through a nonlinear activation function. By having many such
    layers chained together in one forward pass, the input of the network can be processed
    and manipulated in a flexible manner. The end result is that a neural network
    can model complex functions well. For a thorough explanation of neural networks
    and their usage, refer to François Chollet’s excellent book, *Deep Learning with
    Python, Second Edition* (Manning, 2021)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，每个层都对应于一个矩阵乘法，其输出然后经过非线性激活函数处理。通过在一次前向传递中将多个这样的层链在一起，可以以灵活的方式处理和操作网络的输入。最终结果是神经网络可以很好地模拟复杂函数。有关神经网络及其用法的详细解释，请参阅François
    Chollet的优秀著作*Deep Learning with Python, Second Edition*（Manning, 2021）。
- en: '![](../../OEBPS/Images/13-09-unnumb-1.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-09-unnumb-1.png)'
- en: The flexibility that neural networks possess can help us tackle the problem
    described in the previous section. If a GP’s kernel, such as the RBF kernel, cannot
    process complex data structures well, we could leave that job to a neural network
    and only feed the processed input to the GP’s kernel. This procedure is visualized
    in figure 13.10, where the input *x* first goes through the neural network layers
    before being passed to the mean function and the kernel of the GP.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络具有的灵活性可以帮助我们解决上一节中描述的问题。如果高斯过程的核，如径向基核，不能很好地处理复杂的数据结构，我们可以让神经网络来处理这项工作，并将处理后的输入仅馈送给高斯过程的核。这个过程在图13.10中进行了可视化，其中输入的*x*首先通过神经网络层，然后再传递给高斯过程的均值函数和核。
- en: '![](../../OEBPS/Images/13-10.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![13-10.png](../../OEBPS/Images/13-10.png)'
- en: Figure 13.10 Combining a neural network with a GP. The neural network processes
    the structured data input *x* first and then feeds the output to the mean function
    and the kernel of the GP.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10 结合了神经网络和GP。 神经网络首先处理结构化数据输入*x*，然后将输出馈送到GP的均值函数和核。
- en: While the end result is still an MVN distribution, the input of the mean function
    and the kernel is now the *processed input* produced by a neural network. This
    approach is promising because with its flexible modeling capability, the neural
    network will be able to extract the important features from the structured input
    data (which is important in terms of providing information for the similarity
    calculation) and boil them down to numerical values that are amenable to the GP’s
    kernel.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最终结果仍然是一个MVN分布，但均值函数和核函数的输入现在是由神经网络产生的*处理过的输入*。 这种方法很有前途，因为它具有灵活的建模能力，神经网络将能够从结构化输入数据中提取重要特征（在提供相似性计算信息方面很重要）并将其化简为适合GP核的数值。
- en: Definition The neural network is often called the *feature extractor* of the
    combined model because the network *extracts* features that are conducive to GP
    modeling from the structured data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 神经网络通常被称为组合模型的*特征提取器*，因为网络从结构化数据中*提取*有利于GP建模的特征。
- en: 'In this way, we can take advantage of a neural network’s flexible learning,
    while maintaining the ability to make uncertainty-calibrated predictions with
    a GP. It’s the best of both worlds! Further, training this combined model follows
    the same procedure of training a regular GP: we define our loss function, which
    is the negative log likelihood, and use gradient descent to find the values for
    our hyperparameters that best explain our data (by minimizing the loss). Instead
    of optimizing only the mean constant and the length and output scales, we now
    additionally optimize the weights of the neural networks. In the next subsection,
    we see that implementing this learning procedure with GPyTorch requires minimal
    changes from what we have been doing in previous chapters.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以利用神经网络的灵活学习能力，同时保持使用GP进行不确定性校准预测的能力。 这是两全其美！ 此外，训练这个组合模型的过程与训练常规GP的过程相同：我们定义我们的损失函数，即负对数似然，然后使用梯度下降来找到最能解释我们的数据的超参数值（通过最小化损失）。
    现在，我们不仅优化均值常数、长度和输出比例，还要额外优化神经网络的权重。 在下一小节中，我们将看到，使用GPyTorch实现这个学习过程几乎不需要做任何改动。
- en: Note This combined framework is a method of *dynamically learning* how to process
    structured data, purely from our training dataset. Previously, we only used a
    fixed kernel that processed data the same way across multiple applications. Here,
    we learn “on the fly” the best way to process our input data unique to the task
    at hand. This is because the weights of the neural network are optimized with
    respect to the training data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这种组合框架是一种*动态学习*如何处理结构化数据的方法，纯粹来自我们的训练数据集。 以前，我们只使用固定的核来处理数据，在多个应用程序中以相同的方式进行处理。
    在这里，我们“动态地”学习处理我们的输入数据的最佳方式，这对于手头的任务是独一无二的。 这是因为神经网络的权重是相对于训练数据进行优化的。
- en: 13.3.2 Implementing the combined model in GPyTorch
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.2 在GPyTorch中实现组合模型
- en: Finally, we now implement this framework and apply it to our MNIST dataset.
    Here, defining our model class is more involved, as we need to implement the neural
    network and hook it into the GP model class. Let’s tackle the first part—implementing
    the neural network—first. We design a simple neural network with the architecture
    in figure 13.11\. This network has four layers with 1,000, 5,000, 50, and 2 nodes,
    respectively, as indicated in the figure. This is a common architecture for the
    MNIST dataset.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们现在实现这个框架并将其应用于我们的MNIST数据集。 在这里，定义我们的模型类更加复杂，因为我们需要实现神经网络并将其连接到GP模型类中。 让我们先解决第一部分——先实现神经网络。
    我们设计一个简单的神经网络，其架构如图13.11所示。 此网络具有四个层，节点数分别为1,000、5,000、50和2，如图中所示。 这是一个常见的MNIST数据集架构。
- en: '![](../../OEBPS/Images/13-11.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![13-11.png](../../OEBPS/Images/13-11.png)'
- en: Figure 13.11 The architecture of the neural network to be implemented. It has
    four layers and produces an array of size two for each input data point.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11 要实现的神经网络架构。 它有四层，并为每个输入数据点产生一个大小为两个的数组。
- en: Note We need to pay attention to the size of the last layer (2), which denotes
    the dimensionality of the processed output to be fed into the mean function and
    the kernel of the GP. By setting the size of this layer to 2, we aim to learn
    a representation of the images that exists in a two-dimensional space. Values
    other than 2 can also be used, but we opt for 2 here for the purpose of visualization.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要关注最后一层（2）的大小，它表示要馈入高斯过程的均值函数和核函数的处理输出的维度。将该层的大小设置为2，目的是学习存在于二维空间中的图像表示。其他值也可以使用，但为了可视化的目的，我们选择了2。
- en: We implement this architecture using the `Linear()` and `ReLU()` classes from
    PyTorch. Here, each layer of our network is implemented as a `torch.nn.Linear`
    module with the appropriate size, as defined by figure 13.11\. Each module is
    also coupled with a `torch.nn.ReLU` activation function module, which implements
    the nonlinear transformation mentioned earlier. This is illustrated in figure
    13.12, which annotates each component of the network architecture with the corresponding
    code that implements it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PyTorch中的`Linear()`和`ReLU()`类实现该体系结构。在这里，我们的网络的每一层都被实现为一个带有相应大小的`torch.nn.Linear`模块，如图13.11所定义的。每个模块还与一个`torch.nn.ReLU`激活函数模块相耦合，该模块实现了前面提到的非线性变换。这在图13.12中得到了说明，其中注释了网络体系结构的每个组件对应于实现它的相应代码。
- en: '![](../../OEBPS/Images/13-12.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-12.png)'
- en: Figure 13.12 The architecture of the neural network to be implemented and the
    corresponding PyTorch code. Each layer is implemented with `torch.nn.Linear` and
    each activation function with `torch.nn.ReLU`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.12中实现的神经网络体系结构及其相应的PyTorch代码。每个层都使用`torch.nn.Linear`实现，每个激活函数都使用`torch.nn.ReLU`实现。
- en: 'By using the convenient `add_module()` method, we are implicitly defining the
    logic of the `forward()` method of our neural network model. We implement the
    model with the `LargeFeatureExtractor` class next. This class passes its input
    `x` sequentially through the layers we implement in the `__init__()` method, which
    takes in `data_dim`, the dimensionality of the input data. In our case, this number
    is 28 × 28 = 784, which we compute with `train_x.size(-1)`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用方便的`add_module()`方法，我们隐含定义了神经网络模型的`forward()`方法的逻辑。接下来，我们使用`LargeFeatureExtractor`类实现模型。该类将其输入`x`依次通过我们在`__init__()`方法中实现的层中，该方法接收`data_dim`，即输入数据的维数。在我们的情况下，该数字为28×28=784，我们使用`train_x.size(-1)`进行计算：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The dimensionality of the data
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据的维度
- en: ❷ The first layer of the network
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 网络的第一层
- en: ❸ The second layer
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第二层
- en: ❹ The third layer
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第三层
- en: ❺ The fourth layer
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 第四层
- en: ❻ Initializes the network
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 初始化网络
- en: 'Next, we discuss the combined model, which is a GP model class that makes use
    of `feature_extractor`, the neural network feature extractor we just initialized.
    We implement its `__init__()` method first, which comprises several components:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论组合模型——一种利用我们刚刚初始化的神经网络特征提取器`feature_extractor`的高斯过程模型类。我们首先实现它的`__init__()`方法，该方法由几个组件组成：
- en: The covariance module is wrapped around by a `gpytorch.kernels.GridInterpolationKernel`
    object, which offers computational speedup for our mid-sized training set of 1,000
    points. We declare the number of dimensions of the input data to be two, as that
    is the dimensionality of the output produced by the feature extractor.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 协方差模块被包装在`gpytorch.kernels.GridInterpolationKernel`对象中，为我们的中等大小训练集（1,000个点）提供计算速度加速。我们声明输入数据的维数为二，因为这是特征提取器生成的输出的维度。
- en: The feature extractor itself is the `feature_extractor` variable we declared
    earlier.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取器本身就是我们之前声明的`feature_extractor`变量。
- en: The output of the feature extractor could take on extreme values (negative or
    positive infinity) if the weights of the neural network are badly initialized.
    To address this, we scale these output values to the range between –1 and 1, using
    the `gpytorch.utils.grid.ScaleToBounds` module.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果神经网络的权重初始化得不好，特征提取器的输出值可能会取极端值（负无穷或正无穷）。为解决这个问题，我们使用`gpytorch.utils.grid.ScaleToBounds`模块将这些输出值缩放到-1和1之间的范围内。
- en: 'The `__init__()` method is implemented as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__()`方法的实现如下：'
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ An RBF kernel with two dimensions with computational speedup
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 具有两个维度的RBF核函数，具有计算速度加速
- en: ❷ The neural network feature extractor
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 神经网络特征提取器
- en: ❸ A module to scale the neural network’s output to reasonable values
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一个用于将神经网络的输出缩放至合理值的模块
- en: 'In our `forward()` method, we bring all of these components together. First,
    we use our neural network feature extractor to process the input. We then feed
    the processed input to the mean and covariance modules of our GP model. In the
    end, we still end up with an MVN distribution as what the `forward()` method returns:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`forward()`方法中，我们将所有这些组件结合在一起。首先，我们使用我们的神经网络特征提取器处理输入。然后，我们将处理后的输入馈送到我们的GP模型的平均值和协方差模块中。最后，我们仍然得到一个MVN分布，就是`forward()`方法返回的结果：
- en: '[PRE18]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Scaled output of the neural network feature extractor
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 缩放后的神经网络特征提取器的输出
- en: ❷ Creates an MVN distribution object from the processed input
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从处理后的输入创建一个MVN分布对象
- en: 'Finally, to train this combined model using gradient descent, we declare the
    following objects. Here, in addition to the regular GP hyperparameters, such as
    the mean constant and the length and output scales, we also want to optimize the
    weights of our neural network feature extractor, which are stored in `model.feature_extractor.parameters()`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了使用梯度下降训练这个组合模型，我们声明了以下对象。在这里，除了常规的GP超参数，如平均常数和长度和输出尺度，我们还想优化神经网络特征提取器的权重，这些权重存储在`model.feature_extractor.parameters()`中：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The likelihood function, the GP model, and the loss function are the same
    as before.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 同之前一样，似然函数、GP模型和损失函数保持不变。
- en: ❷ The gradient descent optimizer Adam now needs to optimize the weights of the
    feature extractor and the GP’s hyperparameters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在，梯度下降优化器Adam需要优化特征提取器的权重和GP的超参数。
- en: 'And with that, we are ready to run gradient descent the same way as before:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以像之前一样运行梯度下降：
- en: '[PRE20]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Enables training mode
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 启用训练模式
- en: ❷ Runs 500 iterations of gradient descent
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行500次梯度下降迭代
- en: ❸ Enables prediction mode
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 启用预测模式
- en: Note As a reminder, when training a GP, we need to enable training mode for
    both the model and the likelihood (using `model.train()` and `likelihood.train()`).
    After training and before making predictions, we then need to enable prediction
    mode (with `model.eval()` and `likelihood .eval()`).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：提醒一下，当训练GP模型时，我们需要同时为模型和似然函数启用训练模式（使用`model.train()`和`likelihood.train()`）。在训练之后并在进行预测之前，我们需要启用预测模式（使用`model.eval()`和`likelihood.eval()`）。
- en: We have now trained the GP model combined with a neural network feature extractor.
    Before we make our predictions on the test set using this model, we can take a
    peek inside the model and see whether the neural network feature extractor has
    learned to process our data well. Remember that each image is transformed into
    a two-element array by the feature extractor. So, we can pass our training data
    through this feature extractor and visualize the output using a scatter plot.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经训练了与神经网络特征提取器结合的GP模型。在使用该模型对测试集进行预测之前，我们可以查看模型的内部，看看神经网络特征提取器是否学会了很好地处理我们的数据。请记住，每个图像都被特征提取器转化为一个二元数组。因此，我们可以将训练数据通过该特征提取器，并使用散点图可视化输出。
- en: Note Training this combined model takes more time than training a regular GP.
    This is because we now have many more parameters to optimize. However, as we see
    shortly, this cost is well worth it, as we achieve a much higher performance gain.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：训练这个组合模型比训练普通的GP模型需要更多时间。这是因为我们现在要优化的参数更多。然而，正如我们马上会看到的那样，这个成本是非常值得的，因为我们获得了更高的性能提升。
- en: 'In this scatter plot, if we see points with the same label (that is, images
    depicting the same digit) cluster together, that will be an indication that the
    feature extractor was able to effectively learn from data. Again, we do this by
    passing the training data through the feature extractor the same way that data
    is processed in the `forward()` method of the model class:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个散点图中，如果我们看到相同标签的点（即，描绘相同数字的图像）聚在一起，这将表明特征提取器能够有效地从数据中学习。同样，我们通过将训练数据通过特征提取器的方式进行处理来做到这一点，这与模型类的`forward()`方法中的数据处理方式相同：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here, `extracted_features` is a 1,000-by-2 PyTorch tensor that stores the two-dimensional
    extracted features of the 1,000 data points in our training set. To visualize
    this tensor in a scatter plot, we use `plt.scatter()` from the Matplotlib library,
    making sure each label corresponds to a color:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`extracted_features`是一个大小为1,000x2的PyTorch张量，存储了我们训练集中1,000个数据点的二维提取特征。为了在散点图中可视化这个张量，我们使用Matplotlib库的`plt.scatter()`方法，确保每个标签对应一个颜色：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Filters the data points with a specific label
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 过滤具有特定标签的数据点
- en: ❷ Creates a scatter plot for the current data points, which share the same color
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为当前数据点创建一个散点图，它们具有相同的颜色
- en: This code produces figure 13.13, although your result might vary, depending
    on the library versions and the system your code runs on. Just as we expected,
    data points of the same label cluster around one another. This means our neural
    network feature extractor is successfully grouping points with the same label
    together. After being processed by the network, two images of the same label become
    two points that are close to each other in a two-dimensional plane, which will
    then have a high covariance if computed by the RBF kernel. This is exactly what
    we wanted our feature extractor to help us do!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成图13.13，尽管你的结果可能会有所不同，这取决于库版本和代码运行的系统。正如我们所预期的，相同标签的数据点围绕在一起。这意味着我们的神经网络特征提取器成功地将具有相同标签的点分组在一起。经过网络处理后，具有相同标签的两个图像变成了二维平面上彼此靠近的两个点，如果由RBF核计算，则它们将具有高的协方差。这正是我们希望我们的特征提取器帮助我们做的事情！
- en: '![](../../OEBPS/Images/13-13.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-13.png)'
- en: 'Figure 13.13 Features extracted from the MNIST dataset by a neural network.
    Not only do data points of the same label cluster together, but there is also
    a label gradient across the plot: going from the bottom to the top, the value
    of the label increases.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.13 由神经网络从MNIST数据集中提取的特征。不仅相同标签的数据点聚集在一起，而且在图中还存在一个标签梯度：从底部到顶部，标签值逐渐增加。
- en: 'Another interesting aspect of figure 13.13 is that there is a clear gradient
    with respect to the label value: going from the bottom to the top cluster, the
    value of the corresponding label gradually increases from 0 to 9\. This is a great
    feature to have in a feature extractor, as it indicates our model has found a
    smooth representation of the MNIST images that respects the label values.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.13另一个有趣的方面是，与标签值相关的梯度明显：从底部到顶部的聚类，相应标签的值从0逐渐增加到9。这是特征提取器中很好的特性，因为它表明我们的模型已经找到了一种平滑的MNIST图像表示，符合标签值。
- en: '![](../../OEBPS/Images/13-13-unnumb-2.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-13-unnumb-2.png)'
- en: 'For example, consider the comparison in figure 13.14, where the left panel
    shows figure 13.13 and the right panel shows a random swapping of the labels of
    the same scatter plot, making the features “rougher.” By “rough,” we mean that
    the label values jump around erratically: the bottom cluster contains the 0s,
    some of the clusters in the middle correspond to 7s and 9s, and the top cluster
    contains the 5s. In other words, the trend of the labels with rough features is
    not monotonic, making it more difficult to train a GP.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑图13.14中的比较，左侧面板显示图13.13，右侧面板显示相同散点图标签的随机交换，使特征变得“粗糙”。所谓“粗糙”，是指标签值在不规律地跳动：底部聚类包含0，中间某些聚类对应于7和9，顶部聚类包含5。换句话说，具有粗糙特征的标签趋势不是单调的，这使得训练GP更加困难。
- en: '![](../../OEBPS/Images/13-14.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-14.png)'
- en: Figure 13.14 A comparison between the smooth extracted features from figure
    13.13 (left) and a random swapping of the labels, making the features less smooth
    (right). The smooth features are easier to learn with a GP than the rough features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.14 图13.13中提取的平滑特征与标签随机交换的比较，使特征变得不那么平滑。平滑的特征比粗糙的特征更容易通过GP学习。
- en: 'So, it seems that the neural network is doing a good job extracting useful
    features from images. To see whether this actually leads to better predictive
    performance, we again compute the mean absolute error (MAE):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来神经网络在从图像中提取有用特征方面表现不错。为了确定这是否确实导致更好的预测性能，我们再次计算平均绝对误差（MAE）：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This result tells us that on average, our prediction is off by 0.85; this is
    a significant improvement from the vanilla GP we had in the previous section,
    whose MAE was roughly 2.7\. This improvement illustrates the superior performance
    of the combined model, which stems from the flexible modeling capability of neural
    networks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果告诉我们，平均而言，我们的预测偏差为0.85；这是对前一节中我们拥有的普通GP的显着改进，其MAE大约为2.7。这种改进说明了联合模型的卓越性能，这源于神经网络灵活的建模能力。
- en: As we said at the beginning, this framework doesn’t only apply to handwritten
    digits but also to various types of structured data that a neural network can
    learn from, including other types of images and graph structures, such as molecules
    and proteins. All we need to do is define an appropriate DL architecture that
    extracts features from these structured data, which are then passed to the GP’s
    mean function and kernel.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在开始时所说，这个框架不仅适用于手写数字，还适用于各种类型的结构化数据，神经网络可以从中学习，包括其他类型的图像和图形结构，如分子和蛋白质。我们所要做的就是定义一个合适的
    DL 架构，从这些结构化数据中提取特征，然后将这些特征传递给 GP 的均值函数和核函数。
- en: This concludes chapter 12\. Throughout this chapter, we have learned about the
    difficulty of learning from structured data, such as images, where common kernels
    cannot effectively compute covariances between data points. By attaching a neural
    network feature extractor in front of a GP, we learn to transform this structured
    data into a form that the GP’s kernel can then process. The end result is a combined
    model that can flexibly learn from structured data but still produces probabilistic
    predictions with uncertainty quantification.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了第 12 章。在本章中，我们了解了从结构化数据中学习的困难，例如图像，在这些数据中，常见的核无法有效地计算数据点之间的协方差。通过在 GP 前面附加一个神经网络特征提取器，我们学会将这些结构化数据转换成
    GP 的核函数可以处理的形式。最终结果是一个结合模型，可以灵活地从结构化数据中学习，但仍然产生具有不确定性量化的概率预测。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Structured data is data whose features need to satisfy constraints, such as
    being an integer or being nonnegative, and cannot be treated as continuous, real-valued
    data. Examples include data from common applications, such as images in computer
    vision and protein structures in drug discovery.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化数据是指其特征需要满足约束条件的数据，例如必须是整数或者非负数，并且不能被视为连续的实值数据。例如，常见应用程序中的数据，如计算机视觉中的图像和药物发现中的蛋白质结构。
- en: Structured data poses a challenge to common kernels for GPs. This is because
    these kernels only consider the numerical values of input data, which may be poor
    predictive features.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化数据对于 GP 的常见核构成挑战。这是因为这些核只考虑输入数据的数值，这可能是不良的预测特征。
- en: A kernel using the wrong features to compute covariances could lead to low-quality
    predictions from the resulting GP. Using the wrong features is particularly common
    with structured data.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用错误特征计算协方差的核可能会导致生成的 GP 的预测质量低下。使用错误特征在结构化数据中特别常见。
- en: For image data specifically, the raw values of the pixels are not an informative
    feature. A kernel that computes covariances using the raw pixel values can lead
    to a low-quality GP.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于图像数据特别是，像素的原始值不是一个信息量丰富的特征。使用原始像素值计算协方差的核可能导致低质量的 GP。
- en: Having multiple layers of nonlinear computation, neural networks are effective
    at learning complex functions and can extract features from structured data. By
    using a neural network to extract continuous, real-valued features from structured
    data, a GP can still learn effectively.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于具有多层非线性计算，神经网络能够有效地学习复杂函数，并且可以从结构化数据中提取特征。通过使用神经网络从结构化数据中提取连续的实值特征，GP 仍然可以有效地学习。
- en: In combining a neural network with a GP, we dynamically learn a way to process
    data that is dedicated to the problem at hand. This flexibility allows this model
    to generalize to many kinds of structured data.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将神经网络与 GP 结合时，我们动态学习一种处理问题的数据方式。这种灵活性使得该模型可以推广到许多种结构化数据。
- en: It’s important to scale the output of a neural network to a small range before
    passing it to a GP. In doing this, we avoid extreme values that could result from
    a badly initialized neural network feature extractor.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将输出缩放到小范围之前，将神经网络的输出传递给 GP 是很重要的。通过这样做，我们避免了由于神经网络特征提取器初始化不良而导致的极端值。
- en: The representation learned from a neural network feature extractor exhibits
    a smooth gradient with respect to the labels. This smooth gradient makes the extracted
    features more amenable to learning with a GP.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从神经网络特征提取器学习到的表示对标签具有平滑的梯度。这种平滑的梯度使得提取的特征更容易通过 GP 学习。
