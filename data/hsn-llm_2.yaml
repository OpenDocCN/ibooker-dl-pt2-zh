- en: Chapter 3\. Text Clustering and Topic Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. 文本聚类与主题建模
- en: Although supervised techniques, such as classification, have reigned supreme
    over the last few years in the industry, the potential of unsupervised techniques
    such as text clustering cannot be understated.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督技术，如分类，在过去几年中在行业中占据主导地位，但无监督技术（如文本聚类）的潜力不可低估。
- en: Text clustering aims to group similar texts based on their semantic content,
    meaning, and relationships, as illustrated in [Figure 3-1](#fig_1_clustering_unstructured_textual_data).
    Just like how we’ve used distances between text embeddings in dense retrieval
    in chapter XXX, clustering embeddings allow us to group the documents in our archive
    by similarity.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 文本聚类旨在根据其语义内容、意义和关系对相似文本进行分组，如[图3-1](#fig_1_clustering_unstructured_textual_data)所示。正如我们在第XXX章中使用文本嵌入之间的距离进行密集检索一样，聚类嵌入使我们能够根据相似性对档案中的文档进行分组。
- en: The resulting clusters of semantically similar documents not only facilitate
    efficient categorization of large volumes of unstructured text but also allows
    for quick exploratory data analysis. With the advent of Large Language Models
    (LLMs) allowing for contextual and semantic representations of text, the power
    of text clustering has grown significantly over the last years. Language is not
    a bag of words, and Large Language Models have proved to be quite capable of capturing
    that notion.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 语义相似文档的聚类结果不仅促进了对大量非结构化文本的高效分类，还允许快速的探索性数据分析。随着大型语言模型（LLMs）的出现，能够提供文本的上下文和语义表示，文本聚类的力量在过去几年中显著增强。语言不是一袋单词，大型语言模型已证明能够很好地捕捉这一概念。
- en: An underestimated aspect of text clustering is its potential for creative solutions
    and implementations. In a way, unsupervised means that we are not constrained
    by a certain task or thing that we want to optimize. As a result, there is much
    freedom in text clustering that allows us to steer from the well-trodden paths.
    Although text clustering would naturally be used for grouping and classifying
    documents, it can be used to algorithmically and visually find improper labels,
    perform topic modeling, speed up labeling, and many more interesting use cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 文本聚类的一个被低估的方面是其创造性解决方案和实施的潜力。从某种意义上说，无监督意味着我们不受限于某个特定的任务或我们想要优化的事物。因此，文本聚类中有很大的自由度，使我们能够偏离常规路径。尽管文本聚类自然会用于文档的分组和分类，但它还可以用于算法性和视觉上发现不恰当的标签，进行主题建模，加速标记，以及许多其他有趣的用例。
- en: '![Clustering unstructured textual data. ](assets/text_clustering_and_topic_modeling_664502_01.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![聚类非结构化文本数据。](assets/text_clustering_and_topic_modeling_664502_01.png)'
- en: Figure 3-1\. Clustering unstructured textual data.
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 聚类非结构化文本数据。
- en: This freedom also comes with its challenges. Since we are not guided by a specific
    task, then how do we evaluate our unsupervised clustering output? How do we optimize
    our algorithm? Without labels, what are we optimizing the algorithm for? When
    do we know our algorithm is correct? What does it mean for the algorithm to be
    “correct”? Although these challenges can be quite complex, they are not insurmountable
    but often require some creativity and a good understanding of the use case.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自由也带来了挑战。由于我们没有特定任务的指导，那么我们如何评估我们的无监督聚类输出？我们如何优化我们的算法？没有标签，我们在优化算法的目标是什么？我们什么时候知道我们的算法是正确的？算法“正确”意味着什么？尽管这些挑战可能相当复杂，但并非不可逾越，通常需要一些创造力和对用例的良好理解。
- en: Striking a balance between the freedom of text clustering and the challenges
    it brings can be quite difficult. This becomes even more pronounced if we step
    into the world of topic modeling, which has started to adopt the “text clustering”
    way of thinking.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本聚类的自由与其带来的挑战之间取得平衡可能相当困难。如果我们进入主题建模的世界，这种平衡变得更加明显，因为主题建模开始采用“文本聚类”的思维方式。
- en: With topic modeling, we want to discover abstract topics that appear in large
    collections of textual data. We can describe a topic in many ways, but it has
    traditionally been described by a set of keywords or key phrases. A topic about
    natural language processing (NLP) could be described with terms such as “deep
    learning”, “transformers”, and “self-attention”. Traditionally, we expect a document
    about a specific topic to contain terms appearing more frequently than others.
    This expectation, however, ignores contextual information that a document might
    contain. Instead, we can leverage Large Language Models, together with text clustering,
    to model contextualized textual information and extract semantically-informed
    topics. [Figure 3-2](#fig_2_topic_modeling_is_a_way_to_give_meaning_to_cluster)demonstrates
    this idea of describing clusters through textual representations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过主题建模，我们希望发现出现在大型文本数据集中的抽象主题。我们可以用多种方式描述主题，但它通常由一组关键字或短语描述。有关自然语言处理（NLP）的主题可以用“深度学习”、“变换器”和“自注意力”等术语来描述。传统上，我们期望关于特定主题的文档包含的术语出现频率高于其他术语。然而，这种期望忽略了文档可能包含的上下文信息。相反，我们可以利用大型语言模型结合文本聚类来建模上下文化的文本信息并提取语义相关的主题。[图3-2](#fig_2_topic_modeling_is_a_way_to_give_meaning_to_cluster)展示了通过文本表示描述集群的想法。
- en: '*![Topic modeling is a way to give meaning to clusters of textual documents.](assets/text_clustering_and_topic_modeling_664502_02.png)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*![主题建模是一种赋予文本文件集群意义的方法。](assets/text_clustering_and_topic_modeling_664502_02.png) '
- en: Figure 3-2\. Topic modeling is a way to give meaning to clusters of textual
    documents.*  *In this chapter, we will provide a guide on how text clustering
    can be done with Large Language Models. Then, we will transition into a text-clustering-inspired
    method of topic modeling, namely BERTopic.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 主题建模是一种赋予文本文件集群意义的方法。* *在本章中，我们将提供关于如何使用大型语言模型进行文本聚类的指南。然后，我们将转向一种受文本聚类启发的主题建模方法，即BERTopic。*
- en: Text Clustering
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本聚类
- en: One major component of exploratory data analysis in NLP is text clustering.
    This unsupervised technique aims to group similar texts or documents together
    as a way to easily discover patterns among large collections of textual data.
    Before diving into a classification task, text clustering allows for getting an
    intuitive understanding of the task but also of its complexity.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: NLP中探索性数据分析的一个主要组成部分是文本聚类。这种无监督技术旨在将相似的文本或文档分组，以便轻松发现大量文本数据中的模式。在深入分类任务之前，文本聚类可以帮助我们直观理解任务及其复杂性。
- en: The patterns that are discovered from text clustering can be used across a variety
    of business use cases. From identifying recurring support issues and discovering
    new content to drive SEO practices, to detecting topic trends in social media
    and discovering duplicate content. The possibilities are diverse and with such
    a technique, creativity becomes a key component. As a result, text clustering
    can become more than just a quick method for exploratory data analysis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本聚类中发现的模式可以应用于多种商业用例。从识别重复的支持问题和发现新内容以推动SEO实践，到检测社交媒体中的主题趋势和发现重复内容，可能性多种多样，运用这样的技术，创造力成为关键要素。因此，文本聚类不仅仅是快速进行探索性数据分析的方法。
- en: Data
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: Before we describe how to perform text clustering, we will first introduce the
    data that we are going to be using throughout this chapter. To keep up with the
    theme of this book, we will be clustering a variety of ArXiv articles in the domain
    of machine learning and natural language processing. The dataset contains roughly
    **XXX** articles between **XXX** and **XXX**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述如何进行文本聚类之前，我们将首先介绍在本章中将使用的数据。为了保持本书的主题，我们将对机器学习和自然语言处理领域的各种ArXiv文章进行聚类。该数据集包含大约**XXX**篇文章，时间跨度为**XXX**到**XXX**。
- en: We start by importing our dataset using [HuggingFace’s dataset package](https://github.com/huggingface/datasets)
    and extracting metadata that we are going to use later on, like the abstracts,
    years, and categories of the articles.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用[HuggingFace的数据集包](https://github.com/huggingface/datasets)导入我们的数据集，并提取稍后要使用的元数据，例如文章的摘要、年份和类别。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How do we perform Text Clustering?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何进行文本聚类？
- en: 'Now that we have our data, we can perform text clustering. To perform text
    clustering, a number of techniques can be employed, from graph-based neural networks
    to centroid-based clustering techniques. In this section, we will go through a
    well-known pipeline for text clustering that consists of three major steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，可以进行文本聚类。进行文本聚类时，可以采用多种技术，从基于图的神经网络到基于中心的聚类技术。在这一部分，我们将介绍一种著名的文本聚类流程，包括三个主要步骤：
- en: Embed documents
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入文档
- en: Reduce dimensionality
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降维
- en: Cluster embeddings
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类嵌入
- en: 1\. Embed documents
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 嵌入文档
- en: The first step in clustering textual data is converting our textual data to
    text embeddings. Recall from previous chapters that embeddings are numerical representations
    of text that capture its meaning. Producing embeddings optimized for semantic
    similarity tasks is especially important for clustering. By mapping each document
    to a numerical representation such that semantically similar documents are close,
    clustering will become much more powerful. A set of popular Large Language Models
    optimized for these kinds of tasks can be found in the well-known sentence-transformers
    framework (reimers2019sentence). [Figure 3-3](#fig_3_step_1_we_convert_documents_to_numerical_represen)
    shows this first step of converting documents to numerical representations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类文本数据的第一步是将我们的文本数据转换为文本嵌入。回想前面的章节，嵌入是文本的数值表示，捕捉其含义。为语义相似性任务优化的嵌入对于聚类尤为重要。通过将每个文档映射到数值表示，使语义相似的文档彼此接近，聚类将变得更加强大。一组为这些任务优化的流行大型语言模型可以在著名的句子转换器框架中找到（reimers2019sentence）。[图3-3](#fig_3_step_1_we_convert_documents_to_numerical_represen)展示了将文档转换为数值表示的第一步。
- en: '*![Step 1  We convert documents to numerical representations  namely embeddings.
    ](assets/text_clustering_and_topic_modeling_664502_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*![第一步 我们将文档转换为数值表示，即嵌入。](assets/text_clustering_and_topic_modeling_664502_03.png)'
- en: 'Figure 3-3\. Step 1: We convert documents to numerical representations, namely
    embeddings.*  *Sentence-transformers has a clear API and can be used as follows
    to generate embeddings from pieces of text:'
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 第一步：我们将文档转换为数值表示，即嵌入。* *句子转换器具有清晰的API，可以如下所示从文本片段生成嵌入：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The sizes of these embeddings differ depending on the model but typically contain
    at least 384 values for each sentence or paragraph. The number of values an embedding
    contains is referred to as the dimensionality of the embedding.*  *### 2\. Reduce
    dimensionality
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入的大小因模型而异，但通常每个句子或段落至少包含384个值。嵌入所包含的值的数量称为嵌入的维度。* *### 2\. 降维
- en: Before we cluster the embeddings we generated from the ArXiv abstracts, we need
    to take care of the curse of dimensionality first. This curse is a phenomenon
    that occurs when dealing with high-dimensional data. As the number of dimensions
    increases, there is an exponential growth of the number of possible values within
    each dimension. Finding all subspaces within each dimension becomes increasingly
    complex. Moreover, as the number of dimensions grows, the concept of distance
    between points becomes increasingly less precise.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们聚类从ArXiv摘要生成的嵌入之前，我们首先需要处理维度灾难。这个诅咒是在处理高维数据时出现的现象。随着维度的增加，每个维度内可能值的数量呈指数级增长。在每个维度内找到所有子空间变得越来越复杂。此外，随着维度的增加，点之间的距离概念变得越来越不精确。
- en: As a result, high-dimensional data can be troublesome for many clustering techniques
    as it gets more difficult to identify meaningful clusters. Clusters are more diffuse
    and less distinguishable, making it difficult to accurately identify and separate
    them.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，高维数据对于许多聚类技术来说可能是麻烦的，因为识别有意义的聚类变得更加困难。聚类变得更加分散和难以区分，使得准确识别和分离它们变得困难。
- en: The previously generated embeddings are high in their dimensionality and often
    trigger the curse of dimensionality. To prevent their dimensionality from becoming
    an issue, the second step in our clustering pipeline is dimensionality reduction,
    as shown in [Figure 3-4](#fig_4_step_2_the_embeddings_are_reduced_to_a_lower_dime).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 先前生成的嵌入具有较高的维度，通常会引发维度灾难。为了防止维度成为问题，我们聚类管道中的第二步是降维，如[图3-4](#fig_4_step_2_the_embeddings_are_reduced_to_a_lower_dime)所示。
- en: '![Step 2  The embeddings are reduced to a lower dimensional space using dimensionality
    reduction. ](assets/text_clustering_and_topic_modeling_664502_04.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![第 2 步 嵌入通过维度减少被降低到一个低维空间。 ](assets/text_clustering_and_topic_modeling_664502_04.png)'
- en: 'Figure 3-4\. Step 2: The embeddings are reduced to a lower dimensional space
    using dimensionality reduction.'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 第 2 步：嵌入通过维度减少被降低到一个低维空间。
- en: Dimensionality reduction techniques aim to preserve the global structure of
    high-dimensional data by finding low-dimensional representations. Well-known methods
    are Principal Component Analysis (PCA) and Uniform Manifold Approximation and
    Projection (UMAP; mcinnes2018umap). For this pipeline, we are going with UMAP
    as it tends to handle non-linear relationships and structures a bit better than
    PCA.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 维度减少技术旨在通过寻找低维表示来保留高维数据的全局结构。著名的方法包括主成分分析（PCA）和均匀流形近似与投影（UMAP; mcinnes2018umap）。对于这个流程，我们选择UMAP，因为它通常比PCA更好地处理非线性关系和结构。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Dimensionality reduction techniques, however, are not flawless. They cannot
    perfectly capture high-dimensional data in a lower-dimensional representation.
    Information will always be lost with this procedure. There is a balance between
    reducing dimensionality and keeping as much information as possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，维度减少技术并非完美无缺。它们无法完美地将高维数据捕捉到低维表示中。在这个过程中信息总会有所丢失。在减少维度和尽可能保留信息之间存在平衡。
- en: 'To perform dimensionality reduction, we need to instantiate our UMAP class
    and pass the generated embeddings to it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行维度减少，我们需要实例化我们的UMAP类并将生成的嵌入传递给它：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can use the ``n_components`` parameter to decide the shape of the lower-dimensional
    space. Here, we used ``n_components=5`` as we want to retain as much information
    as possible without running into the curse of dimensionality. No one value does
    this better than another, so feel free to experiment!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用``n_components``参数来决定低维空间的形状。在这里，我们使用了``n_components=5``，因为我们希望尽可能保留信息而不陷入维度灾难。没有哪个值比另一个更好，因此请随意尝试！
- en: 3\. Cluster embeddings
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 聚类嵌入
- en: As shown in [Figure 3-5](#fig_5_step_3_we_cluster_the_documents_using_the_embeddi),
    the final step in our pipeline is to cluster the previously reduced embeddings.
    Many algorithms out there handle clustering tasks quite well, from centroid-based
    methods like k-Means to hierarchical methods like Agglomerative Clustering. The
    choice is up to the user and is highly influenced by the respective use case.
    Our data might contain some noise, so a clustering algorithm that detects outliers
    would be preferred. If our data comes in daily, we might want to look for an online
    or incremental approach instead to model if new clusters were created.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3-5](#fig_5_step_3_we_cluster_the_documents_using_the_embeddi)所示，我们流程中的最后一步是对之前减少的嵌入进行聚类。许多算法能够很好地处理聚类任务，从基于质心的方法如k-Means到层次方法如凝聚聚类。选择取决于用户，并受到相应用例的高度影响。我们的数据可能包含一些噪声，因此更倾向于使用能检测异常值的聚类算法。如果我们的数据是每日产生的，我们可能希望寻找在线或增量的方法来建模是否创建了新的聚类。
- en: '![Step 3  We cluster the documents using the embeddings that were reduced in
    their dimensionality. ](assets/text_clustering_and_topic_modeling_664502_05.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![第 3 步 我们使用减少维度的嵌入对文档进行聚类。 ](assets/text_clustering_and_topic_modeling_664502_05.png)'
- en: 'Figure 3-5\. Step 3: We cluster the documents using the embeddings that were
    reduced in their dimensionality.'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 第 3 步：我们使用减少维度的嵌入对文档进行聚类。
- en: A good default model is Hierarchical Density-Based Spatial Clustering of Applications
    with Noise (HDBSCAN; mcinnes2017hdbscan). HDBSCAN is a hierarchical variation
    of a clustering algorithm called DBSCAN which allows for dense (micro)-clusters
    to be found without us having to explicitly specify the number of clusters. As
    a density-based method, it can also detect outliers in the data. Data points that
    do not belong to any cluster. This is important as forcing data into clusters
    might create noisy aggregations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的默认模型是基于密度的空间聚类算法（HDBSCAN; mcinnes2017hdbscan）。HDBSCAN是名为DBSCAN的聚类算法的一种层次变体，允许找到密集（微）聚类，而无需明确指定聚类数量。作为一种基于密度的方法，它也可以检测数据中的异常值。数据点如果不属于任何聚类。这一点很重要，因为强行将数据归入聚类可能会产生噪声聚合。
- en: 'As with the previous packages, using HDBSCAN is straightforward. We only need
    to instantiate the model and pass our reduced embeddings to it:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的包一样，使用HDBSCAN非常简单。我们只需实例化模型并将我们的减少嵌入传递给它：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, using our previously generated 2D-embeddings, we can visualize how HDBSCAN
    has clustered our data:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，利用我们之前生成的2D嵌入，我们可以可视化HDBSCAN如何对数据进行聚类：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see in [Figure 3-6](#fig_6_the_generated_clusters_colored_and_outliers_gre),
    it tends to capture major clusters quite well. Note how clusters of points are
    colored in the same color, indicating that HDBSCAN put them in a group together.
    Since we have a large number of clusters, the plotting library cycles the colors
    between clusters, so don’t think that all blue points are one cluster, for example.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[图 3-6](#fig_6_the_generated_clusters_colored_and_outliers_gre)中看到的，它能够很好地捕捉主要聚类。注意这些点的聚类被涂成相同的颜色，表明HDBSCAN将它们分为一组。由于我们有大量聚类，绘图库在聚类之间循环颜色，所以不要认为所有蓝色点都是一个聚类，例如。
- en: '![The generated clusters  colored  and outliers  grey  are represented as a
    2D visualization.](assets/text_clustering_and_topic_modeling_664502_06.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![生成的聚类（彩色）和离群点（灰色）作为2D可视化呈现。](assets/text_clustering_and_topic_modeling_664502_06.png)'
- en: Figure 3-6\. The generated clusters (colored) and outliers (grey) are represented
    as a 2D visualization.
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6\. 生成的聚类（彩色）和离群点（灰色）作为2D可视化呈现。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Using any dimensionality reduction technique for visualization purposes creates
    information loss. It is merely an approximation of what our original embeddings
    look like. Although it is informative, it might push clusters together and drive
    them further apart than they actually are. Human evaluation, inspecting the clusters
    ourselves, is, therefore, a key component of cluster analysis!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 任何用于可视化目的的降维技术都会导致信息损失。这仅仅是对我们原始嵌入的近似。尽管这很有信息量，但它可能将聚类推得更近或更远于它们实际的位置。因此，人类评估，亲自检查聚类，是聚类分析的关键组成部分！
- en: 'We can inspect each cluster manually to see which documents are semantically
    similar enough to be clustered together. For example, let us take a few random
    documents from cluster **XXX**:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动检查每个聚类，以查看哪些文档在语义上足够相似以被聚类在一起。例如，让我们从聚类**XXX**中随机取出几个文档：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These printed documents tell us that the cluster likely contains documents that
    talk about **XXX**. We can do this for every created cluster out there but that
    can be quite a lot of work, especially if we want to experiment with our hyperparameters.
    Instead, we would like to create a method for automatically extracting representations
    from these clusters without us having to go through all documents.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些打印的文档告诉我们，该聚类可能包含关于**XXX**的文档。我们可以对每个创建的聚类执行此操作，但这可能会很繁琐，特别是如果我们想尝试调整超参数。相反，我们希望创建一种方法，能够自动从这些聚类中提取表示，而无需逐一检查所有文档。
- en: This is where topic modeling comes in. It allows us to model these clusters
    and give singular meaning to them. Although there are many techniques out there,
    we choose a method that builds upon this clustering philosophy as it allows for
    significant flexibility.*  *# Topic Modeling
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是主题建模发挥作用的地方。它使我们能够对这些聚类进行建模，并赋予它们单一的意义。尽管有许多技术可供选择，我们选择了一种基于这种聚类理念的方法，因为它具有显著的灵活性。*  *#
    主题建模
- en: Traditionally, topic modeling is a technique that aims to find latent topics
    or themes in a collection of textual data. For each topic, a set of keywords or
    phrases are identified that best represent and capture the meaning of the topic.
    This technique is ideal for finding common themes in large corpora as it gives
    meaning to sets of similar content. An illustrated overview of topic modeling
    in practice can be found in [Figure 3-7](#fig_7_an_overview_of_traditional_topic_modeling).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，主题建模是一种旨在在一组文本数据中寻找潜在主题或主题的技术。对于每个主题，会识别出一组最佳代表该主题含义的关键词或短语。这种技术非常适合在大语料库中寻找共同主题，因为它为相似内容集赋予意义。关于主题建模实践的图示概述可以在[图 3-7](#fig_7_an_overview_of_traditional_topic_modeling)中找到。
- en: Latent Dirichlet Allocation (LDA; blei2003latent) is a classical and popular
    approach to topic modeling that assumes that each topic is characterized by a
    probability distribution over words in a corpus vocabulary. Each document is to
    be considered a mixture of topics. For example, a document about Large Language
    Models might have a high probability of containing words like “BERT”, “self-attention”,
    and “transformers”, while a document about reinforcement learning might have a
    high probability of containing words like “PPO”, “reward”, “rlhf”.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA；blei2003latent）是一种经典且流行的主题建模方法，它假设每个主题由语料库词汇中的单词概率分布特征化。每个文档被视为主题的混合。例如，关于大型语言模型的文档可能高度概率地包含“BERT”、“自注意力”和“变换器”等词，而关于强化学习的文档可能高度概率地包含“PPO”、“奖励”和“rlhf”等词。
- en: '![An overview of traditional topic modeling.](assets/text_clustering_and_topic_modeling_664502_07.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![传统主题建模概述](assets/text_clustering_and_topic_modeling_664502_07.png)'
- en: Figure 3-7\. An overview of traditional topic modeling.
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. 传统主题建模概述。
- en: To this day, the technique is still a staple in many topic modeling use cases,
    and with its strong theoretical background and practical applications, it is unlikely
    to go away soon. However, with the seemingly exponential growth of Large Language
    Models, we start to wonder if we can leverage these Large Language Models in the
    domain of topic modeling.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 至今，这项技术仍然是许多主题建模应用中的基础，凭借其强大的理论背景和实际应用，它不太可能很快消失。然而，随着大型语言模型的似乎呈指数增长，我们开始想知道是否可以在主题建模领域利用这些大型语言模型。
- en: There have been several models adopting Large Language Models for topic modeling,
    like the [embedded topic model](https://github.com/adjidieng/ETM) and the [contextualized
    topic model](https://github.com/MilaNLProc/contextualized-topic-models). However,
    with the rapid developments in natural language processing, these models have
    a hard time keeping up.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有几种模型采用大型语言模型进行主题建模，例如[嵌入式主题模型](https://github.com/adjidieng/ETM)和[上下文化主题模型](https://github.com/MilaNLProc/contextualized-topic-models)。然而，随着自然语言处理领域的快速发展，这些模型难以跟上。
- en: A solution to this problem is BERTopic, a topic modeling technique that leverages
    a highly-flexible and modular architecture. Through this modularity, many newly
    released models can be integrated within its architecture. As the field of Large
    Language Modeling grows, so does BERTopic. This allows for some interesting and
    unexpected ways in which these models can be applied in topic modeling.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的方案是BERTopic，这是一种利用高度灵活和模块化架构的主题建模技术。通过这种模块化，许多新发布的模型可以集成到其架构中。随着大型语言模型领域的发展，BERTopic也在不断发展。这使得这些模型在主题建模中应用的方式变得有趣且意想不到。
- en: BERTopic
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERTopic
- en: BERTopic is a topic modeling technique that assumes that clusters of semantically
    similar documents are a powerful way of generating and describing clusters. The
    documents in each cluster are expected to describe a major theme and combined
    they might represent a topic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic是一种主题建模技术，它假设语义相似文档的集群是生成和描述集群的有效方式。每个集群中的文档预计描述一个主要主题，合在一起可能代表一个主题。
- en: As we have seen with text clustering, a collection of documents in a cluster
    might represent a common theme but the theme itself is not yet described. With
    text clustering, we would have to go through every single document in a cluster
    to understand what the cluster is about. To get to the point where we can call
    a cluster a topic, we need a method for describing that cluster in a condensed
    and human-readable way.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在文本聚类中看到的，集群中的文档集合可能代表一个共同主题，但主题本身尚未被描述。通过文本聚类，我们必须逐一查看集群中的每个文档，以了解该集群的内容。要使一个集群被称为主题，我们需要一种以简洁且易于理解的方式描述该集群的方法。
- en: Although there are quite a few methods for doing so, there is a trick in BERTopic
    that allows it to quickly describe a cluster, and therefore make it a topic, whilst
    generating a highly modular pipeline. The underlying algorithm of BERTopic contains,
    roughly, two major steps.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有很多方法可以做到这一点，但BERTopic中有一个技巧，可以快速描述一个集群，从而将其定义为一个主题，同时生成一个高度模块化的管道。BERTopic的基础算法大致包含两个主要步骤。
- en: First, as we did in our text clustering example, we embed our documents to create
    numerical representations, then reduce their dimensionality and finally cluster
    the reduced embeddings. The result is clusters of semantically similar documents.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，正如我们在文本聚类示例中所做的那样，我们嵌入文档以创建数值表示，然后降低它们的维度，最后对降维后的嵌入进行聚类。结果是语义上相似文档的聚类。
- en: '[Figure 3-8](#fig_8_the_first_part_of_bertopic_s_pipeline_is_clusterin) describes
    the same steps as before, namely using sentence-transformers for embedding the
    documents, UMAP for dimensionality reduction, and HDBSCAN for clustering.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-8](#fig_8_the_first_part_of_bertopic_s_pipeline_is_clusterin)描述了与之前相同的步骤，即使用句子变换器对文档进行嵌入，使用UMAP进行降维，以及使用HDBSCAN进行聚类。'
- en: '*![The first part of BERTopic s pipeline is clustering textual data.](assets/text_clustering_and_topic_modeling_664502_08.png)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*![BERTopic流程的第一部分是对文本数据进行聚类。](assets/text_clustering_and_topic_modeling_664502_08.png)'
- en: 'Figure 3-8\. The first part of BERTopic’s pipeline is clustering textual data.*  *Second,
    we find the best-matching keywords or phrases for each cluster. Most often, we
    would take the centroid of a cluster and find words, phrases, or even sentences
    that might represent it best. There is a disadvantage to this however: we would
    have to continuously keep track of our embeddings, and if we were to have millions
    of documents storing and keeping track becomes computationally difficult. Instead,
    BERTopic uses the classic bag-of-words method to represent the clusters. A bag
    of words is exactly what the name implies, for each document we simply count how
    often a certain word appears and use that as our textual representation.'
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8\. BERTopic流程的第一部分是对文本数据进行聚类。* *第二，我们为每个聚类找到最佳匹配的关键词或短语。通常，我们会取一个聚类的中心点，寻找可能最好地代表它的单词、短语或甚至句子。然而，这有一个缺点：我们必须持续跟踪我们的嵌入，如果我们有数百万个文档，存储和跟踪变得计算上困难。相反，BERTopic使用经典的词袋方法来表示聚类。词袋正是名称所暗示的，对于每个文档，我们简单地计算某个单词出现的频率，并将其用作我们的文本表示。
- en: However, words like “the”, “and”, and “I” appear quite frequently in most English
    texts and are likely to be overrepresented. To give proper weight to these words,
    BERTopic uses a technique called c-TF-IDF, which stands for class-based term-frequency
    inverse-document frequency. c-TF-IDF is a class-based adaptation of the classic
    TF-IDF procedure. Instead of considering the importance of words within documents,
    c-TF-IDF considers the importance of words between clusters of documents.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，“the”、“and”和“I”等词在大多数英语文本中出现相当频繁，可能会被过度代表。为了给予这些词适当的权重，BERTopic使用了一种称为c-TF-IDF的技术，意为基于类别的词频逆文档频率。c-TF-IDF是经典TF-IDF过程的类别适应版本。与考虑文档内单词的重要性不同，c-TF-IDF考虑的是文档聚类之间单词的重要性。
- en: To use c-TF-IDF, we first concatenate each document in a cluster to generate
    one long document. Then, we extract the frequency of the term *f_x* in class *c*,
    where *c* refers to one of the clusters we created before. Now we have, per cluster,
    how many and which words they contain, a mere count.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用c-TF-IDF，我们首先将聚类中的每个文档连接成一个长文档。然后，我们提取类别*c*中术语*f_x*的频率，其中*c*指的是我们之前创建的聚类之一。现在我们可以知道每个聚类中包含多少个和哪些单词，仅仅是一个计数。
- en: To weight this count, we take the logarithm of one plus the average number of
    words per cluster *A* divided by the frequency of term *x* across all clusters.
    Plus one is added within the logarithm to guarantee positive values which is also
    often done within TF-IDF.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加权这个计数，我们取一个加一后的聚类中平均单词数*A*的对数，然后除以所有聚类中术语*x*的频率。加一是在对数内添加的，以确保得到正值，这在TF-IDF中也是常见的做法。
- en: As shown in [Figure 3-9](#fig_9_the_second_part_of_bertopic_s_pipeline_is_represen),
    the c-TF-IDF calculation allows us to generate, for each word in a cluster, a
    weight corresponding to that cluster. As a result, we generate a topic-term matrix
    for each topic that describes the most important words they contain. It is essentially
    a ranking of a corpus’ vocabulary in each topic.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图3-9](#fig_9_the_second_part_of_bertopic_s_pipeline_is_represen)所示，c-TF-IDF计算使我们能够为每个聚类中的单词生成一个对应于该聚类的权重。因此，我们为每个主题生成一个主题-词矩阵，描述它们所包含的最重要的单词。它本质上是每个主题中语料库词汇的排名。
- en: '*![The second part of BERTopic s pipeline is representing the topics. The calculation
    of the weight of term  x  in a class  c . ](assets/text_clustering_and_topic_modeling_664502_09.png)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*![BERTopic流程的第二部分是表示主题。术语x在类别c中的权重计算。](assets/text_clustering_and_topic_modeling_664502_09.png)'
- en: Figure 3-9\. The second part of BERTopic’s pipeline is representing the topics.
    The calculation of the weight of term *x* in a class *c*.*  *Putting the two steps
    together, clustering and representing topics, results in the full pipeline of
    BERTopic, as illustrated in [Figure 3-10](#fig_10_the_full_pipeline_of_bertopic_roughly_consists_o).
    With this pipeline, we can cluster semantically similar documents and from the
    clusters generate topics represented by several keywords. The higher the weight
    of a keyword for a topic, the more representative it is of that topic.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. BERTopic管道的第二部分是表示主题。计算术语*x*在类别*c*中的权重。*将这两个步骤结合起来，即聚类和表示主题，形成了BERTopic的完整管道，如[图3-10](#fig_10_the_full_pipeline_of_bertopic_roughly_consists_o)所示。通过这个管道，我们可以对语义相似的文档进行聚类，并从这些聚类生成由多个关键词表示的主题。关键词对主题的权重越高，它就越能代表该主题。
- en: '![The full pipeline of BERTopic  roughly  consists of two steps  clustering
    and topic representation.](assets/text_clustering_and_topic_modeling_664502_10.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![BERTopic的完整管道大致由两个步骤：聚类和主题表示。](assets/text_clustering_and_topic_modeling_664502_10.png)'
- en: Figure 3-10\. The full pipeline of BERTopic, roughly, consists of two steps,
    clustering and topic representation.
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. BERTopic的完整管道大致由两个步骤组成：聚类和主题表示。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Interestingly, the c-TF-IDF trick does not use a Large Language Model and therefore
    does not take the context and semantic nature of words into account. However,
    like with neural search, it allows for an efficient starting point after which
    we can use the more compute-heavy techniques, such as GPT-like models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，c-TF-IDF技巧不使用大型语言模型，因此不考虑单词的上下文和语义特性。然而，就像神经搜索一样，它提供了一个高效的起点，之后我们可以使用计算量较大的技术，例如类似GPT的模型。
- en: One major advantage of this pipeline is that the two steps, clustering and topic
    representation, are relatively independent of one another. When we generate our
    topics using c-TF-IDF, we do not use the models from the clustering step, and,
    for example, do not need to track the embeddings of every single document. As
    a result, this allows for significant modularity not only with respect to the
    topic generation process but the entire pipeline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道的一个主要优点是这两个步骤，聚类和主题表示，相对独立。当我们使用c-TF-IDF生成主题时，不使用聚类步骤的模型，例如，不需要跟踪每个文档的嵌入。因此，这为主题生成过程以及整个管道提供了显著的模块化。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: With clustering, each document is assigned to only a single cluster or topic.
    In practice, documents might contain multiple topics, and assigning a multi-topic
    document to a single topic would not always be the most accurate method. We will
    go into this later, as BERTopic has a few ways of handling this, but it is important
    to understand that at its core, topic modeling with BERTopic is a clustering task.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类过程中，每个文档仅分配到一个单一的聚类或主题。在实践中，文档可能包含多个主题，将多主题文档分配到单一主题并不总是最准确的方法。我们稍后会深入讨论这一点，因为BERTopic有几种处理方法，但理解BERTopic的主题建模本质上是一项聚类任务是很重要的。
- en: The modular nature of BERTopic’s pipeline is extensible to every component.
    Although sentence-transformers are used as a default embedding model for transforming
    documents to numerical representations, nothing is stopping us from using any
    other embedding technique. The same applies to the dimensionality reduction, clustering,
    and topic generation process. Whether a use case calls for k-Means instead of
    HDBSCAN, and PCA instead of UMAP, anything is possible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic管道的模块化特性可以扩展到每个组件。尽管句子变换器作为默认嵌入模型用于将文档转换为数值表示，但我们并不受限于使用任何其他嵌入技术。维度减少、聚类和主题生成过程同样适用。无论用例是选择k-Means而不是HDBSCAN，还是选择PCA而不是UMAP，都是可能的。
- en: You can think of this modularity as building with lego blocks, each part of
    the pipeline is completely replaceable with another, similar algorithm. This “lego
    block” way of thinking is illustrated in [Figure 3-11](#fig_11_the_modularity_of_bertopic_is_a_key_component_and).
    The figure also shows an additional algorithmic lego block that we can use. Although
    we use c-TF-IDF to create our initial topic representations, there are a number
    of interesting ways we can use LLMs to fine-tune these representations. In the
    “**Representation Models**” section below, we will go into extensive detail on
    how this algorithmic lego block works.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把这种模块化看作是用乐高积木构建，管道的每个部分都可以完全替换为另一个类似的算法。这种“乐高积木”思维方式在[图3-11](#fig_11_the_modularity_of_bertopic_is_a_key_component_and)中得到了说明。该图还展示了我们可以使用的一个额外的算法乐高块。尽管我们使用c-TF-IDF来创建初始主题表示，但还有许多有趣的方法可以利用LLMs来微调这些表示。在下面的“**表示模型**”部分，我们将详细探讨这个算法乐高块的工作原理。
- en: '![The modularity of BERTopic is a key component and allows you to build your
    own topic model whoever you want.](assets/text_clustering_and_topic_modeling_664502_11.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![BERTopic的模块化是一个关键组件，允许你根据需要构建自己的主题模型。](assets/text_clustering_and_topic_modeling_664502_11.png)'
- en: Figure 3-11\. The modularity of BERTopic is a key component and allows you to
    build your own topic model whoever you want.
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-11\. BERTopic的模块化是一个关键组件，允许你根据需要构建自己的主题模型。
- en: Code Overview
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码概述
- en: 'Enough talk! This is a hands-on book, so it is finally time for some hands-on
    coding. The default pipeline, as illustrated previously in [Figure 3-10](#fig_10_the_full_pipeline_of_bertopic_roughly_consists_o),
    only requires a few lines of code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，开始动手吧！这是一本实践型书籍，现在是时候进行一些实际编码了。默认管道，如之前在[图3-10](#fig_10_the_full_pipeline_of_bertopic_roughly_consists_o)中所示，只需要几行代码：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, the modularity that BERTopic is known for and that we have visualized
    thus far can also be visualized through a coding example. First, let us import
    some relevant packages:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BERTopic的模块化特性以及我们迄今为止可视化的内容，也可以通过编码示例进行可视化。首先，让我们导入一些相关的包：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you might have noticed, most of the imports, like UMAP and HDBSCAN, are
    part of the default BERTopic pipeline. Next, let us build the default pipeline
    of BERTopic a bit more explicitly and go each individual step:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，大多数导入的包，如UMAP和HDBSCAN，是默认BERTopic管道的一部分。接下来，让我们更明确地构建BERTopic的默认管道，逐步进行每个个体步骤：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code allows us to go through all steps of the algorithm explicitly and
    essentially let us build the topic model however we want. The resulting topic
    model, as defined in the variable `topic_model`, now represents the base pipeline
    of BERTopic as illustrated back in [Figure 3-10](#fig_10_the_full_pipeline_of_bertopic_roughly_consists_o).**  **##
    Example
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使我们能够明确地经历算法的所有步骤，并且基本上让我们以任何我们想要的方式构建主题模型。所得到的主题模型，在变量`topic_model`中定义，现已代表BERTopic的基本管道，如之前在[图3-10](#fig_10_the_full_pipeline_of_bertopic_roughly_consists_o)中所示。**  **##
    示例
- en: We are going to keep using the abstracts of ArXiv articles throughout this use
    case. To recap what we did with text clustering, we start by importing our dataset
    using HuggingFace’s dataset package and extracting metadata that we are going
    to use later on, like the abstracts, years, and categories of the articles.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个使用案例中，我们将继续使用ArXiv文章的摘要。为了回顾我们在文本聚类中所做的工作，我们开始使用HuggingFace的数据集包导入数据集，并提取我们稍后要使用的元数据，如摘要、年份和文章类别。
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Using BERTopic is quite straightforward, and it can be used in just three lines:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BERTopic非常简单，只需三行代码即可完成：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With this pipeline, you will have 3 variables returned, namely `topic_model`,
    `topics`, and `probs`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个管道，你将获得3个返回变量，即`topic_model`、`topics`和`probs`：
- en: '`topic_model` is the model that we have just trained before and contains information
    about the model and the topics that we created.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topic_model`是我们刚刚训练的模型，包含有关模型和我们创建的主题的信息。'
- en: '`topics` are the topics for each abstract.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topics`是每个摘要的主题。'
- en: '`probs` are the probabilities that a topic belongs to a certain abstract.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`probs`是某个主题属于特定摘要的概率。'
- en: Before we start to explore our topic model, there is one change that we will
    need to make the results reproducible. As mentioned before, one of the underlying
    models of BERTopic is UMAP. This model is stochastic in nature which means that
    every time we run BERTopic, we will get different results. We can prevent this
    by passing a `random_state` to the UMAP model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索主题模型之前，有一个变化需要使结果可复现。如前所述，BERTopic的一个基础模型是UMAP。这个模型具有随机性，这意味着每次运行BERTopic时，我们都会得到不同的结果。我们可以通过将`random_state`传递给UMAP模型来防止这种情况。
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let’s start by exploring the topics that were created. The `get_topic_info()`
    method is useful to get a quick description of the topics that we found:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始探索创建的主题。`get_topic_info()`方法可以快速描述我们找到的主题：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are many topics generated from our model, **XXX**! Each of these topics
    is represented by several keywords, which are concatenated with a “_” in the Name
    column. This Name column allows us to quickly get a feeling of what the topic
    is about as it shows the four keywords that best represent it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的模型中生成了许多主题，**XXX**！每个主题由几个关键字表示，这些关键字在名称列中用“_”连接。这个名称列使我们能够快速了解主题内容，因为它显示了最能代表该主题的四个关键字。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You might also have noticed that the very first topic is labeled -1\. That topic
    contains all documents that could not be fitted within a topic and are considered
    to be outliers. This is a result of the clustering algorithm, HDBSCAN, that does
    not force all points to be clustered. To remove outliers, we could either use
    a non-outlier algorithm like k-Means or use BERTopic’s `reduce_outliers()` function
    to remove some of the outliers and assign them to topics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也注意到第一个主题标记为-1。这个主题包含所有无法归入某个主题的文档，并被视为离群值。这是聚类算法HDBSCAN的结果，它并不强制所有点都被聚类。为了去除离群值，我们可以使用非离群算法，如k-Means，或使用BERTopic的`reduce_outliers()`函数去除一些离群值并将它们分配给主题。
- en: 'For example, topic 2 contains the keywords “summarization”, “summaries”, “summary”,
    and “abstractive”. Based on these keywords, it seems that the topic is summarization
    tasks. To get the top 10 keywords per topic as well as their c-TF-IDF weights,
    we can use the get_topic() function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，主题2包含关键字“summarization”、“summaries”、“summary”和“abstractive”。根据这些关键字，似乎这个主题是关于总结任务的。为了获取每个主题的前10个关键字及其c-TF-IDF权重，我们可以使用get_topic()函数：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This gives us a bit more context about the topic and helps us understand what
    the topic is about. For example, it is interesting to see the word “rogue” appear
    since that is a common metric for evaluating summarization models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了更多关于主题的背景，有助于我们理解主题的内容。例如，看到“rogue”这个词出现是很有趣的，因为这是评估摘要模型的一个常见指标。
- en: 'We can use the `find_topics()` function to search for specific topics based
    on a search term. Let’s search for a topic about topic modeling:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`find_topics()`函数根据搜索词搜索特定主题。让我们搜索一个关于主题建模的主题：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It returns that topic 17 has a relatively high similarity (0.675) with our
    search term. If we then inspect the topic, we can see that it is indeed a topic
    about topic modeling:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回主题17与我们的搜索词具有相对较高的相似度（0.675）。如果我们检查该主题，可以看到它确实是关于主题建模的主题：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Although we know that his topic is about topic modeling, let us see if the
    BERTopic abstract is also assigned to this topic:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们知道这个主题是关于主题建模的，但让我们看看BERTopic的摘要是否也分配给了这个主题：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It is! It seems that the topic is not just about LDA-based methods but also
    cluster-based techniques, like BERTopic.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！看起来这个主题不仅涉及基于LDA的方法，还有基于聚类的技术，比如BERTopic。
- en: Lastly, we mentioned before that many topic modeling techniques assume that
    there can be multiple topics within a single document or even a sentence. Although
    BERTopic leverages clustering, which assumes a single assignment to each data
    point, it can approximate the topic distribution.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们之前提到许多主题建模技术假设一个文档甚至一句话中可能包含多个主题。尽管BERTopic利用聚类，这假设每个数据点只有一个分配，但它可以近似主题分布。
- en: 'We can use this technique to see what the topic distribution is of the first
    sentence in the BERTopic paper:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这种技术查看BERTopic论文第一句话的主题分布：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![A wide range of visualization options are available in BERTopic. ](assets/text_clustering_and_topic_modeling_664502_12.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![BERTopic中提供了多种可视化选项。](assets/text_clustering_and_topic_modeling_664502_12.png)'
- en: Figure 3-12\. A wide range of visualization options are available in BERTopic.
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12\. BERTopic中提供了多种可视化选项。
- en: The output, as shown in [Figure 3-12](#fig_12_a_wide_range_of_visualization_options_are_availabl),
    demonstrates that the document, to a certain extent, contains multiple topics.
    This assignment is even done on a token level!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图3-12](#fig_12_a_wide_range_of_visualization_options_are_availabl)所示的输出表明，文档在一定程度上包含多个主题。此分配甚至是在令牌级别上完成的！
- en: (Interactive) Visualizations
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （互动）可视化
- en: Going through **XXX** topics manually can be quite a task. Instead, several
    helpful visualization functions allow us to get a broad overview of the topics
    that were generated. Many of which are interactive by using the Plotly visualization
    framework.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 手动处理**XXX**主题可能是一项艰巨的任务。相反，多个有用的可视化功能让我们能够广泛了解生成的主题。其中许多使用Plotly可视化框架进行互动。
- en: '[Figure 3-13](#fig_13_a_wide_range_of_visualization_options_are_availabl)shows
    all possible visualization options in BERTopic, from 2D document representations
    and topic bar charts to topic hierarchy and similarity. Although we are not going
    through all visualizations, there are some worth looking into.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-13](#fig_13_a_wide_range_of_visualization_options_are_availabl)显示了BERTopic中所有可能的可视化选项，从二维文档表示和主题条形图到主题层次和相似性。虽然我们没有逐一介绍所有可视化，但有些值得关注。'
- en: '![A wide range of visualization options are available in BERTopic. ](assets/text_clustering_and_topic_modeling_664502_13.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![BERTopic中可用多种可视化选项。](assets/text_clustering_and_topic_modeling_664502_13.png)'
- en: Figure 3-13\. A wide range of visualization options are available in BERTopic.
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13。BERTopic中可用多种可视化选项。
- en: To start, we can create a 2D representation of our topics by using UMAP to reduce
    the c-TF-IDF representations of each topic.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过使用UMAP来减少每个主题的c-TF-IDF表示，创建主题的二维表示。
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![The intertopic distance map of topics represented in 2D space.](assets/text_clustering_and_topic_modeling_664502_14.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![主题在二维空间中的主题间距离图。](assets/text_clustering_and_topic_modeling_664502_14.png)'
- en: Figure 3-14\. The intertopic distance map of topics represented in 2D space.
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14。主题在二维空间中的主题间距离图。
- en: As shown in [Figure 3-14](#fig_14_the_intertopic_distance_map_of_topics_represented),
    this generates an interactive visualization that, when hovering over a circle,
    allows us to see the topic, its keywords, and its size. The larger the circle
    of a topic is, the more documents it contains. We can quickly see groups of similar
    topics through interaction with this visualization.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图3-14](#fig_14_the_intertopic_distance_map_of_topics_represented)所示，这生成了一个互动可视化，当鼠标悬停在一个圆圈上时，我们可以看到主题、其关键词及其大小。主题的圆圈越大，包含的文档越多。通过与此可视化的交互，我们可以快速看到相似主题的组。
- en: We can use the `visualize_documents()` function to take this analysis to the
    next level, namely analyzing topics on a document level.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`visualize_documents()`函数将分析提升到另一个层次，即在文档层面分析主题。
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Abstracts and their topics are represented in a 2D visualization.](assets/text_clustering_and_topic_modeling_664502_15.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![摘要及其主题在二维可视化中表示。](assets/text_clustering_and_topic_modeling_664502_15.png)'
- en: Figure 3-15\. Abstracts and their topics are represented in a 2D visualization.
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15。摘要及其主题在二维可视化中表示。
- en: '[Figure 3-15](#fig_15_abstracts_and_their_topics_are_represented_in_a_2d) demonstrates
    how BERTopic can visualize documents in a 2D-space.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-15](#fig_15_abstracts_and_their_topics_are_represented_in_a_2d)演示了BERTopic如何在二维空间中可视化文档。'
- en: Note
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We only visualized a selection of topics since showing all 300 topics would
    result in quite a messy visualization. Also, instead of passing `abstracts`, we
    passed `titles` since we only want to view the titles of each paper when we hover
    over a document and not the entire abstract.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只可视化了部分主题，因为显示所有300个主题会导致可视化变得相当杂乱。此外，我们传递的是`titles`而不是`abstracts`，因为我们只想在鼠标悬停在文档上时查看每篇论文的标题，而不是整个摘要。
- en: 'Lastly, we can create a bar chart of the keywords in a selection of topics
    using visualize_barchart():'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用visualize_barchart()创建一个关键词的条形图，基于一部分主题：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![The top 5 keywords for the first 8 topics. ](assets/text_clustering_and_topic_modeling_664502_16.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![前8个主题的前5个关键词。](assets/text_clustering_and_topic_modeling_664502_16.png)'
- en: Figure 3-16\. The top 5 keywords for the first 8 topics.
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-16。前8个主题的前5个关键词。
- en: The bar chart in [Figure 3-16](#fig_16_the_top_5_keywords_for_the_first_8_topics)
    gives a nice indication of which keywords are most important to a specific topic.
    Take topic 2 for example–it seems that the word “summarization” is most representative
    of that topic and that other words are very similar in importance.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-16](#fig_16_the_top_5_keywords_for_the_first_8_topics) 中的柱状图很好地指示了哪些关键词对特定主题最重要。以主题
    2 为例——似乎单词“总结”最能代表该主题，而其他单词在重要性上非常相似。'
- en: Representation Models
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示模型
- en: With the neural-search style modularity that BERTopic employs, it can leverage
    many different types of Large Language Models whilst minimizing computing. This
    allows for a large range of topic fine-tuning methods, from part-of-speech to
    text-generation methods, like ChatGPT. [Figure 3-17](#fig_17_after_applying_the_c_tf_idf_weighting_topics_can)
    demonstrates the variety of LLMs that we can leverage to fine-tune topic representations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 借助 BERTopic 采用的神经搜索风格的模块化，它可以利用多种不同类型的大型语言模型，同时最小化计算。这使得各种主题微调方法得以实现，从词性标注到文本生成方法，例如
    ChatGPT。[图 3-17](#fig_17_after_applying_the_c_tf_idf_weighting_topics_can)展示了我们可以利用来微调主题表示的各种
    LLM。
- en: '![After applying the c TF IDF weighting  topics can be fine tuned with a wide
    variety of representation models. Many of which are Large Language Models. ](assets/text_clustering_and_topic_modeling_664502_17.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![在应用 c TF IDF 权重后，主题可以使用多种表示模型进行微调。其中许多是大型语言模型。](assets/text_clustering_and_topic_modeling_664502_17.png)'
- en: Figure 3-17\. After applying the c-TF-IDF weighting, topics can be fine-tuned
    with a wide variety of representation models. Many of which are Large Language
    Models.
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-17。在应用 c-TF-IDF 权重后，可以使用多种表示模型对主题进行微调。其中许多是大型语言模型。
- en: Topics generated with c-TF-IDF serve as a good first ranking of words with respect
    to their topic. In this section, these initial rankings of words can be considered
    candidate keywords for a topic as we might change their rankings based on any
    representation model. We will go through several representation models that can
    be used within BERTopic and that are also interesting from a Large Language Modeling
    standpoint.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 c-TF-IDF 生成的主题是与其主题相关的单词的良好初步排名。在本节中，这些单词的初步排名可以视为主题的候选关键词，因为我们可能会根据任何表示模型来改变它们的排名。我们将介绍几种可以在
    BERTopic 中使用的表示模型，并且从大型语言模型的角度来看，这些模型也非常有趣。
- en: 'Before we start, we first need to do two things. First, we are going to save
    our original topic representations so that it will be much easier to compare with
    and without representation models:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们首先需要做两件事。第一，我们将保存原始主题表示，这样与有无表示模型进行比较时将更容易：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Second, let’s create a short wrapper that we can use to quickly visualize the
    differences in topic words to compare with and without representation models:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，让我们创建一个简短的包装，以便快速可视化主题词的差异，以便比较有无表示模型的情况：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: KeyBERTInspired
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KeyBERTInspired
- en: c-TF-IDF generated topics do not consider the semantic nature of words in a
    topic which could end up creating topics with stopwords. We can use the module
    **bertopic.representation****_model.KeyBERTInspired****()** to fine-tune the topic
    keywords based on their semantic similarity to the topic.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: c-TF-IDF 生成的主题并未考虑主题中单词的语义性质，这可能导致生成包含停用词的主题。我们可以使用模块 **bertopic.representation_model.KeyBERTInspired()**
    根据关键词与主题的语义相似性来微调主题关键词。
- en: KeyBERTInspired is, as you might have guessed, a method inspired by the [keyword
    extraction package, KeyBERT](https://github.com/MaartenGr/KeyBERT). In its most
    basic form, KeyBERT compares the embeddings of words in a document with the document
    embedding using cosine similarity to see which words are most related to the document.
    These most similar words are considered keywords.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: KeyBERTInspired 是一种方法，正如你可能猜到的，灵感来自于 [关键词提取包 KeyBERT](https://github.com/MaartenGr/KeyBERT)。在最基本的形式中，KeyBERT
    通过余弦相似度比较文档中单词的嵌入与文档嵌入，以查看哪些单词与文档最相关。这些最相似的单词被视为关键词。
- en: In BERTopic, we want to use something similar but on a topic level and not a
    document level. As shown in [Figure 3-18](#fig_18_the_procedure_of_the_keybertinspired_representatio),
    KeyBERTInspired uses c-TF-IDF to create a set of representative documents per
    topic by randomly sampling 500 documents per topic, calculating their c-TF-IDF
    values, and finding the most representative documents. These documents are embedded
    and averaged to be used as an updated topic embedding. Then, the similarity between
    our candidate keywords and the updated topic embedding is calculated to re-rank
    our candidate keywords.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERTopic中，我们希望使用类似的方法，但在主题层面而不是文档层面。如[图3-18](#fig_18_the_procedure_of_the_keybertinspired_representatio)所示，KeyBERTInspired使用c-TF-IDF为每个主题创建一组代表性文档，方法是随机抽取每个主题的500个文档，计算它们的c-TF-IDF值，并找到最具代表性的文档。这些文档被嵌入并平均，用作更新后的主题嵌入。然后，计算我们的候选关键词与更新后的主题嵌入之间的相似度，以重新排序我们的候选关键词。
- en: '![The procedure of the KeyBERTInspired representation model](assets/text_clustering_and_topic_modeling_664502_18.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![KeyBERTInspired表示模型的过程](assets/text_clustering_and_topic_modeling_664502_18.png)'
- en: Figure 3-18\. The procedure of the KeyBERTInspired representation model
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-18\. KeyBERTInspired表示模型的过程
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`Topic: 0 question | qa | questions | answer | answering --> questionanswering
    | answering | questionanswer | attention | retrieval`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 0 问题 | qa | 问题 | 答案 | 回答 --> 问答 | 回答 | 问答 | 注意 | 检索`'
- en: '`Topic: 1 hate | offensive | speech | detection | toxic --> hateful | hate
    | cyberbullying | speech | twitter`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 1 仇恨 | 攻击性 | 言论 | 检测 | 有毒 --> 仇恨的 | 仇恨 | 网络欺凌 | 言论 | 推特`'
- en: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    summarizers | summarizer | summarization | summarisation | summaries`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 2 摘要 | 总结 | 总结 | 抽象 | 提取 --> 摘要生成器 | 摘要生成 | 摘要 | 摘要 | 总结`'
- en: '`Topic: 3 parsing | parser | dependency | amr | parsers --> parsers | parsing
    | treebanks | parser | treebank`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 3 解析 | 解析器 | 依赖 | amr | 解析器 --> 解析器 | 解析 | 树库 | 解析器 | 树库`'
- en: '`Topic: 4 word | embeddings | embedding | similarity | vectors --> word2vec
    | embeddings | embedding | similarity | semantic`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 4 词 | 嵌入 | 嵌入 | 相似性 | 向量 --> word2vec | 嵌入 | 嵌入 | 相似性 | 语义`'
- en: '`Topic: 5 gender | bias | biases | debiasing | fairness --> bias | biases |
    genders | gender | gendered`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 5 性别 | 偏见 | 偏差 | 去偏见 | 公平 --> 偏见 | 偏差 | 性别 | 性别 | 性别化`'
- en: '`Topic: 6 relation | extraction | re | relations | entity --> relations | relation
    | entities | entity | relational`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 6 关系 | 提取 | re | 关系 | 实体 --> 关系 | 关系 | 实体 | 实体 | 关系的`'
- en: '`Topic: 7 prompt | fewshot | prompts | incontext | tuning --> prompttuning
    | prompts | prompt | prompting | promptbased`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 7 提示 | 少量实例 | 提示 | 上下文 | 调整 --> 提示调整 | 提示 | 提示 | 提示中 | 基于提示`'
- en: '`Topic: 8 aspect | sentiment | absa | aspectbased | opinion --> sentiment |
    aspect | aspects | aspectlevel | sentiments`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 8 方面 | 情感 | absa | 基于方面 | 意见 --> 情感 | 方面 | 方面 | 方面级别 | 情感`'
- en: '`Topic: 9 explanations | explanation | rationales | rationale | interpretability
    --> explanations | explainers | explainability | explaining | attention`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 9 解释 | 解释 | 理由 | 理由 | 可解释性 --> 解释 | 解释者 | 可解释性 | 解释 | 注意`'
- en: The updated model shows that the topics are much easier to read compared to
    the original model. It also shows the downside of using embedding-based techniques.
    Words in the original model, like “amr” and “qa” are perfectly reasonable words
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的模型显示，与原始模型相比，主题的可读性大大提高。同时，它也显示了使用基于嵌入技术的缺点。原始模型中的词汇，例如“amr”和“qa”，都是合理的词汇。
- en: Part-of-Speech
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性
- en: c-TF-IDF does not make any distinction of the type of words it deems to be important.
    Whether it is a noun, verb, adjective, or even a preposition, they can all end
    up as important keywords. When we want to have human-readable labels that are
    straightforward and intuitive to interpret, we might want topics that are described
    by, for example, nouns only.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: c-TF-IDF并不区分其认为重要的词的类型。无论是名词、动词、形容词，甚至是介词，它们都可能成为重要关键词。当我们希望有易于人类理解的标签，简单直观时，我们可能希望主题仅由名词来描述。
- en: This is where the well-known SpaCy package comes in. An industrial-grade NLP
    framework that comes with a variety of pipelines, models, and deployment options.
    More specifically, we can use SpaCy to load in an English model that is capable
    of detecting part of speech, whether a word is a noun, verb, or something else.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里就是著名的SpaCy包派上用场的地方。这是一个工业级的自然语言处理框架，提供多种管道、模型和部署选项。更具体地说，我们可以使用SpaCy加载一个能够检测词性（无论是名词、动词还是其他）的英语模型。
- en: As shown in [Figure 3-19](#fig_19_the_procedure_of_the_partofspeech_representation_m),
    we can use SpaCy to make sure that only nouns end up in our topic representations.
    As with most representation models, this is highly efficient since the nouns are
    extracted from only a small but representative subset of the data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3-19](#fig_19_the_procedure_of_the_partofspeech_representation_m)所示，我们可以使用
    SpaCy 确保只有名词进入我们的主题表示。与大多数表示模型一样，这种方法非常高效，因为名词仅从一个小而具有代表性的数据子集提取。
- en: '![The procedure of the PartOfSpeech representation model](assets/text_clustering_and_topic_modeling_664502_19.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![词性表示模型的过程](assets/text_clustering_and_topic_modeling_664502_19.png)'
- en: Figure 3-19\. The procedure of the PartOfSpeech representation model
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-19\. 词性表示模型的过程
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Topic: 0 question | qa | questions | answer | answering --> question | questions
    | answer | answering | answers`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：0 问题 | qa | 问题 | 答案 | 回答 --> 问题 | 问题 | 答案 | 回答 | 答案`'
- en: '`Topic: 1 hate | offensive | speech | detection | toxic --> hate | offensive
    | speech | detection | toxic`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：1 仇恨 | 冒犯 | 言论 | 检测 | 有毒 --> 仇恨 | 冒犯 | 言论 | 检测 | 有毒`'
- en: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    summarization | summaries | summary | abstractive | extractive`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：2 摘要 | 摘要 | 总结 | 抽象 | 提取 --> 摘要 | 摘要 | 总结 | 抽象 | 提取`'
- en: '`Topic: 3 parsing | parser | dependency | amr | parsers --> parsing | parser
    | dependency | parsers | treebank`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：3 解析 | 解析器 | 依赖 | amr | 解析器 --> 解析 | 解析器 | 依赖 | 解析器 | 树库`'
- en: '`Topic: 4 word | embeddings | embedding | similar`ity | vectors --> word |
    embeddings | similarity | vectors | words'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：4 单词 | 嵌入 | 嵌入 | 相似性 | 向量 --> 单词 | 嵌入 | 相似性 | 向量 | 单词`'
- en: '`Topic: 5 gender | bias | biases | debiasing | fairness --> gender | bias |
    biases | debiasing | fairness`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：5 性别 | 偏见 | 偏见 | 去偏见 | 公平 --> 性别 | 偏见 | 偏见 | 去偏见 | 公平`'
- en: '`Topic: 6 relation | extraction | re | relations | entity --> relation | extraction
    | relations | entity | distant`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：6 关系 | 提取 | re | 关系 | 实体 --> 关系 | 提取 | 关系 | 实体 | 远程`'
- en: '`Topic: 7 prompt | fewshot | prompts | incontext | tuning --> prompt | prompts
    | tuning | prompting | tasks`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：7 提示 | 少样本 | 提示 | 上下文 | 调整 --> 提示 | 提示 | 调整 | 提示 | 任务`'
- en: '`Topic: 8 aspect | sentiment | absa | aspectbased | opinion --> aspect | sentiment
    | opinion | aspects | polarity`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：8 方面 | 情感 | absa | 基于方面 | 意见 --> 方面 | 情感 | 意见 | 方面 | 极性`'
- en: '`Topic: 9 explanations | explanation | rationales | rationale | interpretability
    --> explanations | explanation | rationales | rationale | interpretability`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：9 解释 | 解释 | 理由 | 理由 | 可解释性 --> 解释 | 解释 | 理由 | 理由 | 可解释性`'
- en: Maximal Marginal Relevance
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大边际相关性
- en: With c-TF-IDF, there can be a lot of redundancy in the resulting keywords as
    it does not consider words like “car” and “cars” to be essentially the same thing.
    In other words, we want sufficient diversity in the resulting topics with as little
    repetition as possible. ([Figure 3-20](#fig_20_the_procedure_of_the_maximal_marginal_relevance_re))
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 c-TF-IDF，生成的关键词可能会有很多冗余，因为它不认为“车”和“汽车”本质上是相同的。换句话说，我们希望生成的主题具有足够的多样性，同时尽可能少重复。（[图
    3-20](#fig_20_the_procedure_of_the_maximal_marginal_relevance_re)）
- en: '*![The procedure of the Maximal Marginal Relevance representation model. The
    diversity of the resulting keywords is represented by lambda    .](assets/text_clustering_and_topic_modeling_664502_20.png)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*![最大边际相关性表示模型的过程。生成关键词的多样性由 λ 表示。](assets/text_clustering_and_topic_modeling_664502_20.png)*'
- en: Figure 3-20\. The procedure of the Maximal Marginal Relevance representation
    model. The diversity of the resulting keywords is represented by lambda (λ).*  *We
    can use an algorithm, called Maximal Marginal Relevance (MMR) to diversify our
    topic representations. The algorithm starts with the best matching keyword to
    a topic and then iteratively calculates the next best keyword whilst taking a
    certain degree of diversity into account. In other words, it takes a number of
    candidate topic keywords, for example, 30, and tries to pick the top 10 keywords
    that are best representative of the topic but are also diverse from one another.
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-20\. 最大边际相关性表示模型的过程。生成关键词的多样性由 λ（λ）表示。*我们可以使用一种名为最大边际相关性（MMR）的算法来使我们的主题表示多样化。该算法从与主题最匹配的关键词开始，然后迭代计算下一个最佳关键词，同时考虑一定程度的多样性。换句话说，它会取一些候选主题关键词，例如30个，并尝试选择最佳代表主题的前10个关键词，同时确保它们彼此多样化。*
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Topic: 0 question | qa | questions | answer | answering --> qa | questions
    | answering | comprehension | retrieval`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 0 问题 | QA | 问题 | 回答 | 回答中 --> QA | 问题 | 回答 | 理解 | 检索`'
- en: '`Topic: 1 hate | offensive | speech | detection | toxic --> speech | abusive
    | toxicity | platforms | hateful`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 1 仇恨 | 冒犯 | 演讲 | 检测 | 有毒 --> 演讲 | 侮辱 | 毒性 | 平台 | 仇恨`'
- en: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    summarization | extractive | multidocument | documents | evaluation`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 2 总结 | 摘要 | 总结 | 抽象 | 提取 --> 总结 | 提取 | 多文档 | 文档 | 评估`'
- en: '`Topic: 3 parsing | parser | dependency | amr | parsers --> amr | parsers |
    treebank | syntactic | constituent`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 3 解析 | 解析器 | 依赖 | AMR | 解析器 --> AMR | 解析器 | 语料库 | 句法 | 成分`'
- en: '`Topic: 4 word | embeddings | embedding | similarity | vectors --> embeddings
    | similarity | vector | word2vec | glove`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 4 词 | 嵌入 | 嵌入 | 相似性 | 向量 --> 嵌入 | 相似性 | 向量 | word2vec | glove`'
- en: '`Topic: 5 gender | bias | biases | debiasing | fairness --> gender | bias |
    fairness | stereotypes | embeddings`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 5 性别 | 偏见 | 偏见 | 去偏见 | 公平 --> 性别 | 偏见 | 公平 | 刻板印象 | 嵌入`'
- en: '`Topic: 6 relation | extraction | re | relations | entity --> extraction |
    relations | entity | documentlevel | docre`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 6 关系 | 提取 | 关系 | 实体 --> 提取 | 关系 | 实体 | 文档级 | 文档提取`'
- en: '`Topic: 7 prompt | fewshot | prompts | incontext | tuning --> prompts | zeroshot
    | plms | metalearning | label`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 7 提示 | 少样本 | 提示 | 上下文 | 调整 --> 提示 | 零样本 | PLMs | 元学习 | 标签`'
- en: '`Topic: 8 aspect | sentiment | absa | aspectbased | opinion --> sentiment |
    absa | aspects | extraction | polarities`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 8 方面 | 情感 | ABSA | 基于方面 | 观点 --> 情感 | ABSA | 方面 | 提取 | 极性`'
- en: '`Topic: 9 explanations | explanation | rationales | rationale | interpretability
    --> explanations | interpretability | saliency | faithfulness | methods`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题: 9 解释 | 解释 | 理由 | 理由 | 可解释性 --> 解释 | 可解释性 | 显著性 | 可信性 | 方法`'
- en: The resulting topics are much more diverse! Topic **XXX**, which originally
    used a lot of “summarization” words, the topic only contains the word “summarization”.
    Also, duplicates, like “embedding” and “embeddings” are now removed.*  *## Text
    Generation
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的主题更加多样化！主题**XXX**原本使用了很多“总结”相关的词汇，而现在该主题只包含“总结”这个词。同时，像“embedding”和“embeddings”的重复词汇也被移除了。*  *##
    文本生成
- en: Text generation models have shown great potential in 2023\. They perform well
    across a wide range of tasks and allow for extensive creativity in prompting.
    Their capabilities are not to be underestimated and not using them in BERTopic
    would frankly be a waste. We talked at length about these models in Chapter **XXX**,
    but it’s useful now to see how they tie into the topic modeling process.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成模型在2023年显示出巨大的潜力。它们在广泛的任务中表现出色，并允许在提示中进行广泛的创造性。它们的能力不容小觑，而不在BERTopic中使用它们无疑是一种浪费。我们在**XXX**章中详细讨论了这些模型，但现在查看它们如何与主题建模过程结合是有益的。
- en: As illustrated in [Figure 3-21](#fig_21_use_text_generative_llms_and_prompt_engineering_to),
    we can use them in BERTopic efficiently by focusing on generating output on a
    topic level and not a document level. This can reduce the number of API calls
    from millions (e.g., millions of abstracts) to a couple of hundred (e.g., hundreds
    of topics). Not only does this significantly speed up the generation of topic
    labels, but you also do not need a massive amount of credits when using an external
    API, such as Cohere or OpenAI.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图3-21](#fig_21_use_text_generative_llms_and_prompt_engineering_to)所示，我们可以通过专注于生成主题级输出而非文档级输出，来高效使用BERTopic。这可以将API调用的数量从数百万（例如，数百万的摘要）减少到几百（例如，数百个主题）。这不仅显著加快了主题标签的生成速度，而且在使用外部API（如Cohere或OpenAI）时，也不需要大量的费用。
- en: '![Use text generative LLMs and prompt engineering to create labels for topics
    from keywords and documents related to each topic. ](assets/text_clustering_and_topic_modeling_664502_21.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![使用文本生成LLMs和提示工程从与每个主题相关的关键词和文档中创建主题标签。](assets/text_clustering_and_topic_modeling_664502_21.png)'
- en: Figure 3-21\. Use text generative LLMs and prompt engineering to create labels
    for topics from keywords and documents related to each topic.
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-21\. 使用文本生成LLMs和提示工程从与每个主题相关的关键词和文档中创建主题标签。
- en: Prompting
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'As was illustrated back in [Figure 3-21](#fig_21_use_text_generative_llms_and_prompt_engineering_to),
    one major component of text generation is prompting. In BERTopic this is just
    as important since we want to give enough information to the model such that it
    can decide what the topic is about. Prompts in BERTopic generally look something
    like this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[图 3-21](#fig_21_use_text_generative_llms_and_prompt_engineering_to)中所示，文本生成的一个主要组成部分是提示。在BERTopic中，这同样重要，因为我们希望向模型提供足够的信息，以便它能决定主题内容。BERTopic中的提示通常看起来像这样：
- en: '[PRE26]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: There are three components to this prompt. First, it mentions a few documents
    of a topic that best describes it. These documents are selected by calculating
    their c-TF-IDF representations and comparing them with the topic c-TF-IDF representation.
    The top 4 most similar documents are then extracted and referenced using the “**[DOCUMENTS]**”
    tag.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 该提示包含三个组成部分。首先，它提到一些最能描述主题的文档。这些文档通过计算它们的c-TF-IDF表示并与主题的c-TF-IDF表示进行比较来选择。然后提取前四个最相似的文档，并使用“**[文档]**”标签进行引用。
- en: '[PRE27]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Second, the keywords that make up a topic are also passed to the prompt and
    referenced using the “**[KEYWORDS]**” tag. These keywords could also already be
    optimized using KeyBERTInspired, PartOfSpeech, or any representation model.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，构成主题的关键词也会传递给提示，并使用“**[关键词]**”标签进行引用。这些关键词也可以通过KeyBERTInspired、词性或任何表示模型进行优化。
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Third, we give specific instructions to the Large Language Model. This is just
    as important as the steps before since this will decide how the model generates
    the label.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们向大型语言模型提供具体指令。这与之前的步骤同样重要，因为这将决定模型如何生成标签。
- en: '[PRE29]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The prompt will be rendered as follows for topic XXX:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 该提示将被呈现为主题XXX：
- en: '[PRE30]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: HuggingFace
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HuggingFace
- en: Fortunately, as with most Large Language Models, there is an enormous amount
    of open-source models that we can use through [HuggingFace’s Modelhub](https://huggingface.co/models).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，与大多数大型语言模型一样，我们可以通过[HuggingFace的Modelhub](https://huggingface.co/models)使用大量开源模型。
- en: One of the most well-known open-source Large Language Models that is optimized
    for text generation, is one from the Flan-T5 family of generation models. What
    is interesting about these models is that they have been trained using a method
    called **instruction tuning**. By fine-tuning T5 models on many tasks phrased
    as instructions, the model learns to follow specific instructions and tasks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的开源大型语言模型之一是Flan-T5生成模型系列，它针对文本生成进行了优化。这些模型的有趣之处在于它们使用一种称为**指令调优**的方法进行训练。通过对许多以指令形式表达的任务进行微调，模型学会了遵循特定的指令和任务。
- en: BERTopic allows for using such a model to generate topic labels. We create a
    prompt and ask it to create topics based on the keywords of each topic, labeled
    with the `[KEYWORDS]` tag.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic允许使用这样的模型生成主题标签。我们创建一个提示，请它根据每个主题的关键词生成主题，并标记为`[关键词]`。
- en: '[PRE31]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`Topic: 0 speech | asr | recognition | acoustic | endtoend --> audio grammatical
    recognition`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：0 演讲 | asr | 识别 | 声学 | 端到端 --> 音频语法识别`'
- en: '`Topic: 1 clinical | medical | biomedical | notes | health --> ehr`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：1 临床 | 医疗 | 生物医学 | 笔记 | 健康 --> ehr`'
- en: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    mds`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：2 摘要 | 总结 | 总结 | 抽象 | 抽取 --> mds`'
- en: '`Topic: 3 parsing | parser | dependency | amr | parsers --> parser`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：3 解析 | 解析器 | 依赖关系 | amr | 解析器 --> 解析器`'
- en: '`Topic: 4 hate | offensive | speech | detection | toxic --> Twitter`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：4 仇恨 | 攻击性 | 演讲 | 检测 | 有毒 --> Twitter`'
- en: '`Topic: 5 word | embeddings | embedding | vectors | similarity --> word2vec`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：5 词 | 嵌入 | 嵌入向量 | 相似性 --> word2vec`'
- en: '`Topic: 6 gender | bias | biases | debiasing | fairness --> gender bias`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：6 性别 | 偏见 | 偏差 | 去偏见 | 公平性 --> 性别偏见`'
- en: '`Topic: 7 ner | named | entity | recognition | nested --> ner`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：7 命名 | 实体 | 识别 | 嵌套 --> ner`'
- en: '`Topic: 8 prompt | fewshot | prompts | incontext | tuning --> gpt3`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：8 提示 | 少样本 | 提示 | 上下文 | 调优 --> gpt3`'
- en: '`Topic: 9 relation | extraction | re | relations | distant --> docre`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：9 关系 | 提取 | re | 关系 | 远程 --> docre`'
- en: There are interesting topic labels that are created but we can also see that
    the model is not perfect by any means.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的主题标签被创建，但我们也可以看到该模型并不是完美无缺的。
- en: OpenAI
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI
- en: When we are talking about generative AI, we cannot forget about ChatGPT and
    its incredible performance. Although not open source, it makes for an interesting
    model that has changed the AI field in just a few months. We can select any text
    generation model from OpenAI’s collection to use in BERTopic.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论生成性AI时，不能忘记ChatGPT及其惊人的表现。尽管不是开源的，但它是一个有趣的模型，在短短几个月内改变了AI领域。我们可以从OpenAI的集合中选择任何文本生成模型在BERTopic中使用。
- en: As this model is trained on RLHF and optimized for chat purposes, prompting
    is quite satisfying with this model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该模型是基于RLHF训练的，并且优化用于聊天目的，因此使用该模型进行提示非常令人满意。
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`Topic: 0 speech | asr | recognition | acoustic | endtoend --> audio grammatical
    recognition`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：0 演讲 | asr | 识别 | 声学 | 端到端 --> 音频语法识别`'
- en: '`Topic: 1 clinical | medical | biomedical | notes | health --> ehr`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：1 临床 | 医疗 | 生物医学 | 记录 | 健康 --> ehr`'
- en: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    mds`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：2 总结 | 摘要 | 总结 | 抽象 | 提取 --> mds`'
- en: '`Topic: 3 parsing | parser | dependency | amr | parsers --> parser`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：3 解析 | 解析器 | 依赖 | amr | 解析器 --> parser`'
- en: '`Topic: 4 hate | offensive | speech | detection | toxic --> Twitter`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：4 仇恨 | 攻击性 | 言论 | 检测 | 有毒 --> Twitter`'
- en: '`Topic: 5 word | embeddings | embedding | vectors | similarity --> word2vec`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：5 词 | 嵌入 | 嵌入向量 | 相似性 --> word2vec`'
- en: '`Topic: 6 gender | bias | biases | debiasing | fairness --> gender bias`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：6 性别 | 偏见 | 偏差 | 去偏见 | 公平性 --> 性别偏见`'
- en: '`Topic: 7 ner | named | entity | recognition | nested --> ner`'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：7 命名 | 实体 | 识别 | 嵌套 --> ner`'
- en: '`Topic: 8 prompt | fewshot | prompts | incontext | tuning --> gpt3`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：8 提示 | 少样本 | 提示 | 上下文 | 调优 --> gpt3`'
- en: '`Topic: 9 relation | extraction | re | relations | distant --> docre`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：9 关系 | 提取 | re | 关系 | 远程 --> docre`'
- en: 'Since we expect ChatGPT to return the topic in a specific format, namely “topic:
    <topic label>” it is important to instruct the model to return it as such when
    we create a custom prompt. Note that we also add the `delay_in_seconds` parameter
    to create a constant delay between API calls in case you have a free account.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们期望ChatGPT以特定格式返回主题，即“主题：<主题标签>”，因此在创建自定义提示时，指示模型按此格式返回非常重要。请注意，我们还添加了`delay_in_seconds`参数，以便在API调用之间创建恒定的延迟，以防你使用的是免费账户。
- en: Cohere
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Cohere
- en: As with OpenAI, we can use Cohere’s API within BERTopic on top of its pipeline
    to further fine-tune the topic representations with a generative text model. Make
    sure to grab an API key and you can start generating topic representations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 与OpenAI一样，我们可以在BERTopic的管道中使用Cohere的API，进一步微调主题表示，结合生成文本模型。确保获取API密钥，这样你就可以开始生成主题表示。
- en: '[PRE33]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Topic: 0 speech | asr | recognition | acoustic | endtoend --> audio grammatical
    recognition`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：0 演讲 | asr | 识别 | 声学 | 端到端 --> 音频语法识别`'
- en: '`Topic: 1 clinical | medical | biomedical | notes | health --> ehr`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：1 临床 | 医疗 | 生物医学 | 记录 | 健康 --> ehr`'
- en: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    mds`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：2 总结 | 摘要 | 总结 | 抽象 | 提取 --> mds`'
- en: '`Topic: 3 parsing | parser | dependency | amr | parsers --> parser`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：3 解析 | 解析器 | 依赖 | amr | 解析器 --> parser`'
- en: '`Topic: 4 hate | offensive | speech | detection | toxic --> Twitter`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：4 仇恨 | 攻击性 | 言论 | 检测 | 有毒 --> Twitter`'
- en: '`Topic: 5 word | embeddings | embedding | vectors | similarity --> word2vec`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：5 词 | 嵌入 | 嵌入向量 | 相似性 --> word2vec`'
- en: '`Topic: 6 gender | bias | biases | debiasing | fairness --> gender bias`'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：6 性别 | 偏见 | 偏差 | 去偏见 | 公平性 --> 性别偏见`'
- en: '`Topic: 7 ner | named | entity | recognition | nested --> ner`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：7 命名 | 实体 | 识别 | 嵌套 --> ner`'
- en: '`Topic: 8 prompt | fewshot | prompts | incontext | tuning --> gpt3`'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：8 提示 | 少样本 | 提示 | 上下文 | 调优 --> gpt3`'
- en: '`Topic: 9 relation | extraction | re | relations | distant --> docre`'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`主题：9 关系 | 提取 | re | 关系 | 远程 --> docre`'
- en: LangChain
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LangChain
- en: To take things a step further with Large Language Models, we can leverage the
    LangChain framework. It allows for any of the previous text generation methods
    to be supplemented with additional information or even chained together. Most
    notably, LangChain connects language models to other sources of data to enable
    them to interact with their environment.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提升大型语言模型的能力，我们可以利用LangChain框架。它允许任何先前的文本生成方法补充额外信息，甚至链式结合。特别是，LangChain将语言模型连接到其他数据源，使它们能够与环境互动。
- en: For example, we could use it to build a vector database with OpenAI and apply
    ChatGPT on top of that database. As we want to minimize the amount of information
    LangChain needs, the most representative documents are passed to the package.
    Then, we could use any LangChain-supported language model to extract the topics.
    The example below demonstrates the use of OpenAI with LangChain.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用它与OpenAI构建一个向量数据库，并在该数据库上应用ChatGPT。由于我们希望尽量减少LangChain所需的信息量，因此将最具代表性的文档传递给该软件包。然后，我们可以使用任何LangChain支持的语言模型来提取主题。下面的示例演示了如何将OpenAI与LangChain结合使用。
- en: '[PRE34]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`Topic: 0 speech | asr | recognition | acoustic | endtoend --> audio grammatical
    recognition`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 0 speech | asr | recognition | acoustic | endtoend --> audio grammatical
    recognition`'
- en: '`Topic: 1 clinical | medical | biomedical | notes | health --> ehr`'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 1 clinical | medical | biomedical | notes | health --> ehr`'
- en: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    mds`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 2 summarization | summaries | summary | abstractive | extractive -->
    mds`'
- en: '`Topic: 3 parsing | parser | dependency | amr | parsers --> parser`'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 3 parsing | parser | dependency | amr | parsers --> parser`'
- en: '`Topic: 4 hate | offensive | speech | detection | toxic --> Twitter`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 4 hate | offensive | speech | detection | toxic --> Twitter`'
- en: '`Topic: 5 word | embeddings | embedding | vectors | similarity --> word2vec`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 5 word | embeddings | embedding | vectors | similarity --> word2vec`'
- en: '`Topic: 6 gender | bias | biases | debiasing | fairness --> gender bias`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 6 gender | bias | biases | debiasing | fairness --> gender bias`'
- en: '`Topic: 7 ner | named | entity | recognition | nested --> ner`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 7 ner | named | entity | recognition | nested --> ner`'
- en: '`Topic: 8 prompt | fewshot | prompts | incontext | tuning --> gpt3`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 8 prompt | fewshot | prompts | incontext | tuning --> gpt3`'
- en: '`Topic: 9 relation | extraction | re | relations | distant --> docre`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic: 9 relation | extraction | re | relations | distant --> docre`'
- en: Topic Modeling Variations
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题建模变体
- en: The field of topic modeling is quite broad and ranges from many different applications
    to variations of the same model. This also holds for BERTopic as it has implemented
    a wide range of variations for different purposes, such as dynamic, (semi-) supervised,
    online, hierarchical, and guided topic modeling. [Figure 3-22](#fig_22__x_topic_modeling_variations_in_bertopic)-X
    shows a number of topic modeling variations and how to implement them in BERTopic.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模的领域相当广泛，涵盖了许多不同的应用和同一模型的变体。BERTopic也不例外，它为不同目的实现了多种变体，例如动态、（半）监督、在线、分层和引导的主题建模。[图
    3-22](#fig_22__x_topic_modeling_variations_in_bertopic)-X展示了一些主题建模变体以及如何在BERTopic中实现它们。
- en: '![ X Topic Modeling Variations in BERTopic](assets/text_clustering_and_topic_modeling_664502_22.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![ BERTopic中的X主题建模变体](assets/text_clustering_and_topic_modeling_664502_22.png)'
- en: Figure 3-22\. -X Topic Modeling Variations in BERTopic***  ***# Summary
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-22\. -X BERTopic中的主题建模变体***  ***# 摘要
- en: In this chapter we discussed a cluster-based method for topic modeling, BERTopic.
    By leveraging a modular structure, we used a variety of Large Language Models
    to create document representations and fine-tune topic representations. We extracted
    the topics found in ArXiv abstracts and saw how we could use BERTopic’s modular
    structure to develop different kinds of topic representations.*****
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一种基于聚类的主题建模方法，BERTopic。通过利用模块化结构，我们使用了多种大型语言模型来创建文档表示并微调主题表示。我们提取了ArXiv摘要中的主题，并观察了如何利用BERTopic的模块化结构来开发不同类型的主题表示。*****
