- en: 8 Understanding Stuff Better with Generative AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 通过生成式 AI 更好地理解事物
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Using GPT to replace large data analytics operations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPT 替代大型数据分析操作
- en: Using GPT to replace sentiment analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPT 替代情感分析
- en: Since GPT burst into all of our lives, most of our interactions with AI - and
    most of the book until this point - have focused on generating content of one
    sort or another. After all, "generate" *is* in the name. But not everything is
    about *creating* new things. There’s also *understanding* old things better.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 GPT 进入我们的生活以来，我们与 AI 的大部分交互 - 以及直到目前为止的大部分书籍 - 都集中在生成各种内容上。毕竟，"生成" *是* 名字中的一部分。但并不是所有事情都是关于*创造*新事物的。还有*更好地理解*旧事物的部分。
- en: It is true that all the way back in chapter 5 we did use the `GPTVectorStoreIndex`
    Python library to better understand some of our own data. But here’s where we
    take that a bit further and deeper. We’ll do that by using AI to help us find
    patterns and key details within large datasets (data analytics) and measuring
    population-scale public opinion using thousands of social media posts (sentiment
    analysis).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章中，我们确实使用了`GPTVectorStoreIndex` Python库来更好地理解我们自己的一些数据。但是现在我们要更进一步，更深入地探讨。我们将通过使用人工智能来帮助我们在大型数据集中找到模式和关键细节（数据分析），以及使用成千上万的社交媒体帖子来测量人口规模的公众意见（情感分析）来做到这一点。
- en: Until now, such tools and insights were normally available only to data professionals.
    Here we’ll see how they can now be accessed by just about anyone.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，这样的工具和见解通常只对数据专业人士可用。在这里，我们将看到它们如何现在几乎可以被任何人访问。
- en: 8.1 Using GPT to replace analytics
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 使用 GPT 替代分析
- en: Absorbing and then summarizing very large quantities of content in just a few
    seconds truly is a big deal. Just last night I received a link to the recording
    of an important 90 minute business video conference that I’d missed a few hours
    before. The reason I’d missed the live version was because I had no time (I was,
    if you must know, rushing to write this book before the universe pops out of existence…​or
    at least before they release GPT-58). Well, a half a dozen hours later I still
    had no time for the video. Inexplicably, the book was still not finished.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在几秒钟内吸收并总结大量内容确实是一件大事。就在昨晚，我收到了一条重要的90分钟商务视频会议录音的链接，我错过了几个小时前的现场版本。我错过现场版的原因是因为我没有时间（你必须知道，我正急于在宇宙消失之前写完这本书...
    或者至少在他们发布GPT-58之前）。然而，几个小时后，我仍然没有时间看视频。令人费解的是，这本书还没有完成。
- en: 'So here’s how I resolved the conflict the GPT way:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我以 GPT 方式解决冲突的方法：
- en: I used OpenAI Whisper (already seen in chapter 7) to generate a transcript based
    on the audio from the recording
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我使用了 OpenAI Whisper（在第7章已经看到）根据录音生成了一个转录
- en: I exported the transcript to a PDF file
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将记录转录成了 PDF 文件
- en: I uploaded the PDF to ChatPDF
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我把 PDF 上传到 ChatPDF。
- en: I prompted ChatPDF for summaries connected to the specific topics that interested
    me
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我提示 ChatPDF 提供与我感兴趣的特定主题相关的摘要
- en: 'Total time to "download" the key moments from the 90 minute call: 10 minutes.
    That’s 10 minutes to convert a dataset made up of around 15,000 spoken words to
    a machine-readable format, and to then digest, analyse, and summarize it.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '"下载"90分钟电话的关键时刻所需的总时间：10分钟。这意味着将由大约15000个口头语言转换为机器可读格式，然后对其进行消化、分析和总结，需要10分钟。'
- en: But all that’s old news by now. The *next-level* level will solve the problem
    of business analytics. Ok. So what *is* the "problem with business analytics"?
    It’s the hard work of building sophisticated code that parses large datasets`
    to make them consistently machine readable (also known as "data wrangling") and
    then applies complex algorithms to tease out useful insights. The figure below
    broadly outlines the process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但所有这些都已经是老生常谈了。*下一个级别*将解决业务分析的问题。好吧。那么"业务分析有什么问题"呢？问题在于要编写复杂的代码来解析大型数据集，使它们始终能够被机器读取（也称为"数据整理"），然后应用复杂的算法来提取有用的见解。下面的图大致概述了这个过程。
- en: Figure 8.1 Using data analytics to derive insights from raw data
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.1 使用数据分析从原始数据中得出见解
- en: '![gai 8 1](images/gai-8-1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![gai 8 1](images/gai-8-1.png)'
- en: A lot of the code that fits that description is incredibly complicated, not
    to mention clever. Inspiring clever data engineers to write that clever code can,
    of course, cost organizations many, many fortunes. The "problem" then, is the
    cost. So solving that problem would involve laying off the quarter-million-dollar-a-year
    engineers and replacing them with a few hundred dollars worth of large language
    model (LLM) API charges. Here’s how I plan to illustrate that.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 符合该描述的许多代码非常复杂，更不用说聪明了。激励聪明的数据工程师编写那种聪明代码当然会给组织带来巨大的成本。那么问题就是成本。因此解决这个问题将涉及解雇每年两十五万美元的工程师，用几百美元的大型语言模型
    (LLM) API 费用来替代他们。这是我的计划。
- en: I’ll need a busy spreadsheet to work with, right? The best place I know for
    good data is the Kaggle website. Kaggle is an online platform for hosting datasets
    (and data science competitions). It’s become in important resource for data scientists,
    machine learning practitioners, and researchers, allowing them to showcase their
    skills, learn from others, and collaborate on projects. The platform offers a
    wide range of public and private datasets, as well as tools and features to support
    data exploration and modeling.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要一个繁忙的电子表格来处理，对吧？我知道的获取良好数据的最佳途径是 Kaggle 网站。Kaggle 是一个托管数据集（和数据科学竞赛）的在线平台。它已经成为数据科学家、机器学习从业者和研究人员的重要资源，让他们展示自己的技能，从他人那里学习，并在项目上进行合作。该平台提供各种公共和私有数据集，以及支持数据探索和建模的工具和功能。
- en: '[The "Investing Program Type Prediction" dataset associated with this code](snassimr.html)
    should work perfectly. From what I can tell, this was data aggregated by a bank
    somewhere in the world that represents its customers'' behavior. Everything has
    been anonymized, of course, so there’s no way for us to know which bank we’re
    talking about, who the customers were, or even where in the world all this was
    happening. In fact, I’m not even 100% sure what each column of data represents.
    What *is* clear is that each customer’s age and neighborhood are there. Although
    the locations have been anonymized as `C1`, `C2`, `C3` etc. Some of the remaining
    columns clearly contain financial information.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[与此代码相关联的“投资项目类型预测”数据集](snassimr.html) 应该完美地运行。据我所知，这是某个世界上某个地方的银行聚合的代表其客户行为的数据。当然，一切都已经匿名化，所以我们不可能知道我们正在谈论哪家银行，客户是谁，甚至发生这一切的地点在世界上哪里。事实上，我甚至不能百分之百确定数据的每一列代表什么。
    *可以* 确定的是每个客户的年龄和社区都在那里。虽然位置已被匿名化为 `C1`、`C2`、`C3` 等。剩下的一些列显然包含财务信息。'
- en: Based on those assumptions, my ultimate goal is to search for statistically
    valid relationships between columns. For instance, are there specific demographic
    features (income, neighborhood, age) that predict a greater likelihood of a customer
    purchasing additional banking products? For this specific example I’ll see if
    I can identify the geographic regions within the data whose average household
    wealth is the highest.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些假设，我的最终目标是寻找列之间的统计有效关系。例如，有没有特定的人口统计特征（收入、社区、年龄），可以预测客户购买更多银行产品的可能性更大？对于这个具体的例子，我将看看是否能确定数据中平均家庭财富最高的地理区域。
- en: 'For normal uses such vaguely described data would be worthless. But since we’re
    just looking to demonstrate the process it’ll do just fine. I’ll *make up* column
    headers that more or less fit the shape of their data. Here’s how I named them:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通常的用途，这种模糊描述的数据将毫无价值。但由于我们只是想展示过程，它将做得很好。我会 *虚构* 出更或多或少符合其数据形状的列标题。下面是我如何命名它们：
- en: Customer ID
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户 ID
- en: Customer age
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户年龄
- en: Geographic location
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地理位置
- en: Branch visits per year
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年度分行访问次数
- en: Total household assets
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 家庭总资产
- en: Total household debt
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 家庭总债务
- en: Total investments with bank
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与银行的总投资额
- en: The column names need to be very descriptive because those will be the only
    clues I’ll give GPT to help it understand the data. I did have to add my own customer
    IDs to that first column (they didn’t originally exist). The fastest way I could
    think of to do that was to insert the `=(RAND())` formula into the top data cell
    in that column (with the file loaded into spreadsheet software like Excel, Google
    Sheets, or LibreOffice Calc) and then apply the formula to the rest of the rows
    of data. When that’s done, all the 1,000 data rows will have unique IDs, albeit
    IDs between 0 and 1 with many decimal places.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列名需要非常描述性，因为那将是我给GPT唯一的提示，帮助它理解数据。我确实不得不将我的客户ID添加到第一列（它们原本不存在）。我能想到的最快的方法是在该列的顶部数据单元格中插入
    `=(RAND())` 公式（使用电子表格软件如 Excel、Google Sheets 或 LibreOffice Calc 加载文件），然后将该公式应用于其余的数据行。完成后，所有
    1,000 行数据将具有唯一的 ID，尽管是 0 到 1 之间带有许多小数位的 ID。
- en: 'With my data prepared, I’ll use our old friend LlamaIndex (first seen back
    in chapter 5) to get to work analysing the numbers. As before, the code I’m going
    to execute will:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好后，我将使用我们的老朋友LlamaIndex（在第5章首次出现）开始分析数字。与之前一样，我将要执行的代码将是：
- en: Import the necessary functionality
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入必要的功能
- en: Add my OpenAI API key
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加我的 OpenAI API 密钥
- en: Read the data file that’s in the directory called `data`
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取位于名为 `data` 的目录中的数据文件
- en: Build the nodes from which we’ll populate our index
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建我们将填充索引的节点
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Finally, I’ll send my prompt:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我会发送我的提示：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here it is again in a format that’s easier on the eyes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更易于理解的格式：
- en: '*Based on the data, which 5 geographic regions had the highest household net
    wealth?*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*根据数据，哪五个地理区域的家庭净财富最高？*'
- en: I asked this question primarily to confirm that GPT understood the data. It’s
    always good to test your model just to see if the responses you’re getting seem
    to resonably reflect what you already know about the data. To answer properly,
    GPT would need to figure out what each of the column headers means and the relationships
    *between* columns. In other words, it would need to know how to calculate net
    worth for each row (account ID) from the values in the `Total household assets`,
    `Total household debt`, and `Total investments with bank` columns. It would then
    need to aggregate all the net worth numbers that it generated by `Geographic location`,
    calculate averages for each location and, finally, compare all the averages and
    rank them.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我问这个问题主要是为了确认 GPT 是否理解了数据。测试模型总是件好事，只是看看得到的响应是否合理地反映了你已经了解的数据。要正确回答，GPT 需要弄清楚每个列标题的含义以及列之间的关系。换句话说，它需要知道如何从“总家庭资产”、“总家庭债务”和“与银行的总投资”列的值中计算每行（帐户
    ID）的净资产。然后，它需要汇总它生成的所有净资产数字，按地理位置计算每个位置的平均值，最后比较所有平均值并排名。
- en: The result? I *think* GPT nailed it. After a minute or two of deep and profound
    thought (and around $0.25 in API charges), I was shown five location codes (G0,
    G90, G96, G97, G84, in case you’re curious). This tells me that GPT understands
    the location column the same way I did and is at least attempting to infer relationships
    between location and demographic features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果呢？我 *认为* GPT 做到了。经过一两分钟的深思熟虑（和大约 0.25 美元的 API 费用），我看到了五个位置代码（G0、G90、G96、G97、G84，如果你感兴趣的话）。这告诉我，GPT
    理解位置列的方式与我相同，并且至少试图推断位置与人口统计特征之间的关系。
- en: What did I mean "I think"? Well I never actually checked to confirm that the
    numbers made sense. For one thing, this isn’t real data anyway and, for all I
    know, I guessed the contents of each column incorrectly. But also because *every*
    data analysis needs checking against the real world so, in that sense, GPT-generated
    analysis is no different. In other words, whenever you’re working with data that’s
    supposed to represent the real world, you should always find a way to calibrate
    your data using known values to confirm that the whole thing isn’t a happy fantasy.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我所说的“我认为”是什么意思？嗯，我从来没有真正检查过数字是否合理。首先，这不是真实数据，而且我不知道，我可能错误地猜测了每列的内容。但也因为 *每一项*
    数据分析都需要与现实世界进行核对，所以从这个意义上说，由 GPT 生成的分析并无不同。换句话说，每当你处理应该代表现实世界的数据时，你都应该找到一种方法，使用已知值来校准你的数据，以确认整个事情不是一个愉快的幻想。
- en: 'I then asked a second question that reflects a real-world query that would
    interest any bank:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我问了第二个问题，反映了一个真实世界的查询，这个查询会引起任何银行的兴趣：
- en: '*Based on their age, geographic location, number of annual visits to bank branch,
    and total current investments, who are the ten customers most likely to invest
    in a new product offering? Show me only the value of the `customer ID` columns
    for those ten customers.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于客户的年龄、地理位置、年度银行分支机构访问次数和总当前投资，哪十名客户最有可能投资于新产品推出？仅显示那十个客户的`客户ID`列的值。*'
- en: Once again GPT spat back a response that at least *seemed* to make sense. This
    question was also designed to test GPT on its ability to correlate multiple metrics
    and submit them to a complex assessment ("…​most likely to invest in a new product
    offering").
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，GPT回答的问题看起来至少有些合理。这个问题也是设计用来测试GPT在协相关多个指标并将它们提交给一个复杂的评估中（“… 最有可能投资于新产品推出”）的能力。
- en: I’ll rate that as another successful experiment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我会把它评为另一个成功的实验。
- en: Takeaway
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 总结
- en: GPT - and other LLMs - are capable of independently parsing, analysing, and
    deriving insights from large data sets. While that greatly simplifies the process,
    success still depends on understanding the real-world context of your data and
    coming up with specific and clever prompts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT-和其他LLM-能够独立解析、分析和从大型数据集中提取洞察。尽管这极大地简化了这个过程，但其成功仍取决于理解数据的真实世界背景，并提出具体而巧妙的提示。 '
- en: 8.2 Using GPT to replace sentiment analysis
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2使用GPT代替情感分析
- en: 'Spoiler alert: this experiment won’t end quite so happily as some of the others
    we’ve seen here. But it’s really about the lessons learned along the way, isn’t
    it?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 神韵已经透露，这个实验不会像我们在这里看到的其他一些实验那样结束得那么幸福。但是，这真的是学到的教训，不是吗？
- en: Ok, so what is sentiment analysis and why should I want to do it?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么情感分析是什么，我为什么要做它呢？
- en: 8.2.1 Some background to sentiment analysis
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1情感分析的一些背景知识
- en: Sentiment analysis, also known as opinion mining, is a technique used to determine
    the sentiment or subjective tone expressed in a piece of text, such as a social
    media post, customer review, or news article. It generally involves analyzing
    the text to classify it as positive, negative, or neutral. It’s primary purpose
    is to understand the opinions, attitudes, and emotions of individuals or groups
    toward a particular topic, product, service, or event.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析，也称为意见挖掘，是一种用于确定文本中表达的情感或主观语气的技术，例如社交媒体帖子、客户评论或新闻文章。它通常涉及分析文本以将其分类为积极、消极或中性。它的主要目的是了解个人或群体对特定主题、产品、服务或活动的意见、态度和情绪。
- en: 'Sentiment analysis can help businesses and organizations:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析可以帮助企业和组织：
- en: Gain insights into how their customers perceive their brand, products, or services
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洞察他们的客户如何看待他们的品牌、产品或服务
- en: Track mentions of their brand or products to monitor and manage their online
    reputation
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪他们品牌或产品的提及，以监控和管理他们的在线声誉
- en: Understand market trends, consumer preferences, and emerging patterns
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解市场趋势、消费者偏好和新兴模式
- en: Analyze customer feedback at scale
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模分析客户反馈
- en: Gauge public sentiment and monitor discussions around political events, social
    issues, or public campaigns
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估公众情感并监测围绕政治事件、社会问题或公共运动的讨论
- en: Monitor market sentiment and detect potential investment risks or opportunities
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监测市场情绪并检测潜在的投资风险或机会
- en: Traditionally, effective sentiment analysis requires analytics code that’ll
    put your data through a series of steps to try to correctly guess the sentiment
    expressed. The core tool involves assessing the *polarity* of each data unit (typically
    a brief survey response or social media post). That’s done by looking for key
    words that indicate whether the post is positive, negative, or neutral. Analysis
    might then look for words or phrases indicating more precise moods, like anger,
    appreciation, or surprise). Software would then compile a big-picture statistical
    profile of the dataset to suggest trends. The figure below offers examples.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，有效的情感分析需要分析代码，通过一系列步骤来尝试正确猜测表达的情感。核心工具涉及评估每个数据单元（通常是简短的调查响应或社交媒体帖子）的极性。这是通过寻找指示帖子是否积极、消极或中性的关键词来完成的。分析可能会然后寻找表达更精确情绪，如愤怒、赏识或惊讶的单词或短语）。软件将编译数据集的大致统计概况，以提供趋势建议。下面的图例提供了示例。
- en: Figure 8.2 Using sentiment analysis to infer the underlying mood (or sentiment)
    of short-form content
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2使用情感分析推断短格式内容的潜在情绪（或情感）
- en: '![gai 8 2](images/gai-8-2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![gai 8 2](images/gai-8-2.png)'
- en: Once again, the "problem" is that building effective sentiment analysis software
    from scratch will be complicated and expensive. And buying it won’t be cheap either.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，“问题”是从头开始构建有效的情感分析软件将会复杂且昂贵。而购买它也不会便宜。
- en: Which is where AI comes in. Getting it done the GPT way will first involve preprocessing
    the text by removing any irrelevant information, such as punctuation, special
    characters, and stopwords (commonly used words like "and," "the," "is," etc.).
    The text may also be converted to lowercase to ensure consistent analysis. Next,
    relevant features or words from the text are extracted to represent the sentiment.
    This can be done using techniques like *bag-of-words*, where the frequency of
    occurrence of each word in the text is counted and used as a feature.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是人工智能发挥作用的地方。以 GPT 的方式完成这项工作首先需要对文本进行预处理，去除任何无关信息，如标点符号、特殊字符和停用词（常用词如 "and"、"the"、"is"
    等）。文本也可能被转换为小写以确保一致的分析。接下来，从文本中提取相关特征或单词以表示情感。这可以使用*词袋*等技术来实现，其中文本中每个单词的出现频率被计算并用作特征。
- en: The extracted features are then used to classify the sentiment of the text.
    This can be done through various approaches, including "rule-based" methods that
    use predefined dictionaries that associate words or phrases with sentiment labels,
    and machine learning algorithms that have been trained on labeled datasets where
    each text is manually annotated with its corresponding sentiment.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用提取的特征来分类文本的情感。这可以通过各种方法来实现，包括使用预定义的字典将单词或短语与情感标签相关联的“基于规则”的方法，以及已经在带有标记的数据集上进行了训练的机器学习算法，其中每个文本都是手动注释的，标有其相应的情感。
- en: Finally, the sentiment analysis results can be further analyzed and interpreted
    based on the specific needs of the application. This may involve visualizing sentiment
    trends over time, identifying key topics or entities associated with sentiment,
    or comparing sentiment across different sources or demographics.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，情感分析结果可以根据应用程序的具体需求进行进一步分析和解释。这可能涉及可视化情感随时间的变化趋势，识别与情感相关的关键主题或实体，或者比较不同来源或人群的情感。
- en: It’s important to note that sentiment analysis is a challenging task due to
    the complexity of language, including sarcasm, irony, and context-dependent sentiment.
    It can also be expensive, as doing it right will often require customizations
    for the specific dataset you’re working with.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，情感分析是一项具有挑战性的任务，因为语言的复杂性，包括讽刺、反讽和依赖语境的情感。这也可能是昂贵的，因为要做好这项工作通常需要针对你正在处理的特定数据集进行定制。
- en: 8.2.2 Testing sentiment analysis through GPT
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 通过 GPT 进行情感分析的测试
- en: Which brings us back to generative AI. What LLMs generally bring to the table
    is simplicity. That is, most of the things they do can be done using different
    tools, but LLMs can do them with *a lot* less complex coding and environment configuration.
    That was nicely demonstrated by the analytics prompts we just saw.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就回到了生成式人工智能。LLM通常带来的是简单性。也就是说，它们大多数所做的事情都可以使用不同的工具来完成，但LLM可以使用*更少的*复杂编码和环境配置来完成。刚才我们看到的分析提示就很好地证明了这一点。
- en: Similarly, if we can provide GPT with a large dataset of comments and - without
    us having to manually direct the process or define our own sentiment dictionary
    - GPT can quickly spit out reliably-accurate sentiment rankings, then we’ll be
    ahead of the game. The trick is to see whether GPT delivers results that are similar
    to or at least close to the traditional methods.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果我们可以为 GPT 提供一个大型评论数据集，并且 - 而无需我们手动指导过程或定义自己的情感词典 - GPT可以快速地输出可靠的情感排名，那么我们就会走在前面。关键是看看GPT是否提供了与传统方法相似或至少接近的结果。
- en: 'To test this, I downloaded a set of 1,000 Twitter messages that contained product
    or service reviews for various companies. The messages are all pre-labelled (meaning,
    the sentiment is already included). Here are a couple of rows so you can see how
    they look:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一点，我下载了一组包含各种公司产品或服务评论的1,000条Twitter消息。这些消息都是预先标记的（意思是，情感已经包含在内）。以下是几行，这样你就可以看到它们的外观：
- en: '| Company | Sentiment | Comment |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 公司 | 情感 | 评论 |'
- en: '| Microsoft | Negative | @Microsoft Why do I pay for WORD when it functions
    so poorly on my @SamsungUS Chromebook? |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 微软 | 负面 | @Microsoft 我为什么在我的 @SamsungUS Chromebook 上购买WORD时它的功能如此糟糕？ |'
- en: '| MaddenNFL | Positive | Thank you @EAMaddenNFL!! |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| MaddenNFL | 正面 | 感谢 @EAMaddenNFL!! |'
- en: My goal is to get GPT to generate its own sentiment labels without extensive
    preparations which I’ll compare with the existing set. That’ll show me how close
    GPT is to replacing the traditional sentiment analysis methodologies. I’ll test
    this using both the GPT-3 and GPT-3.5 engines.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标是让 GPT 生成它自己的情感标签，而不需要进行大量的准备工作，然后将其与现有集合进行比较。这将向我展示 GPT 距离取代传统的情感分析方法有多近。我将使用
    GPT-3 和 GPT-3.5 引擎来测试这一点。
- en: While experimenting with various various formulations of API requests, I experienced
    some problems accessing the GPT API. The first setback in my plans came from an
    unexpected `RateLimitError` message. Trying to assess all 1,000 tweets consistently
    failed, with each failure costing me about $0.40.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用多种 API 请求的组合时，我遇到了一些访问 GPT API 的问题。我的计划遭遇了一个意外的`RateLimitError`错误信息。尝试一致地评估所有的
    1,000 条推文都失败了，而每次失败都让我损失了大约$0.40。
- en: Even when I dropped 950 of the messages from the CSV file (leaving only 50),
    I still hit the `RateLimitError` nearly as often as not. If nothing else, this
    gives us another strong use-case for the build-your-own LLM servers we’ll discuss
    in the next chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我从 CSV 文件中删除了950条消息（只剩下50条），我仍然几乎和以前一样经常遇到`RateLimitError`错误。无论如何，这都为我们提供了另一个强大的自定义LLM服务器的用例，在下一章我们将讨论这个问题。
- en: In any case, I adapted the Python code for this experiment from [this excellent
    Sentiment Analysis project on GitHub](main.html). I begin by loading all the necessary
    libraries, passing my API key, and reading my .CSV spreadsheet file. Nothing new
    there.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我从 [这个出色的 GitHub 情感分析项目](main.html)中改编了这个实验的 Python 代码。我首先加载了所有必要的库，传递我的
    API 密钥，并读取了我的 .CSV 电子表格文件。没有什么新鲜的东西。
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, I’ll create two functions. The first (`analyze_gpt35(text)`) will set
    up a context and the prompt we’ll apply to each comment for the GPT-3.5 model.
    The context takes the form of a *system* role that tells GPT how it should act
    as an analyst. The actual prompt, which is a *user* role, consists of our specific
    instructions, asking GPT to perform sentiment analysis. The request writes the
    GPT completion to a variable called `response_text` using the `gpt-3.5-turbo`
    engine.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将创建两个函数。第一个函数(`analyze_gpt35(text)`)将设置一个上下文和我们将应用于每个评论的 GPT-3.5 模型的提示。这个上下文以一个*系统*角色的形式存在，告诉
    GPT 模型作为一个分析员该如何行动。而实际的提示，即一个*用户*角色的指令，包含我们具体的要求，要求 GPT 执行情感分析。此请求将 GPT 的输出写入名为`response_text`的变量中，使用`gpt-3.5-turbo`引擎。
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The second function does pretty much the same thing as the first, but for the
    older GPT-3 model. The goal here is to eventually be able to compare the accuracy
    of the two models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数和第一个函数几乎做了同样的事情，只不过针对的是旧的 GPT-3 模型。这里的目标是最终能够比较两个模型的准确性。
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Remember: we created a data frame called `df` containing the original data
    we downloaded. Now we’re ready to actually run those two functions against each
    row of the `Comment` column in that data frame and write the analysis to new columns
    (which the code will create). If you run into that rate limit error, you can try
    running just one of those two commands at a time.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 记住：我们创建了一个名为`df`的数据框，其中包含我们下载的原始数据。现在我们准备好对这些数据框中"评论"列的每一行运行这两个函数，并将分析结果写入新列（代码将创建这些列）。如果遇到了速率限制错误，可以尝试一次只运行这两个命令中的一个。
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With a nicely-populated data frame waiting for us, let’s compare the results
    of GPT-3, GPT-3.5 with the pre-existing labels. I’ll use `value_counts()` - which
    counts the incidents of each value in a data frame column - for that:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个数据框，我们可以对比潜在数据和预先存在的标签的 GPT-3 和 GPT-3.5 的结果。我将使用`value_counts()`方法对这一点进行统计，该方法可以计算数据框列中每个值的出现次数：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'What came out the other end represents the number of times each possible combination
    of results occurred. So, for instance, the most common outcome (occurring 12 times)
    was a negative rating for each of the training data ("Label"), the GPT-3 model,
    and the GPT-3.5 model. There were ten instances where all three models delivered
    positive ratings. Here’s the full output as a chart:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出的结果代表了每种可能组合的结果出现的次数。例如，最常见的结果（出现了12次）是每个训练数据("Label")、GPT-3 模型和 GPT-3.5
    模型都给出了负面评价。有十个实例中三个模型都给出了正面评价。以下是完整的输出结果表：
- en: '| Label | predicted_gpt3 | predicted_gpt35 | Frequency |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|标签|GPT-3 预测结果|GPT-3.5 预测结果|频率|'
- en: '| Negative | negative | negative | 12 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|负面|负面|负面|12|'
- en: '| Positive | positive | positive | 10 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|积'
- en: '| Neutral | negative | negative | 7 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|中性|负面|负面|7|'
- en: '| Irrelevant | negative | negative | 2 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|无关|负面|负面|2|'
- en: '| Neutral | positive | positive | 2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|中性|积极|积极|2|'
- en: '|  |  | not sure | 2 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 不确定 | 2 |'
- en: '| Negative | positive | positive | 2 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 负面 | 积极 | 积极 | 2 |'
- en: '|  | negative | negative | 2 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | 负面 | 负面 | 2 |'
- en: '| Irrelevant | positive | positive | 2 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 不相关 | 积极 | 积极 | 2 |'
- en: '| Negative | positive | positive | 1 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 负面 | 积极 | 积极 | 1 |'
- en: '| Neutral |  | negative | 1 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 中立 |  | 负面 | 1 |'
- en: '| Irrelevant | neutral | not sure | 1 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 不相关 | 中立 | 不确定 | 1 |'
- en: '| Neutral |  | positive | 1 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 中立 |  | 积极 | 1 |'
- en: '| Positive | negative | negative | 1 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 积极 | 负面 | 负面 | 1 |'
- en: '|  |  | negative | 1 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 负面 | 1 |'
- en: '|  |  | not sure | 1 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 不确定 | 1 |'
- en: '|  |  | positive | 1 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 积极 | 1 |'
- en: '| Neutral | neutral | not sure | 1 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 中立 | 中立 | 不确定 | 1 |'
- en: Of our 50 comments, both GPT-3 and GPT-3.5 successfully matched the original
    label only 22 times (12 instances where all three scored "negative" and ten where
    all three scored "positive"). For all intents and purposes, the two GPT models
    also performed pretty much identically to each other.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的50条评论中，GPT-3和GPT-3.5仅在22次中成功匹配到了原始标签（12次所有三个模型都给出了“负面”评分，10次所有三个模型都给出了“积极”评分）。从根本上说，这两个GPT模型在性能上也几乎完全一样。
- en: A 44% success rate isn’t great, but it is probably good enough for at least
    some use-cases. Perhaps successfully running this against a much larger dataset
    would have given us better results. But I can imagine projects where you’re looking
    for broad trends rather than absolute accuracy. There’s certainly still some more
    work to do here.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 44%的成功率并不算很高，但对于至少某些用例来说可能已经足够了。也许如果我们对一个更大的数据集运行这个实验，就能获得更好的结果。但我可以想象到一些项目，你是在寻找广泛的趋势而不是绝对的准确性。在这方面肯定还有更多工作要做。
- en: 8.3 Summary
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 总结
- en: We used llama_index to analyse large datasets to deliver sophisticated financial
    and consumer insights into likely consumer behavior. We showed how results can
    (and must) be checked against the real world to confirm that our LLM isn’t making
    stuff up.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用llama_index对大型数据集进行分析，提供关于消费者行为的复杂金融和消费者洞察。我们展示了如何将结果与现实世界进行核对，以确认我们的LLM不是编造出来的。
- en: With mixed results, we used GPT to execute sentiment analysis against comments
    on consumer products and services.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在结果有所不同的情况下，我们使用GPT对消费品和服务的评论进行了情感分析。
- en: 8.4 Try this for yourself
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 你也来试试吧
- en: Now that GPT-4 is widely available, why not try it out on our sentiment analysis
    experiment and see if you get better results. Also, look for different data sources
    - and let us know what you discover.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在GPT-4已经广为可用，为什么不在我们的情感分析实验中尝试一下，看看是否能获得更好的结果。同时，寻找不同的数据来源，并告诉我们你发现了什么。
