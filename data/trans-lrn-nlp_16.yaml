- en: appendix B Introduction to fundamental deep learning tools
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B 初级深度学习工具介绍
- en: This appendix covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录涵盖
- en: Introducing five fundamental algorithmic and software tools used in the book
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍本书中使用的五种基本算法和软件工具
- en: Overviewing stochastic gradient descent, the algorithm used to train neural
    networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对训练神经网络使用的算法——随机梯度下降进行概述
- en: Getting started with TensorFlow for neural network modeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以TensorFlow开始进行神经网络建模
- en: Getting started with PyTorch for neural network modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以PyTorch开始进行神经网络建模
- en: Overviewing higher-level neural network modeling frameworks Keras, fast.ai,
    and Hugging Face transformers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对较高级别的神经网络建模框架Keras、fast.ai和Hugging Face transformers进行概述
- en: In this appendix, we attempt to provide a brief primer on a few fundamental
    tools and concepts that are used throughout the book. The brief introductions
    to these tools are not absolutely necessary to work through and fully benefit
    from the book. However, going through them can help one get oriented, and they
    are probably most useful to a new entrant into the field of deep learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们试图对本书中使用的一些基本工具和概念提供一个简要的入门。对这些工具的简要介绍并不绝对必要，以便理解并充分从本书中受益。不过，阅读它们可以帮助一个新的深度学习领域的人快速融入，并且对他们可能是最有用的。
- en: 'Specifically, we first introduce the reader to the fundamental algorithm behind
    the deep learning revolution we are presently experiencing. This is, of course,
    the *stochastic gradient-descent algorithm*, which is used to train neural networks.
    We follow that up with introductions to two fundamental neural network modeling
    frameworks, PyTorch and TensorFlow. We then introduce three tools built on top
    of these modeling frameworks to provide a higher-level interface to them: Keras,
    fast.ai, and Hugging Face transformers. These tools all complement each other,
    and you are likely to use them all at some point in your career. Our exposition
    of the concepts is not exhaustive; it offers a “bird’s-eye view” into why the
    various tools are needed and how they compare to and complement each other. We
    touch on introductory concepts and cite curated references to facilitate a deeper
    dive. If you feel your experience with these tools is minimal, you may benefit
    from diving deeper before beginning to work through this book.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们首先向读者介绍了我们目前经历的深度学习革命背后的基本算法。当然，这就是用于训练神经网络的*随机梯度下降算法*。接着我们介绍了两种基本的神经网络建模框架，PyTorch和TensorFlow。然后我们介绍了这两个建模框架之上构建的三种工具，以提供一个更高级别的接口：Keras、fast.ai和Hugging
    Face transformers。这些工具相互补充，你可能在职业生涯的某个时候都会用到它们。我们对概念的阐述并不是穷尽的；它提供了一个“鸟瞰”为什么这些工具是需要的以及它们如何相互比较和相互补充。我们涉及了介绍性的概念，并引用了精心筛选的参考资料，以便深入研究。如果你觉得自己对这些工具的经验很少，你可能需要在开始阅读本书之前深入研究一下它们。
- en: Let’s start with the algorithmic engine behind the deep learning revolution,
    the stochastic gradient-descent algorithm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从深度学习革命背后的算法引擎开始，即随机梯度下降算法。
- en: B.1 Stochastic gradient descent
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 随机梯度下降
- en: A neural network has a set of parameters, called *weights*, which determine
    how it is going to transform input data into output data. Determining which set
    of weights allows the network to most closely approximate a set of training data
    is called *training* the network. Stochastic gradient descent is the method used
    to achieve this.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有一组参数，称为*权重*，确定它将如何将输入数据转换为输出数据。确定哪组权重允许网络最接近地逼近一组训练数据称为*训练*网络。随机梯度下降是实现这一目标的方法。
- en: 'Let’s denote the weights by `W`, the input data as `x`, and output data as
    `y`. Let’s also denote the output data predicted by the neural network for the
    input `x` as `y_pred`. The loss function, which measures how close `y` is to `y_pred`,
    is denoted as the function `f`. Note that it is a function of `x`, `y`, and `W`.
    The stochastic gradient-descent algorithm is formulated as a procedure to find
    the minimum of `f`, that is, where the predictions are as close as possible to
    the training data. If the gradient for `f`, denoted by `f’`, exists—if it is a
    *differentiable* function—we know `f’=0` at such a point. The algorithm attempts
    to find such points using the following sequence of steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用`W`表示权重，`x`表示输入数据，`y`表示输出数据。我们还用`y_pred`表示神经网络对输入`x`预测的输出数据。损失函数，用于衡量`y`与`y_pred`之间的接近程度，被表示为函数`f`。注意它是`x`、`y`和`W`的函数。随机梯度下降算法被制定为一个过程，以找到`f`的最小值，即预测尽可能接近训练数据的位置。如果`f`的梯度，用`f'`表示，存在——如果它是一个*可微*函数——我们知道在这样的点上`f'=0`。算法试图使用以下步骤序列找到这样的点：
- en: Draw a random input-output batch `x-y` of data from the training set. This randomness
    is the reason the algorithm is qualified as *stochastic*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练集中随机抽取一个输入-输出批次`x-y`的数据。这种随机性是算法被称为*随机*的原因。
- en: Pass inputs through the network with the current values of `W` to obtain `y_pred`*.*
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用当前`W`的值将输入通过网络以获得`y_pred`*.*。
- en: Compute the corresponding loss function value `f`.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算相应的损失函数值`f`。
- en: Compute corresponding gradient `f’` of the loss function with respect to W.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失函数相对于`W`的相应梯度`f'`。
- en: Change `W` a bit in the opposite direction of the gradient to lower `f`. The
    size of the step is determined by picking the *learning rate* of the algorithm,
    a very important hyperparameter for convergence.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍微改变`W`的方向以降低`f`。步长的大小由算法的*学习率*决定，这是收敛的一个非常重要的超参数。
- en: This process is illustrated for the overly simple case of a single weight, with
    the minimum found at step 2 of the algorithm in figure B.1\. This figure is inspired
    by figure 2.11 of François Chollet’s excellent book, *Deep Learning with Python*
    (Manning Publications, 2018), which you should also check out for a very intuitive
    explanation of the algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于过度简单的单个权重的情况，该过程在图 B.1 中的第2步找到了算法的最小值。这张图受到弗朗索瓦·朱利叶的优秀书籍《深度学习与Python》（Manning
    Publications，2018）中图2.11的启发，你也应该查看这本书以获得对该算法非常直观的解释。
- en: '![APPB_01](../Images/APPB_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![APPB_01](../Images/APPB_01.png)'
- en: Figure B.1 Stochastic gradient descent illustrated in the overly simple case
    of a single weight. At each step, compute the gradient with respect to W, and
    take a step of a prespecified size, as determined by the learning rate, in the
    opposite direction of the gradient of the loss function. In this hypothetical
    scenario, the minimum is found at step 2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.1 展示了随机梯度下降在单个权重的过度简单情况下的示意图。在每一步中，计算相对于`W`的梯度，并采取预先确定大小的步骤，由学习率决定，沿着损失函数梯度的相反方向。在这个假设的情景中，最小值在第2步找到。
- en: A number of variants of this algorithm exist, including Adam, RMSprop, and Adagrad.
    These tend to focus on avoiding local minima and being adaptive in various ways
    (such as in learning rate) to converge faster. The concept of *momentum*—which
    manifests as an extra additive term in the `W` update at each step—is used by
    several such variants to avoid local minima. Some of the most popular variants,
    with brief descriptions, follow.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多此算法的变体存在，包括Adam、RMSprop和Adagrad。这些算法倾向于专注于避免局部最小值，并以各种方式（如学习率）进行自适应，以更快地收敛。*动量*的概念——它表现为每一步`W`更新中的额外加法项——被几种这样的变体用来避免局部最小值。以下是一些最流行的变体及简要描述。
- en: '*Adagrad* adapts the learning rate in response to how often a parameter is
    encountered. Infrequent parameters are updated with larger steps to achieve a
    balance. This technique was used to train the GloVe static word embedding, described
    in chapter 4 of this book. It was needed in this case to appropriately handle
    rare words in language.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adagrad* 根据参数遇到的频率调整学习率。罕见的参数会以更大的步长进行更新，以实现平衡。该技术被用于训练GloVe静态词嵌入，该词嵌入在本书的第4章中描述。在这种情况下，它需要适当处理语言中的稀有单词。'
- en: '*RMSprop* was developed to address the observation that Adagrad’s learning
    rate often decreased too quickly. We can partially alleviate the issue by scaling
    updates by an exponentially decaying average of squared gradients.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*RMSprop*是为了解决Adagrad的学习速率经常过快下降的问题而开发的。我们可以通过将更新缩放为平方梯度的指数衰减平均值来部分缓解这个问题。'
- en: '*Adam*, which stands for *Adaptive Moment Estimation*, also varies the learning
    rate for different parameters. It shares a similarity to RMSprop in that it uses
    the decaying squared gradient average to perform updates. Both the first and second
    moments of the decaying squared gradient average are estimated, updated, and then
    used to update the parameters at every step. This is a popular algorithm to try
    first for many problems.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adam*表示*自适应矩估计*，也针对不同参数变化学习率。它与RMSprop具有相似之处，因为它使用衰减平方梯度平均值来执行更新。衰减平方梯度平均值的第一和二个时刻被估计，更新，然后用于在每个步骤中更新参数。这是尝试解决许多问题的流行算法。'
- en: '*Nadam,* short for *Nesterov-accelerated Adam*, employs an innovation called
    the *Nesterov-accelerated gradient* to improve Adam convergence further.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*Nadam*是*Nesterov加速Adam*的缩写，采用称为*Nesterov加速梯度*的创新来进一步改善Adam的收敛性。'
- en: Because the exposition here is meant to be only a brief introduction, and not
    a detailed treatment, we do not explore these variants further. This subject has
    been covered in detail by many excellent references,[¹](#pgfId-1036974),[²](#pgfId-1036977)
    and we encourage you to dive deeper to gain a better understanding. Even though
    you can use modern frameworks without a deep understanding of these variants,
    understanding them better can certainly help you tune hyperparameters and ultimately
    deploy better models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这里的介绍只是一个简短的介绍，而不是详细的处理，所以我们不会更深入地探讨这些变体。这个主题已经被许多优秀的参考文献详细覆盖了[¹](#pgfId-1036974)，[²](#pgfId-1036977)，我们鼓励您深入研究以获得更好的理解。即使您可以在不深入了解这些变体的情况下使用现代框架，更好地了解它们可以帮助您调整超参数，并最终部署更好的模型。
- en: B.2 TensorFlow
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 TensorFlow
- en: As described in the previous section, knowing the gradient of the loss function
    with respect to the neural network weights is critical to training the network.
    Because modern neural networks are huge, reaching into billions of parameters,
    this gradient function would be impossible to compute by hand. Instead, using
    a fundamental neural network modeling tool such as TensorFlow, the gradient is
    found automatically by applying the chain rule of taking derivatives to the functions
    composing the neural network model. This process is known as *automatic differentiation*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，了解损失函数相对于神经网络权重的梯度对于训练网络至关重要。由于现代神经网络是巨大的，达到数十亿个参数，因此手动计算此梯度函数将是不可能的。相反，使用TensorFlow等基本神经网络建模工具，通过应用取导数的链式法则自动找到梯度来计算它。这个过程被称为*自动微分*。
- en: The basic data structure within Tensorflow is a *tensor*, on which operations
    are performed by building a *graph*. In versions 1.x of the framework, the graph
    is put together with various `tf.*` API calls, and a `Session` object is used
    to compile and execute it to yield numerical values. An illustrative example of
    using this API to define a graph, and execute both it and its gradient, is shown
    in listing B.1\. Specifically, we are interested in computing the matrix product
    `z = x*y`, where `x` is a simple column vector and `y` is a simple row vector.
    We are also interested in computing its gradient, with respect to both `x` and
    `y`, automatically.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow中的基本数据结构是*张量*，通过构建一个*计算图*来对其进行操作。在框架的1.x版本中，通过多种API调用`tf.*`来构建图，并使用`Session`对象编译和执行它以产生数值。示例B.1中演示了使用此API定义图形并执行其梯度计算的说明性示例。具体来说，我们需要计算矩阵乘积`z
    = x*y`，其中`x`是简单的列向量，而`y`是简单的行向量。我们还希望自动计算它相对于`x`和`y`的梯度。
- en: Listing B.1 Computing the matrix product z = x*y and its gradient with TensorFlow
    1
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 示例B.1使用TensorFlow 1计算矩阵乘积z = x * y及其梯度
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Always imports TensorFlow first
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❶总是先导入TensorFlow
- en: ❷ Eager execution was introduced as the nondefault earlier than 2.0, so here
    we ensure it is off.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❷Eager执行在2.0之前作为非默认值引入，因此在此确保其关闭。
- en: ❸ Defines the placeholders for vector variables to assign values to later
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❸定义留给后面分配值的向量变量占位符
- en: ❹ Defines the vector derivative graph of the product, with respect to both x
    and y. Parameter grad_ys multiplies the output and can be used to take chain derivatives,
    so we set it to identity matrix for no effect.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义了乘积的向量导数图，相对于 x 和 y。参数 `grad_ys` 乘以输出，可用于取链导数，因此我们将其设置为单位矩阵以无效果。
- en: ❺ Executes the graph using the Session object
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 Session 对象执行图
- en: ❻ Runs the function, specifying values for placeholders
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 运行函数，指定占位符的值
- en: ❼ Runs the gradient, specifying values for placeholders
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 运行梯度，指定占位符的值
- en: ❽ Displays the results
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 显示结果
- en: Executing this code yields the following output. You should be able to verify
    by hand that these values are correct, using your basic linear algebra knowledge,
    which is a prerequisite for this book. We have also included a Kaggle kernel notebook
    executing these commands in the book’s companion repository:[³](#pgfId-1037017)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将产生以下输出。您应该能够手动验证这些值是否正确，使用您的基本线性代数知识，这是本书的先决条件。我们还在书的伴随存储库中包含了一个 Kaggle
    内核笔记本，执行这些命令。[³](#pgfId-1037017)
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Version 2.0 and later of the framework made the more “Pythonic” *eager execution*
    mode the default, which made the framework more accessible. It also now included
    Keras, making it easier to use a wide variety of higher-level functions. An illustrative
    example of using this API to define and execute the same graph from listing B.1
    is shown in the next listing. The greater accessibility is immediately evident,
    with eager mode enabling execution right away instead of via a `Session` object
    on a graph.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 框架的 2.0 版及更高版本将更“Pythonic”的 *eager execution* 模式作为默认模式，这使得框架更易于使用。它现在还包括了 Keras，使得使用各种高级功能更加容易。下一个列表中显示了使用此
    API 定义和执行与列表 B.1 中相同图形的说明性示例。更易于访问性立即变得明显，eager 模式使得立即执行变得可能，而不是通过图上的 `Session`
    对象。
- en: Listing B.2 Computing the matrix product z = x*y and its gradient with TensorFlow
    2
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 B.2 计算矩阵乘积 z = x*y 及其在 TensorFlow 2 中的梯度
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Column vector
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列向量
- en: ❷ Row vector
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 行向量
- en: ❸ This is how you would compute automatic derivatives with respect to x. The
    word “Tape” here means all states are “recorded” and can be played back to retrieve
    the info we need.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这是如何相对于 x 计算自动导数的。这里的“Tape”一词表示所有状态都被“记录”，可以播放回来以检索我们需要的信息。
- en: ❹ This is how you would compute automatic derivatives with respect to y. Parameter
    output_gradients multiplies the output and can be used to take chain derivatives,
    so we set it to identity matrix for no effect.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这是如何相对于 y 计算自动导数的。参数 `output_gradients` 乘以输出，可用于取链导数，因此我们将其设置为单位矩阵以无效果。
- en: ❺ Displays the results
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 显示结果
- en: Executing this code should yield the same output values as before.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码应产生与之前相同的输出值。
- en: The framework is organized hierarchically, with both high-level and low-level
    APIs, as shown in figure B.2.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 框架按层次结构组织，具有高级和低级 API，如图 B.2 所示。
- en: '![APPB_02](../Images/APPB_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![APPB_02](../Images/APPB_02.png)'
- en: Figure B.2 Illustration of the hierarchical organization of the TensorFlow framework
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.2 TensorFlow 框架的分层组织示意图
- en: This figure is heavily influenced by figure 1 of the official TensorFlow documentation.[⁴](#pgfId-1037068)
    If you are a beginner at using the framework, skimming through this reference
    in more detail can be helpful. The TensorFlow version of Keras, to be discussed
    further in the last section of this appendix, is also shown in the diagram.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此图受官方 TensorFlow 文档第 1 图的影响。[⁴](#pgfId-1037068) 如果您是初学者，并且想更详细地浏览此参考资料，这可能会有所帮助。在附录的最后一节将进一步讨论
    TensorFlow 版本的 Keras，该版本也显示在图中。
- en: The better way to pick up the ins and outs of the various features of TensorFlow
    is to get your hands dirty experimenting with relevant Kaggle kernel/notebook
    tutorials, as described in appendix A. In particular, just going to kaggle.com
    and searching for “TensorFlow tutorial” will bring up a myriad of wonderful tutorials,
    and you can choose something that works best for your learning style and experience
    level. The tutorial at [https://www.kaggle.com/akashkr/tensorflow-tutorial](https://www.kaggle.com/akashkr/tensorflow-tutorial)
    seems to be a good one for beginners.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 更好地了解 TensorFlow 各种特性的方法是动手尝试相关的 Kaggle 内核/笔记本教程，如附录 A 中所述。特别是，只需访问 kaggle.com
    并搜索“TensorFlow 教程”即可找到大量精彩的教程，您可以选择最适合您的学习风格和经验水平的内容。[https://www.kaggle.com/akashkr/tensorflow-tutorial](https://www.kaggle.com/akashkr/tensorflow-tutorial)
    上的教程似乎对初学者很有帮助。
- en: B.3 PyTorch
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 PyTorch
- en: This framework was released by Facebook after TensorFlow (in 2016 versus 2015).
    However, it quickly became preferred by many researchers, as evidenced by declining
    relative popularity of TensorFlow to PyTorch in terms of academic paper citations.[⁵](#pgfId-1037076)
    This increased popularity has widely been attributed to the framework’s ability
    to modify various PyTorch model objects programmatically at runtime, making for
    easier code optimization during the research process. Indeed, the introduction
    of eager mode in TensorFlow 2.0 is widely believed to have been influenced by
    PyTorch’s success. Although the differences between the platforms became much
    smaller after the release of TensorFlow 2.0, the popular wisdom is that PyTorch
    is preferred by researchers whereas TensorFlow is preferred for deployment in
    production.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架在 TensorFlow 之后发布（2016 年对比 2015 年）。然而，它很快就成为许多研究人员首选的框架，如 TensorFlow 相对
    PyTorch 在学术论文引用方面的相对流行度下降所证明的那样。[⁵](#pgfId-1037076) 这种增加的流行度被普遍认为是因为该框架能够在运行时以编程方式修改各种
    PyTorch 模型对象，从而在研究过程中更容易进行代码优化。事实上，TensorFlow 2.0 中的急切模式的引入被普遍认为受到了 PyTorch 成功的影响。尽管在
    TensorFlow 2.0 发布后，这两个平台之间的差异变得更小了，但普遍的观点是，研究人员更喜欢 PyTorch，而 TensorFlow 更适用于在生产中部署。
- en: As an illustration, we perform the same sequence of operations from listings
    B.1\. and B.2—vector multiplication and its derivative, which is core to neural
    network models—in PyTorch and show the corresponding code in the next listing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例证，我们在 PyTorch 中执行与清单 B.1 和 B.2 相同的操作序列 —— 向量乘法及其导数，这是神经网络模型的核心 —— 并在下一个清单中展示相应的代码。
- en: Listing B.3 Computing the matrix product z = x*y and its gradient in PyTorch
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 B.3 在 PyTorch 中计算矩阵乘积 z = x*y 及其梯度
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Always import PyTorch first.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 总是首先导入 PyTorch。
- en: ❷ Imports the grad function for automatic differentiation
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入 grad 函数进行自动微分
- en: ❸ Column vector
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 列向量
- en: ❹ Row vector
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 行向量
- en: ❺ This ensures that gradients can be computed with respect to x.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 这确保了可以针对 x 计算梯度。
- en: ❻ Computes the product
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算乘积
- en: ❼ Computes automatic derivatives with respect to x. retain_graph ensures that
    we can keep taking derivatives; otherwise, the “Tape” is discarded and can’t be
    played back.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 针对 x 计算自动导数。retain_graph 确保我们可以继续进行导数计算；否则，“Tape” 将被丢弃，无法回放。
- en: ❽ Computes automatic derivatives with respect to y. The parameter grad_outputs
    multiplies the output and can be used to take chain derivatives, so we set it
    to identity matrix for no effect.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 针对 y 计算自动导数。参数 grad_outputs 乘以输出，可以用于进行链式导数，因此我们将其设置为单位矩阵以无效果。
- en: ❾ Displays the results
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 显示结果
- en: Executing this code should yield the same result as in the previous section.
    We have also included a Kaggle kernel notebook executing these commands in the
    book’s companion repository.[⁶](#pgfId-1037115)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码应该产生与前一节相同的结果。我们还在本书的伴随存储库中包含了一个 Kaggle 内核笔记本，执行这些命令。
- en: As before, we recommend working through some Kaggle kernels to pick up the ins
    and outs of PyTorch, if you are feel that you might need more experience. The
    tutorial at [https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)
    seems to be a good one for beginners.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 和以前一样，我们建议通过一些 Kaggle 内核来熟悉 PyTorch 的各个方面，如果你觉得自己可能需要更多经验的话。[https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)
    上的教程似乎是初学者的好选择。
- en: B.4 Keras, fast.ai, and Transformers by Hugging Face
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 由 Hugging Face 提供的 Keras、fast.ai 和 Transformers
- en: As mentioned earlier in the appendix, the Keras library is a higher-level neural
    network modeling framework, which is also now included in TensorFlow versions
    2.0 and later. By using it, you can specify neural network architectures in both
    TensorFlow and PyTorch from a single API—just change the backend as needed! It
    comes prepackaged with TensorFlow, as we illustrated in figure B.2\. Its API is
    relatively simple compared to both TensorFlow and PyTorch, which has made it very
    popular. Many excellent resources exist for learning it, perhaps the best one
    being the author’s own book.[⁷](#pgfId-1037123) This is also an excellent reference
    for learning TensorFlow, and about neural networks in general, and we highly recommend
    it if you feel you need to brush up on these topics. You can also work through
    some Kaggle kernels to pick up the basics, with the tutorial at [https://www.kaggle.com/prashant111/keras-basics-for-beginners](https://www.kaggle.com/prashant111/keras-basics-for-beginners)
    being just one good example.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在附录中早前提到的，Keras 库是一个更高级的神经网络建模框架，现在也包含在 TensorFlow 2.0 及更高版本中。通过使用它，你可以在 TensorFlow
    和 PyTorch 中指定神经网络架构，只需从一个 API 中切换后端即可！它与 TensorFlow 预先打包在一起，如我们在图 B.2 中所示。与 TensorFlow
    和 PyTorch 相比，其 API 相对简单，这使得它非常受欢迎。存在许多优秀的学习资源，也许最好的资源之一是作者自己的书籍。这也是学习 TensorFlow
    和神经网络的绝佳参考资料，如果你觉得需要复习这些主题，我们强烈推荐它。你也可以通过一些 Kaggle 内核来学习一些基础知识，例如 [https://www.kaggle.com/prashant111/keras-basics-for-beginners](https://www.kaggle.com/prashant111/keras-basics-for-beginners)
    上的教程只是一个很好的例子。
- en: Another popular higher-level modeling API in the field is fast.ai. This library
    was developed as a companion to the massive open online course (MOOC) of the same
    name and implements state-of-the-art methods in a way that makes them extremely
    easy to use. One of its motivations was democratizing these tools for the developing
    world. A popular feature of this library is its learning rate determination utility,
    which we use in chapter 9 of this book. The framework is used for both NLP and
    computer vision and runs on PyTorch. Naturally, the best reference for learning
    the library is the fast.ai MOOC[⁸](#pgfId-1037128) itself. The free course covers
    the basics of neural networks and deep learning and is another wonderful resource
    that we highly recommend. The library achieves simplicity by defining its own
    set of data structures that handle a lot of boilerplate stuff for the user. On
    the other hand, this might make it harder to customize for nonstandard use cases.
    In this author’s experience, it is a wonderful tool to have in one’s arsenal.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在该领域另一个流行的高级建模 API 是 fast.ai。这个库是作为同名大型在线课程（MOOC）的伴侣而开发的，并以一种极易使用的方式实现了最先进的方法。其动机之一是将这些工具普及到发展中国家。该库的一个受欢迎功能是其学习率确定实用程序，我们在本书的第
    9 章中使用了它。该框架用于自然语言处理和计算机视觉，并运行在 PyTorch 上。自然，学习该库的最佳参考资料是 fast.ai MOOC 本身。这门免费课程涵盖了神经网络和深度学习的基础知识，是另一个我们强烈推荐的精彩资源。该库通过定义其自己的一套数据结构来实现简单化，这些数据结构处理了用户的大量样板代码。另一方面，这可能会使其对非标准用例的定制更加困难。在作者的经验中，这是一个拥有的绝佳工具。
- en: Finally, transformers by Hugging Face is a higher-level modeling framework specifically
    for transformer-based models. These have become arguably the most important architecture
    for modern NLP. You will learn exactly why that is the case throughout the book.
    The library might be the most popular library in the space today, due to the sheer
    ease of using it to deploy these models. Before this library existed, deploying
    transformer models in Keras, TensorFlow, and/or PyTorch was quite tedious. The
    library simplified the process into a few lines of Python code in some cases,
    which led to an explosion in its popularity, and it is considered indispensable
    for the modern NLP practitioner. Due to the transparency and simplicity of the
    API, you can probably get away with just reading through the book and working
    through relevant examples, even without any prior experience with it. For further
    reference, check out the intro notebooks from the authors on GitHub^([9](#pgfId-1037132))
    and their official quick start[^(10)](#pgfId-1037135) documentation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Hugging Face 的 Transformers 是一个专门针对基于 Transformer 模型的高级建模框架。这些模型已经成为现代自然语言处理中可能是最重要的架构。你将在整本书中准确了解其中的原因。这个库可能是当今领域中最受欢迎的库，因为使用它部署这些模型非常简单。在这个库存在之前，使用
    Keras、TensorFlow 和/或 PyTorch 部署 Transformer 模型相当繁琐。该库在某些情况下简化了这个过程，只需几行 Python
    代码，导致其受欢迎程度激增，并被认为是现代自然语言处理从业者不可或缺的工具。由于 API 的透明度和简单性，你可能只需阅读本书并通过相关示例进行工作，甚至无需任何先前的使用经验即可。如需进一步参考，请查看作者在
    GitHub 上的入门笔记[^(9)](#pgfId-1037132)以及官方快速入门文档[^(10)](#pgfId-1037135)。
- en: 1. F. Chollet, Deep Learning with Python (Manning Publications, 2018).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 1. F. Chollet，《Deep Learning with Python》（Manning Publications，2018）。
- en: 2. S. Ruder, “An Overview of Gradient Descent Optimization Algorithms,” arXiv
    (2016).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2. S. Ruder，“梯度下降优化算法概述”，arXiv（2016）。
- en: 3. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 3. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
- en: 4. [https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 4. [https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/toolkit)
- en: 5. [https://en.wikipedia.org/wiki/TensorFlow](https://en.wikipedia.org/wiki/TensorFlow)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 5. [https://en.wikipedia.org/wiki/TensorFlow](https://en.wikipedia.org/wiki/TensorFlow)
- en: 6. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 6. [https://github.com/azunre/transfer-learning-for-nlp](https://github.com/azunre/transfer-learning-for-nlp)
- en: 7. F. Chollet, Deep Learning with Python (Manning Publications, 2018).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 7. F. Chollet，《Deep Learning with Python》（Manning Publications，2018）。
- en: 8. [https://www.fast.ai/](https://www.fast.ai/)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 8. [https://www.fast.ai/](https://www.fast.ai/)
- en: 9. [https://github.com/huggingface/transformers/tree/master/notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 9. [https://github.com/huggingface/transformers/tree/master/notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)
- en: 10. [https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 10. [https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html)
