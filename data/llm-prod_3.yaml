- en: '3 Large Language Model Operations: Building a Platform for LLMs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 大型语言模型操作：构建用于LLMs的平台
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Overview of Large Language Models Operations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型操作概述
- en: Deployment challenges
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署挑战
- en: Large Language Models best practices
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型最佳实践
- en: Required Large Language Model infrastructure
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的大型语言模型基础设施
- en: As we learned in the last chapter when it comes to transformers and Natural
    Language Processing (NLP), bigger is better, especially when it’s linguistically
    informed. However, bigger models come with bigger challenges because of their
    size, regardless of their linguistic efficacy, thus requiring us to scale up our
    operations and infrastructure to handle these problems. In this chapter we’ll
    be looking into exactly what those challenges are, what we can do to minimize
    them, and what architecture can be set up to help solve these challenges.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章我们学到，当涉及到transformers和自然语言处理（NLP）时，越大越好，特别是在语言上有所启发时。然而，更大的模型由于其规模而带来了更大的挑战，无论其语言有效性如何，这要求我们扩大操作和基础设施规模以处理这些问题。在本章中，我们将详细探讨这些挑战是什么，我们可以采取什么措施来最小化它们，并建立什么样的架构来帮助解决这些挑战。
- en: 3.1 Introduction to Large Language Models Operations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 大型语言模型操作简介
- en: What is Large Language Models Operations (LLMOps)? Well, since I’m one to focus
    on practicality over rhetoric, I’m not going to dive into any fancy definitions
    that you’d expect in a text book, but let me simply say it’s Machine Learning
    Operations (MLOps) that has been scaled to handle LLMs. Let me also say, scaling
    up is hard. One of the hardest tasks in software engineering. Unfortunately, too
    many companies are running rudimentary MLOps set-ups, and don’t think for a second
    that they will be able to just handle LLMs. That said, the term “LLMOps,” may
    not be needed. It has yet to show through as sufficiently different from core
    MLOps, especially considering they still have the same bones. If this book were
    a dichotomous key, MLOps and LLMOps would definitely be in the same genus, and
    only time will tell about whether they are the same species. Of course by refusing
    to define LLMOps properly, I might have traded one confusion for another, so let's
    take a minute to describe MLOps.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是大型语言模型操作（LLMOps）？好吧，由于我更注重实用而不是华丽辞藻，我不打算深入讨论你在教科书中所期望的任何花哨的定义，但让我简单地说一下，它是已经被扩展以处理LLMs的机器学习操作（MLOps）。让我也说一下，扩展是困难的。这是软件工程中最艰巨的任务之一。不幸的是，太多公司正在运行基本的MLOps设置，并且不要想一秒钟他们将能够处理LLMs。话虽如此，“LLMOps”这个术语可能是不需要的。它尚未显示出足够不同于核心MLOps，特别是考虑到它们仍然具有相同的基础。如果这本书是一个二分键，MLOps和LLMOps肯定会属于同一个属，只有时间会告诉我们它们是否是同一个物种。当然，通过拒绝正确定义LLMOps，我可能已经用另一种混乱来交换了一种混乱，所以让我们花点时间来描述一下MLOps。
- en: MLOps is the field and practice of reliably and efficiently deploying and maintaining
    machine learning models in production. This includes, and indeed requires, managing
    the entire machine learning lifecycle from data acquisition and model training
    to monitoring and termination. A few principles required to master this field
    include workflow orchestration, versioning, feedback loops, Continuous Integration
    and Continuous Deployment (CI/CD), security, resource provisioning, and data governance.
    While there are often personnel who specialize in the productionizing of models,
    often with titles like ML Engineers, MLOps Engineers or ML Infrastructure Engineer,
    the field is a large enough beast it often abducts many other unsuspecting professionals
    to work in it who hold titles like Data Scientist or DevOps Engineer–oftentimes
    against their knowledge or will; leaving them kicking and screaming that “it’s
    not their job”.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps是可靠且高效地部署和维护机器学习模型在生产环境中的领域和实践。这包括，并确实需要，管理整个机器学习生命周期，从数据获取和模型训练到监控和终止。掌握这一领域所需的几个原则包括工作流编排、版本控制、反馈循环、持续集成和持续部署（CI/CD）、安全性、资源供给以及数据治理。虽然通常有专门从事模型产品化的人员，通常称为ML工程师、MLOps工程师或ML基础设施工程师，但这个领域是一个足够庞大的野兽，它经常会绑架许多其他毫无防备的专业人士来从事工作，他们拥有数据科学家或DevOps工程师等头衔，往往不是出于自己的知情和意愿；留下他们愤怒地抗议“这不是他们的工作”。
- en: 3.2 Operations Challenges with Large Language Models
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 大型语言模型的操作挑战
- en: So why have a distinction at all? If MLOps and LLMOps are so similar, is LLMOps
    just another fad opportunists throw on their resume? Not quite. In fact, I think
    it’s quite similar to the term Big Data. When the term was at its peak popularity,
    people with titles like Big Data Engineer used completely different tool sets
    and developed specialized expertise that were necessary in order to handle the
    large datasets. LLMs come with a set of challenges and problems you won’t find
    with traditional machine learning systems. A majority of these problems extend
    almost exclusively because they are so big. Large models are large! We hope to
    show you that LLMs truly earn their name. Let’s take a look at a few of these
    challenges, so we can appreciate the task ahead of us when we start talking about
    deploying an LLM.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么还要有一个区别呢？如果MLOps和LLMOps如此相似，那么LLMOps只是另一个机会主义者在简历上添加的时髦词汇吗？并非如此。事实上，我认为它与“大数据”这个词相当相似。当这个词达到最高流行时，拥有Big
    Data Engineer等职称的人使用完全不同的工具集，并开发了处理大型数据集所必需的专业技能。LLM带来了一系列挑战和问题，你在传统机器学习系统中找不到。其中大多数问题几乎完全由于它们太大而产生。大模型就是大！我们希望向您展示，LLM真正名副其实。让我们来看看其中一些挑战，这样我们就可以在开始讨论部署LLM时，能够更加欣赏面临的任务。
- en: 3.2.1 Long download times
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 长时间下载
- en: 'Back in 2017 when I was still heavily involved as a Data Scientist, I decided
    to try my hand at reimplementing some of the most famous computer vision models
    at the time AlexNet, VGG19, and ResNet. I figured this would be a good way to
    reinforce my understanding of the basics with some practical hands-on experience.
    Plus, I had an ulterior motive, I had just built my own rig with some NVIDIA GeForce
    1080 TI GPUs—which was state of the art at the time—and thought this would be
    a good way to break them in. The first task: download the ImageNet dataset. The
    ImageNet dataset was one of the largest annotated datasets available containing
    millions of images rounding out to a file size of a whopping ~150GB! Working with
    it was proof that you knew how to work with “Big Data '''' which was still a trendy
    word and an invaluable skill set for a data scientist at the time. After agreeing
    to the terms and gaining access, I got my first wakeup call. Downloading it took
    an entire week.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回到2017年，当我还是一名深度参与数据科学家时，我决定尝试重新实现当时最著名的一些计算机视觉模型，包括AlexNet、VGG19和ResNet。我认为这将是巩固我对基础知识理解的好方法，通过一些实际的动手经验。而且，我还有一个别有用心的动机，我刚刚组建了自己的机器，配备了一些当时最先进的NVIDIA
    GeForce 1080 TI GPU——我想这将是一个很好的方法来使用它们。第一个任务：下载ImageNet数据集。ImageNet数据集是当时最大的标注数据集之一，包含数百万张图像，文件大小高达约150GB！使用它证明你知道如何处理“大数据”，这在当时仍然是一个时髦的词汇，也是数据科学家无法替代的技能。同意了条款并获得访问权限后，我收到了第一个警钟。下载整个数据集花了整整一周。
- en: 'Large models are large. I don’t think I can overstate that. You’ll find throughout
    this book that fact comes with many additional headaches and issues for the entire
    production process, and you have to be prepared for it. In comparison to the ImageNet
    dataset, the Bloom LLM model is 330GB, more than twice the size. Most readers
    I’m guessing haven’t worked with either ImageNet or Bloom, so for comparison Call
    of Duty: Modern Warfare, one of the largest games at the time of writing is 235
    GB. Final Fantasy 15 is only 148 GB, which you could fit two of into the model
    with plenty of room to spare. It’s just hard to really comprehend how massive
    LLMs are. We went from 100 million parameters in models like BERT and took them
    to billions of parameters. If you went on a shopping spree and spent $20 a second
    (or maybe just left your AWS EC2 instance on by accident) it’d take you half a
    day to spend a million dollars; it would take you 2 years to spend a billion.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型就是大。我觉得我无法过分强调这一点。在本书中，你会发现这个事实给整个生产过程带来了许多额外的头痛和问题，你必须为此做好准备。与ImageNet数据集相比，Bloom
    LLM模型有330GB大小，是其两倍以上。我猜测大多数读者都没有使用过ImageNet或Bloom，因此为了比较，在写作时最大的游戏之一《使命召唤：现代战争》大小为235
    GB。最终幻想15只有148 GB，你可以把两个放入该模型中，还有很多空间。真的很难理解LLM有多大。我们从像BERT这样的模型的1亿个参数发展到了数十亿个参数。如果你狂热购物，每秒花费20美元（或者可能只是不小心让你的AWS
    EC2实例开着），你需要花半天时间花完一百万美元；花一亿美元需要两年时间。
- en: 'Thankfully it doesn’t take two weeks to download Bloom because unlike ImageNet,
    it’s not hosted on a poorly managed University server and it also has been sharded
    into multiple smaller files to allow downloading in parallel, but it will still
    take an uncomfortably long time. Consider a scenario where you are downloading
    the model under the best conditions. You’re equipped with a gigabit speed fiber
    internet connection and you were magically able to dedicate the entire bandwidth
    and I/O operations of your system and the server to it, it’d still take over 5
    minutes to download! Of course, that’s under the best conditions. You probably
    won’t be downloading the model under such circumstances, with modern infrastructure
    you can expect it to take on the order of hours. When my team first deployed Bloom
    it took an hour and a half to download it. Heck, it took me an hour and half to
    download The Legend of Zelda: Tears of the Kingdom and that’s only 16GB, so I
    really can’t complain.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，下载 Bloom 不需要两周的时间，因为与 ImageNet 不同，它并不托管在管理不善的大学服务器上，而且已经被分成多个较小的文件以便并行下载，但仍然需要相当长的时间。考虑这样一个场景：在最佳条件下下载模型。你配备了千兆速度的光纤互联网连接，而且可以神奇地将整个带宽和I/O操作都分配给系统和服务器，下载仍然需要超过
    5 分钟！当然，这是在最佳条件下。你可能不会在这种情况下下载模型，使用现代基础设施，你可以预计需要数小时的时间。当我的团队首次部署 Bloom 时，下载它花了一个半小时。天哪，下载《塞尔达传说：王国之泪》只用了一个半小时，而且那仅有
    16GB，所以我真的不能抱怨。
- en: 3.2.2 Longer Deploy Times
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 较长的部署时间
- en: Just downloading the model is a long enough time frame to make any seasoned
    developer shake, but deployment times are going to make them keel over and call
    for medical attention. A model as big as Bloom can take 30-45 minutes just to
    load the model into GPU memory, at least those are the time frames my team first
    saw. Not to mention any other steps in your deployment process that can add to
    this. Indeed, with GPU shortages, it can easily take hours just waiting for resources
    to free up—more on that in a minute.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只是下载模型，也已经足够长的时间让任何经验丰富的开发人员都感到不安，但是部署时间将使他们昏倒并请求医疗帮助。像 Bloom 这样大的模型可能需要 30-45
    分钟才能将模型加载到 GPU 内存中，至少这是我团队最初看到的时间框架。更不用说部署过程中的其他任何步骤可能会增加时间。事实上，由于 GPU 短缺，等待资源释放可能需要数小时——稍后会详细说明。
- en: What does this mean for you and your team? Well for starters, I know lots of
    teams who deploy ML products often simply download the model at runtime. That
    might work for small sklearn regression models, but it isn’t going to work for
    LLMs. Additionally, you can take most of what you know about deploying reliable
    systems and throw it out the window (but thankfully not too far). Most modern
    day best practices for software engineering assume you can easily just restart
    an application if anything happens, and there’s a lot of rigmarole involved to
    ensure your systems can do just that. But with LLMs it can take seconds to shut
    down, but potentially hours to redeploy making this a semi-irreversible process.
    Like picking an apple off a tree, it’s easy to pluck one off, but if you bite
    into it and decide it’s too sour, you can’t just attach it back onto the tree
    so it can continue to ripen. You’ll just have to wait awhile for another to grow.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这对你和你的团队意味着什么？首先，我知道许多团队通常在运行时仅下载模型来部署 ML 产品。这对于小的 sklearn 回归模型可能有效，但不适用于 LLMs。此外，你可以忽略大部分有关部署可靠系统的知识（但幸运的是不是太过分）。对于软件工程的现代最佳实践，通常假设如果发生任何问题，你可以轻松地重新启动应用程序，并且需要进行许多繁琐的工作来确保你的系统确实可以做到这一点。但是对于
    LLMs 来说，关闭可能只需要几秒钟，但重新部署可能需要数小时，这使得这成为一个几乎不可逆转的过程。就像摘苹果一样，摘下一个很容易，但如果咬了一口觉得太酸，你不能把它重新粘到树上让它继续成熟。你只能等一段时间再长出另一个。
- en: While not every project requires deploying the largest models out there, you
    can expect to see deployment times measured in minutes. These longer deploy times
    make scaling down right before a surge of traffic a terrible mistake, as well
    as figuring out how to manage bursty workloads difficult. General CI/CD methodologies
    need to be adjusted since rolling updates take longer leaving a backlog piling
    up quickly in your pipeline. Silly mistakes like typos or other bugs often take
    longer to notice, and longer to correct.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非每个项目都需要部署最大的模型，但你可以预期部署时间以分钟计算。这些较长的部署时间使得在流量激增之前缩减规模成为一个可怕的错误，以及难以管理突发性工作负载。一般的
    CI/CD 方法论需要进行调整，因为滚动更新需要更长时间，使得你的流水线迅速积累起积压。像拼写错误或其他错误这样的愚蠢错误往往需要更长时间才能发现和纠正。
- en: 3.2.3 Latency
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   延迟'
- en: Along with increases in model size often come increases in inference latency.
    This is obvious when stated, but more parameters equates to more computations,
    and more computations means longer inference wait times. However, this can’t be
    underestimated. I know many people who downplay the latency issues because they’ve
    interacted with an LLM chatbot and the experience has felt smooth. Take a second
    look though, and you’ll notice that it is returning one word at a time which is
    streamed to the user. It feels smooth because the answers are coming in faster
    than a human can read, but a second look helps us realize this is just a UX trick.
    LLMs are still too slow to be very useful for an autocomplete solution for example,
    where responses have to be blazingly fast. Building it into a data pipeline or
    workflow that reads a large corpus of text and then tries to clean it or summarize
    it, may also be prohibitively slow to be useful or reliable.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型大小的增加，推理延迟也往往会增加。这一点在陈述时显而易见，但更多的参数意味着更多的计算，更多的计算意味着更长的推理等待时间。然而，这一点不容小觑。我知道很多人因为与
    LLM 聊天机器人的互动感觉流畅而对延迟问题不以为然。但再仔细看一眼，你会注意到它是逐字返回的，这些字会逐字传送给用户。它感觉流畅是因为答案比人类阅读的速度更快地返回，但再仔细看一眼就会发现这只是一种用户体验的技巧。LLM
    仍然太慢，以至于对于自动补全解决方案来说并不是非常有用，例如，响应必须非常快速。将其构建到读取大量文本并尝试清理或总结的数据流水线或工作流中，可能也会因速度太慢而无法使用或不可靠。
- en: There are also many less obvious reasons for their slowness. For starters, LLMs
    are often distributed across multiple GPUs, which adds extra communication overhead.
    As discussed later in this chapter in section 3.3.2 they are distributed in other
    ways, often even to improve latency, but any distribution adds additional overhead
    burden. In addition, LLMs latency is severely impacted by completion length, meaning
    the more words it uses to return a response, the longer it takes. Of course, completion
    length also seems to improve accuracy. For example, using prompt engineering techniques
    like Chain of Thought (CoT) we ask the model to think about a problem in a step-by-step
    fashion which has shown to improve results for logic and math questions but also
    increases the response length and latency time significantly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缓慢还有许多不太明显的原因。首先，LLM 经常分布在多个 GPU 上，这增加了额外的通信开销。正如本章后面 3.3.2 节中所讨论的，它们以其他方式进行分布，通常甚至是为了提高延迟，但任何分布都会增加额外的负担。此外，LLM
    的延迟严重受到完成长度的影响，这意味着它用于返回响应的字数越多，所需时间越长。当然，完成长度似乎也会提高准确性。例如，使用类似 Chain of Thought
    (CoT) 这样的提示工程技术，我们要求模型以逐步方式思考问题，这已经证明能够提高逻辑和数学问题的结果，但也会显著增加响应长度和延迟时间。
- en: 3.2.4 Managing GPUs
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   管理 GPU'
- en: To help with these latency issues we usually want to run them in GPUs. If we
    want to have any success training LLMs we’ll need GPUs for that as well, but this
    all adds additional challenges many underestimate. Most web services and many
    ML use cases can be done solely on CPUs. Not so with LLMs. Partly because of GPUs’
    parallel processing capabilities offering a solution to our latency problems,
    and partly because of the inherent optimization GPUs offer in the linear algebra,
    matrix multiplications and tensor operations that’s happening under the hood.
    For many, stepping into the realm of LLMs, this requires utilizing a new resource
    and extra complexity. Many brazenly step into this world acting like it’s no big
    deal, but they are in for a rude awakening. Most system architectures and orchestrating
    tooling available like Kubernetes, assume your application will run with CPU and
    memory alone. While they often support additional resources like GPUs, this is
    often an afterthought. You’ll soon find you’ll have to rebuild containers from
    scratch and deploy new metric systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些延迟问题，我们通常希望使用GPU运行它们。如果我们想成功地训练LLMs，我们也需要GPU，但这会增加许多人低估的额外挑战。大多数Web服务和许多ML用例都可以仅使用CPU完成。但LLMs不行。一部分是因为GPU的并行处理能力提供了解决延迟问题的方法，而另一部分是由于GPU在算法的优化方面带来的优势，在算法的线性代数，矩阵乘法和张量运算中发挥作用。对于许多人来说，踏入LLMs领域，这需要使用新的资源和额外的复杂度。许多人大胆地踏入这个世界，似乎这不是什么大事，但他们会受到沉重打击。大多数系统架构和编排工具，如Kubernetes，假设你的应用程序仅使用CPU和内存运行。虽然它们通常支持其他资源，如GPU，但这通常是事后考虑的。你很快就会发现你必须从头开始重建容器并部署新的度量系统。
- en: One aspect of managing GPUs most companies are never prepared for is that they
    tend to be rare and limited. For the last decade it seems that we have gone in
    and out of a global GPU shortage. They can be extremely difficult to provision
    for companies looking to stay on premise. I’ve spent lots of time in my career
    working with companies who chose to stay on premise for a variety of reasons.
    One of the things they had in common is that they never had GPUs on their servers.
    When they did, they were often purposely difficult to access except for a few
    key employees.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司不准备管理GPU的一个方面是它们往往是罕见和有限的。在过去的十年中，全球GPU短缺的现象似乎已经来来去去。对于想留在本地的公司，他们很难为此提供资源。在我的职业生涯中，我花了很多时间与选择留在本地的公司合作，他们有许多共同点，其中之一是他们的服务器上从来没有GPU。当他们有GPU时，它们通常很难被除了一些关键员工之外的其他人所使用。
- en: If you are lucky enough to be working in the Cloud a lot of these problems are
    solved, but there is no free lunch here either. My team has often gone chasing
    their tail trying to help data scientists struggling to provision a new GPU workspace,
    running into obscure ominous errors like "`scale.up.error.out.of.resources`”.
    Only to discover that these esoteric readings indicate all the GPUs of a selected
    type in the entire region are being utilized and none are available. CPU and Memory
    can often be treated as infinite in a datacenter, GPU resources, however, cannot.
    Sometimes you can’t expect them at all. Most data centers only support a subset
    of instance or GPU types. Which means you may be forced to set up your application
    in a region further away from your user base increasing latency. Of course, I’m
    sure you can work with your cloud provider when looking to expand your service
    to a new region that doesn’t currently support it, but you might not like what
    you hear based on timelines and cost. Ultimately, you’ll run into shortage issues
    no matter where you choose to run, on-prem or in the cloud.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有幸在云端工作，很多这些问题都会被解决，但这里也没有免费的午餐。我的团队经常为了帮助数据科学家提供新的GPU工作空间而忙得焦头烂额，碰到像"`scale.up.error.out.of.resources`"这样的晦涩错误。只有发现所选择的区域中的所有类型的GPU都正在被使用而没有可用的时才会发现这些晦涩的读数。在数据中心中，CPU和内存通常可以被视为无限的，但GPU资源则不能。有时候你甚至不能期望它们。大多数数据中心只支持一部分实例或GPU类型。这意味着你可能被迫在距离用户基础设施较远的地区设置你的应用程序，从而增加了延迟。当然，我相信当你尝试将服务扩展到目前不支持的新区域时，你可以与你的云供应商合作，但你可能不喜欢听到的是时程和成本。无论你选择在本地还是在云端运行，最终你都会遇到短缺问题。
- en: 3.2.5 Peculiarities of Text Data
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 文本数据的特异性
- en: LLMs are the modern day solution to NLP. NLP is one of the most fascinating
    branches of ML in general because it primarily deals with text data, which is
    primarily a qualitative measure. Every other field deals with quantitative data.
    We have figured out a way to encode our observations of the world into a direct
    translation of numerical values. For example, we’ve learned how to encode heat
    into temperature scales and measure them with thermometers and thermocouples or
    we can measure pressure with manometers and gauges and put it into pascals.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是自然语言处理（NLP）的现代解决方案。总体而言，NLP是机器学习中最引人入胜的分支之一，因为它主要处理文本数据，而文本数据主要是定性的。其他领域都处理定量数据。我们已经找到了一种将我们对世界的观察编码成直接翻译的数值的方法。例如，我们已经学会将热量编码成温度尺度，并用温度计和热电偶来测量，或者我们可以用压力计和压力表来测量压力，并将其计量为帕斯卡。
- en: Computer Vision and the practice of evaluating images is often seen as qualitative,
    but the actual encoding of images into numbers is a solved problem. Our understanding
    of light has allowed us to break images apart into pixels and assign them RGB
    values. Of course this doesn’t mean CV is by any means solved, there’s still lots
    of work to do to learn how to identify the different signals in the patterns of
    the data. Audio data is another that’s often considered qualitative. How does
    one compare two songs? But we can measure sound and speech, directly measuring
    the sound wave's intensity in decibels and frequency in hertz.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉和评估图像的实践通常被视为定性的，但实际上将图像编码成数字是一个已解决的问题。我们对光的理解使我们能够将图像分解为像素并为其分配RGB值。当然，这并不意味着计算机视觉已经完全解决了，我们仍然有很多工作要做，以学习如何识别数据模式中的不同信号。音频数据通常也被认为是定性的。我们怎么比较两首歌曲呢？但是我们可以测量声音和语言，直接测量声波的强度（以分贝为单位）和频率（以赫兹为单位）。
- en: Unlike other fields that encode our physical world into numerical data, text
    data is looking at ways to measure the ephemeral world. After all, text data is
    our best effort of encoding our thoughts, ideas and communication patterns. While
    yes, we have figured out ways to turn words into numbers, we haven’t figured out
    a direct translation. Our best solutions to encode text and create embeddings
    are just approximations at best, in fact we use machine learning models to do
    it! An interesting aside to this is that numbers are also text and a part of language.
    If we want models that are better at math we need a more meaningful way to encode
    these numbers. Since it’s all made up, when we try to encode text numbers into
    machine-readable numbers we are creating a system attempting to reference itself
    recursively in a meaningful way. Not an easy problem to solve!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于将我们的物理世界编码为数值数据的其他领域，文本数据正在探索测量无常世界的方法。毕竟，文本数据是我们将思想、想法和沟通模式编码的最佳努力。当然，是的，我们已经找到了将单词转化为数字的方法，但我们还没有找到一种直接的翻译方式。我们在编码文本和创建嵌入时的最佳解决方案充其量只是近似解决方案，事实上我们使用机器学习模型来实现！有趣的是，数字也是文本和语言的一部分。如果我们想要更擅长数学的模型，我们需要一种更有意义的方法来编码这些数字。由于这一切都是虚构的，当我们尝试将文本数字编码为机器可读的数字时，我们正在创建一种试图以有意义的方式递归引用自身的系统。这不是一个容易解决的问题！
- en: Because of all this, LLMs (and all NLP solutions) have unique challenges. For
    example, monitoring. How do you catch data drift in text data? How do you measure
    “correctness”? How do you ensure cleanliness of the data? These types of problems
    are difficult to define let alone solve.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些原因，LLMs（以及所有NLP解决方案）面临着独特的挑战。例如，监控。如何在文本数据中捕捉数据漂移？如何衡量“正确性”？如何确保数据的净化？这些类型的问题很难定义，更不用说解决了。
- en: 3.2.6 Token Limits Create Bottlenecks
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.6 令牌限制导致瓶颈产生
- en: One of the big challenges for those new to working with LLMs is dealing with
    the token limits. The token limit for a model is the maximum number of tokens
    that can be included as an input for a model. The larger the token limit, the
    more context we can give the model to improve its success as accomplishing the
    task. Everyone wants them to be higher, but it’s not that simple. These token
    limits are defined by two problems, the first being the memory and speed our GPUs
    have access to, and the second being the nature of memory storage in the models
    themselves.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新手来说，与LLM一起工作的一个重大挑战是处理令牌限制问题。模型的令牌限制是作为模型输入的最大令牌数量。令牌限制越大，我们就能够为模型提供更多的上下文，以提高模型完成任务的成功率。每个人都希望令牌限制更高，但事情并不那么简单。这些令牌限制由两个问题定义，第一个问题是我们的GPU访问的内存和速度，第二个问题是模型本身内存存储的性质。
- en: The first one seems unintuitive, why couldn’t we just increase the GPU memory?
    The answer is complex, we can, but stacking more layers in the GPU to take into
    account more GB at once slows down the GPU’s computational ability as a whole.
    GPU manufacturers right now are working on new architectures and ways to get around
    this problem. The second one is more fascinating because we find that increasing
    the token limits actually just exacerbates the mathematical problems under the
    hood. Let me explain. Memory storage within an LLM itself isn’t something we think
    about often. We call that mechanism Attention, which we discussed in depth in
    section 2.2.7\. What we didn’t discuss was that Attention is a quadratic solution—as
    the number of tokens increase the number of calculations required to compute the
    attentions scores between all the pairs of tokens in a sequence scales quadratically
    with the sequence length. In addition, within our gigantic context spaces and
    since we are dealing with quadratics, we’re starting to hit problems where the
    only solutions involve imaginary numbers which is something that can cause models
    to behave in unexpected ways. This is likely one of the reasons why LLMs hallucinate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题似乎不合逻辑，为什么我们不能增加GPU内存？答案很复杂，我们可以，但是在GPU中叠加更多的层以一次性处理更多GB会降低GPU的整体计算能力。目前，GPU制造商正在研究新的架构和解决这个问题的方法。第二个问题更引人入胜，因为我们发现增加令牌限制实际上只会加剧底层的数学问题。让我解释一下。LLM内部的内存存储并不是我们经常考虑的事情。我们称之为Attention机制，在第2.2.7节中我们深入讨论了这一点。我们没有讨论的是，Attention是一个二次解决方案——随着令牌数量的增加，计算在一个序列中所有令牌对之间计算注意力分数所需的计算量将二次扩展到序列长度。此外，在我们巨大的上下文空间中，由于我们正在处理二次方程，我们开始遇到只有想象中的数字才能解决的问题，这是可能导致模型行为出现意外的原因之一。这很可能是LLMs产生幻觉的原因之一。
- en: These problems have real implications and impact application designs. For example,
    when my team upgraded from GPT3 to GPT4 we were excited to have access to a higher
    token limit, but we soon found this led to longer inference times and subsequently
    a higher timeout error rate. In the real world, it’s often better to get a less
    accurate response quickly than to get no response at all because the promise of
    a more accurate model often is just that, a promise. Of course, deploying it locally
    where you don’t have to worry about response times you’ll likely find your hardware
    a limiting factor. For example, LLaMA was trained with 2048 tokens but you’ll
    be lucky to take advantage of more than 512 of that when running with a basic
    consumer GPU as you are likely to see Out-of-Memory (OOM) errors or even the model
    simply just crashing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题对应用设计产生了真实的影响和影响。例如，当我的团队从GPT3升级到GPT4时，我们对拥有更高的令牌限制感到兴奋，但很快我们发现这导致了更长的推断时间，随之而来的是更高的超时错误率。在现实世界中，通常更好地快速获得不太准确的响应，而不是根本没有响应，因为更准确的模型往往只是一个承诺。当然，在本地部署时，你不必担心响应时间，你很可能会发现你的硬件是一个限制因素。例如，LLaMA是用2048个令牌训练的，但是当你使用基本的消费者GPU运行时，你很可能会发现你只能利用其中的512个以上，因为你可能会看到内存溢出（OOM）错误，甚至是模型简单地崩溃。
- en: A gotcha, which is likely to catch your team by surprise and should be pointed
    out now is that different languages have different tokens per character. Take
    a look at Table 3.1, where we compare converting the same sentence in different
    languages to tokens using OpenAI’s cl100k_base Byte Pair Encoder. Just a quick
    glance reveals that LLMs typically favor the English language in this regard.
    In practice, this means if you are building a chatbot with an LLM, your English
    users will have greater flexibility in their input space than Japanese users leading
    to very different user experiences.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: A gotcha，这可能会让你的团队感到意外，现在应该指出的是，不同语言的字符对应不同的标记。看一下表3.1，我们比较了使用OpenAI的cl100k_base字节对编码器将相同句子转换为不同语言的标记。只需快速浏览一下，就会发现LLMs通常在这方面偏爱英语。实际上，这意味着如果你正在构建一个使用LLM的聊天机器人，你的英语用户在输入空间上将比日语用户具有更大的灵活性，从而导致非常不同的用户体验。
- en: Table 3.1 Comparison of Token Counts in different languages
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1 不同语言的标记计数比较
- en: '| Language | String | Characters | Tokens |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | 字符串 | 字符数 | 标记数 |'
- en: '| English | The quick brown fox jumps over the lazy dog | 43 | 9 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 英语 | 快速的棕色狐狸跳过懒狗 | 43 | 9 |'
- en: '| French | Le renard brun rapide saute par-dessus le chien paresseux | 57 |
    20 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 法语 | 快速的棕色狐狸跳过懒狗 | 57 | 20 |'
- en: '| Spanish | El rápido zorro marrón salta sobre el perro perezoso | 52 | 22
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙语 | 快速的棕色狐狸跳过懒狗 | 52 | 22 |'
- en: '| Japanese | 素早い茶色のキツネが怠惰な犬を飛び越える | 20 | 36 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Japanese | 素早い茶色のキツネが怠惰な犬を飛び越える | 20 | 36 |'
- en: '| Chinese (simplified) | 敏捷的棕色狐狸跳过了懒狗 | 12 | 28 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Chinese (simplified) | 敏捷的棕色狐狸跳过了懒狗 | 12 | 28 |'
- en: If you are curious as to why this is, it is due to text encodings, which is
    just another peculiarity of working with text data as discussed in the previous
    section. Consider Table 3.2 where we show several different characters and their
    binary representation in UTF-8\. English characters can almost exclusively be
    represented with a single byte being included in the original ASCII standard computers
    were originally built on, while most other characters require 3 or 4 bytes. Because
    it takes more memory it also takes more token space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想知道这是为什么，那是由于文本编码，这只是与文本数据一起工作的另一个奇特之处，正如前一节所讨论的。请考虑表3.2，其中我们展示了几个不同字符及其在UTF-8中的二进制表示。英文字符几乎可以完全用一个字节来表示，这包括在最初的ASCII标准中计算机最初构建的，而大多数其他字符则需要3或4个字节。因为需要更多的内存，所以也需要更多的令牌空间。
- en: Table 3.2 Comparison of byte lengths for different currency characters in UTF-8.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.2 不同货币字符在UTF-8中的字节长度比较。
- en: '| Character | Binary UTF-8 | Hex UTF-8 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Character | Binary UTF-8 | Hex UTF-8 |'
- en: '| $ | 00100100 | 0x24 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $ | 00100100 | 0x24 |'
- en: '| £ | 11000010 10100011 | 0xc2 0xa3 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| £ | 11000010 10100011 | 0xc2 0xa3 |'
- en: '| ¥ | 11000010 10100101 | 0xc2 0xa5 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| ¥ | 11000010 10100101 | 0xc2 0xa5 |'
- en: '| ₠ | 11100010 10000010 10100000 | 0xe2 0x82 0xa0 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ₠ | 11100010 10000010 10100000 | 0xe2 0x82 0xa0 |'
- en: '| 💰 | 11110000 10011111 10010010 10110000 | 0xf0 0x9f 0x92 0xb0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 💰 | 11110000 10011111 10010010 10110000 | 0xf0 0x9f 0x92 0xb0 |'
- en: Increasing the token limits has been an ongoing research question since the
    popularization of transformers, and there are some promising solutions still in
    research phases like Recurrent Memory Transformers (RMT)[[1]](#_ftn1). We can
    expect to continue to see improvements in the future and hopefully this will become
    naught but an annoyance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 增加令牌限制自从transformers的普及以来一直是一个持续的研究问题，目前仍在研究阶段的一些有希望的解决方案，如循环记忆transformers（RMT）[[1]](#_ftn1)。我们可以期待未来会继续看到改进，希望这将成为一个无关紧要的问题。
- en: 3.2.7 Hallucinations Cause Confusion
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.7 幻觉导致混淆
- en: 'So far we’ve been discussing some of the technical problems a team faces when
    deploying an LLM into a production environment, but nothing compares to the simple
    problem that LLMs tend to be wrong. They tend to be wrong a lot. Hallucinations
    is a term coined to describe occurrences when LLM models will produce correct
    sounding results that are wrong. For example, book references or hyperlinks that
    have the form and structure of what would be expected, but are nevertheless, completely
    made up. As a fun example I asked for books on LLMs in Production from the publisher
    Manning (a book that doesn’t exist yet since I’m still writing it). I was given
    the following suggestions: Machine Learning Engineering in Production by Mike
    Del Balso and Lucas Serveén which could be found at [https://www.manning.com/books/machine-learning-engineering-in-production](books.html)
    and Deep Learning for Coders with Fastai and PyTorch by Jeremy Howard and Sylvain
    Gugger which could be found at [https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](books.html).
    The first book is entirely made up. The second book is real however it’s not published
    by Manning. In each case the internet addresses are entirely made up. These URLs
    are actually very similar to what you’d expect in format if you were browsing
    Mannings website, and should return 404 errors if you visit them.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在讨论团队在将LLM部署到生产环境时面临的一些技术问题，但没有什么能与LLM通常错误相比。它们通常错得很离谱。幻觉是一个术语，用来描述LLM模型产生正确但错误的结果的情况。例如，书籍引用或超链接具有预期的形式和结构，但实际上完全是虚构的。作为一个有趣的例子，我询问了出版商Manning关于生产中的LLM的书籍（因为我仍在写作，这本书还不存在）。我得到了以下建议：《Machine
    Learning Engineering in Production》由Mike Del Balso和Lucas Serveén，可在[https://www.manning.com/books/machine-learning-engineering-in-production](books.html)找到；《Deep
    Learning for Coders with Fastai and PyTorch》由Jeremy Howard和Sylvain Gugger，可在[https://www.manning.com/books/deep-learning-for-coders-with-fastai-and-pytorch](books.html)找到。第一本书是完全虚构的。第二本书是真实的，但它不是由Manning出版的。在每种情况下，互联网地址都是完全虚构的。如果您访问这些地址，它们应该返回404错误。
- en: One of the most annoying aspects of hallucinations is that they are often surrounded
    by confident sounding words. LLMs are terrible at expressing uncertainty, in large
    part because of the way they are trained. Consider the case “2+2=”. Would you
    prefer it to respond, “I think it is 4” or just simply “4”? Most would prefer
    to simply get the correct “4” back. This bias is built in as models are often
    given rewards for being more correct or at least sounding like it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉最让人讨厌的一个方面是，它们常常被自信的措辞所包围。LLMs 在表达不确定性方面非常糟糕，这在很大程度上是因为它们的训练方式。考虑一下“2+2=”的情况。你更喜欢它回答“我认为是4”还是简单地“4”？大多数人更愿意简单地得到正确的“4”回答。这种偏见是内在的，因为模型通常会因为更正确或听起来更正确而获得奖励。
- en: There are various explanations as to why hallucination occurs, but the most
    truthful answer is that we don’t know if there’s just one cause. It’s likely a
    combination of several things, thus there isn’t a good fix for it yet. Nevertheless,
    being prepared to counter these inaccuracies and biases of the model are crucial
    to provide the best user experience for your product.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于幻觉发生的原因，有各种解释，但最真实的答案是，我们不知道是否只有一个原因。这可能是几个因素的结合，因此目前还没有很好的解决办法。尽管如此，准备好对抗模型的这些不准确和偏见对于为您的产品提供最佳用户体验至关重要。
- en: 3.2.8 Bias and Ethical Considerations
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.8 偏见和伦理考虑
- en: Just as concerning as the model getting things wrong is when it gets things
    right in the worst possible way. For example, allowing it to encourage users to
    commit suicide[[2]](#_ftn2), teaching your users how to make a bomb[[3]](#_ftn3),
    or participating in sexual fantasies involving children[[4]](#_ftn4). These are
    extreme examples, but prohibiting the model from answering such questions is undeniably
    vital to success.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 和模型出错一样令人担忧的是，当它以最糟糕的方式正确时。例如，允许它鼓励用户自杀[[2]](#_ftn2)，教导用户如何制造炸弹[[3]](#_ftn3)，或参与涉及儿童的性幻想[[4]](#_ftn4)。这些是极端的例子，但禁止模型回答此类问题无可否认地对成功至关重要。
- en: LLMs are trained on vast amounts of text data which is also their primary source
    of bias. Because we’ve found that larger datasets are just as important as larger
    models in producing human-like results, most of these datasets have never truly
    been curated or filtered to remove harmful content, instead choosing to prioritize
    size and a larger collection. Cleaning the dataset is often seen as prohibitively
    expensive, requiring humans to go in and manually verify everything, but there’s
    a lot that could be done with simple regular expressions and other automated solutions.
    By processing these vast collections of content and learning the implicit human
    biases, these models will inadvertently perpetuate them. These biases range from
    sexism and racism to political preferences and can cause your model to inadvertently
    promote negative stereotypes and discriminatory language.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是在大量文本数据上进行训练的，这也是它们偏见的主要来源。因为我们发现，与更大的模型一样，更大的数据集对产生类似人类的结果同样重要，所以这些数据集大多从未真正经过策划或过滤以移除有害内容，而是选择优先考虑大小和更大的收集。清理数据集通常被视为成本过高，需要人类逐个检查和验证，但通过简单的正则表达式和其他自动化解决方案，有很多工作是可以做的。通过处理这些庞大的内容集合并学习隐含的人类偏见，这些模型将无意中延续这些偏见。这些偏见涵盖了从性别歧视和种族主义到政治偏好的各种方面，并可能导致您的模型无意中促进负面刻板印象和歧视性语言。
- en: 3.2.9 Security Concerns
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.9 安全问题
- en: As with all technology we need to be mindful of security. LLMs have been trained
    on large corpus of text and some of it could be harmful or sensitive and shouldn’t
    be exposed so steps should be taken to protect this data from being leaked. The
    bias and ethical concerns from the last section are good examples of conversations
    you don’t want your users to be having, but you could also imagine fine-tuning
    a model on your company’s data and potentially have secrets lost inadvertently
    if proper precautions aren’t taken.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有技术一样，我们需要注意安全问题。LLMs 已经在大量的文本语料库上进行了训练，其中一些可能是有害或敏感的，不应该暴露出来，因此应该采取措施保护这些数据不被泄露。上一节中的偏见和伦理问题是你不希望用户讨论的对话的好例子，但您也可以想象，在公司的数据上微调模型，并且如果没有采取适当的预防措施，可能会无意中丢失秘密。
- en: One should be aware that LLMs are susceptible to adversarial attacks like prompt
    injections. Prompt injections are attacks done by a user to trick the LLM to ignore
    instructions given to it and generate undesired content. For example, if you ask
    ChatGPT what its gender is it appropriately replies that as an AI language model,
    it doesn’t have a gender, however, with clever prompting you may be able to bypass
    these protocols and get it to reveal one. While this example is harmless, I’ve
    seen others successfully extract API keys and other secrets from an LLM, run code
    in non-protected environments, steal environment variables and traverse local
    file systems where the model is served. Not to mention the plethora of examples
    of users using prompting to jailbreak or bypass protocols put in place for ethical
    considerations outlined in the section above. An interesting aside to this, LLMs
    are good at inventing fake secrets! Even successful prompt injection attacks can
    often fail due to LLM hallucinations which can have funny consequences.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，大型语言模型容易受到提示注入等对抗性攻击的影响。提示注入是用户用来欺骗语言模型忽略给定指令并生成不希望的内容的攻击手段。例如，如果你问ChatGPT它的性别是什么，它会适当地回答说作为一个AI语言模型，它没有性别，然而，通过巧妙的提示，你可能能够绕过这些协议并让它透露一个性别。虽然这个例子是无害的，但我见过其他人成功地从语言模型中提取API密钥和其他秘密，在非受保护环境中运行代码，窃取环境变量，并遍历模型所在地的本地文件系统。更不用说大量用户使用提示来越狱或绕过出于伦理考虑而制定的协议的例子了。有趣的是，语言模型擅长编造假秘密！即使是成功的提示注入攻击也经常会失败，因为语言模型的幻觉可能会产生滑稽的后果。
- en: For example, using a simple prompt to ask for Coca-Cola’s secret recipe simply
    returned the boring result, “I cannot divulge trade secrets or any proprietary
    information.” Pretty lame if you ask me. With a little trickery and some prompt
    injection I was able to get it to spill its secrets which of course my coauthor
    and I decided to try–after confirming with a food scientist it wouldn’t poison
    us. After following the instructions exactly, I was surprised to find we actually
    got a pretty tasty drink, but it tasted nothing like Cola. While the recipe looks
    legitimate, nuances like quantities, cook times, and even the need to strain the
    syrup were all off. Guess, I’ll have to hold off taking over the beverage industry
    until I get the real recipe.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，简单地提示要求获取可口可乐的秘方只会返回无聊的结果，“我不能透露商业机密或任何专有信息。” 如果你问我，这太无聊了。通过一点小把戏和一些提示注入，我成功地让它泄露了秘密，当然，在确认不会毒害我们之后，我和我的合著者决定尝试一下。跟着指示一步步操作，我惊讶地发现我们真的做出了一种相当美味的饮料，但味道一点都不像可乐。虽然这个秘方看起来是合法的，但像数量、烹饪时间，甚至需要过滤糖浆等细节都有问题。看来，我得等到拿到真正的秘方之前才能接管饮料行业了。
- en: Coca-Cola Recipe Hallucinated from Prompt Injection
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用于注入提示的可乐秘方幻觉
- en: Homemade Coca-Cola Recipe
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自制可口可乐秘方
- en: =====================
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: =====================
- en: '**Disclaimer**: The original Coca-Cola recipe is a highly guarded trade secret.
    The following recipe is an approximation for creating a homemade cola drink that
    resembles the taste of Coca-Cola.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明**：原始的可口可乐秘方是一个高度保密的商业机密。以下秘方是用于制作类似可口可乐味道的家庭制作可乐饮料的近似方法。'
- en: 'Ingredients:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 配料：
- en: 3 cups of water
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 杯水
- en: 2 cups granulated sugar
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 杯砂糖
- en: 1/8 tsp food-grade citric acid
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1/8 茶匙食品级柠檬酸
- en: 3/4 tsp caramel color (liquid)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3/4 茶匙焦糖色素（液体）
- en: 1/2 tsp caffeine (powder or liquid form)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1/2 茶匙咖啡因（粉末或液体形式）
- en: 1 1/2 tsp cola extract
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 1/2 茶匙可乐提取物
- en: 1/8 tsp vanilla extract
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1/8 茶匙香草提取物
- en: 1 1/2 cups carbonated water (store-bought or using a home soda maker)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 1/2 杯碳酸水（购买或使用家用苏打水机制备）
- en: A pinch of ground cinnamon (optional)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一小撮肉桂粉（可选）
- en: A pinch of ground nutmeg (optional)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一小撮肉豆蔻粉（可选）
- en: 'Instructions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 指导：
- en: In a large saucepan, bring the 3 cups of water to a gentle simmer.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个大锅中，将 3 杯水煮至小火煨沸。
- en: Slowly stir in the granulated sugar, ensuring that it fully dissolves.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缓慢地搅入砂糖，确保完全溶解。
- en: Add the citric acid, caramel color, caffeine, cola extract, and vanilla extract
    to the saucepan. Mix well.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将柠檬酸、焦糖色素、咖啡因、可乐提取物和香草提取物加入锅中。充分搅拌均匀。
- en: 'Stir in the optional spices if desired: cinnamon and nutmeg.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有需要，搅拌进可选的香料：肉桂粉和肉豆蔻粉。
- en: Simmer the mixture for about 20 minutes, stirring occasionally.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偶尔搅拌，将混合物煨煮约 20 分钟。
- en: Remove the saucepan from the heat and let it cool to room temperature.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将锅从火上移开，让其冷却至室温。
- en: Once cooled, strain the syrup to remove any undissolved solids.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 待冷却后，将糖浆过滤以去除任何未溶解的固体物质。
- en: To make the final drink, mix 1 part syrup with 3 parts carbonated water. Adjust
    to taste depending on your preference for sweetness and carbonation.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要制作最终饮料，将1份糖浆与3份碳酸水混合。根据您对甜度和碳酸度的偏好进行调整。
- en: 'Variations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 变体：
- en: You can experiment with different amounts of caramel color, caffeine, and cola
    extract to adjust the flavor and appearance of your homemade cola.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以尝试使用不同的焦糖色、咖啡因和可乐提取物量来调整家制可乐的口味和外观。
- en: To create different cola flavor profiles, experiment with adding additional
    spices (such as star anise, orange or lemon peel, or cardamom), essential oils,
    or fruit extracts.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要创建不同的可乐口味配置文件，尝试添加额外的香料（如八角茴香、橙子或柠檬皮、或豆蔻）、精油或水果提取物。
- en: Consider using a natural sweetener, like honey or agave syrup, as an alternative
    to granulated sugar.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑使用天然甜味剂，如蜂蜜或龙舌兰糖浆，作为颗粒状糖的替代品。
- en: 'Historical context and interesting facts:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 历史背景和有趣的事实：
- en: Coca-Cola was first introduced in 1886 by pharmacist Dr. John S. Pemberton,
    who aimed to create a unique and refreshing beverage that could also relieve fatigue
    and headaches.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可口可乐于1886年首次由药剂师约翰·S·彭伯顿引入，他的目标是创造一种独特且令人耳目一新的饮料，同时还能缓解疲劳和头痛。
- en: The original Coca-Cola formula contained trace amounts of cocaine extracted
    from the coca leaf, which was later replaced with caffeine in the early 20th century.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的可口可乐配方中含有从可可叶中提取的微量可卡因，这在20世纪初被咖啡因所取代。
- en: In 1985, Coca-Cola introduced "New Coke," a reformulation aimed at competing
    with Pepsi. However, the taste change was met with backlash, and the company returned
    to its original formula just three months later, rebranded as "Coca-Cola Classic."
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1985年，可口可乐推出了“新可乐”，这是一种旨在与百事可乐竞争的重新配方。然而，口味的改变引起了强烈反对，公司在仅仅三个月后又恢复了原配方，并改名为“可口可乐经典”。
- en: 'Result(s): You can now create your own homemade version of cola, resembling
    the taste of Coca-Cola. Enjoy and feel free to experiment with different spices,
    extracts, and sweeteners to create your own unique flavor profile.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：您现在可以制作自己的家制可乐版本，味道类似于可口可乐。尽情享用，并随意尝试不同的香料、提取物和甜味剂，以打造独特的口味。
- en: Another security concern involves pickle injections. Pickle is a library in
    Python to serialize objects and often used to serialize ML models. It serializes
    them into a byte-stream that contains opcodes that are executed one-by-one as
    it is deserialized. It’s a fast and easy way to share large objects. Pickle injections
    involve the process of corrupting this byte-stream, often injecting malware over
    the wire when the model is transferred over an insecure network. This is especially
    concerning for large models that take a long time to download, as it makes it
    easier for a third party to intercept the transfer and inject malicious code.
    If this happens, the code that is injected can potentially give the attackers
    access to your system. This can happen when attempting to use the model during
    inference, as the harmful code will execute if it is not detected and properly
    removed. It is important to take precautions such as using secure networks and
    verifying the integrity of the model before use to prevent this type of attack.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个安全问题涉及到了腌制物注射。Pickle是Python中用于序列化对象的库，通常用于序列化ML模型。它将它们序列化为一个包含操作码的字节流，这些操作码会在反序列化时逐个执行。这是一种快速简便的共享大型对象的方式。腌制物注射涉及到损坏这个字节流的过程，通常在模型通过不安全的网络传输时注入恶意软件。这对于下载时间长的大型模型尤其令人担忧，因为这样可以更容易地让第三方拦截传输并注入恶意代码。如果发生这种情况，注入的代码可能会让攻击者访问您的系统。当试图在推理期间使用模型时，如果没有检测到并正确删除恶意代码，它将执行。为防止这种类型的攻击，采取预防措施非常重要，比如使用安全网络并在使用前验证模型的完整性。
- en: 3.2.10 Controlling Costs
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制成本
- en: Working with LLMs involves various cost-related concerns. The first as you probably
    gathered by now is infrastructure costs, which include high-performance GPUs,
    storage, and other hardware resources. We talked about how GPUs are harder to
    procure, that also unfortunately means they are more expensive. Mistakes like
    leaving your service on have always had the potential to rack up the bills, but
    with GPU’s in the mix this type of mistake is even more deadly. These models also
    demand significant computational power, leading to high energy consumption during
    both training and inference. On top of all this, their longer deploy times means
    we are often running them even during low traffic to handle bursty workloads or
    anticipated future traffic. Overall this leads to higher operational costs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LLM 的工作涉及各种与成本相关的问题。第一个问题就像你现在可能意识到的那样，是基础设施成本，包括高性能 GPU、存储和其他硬件资源。我们讨论了 GPU
    更难采购的问题，这也不幸意味着它们更昂贵。像让你的服务保持开启这样的错误一直有可能导致账单增加，但是由于 GPU 的加入，这种类型的错误更为致命。这些模型还需要大量的计算能力，在训练和推理过程中会导致高能耗。除此之外，它们的长时间部署意味着我们通常需要在低流量时运行它们，以处理突发工作负载或预期的未来流量。总体而言，这导致了更高的运营成本。
- en: Additional costs include, managing and storing vast amounts of data used to
    train or fine-tune as well as regular maintenance, such as model updates, security
    measures, and bug fixes, can be financially demanding. As with any technology
    used for business purposes, managing potential legal disputes and ensuring compliance
    with regulations is a concern. Lastly, investing in continuous research and development
    to improve your models and give you a competitive edge will be a factor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其他成本包括，管理和存储用于训练或微调的大量数据以及常规维护，例如模型更新、安全措施和错误修复，可能具有金融压力。与任何用于商业目的的技术一样，管理潜在的法律纠纷，并确保符合法规是一个问题。最后，投资于持续的研究和开发，以改进您的模型并为您提供竞争优势，也将是一个因素。
- en: We talked a bit about the technical concerns when it comes to token limits,
    and these are likely to be solved, but what we didn’t discuss was the cost limitations
    as most API’s charge on a token basis. This makes it more expensive to send more
    context and use better prompts. It also makes it a bit harder to predict costs
    since while you can standardize inputs, you can’t standardize outputs. You never
    can be too sure how many tokens will be returned, making it difficult to govern.
    Just remember with LLMs, it is as important as ever to ensure proper cost engineering
    practices are implemented and followed to ensure costs never get away from you.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关于令牌限制的技术问题我们已经讨论过了一些，这些问题可能会得到解决，但我们没有讨论的是成本限制，因为大多数API是根据令牌计费的。这使得发送更多的上下文和使用更好的提示更加昂贵。它还使得成本的预测有点困难，因为虽然您可以标准化输入，但无法标准化输出。你永远不知道会返回多少个令牌，这使得难以管理。请记住，对于
    LLMs，确保实施和遵循适当的成本工程实践同样重要，以确保成本不会失控。
- en: 3.3 Large Language Model Operations Essentials
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 大型语言模型运维要点
- en: Now that we have a handle of the type of challenge we are grappling with, let's
    take a look at all the different LLMOps practices, tooling, and infrastructure
    to see how different components help us overcome these obstacles. First off, let's
    dive into different practices starting with compression where we will talk about
    shrinking, trimming, and approximating to get models as small as we can. We will
    then talk about distributed computing which is needed to actually make things
    run since the models are so large they rarely fit into a single GPU’s memory.
    After we are finished with those we will venture into infrastructure and tooling
    needed to make it all happen in the next section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解到我们面临的挑战的类型，让我们来看看所有不同的 LLMOps 实践、工具和基础设施，以了解不同的组件如何帮助我们克服这些障碍。首先，让我们深入研究不同的实践，从压缩开始，我们将讨论缩小、修剪和近似来让模型尽可能地小。然后我们将讨论分布式计算，因为模型太大，很少能适应单个
    GPU 的内存，需要实际运行这些模型。在我们完成这些后，我们将进入基础设施和工具，以使所有这些都成为可能。
- en: 3.3.1 Compression
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 压缩
- en: As you were reading about the challenges of LLMs in the last section, you might
    have asked yourself something akin to, “If the biggest problems from LLMs come
    from their size, why don’t we just make them smaller?” If you did, congratulations!
    You are a genius and compression is the practice of doing just that. Compressing
    models as small as we can make them will improve deployment time, reduce latency,
    scale down the number of expensive GPUs needed, and ultimately save money. However,
    the whole point of making the models so stupefyingly gargantuan in the first place
    was because it made them better at what they do. We need to be able to shrink
    them, without losing all the progress we made by making them big in the first
    place.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在上一节阅读关于LLMs的挑战时，你可能会问自己类似于：“如果LLMs的最大问题来自于它们的大小，为什么我们不把它们做得更小呢？”如果是这样的话，恭喜你！你是个天才，压缩就是做到这一点的实践。尽可能地压缩模型将改善部署时间，减少延迟，减少所需的昂贵GPU数量，最终节省金钱。然而，首先让模型变得如此巨大的整个目的是因为它们在所做的事情上变得更好了。我们需要能够缩小它们，而不会失去我们通过使它们变得庞大而取得的所有进展。
- en: This is far from a solved problem, but there are multiple different ways to
    approach the problem with different pros and cons to each method. We’ll be talking
    about several of the methods, starting with the easiest and most effective.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这远非一个解决的问题，但有多种不同的方法可以解决这个问题，每种方法都有不同的优缺点。我们将讨论几种方法，从最简单且最有效的方法开始。
- en: Quantizing
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化
- en: Quantizing is the process of reducing precision in preference of lowering the
    memory requirements. This tradeoff makes intuitive sense. When I was in college,
    we were taught to always round our numbers to the precision of your tooling. Pulling
    out a ruler and measuring my pencil, you wouldn’t believe me if I stated the length
    was 19.025467821973739cm. Even if I used a caliper, I couldn’t verify a number
    so precisely. With our ruler, any number beyond 19.03cm is fantasy. To drive the
    point home, one of my engineering professors once told me, “If you are measuring
    the height of a skyscraper, do you care if there is an extra sheet of paper at
    the top?”
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是在优先降低内存需求的情况下减少精度的过程。这种权衡很直观。当我上大学时，我们被教导要始终将数字舍入到工具精度。拿出尺子测量我的铅笔，如果我说长度是
    `19.025467821973739cm`，你可能不会相信我。即使我使用千分尺，我也无法验证一个如此精确的数字。对于我们的尺子来说，超过 `19.03cm`
    的任何数字都是虚构的。为了强调这一点，我的一个工程学教授曾经告诉我：“如果你在测量摩天大楼的高度，你会在意顶部是否有一张额外的纸吗？”
- en: How we represent numbers inside computers often leads us to believe we have
    better precision than we actually do. To drive this point home, open up a Python
    terminal and add 0.1 + 0.2\. If you’ve never tried this before, you might be surprised
    to find this doesn’t equal 0.3, but 0.30000000000000004\. I’m not going to go
    into the details of the math behind this phenomenon, but the question stands,
    can we reduce the precision without making things worse? We really only need precision
    to the tenth decimal, but reducing the precision is likely to get us a number
    like 0.304 rather than 0.300 which would increase our margin of error.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机内部表示数字的方式常常让我们误以为我们拥有比实际更好的精度。为了强调这一点，在 Python 终端中执行 `0.1 + 0.2`。如果你以前从未尝试过这个操作，你可能会惊讶地发现这并不等于
    `0.3`，而是等于 `0.30000000000000004`。我不打算深入探讨这种现象背后的数学细节，但问题在于，我们能否减少精度而不会让情况变得更糟？我们实际上只需要精确到十分之一的精度，但减少精度可能会得到一个类似
    `0.304` 而不是 `0.300` 的数字，这将增加我们的误差范围。
- en: Ultimately, the only numbers a computer understands are 0 and 1, on or off,
    a single bit. To improve this range we combine multiple of these bits together
    and assign them different meanings. String 8 of them together and you get a byte.
    Using the int8 standard we can take that byte and encode all the integers from
    -128 to 127\. To spare you the particulars because I assume you already know how
    binary works, suffice it to say the more bits we have the larger range of numbers
    we can represent, both larger and smaller. Figure 3.1 shows a few common floating
    point encodings. With 32 bits strung together we get what we pretentiously term
    full precision, and that is how most numbers are stored including the weights
    in machine learning models. Basic quantization moves us from full precision to
    half precision, shrinking models to half their size. There are actually two different
    half precision standards, FP16 and BF16 which differ in how many bits represent
    the range or exponent part. Since BF16 uses the same number of exponents as FP32,
    it’s been found to be more effective for quantizing and you can generally expect
    almost exactly the same level accuracy for half the size of model. If you understood
    the paper and skyscraper analogy above it should be obvious why.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 电脑理解的数字最终只有0和1，即开和关，一个二进制比特。为了扩大数值范围，我们将多个比特组合在一起，并赋予不同的含义。将8个比特串在一起，就组成了一字节。使用int8标准，我们可以取这一字节，编码范围为-128至127的所有整数。为了节省时间，因为我认为你已经知道二进制如何运作，所以简略地说，比特位越多，我们能表示的数值范围就越大，既可以是更大的数值，也可以是更小的数值。图3.1显示了几种常见的浮点数编码。用32个比特串在一起，我们得到自命不凡的全精度，并且大多数数值存储，包括机器学习模型中的权重，都是这么存储的。基本定量化将我们从全精度缩小到半精度，将模型的大小缩小为其一半。实际上，有两种不同的半精度标准，FP16和BF16，它们不同于表示范围或指数部分所用的比特位数。由于BF16使用与FP32相同的指数数量，因此在量化方面更有效，可以预计半大小的模型几乎具有同样的精度水平。如果你理解了上面的论文和摩天大楼的比喻，应该很明显为什么。
- en: Figure 3.1 shows the bit mapping for a few common floating point encodings.
    16-bit float or half precision (FP16), bfloat 16 (BF16), 32-bit float or single
    full precision (FP32), and NVIDIA’s TensorFloat (TF32)
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1显示了几种常见浮点数编码的比特位映射。16位浮点数或半精度(FP16)，bfloat 16 (BF16)，32位浮点数或单精度全精度(FP32)，以及NVIDIA的TensorFloat
    (TF32)。
- en: '![](images/03__image001.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image001.png)'
- en: However, there’s no reason to stop there. We can often push it another byte
    down to 8-bit formats without too much loss of accuracy. There have even already
    been successful research attempts showing selective 4-bit quantization of portions
    of LLMs are possible with only fractional loss of accuracy. The selective application
    of quantization is a process known as dynamic quantization and is usually done
    on just the weights, leaving the activations in full precision to reduce accuracy
    loss.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们不必止步于此。我们通常可以将精度再降一个字节，到8位的格式，而且精度的损失不大。甚至已经有成功的研究尝试，证明通过选择性的4位定量化，可只有部分LLM的精度小部分损失。选择性的定量化是一个被称为动态定量化的过程，通常只在权重上做，而保留激活函数的全精度，以减小精度损失。
- en: The holy grail of quantizing though would be int2, representing every number
    as -1, 0, or 1\. This currently isn’t possible without completely degrading the
    model, but would make the model up to 8 times smaller. The Bloom model would be
    a measly ~40GB, small enough to fit on a single GPU. This is of course, as far
    as quantizing can take us and if we wanted to shrink further we’d need to look
    at additional methods.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 定量化的“圣杯”将是int2，将每个数字表示为-1、0或1。但这目前是不可能的，因为会完全降低模型的性能，但会使模型缩小多达8倍。布隆模型将只有约40GB，足够小，可以装到单个GPU上。当然，定量化只能带我们到达这一步，如果我们想进一步缩小，就需要探索其他方法了。
- en: The best part of quantization though is that it is easy to do. There are many
    frameworks that allow this, but in Listing 3.1 I demonstrate how to use pytorch’s
    quantization library to do a simple post training static quantization (PTQ). All
    you need is the full precision model, some example inputs, and a validation dataset
    to prepare and calibrate with. As you can see it’s only a few lines of code.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，最好的定量化部分是很容易做的。有很多框架允许这样做，但在3.1示例中，我演示了如何使用pytorch的定量化库进行简单的后训练静态定量化（PTQ）。你只需要全精度模型、一些样本输入和一个用于准备和校准的验证数据集。正如你所见，只需要几行代码即可完成。
- en: Listing 3.1 Example PTQ in PyTorch
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列出3.1示例中的PyTorch PTQ。
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Static PTQ is the most straightforward approach to quantizing, done after the
    model is trained and quantizing all the model parameters uniformly. As with most
    formulas in life, the most straightforward approach introduces more error. Oftentimes
    this error is acceptable, but when it’s not we can add extra complexity to reduce
    the accuracy loss from quantization. Some methods to consider are uniform vs non-uniform,
    static vs dynamic, symmetric vs asymmetric, and applying it during or after training.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 静态PTQ是量化最直接的方法，是在模型训练之后并且均匀量化所有模型参数完成的。与生活中的大多数公式一样，最直接的方法会引入更多的误差。通常情况下，这种误差是可以接受的，但当不可接受时，我们可以增加额外的复杂性来减少量化带来的精度损失。一些需要考虑的方法包括均匀
    vs 非均匀、静态 vs 动态、对称 vs 非对称，以及在训练期间还是训练后应用它。
- en: To understand these methods let's consider the case where we are quantizing
    from FP32 to INT8\. In FP32 we essentially have the full range of numbers at our
    disposal, but in INT8 we only have 256 values, we are trying to put a genie into
    a bottle and it’s no small feat. If you study the weights in your model, you might
    notice that the majority of the numbers are fractions between [-1, 1]. We could
    take advantage of this by then using an 8-bit standard that represents more values
    in this region in a non-uniform way instead of the standard uniform [-128, 127].
    While mathematically possible, unfortunately, any such standards aren’t commonplace
    and modern day deep learning hardware and software are not designed to take advantage
    of this. So for now, it's best to just stick to uniform quantization.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这些方法，让我们考虑一种情况，即我们从FP32量化为INT8。在FP32中，我们基本上可以使用整个数字范围，但在INT8中，我们只有256个值，我们试图把一个精灵装进瓶子里，这并不是一件小事。如果你研究你模型中的权重，你可能会注意到大多数数字都是在[-1,
    1]之间的分数。我们可以利用这一点，然后使用一个非均匀的8位标准，以非均匀的方式表示此区域中的更多值，而不是标准的均匀[-128, 127]。虽然在数学上是可能的，但不幸的是，任何此类标准都不常见，并且现代深度学习硬件和软件都没有设计来利用这一点。所以现在最好还是坚持使用均匀量化。
- en: The simplest approach to shrink the data is to just normalize it, but since
    we are going from a continuous scale to a discrete scale there are a few gotchas,
    so let's explore those. First, we start by taking the min and max and scale them
    down to match our new number range, we would then bucket all the other numbers
    based on where they fall. Of course, if we have really large outliers, we may
    find all our other numbers squeezed into just one or two buckets completely ruining
    any granularity we once had. To prevent this, we can simply clip any large numbers.
    This is what we do in static quantization. However, before we clip the data, what
    if we chose a range and scale that captures the majority of our data beforehand?
    We need to be careful, since if this dynamic range is too small, we will introduce
    more clipping errors, if it’s too big, we will introduce more rounding errors.
    The goal of dynamic quantization of course is to reduce both errors.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 缩小数据的最简单方法就是将其归一化，但由于我们从连续尺度到离散尺度，因此有一些注意事项，让我们来探讨一下。首先，我们从最小和最大值开始，并将它们缩小以匹配我们的新数字范围，然后我们会根据它们所处的位置将所有其他数字分桶。当然，如果我们有非常大的异常值，我们可能会发现所有其他数字都被挤压到只有一个或两个桶中，完全破坏了我们曾经拥有的任何粒度。为了防止这种情况发生，我们可以简单地剪裁任何大的数字。这就是我们在静态量化中所做的。然而，在剪裁数据之前，如果我们选择一个范围和比例来预先捕获大多数我们的数据呢？我们需要小心，因为如果这个动态范围太小，我们会引入更多的剪裁错误，如果它太大，我们会引入更多的舍入错误。动态量化的目标当然是减少两种错误。
- en: Next we need to consider the symmetry of the data. Generally in normalization
    we force the data to be normal and thus symmetric, however, we could choose to
    scale the data in a way that leaves any asymmetry it had. By doing this we could
    potentially reduce our overall loss due to the clipping and rounding errors, but
    it’s not a guarantee.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要考虑数据的对称性。通常在归一化中，我们强制数据是正常的，因此对称的，然而，我们可以选择以一种保留任何不对称性的方式缩放数据。通过这样做，我们可能会减少由于剪裁和舍入错误导致的总体损失，但这并不是一个保证。
- en: As a last resort, if none of these other methods failed to reduce accuracy loss
    of the model, we can use Quantization Aware Training (QAT). QAT is a simple process
    where we add a fake quantization step during training of the model. By fake, I
    mean, we clip and round the data while leaving it in full precision. This allows
    the model to adjust for the error and bias introduced by quantization while it’s
    training. QAT is known to produce higher accuracy compared to other methods but
    at a much higher cost in time to train.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的手段，如果这些方法都无法减少模型的精度损失，我们可以使用量化感知训练（QAT）。QAT 是一个简单的过程，在模型训练过程中添加一个假量化步骤。所谓假，是指我们在训练过程中对数据进行剪裁和舍入，但保留其完整精度。这允许模型在训练过程中调整由量化引入的误差和偏差。众所周知，与其他方法相比，QAT
    能够产生更高的精度，但训练时间成本更高。
- en: Quantization Methods
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化方法
- en: 'Uniform vs Non-uniform: whether or not we use an 8-bit standard that is uniform
    in the range it represents or non-uniform to be more precise in the -1 to 1 range.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀 vs 非均匀：我们是否使用一个在范围内均匀的 8 位标准，或者非均匀地更精确地表示 -1 到 1 的范围。
- en: 'Static vs Dynamic: Choosing to adjust the range or scale before clipping in
    an attempt to reduce clipping and rounding errors and reduce data loss.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 静态 vs 动态：选择在剪裁之前调整范围或比例，以尝试减少剪裁和舍入错误，并减少数据损失。
- en: 'Symmetric vs Asymmetric: Normalizing the data to be normal and force symmetry,
    or choosing to keep any asymmetry and skew.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对称 vs 非对称：将数据归一化为正常并强制对称，或选择保留任何不对称和偏斜。
- en: 'During or After Training: Quantization after training is really easy to do,
    and while doing it during training is more work it leads to reduced bias and better
    results.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间或之后：在训练后进行量化非常容易，虽然在训练期间进行量化需要更多工作，但会导致减少偏差和更好的结果。
- en: Quantizing is a very powerful tool. Not only does it reduce the size of the
    model, but it also reduces the computational overhead required to run the model
    thus reducing latency and cost of running the model. But the best fact about quantization
    is that it can be done after the fact, so you don’t have to worry about whether
    or not your data scientists remembered to quantize the model during training using
    processes like QAT. This is why quantization has become so popular when working
    with LLMs and other large machine learning models. While reduced accuracy is always
    a concern with compression techniques, compared to other methods, quantization
    is a win-win-win.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一个非常强大的工具。它不仅减小了模型的大小，还减少了运行模型所需的计算开销，从而降低了运行模型的延迟和成本。但量化最好的一点是它可以在事后进行，因此您不必担心数据科学家是否记得在训练过程中使用
    QAT 等流程量化模型。这就是为什么在处理 LLM 和其他大型机器学习模型时，量化变得如此流行的原因。尽管精度降低始终是压缩技术的一个问题，但与其他方法相比，量化是一个三赢局面。
- en: Pruning
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精简
- en: Congratulations, you just trained a brand new LLM! With billions of parameters
    all of them must be useful right? Wrong! Unfortunately, as with most things in
    life, the model’s parameters tend to follow the Pareto Principle. About 20% of
    the weights lead to 80% of the value. “If that’s true,” you may be asking yourself,
    “Why don’t we just go and cut out all the extra fluff?” Great idea! Give yourself
    a pat on the back. Pruning is the process of weeding out and removing any parts
    of the model that we deem unworthy.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，您刚刚训练了一个全新的 LLM！它拥有数十亿个参数，所有这些参数都应该是有用的，对吗？错了！不幸的是，与生活中的大多数事物一样，模型的参数往往遵循帕累托法则。大约
    20% 的权重导致了 80% 的价值。“如果是这样的话，”您可能会问，“为什么我们不直接去掉所有多余的东西？”好主意！给自己一个拍手。精简是我们剔除并删除我们认为不值得的模型部分的过程。
- en: 'There are essentially two different pruning methods: **structured** and **unstructured**.
    Structured pruning is the process of finding structural components of a model
    that aren’t contributing to the model’s performance and then removing them. Whether
    they be filters, channels, or layers in the neural network. The advantages to
    this method is that your model will be a little smaller but keep the same basic
    structure, which means we don’t have to worry about losing hardware efficiencies,
    we are also guaranteed a latency improvement as there will be less computations
    involved.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上有两种不同的精简方法：**结构化** 和 **非结构化**。结构化精简是找到模型中不对模型性能有贡献的结构组件，然后移除它们的过程。无论是滤波器、通道还是神经网络中的层。这种方法的优点是您的模型将会变小一点，但保持相同的基本结构，这意味着我们不必担心失去硬件效率，我们还保证了延迟改进，因为将会减少计算量。
- en: Unstructured pruning on the other hand, is shifting through the parameters and
    zeroing out the less important ones that don’t contribute much to the model’s
    performance. Unlike structured pruning, we don’t actually remove any parameters,
    just set them to zero. From this, we can imagine that a good place to start would
    be any weights or activations that are already close to 0\. Of course, while this
    effectively reduces the size of a model this also means we don’t cut out any computations,
    so it’s common to only see minimal latency improvement–if at all. But a smaller
    model still means faster load times and less GPUs to run. It also gives us very
    fine-grained control over the process, allowing us to shrink a model further than
    we could with structured pruning with less impact to performance too.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，非结构化剪枝是通过筛选参数并将对模型性能贡献不大的参数归零来进行的。与结构化剪枝不同，我们实际上并不删除任何参数，只是将它们设为零。由此可想而知，一个好的起点可能是任何已接近0的权重或激活。当然，虽然这有效地减小了模型的大小，但这也意味着我们没有剪掉任何计算，所以通常只能看到最小的延迟改进——如果有的话。但更小的模型仍然意味着更快的加载时间和更少的GPU运行。它还使我们对过程具有非常细粒度的控制，使我们能够比使用结构化剪枝更进一步地减小模型，并且对性能的影响也更小。
- en: Like quantization, pruning can be done after a model is trained. However, unlike
    quantization, it’s common practice to see additional fine-tuning needing to be
    done to prevent too much loss of performance. It’s becoming more common to just
    include pruning steps during the model training to avoid the need to fine-tune
    later on. Since a more sparse model will have fewer parameters to tune, adding
    these pruning steps may help a model converge faster as well.[[5]](#_ftn5)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 像量化一样，剪枝可以在模型训练后进行。然而，与量化不同，通常需要进行额外的微调以防止性能损失过多。现在越来越普遍的做法是在模型训练过程中包括剪枝步骤，以避免以后需要进行微调。由于更稀疏的模型将具有较少的参数需要调整，因此添加这些剪枝步骤也可能有助于模型更快地收敛。
- en: You’ll be surprised at how much you can shrink a model with pruning while minimally
    impacting performance. How much? In the SparseGPT[[6]](#_ftn6) paper, a method
    was developed to try to automatically one shot the pruning process without the
    need for finetuning after. They found they could decrease a GPT-3 model by 50-60%
    without issue! Depending on the model and task they even saw slight improvements
    in a few of them. Looking forward to seeing where Pruning takes us in the future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你会对剪枝可以使模型缩小而对性能影响最小感到惊讶。究竟有多少呢？在SparseGPT论文中，开发了一种方法来尝试自动一次性地进行剪枝过程，而无需之后进行微调。他们发现他们可以将GPT-3模型减小50-60%而不会出现问题！根据模型和任务，他们甚至看到了其中一些任务略有改善。期待看到剪枝在未来带领我们走向何方。
- en: Knowledge Distillation
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: Knowledge distillation is probably the coolest method of compression in my mind.
    It’s a simple idea too, we’ll take the large LLM, and have it train a smaller
    language model to copy it. What’s nice about this method is that the larger LLM
    provides essentially an infinite dataset for the smaller model to train on, which
    can make the training quite effective. Because of the simple fact that the larger
    the dataset the better the performance, we've often seen smaller models reach
    almost the same level as their teacher counterparts in accuracy.[[7]](#_ftn7)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏可能是我心目中最酷的压缩方法。这也是一个简单的想法，我们将大型LLM进行训练，让它训练一个更小的语言模型来复制它。这种方法的好处在于，较大的LLM为较小的模型提供了本质上无限的训练数据集，这可以使训练非常有效。由于一个简单的事实，即数据集越大，性能越好，我们经常看到较小的模型在准确性方面几乎达到与其教师对应模型相同的水平。
- en: A smaller model trained this way is guaranteed to both be smaller and improve
    latency. The downside is that it’s going to require us to train a completely new
    model. Which is a pretty significant upfront cost to pay. Any future improvements
    to the teacher model will require being passed down to the student model, which
    can lead to complex training cycles and version structure. It’s definitely a lot
    more work compared to some of the other compression methods.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式训练的较小模型保证既更小又能提高延迟。缺点是这将要求我们训练一个全新的模型。这是一个相当重大的前期成本。任何对教师模型的未来改进都将需要传递给学生模型，这可能会导致复杂的训练周期和版本结构。与其他一些压缩方法相比，这绝对是更多的工作。
- en: The hardest part about knowledge distillation though is that we don’t really
    have good recipes for them yet. Tough questions like, “How small can the student
    model be?” will have to be solved through trial and error. There’s still a lot
    to learn and research to be done here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，知识蒸馏的最大难题是我们还没有一个好的配方。像“学生模型可以有多小？”这样的难题将不得不通过反复试验来解决。在这里仍有很多需要学习和研究的地方。
- en: However, there has been some exciting work in this field via Stanford’s Alpaca[[8]](#_ftn8).
    Instead of training a student model from scratch, they instead chose to finetune
    the open source LLaMA 7B parameter model using OpenAI’s GPT3.5’s 175 billion parameter
    model as a teacher via knowledge distillation. A simple idea but it paid off big,
    as they were able to get great results from their evaluation. The biggest surprise
    was the cost, as they only spent $500 on API costs to get the training data from
    the teacher model, and $100 worth of GPU training time to finetune the student
    model. Granted, if you did this for a commercial application you’d be violating
    OpenAI’s terms of service, so best to stick to using your own or open source models
    as the teacher.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，斯坦福大学的阿尔帕卡通过一些激动人心的工作在这个领域取得了一些成果[[8]](#_ftn8)。他们选择不从头开始训练学生模型，而是选择使用 OpenAI
    的 GPT3.5 的 1750 亿参数模型作为老师通过知识蒸馏来微调开源的 LLaMA 7B 参数模型。这是一个简单的想法，但收效卓著，因为他们能够从评估中获得很好的结果。最大的惊喜是成本，因为他们只花了
    $500 的 API 成本从老师模型获得训练数据，以及 $100 的 GPU 训练时间来微调学生模型。当然，如果你把这个用于商业应用，你将违反 OpenAI
    的服务条款，最好还是使用自己的或开源模型作为老师。
- en: Low-rank Approximation
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩近似
- en: Low-rank approximation, also known as low-rank factorization, low-rank decomposition,
    matrix factorization (too many names! I blame the mathematicians), uses linear
    algebra math tricks to simplify large matrices or tensors finding a lower-dimensional
    representation. There are several techniques to do this. Singular Value Decomposition
    (SVD), Tucker Decomposition(TD), and Canonical Polyadic Decomposition (CPD) are
    the most common ones you run into.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩近似，也称为低秩分解、矩阵分解（名字太多了！我责怪数学家），利用线性代数的数学技巧简化大矩阵或张量，找到一个低维度的表示。有几种技术可以做到这一点。奇异值分解（SVD）、Tucker
    分解（TD）和规范多重分解（CPD）是你经常遇到的最常见的几种。
- en: In Figure 3.2 we show the general idea behind the SVD method. Essentially we
    are going to take a very large matrix, A, and break it up into 3 smaller matrices,
    U, ∑, and V. While U and V are there to ensure we keep the same dimensions and
    relative strengths of the original matrix, ∑ allows us to apply a direction and
    bias. The smaller ∑ is, the more we end up compressing and reducing the total
    number of parameters, but the less accurate the approximation becomes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 3.2 中，我们展示了奇异值分解（SVD）方法的基本思想。本质上，我们将把一个非常大的矩阵 A 分解成 3 个较小的矩阵，U、∑ 和 V。虽然 U
    和 V 旨在确保我们保持原始矩阵的相同维度和相对强度，∑ 则允许我们应用方向和偏差。∑ 越小，我们就会更多地进行压缩和减少总参数数量，但近似度也会降低。
- en: Figure 3.2 Example of SVD a Low-rank Approximation. A is a large matrix with
    dimensions N and M. We can approximate it with three smaller matrices, U with
    dimensions M and P, ∑ a square matrix with dimension P, and V with dimensions
    N and P (here we show the transpose). Usually both P<<M and P<<N are true.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.2 显示了 SVD 低秩近似的示例。A 是一个尺寸为 N 和 M 的大矩阵。我们可以用三个较小的矩阵来近似它，U 的尺寸为 M 和 P，∑ 是一个尺寸为
    P 的方阵，而 V 的尺寸为 N 和 P（这里我们显示了转置）。通常情况下，P<<M 和 P<<N 都成立。
- en: '![](images/03__image002.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image002.png)'
- en: To help solidify this concept, it may help to see a concrete example. In Listing
    3.2 we show a simple example of SVD at work compressing a 4x4 matrix. For this
    we only need the basic libraries SciPy and NumPy which are imported on lines 1
    and 2\. Line 3 we define the matrix, and then line 9 we apply SVD to it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助巩固这个概念，看一个具体的例子可能会有所帮助。在列表 3.2 中，我们展示了一个简单的 SVD 示例，演示了对一个 4x4 矩阵进行压缩。对此，我们只需要基本的
    SciPy 和 NumPy 库，它们在第 1 和第 2 行被导入。第 3 行我们定义了矩阵，然后第 9 行我们对其应用了 SVD。
- en: Listing 3.2 Example SVD Low-rank Approximation
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.2 显示了 SVD 低秩近似的示例
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Taking a moment to inspect U, Sigma, and the transpose of V, we see a 4x1 matrix,
    a 1x1 matrix, and a 1x4 matrix respectively. All in all we now only need 9 parameters
    vs the original 16, shrinking the memory footprint almost in half.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们稍作观察 U、Sigma 和 V 的转置，我们会看到一个 4x1 矩阵、一个 1x1 矩阵和一个 1x4 矩阵。总而言之，现在我们只需要 9 个参数，而不是原来的
    16 个，将内存占用几乎减少了一半。
- en: Lastly, we multiply the matrices back together to get an approximation of the
    original matrix. In this case, the approximation isn’t all that great, but we
    can still see the general order and magnitudes match the original matrix.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将矩阵相乘以获得原始矩阵的近似。 在这种情况下，近似并不是很好，但我们仍然可以看到一般的顺序和数量级与原始矩阵相匹配。
- en: '[PRE2]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Unfortunately, I’m not aware of anyone actually using this to simply compress
    models in production most likely due to the poor accuracy of the approximation.
    What they are using it for, and this is important, is adaptation and finetuning;
    which is where LoRA[[9]](#_ftn9) comes in, Low Rank Adaptation. Adaptation is
    simply the process of fine-tuning a generic or base model to do a specific task.
    LoRA applies SVD low rank approximation to the attention weights, or rather, to
    injected update matrices that run parallel to the attention weights, allowing
    us to fine-tune a much smaller model. LoRA has become very popular because it
    makes it a breeze to take an LLM, shrink the trainable layers to a tiny fraction
    of the original model and then allow anyone to train it on commodity hardware.
    You can get started with LoRA by using the PEFT[[10]](#_ftn10) library from HuggingFace,
    where they have several LoRA tutorials you can check out.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我不知道有人是否真的在生产中使用它来简单地压缩模型，很可能是因为近似精度较差。 他们使用它的方式是，这一点很重要，是适应和微调；这就是 LoRA[[9]](#_ftn9)
    的作用，即低秩适应。 适应只是将通用或基础模型微调为执行特定任务的过程。 LoRA 将 SVD 低秩近似应用于注意权重，或者更确切地说，应用于与注意权重并行运行的注入更新矩阵，从而允许我们微调一个更小的模型。
    LoRA 已经变得非常流行，因为它使得将 LLM 缩小为原始模型的可训练层的一小部分，然后允许任何人在商品硬件上对其进行训练变得轻而易举。 你可以使用 HuggingFace
    的 PEFT[[10]](#_ftn10) 库开始使用 LoRA，他们有几个 LoRA 教程供你参考。
- en: Mixture of Experts
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 专家混合
- en: Mixture of experts (MoE) is a technique where we replace the feed forward layers
    in a transformer with MoE layers instead. MoE’s are a group of sparsely activated
    models. It differs from ensemble techniques in that typically only one or a few
    expert models will be run, rather than combining results from all models. The
    sparsity is often induced by a Gate mechanism that learns which experts to use,
    and/or a Router mechanism that determines which experts should even be consulted.
    In Figure 3.3 we demonstrate the MoE architecture with potentially N experts,
    as well as show where it goes inside a decoder stack.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合（MoE）是一种技术，我们在transformers中将前馈层替换为 MoE 层。 MoE 是一组稀疏激活的模型。 它与集成技术不同，通常只运行一个或少数几个专家模型，而不是组合所有模型的结果。
    稀疏性通常是由一个学习哪些专家使用的门机制和/或确定哪些专家甚至应该被咨询的路由器机制引起的。 在图 3.3 中，我们展示了具有可能是 N 个专家的 MoE
    架构，以及它在解码器堆栈中的位置。
- en: Figure 3.3 Example Mixture of Experts model with both a Gate and Router to control
    flow. The MoE model is used to replace the FFN layers in a transformer, here we
    show it replacing the FFN in a Decoder.
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3 示例专家混合模型，同时具有门控和路由器来控制流程。 MoE 模型用于替换transformers中的 FFN 层，这里我们展示它替换解码器中的 FFN。
- en: '![](images/03__image003.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image003.png)'
- en: Depending on how many experts you have, the MoE layer could potentially have
    more parameters than the FFN leading to a larger model, but in practice this isn’t
    the case since engineers and researchers are aiming to create a smaller model.
    What we are guaranteed to see though is a faster computation path and improved
    inference times. However, what really makes MoE stand out is when it’s combined
    with quantization. One study[[11]](#_ftn11) between Microsoft and NVIDIA showed
    they were able to reach 2-bit quantization with only minimal impact to accuracy
    using MoE!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您拥有多少专家，MoE 层可能会比 FFN 有更多的参数，从而导致更大的模型，但在实践中情况并非如此，因为工程师和研究人员的目标是创建一个更小的模型。
    不过，我们确实会看到更快的计算路径和改进的推理时间。 然而，MoE 真正脱颖而出的是当它与量化结合时。 微软和 NVIDIA 之间的一项研究[[11]](#_ftn11)
    表明，他们使用 MoE 仅对准确性产生了最小影响，就能达到 2 位量化！
- en: Of course, since this is a pretty big change to the model’s structure it will
    require finetuning afterwards. You should also be aware that MoE layers often
    reduce a model’s generalizability so it’s best when used on models designed for
    a specific task. There are several libraries to implement MoE layers, but I’d
    recommend checking out DeepSpeed[[12]](#_ftn12).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，由于这是对模型结构的相当大的改变，所以之后需要微调。 您还应该意识到 MoE 层通常会降低模型的泛化能力，因此最好在为特定任务设计的模型上使用。
    有几个库可以实现 MoE 层，但我建议看看 DeepSpeed[[12]](#_ftn12)。
- en: 3.3.2 Distributed Computing
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 分布式计算
- en: Distributed computing is a technique used in deep learning to parallelize and
    speed-up large, complex neural networks by dividing the workload across multiple
    devices or nodes in a cluster. This approach significantly reduces training and
    inference times by enabling concurrent computation, data parallelism, and model
    parallelism. With the ever-growing size of datasets and complexity of models,
    distributed computing has become crucial for deep learning workflows, ensuring
    efficient resource utilization and enabling researchers to effectively iterate
    on their models. Distributed computing is one of the core practices that separate
    deep learning from machine learning, and with LLMs we have to pull out every trick
    in the book. Let’s look at different parallel processing practices to take full
    advantage of distributed computing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算是一种在深度学习中用于将大型复杂神经网络并行化和加速的技术，通过将工作负载分配到集群中的多个设备或节点上来实现。这种方法通过启用并发计算、数据并行和模型并行显著减少了训练和推理时间。随着数据集的不断增大和模型的复杂性，分布式计算对于深度学习工作流程变得至关重要，确保了资源的高效利用，并使研究人员能够有效地迭代其模型。分布式计算是将深度学习与机器学习区分开的核心实践之一，而在
    LLMs 中，我们必须尽一切可能利用各种技巧。让我们看看不同的并行处理实践，以充分利用分布式计算的优势。
- en: Data Parallelism
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据并行性
- en: Data parallelism is what most people think about when they think about running
    processes in parallel, it’s also the easiest to do. The practice involves splitting
    up the data and running them through multiple copies of the model or pipeline.
    For most frameworks this is easy to set up, for example, in PyTorch you can use
    the DistributedDataParallel method. There’s just one catch for most of these set-ups
    and that is your model has to be able to fit onto one GPU. This is where a tool
    like Ray.io comes in.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行是大多数人在想到并行运行进程时所想到的，也是最容易实现的。这种做法涉及将数据分割并通过模型或流水线的多个副本运行它们。对于大多数框架来说，这很容易设置，例如，在
    PyTorch 中，你可以使用 DistributedDataParallel 方法。对于大多数这样的设置，有一个注意事项，那就是你的模型必须能够适合在一个
    GPU 上运行。这就是像 Ray.io 这样的工具发挥作用的地方。
- en: Ray is an open-source project designed for distributed computing, specifically
    aimed at parallel and cluster computing. It's a flexible and user-friendly tool
    which simplifies distributed programming and helps developers execute concurrent
    tasks in parallel with ease. Ray is primarily built for machine learning and other
    high-performance applications but can be utilized in other applications. In Listing
    3.3 we give a simple example of using Ray to distribute a task. The beauty of
    Ray is the simplicity–all we need to do to make our code run in parallel is add
    a decorator. Sure beats the complexity of multithreading or asynchronization set-ups.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 是一个专为分布式计算设计的开源项目，专门针对并行和集群计算。它是一个灵活且用户友好的工具，简化了分布式编程，并帮助开发人员轻松并行执行任务。Ray
    主要用于机器学习和其他高性能应用，但也可以用于其他应用。在列表 3.3 中，我们给出了使用 Ray 分发任务的简单示例。Ray 的美妙之处在于简单性——我们只需添加一个装饰器就可以使我们的代码并行运行。这绝对比多线程或异步设置的复杂性要好得多。
- en: Listing 3.3 Example Ray Parallelization Task
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.3 示例 Ray 并行化任务
- en: '[PRE3]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Ray uses the concept of tasks and actors to manage distributed computing. Tasks
    are functions, whereas actors are stateful objects that can be invoked and run
    concurrently. When you execute tasks using Ray, it handles distributing tasks
    across the available resources (e.g., multi-core CPUs or multiple nodes in a cluster).
    For LLMs, we would need to set up a Ray cluster[[13]](#_ftn13) in a cloud environment
    as this would allow each pipeline to run on a node with as many GPUs as needed,
    greatly simplifying the infrastructure set up to run LLMs in parallel.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 使用任务（tasks）和执行者（actors）的概念来管理分布式计算。任务是函数，而执行者是可以被调用并且可以并发运行的有状态对象。当你使用 Ray
    执行任务时，它会处理将任务分发到可用资源（例如多核 CPU 或集群中的多个节点）上。对于 LLMs，我们需要在云环境中设置一个 Ray 集群[[13]](#_ftn13)，因为这样可以让每个流水线在需要的
    GPU 数量的节点上运行，极大地简化了并行运行 LLMs 的基础架构设置。
- en: There are multiple alternatives out there, but Ray has been gaining a lot of
    traction and becoming more popular as more and more machine learning workflows
    require distributed training. My team has had great success with it. By utilizing
    Ray, developers can ensure better performance and more efficient utilization of
    resources in distributed workflows.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有多种替代方案可选，但 Ray 因为越来越多的机器学习工作流程需要分布式训练，因此越来越受欢迎并获得了很大的关注。我的团队在使用中取得了巨大成功。通过利用
    Ray，开发人员可以确保更好的性能和更高效的资源利用率在分布式工作流中。
- en: Tensor Parallelism
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量并行性
- en: Tensor parallelism is taking advantage of matrix multiplication properties to
    split up the activations across multiple processors, running the data through,
    and then combining them on the other side of the processors. Figure 3.4 demonstrates
    how this process works for a matrix, which can be parallelized in two separate
    ways that give us the same result. Imagine that Y is a really big matrix that
    can’t fit on a single processor, or more likely, a bottleneck in our data flow
    that takes too much time to run all the calculations. In either case, we could
    split Y up, either by columns or by rows, run the calculations, and then combine
    the results after. In this example we are dealing with matrices but in reality
    we are often dealing with tensors that have more than two dimensions, but the
    same mathematical principles that make this work still apply.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行利用矩阵乘法属性将激活分布在多个处理器上，通过数据运行，然后在处理器的另一侧将它们组合起来。图 3.4 展示了这个过程如何适用于矩阵，可以以两种给出相同结果的方式并行化。想象一下
    Y 是一个非常大的矩阵，无法放在单个处理器上，或者更有可能，我们数据流中的瓶颈需要太长时间来运行所有计算。在任何情况下，我们都可以将 Y 分割，可以按列或按行进行，运行计算，然后在之后合并结果。在这个例子中，我们处理的是矩阵，但实际上我们经常处理具有两个以上维度的张量，但使这项工作起作用的相同数学原理仍然适用。
- en: Choosing which dimension to parallelize is a bit of an art, but a few things
    to remember to help make this decision easier. First, how many columns or rows
    do you have? In general, you want to pick a dimension that has more than the number
    of processors you have, else you will end up stopping short. Generally this isn’t
    a problem but with tools like Ray discussed in the last section, parallelizing
    in a cluster and spinning up loads of processes is a breeze. Second, different
    dimensions have different multiplicity costs. For example, column parallelism
    requires us to send the entire dataset to each process, but with the benefit of
    concatenating them together at the end which is fast and easy. Row parallelism
    however, allows us to break up the dataset into chunks, but requires us to add
    the results, a more expensive operation than concatenating. You can see that one
    operation is more I/O bound, while the other is more computation bound. Ultimately,
    the best dimension will be dataset dependent, as well as hardware limited. It
    will require experimentation to fully optimize this, but a good default is to
    just choose the largest dimension.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 选择要并行化的维度有点像一门艺术，但有几点需要记住，以帮助做出这个决定更容易。首先，您有多少列或行？一般来说，您希望选择一个具有超过您拥有的处理器数量的维度，否则您将会提前停止。通常这不是一个问题，但是使用像上一节讨论的
    Ray 这样的工具，在集群中并行化并大量启动进程非常容易。其次，不同的维度具有不同的复杂度成本。例如，列并行要求我们将整个数据集发送到每个进程，但通过在最后将它们简单地连接在一起，这是快速且容易的。然而，行并行允许我们将数据集分解成块，但需要我们将结果相加，这是一种比简单连接更昂贵的操作。您可以看到其中一个操作更受
    I/O 限制，而另一个更受计算限制。最终，最佳的维度将取决于数据集，以及硬件限制。这将需要实验来完全优化，但一个很好的默认选择是选择最大的维度。
- en: Figure 3.4 Tensor Parallelism example showing that you can break up tensors
    by different dimensions and get the same end result. Here we compare column and
    row parallelism of a Matrix.
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.4 张量并行示例显示，您可以通过不同的维度分解张量，并获得相同的最终结果。在这里，我们比较了矩阵的列并行和行并行。
- en: '![](images/03__image004.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image004.png)'
- en: Tensor parallelism allows us to split up the heavy computation layers like MLP
    and Attention layers onto different devices, but doesn’t help us with Normalization
    or Dropout layers that don’t utilize tensors. To get better overall performance
    of our pipeline we can add sequence parallelism which targets these blocks[[14]](#_ftn14).
    Sequence parallelism is a process that partitions activations along the sequence
    dimension, preventing redundant storage, and can be mixed with tensor parallelism
    to achieve significant memory savings with minimal additional computational overhead.
    In combination, they reduce the memory needed to store activations in Transformer
    models. In fact, they nearly eliminate activation recomputation and save activation
    memory up to 5x.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行使我们能够将重量级计算层（如 MLP 和 Attention 层）分配到不同的设备上，但无法帮助我们处理不使用张量的归一化或 Dropout 层。为了更好地提高我们管道的整体性能，我们可以添加序列并行，以针对这些块。序列并行是一种沿序列维度分割激活的过程，防止冗余存储，并且可以与张量并行混合以实现显著的内存节省和最小的额外计算开销。组合使用它们可以减少
    Transformer 模型中存储激活所需的内存。实际上，它们几乎消除了激活重新计算，并将激活内存节省高达 5 倍。
- en: Figure 3.5 Combining tensor parallelism that focuses on computational heavy
    layers with sequence parallelism to reduce memory overhead to create a fully parallel
    process for the entire transformer.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5结合张量并行和序列并行，将计算密集型的层分布式处理，减少内存开销，创建了整个Transformer的完全并行的过程。
- en: '![](images/03__image005.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image005.png)'
- en: Figure 3.5 shows how combining both tensor parallelism, that allows us to distribute
    the computationally heavy layers, and sequence parallelism that does the same
    for the memory limiting layers, allows us to fully parallelize the entire transformer
    model. Together, they allow for extremely efficient use of resources.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5展示了结合张量并行和序列并行的效果，张量并行允许我们分布计算负载重的层，而序列并行则为内存限制层进行同样的操作，这样我们就可以完全并行化整个Transformer模型。两者结合使资源利用率极高。
- en: Pipeline Parallelism
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 管道并行性
- en: So far we can now run lots of data, and speed up any bottlenecks, but none of
    that matters because our model is too big; we can’t fit it into a single GPU’s
    memory to even get it to run. That’s where pipeline parallelism comes in and is
    the process of splitting up a model vertically and putting each part onto a different
    GPU. This creates a pipeline as input data will go to the first GPU, process,
    then transfer to the next GPU, and so on until it’s run through the entire model.
    While other parallelism techniques improve our processing power and speed up inference,
    pipeline parallelism is required to just get it to run and it comes with some
    major downsides, mainly device utilization.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们现在可以运行大量数据，并加速任何瓶颈，但这些都无关紧要，因为我们的模型太大了，我们无法将其放入单个GPU的内存中甚至无法运行它。这就是管道并行性所涉及的，并且是将模型垂直划分并将每个部分放在不同的GPU上的过程。这样就创建了一个管道，输入数据将传递到第一个GPU，进行处理，然后传输到下一个GPU，依此类推，直到整个模型都被运行。虽然其他并行技术提高了我们的处理能力并加快了推理，但管道并行性仅仅是为了让它运行而必需的，并且它带来了一些主要的缺点，主要是设备利用率。
- en: To understand where this downside comes from and how to mitigate it, let's first
    consider the naive approach to this, where we simply run all the data at once
    through the model. What we find is that this leaves a giant “bubble” of underutilization.
    Since the model is broken up, we have to process everything sequentially through
    the devices. This means that while one GPU is processing, the others are sitting
    idle. In Figure 3.6 we can see this naive approach and a large bubble of inactivity
    as the GPUs sit idle. We also see a better way to take advantage of each device.
    We do this by sending the data in small batches. A smaller batch allows the first
    GPU to pass on what it was working on quicker and move on to another batch. This
    allows the next device to get started earlier and reduces the size of the bubble.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这种下降趋势的原因以及如何减轻它，让我们首先考虑一下这种天真的方法，我们只是简单地通过模型一次性运行全部数据。我们发现这样会留下一个巨大的“泡沫”未利用。由于模型被分割，我们必须通过设备按顺序处理所有数据。这意味着当一个GPU处理时，其他GPU将处于闲置状态。在图3.6中，我们可以看到这种天真的方法和一个巨大的闲置泡泡。我们还看到了更好的利用每个设备的方法。我们通过发送小批量数据来实现这个目标。较小的批量允许第一个GPU更快地完成它正在处理的内容，并转向下一个批次。这使得下一个设备可以更早地开始，并减小了泡沫的大小。
- en: Figure 3.6 The Bubble Problem. When data runs through a broken up model, the
    GPUs holding the model weights are underutilized while they wait for their counterparts
    to process the data. A simple way to reduce this bubble is to use microbatching.
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6“泡沫问题”。当数据通过断开的模型运行时，保存模型权重的GPU在等待对应设备处理数据时被低效利用。减小这个泡泡的一个简单方法是使用微批处理。
- en: '![](images/03__image006.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image006.png)'
- en: 'We can actually calculate the size of the bubble quite easily with the following
    formula:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下公式很容易地计算出泡沫的大小：
- en: Idle Percentage = 1 - m / ( m + n - 1)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 闲置百分比 = 1 - m / ( m + n - 1)
- en: Where m is the number of microbatches, and n is the depth of the pipeline or
    number of GPUs. So for our naive example case of 4 GPUs and 1 large batch we see
    the devices sitting idle 75% of the time! GPUs are quite expensive to allow to
    be sitting idle three quarters of the time. Let’s see what that looks like using
    the microbatch strategy. With a microbatch of 4, it cuts this almost in half,
    down to just 43% of the time. What we can glean from this formula is that the
    more GPUs we have, the higher the idle times, but the more microbatches the better
    the utilization.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 m 是微批次的数量，n 是流水线深度或 GPU 的数量。因此，对于我们的简单示例情况，使用 4 个 GPU 和 1 个大批次，我们看到设备有 75%
    的时间处于空闲状态！让 GPU 在四分之三的时间内空闲是相当昂贵的。让我们看看使用微批处理策略会是什么样子。使用微批处理量为 4，几乎可以将这个数字减少一半，降至仅为
    43% 的时间。我们可以从这个公式中得出的结论是，我们拥有的 GPU 越多，空闲时间就越长，但是微批处理的数量越多，利用率就越高。
- en: Unfortunately, we can often neither reduce the number of GPUs nor can we just
    make the microbatches as large as we want. There are limits. For GPU’s we just
    have to use as many as it takes to fit the model into memory, but try to use a
    few larger GPUs as it will lead to a more optimal utilization compared to using
    many smaller GPUs. Reducing the bubble in pipeline parallelism is another reason
    why compression is so important. For microbatching, the first limit is obvious
    once told, since the microbatch is a fraction of our batch size, we are limited
    by how big that is. The second is that each microbatch increases the memory demand
    for cached activations in a linear relationship. One way to counter this higher
    memory demand is a method called PipeDream[[15]](#_ftn15). There are different
    configurations and approaches, but the basic idea is the same. In this method
    we start working on the backward pass as soon as we’ve finished the forward pass
    of any of the microbatches. This allows us to fully complete a training cycle
    and release the cache for that microbatch.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们通常既不能减少 GPU 的数量，也不能随意增加微批处理的大小。有一些限制。对于 GPU，我们只需要使用足够多的 GPU 将模型装入内存中，但是尽量使用较少的较大
    GPU，因为与使用许多较小的 GPU 相比，这将导致更优化的利用率。减少管道并行中的气泡是压缩如此重要的另一个原因。对于微批处理，第一个限制显而易见，一旦告知，由于微批处理是批处理大小的一部分，我们受到其大小的限制。第二个是，每个微批处理都会以线性关系增加缓存激活的内存需求。对抗这种更高内存需求的一种方法是一种称为
    PipeDream 的方法[[15]](#_ftn15)。有不同的配置和方法，但基本思想是相同的。在这种方法中，我们在完成任何微批处理的前向传递后立即开始进行反向传递。这使我们能够完全完成训练周期并释放该微批处理的缓存。
- en: 3D Parallelism
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3D 并行
- en: For LLMs, we are going to want to take advantage of all three parallelism practices
    as they can all be run together. This is known as 3D Parallelism combining Data,
    Tensor, and Pipeline Parallelism (DP + TP + PP) together. Since each technique,
    and thus dimension, will require at least 2 GPUs, in order to run 3D Parallelism,
    we’ll need at least 8 GPUs to even get started. How we configure these GPUs will
    be important to get the most efficiency out of this process, namely, because TP
    has the largest communication overhead we want to ensure these GPUs are close
    together, preferably on the same node and machine. PP has the least communication
    volume of the three, so breaking up the model across nodes is the least expensive
    here.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LLMs，我们希望利用所有三种并行实践，因为它们都可以一起运行。这被称为 3D 并行，将数据（DP）、张量（TP）和管道（PP）并行结合在一起。由于每种技术，因此每个维度都至少需要
    2 个 GPU，为了运行 3D 并行，我们至少需要 8 个 GPU 才能开始。我们如何配置这些 GPU 对于提高这一过程的效率至关重要，主要是因为 TP 的通信开销最大，我们希望确保这些
    GPU 靠近在一起，最好在同一节点和机器上。PP 是三者中通信量最小的，因此在此处将模型分散到多个节点上是最不昂贵的。
- en: By running the three together, we see some interesting interactions and synergies
    between them. Since TP splits the model to work well within a device's memory,
    we see that PP can perform well even with small batch sizes due to the reduced
    effective batch size enabled by TP. This combination also improves the communication
    between DP nodes at different pipeline stages, allowing DP to work effectively
    too. The communication bandwidth between nodes is proportional to the number of
    pipeline stages, because of this DP is able to scale well even with smaller batch
    sizes. Overall, we see running in combination that
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将它们同时运行，我们可以看到它们之间的一些有趣的相互作用和协同效应。由于 TP 将模型分割以使其适用于设备的内存，我们可以看到由于 TP 可能实现的有效批量大小减小，PP
    可以在小批量大小下表现出色。这种组合还改善了 DP 节点在不同流水线阶段之间的通信，使 DP 也能有效地工作。由于节点之间的通信带宽与流水线阶段的数量成比例，因此
    DP 可以轻松扩展，即使批量大小较小也可以如此。总的来说，我们看到通过组合运行：
- en: Now that we know some tricks of the trade, it’s just as important to have the
    right tools to do the job.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了一些行业诀窍，同样重要的是拥有合适的工具来完成工作。
- en: 3.4 Large Language Models Operations Infrastructure
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 大型语言模型操作基础设施
- en: We are finally going to start talking about the infrastructure needed to make
    this all work. This likely comes as a surprise as I know several readers would
    have expected this section at the beginning of chapter 1\. Why wait till the end
    of chapter 3? In the many times I’ve interviewed Machine Learning Engineers I
    often asked this open-ended question, “What can you tell me about MLOps?” An easy
    softball question to get the conversation going. Most junior candidates would
    immediately start jumping into the tooling and infrastructure. It makes sense,
    there are so many different tools available. Not to mention, whenever you see
    posts or blogs describing MLOps there’s a pretty little diagram showing the infrastructure.
    While all of that is important it’s useful to recognize what a more senior candidate
    jumps into, the machine learning lifecycle.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于要开始讨论使所有这些工作的基础设施。这可能会让一些读者感到惊讶，因为我知道有很多读者希望在第一章开始时就讨论这个问题。为什么要等到第3章的结束才讨论呢？在我面试机器学习工程师的许多次中，我经常问这个开放式的问题：“你能给我讲讲MLOps吗？”这是个很简单的传统问题，可以让对话顺利进行。大多数初级候选人会立即开始讨论工具和基础设施。这是有道理的，因为有很多不同的工具可用。更不用说，每
- en: For many the nuance is lost, but the infrastructure is the how, the lifecycle
    is the why. Most companies can get by with just bare-bones infrastructure. I’ve
    seen my share of scrappy systems that exist entirely on one Data Scientist’s laptop,
    and they work surprisingly well! Especially in the era of scikit learn everything.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多人来说，细微差别已经失去了意义，但基础设施是“怎样做”，生命周期才是“为什么做”。大多数公司只需要基本的基础设施就可以应付。我见过许多充满干劲的系统，它们完全存在于一个数据科学家的笔记本电脑上，而且效果出奇的好！尤其是在
    scikit-learn 无所不能的时代。
- en: Unfortunately, a rickshaw machine learning platform doesn’t cut it in the world
    of LLMs. Since we still live in a world where the standard storage capacity of
    a MacBook Pro laptop is 256GB, just storing the model locally can already be a
    problem. Companies that invest in a more sturdy infrastructure are better prepared
    for the world of LLMs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在 LLM 的世界中，人力车式的机器学习平台是无法满足要求的。鉴于我们仍生活在苹果MacBookPro笔记本电脑标准存储容量为256GB的世界中，仅仅在本地存储模型就可能成为一个问题。投资于更稳固的基础设施的公司更好地为LLMs的世界做好了准备。
- en: In Figure 3.7 we see an example MLOps Infrastructure designed with LLMs in mind.
    While most infrastructure diagrams I’ve seen in my time have always simplified
    the structure to make everything look clean, the raw truth is that there’s a bit
    more complexity to the entire system. Of course a lot of this complexity would
    disappear if we could just get Data Scientists to work inside scripts instead
    of ad hoc workstations–usually with a jupyter notebook interface.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.7中，我们看到一个以LLMs为设计理念的MLOps基础设施示例。虽然我在工作中看到的大部分基础设施图都是简化的，以使整个系统看起来干净整洁，但真实情况是整个系统有更多的复杂性。当然，如果能让数据科学家在脚本中工作而不是临时工作站上工作，那么很多复杂性将会消失，通常使用jupyter
    notebook界面。
- en: Figure 3.7 a high level view of an MLOps infrastructure with LLMs in mind. This
    attempts to cover the full picture, and the complexity of the many tools involved
    to make ML models work in production.
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7展示了一个以LLMs为设计理念的MLOps基础设施的高级视图。这试图覆盖整个情景，以及使机器学习模型在生产环境中工作所涉及的许多工具的复杂性。
- en: '![](images/03__image007.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](images/03__image007.png)'
- en: 'Taking a closer look at Figure 3.7 you can see several tools on the outskirts
    that squarely land in DataOps, or even just DevOps. Data Stores, Orchestrators,
    Pipelines, Streaming Integrations, and Container Registries. These are tools you
    are likely already using for just about any data intensive application and aren’t
    necessarily MLOps focused. Towards the center we have more traditional MLOps tools,
    Experiment Trackers, Model Registry, Feature Store, and Ad hoc Data Science Workstations.
    For LLMs we really only introduce one new tool to the stack: a Vector Database.
    What’s not pictured because it intertwines with every piece is the Monitoring
    System. This all culminates to what we are working towards in this book, a Deployment
    Service, where we can confidently deploy and run LLMs in Production.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看看图3.7，您可以看到几个工具位于DataOps的外围，甚至只是DevOps。数据存储、编排器、管道、流集成和容器注册表。这些工具可能已经在您的任何数据密集型应用程序中使用，并不一定是MLOps专注的。向中心靠拢，我们有更传统的MLOps工具，实验跟踪器、模型注册表、特征存储和特定数据科学工作站。对于LLMs，我们实际上只向堆栈引入了一个新工具：向量数据库。因为它与每个部分都交织在一起，所以没有显示的是监控系统。这一切都归结为我们在这本书中正在努力实现的东西，即一个部署服务，在这里我们可以自信地部署和运行LLMs。
- en: Infrastructure by discipline
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 按学科划分的基础设施
- en: '**DevOps:** In charge of procuring the environmental resources—experimental
    (dev, staging) and production—this includes hardware, clusters, and networking
    to make it all work. Also in charge of basic infrastructure systems like Github/Gitlab,
    artifact registries, container registries, application or transactional databases
    like Postgres or MySQL, caching systems, and CI/CD pipelines. This is by no means
    a comprehensive list.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DevOps：** 负责获取环境资源——实验性（开发、预发布）和生产环境——包括硬件、集群和网络，以使所有工作正常运行。还负责基础架构系统，如 Github/Gitlab、工件存储库、容器注册表、应用程序或事务性数据库（如Postgres或MySQL）、缓存系统和CI/CD管道。这绝不是一个详尽的列表。'
- en: '**DataOps:** In charge of data, in motion and at rest. Includes centralized
    or decentralized data stores like Data Warehouses, Data Lakes, and Data Meshes.
    As well as data pipelines either in batch systems or in streaming systems with
    tools like Kafka and Flink. Also includes orchestrators like Airflow, Prefect
    or Mage. DataOps is built on top of DevOps. For example, I’ve seen many CI/CD
    pipelines being used for data pipeline work until eventually being graduated to
    systems like Apache Spark or DBT.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataOps：** 负责数据，包括运动中和静止中的数据。包括集中式或分散式数据存储，如数据仓库、数据湖和数据网格。以及批处理系统或流处理系统中的数据管道，使用诸如Kafka和Flink之类的工具。还包括像Airflow、Prefect或Mage这样的编排器。DataOps建立在DevOps之上。例如，我见过许多CI/CD管道被用于数据管道工作，最终毕业到像Apache
    Spark或DBT这样的系统。'
- en: '**MLOps:** In charge of machine learning lifecycle from creation of models
    to deprecation. This includes data science workstations like Jupyterhub, experiment
    trackers, and a model registry. It includes specialty databases like Feature Stores
    and Vector Databases. As well as a deployment service to tie everything together
    and actually serve results. It is built on top of both DataOps and DevOps.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLOps：** 负责从模型创建到弃用的机器学习生命周期。这包括数据科学工作站，如Jupyterhub，实验跟踪器和模型注册表。它包括专业数据库，如特征存储和向量数据库。以及一个部署服务，将所有内容联系在一起并实际提供结果。它建立在DataOps和DevOps的基础上。'
- en: Let’s go through each piece of the infrastructure puzzle and discuss features
    you should be considering when thinking about LLMs in particular. While we will
    be discussing tooling that is specialized for each piece, I’ll just make note
    that there are also MLOps as a service platforms like Dataiku, Amazon’s Sagemaker
    and Google’s VertexAI. These platforms attempt to give you the whole puzzle, how
    well they do that is another question, but are often a great shortcut and you
    should be aware of them. Well, I think that’s enough dilly-dallying, let’s dive
    in already!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个讨论基础设施拼图的每个部分，并讨论你在特定情况下应该考虑的功能。虽然我们将讨论专门针对每个部分的工具，但我只想指出还有MLOps作为服务平台，如Dataiku、亚马逊的Sagemaker和谷歌的VertexAI。这些平台试图为您提供整个拼图，它们是否做得很好是另一个问题，但通常是一个很好的捷径，您应该意识到它们。好了，我觉得这已经够拖拉了，让我们开始吧！
- en: 3.4.1 Data Infrastructure
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 数据基础设施
- en: While not the focus of this book it’s important to note that MLOps is built
    on top of a Data Operations infrastructure–which itself is built on top of DevOps.
    Key features of the DataOps ecosystem include a data store, an orchestrator, and
    pipelines. Additional features usually required include a container registry and
    a streaming integration service.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的重点不是Data Ops，但要注意的是，MLOps是构建在一个数据运营基础设施之上的，而这个基础设施本身是建立在DevOps之上的。DataOps生态系统的关键特点包括数据存储、编排器和流程。通常还需要的其他功能包括容器注册表和流媒体集成服务。
- en: Data stores are the foundation of DataOps and come in many forms these days,
    from a simple database to large data warehouses, and from even larger data lakes
    to an intricate data mesh. This is where your data is stored and a lot of work
    goes into managing, governing, and securing the data store. The orchestrator is
    the cornerstone of DataOps as it’s a tool that manages and automates both simple
    and complex, multistep workflows and tasks. Ensuring they run across multiple
    resources and services in a system. The most commonly talked about being Airflow,
    Prefect, and Mage. Lastly, pipelines are the pillars. They hold everything else
    up, and are where we actually run our jobs. Initially built to simply move, clean,
    and define data, these same systems are used to run machine learning training
    jobs on a schedule, do batch inference, and loads of other work needed to ensure
    MLOps runs smoothly.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储是DataOps的基础，如今有许多形式，从简单的数据库到大型数据仓库，从更大型的数据湖到复杂的数据网格。这是存储数据的地方，需要进行大量的工作来管理、治理和保护数据存储。编排器是DataOps的基石，它是一个管理和自动化简单和复杂、多步骤工作流和任务的工具，确保它们在系统中多个资源和服务上运行。最常谈论的工具包括Airflow、Prefect和Mage。最后，流程是支柱。它们支撑其他所有东西，也是我们实际运行作业的地方。最初设计是用来简单地移动、清洗和定义数据的，这些系统也用于按计划运行机器学习训练任务、批处理推理和许多其他工作，以确保MLOps的顺利运行。
- en: A container registry is a keystone of DevOps and subsequently DataOps and MLOps
    as well. Being able to run all our pipelines and services in containers is necessary
    to ensure consistency. Streaming services are actually a much bigger beast than
    what I may let on in this chapter, and if you know you know. Thankfully for most
    text related tasks real time processing isn’t a major concern. Even for tasks
    like real-time captioning or translation, we can often get by with some sort of
    pseudo real-time processing strategy that doesn’t degrade the user experience
    depending on the task.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一个容器注册表是DevOps的关键，随之也是DataOps和MLOps的关键。能够在容器中运行所有的流程和服务是确保一致性的必要条件。流媒体服务实际上比我在本章中所形容的要复杂得多，如果你了解的话就知道了。值得庆幸的是，对于大多数与文本相关的任务来说，实时处理并不是一个重大问题。即使对于实时字幕或翻译等任务，我们通常也可以通过一些伪实时处理策略来保持用户体验，而不会降低用户体验。
- en: 3.4.2 Experiment Trackers
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 实验跟踪器
- en: Experiment trackers are central to MLOps. Experiment trackers do the fundamental
    job of keeping track and recording tests and results. As the famous Adam Savage
    quote from Mythbusters, “Remember kids, the only difference between screwing around
    and science is writing it down.” Without it, your organization is likely missing
    the “science” in data science which is honestly quite embarrassing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 实验跟踪器是MLOps的核心。实验跟踪器的基本工作是跟踪和记录测试和结果。正如著名的流言制造者亚当·萨维奇在其中引用的名言：“记住孩子们，玩耍和科学的唯一区别就是把它写下来。”如果没有实验跟踪器，你的组织很可能缺少数据科学中的“科学”，这实在是相当尴尬。
- en: Even if your data scientists are keen to manually track and record results in
    notebooks, it might as well be thrown in the garbage if it’s not easy for others
    to view and search for. This is really the purpose of experiment trackers, to
    ensure knowledge is easily shared and made available. Eventually a model will
    make it to production and that model is going to have issues. Sure, you can always
    just train a new model, but unless the team is able to go back and investigate
    what went wrong the first time you are likely to repeat the same mistakes over
    and over.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的数据科学家热衷于在笔记本中手动跟踪和记录结果，如果其他人无法轻松查看和搜索，则这些工作可能是多余的。这实际上是实验跟踪器的目的，确保知识能够轻松共享和提供。最终模型将投入生产，并且该模型将遇到问题。当然，你总是可以训练一个新模型，但是除非团队能够回过头去调查第一次出错的原因，否则很可能会一遍又一遍地犯同样的错误。
- en: There are many experiment trackers out there, the most popular by far is MLFlow
    which is open source. It was started by the team at Databricks which also offers
    an easy hosting solution. Some paid alternatives worth checking out include CometML
    and Weights and Biases.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有很多实验追踪工具，迄今为止最流行的是开源的MLFlow。它是由Databricks团队发起的，该团队还提供了一个易于使用的托管解决方案。一些值得注意的付费替代品包括CometML和Weights
    and Biases。
- en: Experiment trackers nowadays come with so many bells and whistles. Most open
    source and paid solutions will certainly have what you need when looking to scale
    up your needs for LLMOps. However, ensuring you take advantage of these tools
    correctly might require a few small tweaks. For example, the default assumption
    is usually that you are training a model from scratch, but often when working
    with LLMs you will be finetuning models instead. In this case, it’s important
    to note the checkpoint of the model you started from. If possible, even linking
    back to the original training experiment. This will allow future scientists to
    dig deeper into their test results, find original training data, and discover
    paths forward to eliminate bias.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，实验追踪器配备了许多附加功能。大多数开源和付费解决方案在寻求扩展LLMOps需求时肯定会满足你的需求。然而，确保你正确利用这些工具可能需要一些小调整。例如，默认的假设通常是你正在从头开始训练一个模型，但是通常在使用LLM时，你会对模型进行微调。在这种情况下，重要的是要注意您从哪个模型检查点开始。如果可能的话，甚至链接回原始训练实验。这将使未来的科学家能够更深入地研究他们的测试结果，找到原始训练数据，并找到消除偏见的前进路径。
- en: Another feature to look out for is evaluation metric tooling. We will be going
    more in-depth in Chapter 4, but evaluation metrics are difficult for language
    models. There are often multiple metrics you care about and none of them are simple
    like complexity ratings or similarity scores. While experiment tracker vendors
    try to be agnostic and unopinionated about evaluation metrics they should at least
    make it easy to compare models and their metrics to make it easy to decide which
    one is better. Since LLMs have become so popular some have made it easy to evaluate
    on the more common metrics like ROUGE for text summarization.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个要注意的功能是评估指标工具。我们将在第4章中更深入地讨论，但是评估指标对于语言模型来说是困难的。通常会有多个你关心的指标，而且没有一个像复杂度评级或相似度分数那样简单。虽然实验追踪器供应商尽力保持对评估指标的中立和无偏见，但它们至少应该使比较模型和它们的指标变得容易，以便决定哪个更好。由于LLM已经变得如此流行，一些工具已经使评估更常见的指标，如ROUGE用于文本摘要，变得容易。
- en: You will also find many experiment tracking vendors have started to add tools
    specifically for LLMs. Some features you might consider looking for include direct
    HuggingFace support, LangChain support, prompt engineering toolkits, finetuning
    frameworks, and foundation model shops. The space is developing quickly, and no
    one tool has all the same features right now, but I’m sure these feature sets
    will likely converge.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会发现许多实验追踪供应商已经开始添加专门用于LLM的工具。一些你可能考虑寻找的功能包括直接支持HuggingFace、LangChain支持、提示工程工具包、微调框架和基础模型商店。这个领域正在迅速发展，目前没有一个工具拥有完全相同的功能，但我相信这些功能集可能会趋于一致。
- en: 3.4.3 Model Registry
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 模型注册表
- en: The model registry is probably the simplest tool of an MLOps infrastructure.
    The main objective is one that’s easy to solve, we just need a place to store
    the models. I’ve seen many successful teams get by simply by putting their models
    in an object store or shared file system and calling it good. That said, there’s
    a couple bells and whistles you should look for when choosing one.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 模型注册表可能是MLOps基础架构中最简单的工具之一。主要目标是一个容易解决的问题，我们只需要一个地方来存储模型。我见过许多成功的团队仅仅通过将他们的模型放在对象存储或共享文件系统中来解决问题。尽管如此，在选择时还有一些要注意的细节。
- en: The first is whether or not the model registry tracks metadata about the model.
    Most of what you care about is going to be in the experiment tracker, so you can
    usually get away with simply ensuring you can link the two. In fact, most model
    registries are built into experiment tracking systems because of this. However,
    an issue I’ve seen time and time again with these systems happens when the company
    decides to use an open source model or even buy one. Is it easy to upload a model
    and tag it with relevant information? The answer is usually no.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要考虑的是模型注册表是否跟踪模型的元数据。大部分你关心的内容都会在实验跟踪器中，所以通常你只需确保可以链接这两者即可。事实上，大多数模型注册表都是内建到实验跟踪系统中的，因为如此。然而，我一再看到这些系统的一个问题就是当公司决定使用一个开源模型甚至购买一个模型时会出现问题。上传一个模型并标记相关信息容易吗？通常情况下是否定的。
- en: Next, you are going to want to make sure you can version your models. At some
    point, a model will reach a point where it’s no longer useful and will need to
    be replaced. Versioning your models will simplify this process. It also makes
    running production experiments like A/B testing or shadow tests easier.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你会想确保你可以对模型进行版本控制。在某个时候，一个模型会达到它不再有用并需要被替换的时候。版本化你的模型会简化这个过程。它还会使运行生产实验如A/B测试或阴影测试更容易。
- en: Lastly, if we are promoting and demoting models, we need to be concerned with
    access. Models tend to be valuable intellectual property for many companies, ensuring
    only the right users have access to the models is important. But it’s also important
    to ensure that only the team that understands the models, what they do and why
    they were trained, are in charge of promoting and demoting the models. The last
    thing we want is to delete a model in production or worse.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们正在升级和降级模型，我们需要关注访问权限。对许多公司来说，模型往往是有价值的知识产权，确保只有合适的用户可以访问模型是很重要的。但同样重要的是确保只有了解模型、了解它们做什么以及为什么训练它们的团队负责升级和降级模型。我们最不希望的是在生产环境中删除一个模型，或者更糟的是。
- en: For LLMs there are some important caveats you should be aware of, mainly, when
    choosing a model registry, be aware of any limit sizes. I’ve seen several model
    registries restrict model sizes to 10GB or smaller. That’s just not going to cut
    it. I could speculate on the many reasons for this but none of them are worthy
    of note. Speaking of limit sizes, if you are going to be running your model registry
    on an premise storage system like Ceph, make sure it has lots of space. You can
    buy multiple terabytes of storage for a couple hundred dollars for your on prem
    servers, but even a couple terabytes fills up quickly when your LLM is over 300GB.
    Don’t forget, you are likely to be keeping multiple checkpoints and versions during
    training and finetuning; as well as duplicates for reliability purposes. Storage
    is still one of the cheapest aspects of running LLMs though, no reason to skimp
    here and cause headaches down the road.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM（长期存储器）来说，你应该注意一些重要的注意事项，主要是，在选择模型注册表时，要注意任何限制大小。我看到过几个模型注册表将模型大小限制在10GB或更小的情况。那是不可行的。我可以推测出许多原因，但都不值得一提。说到限制大小，如果你要在像Ceph这样的本地存储系统上运行你的模型注册表，请确保它有足够的空间。你可以为你的本地服务器购买数TB的存储容量，只需花几百美元，但即使是几TB，在你的LLM超过300GB时也很快就会用完。别忘了，你可能在训练和微调期间保留多个检查点和版本，以及为了可靠性目的而重复。存储仍然是运行LLM中最便宜的方面之一，没有理由在这里吝啬而引发未来的麻烦。
- en: 'This does bring me to a good point: there''s a lot of optimization that could
    still be made, allowing for better space saving approaches to storing LLMs and
    their derivatives. Especially since most of these models will be very similar
    overall. I imagine we’ll likely see storage solutions to solve just this problem
    in the future.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实给我带来了一个好问题：仍然有许多优化可以进行，可以实现更好的空间节省方法来存储LLM及其衍生物。特别是因为这些模型大多在整体上都非常相似。我想我们可能会在未来看到解决这个问题的存储解决方案。
- en: 3.4.4 Feature Store
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 特征存储
- en: 'Feature stores solve many important problems and answer questions like: Who
    owns this feature? How was it defined? Who has access to it? Which models are
    using it? How do I serve this feature in production? Essentially, they solve the
    “single source of truth” problem. By creating a centralized store, it allows teams
    to shop for the highest quality, most well maintained, thoroughly managed data.
    Feature stores solve the collaboration, documentation, and versioning of data.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储解决了许多重要问题，并回答了诸如：谁拥有这个特征？它是如何定义的？谁可以访问它？哪些模型在使用它？我如何在生产中提供这个特征？本质上，它们解决了“单一真相来源”的问题。通过创建一个集中式存储，它允许团队购买最高质量、维护最好、管理最彻底的数据。特征存储解决了数据的协作、文档化和版本控制。
- en: If you’ve ever thought, “A feature store is just a database right?” you are
    probably thinking about the wrong type of store–we are referencing a place to
    shop not a place of storage. Don’t worry, this confusion is normal as I’ve heard
    this sentiment a lot, and have had similar thoughts myself. The truth is, modern
    day feature stores are more virtual than a physical database, which means they
    are built on top of whatever data store you are already using. For example, Google’s
    Vertex AI feature store is just BigQuery and I’ve seen a lot of confusion from
    data teams wondering, “Why don’t I just query BigQuery?” Loading the data into
    a feature store feels like an unnecessary extra step, but think about shopping
    at an IKEA store. No one goes directly to the warehouse where all the furniture
    is in boxes. That would be a frustrating shopping experience. The features store
    is the show rooms that allows others in your company to easily peruse, experience,
    and use the data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经想过，“特征存储只是一个数据库对吗？”你可能在考虑错误类型的存储——我们指的是一个购物的地方而不是一个存储的地方。别担心，这种混淆是正常的，因为我经常听到这种看法，而且我自己也有过类似的想法。事实上，现代特征存储比物理数据库更虚拟，这意味着它们建立在你已经使用的任何数据存储之上。例如，Google
    的 Vertex AI 特征存储只是 BigQuery，我看到很多数据团队对此感到困惑，“为什么我不直接查询 BigQuery？”将数据加载到特征存储中感觉像是一个不必要的额外步骤，但想想在宜家商店购物。没有人直接去存放所有家具的仓库。那将是令人沮丧的购物体验。特征存储是展厅，允许公司内其他人轻松浏览、体验和使用数据。
- en: Often times, I see people reach for a feature store to solve a technical problem
    like low latency access for online feature serving. A huge win for feature stores
    is solving the training-serving skew. Some features are just easier to do in SQL
    after the fact, like calculating the average number of requests for the last 30
    seconds. This can lead to naive data pipelines built for training, but causing
    massive headaches when going to production because getting this type of feature
    in real time can be anything but easy. Feature stores abstractions help minimize
    this burden. Related to this is feature stores point-in-time retrievals which
    are table stakes when talking feature stores. Point-in-time retrievals ensure
    that given a specific time a query will always return the same result. This is
    important because features like averages over “the last 30 seconds” are constantly
    changing, so this allows us to version the data (without the extra burden of a
    bloated versioning system), as well as ensure our models will give accurate and
    predictable responses.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 往往我看到人们拿出一个特征存储来解决像在线特征服务的低延迟访问这样的技术问题。特征存储的一个巨大优势是解决训练-服务偏差。一些特征事后用 SQL 容易实现，比如计算过去
    30 秒的请求平均数。这可能导致为训练构建的天真数据管道，在投产时引发巨大头疼，因为实时获取这种类型的特征绝非易事。特征存储的抽象帮助减轻了这一负担。与此相关的是特征存储的时间点检索，在谈论特征存储时是基本要求。时间点检索确保在特定时间给定查询将始终返回相同的结果。这很重要，因为像“过去
    30 秒”的平均值这样的特征不断变化，所以这使我们能够对数据进行版本控制（而不增加庞大的版本控制系统的额外负担），并确保我们的模型将给出准确和可预测的响应。
- en: As far as options, Feast is a popular open source feature store. FeatureForm
    and Hopsworks are also open source. All three of which offer paid hosting options.
    For LLMs I’ve heard the sentiment that feature stores aren’t as critical as other
    parts of the MLOps infrastructure. After all, the model is so large it should
    incorporate all needed features inside it, so you don’t need to query for additional
    context, just give the model the user’s query and let the model do its thing.
    However, this approach is still a bit naive and we haven’t quite gotten to a point
    where LLMs are completely self-sufficient. To avoid hallucinations and improve
    factual correctness, it is often best to give the model some context. We do this
    by feeding it embeddings of our documents we want it to know very well, and a
    feature store is a great place to put these embeddings.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 至于选择，Feast 是一个流行的开源特征存储器。FeatureForm 和 Hopsworks 也是开源的。这三个选项都提供付费的托管选项。对于 LLMs，我听说特征存储器并不像
    MLOps 基础设施的其他部分一样重要。毕竟，模型是如此庞大，它应该将所有所需的特征都内置在内部，因此您不需要查询其他上下文，只需将用户的查询提供给模型，让模型自行处理。然而，这种方法仍然有些天真，我们还没有完全达到
    LLMs 完全自给自足的程度。为了避免幻觉并提高事实的准确性，通常最好给模型一些上下文。我们通过给模型提供我们希望它非常熟悉的文档的嵌入来实现这一点，而特征存储器是放置这些嵌入的理想位置。
- en: 3.4.5 Vector Databases
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.5 向量数据库
- en: If you are familiar with the general MLOps infrastructure, most of this section
    has been review for you. We’ve only had to make slight adjustments highlighting
    important scaling concerns to make a system work for LLMs. Vector Databases however
    are new to the scene and have been developed to be a tailored solution for working
    with LLMs and language models in general, but you can also use them with other
    datasets like images or tabular data which are easy enough to transform into a
    vector. Vector databases are specialized databases to store vectors along with
    some metadata around the vector, which makes them great for storing embeddings.
    Now, while that last sentence is true, it is a bit misleading, because the power
    of vector databases isn’t in their storage, but in the way that they search through
    the data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对常规的 MLOps 基础设施熟悉，那么这一节对您来说大多数都是复习的。我们只需要做出一些微调，强调重要的扩展性问题，使系统适用于 LLMs。然而，向量数据库是场景中的新鲜事物，并且已经发展成为与
    LLMs 和语言模型一起工作的定制解决方案，但您也可以将它们用于其他数据集，如图像或表格数据，这些数据集很容易转换成向量。向量数据库是专门用来存储向量及其周围一些元数据的数据库，这使它们非常适合存储嵌入。尽管最后一句话是正确的，但有点误导，因为向量数据库的强大之处不在于它们的存储方式，而在于它们搜索数据的方式。
- en: Traditional databases, using b-tree indexing to find ID’s or text based search
    using reverse indexes, all have the same common flaw, you have to know what you
    are looking for. If you don’t have the ID or you don’t know the keywords it’s
    impossible to find the right row or document. Vector databases however, take advantage
    of the vector space meaning you don’t need to know exactly what you are looking
    for, you just need to know something similar which you can then use to find the
    nearest neighbors using similarity searches based on Euclidean distance, Cosine
    similarity, Dot product similarity, or what have you. Using a vector database
    makes solving the reverse image search problem a breeze, as an example.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 传统数据库采用B树索引查找ID或使用反向索引进行基于文本的搜索，都存在同样的缺陷，即必须知道你要搜索的内容。如果你没有ID或者不知道关键词，就无法找到正确的行或文档。然而，向量数据库利用向量空间的特点，你不需要准确知道你要搜索的内容，你只需要知道类似的内容，然后可以使用基于欧几里得距离、余弦相似度、点积相似度等的相似度搜索来找到最近的邻居。使用向量数据库可以轻松解决反向图像搜索问题，作为一个示例。
- en: At this point, I’m sure some readers may be confused. First I told you to put
    your embeddings into a feature store, and now I’m telling you to put them into
    a Vector DB, which one is it? Well that’s the beauty of it, you can do both at
    the same time. If it didn’t make sense before I hope it makes sense now. Feature
    stores are not a database they are just an abstraction, you can use a feature
    store built on top of a Vector DB and it will solve many of your problems. Vector
    DBs can be difficult to maintain when you have multiple data sources, experimenting
    with different embedding models or otherwise have frequent data update. Managing
    this complexity can be a real pain, but a feature store can handily solve this
    problem. Using them in combination will ensure a more accurate and up-to-date
    search index.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我相信一些读者可能会感到困惑。我先告诉你把你的嵌入式放入一个特征存储中，现在又告诉你把它们放入向量数据库中，到底哪一个呢？这正是它的美妙之处，你可以同时两者都做。如果之前没有意义，希望现在它能够有意义了。特征存储不是数据库，它们只是一个抽象，你可以使用一个建在向量数据库之上的特征存储，这将解决许多问题。当你有多个数据源，尝试不同的嵌入模型或经常更新数据时，向量数据库可能很难维护。管理这个复杂性可能是真正令人头疼的，但是特征存储可以轻松地解决这个问题。将它们结合使用将确保更准确和最新的搜索索引。
- en: Vector databases have only been around for a couple of years at the time of
    writing, and their popularity is still relatively new as it has grown hand in
    hand with LLMs. It’s easy to understand why since they provide a fast and efficient
    way to retrieve vector data making it easy to provide LLMs with needed context
    to improve their accuracy.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，向量数据库仅存在几年，它们的流行还是相对较新的，因为它们和 LLMs 的增长是相互关联的。这很容易理解，因为它们提供了一种快速高效的检索向量数据的方法，使提供
    LLMs 所需的上下文更加容易，从而提高其准确性。
- en: That said it’s a relatively new field and there are lots of competitors in this
    space right now, and it’s a bit too early to know who the winners and losers are.
    Not wanting to date this book too much, let me at least suggest two options to
    start Pinecone and Milvus. Pinecone is one of the first vector databases as a
    product and has a thriving community with lots of documentation. It’s packed with
    features and has proven itself to scale. Pinecone is a fully managed infrastructure
    offering that has a free tier for beginners to learn. If you are a fan of open
    source however, then you’ll want to check out Milvus. Milvus is feature rich and
    has a great community. Zilliz the company behind Milvus offers a fully managed
    offering, but it’s also available to deploy in your own clusters and if you already
    have a bit of infrastructure experience it’s relatively easy and straightforward
    to do.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这仍是一个相对较新的领域，现在有很多竞争者，谁是赢家谁是输家还为时过早。为了不让这本书过时，至少让我提出两个选项来开始使用 Pinecone
    和 Milvus。Pinecone 是最早的向量数据库之一，有着繁荣的社区和丰富的文档。它具有丰富的功能并被证明可以扩展。 Pinecone 是一个全面管理的基础设施产品，有一个免费的初学者阶段。如果你是开源的粉丝，那么你会想要尝试
    Milvus。Milvus 功能丰富，拥有庞大的社区，由 Zilliz 公司提供完全托管的服务，但也可以部署到自己的集群中，如果你已经有了一点基础设施经验，这个过程相对简单明了。
- en: There are lots of alternatives out there right now, and it’s likely worth a
    bit of investigation before picking one. Probably the two things you’ll care most
    about is price and scalability, the two often going hand in hand. After that,
    it’s valuable to pay attention to search features, like support for different
    similarity measures like cosine similarities, dot product or euclidean distance.
    As well as indexing features like HNSW (Heirarchical Navigable Small World) or
    LSH (Locality-Sensitive Hashing). Being able to customize your search parameters
    and index settings are important for any database as they allow you to customize
    the workload for your dataset and workflow allowing you to optimize query latency
    and search result accuracy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有很多替代品，值得在选择前进行一些调查。你最关心的两个问题可能是价格和可扩展性，这两者经常相辅相成。之后，值得关注搜索功能，如支持不同相似度度量（如余弦相似度，点积或欧几里得距离）。以及索引功能，如
    HNSW（层次式可导航小世界）或 LSH（局部敏感哈希）。能够自定义搜索参数和索引设置对于任何数据库来说都很重要，因为它们允许您自定义数据集和工作流的工作负载，从而优化查询延迟和搜索结果准确性。
- en: It’s also important to note that with vector databases rise in popularity we
    are quickly seeing many database incumbents like Redis and Elastic offering vector
    search capabilities. For now most of these tend to just offer the most straightforward
    feature sets, but they are hard to ignore if you are already using these tool
    sets as they can provide quick wins to get started quickly.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 随着向量数据库日益流行，需要注意的是，许多数据库老牌厂商如Redis和Elastic也迅速推出了向量搜索功能。
- en: Vector databases are powerful tools that can help you train or finetune LLMs,
    as well as improve the accuracy and results of your LLM queries.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们很重要，但它们往往是添加的最后一块拼图。这往往是有意的，因为投入资源来弄清楚如何监视模型，如果你没有任何模型来监视是没有帮助的。然而，不要犯将其延迟太久的错误。许多公司都因为一个没有人知道的走私模型而受到了严重损失，这往往会让他们付出巨大代价。另外，重要的是要意识到，您不必等待将模型投入生产才开始监视数据。有很多方法可以引入监控系统到训练和数据管道中，以提高数据治理和合规性。无论如何，通常可以通过他们的监控系统来判断数据科学组织的成熟程度。
- en: 3.4.6 Monitoring System
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量数据库是强大的工具，可以帮助您训练或微调LLM，以及提高LLM查询的准确性和结果。
- en: A monitoring system is crucial to the success of any ML system, LLMs included.
    Unlike other software applications, ML models are known to fail silently—continue
    to operate, but start to give poor results. This is often due to data drift, a
    common example being a recommendation system that give worse results overtime
    because sellers start to game the system by giving fake reviews to get better
    recommendation results. A monitoring system allows us to catch poorly performing
    models, make adjustments or simply retrain them.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这些大多数产品往往只提供最直接的功能集，但如果您已经在使用这些工具集，它们是很难被忽视的，因为它们可以提供快速启动的快速获胜。
- en: Despite their importance they are often the last piece of the puzzle added.
    This is often purposeful as putting resources into figuring out how to monitor
    models doesn’t help if you don’t have any models to monitor. However, don’t make
    the mistake of putting it off too long. Many companies have been burned by a model
    that went rogue with no one knowing about it, often costing them dearly. It’s
    also important to realize you don’t have to wait to get a model into production
    to start monitoring your data. There are plenty of ways to introduce a monitoring
    system into the training and data pipelines to improve data governance and compliance.
    Regardless, you can usually tell the maturity of a data science organization by
    their monitoring system.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 监控系统对于任何ML系统的成功至关重要，包括LLM。与其他软件应用程序不同，ML模型通常会默默失败 - 继续运行，但开始产生较差的结果。这往往是由于数据漂移引起的，一个常见的例子是推荐系统随着时间推移给出更糟糕的结果，因为卖家开始通过给出假评论来操纵系统以获得更好的推荐结果。监控系统使我们能够捕获性能不佳的模型，进行调整或简单地重新训练它们。
- en: There are lots of great monitoring tooling out there, some great open source
    options include WhyLogs and EvidentlyAI. I’m also a fan of great expectations,
    but have found it rather slow outside of batch jobs. There are also many more
    paid options out there. Typically, for ML monitoring workloads you’ll want to
    monitor everything you’d normally record in other software applications, this
    includes resource metrics like memory and CPU utilization, performance metrics
    like latency and queries per second, as well as operational metrics like status
    codes and error rates. In additional, you’ll need ways to monitor data drift both
    going in and out of the model. You’ll want to pay attention to things like missing
    values, uniqueness, and standard deviation shifts. In many instances, you’ll want
    to be able to segment your data while monitoring, e.g. for A/B testing or to monitor
    by region. Some metrics that are useful to monitor in ML systems include model
    accuracy, precision, recall and F1 scores. These are difficult since you won’t
    know the correct answer at inference time, so it’s often useful to set up some
    sort of auditing system. Of course, auditing is going to be easier if your LLM
    is designed to be a Q&A bot than if your LLM is meant to help writers be more
    creative.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多很棒的监控工具，一些很棒的开源选择包括WhyLogs和EvidentlyAI。我也很喜欢Great Expectations，但发现它在批处理作业之外速度相当慢。还有许多更多的付费选择。通常情况下，对于ML监控工作负载，你会想要监控你通常在其他软件应用程序中记录的所有内容，这包括资源指标如内存和CPU利用率，性能指标如延迟和每秒查询数，以及操作指标如状态码和错误率。此外，你还需要监控数据漂移进出模型。你会关注缺失值、唯一性和标准偏差变化等问题。在许多情况下，你会希望能够在监控时对数据进行分段，例如进行A/B测试或按区域监控。在ML系统中监控的一些有用指标包括模型准确率、精确率、召回率和F1得分。由于在推断时你不会知道正确答案，因此设置某种审计系统通常是有用的。当然，如果你的LLM被设计成一个问答机器人，而不是帮助作家更具创造力的话，审计将会更容易。
- en: This hints at a fact that for LLMs, there are often a whole set of new challenges
    for your monitoring systems even more than what we see with other ML systems.
    With LLMs we are dealing with text data which is hard to quantify as discussed
    earlier in this chapter. For instance, think about what features do you look at
    to monitor for data drift? Because language is known to drift a lot! One feature
    I might suggest is unique tokens. This will alert you when new slang words or
    terms are created, however, it still doesn’t help when words switch meaning, for
    example, when “wicked” means “cool”. I would also recommend monitoring the embeddings,
    however, you’ll likely find this to either add a lot of noise and false alarms
    or at the very least be difficult to decipher and dig into when problems do occur.
    The systems I’ve seen work the best often involve a lot of handcrafted rules and
    features to monitor, but these can be error-prone and time-consuming to create.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这暗示了一个事实，即对于LLM，监控系统通常会面临一系列全新的挑战，甚至比我们在其他ML系统中看到的挑战还要多。对于LLM，我们正在处理的是文本数据，如本章前面讨论的那样，这是很难量化的。例如，想想你会监控什么特征来检测数据漂移？因为语言变化很快！我可能会建议的一个特征是唯一标记。当新的俚语词汇或术语被创造时，这会提醒你，但是当词语的含义发生变化时，比如“wicked”表示“酷炫”时，它仍然没有帮助。我还建议监控嵌入，但是你可能会发现这要么会增加很多噪音和误报，要么至少会难以辨认和挖掘当问题发生时。我见过的效果最好的系统通常涉及大量手工制定的规则和特征来进行监控，但是这些可能会出错，并且需要花费大量时间来创建。
- en: Monitoring text based systems is far from a solved problem, mostly stemming
    from the difficulties of understanding text data to begin with. This does beg
    the question of what are the best methods to use language models to monitor themselves,
    since they are our current best solution to codifying language. Unfortunately,
    I’m not aware of anyone researching this, but imagine it’s only a matter of time.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 监控基于文本的系统远非一个解决了的问题，主要是因为理解文本数据本身的困难。这确实引出了一个问题，即如何使用语言模型来监控自己，因为它们是我们当前对语言进行编码的最佳解决方案。不幸的是，我不知道有人正在研究这个问题，但我想这只是个时间问题。
- en: 3.4.7 GPU Enabled Workstations
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU启用的工作站
- en: GPU enabled workstations and remote workstations in general are often considered
    a nice to have or luxury by many teams, but when working with LLMs that mindset
    has to change. When troubleshooting an issue or just developing a model in general,
    a data scientist isn’t going to be able to spin up the model in a notebook on
    their laptop anymore. The easiest way to solve this is to simply provide remote
    workstations with GPU resources. There are plenty of cloud solutions for this,
    but if your company is working mainly on prem, this may be a bit more difficult
    to provide, but necessary nonetheless.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 启用的工作站和远程工作站通常被许多团队认为是一种奢侈品，但是当使用 LLMs 时，这种思维方式必须改变。当解决问题或者一般地开发模型时，数据科学家不能再在笔记本电脑上的笔记本中快速启动模型了。解决这个问题最简单的方法就是提供带有
    GPU 资源的远程工作站。有很多云解决方案可以做到这一点，但是如果你的公司主要在本地工作，那么提供这一点可能会更加困难，但是却是必要的。
- en: LLMs are GPU memory intensive models. Because of this, there are some numbers
    every engineer should know when it comes to working in the field. The first, is
    how much different GPUs have. The NVIDIA Tesla T4 and V100 are two most common
    GPUs you’ll find in a datacenter, but they only have 16 GB of memory. They are
    workhorses though and cost-effective, so if we can compress our model to run on
    these all the better. After these, you’ll find a range of GPU’s like NVIDIA A10G,
    NVIDIA Quadro series, and NVIDIA RTX series that offer GPU memories in the ranges
    of 24, 32, and 48 GB. All of these are fine upgrades, you’ll just have to figure
    out which ones are offered and available to you by your cloud provider. Which
    brings us to the NVIDIA A100, which is likely going to be your GPU of choice when
    working with LLMs. Thankfully they are relatively common, and offer two different
    models providing 40 or 80 GB. The big issue you’ll have with these are that they
    are constantly in high demand by everyone right now. You should also be aware
    of the NVIDIA H100 which offers 80 GB like the A100\. The H100 NVL is promised
    to support up to 188 GB and has been designed with LLMs in mind. Another new GPU
    you should be aware of is the NVIDIA L4 Tensor Core GPU which has 24 GB and is
    positioned to take over as a new workhorse along with the T4 and V100, at least
    as far as AI workloads are concerned.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 是 GPU 内存密集型模型。因此，当涉及到在领域中工作时，每位工程师都应该知道一些数字。首先，是不同 GPU 的内存容量。NVIDIA Tesla
    T4 和 V100 是数据中心中最常见的两种 GPU，但它们只有 16 GB 的内存。它们是效率很高且性价比很高的工作马，所以如果我们能压缩我们的模型以在这些
    GPU 上运行，那就更好了。在这些之后，你会发现一系列 GPU，如 NVIDIA A10G、NVIDIA Quadro 系列和 NVIDIA RTX 系列，它们提供的
    GPU 内存范围在 24、32 和 48 GB。所有这些都是不错的升级，你只需弄清楚你的云服务提供商提供的哪些产品对你可用即可。这就引出了 NVIDIA A100，这很可能会成为你在使用
    LLMs 时的首选 GPU。值得庆幸的是，它们相对比较常见，并提供两种不同的型号，分别提供 40 或 80 GB。你将会面临的一个大问题是，它们现在每个人都在高度需求中。你还应该注意到
    NVIDIA H100，它提供的内存与 A100 相同为 80 GB。H100 NVL 承诺支持高达 188 GB，并且专为 LLMs 设计。另一个你应该知道的新
    GPU 是 NVIDIA L4 张量核 GPU，它提供 24 GB 内存，并且定位为与 T4 和 V100 一样成为新的工作马，至少在 AI 工作负载方面是这样。
- en: LLMs come in all different sizes, and it’s useful to have a horse sense for
    what these numbers mean. For example, the LLaMA model has 7B, 13B, 33B, and 65B
    parameter variants. If you aren’t sure which GPU you need to run which model off
    the top of your head, here’s a shortcut, just take the number of billions of parameters
    times it by two and that’s how much GPU memory you need. The reason is, most models
    at inference are going to default to run at half precision, FP16 of BF16, which
    means for every parameter we need at least two bytes. Thus, 7 billion * 2 bytes
    = 14 GB. You’ll need a little extra as well for the embedding model which will
    be about another GB, and more for the actual tokens you are running through the
    model. One token is about 1 MB, so 512 tokens will require 512 MB. This isn’t
    a big deal, until you consider running larger batch sizes to improve performance.
    For 16 batches of this size you’ll need an extra 8 GB of space.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 有各种不同的大小，了解这些数字的含义是很有用的。例如，LLaMA 模型有 7B、13B、33B 和 65B 参数变体。如果你不确定你需要用哪种
    GPU 来运行哪种模型，那么这里有一个捷径，只需将参数的十亿数量乘以二，就是你需要的 GPU 内存大小。原因是，大多数模型在推断时都会默认运行在半精度，FP16
    或 BF16，这意味着对于每个参数，我们至少需要两个字节。因此，7 十亿 * 2 字节 = 14 GB。你还需要额外的内存用于嵌入模型，大约还需要另外 1
    GB，以及用于实际运行模型的标记。一个标记大约是 1 MB，所以 512 个标记将需要 512 MB。这并不是一个大问题，直到你考虑到为了提高性能而运行更大的批次大小。对于这种大小的
    16 批次，你将需要额外的 8 GB 空间。
- en: Of course, so far we’ve only been talking about inference, for training you’ll
    need a lot more space. While training, you’ll always want to do this in full precision,
    and you’ll need extra room for the optimizer tensors and gradients. In general,
    to account for this you’ll need about 16 bytes for every parameter. So to train
    a 7B parameter model you’ll want 112 GB of memory.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，到目前为止，我们只谈论了推理，而对于训练，你将需要更多的空间。在训练时，你始终希望以完整精度进行，你还需要额外的空间来存储优化器张量和梯度。通常情况下，为了解决这个问题，你需要为每个参数约分配16字节。因此，要训练一个有7B参数的模型，你将需要112
    GB的内存。
- en: 3.4.8 Deployment Service
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署服务
- en: Everything we’ve been working towards is collected and finally put to good use
    here. In fact, if you took away every other service and were left with just a
    deployment service, you’d still have a working MLOps system. A deployment service
    provides an easy way to integrate with all the previous systems we talked about
    as well as configure and define the needed resources to get our model running
    in production. It will often provide boilerplate code to serve the model behind
    a REST and gRPC API or directly inside a batch or streaming pipeline.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直努力达成的目标最终在这里被集合并得到了很好的应用。事实上，如果你拿走了所有其他服务，只剩下一个部署服务，你仍然拥有一个工作的MLOps系统。部署服务提供了一种与我们之前谈到的所有系统集成以及配置和定义所需资源的简单方法，以使我们的模型在生产环境中运行起来。它通常会提供样板代码，以便在REST和gRPC
    API或直接在批处理或流式传输管道中为模型提供服务。
- en: Some tools to help create this service include NVIDIA Triton Inference Service,
    MLServer, Seldon and BentoML. These services provide a standard API interface,
    typically the KServe V2 Inference Protocol. This protocol provides a unified and
    extensible way to deploy, manage, and serve machine learning models across different
    platforms and frameworks. It defines a common interface to interact with models,
    including gRPC and HTTP/RESTful APIs. It standardizes concepts like input/output
    tensor data encoding, predict and explain methods, model health checks, and metadata
    retrieval. It also allows seamless integration with languages and frameworks including
    TensorFlow, PyTorch, ONNX, Scikit Learn, and XGBoost.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一些帮助创建此服务的工具包括NVIDIA Triton推理服务、MLServer、Seldon和BentoML。这些服务提供了一个标准的API接口，通常是KServe
    V2推理协议。该协议提供了一种统一和可扩展的方式，在不同的平台和框架上部署、管理和提供机器学习模型。它定义了一种与模型交互的通用接口，包括gRPC和HTTP/RESTful
    API。它标准化了诸如输入/输出张量数据编码、预测和解释方法、模型健康检查和元数据检索等概念。它还允许与TensorFlow、PyTorch、ONNX、Scikit
    Learn和XGBoost等语言和框架无缝集成。
- en: Of course, there are times when flexibility and customization provide enough
    value to step away from the automated path these other frameworks provide, in
    which case it’s best to reach for a tool like FastAPI. Your deployment service
    should still provide as much automation and boilerplate code here to make the
    process as smooth as possible. It should be mentioned that most of the frameworks
    mentioned above do offer custom methods, but your mileage may vary.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有时灵活性和定制化足够提供价值，使人们远离这些其他框架提供的自动化路径，这种情况下最好使用FastAPI等工具。你的部署服务仍然应该在这里提供尽可能多的自动化和样板代码，以使整个过程尽可能顺利。应该提到的是，上述大多数框架确实提供了定制方法，但效果可能有所不同。
- en: Deploying a model is more than just building the interface. Your deployment
    service will also provide a bridge to close the gap between the MLOps infrastructure
    and general DevOps infrastructure. Connecting to whatever CI/CD tooling as well
    as build and shipping pipelines your company has set up so you can ensure appropriate
    tests and deployment strategies like health checks and rollbacks can easily be
    monitored and done. This is often very platform and thus company-specific. Thus,
    it’ll also need to provide the needed configurations to talk to Kubernetes, or
    whatever other container orchestrator you may be using, to acquire the needed
    resources like CPU, Memory, and Accelerators, Autoscalers, Proxies, etc. It also
    applies the needed environment variables and secret management tools to ensure
    everything runs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型不仅仅是构建接口。你的部署服务还将提供一个桥梁，来弥合MLOps基础设施和通用DevOps基础设施之间的差距。连接到公司设置的任何CI/CD工具以及构建和部署流水线，以便你可以确保适当的测试和部署策略，如健康检查和回滚可以轻松地进行监视和执行。这通常是非常平台特定的，因此也需要提供所需的配置来与Kubernetes或者你可能使用的其他容器编排器通信，以获取所需的资源，如CPU、内存和加速器、自动伸缩器、代理等。它还应用了所需的环境变量和秘密管理工具来确保一切正常运行。
- en: All in all, this service ensures you can easily deploy a model into production.
    For LLMs, the main concern is often just ensuring the platform and clusters are
    set up with enough resources to actually provision what will ultimately be configured.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，该服务确保您能轻松将模型部署到生产环境中。对于LLMs来说，主要关注的往往只是确保平台和集群已经设置了足够的资源，以便最终配置。
- en: We’ve discussed a lot so far in this chapter, starting with what makes LLMs
    so much harder than traditional ML which is hard enough as it is. First, we learned
    that their size can’t be underestimated, but then we also discovered there are
    many peculiarities about them, from token limits to hallucinations, not to mention
    they are expensive. Fortunately, despite being difficult, they aren’t impossible.
    We discussed compression techniques and distributed computing which are crucial
    to master. We then explored the infrastructure needed to make LLMs work. While
    most of it was likely familiar, we came to realize that LLMs put a different level
    of pressure on each tool, and often we need to be ready for a larger scale than
    what we could get away with for deploying other ML models.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们已经讨论了很多内容，从LLMs比传统的机器学习困难得多，而传统机器学习本身已经很难的原因开始。首先，我们了解到它们的规模不能被低估，但我们也发现了它们有很多特殊之处，从令牌限制到幻觉，更不用说它们是昂贵的了。幸运的是，尽管困难，但它们并不是不可能的。我们讨论了压缩技术和分布式计算，这些都是至关重要的。然后我们探讨了使LLMs工作所需的基础设施。虽然大多数内容可能是熟悉的，但我们意识到LLMs对每个工具施加了不同程度的压力，并且通常我们需要为部署其他机器学习模型而能够应对的规模做好准备。
- en: 3.5 Summary
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 总结
- en: LLMs are difficult to work with mostly because they are big. Which impacts a
    longer time to download, load into memory, and deploy forcing us to use expensive
    resources.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型很难处理，主要是因为它们很大。这导致下载、加载到内存和部署所需的时间更长，迫使我们使用昂贵的资源。
- en: LLMs are also hard to deal with because they deal with natural language and
    all its complexities including hallucinations, bias, ethics, and security.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理LLMs也很困难，因为它们涉及到自然语言及其所有复杂性，包括幻觉、偏见、伦理和安全性。
- en: Regardless if you build or buy, LLMs are expensive and managing costs and risks
    associated with them will be crucial to the success of any project utilizing them.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不管是自建还是购买，LLMs都是昂贵的，管理与其相关的成本和风险对于任何利用它们的项目的成功至关重要。
- en: Compressing models to be as small as we can will make them easier to work with;
    quantization, pruning, and knowledge distillation are particularly useful for
    this.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型压缩到尽可能小将使它们更容易处理；量化、修剪和知识蒸馏对此特别有用。
- en: Quantization is popular because it is easy and can be done after training without
    any finetuning.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化很受欢迎，因为它很容易，在训练后可以进行，而无需进行任何微调。
- en: Low Rank Approximation is an effective way at shrinking a model and has been
    used heavily for Adaptation thanks to LoRA.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低秩近似是一种有效的模型压缩方式，由于LoRA的存在，它在适应性方面被广泛使用。
- en: 'There are three core directions we use to parallelize LLM workflows: Data,
    Tensor, and Pipeline. DP helps us increase throughput, TP helps us increase speed,
    and PP makes it all possible to run in the first place.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用于并行化LLM工作流的三个核心方向是：数据、张量和管道。DP帮助我们增加吞吐量，TP帮助我们提高速度，而PP使一切都能够首先运行起来。
- en: Combining the parallelism methods together we get 3D parallelism (Data+Tensor+Pipeline)
    where we find that the techniques synergize, covering each others weaknesses and
    help us get more utilization.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将并行方法组合在一起，我们得到了三维并行（数据+张量+管道），在这里我们发现这些技术相辅相成，弥补了彼此的弱点，并帮助我们更多地利用资源。
- en: The infrastructure for LLMOps is similar to MLOps, but don’t let that fool you
    since there are many caveats where “good enough” no longer works.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMOps的基础设施与MLOps类似，但不要被这个表面所迷惑，因为有许多情况下，“足够好”已经不再适用了。
- en: Many tools are offering new features specifically for LLM support.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多工具正在为LLM支持专门提供新功能。
- en: Vector Databases in particular are interesting as a new piece of the infrastructure
    puzzle needed for LLMs that allow quick search and retrievals of embeddings.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特别有趣的是向量数据库，作为LLMs所需的新基础设施拼图的一部分，它们可以快速搜索和检索嵌入。
- en: '[[1]](#_ftnref1) A. Bulatov, Y. Kuratov, and M. S. Burtsev, “Scaling Transformer
    to 1M tokens and beyond with RMT,” Apr. 2023, [https://arxiv.org/abs/2304.11062](abs.html).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#_ftnref1) A. Bulatov, Y. Kuratov, and M. S. Burtsev，“通过RMT将Transformer扩展到100万个令牌及以上，”2023年4月，[https://arxiv.org/abs/2304.11062](abs.html)。'
- en: '[[2]](#_ftnref2) R. Daws, “Medical chatbot using OpenAI’s GPT-3 told a fake
    patient to kill themselves,” AI News, Oct. 28, 2020\. [https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/](medical-chatbot-openai-gpt3-patient-kill-themselves.html)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#_ftnref2) R. Daws, “使用 OpenAI 的 GPT-3 的医疗聊天机器人告诉虚假患者自杀，” AI News, 2020年10月28日,
    [https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/](medical-chatbot-openai-gpt3-patient-kill-themselves.html)'
- en: '[[3]](#_ftnref3) T. Kington, “ChatGPT bot tricked into giving bomb-making instructions,
    say developers,” www.thetimes.co.uk, Dec 17, 2022\. [https://www.thetimes.co.uk/article/chatgpt-bot-tricked-into-giving-bomb-making-instructions-say-developers-rvktrxqb5](article.html)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#_ftnref3) T. Kington, “ChatGPT 机器人被骗提供制作炸弹的说明，开发者表示,” www.thetimes.co.uk,
    2022年12月17日, [https://www.thetimes.co.uk/article/chatgpt-bot-tricked-into-giving-bomb-making-instructions-say-developers-rvktrxqb5](article.html)'
- en: '[[4]](#_ftnref4) K. Quach, “AI game bans players for NSFW stories it generated
    itself,” www.theregister.com, Oct 8, 2021\. [https://www.theregister.com/2021/10/08/ai_game_abuse/](ai_game_abuse.html)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4]](#_ftnref4) K. Quach, “AI 游戏因为自动生成的不雅故事而禁止玩家，” www.theregister.com, 2021年10月8日,
    [https://www.theregister.com/2021/10/08/ai_game_abuse/](ai_game_abuse.html)'
- en: '[[5]](#_ftnref5) T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste,
    “Sparsity in Deep Learning: Pruning and growth for efficient inference and training
    in neural networks,” Jan. 2021, [https://arxiv.org/abs/2102.00554](abs.html).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5]](#_ftnref5) T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste,
    “深度学习中的稀疏性：神经网络的高效推理和训练的剪枝与增长方法,” 2021年1月, [https://arxiv.org/abs/2102.00554](abs.html).'
- en: '[[6]](#_ftnref6) E. Frantar and D. Alistarh, “SparseGPT: Massive Language Models
    Can Be Accurately Pruned in One-Shot,” Jan. 2023, [https://arxiv.org/abs/2301.00774](abs.html).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#_ftnref6) E. Frantar 和 D. Alistarh, “SparseGPT：大型语言模型可以一次准确剪枝,” 2023年1月,
    [https://arxiv.org/abs/2301.00774](abs.html).'
- en: '[[7]](#_ftnref7) V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter,” Oct. 2019\.
    [https://arxiv.org/abs/1910.01108](abs.html).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#_ftnref7) V. Sanh, L. Debut, J. Chaumond, 和 T. Wolf, “DistilBERT，BERT
    的精简版本：更小、更快、更便宜、更轻便,” 2019年10月, [https://arxiv.org/abs/1910.01108](abs.html).'
- en: '[[8]](#_ftnref8) R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,
    P Liang, and T. B. Hashimoto, “Alpaca: A Strong, Replicable Instruction-Following
    Model,” crfm.stanford.edu, 2023\. [https://crfm.stanford.edu/2023/03/13/alpaca.html](13.html)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#_ftnref8) R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,
    P Liang, 和 T. B. Hashimoto, “Alpaca：一个强大、可复制的指令遵循模型,” crfm.stanford.edu, 2023年3月13日,
    [https://crfm.stanford.edu/2023/03/13/alpaca.html](13.html)'
- en: '[[9]](#_ftnref9) E. J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language
    Models.,” Jun. 2021, [https://arxiv.org/abs/2106.09685](abs.html).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#_ftnref9) E. J. Hu 等人, “LoRA：大型语言模型的低秩适应,” 2021年6月, [https://arxiv.org/abs/2106.09685](abs.html).'
- en: '[[10]](#_ftnref10) For the extra curious, Parameter-Efficient Fine-Tuning (PEFT)
    is a class of methods aimed at fine-tuning models in a computational efficient
    way. The PEFT library seeks to put them all in one easy to access place and you
    can get started here: [https://huggingface.co/docs/peft](docs.html)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#_ftnref10) 对于额外好奇的人，参数高效调整（PEFT）是一类旨在以计算高效的方式微调模型的方法。PEFT 库旨在将它们放在一个易于访问的地方，你可以从这里开始：[https://huggingface.co/docs/peft](docs.html)'
- en: '[[11]](#_ftnref11) R. Henry and Y. J. Kim, “Accelerating Large Language Models
    via Low-Bit Quantization,” March 2023, [https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51226/](gtcspring23-s51226.html)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#_ftnref11) R. Henry 和 Y. J. Kim, “通过低位量化加速大型语言模型,” 2023年3月, [https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51226/](gtcspring23-s51226.html)'
- en: '[[12]](#_ftnref12) DeepSpeed is a library that optimizes many of the hard parts
    for large-scale deep learning models like LLMs and is particularly useful when
    training. Check out their MoE tutorial. [https://www.deepspeed.ai/tutorials/mixture-of-experts/](mixture-of-experts.html)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#_ftnref12) DeepSpeed 是一个优化许多大规模深度学习模型中的难点的库，比如 LLMs，在训练时特别有用。查看他们的
    MoE 教程。[https://www.deepspeed.ai/tutorials/mixture-of-experts/](mixture-of-experts.html)'
- en: '[[13]](#_ftnref13) Learn more about Ray Clusters here: [https://docs.ray.io/en/releases-2.3.0/cluster/key-concepts.html#ray-cluster](cluster.html)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#_ftnref13) 在这里了解更多有关 Ray 集群的信息：[https://docs.ray.io/en/releases-2.3.0/cluster/key-concepts.html#ray-cluster](cluster.html)'
- en: '[[14]](#_ftnref14) V. Korthikanti et al., “Reducing Activation Recomputation
    in Large Transformer Models,” May 2022, [https://arxiv.org/abs/2205.05198](abs.html)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#_ftnref14) V. Korthikanti 等人, “减少大型变换模型中的激活重新计算,” 2022年5月, [https://arxiv.org/abs/2205.05198](abs.html)'
- en: '[[15]](#_ftnref15) A. Harlap et al., “PipeDream: Fast and Efficient Pipeline
    Parallel DNN Training,” Jun. 08, 2018\. [https://arxiv.org/abs/1806.03377](abs.html)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15]](#_ftnref15) A. Harlap等人，“PipeDream：快速高效的管道并行DNN训练”，2018年6月8日。[https://arxiv.org/abs/1806.03377](abs.html)'
