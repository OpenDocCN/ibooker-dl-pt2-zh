# 第一章：了解生成式人工智能基础

### 本章内容包括

+   生成式 AI 的简介：在底层到底发生了什么？

+   区分众多生成式 AI 模型

+   回顾带领我们走向生成式 AI 革命的全球趋势

欢迎！正如广告所说，这本书已经过时了。这意味着当你打算打开它的时候，这里所写的大部分都行不通，或者已经过时到无用。现在我打赌你一定对把好钱花在这样的产品上感到有点傻。好吧，我向你保证：你买这本书的感觉可能还不如我写这本书的感觉怪异。

我们一定会开始玩乐趣的东西 - 或者至少在我刚开始写这篇文章的**以前时代**是有趣的东西 - 很快会到来。我们将学习生成式人工智能如何可以用于远远不止独立的 ChatGPT 提示。好奇是否能看到：

+   AI 能够阅读统计数据档案，然后获得重大的见解吗？

+   AI 如何访问互联网，从多个网站汇总数据，并利用该数据选择出真实世界的趋势？

+   AI 能够准确地总结你自己的大量基于文本的内容吗？

+   AI 模型是否可以微调以提供与您需求更匹配的回答？

+   AI 模型是否可以用于生成原创视频和音频内容？

我也很好奇。让我们来找出答案吧。

这本书将专注于使用生成式 AI 工具进行实际任务。这意味着我们将尽量减少驱动这些技术的底层理论和技术背景，并专注于有效的项目执行。期望您几乎马上就能学习到新的强大工具，并在接下来的内容中不断增加技能。

更重要的是：期望您在几乎立即就能变得更快更有效地完成任何事情。这只是部分原因，因为像 ChatGPT 这样生成所有那些“生成式 AI”东西的大型语言模型(LLM)聊天工具可以给出你提出的问题的惊人答案。但是，通过使用我将向您展示的*自动化和脚本工具*与 LLM 进行交互，您将很快发现这将达到一个完全不同的水平。

尽管如此，我不会撒谎：如果没有对*模型*、*温度*和*文本注入*等移动部件的逻辑至少有一定的欣赏，您可能无法从您的 AI 提示中充分发挥出 AI 的优势。我们在这里做的每个项目的每个步骤都会*起作用*，并且在我所展示的情境中甚至会有意义。但是，如果没有一些技术背景，自定义配置可能有时会有一些挑战。因此，我在书的附录中添加了一个完整的定义集合。

顺便说一下，即使不了解上述内容，您也可以在这些技术上有很大的发展空间，但是“GPT”的意思是：生成式预训练转换器(Generative Pre-trained Transformer)。这重要吗？实际上并不重要。

但首先，生成式人工智能究竟是什么，它是如何工作的，以及*AI 模型*是什么？

## 1.1 踏入生成式人工智能世界

好的，你进来了。接下来呢？

与现代 AI 工具聊天会让人感到欺骗性地 - *图灵测试* 接近 - 就像与真正的人类交谈一样。图灵测试是由人工智能先驱艾伦·图灵三分之一世纪前制定的标准。如果人类无法可靠地区分他们刚刚与另一个人类还是机器进行了交互，那么机器就被认为已经达到了标准。

我可以明确地说，如果我没有故意发起连接，我最近与 GPT 等工具的许多交互将使我对这一点不确定。但我确实在我的描述中加入了“欺骗性”一词。因为，在现实中，这都是假的。至少在这一点上，即使是最好的 AI 模型也不是以人类方式真正智能的，而且绝对不会意识到自己的存在。这实际上只是 clever 软件与大量数据集的结合，给人以智能的*印象*。

这是如何运作的呢？该软件使用*自然语言处理*来分析您的提示文本，然后在模型的训练和配置的指导下，预测最佳可能的响应。我们将在下一章更详细地讨论模型。但现在，我们要注意，“训练”包括向模型提供（几乎）整个公共互联网。所有这些内容都用于分析人类生成的文本，以便它可以使用概率计算来预测形成自己新文本的最适当方式。

对于您特定提示的初始草稿将根据预设的标准和偏好进行测试，并在最终版本显示给您之前进行迭代改进。如果您以后提出了跟进提示，LLM 将把会话中的先前交互添加到其上下文中，并重复这个过程，努力撰写自己的新响应。

正如我们在本书的其余部分中将一再看到的那样，这些相同的过程可以以快速增长的方式用于多种方式。除了文本响应外，我们已经在多模式学习方面看到了显著的进展，其中文本提示可以用于生成音频、图像、视频等等，还有谁知道什么其他的东西。

## 1.2 按功能和目标对 AI 模型进行分类

*模型*是提供特定功能和功能的软件框架。对于我们的目的，术语“模型”通常指的是设计用于理解、生成或操作人类语言的计算框架，并且通常描述为*大型语言模型*（LLM）。它从大量文本数据中学习模式、语义和语法，使其能够执行翻译、文本生成和问答等任务。LLM 的有效性依赖于其预测和生成连贯词序列的能力，使其成为跨多个应用领域的自然语言理解和生成的多功能工具。

LLM 是驱动特定提供商产品的引擎。因此，OpenAI 目前使用 GPT-(x)，而 Google 的巴德是建立在对话应用语言模型（LaMDA）和路径语言模型 2（PaLM-2）上的。我们被告知 PaLM-2 是取代 LaMDA LLM 的 LLM - 后者主要侧重于基于文本的交互。

但事情并不是那么简单。甚至在 LLM 世界*内*，“模型”这个词都可能有不同的含义。现在明确这一点可以帮助避免以后的麻烦。例如，根据他们自己的统计，OpenAI 拥有*七个*通用顶级模型，包括 GPT-3、GPT-3.5 和 GPT-4。但是，在 OpenAI 产品的上下文中，以下一些专门工具通常也被认为是模型，尽管它们实际上是利用了一个或另一个顶级模型的功能：

+   **DALL-E**用于从文本提示生成图像

+   **Whisper**，多语言语音识别模型

+   专门设计用于优化遵守 OpenAI 使用政策的**审查**模型 - 以帮助确保 LLM 不被误用

+   **嵌入**，一种用于衡量“两段文字之间相关性”的分类工具 - 这是 LLM 所做工作的关键要素

+   **Codex**，Copilot 使用的编程助手引擎 - GitHub 的人工智能工具，用于生成具有上下文意识的编程代码

但这些不应与长列表的 GPT 模型“风味”混淆，可供选择（如 code-davinci-002 或 gpt-3.5-turbo）。出于某种原因，OpenAI 也将这些都称为“模型”。虽然称这些为“模型”并不完全错误，但将它们描述为顶级 GPT 模型的专门化*版本*可能更准确一些。

无论你更喜欢如何称呼它们，了解它们的工作原理将是有用的。因此，让我们看看您可以选择的（当前）活动模型的每一个。即使在此列出的确切名称可能与您在深远未来（我不知道，也许是下周四）的官方网站上看到的名称有所不同，熟悉这些名称仍将提供有用的背景知识。

### 1.2.1 理解使用令牌

将令牌视为语言字符的单位可能会有所帮助。至少在 GPT 的宇宙中，一个令牌大致等于四个英文文本字符。有时我们对任务将消耗的*令牌数量*感兴趣，其他时候我们对完成任务效果最好的*令牌种类*感兴趣。各种模型风味之间最明显的区别是它们的最大令牌限制和其训练数据的截止日期。通常按照提示消耗的此类单位数量计费。

例如，基于 GPT-3 的模型只是在 2021 年 9 月之前的数据上进行了训练。它们不允许单个请求在提示和完成（即，响应）之间消耗超过 2049 个令牌。相比之下，更新的 GPT-4 模型将允许每个提示/完成最多使用 8192 或 32768 个令牌（标准模型允许 8192，但您可以通过有限访问 API 获得 32768）。这些限制将影响您可以整合到提示中的内容量，以及您可以从响应中期望的深度。

例如，2049 个令牌的限制意味着您的提示和响应的总内容不能超过大约 1600 个字。所以如果您的提示已经有了 1000 个字，那么剩下的空间就不多了。

然而，正如我们稍后将看到的，目前有各种工具可用于规避至少一些模型的令牌限制。

### 1.2.2 GPT-4 模型

目前有四个属于 GPT-4 家族的模型，尽管其中两个似乎是短期的“占位符”，将在达到某个内部公司里程碑时被弃用。两个更长久的模型是 `gpt-4` 和 `gpt-4-32k-0314`。如果您仔细观察他们使用的命名约定，您会发现第二个模型的名称似乎表明它提供了 32,000 个令牌的限制（`32k`），并且它是在 2023 年 3 月 14 日发布的（`0314`）。

我今天早上起床时，至少到目前为止，GPT-4 仍然没有普遍发布，甚至测试版也没有在所有平台或所有国家都可用。此外，洗衣篮还没有收好，我在去浴室的路上撞到了它。

### 1.2.3 GPT-3.5 模型

基于 GPT-3.5 的模型有四个长期模型。除了 `code-davinci-002` 外，其余模型允许 4097 个令牌。单个 `code-davinci-002` 的提示/完成可以使用多达 8001 个令牌。让我们描述一下这些模型。

+   `gpt-3.5-turbo` 专为聊天（ChatGPT 类型）进行了优化，尽管它仍然是一个很好的通用模型，而且它既更强大又价格显著更低廉，比其他 GPT-3.5 模型都更便宜。

+   `text-davinci-003` 专注于基于语言的任务，并已经针对“一致的指令遵循”进行了优化。这指的是语言模型能够始终准确地遵循用户或提示提供的一系列指令的能力。

+   `text-davinci-002` 可与 `text-davinci-003` 相提并论，但它是使用监督式微调训练的，这是一种机器学习技术，用于改进预训练模型的性能，以使它们适应执行特定任务或使它们更适用于特定应用。

+   `code-davinci-002` 主要优化用于涉及编程代码补全的任务，以帮助用户解决编程问题。

### 1.2.4 GPT-3 模型

正如你自己注意到的那样，OpenAI 在给他们的模型命名时使用了科学技术领域伟大创新者的名字。这一点在他们为 GPT-3 使用的名称中尤为明显。

+   `text-curie-001` 被描述为具有能力，同时价格特别低廉。

+   `text-babbage-001` 也许不是一个通用工具，但是对于文本分类来说表现出色。这可能包括确定客户评论或社交媒体帖子的情感（积极、消极、中性）。这就是所谓的情感分析。

+   对于大多数目的而言，`text-ada-001` 是非常快速的，但是它在简单的自然语言任务（如对话）中效果最好。

+   `davinci` 是一个出色的通用模型，能够处理更复杂的文本处理以更好地理解人类语言的细微差别。

+   `curie` 比 `davinci` 更快，更便宜。

+   `babbage` 和 `text-babbage-001` 被描述为相同的术语，尽管它的容量（1.25 亿个参数）远远低于 `text-babbage-001` 的 12 亿个参数。

+   `ada` 和 `ada-001` 被描述为相同的术语，但是与 `text-ada-001` 相比，其容量（4000 万个参数）要低得多（12.5 亿个参数）。

##### 训练参数

将更多参数纳入大型语言模型（LLM）的训练中增强了其捕捉复杂语言模式和知识的能力，从而提高了性能。模型越大，对上下文的理解越好，生成的文本就越细致。所以，如果“更大更好”，为什么不所有模型都使用 100 亿个参数？这是因为这需要大量的计算资源、数据和培训成本。

如果所有这些模型用例之间的区别让你感觉有些抽象，不要担心。事实上，所有现有的模型可能在你投入的几乎所有任务上表现出色。重要的是要知道专业化存在，并且如果你有特别前沿的需求，你可能需要寻找合适的模型。

## 1.3 模型微调

*Fine-tuning* 是指在特定任务或领域上进一步训练预训练语言模型的过程，使用带标签的数据或提示。Fine-tuning 的目标是使预训练模型适应特定任务，使其更专业化，能够生成更准确和具有上下文相关性的响应。Fine-tuning *可以* 是 ChatGPT 提示创建过程的一部分。然而，fine-tuning 的大局远远超出了简单提示的范围，包括更复杂的 AI 模型配置。我将在这里列出整个过程中可以使用的步骤：

**预训练。**语言模型最初在大量的文本数据语料库上进行训练，以学习一般的语言模式、语法和语义表示。这个预训练阶段使模型能够对语言有广泛的理解，并获取关于各种领域和主题的知识。

**任务特定数据集。**为了对预训练模型进行微调以执行特定任务，需要一个带有标签的数据集或与该任务相关的提示。数据集包含与所需输出或正确响应配对的示例或提示。例如，在情感分析中，数据集将包含被标记为积极或消极情感的句子。

**架构适应。**预训练语言模型的架构通常会被修改或扩展，以适应特定的任务或要求。这可能涉及添加任务特定的层、修改模型的注意机制，或调整输出层以匹配所需的任务格式。

**微调过程。**然后，预训练模型进一步在任务特定的数据集或提示上进行训练。在微调过程中，使用梯度优化算法（例如随机梯度下降（SGD）或 Adam）更新模型的参数，以使模型在标记的数据集中的预测与所需输出之间的差异最小化。这个过程使模型能够专门化，并适应其表示以适应手头的特定任务。

**迭代细化。**微调通常是一个迭代过程。模型在任务特定数据集上进行多次 epoch 的训练，调整参数，并随着时间的推移优化模型的性能。微调过程旨在提高模型的准确性、上下文理解，并生成任务特定的响应。

通过对预训练的语言模型进行微调，模型可以利用其对通用语言的理解，并将其调整以在特定任务或领域上更有效地执行并更准确地执行。与从头开始训练模型相比，这种方法节省了大量的计算资源和训练时间。微调允许任务专业化，并使模型能够根据其训练的特定提示或任务生成具有上下文相关性的响应。

## 1.4 使生成式人工智能运作的技术

我们可以花费大量篇幅描述推动人工智能爆炸的关键软件框架和方法论。事实上，你可以在我刚提到的附录中找到那些页面。但那些只代表了*想法*，而且通常是数十年前的想法。这些想法到底被拖延了多久？不像是 1970 年、1980 年和 2000 年时没有一大群极其聪明的工程师、数学家和理论研究人员在解决这个问题。也不像是 1970 年、1980 年和 2000 年时没有一大群雄心勃勃的科技企业家在积极寻找下一个大事件。是什么阻止了所有这些事情 30 年甚至 10 年前就发生？

大部分瓶颈都是硬件限制。如果你够老，还记得处理器速度、磁盘存储和易失性内存的成本和物理限制在 1990 年左右带来的非常不同的计算体验。那是我得到的第一台工作电脑的时候，一台由一家商业公司借给我的，之前一直在用于前沿科学研究。那个庞然大物拥有令人瞠目结舌的 640k RAM，10MB 硬盘，和只能显示文本的显示器。视频显存？别逗了。它的 CPU 甚至没有数学协处理器。

我当前使用的工作站的内存比以前多了 20,000 **倍**，存储空间多了 5,000 **倍**。而且，价格只有原价的四分之一（根据通货膨胀调整后）。我相信你明白其中的含义。

如果没有廉价的存储、内存、处理器，特别是图形处理单元（GPU）和张量处理单元（TPU），简直无法想象训练和部署像 GPT 这样的原始先驱 LLMs。

此外，云平台上所有这些资源的轻松获取，尤其是微软的 Azure，可能削减了几年的开发时间。从我在 IT 行业的早期职业生涯中，我知道研究、招标、寻求批准、购买、等待交付，然后实际在现场部署硬件需要多长时间。而且那时只是每次购买一两台机架服务器或网络交换机。我几乎无法想象组建驱动 GPT 开发所需的硬件需要多长时间。但是有了云，只需要输入信用卡信息然后点击几下按钮就可以了。

除了实际的硬件基础设施外，还有三个关键趋势使现代人工智能成为可能：

+   获得大规模数据集的访问（即互联网）：存在大量经过标记的（意思是：已经用验证过的描述标记的数据或图像）和未经标记的数据，通常称为大数据，通过提供丰富多样的、真实世界的样本，促进了生成式人工智能模型的训练。

+   提高计算效率：优化技术，例如并行处理、分布式计算和模型压缩，在提高生成式 AI 模型的效率、使其更适用于实际应用方面起着至关重要的作用。

+   研究合作和知识共享：研究社区内的积极合作和思想交流加速了生成式 AI 的进展，实现了技术、方法和最佳实践的相互交叉。

最后，还有摩尔定律：由英特尔联合创始人戈登·摩尔在 1965 年提出的观察和预测。它指出，微芯片上的晶体管数量大约每两年翻一番，从而在降低成本的同时显著提高计算能力。换句话说，集成电路上的晶体管密度往往会每 18 至 24 个月翻一倍。晶体管数量的指数增长是推动技术飞速发展的驱动力，使计算机更强大、更高效，以及电子设备更小巧、更强大。虽然摩尔定律不是物理定律，但它已经保持了几十年，并指导了半导体行业的进步。

## 1.5 AI 和数据隐私和所有权

通过本书，我们将以各种方式使用各种生成式 AI 工具。当我说“使用生成式 AI 工具”时，我真正指的是将你的提示和在许多情况下数据资源暴露给在线服务。这可能引起有关个人数据收集和使用的担忧，特别是如果数据敏感或包含可识别个人身份信息（PII）。了解 AI 如何收集和使用数据，并只提供必要和适用于预期目的的数据是很重要的。

一些 AI 工具可能会监控用户活动并收集有关用户与技术交互的信息。这可能会引起有关监视和个人信息滥用的担忧。用户在使用 AI 之前应了解正在收集什么信息以及将如何使用。

如果公开可用的生成式人工智能没有得到适当的安全保护，它们可能会带来安全风险。例如，如果攻击者获得了 AI 的训练数据或模型架构，他们可能会利用这些信息对用户（也就是你）发起有针对性的攻击。在将 LLMs 集成到诸如电网或金融网络等关键基础设施系统中存在风险。因此，如果你在 - 哦，我不知道 - 某个核武器设施工作，那么在公司引入 GPT 之前，你应该慎重考虑。

抱有最好的期望始终是一种方法。但至少考虑安全和隐私问题可能也是一个好主意。请考虑以下最佳实践：

+   选择来自声誉良好的开发者的 AI 工具，这些开发者优先考虑隐私和伦理。

+   查阅工具的文档和服务条款，了解它们如何收集、使用和保护用户数据。

+   养成只提供必要和适用于预期目的的数据的习惯。

+   保护自己的编程代码和基础设施，防止未经授权的访问和利用。

另一方面，通过使用生成 AI 服务，您也应该考虑，*您*可能会侵犯*他人*的权利。虽然不太可能，但 AI 可能会生成与其训练内容非常相似而令人不舒服的文本。如果其中任何内容不属于公共领域或是具有免版权的，您可能会发布其他人的受保护财产作为自己的。我们称之为*抄袭*。

话虽如此，出于好奇，我曾让一个朋友向一家专业的学术抄袭检测服务提交了一份大量 GPT 生成的文字样本，看看会有什么结果。在样本中的数以万计由 AI 生成的文字中，没有一个被识别为问题。所以在现实世界中，你很少会遇到这种麻烦。话虽如此，当你进入第三章时，你会亲眼见到一个恶劣的现实情况，那时你会明白保持一丁点的多虑没有坏处。谨慎总比后悔好。

## 1.6 AI 和可靠性

我们也应该谈谈*幻觉*。尽管在我们开始之前，你可能要确保 GPT（以及朋友们）不能听到。从经验来看，我可以告诉你，他们对这些讨论反应不太好。

直白地说，有时候 AI 生成的输出更多地属于*创作*而不是*聪明*。它们已被发现创造法律先例、学术论文、作者，甚至整个大学。把这放在背景中，我曾经有一个高中学生也会做这些事情。但他只是开心地戏弄系统，看看有没有人注意到。他取得了成功的学术和职业生涯。相比之下，友善的大语言模型完全不知道有什么问题，并且经常礼貌地暗示这一切都是你的错（"对于混淆我向您道歉...）"。

同样地，AI 通常不会比它们所接收到的内容更好。尽管 OpenAI 和其他 AI 公司已经尽力减小这个问题，但有一些证据表明，有时 LLM 会采纳其训练内容中的主观政治或社会观点，并在有争议的话题上显露出自己的立场。在使用 AI 响应时，这也应该成为一个考虑因素。

LLMs 在简单算术方面也是出了名的差。最近我把一份包含历史销售数据的 PDF 文件输入到一个 AI 中。文件中有些书名有多个条目 - 表示同一本书的多个版本。我想让 AI 做计算，以免花费我五到十分钟的时间设置一个简单的电子表格。

这是我的提示：

##### 提示工程

根据 PDF，你能告诉我每个标题的总销售数量吗？我希望您忽略个别的 ISBN 号，只看书名。

无论我多么频繁和精确地重新表述提示，AI 都坚持选择一个数值，似乎是随机的，忽略了其他所有的内容，并将这个单一的数字呈现为总数。但它总是非常礼貌：

对于造成的混乱，我感到抱歉。您是正确的，我错过了一些条目。以下是修正后的每本书总净销售量，考虑到 PDF 中的所有条目：

教训是我们应该像记者对待消息源一样对待 LLMs：“如果你的母亲说她爱你，要求证实。” 换句话说，在发布 AI 输出之前，请自行核实事实和来源。

## 1.7 未来展望：

在继续之前，我想让你了解大局。这是我们计划要涵盖的内容：

+   根据您组织的数据和具体需求定制文本、代码和媒体内容创作

+   在本地数据存储或实时互联网上对 AI 模型进行训练

+   探索 AI 的商业智能和分析应用

+   构建您自己的 AI 模型

+   展望生成 AI 的未来

从这一端看，情况就是这样了。现在开始阅读吧。我们在另一端见。

## 1.8 摘要

+   生成 AI 建立在数十种工具、方法和技术之上，包括自然语言处理、强化学习和神经网络。

+   数据存储、图形处理和网络连接技术的技术进步，以及硬件成本的稳定降低，推动了生成 AI 革命。
